,resume,work_exp,edu,univ,education_json
0,https://resumes.indeed.com/resume/415e74322bcea652,"[u'Data Science Consultant\nCarlson Analytics Lab - Minneapolis, MN\nJune 2017 to Present\nClient: One of the top 5 CPGs (Engagement Manager role)\n\u2022 Helping business identify the drivers of market share and further design a Discrete Choice Model to examine share\nimpact for Deodorant category in Los Angeles market.\n\u2022 Building a framework using model results in dashboard to device promotional strategies in the competitive market', u'Decision Scientist\nMU SIGMA Inc - Bengaluru, Karnataka\nNovember 2016 to April 2017\nPartnered with the HR analytics department of a leading mass media company, to track and act upon developers\nfilling timecards inconsistently and optimized reporting system. Better tracking resulted in improved resource\nallocation, project management and budget allocation for each quarter.\n\u2022 Mentored 5 new team members during their mock project training at Mu Sigma. I steered them in the right direction\nand helped communicate business insights to stakeholders towards the end of the project.', u'Trainee Decision Scientist\nJune 2015 to October 2016\n\u2022 Predicted potential churners to curb attrition for a leading US Energy Corporation by analyzing 80 hypothesis across\n4 customer segments. Deployed 19 predictive models that helped save $6M and 60% reduction in marketing.\n\u2022 Pitched and converted a $1M proof of concept to Fortune 100 energy company by designing a three-phase approach\nto improve and customize the digital marketing services currently provided.\n\u2022 Reduced turnaround time by 88% to analyze investment feasibility in oil/gas wells of a region through automation\nof heavy geo-spatial reporting, leading to savings of $5M.']","[u'Master of Science in Business Analytics in Business', u'Bachelor of Technology in Computer Science']","[u'UNIVERSITY OF MINNESOTA Minneapolis, MN\nMay 2018', u'NIRMA UNIVERSITY Ahmedabad, Gujarat\nMay 2015']","degree_1 : Master of Science in Bsiness Analytics in Bsiness, degree_2 :  Bachelor of Technology in Compter Science"
0,https://resumes.indeed.com/resume/dba6251cdaaf7827,"[u'Data Scientist Intern\nComcast Corporation - New York, NY\nMay 2017 to August 2017\nInvestigated the reasons behind the downfall of ratings for the Tonight show. Created a Sankey diagram to visualize the flow of audience across viewing segments for consecutive years\n\u25cf Developed HiveQL scripts to create cubes for a Tableau Dashboard\nCustomer Churn Model Logistic Regression, PySpark, R\n\u25cf Built a predictive model to identify the users that would come back to the season 2 from season 1 of TV shows with an accuracy of 80%', u'Data Analyst\nOla Cabs - Bengaluru, Karnataka\nJuly 2015 to July 2016\nFulfilled the role of a Program Manager to communicate between Business, Product and Engineering teams\n\u25cf Modified the broadcasting algorithm to improve customer satisfaction and driver engagement that resulted in doubling the driver base and reducing the customer complaints by 3 times\n\u25cf Developed analytics Dashboard to measure and analyze financial and business metrics using MicroStrategy\n\u25cf Conceptualized algorithm and operational process to detect customer and driver partner frauds, which resulted in saving over $1 million a month\n\u25cf Launched Freemium app subscription model for sustainable revenue generation, achieved 25% business growth\n\u25cf Automated frequent data reports requirement to all stakeholders to facilitate business growth and understanding, leading to faster decision making', u""Decision Scientist\nMu Sigma Business Solutions - Bengaluru, Karnataka\nJuly 2013 to July 2015\nRetention Campaign Logistic Regression, SQL, SAS\n\u25cf Developed a structured framework to enable a targeted, consistent and informed training program to drive usage\n\u25cf Primary focus is to enhance the quality of the training being delivered to users - giving guidance to trainers on who/what to train based on the user they are training, optimizing the bandwidth currently directed towards training\n\u25cf Impact of the program is measured by monitoring usage among users trained following training intervention with A/B testing\nDrivers of Preference for a Drug Hypothesis Testing, SQL\n\u25cf Assisted one of the major US pharmaceutical companies in identifying the patient attributes that are key in determining a patient's need and attitude towards a drug""]","[u'M.S., Business in Analytics', u'B.S. in Electrical & Electronics']","[u'Oklahoma State University Stillwater, OK\nMay 2018', u'BITS-Pilani Pilani, Rajasthan\nJuly 2013']","degree_1 : M.S., degree_2 :  Bsiness in Analytics, degree_3 :  B.S. in Electrical & Electronics"
0,https://resumes.indeed.com/resume/e5e72829213f1876,"[u""Data Scientist\nWELLSFARGO\nJuly 2017 to Present\nDescription: Wells Fargo: Provider of banking, mortgage, investing, credit card, insurance, and personal, small business, and commercial financial services. Wells Fargo became the world's largest bank by market capitalization, edging past ICBC, before slipping behind JP Morgan Chase in September 2016, in the wake of a scandal involving the creation of over 2 million fake bank accounts by Wells Fargo employees.\n\nResponsibilities:\n\u2022 Utilized Spark, Scala, Hadoop, HBase, Kafka, Spark Streaming, MLLib, R, a broad variety of machine learning methods including classifications, regressions, dimensionally reduction etc. and Utilized the engine to increase user lifetime by 45% and triple user conversations for target categories.\n\u2022 Participated in features engineering such as feature intersection generating, feature normalize and label encoding with Scikit-learn preprocessing.\n\u2022 Used Python 3.X (numpy, scipy, pandas, scikit-learn, seaborn) and Spark2.0 (PySpark, MLlib) to develop variety of models and algorithms for analytic purposes.\n\u2022 Developed and implemented predictive models using machine learning algorithms such as linear regression, classification, multivariate regression, Naive Bayes, Random Forests, K-means clustering, KNN, PCA and regularization for data analysis.\n\u2022 Ensure solutions architecture / technical architectures are documented & maintained, while setting standards and offering consultative advice to technical & management teams and involved in recommending the roadmap and an approach for implementing the data integration architecture (with Cost, Schedule & Effort Estimates)\n\u2022 Designed and developed NLP models for sentiment analysis.\n\u2022 Led discussions with users to gather business processes requirements and data requirements to develop a variety of Conceptual, Logical and Physical Data Models. Expert in Business Intelligence and Data Visualization tools: Tableau, Microstrategy.\n\u2022 Developed and evangelized best practices for statistical analysis of Big Data.\n\u2022 Designed and implemented system architecture for Amazon EC2 based cloud-hosted solution for client.\n\u2022 Designed the Enterprise Conceptual, Logical, and Physical Data Model for 'Bulk Data Storage System 'using Embarcadero ER Studio, the data models were designed in 3NF\n\u2022 Worked on machine learning on large size data using Spark and Map Reduce.\n\u2022 Collaborated with data engineers and operation team to implement ETL process, wrote and optimized SQL queries to perform data extraction to fit the analytical requirements.\n\u2022 Performed data analysis by using Hive to retrieve the data from Hadoop cluster, SQL to retrieve data from RedShift.\n\u2022 Explored and analyzed the customer specific features by using SparkSQL.\n\u2022 Performed data imputation using Scikit-learn package in Python.\n\u2022 Let the implementation of new statistical algorithms and operators on Hadoop and SQL platforms and utilized optimizations techniques, linear regressions, K-means clustering, Native Bayes and other approaches.\n\u2022 Developed Spark/Scala, SAS and R programs for regular expression (regex) project in the Hadoop/Hive environment with Linux/Windows for big data resources.\n\u2022 Conducted analysis on assessing customer consuming behaviours and discover value of customers with RMF analysis; applied customer segmentation with clustering algorithms such as K-Means Clustering and Hierarchical Clustering.\n\u2022 Built regression models include: Lasso, Ridge, SVR, XGboost to predict Customer Life Time Value.\n\u2022 Built classification models include: Logistic Regression, SVM, Decision Tree, Random Forest to predict Customer Churn Rate.\n\u2022 Used F-Score, AUC/ROC, Confusion Matrix, MAE, RMSE to evaluate different Model performance.\n\nEnvironments: AWS RedShift, EC2, EMR, Hadoop Framework, S3,HDFS, Spark(Pyspark, MLlib, Spark SQL), Python 3.x (Scikit-Learn/Scipy/Numpy/Pandas/Matplotlib/Seaborn),Tableau Desktop (9.x/10.x), Tableau Server (9.x/10.x), Machine Learning (Regressions, KNN, SVM, Decision Tree, Random Forest, XGboost, Light GBM, Collaborative filtering, Ensemble), Teradata, Git 2.x, Agile/SCRUM"", u'Data Scientist\nGENERAL DYNAMICS INFORMATION TECHNOLOGY\nApril 2016 to June 2017\nDescription: As a trusted systems integrator for more than 50 years, General Dynamics Information Technology provides information technology (IT), systems engineering, professional services and simulation and training to customers in the defense, federal civilian government, health, homeland security.\n\nResponsibilities:\n\u2022 Tackled highly imbalanced Fraud dataset using under sampling, oversampling with SMOTE and cost sensitive algorithms with Python Scikit-learn.\n\u2022 Wrote complex Spark SQL queries for data analysis to meet business requirement.\n\u2022 Developed Map Reduce/Spark Python modules for predictive analytics & machine learning in Hadoop on AWS.\n\u2022 Worked on data cleaning and ensured data quality, consistency, integrity using Pandas, Numpy.\n\u2022 Participated in feature engineering such as feature intersection generating, feature normalize and label encoding with Scikit-learn preprocessing.\n\u2022 Improved fraud prediction performance by using random forest and gradient boosting for feature selection with Python Scikit-learn.\n\u2022 Performed Na\xefve Bayes, KNN, Logistic Regression, Randomforest, SVMandXGboost to identify whether a loan will default or not.\n\u2022 Implemented Ensemble of Ridge, Lasso Regression and XGboost to predict the potential loan default loss.\n\u2022 Used various metrics(RMSE, MAE, F-Score, ROC and AUC) to evaluate the performance of each model.\n\u2022 Used big data tools Spark (Pyspark, SparkSQL, Mllib) to conduct real time analysis of loan default based on AWS.\n\u2022 Conducted Data blending, Data preparation using Alteryx and SQL for tableau consumption and publishing data sources to Tableau server.\n\u2022 Created multiple custom SQL queries in Teradata SQL Workbench to prepare the right data sets for Tableau dashboards. Queries involved retrieving data from multiple tables using various join conditions that enabled to utilize efficiently optimized data extracts for Tableau workbooks.\n\nEnvironment: MS SQL Server 2014, Teradata, ETL, SSIS, Alteryx, Tableau (Desktop 9.x/Server 9.x), Python3.x(Scikit-Learn/Scipy/Numpy/Pandas), Machine Learning (Na\xefve Bayes, KNN, Regressions, Random Forest, SVM, XGboost, Ensemble), AWS Redshift, Spark(Pyspark, MLlib, Spark SQL),Hadoop 2.x, Map Reduce, HDFS, SharePoint', u""Data Analyst/Data Scientist\nAdvance Health - Chantilly, VA\nDecember 2014 to March 2016\nDescription: Advancing Member Engagement. By engaging members when and how they want, we collect the most accurate and timely information to share with providers and health plans. Advance Health provides industry leading managed care prospective health solutions. Our combination of proprietary mobile workflow technology and highly experienced, dedicated care providers yields outstanding program results and better outcomes for our clients and their health plan members.\n\nResponsibilities:\n\u2022 Gathered, analyzed, documented and translated application requirements into data models and Supports standardization of documentation and the adoption of standards and practices related to data and applications.\n\u2022 Participated in Data Acquisition with Data Engineer team to extract historical and real-time data by using Sqoop, Pig, Flume, Hive, Map Reduce and HDFS.\n\u2022 Wrote user defined functions (UDFs) in Hive to manipulate strings, dates and other data.\n\u2022 Performed Data Cleaning, features scaling, features engineering using pandas and numpy packages in python.\n\u2022 Applied clustering algorithms i.e. Hierarchical, K-means using Scikit and Scipy.\n\u2022 Created logical data model from the conceptual model and it's conversion into the physical database design using ERWIN.\n\u2022 Mapped business needs/requirements to subject area model and to logical enterprise model.\n\u2022 Worked with DBA's to create a best fit physical data model from the logical data model\n\u2022 Redefined many attributes and relationships in the reverse engineered model and cleansed unwanted tables/ columns as part of data analysis responsibilities.\n\u2022 Enforced referential integrity in the OLTP data model for consistent relationship between tables and efficient database design.\n\u2022 Developed the data warehouse model (star schema) for the proposed central model for the project.\n\u2022 Created 3NF business area data modeling with de-normalized physical implementation data and information requirements analysis using ERWIN tool.\n\u2022 Worked on the Snow-flaking the Dimensions to remove redundancy.\n\u2022 Worked in using Teradata14 tools like Fast Load, Multi Load, T Pump, Fast Export, Teradata Parallel Transporter (TPT) and BTEQ.\n\u2022 Helped in migration and conversion of data from the Sybase database into Oracle database, preparing mapping documents and developing partial SQL scripts as required.\n\u2022 Generated ad-hoc SQL queries using joins, database connections and transformation rules to fetch data from legacy Oracle and SQL Server database systems\n\nEnvironment: Machine learning(KNN, Clustering, Regressions, Random Forest, SVM, Ensemble), Linux, Python 2.x (Scikit-Learn/Scipy/Numpy/Pandas), R, Tableau (Desktop 8.x/Server 8.x), Hadoop, Map Reduce, HDFS, Hive, Pig, HBase, Sqoop, Flume, Oracle 11g, SQL Server 2012"", u""BI Developer/Data Analyst\nSanofi - Atlanta, GA\nApril 2013 to November 2014\nDescription: Sanofi is a global life sciences company committed to improving access to healthcare and supporting the people we serve throughout the continuum of care. From prevention to treatment, Sanofi transforms scientific innovation into healthcare solutions, in human vaccines, rare diseases, multiple sclerosis, oncology, immunology, infectious diseases, diabetes and cardiovascular solutions, consumer healthcare, established prescription products and generics.\n\nResponsibilities:\n\u2022 Used SSIS to create ETL packages to Validate, Extract, Transform and Load data into Data Warehouse and Data Mart.\n\u2022 Maintained and developed complex SQL queries, stored procedures, views, functions and reports that meet customer requirements using Microsoft SQL Server 2008 R2.\n\u2022 Created Views and Table-valued Functions, Common Table Expression (CTE), joins, complex sub queries to provide the reporting solutions.\n\u2022 Optimized the performance of queries with modification in T-SQL queries, removed the unnecessary columns and redundant data, normalized tables, established joins and created index.\n\u2022 Created SSIS packages using Pivot Transformation, Fuzzy Lookup, Derived Columns, Condition Split, Aggregate, Execute SQL Task, Data Flow Task and Execute Package Task.\n\u2022 Migrated data from SAS environment to SQL Server 2008 via SQL Integration Services (SSIS).\n\u2022 Developed and implemented several types of Financial Reports (Income Statement, Profit& Loss Statement, EBIT, ROIC Reports) by using SSRS.\n\u2022 Developed parameterized dynamic performance Reports (Gross Margin, Revenue base on geographic regions, Profitability based on web sales and smart phone app sales) and ran the reports every month and distributed them to respective departments through mailing server subscriptions and SharePoint server.\n\u2022 Designed and developed new reports and maintained existing reports using Microsoft SQL Reporting Services (SSRS) and Microsoft Excel to support the firm's strategy and management.\n\u2022 Created sub-reports, drill down reports, summary reports, parameterized reports, and ad-hoc reports using SSRS.\n\u2022 Used SAS/SQL to pull data out from databases and aggregate to provide detailed reporting based on the user requirements.\nUsed SAS for pre-processing data, SQL queries, data analysis, generating reports, graphics, and statistical analyses.\n\u2022 Provided statistical research analyses and data modeling support for mortgage product.\n\u2022 Perform analyses such as regression analysis, logistic regression, discriminant analysis, cluster analysis using SAS programming.\nEnvironment: SQL Server 2008 R2, DB2,Oracle,SQL Server Management Studio, SAS/ BASE, SAS/SQL, SAS/Enterprise Guide, MS BI Suite(SSIS/SSRS), T-SQL, SharePoint 2010, Visual Studio 2010,Agile/SCRUM"", u""Data Analyst\nExceloid Soft Systems, India - IN\nNovember 2011 to March 2013\nResponsibilities:\n\u2022 Wrote SQL queries for data validation on the backend systems and used various tools like TOAD& DB Visualizer for DBMS(Oracle)\n\u2022 Perform Data analysis, Backend Database testing, Data Modeling and Developing SQL Queries to solve problems and meet user's need for Database management in Data Warehouse.\n\u2022 Utilize object-oriented languages, concepts, database design, star schemas and databases.\n\u2022 Create algorithms as needed to manage and implement proposed solutions.\n\u2022 Participate in test planning and test execution for functional, system, integration, regression, UAT (User Acceptance Testing), load and performance testing.\n\u2022 Work with test automation tools for recording/coding in Database, and execute in regression testing cycles.\n\u2022 Transferred data from various OLTP data sources, such as Oracle, MS Access, MS Excel, Flat files, CSV files into SQL Server.\n\u2022 Working with Databases DB2, Oracle DM, SQL Server for Database testing and maintenance.\n\u2022 Involved in writing and executing User Acceptance Testing (UAT) with end users.\n\u2022 Involved in Post- Implementation validations after the changes have been to the Data Marts.\n\u2022 Chart out Graphs, and Reports alike in QC to point out the percentage of Test Cases passed, and thereby to point out the percentage of Quality achieved and uploading the status daily to ART reports an in-house tool.\n\u2022 Performed extensive Data Validation, Data Verification against Data Warehouse.\n\u2022 Used UNIX to check the Data marts, Tables and Updates made to the tables.\n\u2022 Writing advanced SQL Queries to query the data from Data marts and Landings to verify the changes has been made.\n\u2022 Involved in Client requirement gathering, participated in discussion & brain storming sessions and documented requirements.\n\u2022 Validating and profiling Flat File Data into Teradata tables using UNIX Shell scripts.\n\u2022 Actively participated Functional, System and User Acceptance testing on all builds and supervised releases to ensure system / functionality integrity.\n\u2022 Closely interacted with designers and software developers to understand application functionality and navigational flow and keep them updated about Business user sentiments.\n\u2022 Interacted with developers to resolve different Quality Related Issues.\n\u2022 Wrote and executed manual test cases for functional, GUI, and regression testing of the application to make sure that new enhancements do not break working features\n\u2022 Writing and executing Manual test cases in HP Quality Center.\n\u2022 Wrote test plans for positive and negative scenarios for GUI and functional testing\n\u2022 Involved in writing SQL queries and stored procedures using Query Analyzer and matched the results retrieved from the batch log files\n\u2022 Created Project Charter documents & Detailed Requirement document and reviewed with Development & other stake holders.\nEnvironment: Subversion, Tortoise SVN, Jira, Agile-Scrum, Web Services, Mainframe, Oracle, Perl, UNIX, LINUX, Shell Scripts, UML, Quality Center, , RequisitePro, SQL, MS Visio, MS Project, Excel, Power Point, Word, SharePoint, Win XP/7 Enterprise."", u""Data Analyst/Data Modeler\nZEN3 Infosolutions, India - IN\nApril 2009 to October 2011\nResponsibilities:\n\u2022 Data analysis and reporting using MY SQL, MS Power Point, MS Access and SQL assistant.\n\u2022 Involved in MY SQL, MS Power Point, MS Access Database design and design new database on Netezza which will have optimized outcome.\n\u2022 Used DB2 Adapters to integrate between Oracle database and Microsoft SQL database in order to transfer data\n\u2022 Designed the data marts using the Ralph Kimball's Dimensional Data Mart modeling methodology using ER Studio.\n\u2022 Involved in writing T-SQL, working on SSIS, SSRS, SSAS, Data Cleansing, Data Scrubbing and Data Migration.\n\u2022 Used Normalization methods up to 3NF and De-normalization techniques for effective performance in OLTP systems.\n\u2022 Initiated and conducted JAD sessions inviting various teams to finalize the required data fields and their formats.\n\u2022 Involved in designing and implementing the data extraction (XML DATA stream) procedures.\n\u2022 Created base tables, views, and index. Built a complex Oracle procedure in PL/SQL for extract, loading, transforming the data into the warehouse via DBMS Scheduler from the internal data.\n\u2022 Involved in writing scripts for loading data to target data Warehouse using Bteq, Fast Load, MultiLoad\n\u2022 Create ETL scripts using Regular Expressions and custom tools (Informatica, Pentaho, and Sync Sort) to ETL data.\n\u2022 Developed SQL Service Broker to flow and sync of data from MS-I to Microsoft's master database management (MDM).\n\u2022 Extensively involved in Recovery process for capturing the incremental changes in the source systems for updating in the staging area and data warehouse respectively\n\u2022 Strong knowledge of Entity-Relationship concept, Facts and dimensions tables, slowly changing dimensions and Dimensional Modeling (Star Schema and Snow Flake Schema).\n\u2022 Involved in loading data between Netezza tables using NZSQL utility.\n\u2022 Worked on Data modeling using Dimensional Data Modeling, Star Schema/Snow Flake schema, and Fact & Dimensional, Physical & Logical data modeling.\n\u2022 Generated Stats pack/AWR reports from Oracle database and analyzed the reports for Oracle8.x wait events, time consuming SQL queries, table space growth, and database growth.\nEnvironment: ER Studio, MY SQL, MS Power Point, MS Access, MY SQL, MS Power Point, MS Access, Netezza, DB2, T-SQL, DTS, Informatica MDM, SSIS, SSRS, SSAS, ETL, MDM, 3NF and De-normalization, Teradata, Oracle8.x, (Star Schema and Snow Flake Schema) etc.""]","[u""Bachelor's""]",[u''],"degree_1 : ""Bachelors"""
0,https://resumes.indeed.com/resume/b49f3a859978d3e0,"[u'Data Scientist\nXPO Logistics - High Point, NC\nOctober 2017 to Present\nObjective: Build Time Series Forecast Models\nResponsibilities:\n\u2022 Worked on Human resources, Supply chain and warehouse management domains with in XPO Logistics.\n\u2022 Leading efforts of establishing Data Analytics & Machine learning platform as part of new initiative.\n\u2022 Worked with business managers to establish requirements for project inceptions.\n\u2022 Trained and monitored Consultants on machine learning techniques including ensemble, parametric & non-parametric models.\n\u2022 Conducted workshops to present and educate business teams to get upto speed with the new models.\n\u2022 Created Use Cases for HR module to perform predictive modelling using Turn Over/ Absenteeism/ Overtime data sets\n\u2022 Created algorithms to effectively predict Workforce at ware house level using Order Details, Shipping Details, weather data.\n\u2022 Created Model to predict SKU Volume using product details as warehouse level for various clients of XPO as part of POC.\n\u2022 Worked with Large Data Sets visualization using Python packages Seaborn and Matplotlib.\n\u2022 Use Python libraries like Pandas/Numpy/SKLearn/SCIPY etc to create ML models.\n\u2022 Use SCALA programming capabilities in SPARK cluster environment using ML libraries.\n\u2022 Worked with time series data using ARIMA method.\n\u2022 Use Boosting and Bagging methods like random forest/Adaboost/ gradient boost/XGBoost.\n\u2022 Use GridSearch/Cross Validation to optimize parameters of model and avoid overfitting.\n\u2022 Working with Data Engineering to create Data Pipelines for ML model building.\n\u2022 Closely work with ETL team & Reporting teams to create datasets for ML model consumption.', u'Data Scientist\nBank Of America - Pennington, NJ\nAugust 2017 to October 2017\nObjective: Build Recommendation System for Merrill Lynch ecommerce\nResponsibilities:\n\u2022 Perform EDA Analysis of Persona Data from Merrill Lynch website\n\u2022 Worked with Large Data Sets visualization using Python packages Seaborn and Matplotlib.\n\u2022 Clustering of Pages and clients to groups using hierarchical clustering and Fuzzy clustering models.\n\u2022 Prepared Machine Learning Engine Architecture for various phases of Recommendation system implementation.\n\u2022 Developing Models using DOE (Design of Experiment) Reinforcement learning and Rule based learning recommendation engines for Phase I.\n\u2022 Working on POC of GLMix/ Graphical/ Collaborative filtering ML models for Recommendation systems.\n\u2022 All the above-mentioned are being performed using Python packages pandas, numpy, sklearn and Hadoop VM environment for POC.', u'Data Scientist\nLowes - Charlotte, NC\nSeptember 2015 to June 2017\nObjective: Modelling to analyse lowes.com traffic to target customers for personalized marketing strategies.\nResponsibilities:\n\u2022 As lead Data Scientist, led and managed a team of senior analysts and an outside vendor that created a robust retention strategy that improved the customer insight, estimated customer lifetime value analysed internal and external data\n\u2022 Involved in various Stages of Software Development Life Cycle including requirement analysis, Risk Analysis, GAP Analysis, design, implementation, testing and support of business applications.\n\u2022 Worked closely with business teams to establish requirements, plan project budgeting and project deadlines.\n\u2022 Personalization, Inflection points tracking, Target Marketing, Customer profiling and Segmentation\n\u2022 Personalization for registered users, Returning visitors and Anonymous visitors\n\u2022 Worked on model building using machine learning algorithms like logistic regression, na\xefve bayes, random forest, SVM, SVR.\n\u2022 Worked on a POC project for NLP(Natural Language processing) with Lowes Review Data.\n\u2022 Propensity to convert modelling using the visit sequence and conversion sequence mapping\n\u2022 Zeroth problem solution for Anonymous visitor, returning visitor and registered users\n\u2022 Text analytics on review data machine learning technique in python using NLTK.\n\u2022 Market basket Analysis using APRIOR for purchase transactions data at store level.\n\u2022 Sequence mining for the conversion paths for different segments\n\u2022 Profiling based on the content access reports, event trigger reports and navigation reports. Profiles are then used for target marketing and segmentation\n\u2022 Custom Website strategy for segments.\n\u2022 Site/Page optimization, Promotional campaigns assessment, Customer segmentation and assessing revenue against targets at regular intervals\n\u2022 Used UCB & Thomson Sampling Intuition for Multi Bandit Testing and A/B testing to perform ad banner choices and product visual choices optimization.\n\u2022 Custom Dashboards product category wise\n\u2022 Developed visualizations using sets, Parameters, Calculated Fields, Dynamic sorting, Filtering, Parameter driven analysis, gathered data from different data marts.\n\u2022 Reporting designs based on the business specific problems, Reporting implementation on Tableau.\n\u2022 Advanced charts, drill downs and intractability are incorporated in the reporting for different stakeholders and Integrating the publishing of reports to the clients SharePoint infrastructure', u'Data Scientist\nBCBSIL - Chicago, IL\nMarch 2015 to September 2015\nObjective: Modelling Individual and Family Insurance rate based on Income and Demography for Market campaigning\nResponsibilities:\n\u2022 Participated in all phases of data mining- data collection, data cleaning, developing models, validation, visualization and performed Gap analysis.\n\u2022 Providing Ad hoc analysis and reports to Executive level management team.\n\u2022 Captured Modelling requirements from Senior Stakeholders to Bench functional requirements for SAS/ R Python\n\u2022 Performed Data Manipulation and Aggregation from Various source including HDFS.\n\u2022 Creating various B2B Predictive and descriptive analytics using R and Tableau\n\u2022 Used pandas, numpy, seaborn, scipy, matplotlib, scikit-learn in Python for developing various machine learning algorithms.\n\u2022 Designed and tested Predictive Algorithms using Historical Data\n\u2022 Utilized machine learning algorithms such as Decision Tree, linear regression, multivariate regression, Naive Bayes, Random Forests, K-means, & KNN.\n\u2022 Parsing data, producing concise conclusions from raw data in a clean, well-structured and easily maintainable format.\n\u2022 Responsible for Big data initiatives and engagement including analysis, brainstorming, POC, and architecture.\n\u2022 Worked on different data formats such as JSON, XML and performed machine learning algorithms in R\n\u2022 Worked on MapReduce/Spark Python modules for machine learning & predictive analytics in Hadoop on AWS.\n\u2022 Worked with (Tableau) Report Writers to Test, Validate Data Integrity of Reports', u'Data Scientist / Analyst\nBCBSNJ - Newark, NJ\nJuly 2013 to February 2015\nResponsibilities:\n\u2022 Conducted qualitative and quantitative research to gather data from data mart\n\u2022 Responsible for data identification, collection, exploration & cleaning for modelling, participate in model development\n\u2022 Visualize, interpret, report findings and develop strategic uses of data.\n\u2022 Understand transaction data and develop Analytics insights using Statistical models using Machine learning.\n\u2022 Involved in gathering requirements while uncovering and defining multiple dimensions. Extracted data from one or more source files and Databases.\n\u2022 Analysed data from both competitive market and the monopolistic market\n\u2022 Collected Database of sales of items in all aspects. Cleaned, filtered and transformed data to specified format.\n\u2022 Designed various reports using various reporting tools\n\u2022 Cleaned data using R, then visualize the data, and derive statistical modelling plots\n\u2022 Interacted with the other departments to understand and identify data needs and requirements and work with other members of the IT organization to deliver data visualization and reporting solutions to address those needs\n\u2022 Responsible for providing reports, analysis and insightful recommendations to business leaders on key performance metrics pertaining to sales & marketing\n\u2022 Worked on various theorems related to determining various insurance pricing (insurance coverages and costs)\n\u2022 Used R to identify product performance via Classification, tree map and regression models along with visualizing data for interactive understanding and decision making.\n\u2022 Accomplished multiple tasks from collecting data to organizing data and interpreting statistical information.\n\u2022 Created dynamic linear models to perform trend analysis on customer transactional data in R\n\u2022 Conducted exploratory and descriptive data analysis of large data sets', u'Data Analyst/ System Admin\nBCBSIL - Chicago, IL\nJanuary 2010 to June 2013\nResponsibilities:\n\u2022 Worked with team involved with the data collection, cleaning and preparation using SQL\n\u2022 Working with the analytical team to design, build, validate and refresh data models\n\u2022 Visualize, interpret, find reports and develop strategic uses of data.\n\u2022 Identify additional data sources to augment the opportunities.\n\u2022 Develop queries to retrieve and analyse data from multiple tables in the database. Find tables using Database diagrams in SQL server, establish relationship in multiple tables using Primary and Foreign keys. Perform analytical request using SQL for internal and external parties.\n\u2022 Conducted research to collect and assemble data for databases - Was responsible for design/development of relational databases for collecting data.\n\u2022 Performed Data output - Made data chart presentations using Tableau, Pivot Table, coded variables from original data, using statistical analysis as and when required and provided summaries of analysis.\n\u2022 Work with users to define data requirements; researches and collects data from a variety of sources.\n\u2022 Work with business stakeholders, application developers, and production teams and across functional units to identify business needs and discuss solution options.\n\u2022 Responsible for 24/7 on call support and support the developers in development all the time by providing logs and other admin support.\n\u2022 Performed Daily and weekly Productions checks and maintenance tasks on daily basis.\n\u2022 Installed & Configured Tools & Web Client with all the Fixed Versions.\n\u2022 Performed the Siebel Schema Upgrade (Upgphys) successfully.\n\u2022 Performed Successfully Siebel Tools Repository Merge Process.', u'Data Analyst/ System Admin\nMonsanto - St. Louis, MO\nMarch 2009 to January 2010\nResponsibilities:\n\u2022 Involved with the team responsible for data identification, collection, cleaning and exploration.\n\u2022 Filter and clean data, review reports and performance indicators to locate and correct code problems Visualize, interpret, find reports and develop strategic uses of data.\n\u2022 Used Excel for Qualitative and Quantitative data analysis techniques to find Trends and Forecasting by using advanced Data Analysis techniques like histogram, pivot table/Pivot charts.\n\u2022 Worked with various Business users to gather reporting requirements and understand the intent of reports and attended meetings to provide updates on status of the projects.\n\u2022 Performed in depth analysis of data & prepare daily reports using SQL, MS Excel, MS PowerPoint and share point.\n\u2022 Managed and manipulated data sets from multiple sources\n\u2022 Compiling data in MS Excel utilizing Pivot Tables, V-Lookups, Macros, etc.\n\u2022 Used R to import, clean data, visualize data, and derive statistical modelling plots\n\u2022 Develop, implement data collection systems and other strategies that optimize statistical efficiency and data quality\n\u2022 Track and analyse results in order to identify trends that will assist business decisions.\n\u2022 Involved in the Upgrade Plan for DEV, TEST, UAT, and PROD Environments.\n\u2022 Maintaining the system security, monitoring and responsible for Prod and non prod tickets resolution from Helpdesk support\n\u2022 Involved in the analysis to estimate the timelines for Upgrade.\n\u2022 Installed & Configured the Siebel Gateway Name Server, Enterprise Server, Database Server, Web Server Software with all Fix Packs i.e. Siebel 8.0.0.6, v7.7, v7.7.2, v7.7.2.8, v7.8.2, v7.8.2.5 to upgrade from Siebel v6.0.2.123.\n\u2022 Installed & Configured the MS SQL Server 2005 with SP2 & MS SQL Server 2000 with SP3 to configure the Siebel database.', u'System Analyst/Admin\nBCBSNJ - Newark, NJ\nJuly 2008 to March 2009\nResponsibilities:\n\u2022 Responsible for 24/7 support by carrying pager and Hot phone\n\u2022 Responsible for resolving the level 3 tickets routed from Help desk\n\u2022 Responsible for Releases and Ongoing maintenance tasks\n\u2022 Monthly and weekly server reboots and Hot fixes\n\u2022 Daily monitoring the Siebel Prod enterprise servers starting from Siebel Web, App, Gateway, BigIP and DB servers.\n\u2022 Responsible for Environment migration from dev2Test, Test2TRG and UAT2PROD\n\u2022 Installation of Actuate eReporting server for all Siebel thin client applications which includes setting up eReporting server, ReportCast and Siebel Report server Access\n\u2022 Setting up of users in Siebel using LDAP authentication and in Actuate Admin Desktop\n\u2022 Extensively involved in resolving Integration related issues\n\u2022 Configuring Siebel servers on Windows cluster environment for all communication servers and Document servers\n\u2022 Involved in Siebel and third party installations\n\u2022 Involved in trouble shooting the Production and other environments\n\u2022 Involved in resolving the priority tickets in prod environment\n\u2022 Responsible for releases and Functions as required.\n\u2022 Responsible environment issues and Builds\n\u2022 Involved in Repository migrations and user set ups\n\u2022 Involved in Pre production and Post Production activities\n\u2022 Trained new administrators who joined in the team.\n\u2022 Actively involved in team meetings and RFC.']",[u'MS in Bio-Informatics/Biology'],"[u'JNT University Hyderabad, Telangana']",degree_1 : MS in Bio-Informatics/Biology
0,https://resumes.indeed.com/resume/f787311e5f44802a,"[u'Data Scientist Intern\nQuicken Loans Inc - Detroit, MI\nMay 2017 to Present\n\u2022 Designed and implemented a pricing model using unsupervised machine learning algorithms and price elasticity modeling which resulted in an increase in revenue by 4% and client satisfaction by 8%\n\u2022 Automated the employee evaluation process by using text analytical techniques which resulted in savings of $1 million\n\u2022 Mapped the entire mortgage loan process, identified important statuses, computed the turn times between statuses and suggested changes to improve the mortgage loan process\n\u2022 Condensed large, complex data and theory into clear concise, easy to understand observations\n\u2022 Identified methods that allow continuous and automated statistical testing to enhance the predictability of deployed\nmodels', u""Data Scientist\nIT Catalyst Software India Pvt Ltd - Bengaluru, Karnataka\nMay 2014 to July 2016\n\u2022 Maintained relationship with client's senior analytics team and addressed their cross-functional challenges on a daily basis\n\u2022 Designed and delivered a customer churn attribution project using classification algorithms in R and built a strategy to improve the customer churn rate that can generate an additional $2 million in annual revenue\n\u2022 Improved the sales and coupon effectiveness for a grocery retailer using machine learning algorithms, price elasticity and market basket analysis. Resulted in savings of $0.8 million and an increase in revenue by 7%\n\u2022 Ran reports and reported results on a regular basis. Performed ad hoc reporting and analyses as needed""]","[u'Master of Science in Data Science', u'Bachelor of Engineering in Computer Science and Engineering']","[u'The University of Texas at Dallas Dallas, TX\nAugust 2016 to May 2018', u'Visvesvaraya Technological University']","degree_1 : Master of Science in Data Science, degree_2 :  Bachelor of Engineering in Compter Science and Engineering"
0,https://resumes.indeed.com/resume/6fdcaa6b2ddffecd,"[u'Data Scientist\nTEKsystems (Ford) - Dearborn, MI\nMay 2017 to Present\nLogistics Optimization\nDevelop, maintain, and optimize existing processes for ETL.\nCommunicate with business to gather suggestions for enhancements and addressing concerns.\nDesign and test different picking algorithms.\n\nQlikView\nDesign dashboards for comparing data quality of different data sources.\n\nSQL\nCreate tables for supporting workflows and dashboards.', u'Market Analyst\nBlueChip Talent (Latcha) - Farmington Hills, MI\nOctober 2016 to May 2017\nTableau\nAutomation and standardization of monthly marketing metrics report.\nData source structuring and construction.\nDevelop ad hoc reports.\n\nFile Validation\nDevelop Python script for verifying quality of mailing lists and standardized list quality descriptions.\n\nSQL\nOptimize stored procedures for readability and scalability.\nCombining target lists from different vendors to determine marketing audience.', u'Systems Architect\nCDI Contractor (Ford) - Dearborn, MI\nOctober 2014 to September 2016\n\u25cf Lead Qlikview and Tableau developer\n\u27a2 Ad Hoc visualizations combining disparate data sources across different servers and file\nformats\no Charting server ages and location\no Measuring discrepancies between different data sources\no Text parsing for data cleansing\no Implementing interval matching to connect ranged data sets\n\u27a2 Consolidate data sources to ease update of visualization\n\u25cf Qlikview Enterprise Technology Metric Dashboard\n\u27a2 Worked with business to determine standards needed for tracking IT software/application\ncomplexity\n\u27a2 Engineered solution and process for measuring status\no Systematically applying different criteria to define states and aggregate statuses\no Created workflow for repeatability\n\u27a2 Used best practices for applying aggregations in script and transforming data before loading\n\u27a2 Used Qlikview set analysis features for drill downs and different views\n\u25cf Tableau Technology Usage Dashboard\n\u27a2 Inherited dashboard for making updates and changes\n\u27a2 Made alterations to migrate data sources and include new metrics\n\u27a2 Added new parameters for increased visibility\n\u25cf Provide technical support and clarification on visualization design and purpose\n\u25cf Developed database and reporting output for complexity scheduling tool\n\u27a2 Worked with international engineering teams to devise method for keeping track of assembly\ninformation\n\u27a2 Using their specifications created a familiar and more robust method for planning and keeping\ntrack of changes', u'Data Analyst\nIPG Contractor (GM) - Warren, MI\nMarch 2014 to October 2014\n\u25cf Developed database and reporting for tracking rental vehicles associated with ignition switch recall\n\u27a2 Provided improved, practical method of providing status of rental vehicles during recall\n\u27a2 Designed database and wrote code for daily standardized reporting\no Performed QA testing and made changes to incorporate new requirements and\nrequests\n\u27a2 Database features:\no Combining data sources\no Cleansing data for joins\no Repeatable process for standardized reporting\no Writing and optimizing queries to track progress\n\u25cf Optimize and maintain Excel VBA macros and add ons\n\u27a2 Studied old macros and implemented best practices to improve performance and functionality of\nadd ons\n\u27a2 Added web scraping functionality\n\u25cf Spent 2 days standardizing and formatting information in order to automate report generation saving at\nleast 3 days of work a month\n\u27a2 Wrote macro for formatting Excel into standardized PowerPoint output\n\u25cf Quality assurance of Cognos reports after updates', u'Data Analyst\nMoeller Manufacturing\nFebruary 2013 to March 2014\n\u25cf Proposed and lead project harvesting historical data for production scheduling\n\u27a2 Writing batch files and Python scripts to consolidate network data\n\u27a2 Devised simple machine learning algorithm for schedule predictions\n\u25cf Developed plant layout production tracker\n\u27a2 Wrote updates to parts tracker to implement network crawler\n\u27a2 Performed routine checking and accounting of parts production\n\u25cf Designed and created Oracle database and web interface for tool tracking']",[u'BS in Mathematics'],[u'Michigan Technological Univ\nJanuary 2008 to January 2012'],degree_1 : BS in Mathematics
0,https://resumes.indeed.com/resume/5be8878791193eb2,"[u'Global Service Data Scientist\nAbbott Laboratories\nOctober 2016 to Present\n- Developed 2 new prognostic health monitoring (PHM) algorithms by analyzing real-time instrument data with\nR. The first algorithm used an ROC Analysis to set a failure threshold. The second algorithm used an ensemble of 4 different models to identify anomalous data points in unlabeled data.\n- Implemented 20+ PHM algorithms, for a series of diagnostic instruments, by translating algorithm parameters into SQL code.\n- Created an application to automatically monitor each algorithm daily, and distribute a report via email\ncontaining any algorithm violations.', u'Data Management Data Analyst\nAbbott Laboratories\nNovember 2014 to October 2016\n- Responsible for the development, improvement and maintenance of 20+ fully automated metric reporting applications for multiple diagnostic instrument platforms. Applications were developed using JSL (an object- oriented programming language) and SQL, and resulted in a significant reduction in man hours. For example, an approximate reduction of 4 headcount for 1 week each month for one application.\n- Contributed to the development of instrument performance metrics, and their visualizations, for weekly and monthly status reports. Example metrics included Mean Tests Between Failures, Calls Per Year and Instrument\nAvailability as measured by Uptime.', u""Prognostic Health Monitoring Data Analyst\nAbbott Laboratories\nOctober 2013 to November 2014\n- Identified instrument components/indicators suitable for PHM, as measured and prioritized by the frequency of related errors, service calls and part replacements, by completing a historical analysis on instrument and service data.\n- Developed PHM infrastructure for Abbott's next generation diagnostic instruments, as measured by successful\nPHM data collection from various instrument servo motors, through collaboration with internal and external\nsoftware and engineering groups.\n- Achieved 40% reduction in customer downtime (75% reduction in unplanned downtime), greater than $500k\nannual savings and increased customer satisfaction, as measured by current field service data and NPS surveys, by implementing three PHM algorithms for Abbott's on-market Architect i2000, i2000SR and i1000SR\nimmunoassay analyzers.\n- Advanced JSL and SQL scripting skills to a high intermediate level by creating complex, automated daily\napplications to collect, analyze and display instrument data in violation of PHM algorithms."", u""Reliability Data Analyst\nAbbott Laboratories\nNovember 2012 to March 2014\n- Improved the instrument reliability data entry application, as measured by internal customer satisfaction and feedback, by redesigning the way instrument part and error information is entered and structured for all of\nAbbott's next generation diagnostic instruments.\n- Increased metric reporting efficiency, as measured by a 50% reduction in time and headcount, by automating the generation of metric data and plots through JMP scripting for weekly and monthly reports.\n- Developed and forecasted an Instrument Availability metric, as measured by hours of planned and unplanned\ndowntime, by completing a historical data analysis on Abbott's on-market versus next generation\nimmunoassay analyzers.""]","[u'M.S. in Predictive Analytics', u'B.S. in Biomedical Engineering']","[u'DePaul University\nMarch 2015 to November 2017', u'University of Wisconsin Madison, WI\nAugust 2005 to December 2009']","degree_1 : M.S. in Predictive Analytics, degree_2 :  B.S. in Biomedical Engineering"
0,https://resumes.indeed.com/resume/b42e5f7fe18f38de,"[u""Data Scientist\nEBSCO Information Services - Franklin, MA\nJune 2017 to Present\nDescription:EBSCO Information Services, headquartered in Ipswich, Massachusetts, is a division of EBSCO Industries Inc., the third largest private company in Birmingham, Alabama, with annual sales of nearly $2 billion according to the BBJ's 2013 Book of Lists.\n\nResponsibilities:\n\u2022 Performed Data Profiling to learn about behavior with various features such as traffic pattern, location, time, Date and Time etc.\n\u2022 Application of various machine learning algorithms and statistical modeling like decision trees, regressionmodels, neural networks, SVM, clustering to identify Volume using scikit-learn package in python, Matlab.\n\u2022 Utilized Spark, Scala, Hadoop, HBase, Cassandra, MongoDB, Kafka, Spark Streaming, MLLib, Python, a broad variety of machine learning methods including classifications, regressions, dimensionally reduction etc. and Utilized the engine to increase user lifetime by 45% and triple user conversations for target categories.\n\u2022 Developed Spark/Scala, Python for regular expression (regex) project in the Hadoop/Hive environment with Linux/Windows for big data resources. Used clustering technique K-Means to identify outliers and to classify unlabeled data.\n\u2022 Evaluated models using Cross Validation, Log loss function, ROC curves and used AUC for feature selection.\n\u2022 Analyze traffic patterns by calculating autocorrelation with different time lags.\n\u2022 Ensured that the model has low False Positive Rate.\n\u2022 Addressed overfitting by implementing of the algorithm regularization methods like L2 and L1.\n\u2022 Used Principal Component Analysis in feature engineering to analyze high dimensional data.\n\u2022 Created and designed reports that will use gathered metrics to infer and draw logical conclusions of past and future behavior.\n\u2022 Performed Multinomial Logistic Regression, Randomforest, Decision Tree, SVM to classify package is going to deliver on time for the new route.\n\u2022 Performed data analysis by using Hive to retrieve the data from Hadoopcluster, Sql to retrieve data from Oracle database.\n\u2022 Used MLlib, Spark's Machine learning library to build and evaluate different models.\n\u2022 Implemented rule based expertise system from the results of exploratory analysis and information gathered from the people from different departments.\n\u2022 Performed Data Cleaning, features scaling, features engineering using pandas and numpy packages in python.\n\u2022 Developed MapReduce pipeline for feature extraction using Hive.\n\u2022 Created Data QualityScripts using SQL and Hive to validate successful data load and quality of the data. Created various types of data visualizations using Python and Tableau.\n\u2022 Communicated the results with operations team for taking best decisions.\n\u2022 Collected data needs and requirements by Interacting with the other departments.\n\nEnvironment: Python 2.x, CDH5, HDFS, Hadoop2.3, Hive, Impala, Linux, Spark, Tableau Desktop, SQL Server 2012, Microsoft Excel, Mat lab, Spark SQL, Pyspark."", u""Data Scientist\n1 Microsoft way - Redmond, WA\nMarch 2016 to May 2017\nDescription:The Microsoft campus is the informal name of Microsoft's corporate headquarters, located at One Microsoft Way in Redmond, Washington.\n\nResponsibilities:\n\u2022 Provided Configuration Management and Build support for more than 5 different applications, built and deployed to the production and lower environments.\n\u2022 Implemented public segmentation using unsupervised machine learning algorithms by implementing k-means algorithm using Pyspark.\n\u2022 Explored and Extracted data from source XML in HDFS, preparing data for exploratory analysis using data munging.\n\u2022 Responsible for different Data mapping activities from Source systems to Teradata\n\u2022 Handled importing data from various data sources, performed transformations using Hive, Map Reduce, and loaded data into HDFS\n\u2022 Used R and python for Exploratory Data Analysis, A/B testing, Anova test and Hypothesis test to compare and identify the effectiveness of Creative Campaigns.\n\u2022 Created clusters to classify Control and test groups and conducted group campaigns.\n\u2022 Analyzed and calculated the lifetime cost of everyone in the welfare system using 20 years of historical data.\n\u2022 Developed triggers, stored procedures, functions and packages using cursors and ref cursor concepts associated with the project using Pl/SQL\n\u2022 Created various types of data visualizations using R, python and Tableau.\n\u2022 Used Python, R, SQL to create Statistical algorithms involving Multivariate Regression, LinearRegression, LogisticRegression, PCA, Random forest models, Decision trees, Support Vector Machine for estimating the risks of welfare dependency.\n\u2022 Identified and targeted welfare high-risk groups with Machinelearningalgorithms.\n\u2022 Conducted campaigns and run real-time trials to determine what works fast and track the impact of different initiatives.\n\u2022 Developed Tableau visualizations and dashboards using Tableau Desktop.\n\u2022 Used Graphical Entity-Relationship Diagramming to create new database design via easy to use, graphical interface.\n\u2022 Created multiple custom SQLqueries in MSSQL to prepare the right data sets for Tableau dashboards\n\u2022 Perform analyses such as regression analysis, logistic regression, discriminant analysis, cluster analysis using Python programming.\n\u2022 Used Meta data tool for importing metadata from repository, new job categories and creating new data elements.\n\u2022 Scheduled the task for weekly updates and running the model in workflow. Automated the entire process flow in generating the analysis and reports.\n\nEnvironment:R3.x, HDFS, Hadoop2.3, Pig, Hive, Linux, R-Studio, Tableau 10, SQL Server, Ms Excel, Pypark."", u""Data Scientist\nTripAdvisor - New York, NY\nNovember 2014 to February 2016\nDescription: TripAdvisor, Inc. is an American travel website company providing reviews of travel-related content. It also includes interactive travel forums.\n\nResponsibilities:\n\u2022 Involved in Design, Development and Support phases of SoftwareDevelopmentLifeCycle (SDLC)\n\u2022 Performed data ETL by collecting, exporting, merging and massaging data from multiple sources and platforms including SSIS (SQLServerIntegrationServices) in SQL Server.\n\u2022 Worked with cross-functional teams (including data engineer team) to extract data and rapidly execute from MongoDB through MongDB connector for Hadoop.\n\u2022 Performed data cleaning and feature selection using MLlib package in PySpark.\n\u2022 Performed partitional clustering into 100 by k-means clustering using Scikit-learn package in Python where similar hotels for a search are grouped together.\n\u2022 Used Python to perform ANOVA test to analyze the differences among hotel clusters.\n\u2022 Implemented application of various machine learning algorithms and statistical modeling like Decision Tree, NaiveBayes, LogisticRegression and LinearRegression using Python to determine the accuracy rate of each model.\n\u2022 Determined the most accurately prediction model based on the accuracy rate.\n\u2022 Used text-mining process of reviews to determine customers' concentrations.\n\u2022 Delivered analysis support to hotel recommendation and providing an online A/B test.\n\u2022 Designed Tableau bar graphs, scattered plots, and geographical maps to create detailed level summary reports and dashboards.\n\u2022 Developed hybrid model to improve the accuracy rate.\n\u2022 Delivered the results to operation team for better decisions and feedbacks.\n\nEnvironment:Python, PySpark, Tableau, MongoDB, Hadoop, SQL Server, SDLC, ETL, SSIS, recommendation systems, Machine Learning Algorithms, text-mining process, A/B test."", u'Hadoop/ BigData Developer\nGordmans Stores Inc - Omaha, NE\nMarch 2013 to October 2014\nDescription:Gordmans Stores, Inc. operates department stores under the Gordmans name in the United States. Its merchandise selection includes a range of apparel, footwear, home fashions products, and accessories, including fragrances.\n\nResponsibilities:\n\u2022 Developed Pig Latin scripts using operators such as LOAD, STORE, DUMP, FILTER, DISTINCT, FOREACH, GENERATE, GROUP, COGROUP, ORDER, LIMIT, UNION, SPLIT to extract data from data files to load into HDFS.\n\u2022 Installed and configured HadoopMapReduce, HDFS, Developed multiple MapReduce jobs in Java for data cleaning and preprocessing.\n\u2022 Worked on Installing and configuring the HDPHortonWork2.X and Cloudera (CDH 5.5.1) Clusters in Dev and Production Environments.\n\u2022 Volumetric Analysis for 43 feeds (CurrentApproximate Size of Data (70TB), Based on which size of ProductionCluster is to be decided.\n\u2022 Multiple Spark Jobs were written to perform Data Quality checks on data before files were moved to Data ProcessingLayer.\n\u2022 Worked on Capacity planning for the ProductionCluster.\n\u2022 Installed HUE Browser.\n\u2022 Involved in loading data from UNIX file system to HDFS.\n\u2022 Involved in creating Hivetables, loading with data and writing hive queries which will run internally in MapReduce way.\n\u2022 Worked on Installation of HORTONWORKS 2.1 in AZURELinuxServers.\n\u2022 Worked on cluster up gradation in Hadoop from HDP2.1 to HDP 2.3.\n\u2022 Responsible for implementation and ongoing administration of Hadoop infrastructure\n\u2022 Managed and reviewed Hadoop log files.\n\u2022 Importing and exporting data from different databases like MySQL, RDBMS into HDFS and HBASE using Sqoop.\n\u2022 Worked on indexing the HBase tables using Solr and indexing the Json data and Nested data.\n\u2022 Responsible for Clustermaintenance, Monitoring, commissioning and decommissioningDatanodes, Troubleshooting, Manage and review data backups, Manage&review log files.\n\u2022 Day to day responsibilities includes solving developer issues, deployments moving code from one environment to another environment, providing access to new users and providing instant solutions to reduce the impact and documenting the same and preventing future issues.\n\u2022 Adding/installation of new components and removal of them through Ambari.\n\u2022 Collaborating with application teams to install the operating system and Hadoopupdates, patches, version upgrades.\n\u2022 Monitored workload, job performance, and capacity planning\n\u2022 Involved in Analyzingsystemfailures, identifyingrootcauses, and recommended a course of actions.\nEnvironment: Hadoop, MapReduce, HDFS, HBase, HDP Horton, Sqoop, Data Processing Layer, HUE, AZURE, UNIX, MySQL, RDBMS, Ambari, Solr Cloud, Cloudera, Lily HBase, Cron.', u'Hadoop/ BigData Developer\nPersistent Systems - Nagpur, Maharashtra\nOctober 2011 to February 2013\nDescription: Persistent Systems is a technology services company which was incorporated on 16 May 1990 as Persistent Systems Private Limited.\n\nResponsibilities:\n\u2022 Driving Digital Products in the bank for IOT for campaigning system, Blockchain for payment and Trading etc.\n\u2022 Defining Architecture Standards, Bigdata Principles, and PADS across Program and usage of VP for Modelling.\n\u2022 Developed pigscripts to transformdata and loaded intoHBase tables.\n\u2022 Developed Hive scripts for implementing dynamic partitions\n\u2022 Created Hive snapshot tables and HiveORC tables from Hive tables.\n\u2022 In the Data ProcessingLayer data is finally stored in Hive Tables in ORC file format using SparkSQL. In this layer logic for maintaining SCD type2 is implemented for non-transactional incremental feeds.\n\u2022 Development of a Ruleengine which would further add columns to existing data based on certain BusinessRules specified by Reference Data provided by Business.\n\u2022 Optimized hive joins for large tables and developed map reduce code for the full outer join of two large tables.\n\u2022 Used spark to parse XML files and extract values from tags and load it into multiple hive tables using map classes.\n\u2022 Experience in using HDFS and My SQL and deployed HBase integration to perform OLAP operations on HBase data.\n\u2022 Involved in creating Hivetables, loading with data and writing hive queries which will run internally in MapReduce way.\n\u2022 Used TalendBigDataOpenStudio5.6.2 to create framework for executing extract framework\n\u2022 Monitored workload, job performance, and capacity planning.\n\u2022 Implemented partitioning and bucketing techniques in Hive\n\u2022 Used different big data components in Talend like throw, thiveInput, tHDFSCopy, tHDFSput, tHDFSGet, tMap, tdenormalize, tFlowtoIterate etc.,\n\u2022 Scheduled different talend jobs using TAC (TalendAdminConsole).\n\u2022 Worked on evaluation and analysis of Hadoop cluster and different big data analytic tools like HBase. Developed MapReduce programs to perform data filtering for unstructured data.\n\u2022 Loaded data from UNIX file system to HDFS and written Hive User Defined Functions\n\u2022 Developed code to pre-process large sets of various types of file formats such as Text, Avro, Sequencefiles, XML, JSON, and Parquet.\n\u2022 Created multi-stage Map-Reduce jobs in Java for ad-hoc purposes\n\u2022 Involved in Analyzing system failures, identifying root causes, and recommended course of actions\n\u2022 Adding/installation of new components and removal of them through Ambari.\n\u2022 Collaborating with application teams to install the operating system and Hadoopupdates, patches, versionupgrades.\n\nEnvironment: Hadoop, MapReduce,TAC, HDFS, HBase, HDP Horton, Sqoop, SparkSQL, Hive ORC, Data Processing Layer, HUE, AZURE, UNIX, MySQL, RDBMS, Ambari, Solr Cloud, Lily HBase, Cron, JSON, XML, Parquet.', u""R Programmer\nCenvien Technologies - Hyderabad, Telangana\nMarch 2009 to September 2011\nDescription: Cenvien technologies gather the requirements by listening and understanding to the client's business requirement to deliver quality products.\n\nResponsibilities:\n\u2022 Analyzed high volume, high dimensional client and survey data from different sources using R\n\u2022 Manipulated large financial datasets, primarily in SQL and R\n\u2022 Used R for large matrix computation\n\u2022 Developed Algorithms (DataMiningQuery's) to extract data from data warehouse & databases to build Rules for the Analyst&Models Team.\n\u2022 Used R to import high volume of data\n\u2022 High level programming efficiency in the use of statistical modeling tools such as SAS, SPSS and R.\n\u2022 Developed predictive models using R to predict customers churn and classification of customers\n\u2022 Worked on Shiny and R application displaying machine learning for improving the forecast of business.\n\u2022 Developed, reviewed, tested & documented R programs/macros.\n\u2022 Created Templates by using R macro for existing reports to reduce the manual intervention.\n\u2022 Created Self-service tools for Onshore/Offshore team for data retrieval.\n\u2022 Worked on daily reports and used them for further analysis.\n\u2022 Developed/Designed templates for new data extraction requests.\n\u2022 Executed weekly reports for CommercialDataAnalyticsTeam.\n\u2022 Communicated progress to key Business partners and Analysts through status reports and tracked issues until resolution.\n\u2022 Created predictive and other analytically derived models for assessing sales.\n\u2022 Provided support in the design and implementation of ad hoc requests for Sales-RelatedPortfolioData.\n\u2022 Responsible for preparing test case documents and Technical specification documents.\n\nEnvironment: Python, R, SAS, SPSS, SQL, Tableau, Spark, Machine Learning Software Package.""]",[u'Bachelor of Computer Science in TOOLS AND TECHNOLOGIES'],[u'Informatica Power Centre'],degree_1 : Bachelor of Compter Science in TOOLS AND TECHNOLOGIES
0,https://resumes.indeed.com/resume/fa2dc08b114e328b,"[u""Data Scientist\nKellogg's - Elmhurst, IL\nAugust 2017 to Present\nDescription: The Kellogg Company (also Kellogg's, Kellogg, and Kellogg's of Battle Creek) is an American multinational food manufacturing company headquartered in Battle Creek, Michigan, United States. Kellogg's produces cereal and convenience foods, including cookies, crackers, toaster pastries, cereal bars, fruit-flavored snacks, frozen waffles, and vegetarian foods.\n\nResponsibilities:\n\u2022 Built models using Statisticaltechniques like Bayesian HMM and MachineLearning classification models like XGBoost, SVM, and RandomForest.\n\u2022 A highly immersive DataScience program involving DataManipulation&Visualization, Web Scraping, MachineLearning, Python programming, SQL, GIT, Unix Commands, NoSQL, MongoDB, Hadoop.\n\u2022 Setup storage and dataanalysis tools in AmazonWebServices cloud computing infrastructure.\n\u2022 Used pandas, numpy, seaborn, scipy, matplotlib, scikit-learn, NLTK in Python for developing various machinelearningalgorithms.\n\u2022 Installed and used CaffeDeepLearningFramework\n\u2022 Worked on different data formats such as JSON, XML and performed machinelearningalgorithms in Python.\n\u2022 Worked as Data Architects and IT Architects to understand the movement of data and its storage and ERStudio9.7\n\u2022 Participated in all phases of datamining; datacollection, datacleaning, developingmodels, validation, visualization and performed Gapanalysis.\n\u2022 DataManipulation and Aggregation from different source using Nexus, Toad, BusinessObjects, PowerBI and SmartView.\n\u2022 Implemented Agile Methodology for building an internal application.\n\u2022 Focus on integration overlap and Informatica newer commitment to MDM with the acquisition of Identity Systems.\n\u2022 Good knowledge of HadoopArchitecture and various components such as HDFS, JobTracker, TaskTracker, NameNode, DataNode, SecondaryNameNode, and MapReduce concepts.\n\u2022 As Architect delivered various complex OLAPdatabases/cubes, scorecards, dashboards and reports.\n\u2022 Programmed a utility in Python that used multiple packages (scipy, numpy, pandas)\n\u2022 Implemented Classification using supervised algorithms like LogisticRegression, Decisiontrees, KNN, NaiveBayes.\n\u2022 Used Teradata15 utilities such as FastExport, MLOAD for handling various tasks data migration/ETL from OLTP Source Systems to OLAP Target Systems\n\u2022 Experience in Hadoop ecosystem components like HadoopMapReduce, HDFS, HBase, Oozie, Hive, Sqoop, Pig, Flume including their installation and configuration.\n\u2022 Updated Pythonscripts to match training data with our database stored in AWSCloud Search, so that we would be able to assign each document a response label for further classification.\n\u2022 Data transformation from various resources, data organization, features extraction from raw and stored.\n\u2022 Validated the machine learning classifiers using ROC Curves and Lift Charts.\n\u2022 Extracted data from HDFS and prepared data for exploratory analysis using datamunging.\nEnvironment: ER Studio 9.7, Tableau 9.03, AWS, Teradata 15, MDM, GIT, Unix, Python 3.5.2, , MLLib, SAS, regression, logistic regression, Hadoop, NoSQL, Teradata, OLTP, random forest, OLAP, HDFS, ODS, NLTK, SVM, JSON, XML, MapReduce."", u""Data Scientist\nNC department of health and human services - Durham, NC\nMay 2016 to July 2017\nDescription: The North Carolina Department of Health and Human Services (DHHS) is a large government agency in the U.S. state of North Carolina, somewhat analogous to the United States Department of Health and Human Services. DHHS has more than 19,000 employees. It is headed by a Secretary (Lanier Cansler as of 2009), who is appointed by the Governor.\n\nResponsibilities:\n\u2022 Coded R functions to interface with CaffeDeepLearning Framework\n\u2022 Working in AmazonWebServices cloud computing environment\n\u2022 Used Tableau to automatically generate reports, Worked with partially adjudicated insurance flat files, internal records, 3rd party data sources, JSON, XML and more.\n\u2022 Worked with several R packages including knitr, dplyr, SparkR, CausalInfer, spacetime.\n\u2022 Implemented end-to-end systems for DataAnalytics, DataAutomation and integrated with custom visualization tools using R,Mahout, Hadoop and MongoDB.\n\u2022 Gathering all the data that is required from multiple data sources and creating datasets that will be used in analysis.\n\u2022 Performed Exploratory DataAnalysis and DataVisualizations using R, and Tableau.\n\u2022 Perform a proper EDA, Univariate and bi-variate analysis to understand the intrinsic effect/combined effects.\n\u2022 Worked with Data governance, Data quality, data lineage, Data architect to design various models and processes.\n\u2022 Independently coded new programs and designed Tables to load and test the program effectively for the given POC's using with Big Data/Hadoop.\n\u2022 Designed data models and data flow diagrams using Erwin and MSVisio.\n\u2022 As an Architect implemented MDM hub to provide clean, consistent data for a SOA implementation.\n\u2022 Developed, Implemented & Maintained the Conceptual, Logical&Physical Data Models using Erwin for Forward/ReverseEngineered Databases.\n\u2022 Established Data architecture strategy, best practices, standards, and roadmaps.\n\u2022 Lead the development and presentation of a dataanalytics data-hub prototype with the help of the other members of the emerging solutions team\n\u2022 Performed datacleaning and imputation of missing values using R.\n\u2022 Worked with Hadoop eco system covering HDFS, HBase, YARN and MapReduce\n\u2022 Take up ad-hoc requests based on different departments and locations\n\u2022 Used Hive to store the data and perform datacleaning steps for huge datasets.\n\u2022 Created dash boards and visualization on regular basis using ggplot2 and Tableau\n\u2022 Creating customized business reports and sharing insights to the management\n\u2022 Worked with BTEQ to submit SQL statements, import and export data, and generate reports in Teradata.\n\u2022 Interacted with the other departments to understand and identify dataneeds and requirements and work with other members of the ITorganization to deliver data visualization and reportingsolutions to address those needs.\nEnvironment: R 3.0, Erwin 9.5, Tableau 8.0, MDM, QlikView, MLLib, PL/SQL, HDFS, Teradata 14.1, JSON, HADOOP (HDFS), MapReduce, PIG, Spark, R Studio, MAHOUT, JAVA, HIVE, AWS."", u'Data Scientist\nEbay Inc - San Jose, CA\nJanuary 2015 to April 2016\nDescription: Ebay is a multinational e-commerce corporation, facilitating online consumer-to consumer and business-to-consumer sales. It is headquartered in San Jose, California. eBay was founded by Pierre Omidyar in 1995, and became a notable success story of the dot-com bubble.\nResponsibilities:\n\u2022 As an Architect design conceptual, logical and physical models using Erwin and build datamarts using hybrid Inmon and Kimball DW methodologies\n\u2022 Interaction with Business Analyst, SMEs and other Data Architects to understand Business needs and functionality for various project solutions\n\u2022 Researched, evaluated, architected, and deployed new tools, frameworks, and patterns to built sustainable Big Data platforms for the clients\n\u2022 Identifying and executing process improvements, hands-on in various technologies such as Oracle, Informatica, BusinessObjects.\n\u2022 Worked closely with business, datagovernance, SMEs and vendors to define data requirements.\n\u2022 Worked with data investigation, discovery and mapping tools to scan every single data record from many sources.\n\u2022 Designed the prototype of the Data mart and documented possible outcome from it for end-user.\n\u2022 Involved in business process modeling using UML\n\u2022 Developed and maintained data dictionary to create metadata reports for technical and business purpose.\n\u2022 Created SQLtables with referential integrity and developed queries using SQL, SQL*PLUS and PL/SQL\n\u2022 Experience in maintaining database architecture and metadata that support the Enterprise Datawarehouse.\n\u2022 Developed various QlikView Data Models by extracting and using the data from various sources files, DB2, Excel, Flat Files and Bigdata.\n\u2022 Handled importing data from various data sources, performed transformations using Hive, Map Reduce, and loaded data into HDFS.\n\u2022 Designed both 3NF data models for ODS, OLTP systems and dimensionaldatamodels using Star and SnowflakeSchemas.\n\u2022 Design, coding, unit testing of ETL package source marts and subject marts using Informatica ETL processes for Oracledatabase.\nEnvironment: Erwin r9.0, Informatica 9.0, ODS, OLTP, Oracle 10g, Hive, OLAP, DB2, Metadata, MS Excel, Mainframes MS Visio, Rational Rose, Requisite Pro, Hadoop, PL/SQL, etc.', u'Java Developer\nEagle Trading Systems - Princeton, NJ\nMay 2013 to December 2014\nDescription: Eagle Trading Systems Inc. is a financial investment advisory firm headquartered in Princeton, New Jersey. The firm manages 5 accounts totaling an estimated $481 Million of assets under management.\n\nResponsibilities:\n\u2022 Designed & developed the application using Spring Framework\n\u2022 Developed class diagrams, sequence and use case diagrams using UML Rational Rose.\n\u2022 Designed the application with reusable J2EE design patterns\n\u2022 Developed test cases for Unit testing using JUnit and performed integration and system testing\n\u2022 Involved in coding for the presentation layer using Struts Framework, JSP, AJAX, XML, XSLT and JavaScript\n\u2022 Closely worked and supported the creation of database schema objects (tables, stored procedures, and triggers) using OracleSQL.\n\u2022 Designed DAO objects for accessing RDBMS\n\u2022 Designed & developed Data Transfer Objects to carry the data between different layers\n\u2022 Developed web pages using JSP, HTML, DHTML and JSTL\n\u2022 Designed and developed a web-based client using Servlets, JSP, TagLibraries, JavaScript, HTML and XML using Struts Framework.\n\u2022 Developed views and controllers for client and manager modules using Spring MVC and Spring Core.\n\u2022 Used Spring Security for securing the web tier Access.\n\u2022 Business logic is implemented using Hibernate.\n\u2022 Developed and modified database objects as per the requirements.\n\u2022 Involved in Unit integration, bug fixing, acceptance testing with test cases, Code reviews.\n\u2022 Interaction with customers and identified System Requirements and developed Software Requirement Specifications.\n\u2022 Implemented Java design patterns wherever required.\n\u2022 Involved in development, maintenance, implementation and support of the System.\n\u2022 Involved in initial project setup and guidelines.\n\u2022 Implemented Multi-threading concepts.\n\nEnvironment: Java, PL/SQL, SQL, HTML, CSS 3.0,Java Script, hibernate, Middleware Technologies, Ajax, Servlets, JSP 2.0, Web logic 10.4, JBoss, WebSphere, XML, XHTML, Eclipse, JMS, Oracle11g, EJB 3.0.', u'Java Developer\nFirst Indian Corporation Private Limited - Hyderabad, Telangana\nDecember 2011 to April 2013\nDescription:First Indian Corporation Private Limited, a member of The First American family of companies, provides specialized offshore transaction services, technology services and analytics to the mortgage industry.\n\nResponsibilities:\n\u2022 Participating in system design, planning, estimation, and implementation.\n\u2022 Involved in developing Use case diagrams, Class diagrams, Sequence diagrams and process flow diagrams for the modules using UML and Rational Rose.\n\u2022 Developed the presentation layer using JSP,AJAX, HTML, XHTML, CSS and client validations using JavaScript.\n\u2022 Developed and implemented the MVC Architectural Pattern using Spring Framework.\n\u2022 Effective usage of J2EE Design Patterns Namely Session Facade, Factory Method, Command, and Singleton to develop various base framework components in the application.\n\u2022 Modified Account View functionality to enable display of blocked accounts details that have tags. This involved modifying beans, JSP changes, and middle tier enhancements.\n\u2022 Developed various EJBs (session and entity beans) for handling business logic.\n\u2022 Developed Session Beans and DAO classes for Accounts and other Modules.\n\u2022 Worked on generating the web services classes by using WSDL, UDDI, and SOAP.\n\u2022 Consumed Web Services using WSDL, SOAP, and UDDI from the third party for authorizing payments to/from customers.\n\u2022 Involved in Units integration using JUnit, bug fixing, and User acceptance testing with test cases.\n\u2022 Used CVS for version control and Maven as a build tool.\n\u2022 Designed and developed systems based on JEE specifications and used Spring Framework with MVC architecture.\n\u2022 Used Spring Roo Framework Design/Enterprise Integration patterns and REST architecture compliance for design and development of applications.\n\u2022 Involved in the application development using Spring Core, Spring Roo, Spring JEE, Spring Aspects modules and Java web-based technologies such as Web Service (REST /SOA /micro services) including micro services implementations and Hibernate ORM.\n\u2022 Used LDAP and Microsoft active directory series for authorization and authentication services.\n\u2022 Implemented different design patterns such as singleton, Session Fa\xe7ade, Factory, and MVC design patterns such as Business delegate, session fa\xe7ade and DAO design patterns.\n\u2022 Used JPA - Object Mapping for the backend data persistence.\n\nEnvironment: R 3.0, Erwin 9.5, Tableau 8.0, MDM, QlikView, MLLib, PL/SQL, HDFS, Teradata 14.1, JSON, HADOOP (HDFS), MapReduce, PIG, Spark, R Studio, MAHOUT, JAVA, HIVE, AWS.', u'Software Engineer\nAccenture - Bengaluru, Karnataka\nMay 2009 to November 2011\nDescription: Accenture PLC is a global management consulting and professional services company that provides strategy, consulting, digital, technology and operations.\n\nResponsibilities:\n\u2022 Involved in design and development of workflow and support.\n\u2022 Used DesignPatterns like Singleton, Factory, SessionFacade and DAO.\n\u2022 Developed using new features of Java 1.7 annotations, generics, enhanced for loop and enums. Used Spring IOC, AOP and HibernateORM for back end tiers.\n\u2022 Involved in writing Thread Safe blocks for multi thread access to make valid transactions.\n\u2022 Created and injected Spring services, Spring controllers and DAOs to achieve dependency injection and to wire objects of business classes.\n\u2022 Configured and build Asynchronous communication with JMS services with MQ Series.\n\u2022 Used Spring Inheritance to develop beans from already developed parent beans.\n\u2022 Worked on Spring Quartz functionality for scheduling tasks such as generating monthly reports for customers and sending them emails about different policies.\n\u2022 Used JMS for the asynchronous exchange of critical data and events among J2EE components. The publisher-subscriber method was used for data loading and Point-To-Point method of JMS was used for event processing.\n\u2022 Used DAO pattern to fetch data from database using Hibernate to carry out the various database.\n\u2022 Used HibernateTransactionManagement, Hibernate Batch Transactions, cache concepts.\n\u2022 Modified the Spring Controllers and Services classes to support the introduction of Spring framework.\n\u2022 Developed various generic JavaScript functions used for validations.\n\u2022 Developed screens using jQuery, JSP, JavaScript, Ajax and ExtJS.\n\u2022 Developed various generic JavaScript functions used for validations.\n\u2022 Developed screens using HTML5, CSS, JavaScript, jQuery and AJAX.\n\u2022 Developed various EJB components to full fill the business functionality.\n\u2022 Used XStreamAPI to transfer data back and forth between Spring MVC and ExtJS, NodeJS.\n\u2022 Used AJAX extensively to implement front end /user interface features in the application.\n\u2022 Developed the presentation layer and GUI framework in JSP and Client-Side validations were done.\n\u2022 Implemented SOA to develop REST-Based Web services using ApacheAxis.\n\u2022 Developed REST Web Services clients to consume those Web Services as well other enterprise-wide WebServices.\n\u2022 Designed and developed custom message adapter components that allowed the message to travel through IBMMQ Series XMLBeans and JMS.\n\u2022 Exposed the Web Services to the client applications by sharing the WSDLs.\n\nEnvironment: Java, J2EE, JSP, Servlets, MVC, Hibernate, Springr3.0, Web Services, Maven 3.2.x, Eclipse, SOAP, WSDL, Eclipse, jQuery, JavaScript, Swings, Oracle, REST API, PL/SQL, Oracle 11g, UNIX.']","[u""Bachelor's""]",[u''],"degree_1 : ""Bachelors"""
0,https://resumes.indeed.com/resume/016e89dc09618fb2,"[u'Data Scientist\nNew York Community Bank - New York, NY\nFebruary 2014 to July 2015\nDescription: New York Community Bank operates as a New York state-chartered savings bank that provides personal and business banking products and services to customers in Metro New York, New Jersey, Florida, Ohio, and Arizona. The project is to build a complex system that can analyze, calculate, and evaluate margin risks. The system should also report, monitors and alerts on margins, gain/loss, market value at client, and trade levels.\n\nResponsibilities:\n\u2022 Utilized Apache Spark with Python to develop and execute Big Data Analytics and Machine learning applications, executed machine Learning use cases under Spark ML and Mllib.\n\u2022 Identified areas of improvement in existing business by unearthing insights by analyzing vast amount of data using machine learning techniques.\n\u2022 Interpret problems and provides solutions to business problems using data analysis, data mining, optimization tools, and machine learning techniques and statistics.\n\u2022 Designed and developed NLP models for sentiment analysis.\n\u2022 Led discussions with users to gather business processes requirements and data requirements to develop a variety of Conceptual, Logical and Physical Data Models. Expert in Business Intelligence and Data Visualization tools: Tableau, Microstrategy.\n\u2022 Worked on machine learning on large size data using Spark and MapReduce.\n\u2022 Let the implementation of new statistical algorithms and operators on Hadoop and SQL platforms and utilized optimizations techniques, linear regressions, K-means clustering, Native Bayes and other approaches.\n\u2022 Developed Spark/Scala, Python for regular expression (regex) project in the Hadoop/Hive environment with Linux/Windows for big data resources.\n\u2022 Data sources are extracted, transformed and loaded to generate CSV data files with Python programming and SQL queries.\n\u2022 Stored and retrieved data from data-warehouses using Amazon Redshift.\n\u2022 Worked on TeradataSQL queries, Teradata Indexes, Utilities such as Mload, Tpump, Fast load and FastExport.\n\u2022 Used Data Warehousing Concepts like Ralph Kimball Methodology, Bill Inmon Methodology, OLAP, OLTP, Star Schema, Snow Flake Schema, Fact Table and Dimension Table.\n\u2022 Refined time-series data and validated mathematical models using analytical tools like R and SPSS to reduce forecasting errors.\n\u2022 Worked on data pre-processing and cleaning the data to perform feature engineering and performed data imputation techniques for the missing values in the dataset using Python.\n\u2022 Created Data Quality Scripts using SQL and Hive to validate successful dasta load and quality of the data. Created various types of data visualizations using Python and Tableau.\n\nEnvironment: Hadoop, Map Reduce, Spark, Spark MLLib, Tableau, SQL, Excel, VBA, SAS, Matlab, AWS, SPSS, Cassandra, Oracle, MongoDB, SQL Server 2012, DB2, T-SQL, PL/SQL, XML, Tableau.', u'Data Scientist\nFirst Atlantic Bank - Jacksonville, FL\nAugust 2012 to January 2014\nDescription: First Atlantic Bank gives subsidizes or advances to individuals with little business prerequisites. Candidates get their advances authorized taking into account their record of loan repayment. The candidate data is kept up in a database alongside the points of interest of the advance for reimbursement. This information is filtered into diverse classifications in light of parameters like kind of record, advance sum, due date. The filtered information is utilized for insights for producing report.\n\nResponsibilities:\n\u2022 Collaborates with cross-functional team in support of business case development and identifying modeling method (s) to provide business solutions. Determines the appropriate statistical and analytical methodologies to solve business problems within specific areas of expertise.\n\u2022 Generating Data Models using Erwin9.6 and developed relational database system and involved in Logical modeling using the Dimensional Modeling techniques such as Star Schema and Snow Flake Schema.\n\u2022 Guide the full lifecycle of a Hadoop solution, including requirements analysis, platform selection, technical architecture design, application design and development, testing, and deployment\n\u2022 Consult on broad areas including data science, spatial econometrics, machine learning, information technology and systems and economic policy with R\n\u2022 Performed Datamapping between source systems to Target systems, logicaldata modeling, created classdiagrams and ERdiagrams and used SQLqueries to filter data\n\u2022 Enabled speedy reviews and first mover advantages by using Oozie to automate data loading into the Hadoop Distributed File System and PIG to pre-process the data.\n\u2022 Used various techniques using R data structures to get the data in right format to be analyzed which is later used by other internal applications to calculate the thresholds.\n\u2022 Maintaining conceptual, logical and physical data models along with corresponding metadata.\n\u2022 Done data migration from an RDBMS to a NoSQL database, and gives the whole picture for data deployed in various data systems.\n\u2022 Developed triggers, stored procedures, functions and packages using cursors and ref cursor concepts associated with the project using PLSQL\n\u2022 Used Meta data tool for importing metadata from repository, new job categories and creating new data elements.\n\nEnvironment: R, Oracle 12c, MS-SQL Server, Hive, NoSQL, PL/SQL, MS- Visio, Informatica, T-SQL, SQL, Crystal Reports 2008, Java, SPSS, SAS, Tableau, Excel, HDFS, PIG, SSRS, SSIS, Metadata.', u'Data Scientist/Data Analyst\nAssurant Specialty Property - Santa Ana, CA\nMay 2008 to December 2010\nDescription: Assurant partners with leaders in mortgage lending, manufactured housing, multifamily housing and other industries to protect client and consumer property.\n\nResponsibilities:\n\u2022 Worked on data cleaning and reshaping, generated segmented subsets using Numpy and Pandas in Python\n\u2022 Wrote and optimized complex SQL queries involving multiple joins and advanced analytical functions to perform data extraction and merging from large volumes of historical data stored in Oracle 11g, validating the ETL processed data in target database\n\u2022 Identified the variables that significantly affect the target\n\u2022 Continuously collected business requirements during the whole project life cycle.\n\u2022 Conducted model optimization and comparison using stepwise function based on AIC value\n\u2022 Applied various machine learning algorithms and statistical modeling like decision tree, logistic regression, Gradient Boosting Machine to build predictive model using scikit-learn package in Python\n\u2022 Developed Python scripts to automate data sampling process. Ensured the data integrity by checking for completeness, duplication, accuracy, and consistency\n\u2022 Generated data analysis reports using Matplotlib, Tableau, successfully delivered and presented the results for C-level decision makers\n\u2022 Generated cost-benefit analysis to quantify the model implementation comparing with the former situation\n\u2022 Worked on model selection based on confusion matrices, minimized the Type II error\n\nEnvironment: Tableau 7, Python 2.6.8, Numpy, Pandas, Matplotlib, Scikit-Learn, MongoDB, Oracle 10g, SQL', u""Data Architect/Data Modeler\nPitney Bowes Inc - Stamford, CT\nMay 2008 to December 2010\nDescription: PITNEY BOWES (PB) is the manufacturer of copiers, faxes and other office automation. Pitney Bowes's operations are aligned under three lines of business, Global Mailing Systems (GMS) - It is the company's core mail automation and shipping business.\n\nResponsibilities:\n\u2022 Develop Integrations jobs to transfer data from source system to Hadoop.\n\u2022 Installation of Talend Studio.\n\u2022 Technical design documents for Transformation processes.\n\u2022 Application of business rules on the data being transferred.\n\u2022 Task allocation for the ETL and Reporting team.\n\u2022 Communicate effectively with client and their internal development team to deliver product functionality requirements.\n\u2022 Architecting and design of data warehouse ETL processes.\n\u2022 Demo of POC built for the prospective customer and provide guidance and gather the feedback to backend ETL testing on SQL Server 2008 using SSIS.\n\u2022 Create the Operational manual Document.\n\u2022 Create Integration Jobs to backup a copy of data in network file system.\n\u2022 Design and implement the ETL Data model and create staging, source and Target tables in SQL\n\u2022 server database.\n\u2022 Gathering and analysis requirements definition meetings with business users and document meeting outcomes.\n\nEnvironment: Hadoop, MS Office, Talend Studio, ETL, ODS, OLAP , SQL Server 2008."", u""Data Analyst/Data Modeler\nNestle - IN\nMay 2008 to December 2010\nDescription: The Nestle is a Swiss transnational food and drink company. Nestle's products include baby food, medical food, bottled water, breakfast cereals, coffee and tea, confectionery, dairy products, ice cream, frozen food, pet foods, and snacks.\n\nResponsibilities:\n\u2022 Created and maintained Logical and Physicalmodels for the data mart. Created partitions and indexes for the tables in the datamart.\n\u2022 Performed data profiling and analysis applied various data cleansing rules designed data standards and architecture/designed the relational models.\n\u2022 Developed SQLscripts for creating tables, Sequences, Triggers, views and materializedviews\n\u2022 Worked on query optimization and performance tuning using SQL Profiler and performance monitoring.\n\u2022 Developed mappings to load Fact and Dimension tables, SCD Type 1 and SCD Type 2 dimensions and Incremental loading and unit tested the mappings.\n\u2022 Utilized Erwin's forward/reverse engineering tools and target database schema conversion process.\n\u2022 Worked on creating enterprise wide Model EDM for products and services in Teradata Environment based on the data from PDM. Conceived, designed, developed and implemented this model from the scratch.\n\u2022 Involved in extensive DATA validation by writing several complex SQL queries and Involved in back-end testing and worked with data quality issues.\n\u2022 Developed and executed load scripts using Teradata client utilities MULTILOAD, FASTLOAD and BTEQ.\n\u2022 Exporting and importing the data between different platforms such as SAS, MS-Excel.\n\u2022 Generated periodic reports based on the statistical analysis of the data using SQL Server Reporting Services (SSRS)\n\u2022 Write SQLscripts to test the mappings and Developed Traceability Matrix of Business Requirements mapped to Test Scripts to ensure any Change Control in requirements leads to test case update.\n\nEnvironment: DB2, Oracle SQL Developer, PL/SQL, Business Objects, Erwin, MS office suite, Windows XP, TOAD, SQL*PLUS, SQL*LOADER."", u'Data Analyst\nAccenture - Bengaluru, Karnataka\nMay 2008 to December 2010\nDescription: Accenture is a global management consulting and professional services company that provides strategy, consulting, digital, technology and operations services.\n\nResponsibilities:\n\u2022 Designed different type of STARschemas for detailed data marts and plan data marts in the OLAP environment.\n\u2022 Developed and executed load scripts using Teradata client utilities MULTILOAD, FASTLOAD and BTEQ.\n\u2022 Responsible for development and testing of conversion programs for importing Data from text files into map Oracle Database utilizing PERL shell scripts & SQL*Loader.\n\u2022 Used Graphical Entity-Relationship Diagramming to create new database design via easy to use, graphical interface.\n\u2022 Responsible for development and testing of conversion programs for importing Data from text files into map Oracle Database utilizing PERL shell scripts &SQL*Loader.\n\u2022 Worked with the ETL team to document the Transformation Rules for Data Migration from OLTP to Warehouse Environment for reporting purposes.\n\u2022 Created SQLscripts to find dataquality issues and to identify keys, data anomalies, and data validation issues.\n\u2022 Formatting the data sets read into SAS by using Format statement in the data step as well as Proc Format.\n\u2022 Applied Business Objects best practices during development with a strong focus on reusability and better performance.\n\u2022 Developed Tableau visualizations and dashboards using Tableau Desktop.\n\u2022 Used Graphical Entity-Relationship Diagramming to create new database design via easy to use, graphical interface.\n\u2022 Co-ordinate with various business users, stakeholders and SME to get Functional expertise, design and business test scenarios review, UAT participation and validation of financial data.\n\u2022 Write SQL scripts to test the mappings and Developed Traceability Matrix of Business Requirements mapped to Test Scripts to ensure any Change Control in requirements leads to test case update.\n\nEnvironment: Oracle SQL Developer, PL/SQL, Business Objects, TOAD, Tableau, Informatica, MS SQL Server, SQL*PLUS, SQL*LOADER, XML.']","[u""Bachelor's""]",[u''],"degree_1 : ""Bachelors"""
0,https://resumes.indeed.com/resume/848f0b3ecf763270,"[u""Data Scientist\nBGS Technologies, Austin\nSeptember 2016 to Present\nRESPONSIBILITIES:\n\u2022 Prepare data (data cleaning) to process for algorithmic trading.\n\u2022 Make a system software to access live market equity data, assess it, and make trading decisions.\n\u2022 Work with machine learning algorithms like linear regression for trading problems.\n\u2022 Estimate machine learning algorithm's performance for time series data (stock price data).\n\u2022 Optimize how machine learning techniques fail and apply an accurate machine learning modal.\n\u2022 Provide guidance to customer for portfolio optimization and hedge fund management to manager\nCAPM Hypothesis testing and q learning\n\nEnvironment:\nLanguages: Python 3.x, 2.7/2.4\nPython libraries: pandas 0.22, numpy 1.14, scipy 0.9, Matplotlib 2.0\nDatabases: MySQL 5.1, MongoDB.\nTools: Microsoft excel\nFrontend Technologies: HTML, seaborn, bokeh\nVersioning Tools: GIT\nIDE: IPython 6.0, PyCharm.\nOperating systems: Linux/Unix, Windows NT/2000/XP/2003/Vista, Mac OSX\nCloud Technologies: AWS"", u'Teaching Assistant\nUniversity of Houston Clear Lake - Houston, TX\nAugust 2015 to August 2016\nDescription: The College of Science and Engineering employs graduate student teaching assistants in fulfilling important duties and responsibilities for providing quality instruction and enhancing the learning environment for CSE students. Course Tele communications and Networking under Dr. Jiang Lu and Dr. George Collins.\n\u2022 Assist faculty members with classroom instruction, exams, record keeping, and other miscellaneous projects.\n\u2022 Tutor or mentor students.\n\u2022 Perform laboratory research.\n\u2022 Clean labs (test electronic waste and dispose) and set up or remove specimens.\n\u2022 Obtain materials needed for classes, including texts and other materials.\n\u2022 Prepare presentations for lectures.\n\u2022 Deliver lectures.\n\u2022 Proctor examinations.\n\u2022 Record grades and inform students of their final grades.\n\u2022 Arrange for teaching observations.\n\u2022 Help professors and teachers develop course plans.\n\u2022 Lead discussion sections.\n\u2022 Teach undergraduate courses.\n\u2022 Create and write materials such as a syllabus, visual aids, answer keys, supplementary notes, and course websites.\n\u2022 Correspond with students on Blackboard or related inter-campus communication system.\n\u2022 Provide librarians with assistance cataloguing or displaying collections.\n\u2022 Record lecture given by professor\n\nCapstone Project:\nDesigned a 16 Bit CPU Duration: 4months\nProject Description: I have designed and validated various components in VHDL such as Load instructions, store instructions, 16 Bit ALU, Instruction memory, data Memory and CPU controller in Xilinx ISE. With all this finally I designed a VHDL project, to design a simple CPU using previously designed components as well as a few new ones. And wrote some test benches for optimum output.']","[u'Masters in Computer Engineering in Computer Engineering', u'Bachelors in Electronics and communication in engineering']","[u'University of Houston\nJanuary 2015 to August 2016', u'JNTU Kakinada']","degree_1 : Masters in Compter Engineering in Compter Engineering, degree_2 :  Bachelors in Electronics and commnication in engineering"
0,https://resumes.indeed.com/resume/3e6b5c23da727162,"[u""Associate Data Scientist\nAmerica Informat Inc\nJune 2016 to January 2018\n\u2022 Designed applications of Machine learning, Statistical Analysis and Data visualizations with challenging large data processing problems resulting in savings more than $1.2M an year.\n\u2022 Responsible for all aspects of management, administration, and support of IBM's internal Linux/UNIX cloud based infrastructure as the premier hosting provider.\n\u2022 Worked working with various databases like Oracle, SQL and performed the computations, log transformations, feature engineering, and Data exploration to identify the insights and conclusions from complex data using R- programming in R-studio\n\u2022 Implemented predictive models using machine learning algorithms linear regression and linear boosting algorithms and performed in- depth analysis on the structure of models, compared the performance of all the models and found tree boosting is the best for the prediction.\n\u2022 Applied concepts of R-squared, R.M.S.E, P-value, in the evaluation stage to extract interesting findings through comparisons.\n\u2022 Performed in-depth statistical analysis and data mining methods using R, including Cluster analysis, Logistic Regression, and boosting models that led to reducing variance by 45%\n\u2022 Proficient in the entire CRISP-DM life cycle and actively involved in all the phases of project life cycle including data acquisition, data cleaning, data engineering,\n\u2022 Extensively used Azure Machine Learning to set up the experiments and creating Web services for the predictive analytics\n\u2022 Performed feature scaling, feature engineering and statistical modeling.\n\u2022 Worked on writing complex SQL queries in performing Data analysis using window functions, joins, improving performance by creating partitioned tables,\n\u2022 Prepared multiple dashboards using Tableau to reflect the data behavior over period of time Analyzed and worked with all aspects of regression models (OLS etc.)\n\u2022 Responsible for working with stakeholders to troubleshoot issues, communicate to team members, leadership and stakeholders on findings to ensure models are well understood and optimized."", u'Senior Data Analyst\nCasa Di Moda Dubai\nMarch 2012 to May 2016\nResponsibilities:\n\u2022 Designed, modeled, validated and tested statistical algorithms against various data sets including behavioral data and deployed predictive models using R-studio\n\u2022 Performed Data Transformation method for Rescaling and Normalizing variables.\n\u2022 Applied different Machine Learning algorithms/methods on data sets to predict credit risk, fraud detection, customer churn, and target marketing.\n\u2022 Worked on data to increase cross-& up-sell revenues, enhance customer value or reduce non-credit losses.\n\u2022 Contributed implementing models to identify, extract, summarize, and reduce or categorize the relevant qualitative financial input information like sentiment/feedback/news according to specific structures (templates) from a source text (digital news) to support decision making.\n\u2022 Analyzed, transformed, and contextualized a variety of ingested data - social data, GIS data, POI& AOI data, and some consumer behavior data for building direct marketing predictive models.\n\u2022 Analyzed customer consuming behavior and discover value of customers.\n\u2022 Applied customer segmentation with Clustering algorithms and develop geo-demographic customer segmentation models.\n\u2022 Delivered Interactive visualizations/dashboards using ggplot and Tableau to present analysis outcomes in terms of patterns, anomalies and predictions.', u'Business Data Analyst\nVarious\nJuly 2000 to February 2012\nPrepared comprehensive documented observations, analyses and interpretations of results including technical reports, summaries, protocols and quantitative analyses.\n\u2022 Worked closely with marketing team to deliver actionable insights from huge volume of data, coming from different marketing campaigns and customer interaction matrices such as web portal usage, email campaign responses, public site interaction, and other customer specific parameters.\n\u2022 Gathered analyzed & translated business requirements into relevant analytic approaches & shared for peer review.\n\u2022 Contributed to Finance and Risk management, Operations management, and Marketing to maximize ROI using Data Analytics\n\u2022 Design, model, validate and test statistical algorithms against various real-world data sets including behavioral data and deploy models in the backend\n\u2022 Performed Data Transformation method for Normalizing variables.\n\u2022 Applied Business Objects best practices during development with a strong focus on reusability and better performance.\n\u2022 Co-ordinated with various business users, stakeholders and SME to get Functional expertise, design and business test scenarios review, UAT participation and validation of financial data.']","[u""Bachelor's""]",[u''],"degree_1 : ""Bachelors"""
0,https://resumes.indeed.com/resume/e6f10e085412069c,"[u""Data Scientist\nVerizon, TX\nFebruary 2017 to Present\nDescription: Verizon Wireless is an American telecommunications company, a wholly owned subsidiary of Verizon Communications, which offers wireless products and services. Verizon Wireless is the largest wireless telecommunications provider in the United States.\nThe company is headquartered in Basking Ridge, New Jersey. It was founded in 2000 as a joint venture of American telecommunications firm Bell Atlantic, which would soon become Verizon Communications, and British multinational telecommunications company Vodafone.\nResponsibilities:\n\n\u2022 Responsible for working with various teams on a project to develop analytics based solution to target roaming subscribers specifically.\n\u2022 Leading a team of 4 data analysts and created multi-dimensional segmentation to define specific cohorts of subscribers.\n\u2022 Preparing the travel prediction model that can predict subscribers' future travel behavior up to a month in advance.\n\u2022 Combination of these elements (travel prediction & multi-dimensional segmentation) would enable operators to conduct highly targeted and personalized roaming services campaigns leading to significant subscriber uptake.\n\u2022 Scaled up to Machine Learning pipelines: 4600 processors, 35000 GB memory achieving 5-minute execution.\n\u2022 Deployed GUI pages by using JSP, JSTL, HTML, DHTML, XHTML, CSS, JavaScript, AJAX.\n\u2022 Configured the project on WebSphere 6.1 application servers\n\n\u2022 Developed a Machine Learning test-bed with 24 different model learning and feature learning algorithms.\n\u2022 By thorough systematic search, demonstrated performance surpassing the state-of-the-art (deep learning).\n\n\u2022 Up to 10 times more accurate predictions over existing state-of-the-art algorithms.\n\u2022 Developed in-disk, huge (100GB+), highly complex Machine Learning models.\n\u2022 Used SAX and DOM parsers to parse the raw XML documents\n\u2022 Used RAD as Development IDE for web applications.\n\n\u2022 Develop and plan required analytic projects in response to business needs.\n\u2022 In conjunction with data owners and department managers, contribute to the development of data models and protocols for mining production databases.\n\u2022 Develop new analytical methods and/or tools as required.\n\u2022 Contribute to data mining architectures, modeling standards, reporting, and data analysis methodologies.\n\u2022 Conduct research and make recommendations on data mining products, services, protocols, and standards in support of procurement and development efforts.\n\u2022 Work with application developers to extract data relevant for analysis.\n\u2022 Collaborate with unit managers, end users, development staff, and other stakeholders to integrate data mining results with existing systems.\n\u2022 Provide and apply quality assurance best practices for data mining/analysis services.\n\nEnvironment: R 9.0, Informatica 9.0, ODS, OLTP, Oracle 10g, Hive, OLAP, DB2, Metadata, MS Excel, Mainframes MS Vision, Rational Rose."", u'Data Scientist\nMarvell Technology Group - Santa Clara, CA\nDecember 2015 to January 2017\nDescription: Marvell Technology Group Limited is a producer of storage, communications and consumer semiconductor products. Marvell\'s U.S. operating headquarters is located in Santa Clara, California, and the company operates design centers in places including Canada, Europe, Israel, India, Singapore and China. Marvell is a ""fabless"" manufacturer of semiconductors. Its market segments include the enterprise, cloud, automotive, industrial and consumer markets.\n\nResponsibilities:\n\u2022 Developed the prediction model for crop yield, based on different kinds of field, weather and imagery data.\n\u2022 Exploratory data analysis and Feature engineering to best fit the regression model.\n\u2022 Designed a static pipeline in MS Azure for data ingestion and dashboarding. Used MS ML Studio for modeling and MS Power BI for dash boarding.\n\u2022 Analyze large datasets to provide strategic direction to the company.\n\u2022 Perform quantitative analysis of product sales trends to recommend pricing decisions.\n\u2022 Conduct cost and benefits analysis on new ideas.\n\u2022 Scrutinize and track customer behavior to identify trends and unmet needs.\n\u2022 Develop statistical models to forecast inventory and procurement cycles.\n\u2022 Assist in developing internal tools for data analysis.\n\u2022 Advising on the suitability of methodologies and suggesting improvements.\n\u2022 Carrying out specified data processing and statistical techniques.\n\u2022 Supplying qualitative and quantitative data to colleagues & clients.\n\u2022 Using Informatica & SAS to extract transform & load source data from transaction systems.\n\u2022 Creating data pipelines using big data technologies like Hadoop, spark etc.\n\u2022 Creating statistical models using distributed and standalone models to build various diagnostics, predictive and prescriptive solution.\n\u2022 Utilize a broad variety of statistical packages like SAS, R , MLIB, Graphs, Hadoop, Spark , MapReduce, Pig and others\n\u2022 Refine and train models based on domain knowledge and customer business objectives\n\u2022 Deliver or collaborate on delivering effective visualizations to support the client business objectives\n\u2022 Communicate to your peers and managers promptly as and when required.\n\u2022 Produce solid and effective strategies based on accurate and meaningful data reports and analysis and/or keen observations.\n\u2022 Establish and maintain communication with clients and/or team members; understand needs, resolve issues, and meet expectations\n\u2022 Developed web applications using .net technologies; work on bug fixes/issues that arise in the production environment and resolve them at the earliest\nEnvironment: SQL Server 2008R2/2005 Enterprise, SSRS, SSIS, Crystal Reports, Hadoop, Windows Enterprise Server 2000, DTS, SQL Profiler, and Query Analyzer.', u""Data Scientist\nEbay Inc - San Jose, CA\nFebruary 2014 to November 2015\nDescription: Ebay is a multinational e-commerce corporation, facilitating online consumer-to consumer and business-to-consumer sales. It is headquartered in San Jose, California. eBay was founded by Pierre Omidyar in 1995, and became a notable success story of the dot-com bubble.\nResponsibilities:\n\u2022 Developed scalable machine learning solutions within a distributed computation framework (e.g. Hadoop, Spark, Storm etc.).\n\u2022 Utilizing NLP applications such as topic models and sentiment analysis to identify trends and patterns within massive data sets.\n\u2022 Creating automated anomaly detection systems and constant tracking of its performance\n\u2022 Strong command of data architecture and data modelling techniques.\n\u2022 Knowledge in ML & Statistical libraries (e.g. Scikit-learn, Pandas).\n\u2022 Having knowledge to build predict models to forecast risks for product launches and operations and help predict workflow and capacity requirements for TRMS operations\n\u2022 Having experience with visualization technologies such as Tableau\n\u2022 Draw inferences and conclusions, and create dashboards and visualizations of processed data, identify trends, anomalies\n\u2022 Generation of TLFs and summary reports, etc. ensuring on-time quality delivery.\n\u2022 Participated in client meetings, teleconferences and video conferences to keep track of project requirements, commitments made and the delivery thereof.\n\u2022 Solved analytical problems, and effectively communicate methodologies and results\n\u2022 Worked closely with internal stakeholders such as business teams, product managers, engineering teams, and partner teams.\n\u2022 Data mining using state-of-the-art methods\n\u2022 Extending company's data with third party sources of information when needed\n\u2022 Enhancing data collection procedures to include information that is relevant for building analytic systems\n\u2022 Processing, cleansing, and verifying the integrity of data used for analysis\n\u2022 Doing ad-hoc analysis and presenting results in a clear manner.\n\u2022 Created automated metrics using complex databases.\n\u2022 Foster culture of continuous engineering improvement through mentoring, feedback, and metrics.\nEnvironment: Erwin r9.0, Informatica 9.0, ODS, OLTP, Oracle 10g, Hive, OLAP, DB2, Metadata, MS Excel, Mainframes MS Visio, Rational Rose, Requisite Pro. Hadoop, PL/SQL, etc."", u'Data Scientist\nEagle Trading Systems - Princeton, NJ\nNovember 2012 to January 2014\nDescription: Eagle Trading Systems Inc. is a financial investment advisory firm headquartered in Princeton, New Jersey. The firm manages 5 accounts totaling an estimated $481 Million of assets under management.\n\nResponsibilities:\n\u2022 Worked with Ajax API calls to communicate with Hadoop through Impala Connection and SQL to render the required data through it .These API calls are similar to Microsoft Cognitive API calls.\n\u2022 Good grip on Cloudera and HDP ecosystem components.\n\u2022 Used ElasticSearch (Big Data) to retrieve data into application as required.\n\u2022 Performed Map Reduce Programs those are running on the cluster.\n\u2022 Developed multiple MapReduce jobs in java for data cleaning and preprocessing.\n\u2022 Statistical Modelling with ML to bring Insights in Data under guidance of Principal Data Scientist\n\u2022 Data modeling with Pig, Hive, Impala.\n\u2022 Ingestion with Sqoop, Flume.\n\u2022 Used SVN to commit the Changes into the main EMM application trunk.\n\u2022 Understanding and implementation of text mining concepts, graph processing and semi structured and unstructured data processing.\n\u2022 Analyzed the partitioned and bucketed data and compute various metrics for reporting.\n\u2022 Involved in loading data from RDBMS and web logs into HDFS using Sqoop and Flume.\n\u2022 Worked on loading the data from MySQL to HBase where necessary using Sqoop.\n\u2022 Developed Hive queries for Analysis across different banners.\n\u2022 Extracted data from Twitter using Java and Twitter API. Parsed JSON formatted twitter data and uploaded to database.\n\u2022 Launching Amazon EC2 Cloud Instances using Amazon Images (Linux/ Ubuntu) and Configuring launched instances with respect to specific applications.\n\u2022 Exported the result set from Hive to MySQL using Sqoop after processing the data.\n\u2022 Analyzed the data by performing Hive queries and running Pig scripts to study customer behavior.\n\u2022 Have hands on experience working on Sequence files, AVRO, HAR file formats and compression.\n\u2022 Used Hive to partition and bucket data.\n\u2022 Experience in writing MapReduce programs with Java API to cleanse Structured and unstructured data.\n\u2022 Wrote Pig Scripts to perform ETL procedures on the data in HDFS.\n\u2022 Created HBase tables to store various data formats of data coming from different portfolios.\n\u2022 Worked on improving performance of existing Pig and Hive Queries.\n\nEnvironment: SQL/Server, Oracle 9i, MS-Office, Teradata, Informatica, ER Studio, XML, Business Objects, HDFS, Teradata 14.1, JSON, HADOOP (HDFS), MapReduce, PIG, Spark, R Studio, MAHOUT, JAVA, HIVE, AWS..', u'Data Architect/Data Modeler\nFirst Indian Corporation Private Limited - Hyderabad, Telangana\nFebruary 2011 to October 2012\nDescription: First Indian Corporation Private Limited, a member of The First American family of companies, provides specialized offshore transaction services, technology services and analytics to the mortgage industry.\n\nResponsibilities:\n\u2022 Worked with large amounts of structured and unstructured data.\n\u2022 Knowledge in Machine Learning concepts (Generalized Linear models, Regularization, Random Forest, Time Series models, etc.)\n\u2022 Worked in Business Intelligence tools and visualization tools such as Business Objects, Tableau, ChartIO, etc.\n\u2022 Deployed GUI pages by using JSP, JSTL, HTML, DHTML, XHTML, CSS, JavaScript, AJAX.\n\u2022 Configured the project on WebSphere 6.1 application servers\n\u2022 Implemented the online application by using Core Java, Jdbc, JSP, Servlets and EJB 1.1, Web Services, SOAP, WSDL.\n\u2022 Handled end-to-end project from data discovery to model deployment.\n\u2022 Monitoring the automated loading processes.\n\u2022 Communicated with other Health Care info by using Web Services with the help of SOAP, WSDL JAX-RPC.\n\u2022 Used Singleton, factory design pattern, DAO Design Patterns based on the application requirements\n\u2022 Used SAX and DOM parsers to parse the raw XML documents\n\u2022 Used RAD as Development IDE for web applications.\n\u2022 Preparing and executing Unit test cases\n\u2022 Used Log4J logging framework to write Log messages with various levels.\n\u2022 Involved in fixing bugs and minor enhancements for the front-end modules.\n\u2022 Implemented Microsoft Visio and Rational Rose for designing the Use Case Diagrams, Class model, Sequence diagrams, and Activity diagrams for SDLC process of the application\n\u2022 Doing functional and technical reviews\n\u2022 Maintenance in the testing team for System testing/Integration/UAT.\n\u2022 Guaranteeing quality in the deliverables.\n\u2022 Conducted Design reviews and Technical reviews with other project stakeholders.\n\u2022 Was a part of the complete life cycle of the project from the requirements to the production support\nEnvironment: R 3.0, Erwin 9.5, Tableau 8.0, MDM, QlikView, MLLib, PL/SQL, HDFS, Teradata 14.1, JSON, HADOOP (HDFS), MapReduce, PIG, Spark, R Studio, MAHOUT, JAVA, HIVE, AWS.', u'Data Analyst/Data Modeler\nAccenture - Bengaluru, Karnataka\nAugust 2009 to January 2011\nDescription: Accenture PLC is a global management consulting and professional services company that provides strategy, consulting, digital, technology and operations.\n\nResponsibilities:\n\u2022 Developed Internet traffic scoring platform for ad networks, advertisers and publishers (rule engine, site scoring, keyword scoring, lift measurement, linkage analysis).\n\u2022 Responsible for defining the key identifiers for each mapping/interface.\n\u2022 Clients include eBay, Click Forensics, Cars.com, Turn.com, Microsoft, and Looksmart.\n\u2022 Designed the architecture for one of the first analytics 3.0. online platforms: all-purpose scoring, with on-demand, SaaS, API services. Currently under implementation.\n\u2022 Web crawling and text mining techniques to score referral domains, generate keyword taxonomies, and assess commercial value of bid keywords.\n\u2022 Developed new hybrid statistical and data mining technique known as hidden decision trees and hidden forests.\n\u2022 Reverse engineering of keyword pricing algorithms in the context of pay-per-click arbitrage.\n\u2022 Implementation of Metadata Repository, Maintaining Data Quality, Data Cleanup procedures, Transformations, Data Standards, Data Governance program, Scripts, Stored Procedures, triggers and execution of test plans\n\u2022 Performed data quality in Talend Open Studio.\n\u2022 Coordinated meetings with vendors to define requirements and system interaction agreement documentation between client and vendor system.\n\u2022 Automated bidding for advertiser campaigns based either on keyword or category (run-of-site) bidding.\n\u2022 Creation of multimillion bid keyword lists using extensive web crawling. Identification of metrics to measure the quality of each list (yield or coverage, volume, and keyword average financial value).\n\u2022 Enterprise Metadata Library with any changes or updates.\n\u2022 Document data quality and traceability documents for each source interface.\n\u2022 Establish standards of procedures.\n\u2022 Generate weekly and monthly asset inventory reports.\n\nEnvironment: Erwin r7.0, SQL Server 2000/2005, Windows XP/NT/2000, Oracle 8i/9i, MS-DTS, UML, UAT, SQL Loader, OOD, OLTP, PL/SQL, MS Visio, Informatica.']","[u""Bachelor's""]",[u''],"degree_1 : ""Bachelors"""
0,https://resumes.indeed.com/resume/e30cbe00c3db015c,"[u'Senior Data Scientist\nHOTELS.COM - Dallas, TX\nJanuary 2016 to Present\nBuilt and maintained near real-time auction models for accurately predicting Cost per Click to be used\nin Google paid online marketing and other auction platforms.\n\u2022 Improved existing bidding models with additional cancellation predictions and seasonality analysis to optimize business values and better understand the market\n\u2022 Worked with stakeholders to develop cutting edge data analytics, identify business opportunities, and drive business insights using advanced machine learning and data visualization techniques', u'Data Scientist\nDATAROBOT - Boston, MA\nJanuary 2015 to January 2016\n\u2022 Assisted in the engineering of the core SaaS product, an automated machine learning platform\n\u2022 Evaluated various data science and deep learning tools and platforms, like Azure ML Platform, H2O,\nAmazon ML at AWS etc.\n\u2022 Built POC predictive data science products across various domains, including Insurance, Banking,\nRetail, Health and Finance.', u'Data Scientist\nAT&T - Plano, TX\nJanuary 2014 to January 2015\nBuilt an integrated model to predict all subsystem failures by utilizing AT&T Fleet demand repair data,\nOBD II sensor data, weather data and refueling data daily and sensor data every 2 minutes (10 Gb/day)\n\u2022 Analyzed health-related data and developed models for providing better insurance plans and options\n\u2022 Utilized NLP techniques for automatically identifying machine flaws', u'Research Assistant\nMichigan State University - East Lansing, MI\nJanuary 2009 to January 2014\n\u2022 Taught various physics and astronomy courses\n\u2022 Conducted research in theoretical particle and condensed matter physics']",[u'Ph.D. in Physics in Physics'],"[u'MICHIGAN STATE UNIVERSITY East Lansing, MI']",degree_1 : Ph.D. in Physics in Physics
0,https://resumes.indeed.com/resume/f45b19563686cee0,"[u'Lead Data Scientist\nNielsen\nJanuary 2014 to Present\nGlobal performance management company that provides a comprehensive understanding of what consumers Watch, Listen to, and Buy\nLead Data Scientist (2014 - Present) reporting to Director of Data Science and SVP of Data Science\n\n\u2022 Lead efforts to improve TV audience/media measurement through the integration of third party provider data. Develop methodologies and proprietary corporate intellectual property for patenting and product development.\n\u2022 Lead common homes analysis between Nielsen National People Meter data and new data from third party providers for methodological development, model creation and calibration as well as KPI quality assurance measurements through a multitude of data visualizations in Tableau.\n\u2022 Responsible for leading the effort to prototype, test, verify and operationalize predictive models for Nielsen Audio Diary measurement and sample representation.\n\u2022 Integral part of a large scale, high profile cross functional effort between Sampling, Methods, Research, Operations, and Information Technology teams to implement a system for survey sample stratification aimed at improving demographic proportionality using rate management, trend analysis, and sophisticated predictive models\n\u2022 Develop operational models for incenting interviewers in the call center based on panel acquisition and performance to control operational costs both in the call center and throughout sample life cycle\n\u2022 Perform deep dive data analytics to address client inquiries, market deliverables, target management, panel management, sample demographic proportionality, root cause analysis, and impact analysis of operational performance rates\n\u2022 Implement machine learning techniques to understand factors that influence sample and panel performance rates and identify risks and strategies for operational improvements.\n\u2022 Perform data life cycle management to include data acquisition, data management, ETL, and data governance\n\u2022 Perform research and analytics, create data visualizations, and presentation materials to translate complex problems and communicate operational performance metrics to corporate management, internal and external auditors (Earnest and Young), as well as the Media Ratings Council', u'Private Tutor\nAcademic Instruction Expertise\nJanuary 2008 to Present\nProvide instruction to high school and college students enrolled in college level mathematics courses', u""Senior Data Scientist/Statistical Analyst\nNielsen Audio - Columbia, MD\nJanuary 2012 to January 2014\nColumbia, Maryland\nConsumer research company that collects individual's exposure to media and entertainment data; best known for its Radio Ratings\nSenior Data Scientist/Statistical Analyst (2012 - 2014) reporting to Vice President of Operations\n\n\u2022 Implement data mining techniques to analyze large amounts of data (historical and transactional) to determine and model diary return rates and PPM panel behavior\n\u2022 Lead project which integrated Bureau of Labor and Statistics American Time Usage data sets to analyze and predict best times to reach/recruit panelists\n\u2022 Lead several projects to provide analysis and monitoring of key metrics (KPIs) such as market designated delivery indices, as well as audio and diary panel performance rates\n\u2022 Exploit data mining techniques to improve operation efficiencies\n\u2022 Employ data visualization techniques to perform attribute analysis\n\u2022 Work on cross functional projects across the organization to provide planning and analysis of incentives designed to improve panel behavior and retention\n\u2022 Deep dive data analytics to address root cause analysis and sentiment analysis of panel behavior\n\u2022 Employ statistical analytic methods to determine factors that influence operational rates\n\u2022 Manage regional markets to assist with recruiting and management of PPM panel to meet client deliverables\n\u2022 Provide management with reports, dashboards and on-the-fly graphical insights of Nielsen Audio data"", u'Mathematics Teacher\nAcademic Instruction Expertise - Ellicott City, MD\nJanuary 2008 to January 2012\nEllicott City, Maryland\nFounded in 1954, GSC is the only independent college preparatory pre-K through grade 12 day school in Howard County.\nMathematics Teacher (2008 - 2012) - Hired to specifically teach the Upper Schools AP Statistics and Calculus courses\n\n\u2022 Design, develop and create educational materials to teach mathematics to students in the Upper School, grades 9 through 12 Teaching assignments include courses in calculus, statistics, geometry, geometry honors, and algebra\n\u2022 Teach advanced placement college level courses which include AP Calculus (both AB and BC levels) and AP Statistics\n\u2022 Advise an assigned group of students throughout their four years in the upper school and provide counseling and support regarding academic performance, course selection, and disciplinary concerns\n\u2022 Co-Director and Advisory Board Leader of GCS National Honors Society (NHS) Charter responsible for creating and upholding GCS specific NHS standards, as well as reviewing applications, selecting student candidates, and coordinating the induction ceremony\n\u2022 Mock Trial Club Coach working with students to prepare and attend county and state wide trial competitions after school with active judges at local courthouses', u'Project Manager/Senior Consultant\nHighStakes Solutions, Inc - Columbia, MD\nJanuary 1998 to January 2012\nColumbia, Maryland\nHighStakes Solutions was a leading provider of reporting software and system consultants to the financial industry.\nProject Manager/Senior Consultant (1998-2012) reporting to managing director\n\n\u2022 Managed and lead programmers developing custom user interfaces, analytical, and reporting products using C, C++, and Java accessing large data sets stored in Vision (time-series object oriented database), Oracle, Access, and Microsoft SQL Server\n\u2022 Provide analytics, time series analysis and forecasting of financial data used by customers to make critical investment decisions.\n\u2022 Performed business analysis and requirements analysis for client specific custom product development\n\u2022 Provided technical training materials for financial products\n\u2022 Designed and reviewed training courses for money management industry which served as both a tool for client acquisition as well as client retention', u'Mathematics Instructor\nHighStakes Solutions, Inc - Columbia, MD\nJanuary 2007 to January 2009\nTaught semester courses in Statistics, Calculus I, Business Calculus, and College Algebra/Geometry', u'Project Manager/Lead Developer\nEye Street Software - Vienna, VA\nJanuary 1997 to January 1998\nVienna, Virginia\nEye Street Software is a leading software development and consulting firm with extensive experience in sophisticated systems integration and development projects\nProject Manager/Lead Developer (1997-1998) reporting to managing partners\n\n\u2022 Supervised, lead and developed software applications and user interfaces through the full-life cycle development (SDLC) process for customer projects using state of the art technologies in Java and relational database management systems\n\u2022 Interfaced software with other COTS applications', u'Project Manager/Lead Developer\nVan Dyke & Associates - Annapolis Junction, MD\nJanuary 1995 to January 1997\nAnnapolis Junction, Maryland\nVan Dyke & Associated is a premier contractor for the Department of Defense. The company focused primarily on data security and message encryption\nProject Manager/Lead Developer (1995-1997) reported to President of Annapolis Junction Division\n\n\u2022 Assumed responsibility for all aspects of project management, development, and client relations/acquisition\n\u2022 Lead team of five technical professionals through the full-life cycle development (SDLC) of custom software applications while tasked as the principal software developer\n\u2022 Devised solutions to allow software integration of various applications using C, C++, and SQL for a variety of vendor specific RDBMS technologies while maximizing performance for large data sets\n\u2022 Designed, developed, and taught corporate training classes for Object Oriented Programming\n\u2022 Gathered customer requirements, defined solutions, created, and edited technical documents for system development projects\n\u2022 Data analytics and development of reporting tools\n\u2022 Business/requirements analysis and client development, support and relations']","[u'MS in Applied Mathematics', u'BS in Mathematics and Computer Science']","[u'Johns Hopkins University', u'University of Maryland Baltimore County Baltimore, MD']","degree_1 : MS in Applied Mathematics, degree_2 :  BS in Mathematics and Compter Science"
0,https://resumes.indeed.com/resume/74e0e0dc9346e5f8,"[u'Data Scientist\nFedEx - Memphis, TN\nApril 2018 to Present\nData Scientist in the Engineering and Decision Support department.\nProvide insights to customers of FedEx on shipment of product details.\nMaximize revenue and minimize cost by analyzing and predicting which features of shipments can be altered.\nProvide dashboards and data visualization using SAS Visual Analytics.\nData analyzed in R and SAS.\nLanguages and tools: R, SAS, SAS Visual Analytics, machine learning, decision trees, cluster analysis', u'Data Science Consultant\nFogelman College of Business and Economics\nJanuary 2018 to Present\nLead initiative on project and project life cycle to analyze 1 GB of proprietary data from Fortune 100 Company.\nPurpose of project is to identify significant features associated with maximization of revenue.\nUtilizing machine learning techniques such as cross validation, Lasso regression, and linear regression.\nCreating a GUI for easy feature selection that utilizes machine learning model.\nDeliverables are expected to be completed by end of the year.\nProvide reports and dashboards for company.\nProvide project updates to company.\nLanguages and tools: R, statistical programming, Lasso regression, bootstrap, cross validation', u""Research Data Specialist\nSt. Jude Children's Research Hospital\nMay 2016 to April 2018\nCreated database that stores data from cancer treatment protocols using customized VBA.\nPrimary database administrator using SQL statements to extract data prior to analysis in SAS and R.\nProvide reports and dashboards for protocol updates and patient progress through interventions.\nImplemented imputation methods along with non-parametric techniques for protocol objectives.\nAnalyzed patient text responses using Multidimensional Scaling.\nLanguages and tools: VBA, SQL, SAS, R, cmdscale, Bootstrap, non-parametric statistics, statistical programming"", u'Data Scientist Consultant\nSchool of Public Health\nJanuary 2016 to December 2017\nAnalyzed large volumes of genomic data to locate associations with diseases.\nUtilized R to iteratively use logistic regression for 548 genes and associations with asthma and covariates.\nUsed Lasso regression for significant variable selection out of 326 variables.\nPrepared a comprehensive statistical report which included R generated figures.\nLanguages and tools: R, Glmnet, Lasso regression, logistic regression, SAS, Python, statistical programming', u'Data Scientist Consultant\nInstitute for Intelligent Systems\nAugust 2011 to December 2017\nProvided statistical support for experimental designs such as nested RBD.\nAnalyzed significant features associated with performance outcomes using GBC.\nAnalyzed changes in proportions of reported outcomes using Cochran-Armitage Trend Test.\nLanguage and tools: Python, R, SAS, statistical inference, PCA, gradient boosting, sklearn, statistical programming\n\nTechnologies/Languages:SAS, R, MATLAB, SPSS, Python Sklearn, MS SQL, VBA, statistical inference, Bootstrap, non-parametric statistics, predictive modeling, PCA, Lasso regression, machine learning', u'Research Statistical Consultant\nSchool of Public Health\nJuly 2013 to May 2016\nAnalyzed 14 GB of electrophysiological brain data via MATLAB and Neuroscan Curry.\nCreated customized MATLAB scripts for matrix manipulation of 21 x 68 x 1,000,000 data.\nUtilized PCA dimensionality reduction prior to data analysis.\nAnalyzed data using SAS mixed modeling with Tukey post-hoc test adjusting for multiple comparisons.\nPrepared comprehensive statistical report for supervisor which included R figures and plots.\nLanguage and tools: MATLAB, R, SAS, PCA, mixed modeling, glm, Neuroscan Curry, statistical programming']","[u""Master's""]",[u''],"degree_1 : ""Masters"""
0,https://resumes.indeed.com/resume/e5d4ffccec89d378,"[u'Data Scientist Intern\nBaylor Miraca Genetics - Houston, TX\nJanuary 2018 to Present\n\u2022 Modify and rebuild models to improve the analytical results utilizing R and data visualization tools\n\u2022 Interpret, validate data using statistical and judgement tests\n\u2022 Support in-dept research by processing, synthesizing, and integrating data with over 33,000 genomic samples']","[u'Master of Science in Mining, Design and Analysis of Experiments', u'Bachelor of Science in Chemical Engineering']","[u'University of Houston Houston, TX\nMay 2018', u'University of Arkansas Fayetteville, AR\nMay 2016']","degree_1 : Master of Science in Mining, degree_2 :  Design and Analysis of Experiments, degree_3 :  Bachelor of Science in Chemical Engineering"
0,https://resumes.indeed.com/resume/c66cb037a14c978a,"[u'Data Scientist (full-time)\nTrexquant Investment LP\nJanuary 2016 to Present\nBuilt automatic data pipeline in a UNIX / Linux environment; parsed unstructured real world data into clean time series data that can be readily utilized by researchers on matlab platform.\n\u2022 Developed computational models that translates insights from real world data into decisions/strategies;\napplied advanced statistical and machine learning algorithms to leverage the predictive power in data.\n\u2022 Work with the data team, research team and business directors simultaneously to coordinate cross team\nprojects for building innovative analytic workflows and problem-solving.', u'Data Scientist\nTrexquant LP - Stamford, CT\nJanuary 2016 to Present\nbuilding data analytic workflows to transform real world raw data into meaningful, research grade data. I develop statistical and machine learning models to leverage the predictive power and hidden insights in these raw datasets.', u'Research Assistant\nComputational Neuroscience, Brown University\nJune 2011 to December 2015\nProcessing Real-world, large-scale bio-medical data (30GB/hr) from medical devices (Brain computer\ninterfaces). Data is in high-dimension, unstructured and noisy.\n\u2022 Independently developed a neuronal data analysis workflow and published the analysis results in several\npeer reviewed journals in the biomedical field, topics including:\n1. Classification of brain states with data from biomedical devices, using logistic regression and GLMs.\n2. Application of dimension reduction techniques (such as PCA and Factor Analysis) in high dimension data.\n3. Neuronal data time series analysis: time-frequency analysis for wave-propagations, spectrograms.\nInvestigated both continuous time series data and data on discrete bio-events.\n\nBIOMEDICAL PUBLICATIONS (selected)\n\u2022 Y Lu, W Truccolo et al., Optogenetically-Induced Spatiotemporal Gamma Oscillations and Neuronal Spiking\nActivity in Primate Motor Cortex, Journal of Neurophysiology, Mar 11 2015, DOI: 10.1152/jn.00792.2014\n\u2022 Y Lu, A. V. Nurmikko et al. Modulating dopamine release by optogenetics in transgenic mice reveals terminal\ndopaminergic dynamics, Neurophotonics. 2(3), Jul 09 2015, DOI: 10.1117/1.NPh.2.3.031207', u'Head TA of NeuroEngineering\nBrown Univ\nJanuary 2012 to January 2013\nInstructed on the software for neural activity simulation, coded neural network models.']","[u'Ph.D. in Physical Chemistry', u'B. Sc. in Chemistry']","[u'Brown University Providence, RI\nDecember 2015', u'Tsinghua University \u5317\u4eac\u5e02\nMay 2010']","degree_1 : Ph.D. in Physical Chemistry, degree_2 :  B. Sc. in Chemistry"
0,https://resumes.indeed.com/resume/5d43fc248856fdd5,"[u'Data Scientist\nWells Fargo - Raleigh, NC\nJune 2017 to Present\nDescription: Wells Fargo & Company is an American international banking and financial services holding company headquartered in San Francisco, with ""hubquarters"" throughout the country. It is the world\'s second-largest bank by market capitalization and the third largest bank in the U.S. by assets.\n\nResponsibilities:\n\u2022 Performed Data Profiling to learn about behavior with various features such as traffic pattern, location, time, Date and Time etc.\n\u2022 Application of various machine learning algorithms and statistical modeling like decision trees, regressionmodels, neural networks, SVM, clustering to identify Volume using scikit-learn package in python, Matlab.\n\u2022 Utilized Spark, Scala, Hadoop, HBase, Cassandra, MongoDB, Kafka, Spark Streaming, MLLib, Python, a broad variety of machine learning methods including classifications, regressions, dimensionally reduction etc. and Utilized the engine to increase user lifetime by 45% and triple user conversations for target categories.\n\u2022 Developed Spark/Scala, Python for regular expression (regex) project in the Hadoop/Hive environment with Linux/Windows for big data resources. Used clustering technique K-Means to identify outliers and to classify unlabeled data.\n\u2022 Evaluated models using Cross Validation, Log loss function, ROC curves and used AUC for feature selection.\n\u2022 Analyze traffic patterns by calculating autocorrelation with different time lags.\n\u2022 Ensured that the model has low False Positive Rate.\n\u2022 Addressed overfitting by implementing of the algorithm regularization methods like L2 and L1.\n\u2022 Used Principal Component Analysis in feature engineering to analyze high dimensional data.\n\u2022 Created and designed reports that will use gathered metrics to infer and draw logical conclusions of past and future behavior.\n\u2022 Performed Multinomial Logistic Regression, Randomforest, Decision Tree, SVM to classify package is going to deliver on time for the new route.\n\u2022 Performed data analysis by using Hive to retrieve the data from Hadoopcluster, Sql to retrieve data from Oracle database.\n\u2022 Used MLlib, Spark\'s Machine learning library to build and evaluate different models.\n\u2022 Implemented rule based expertise system from the results of exploratory analysis and information gathered from the people from different departments.\n\u2022 Performed Data Cleaning, features scaling, features engineering using pandas and numpy packages in python.\n\u2022 Developed MapReduce pipeline for feature extraction using Hive.\n\u2022 Created Data QualityScripts using SQL and Hive to validate successful data load and quality of the data. Created various types of data visualizations using Python and Tableau.\n\u2022 Communicated the results with operations team for taking best decisions.\n\u2022 Collected data needs and requirements by Interacting with the other departments.\n\nEnvironment: Python 2.x, CDH5, HDFS, Hadoop2.3, Hive, Impala, Linux, Spark, Tableau Desktop, SQL Server 2012, Microsoft Excel, Mat lab, Spark SQL, Pyspark.', u'Data Scientist\nVM ware - Palo Alto, CA\nApril 2016 to June 2017\nDescription: VMware, Inc. is a subsidiary of Dell Technologies that provides cloud computing and platform virtualization software and services. It was the first commercially successful company to virtualize the x86 architecture.\n\nResponsibilities:\n\u2022 Provided Configuration Management and Build support for more than 5 different applications, built and deployed to the production and lower environments.\n\u2022 Implemented public segmentation using unsupervised machine learning algorithms by implementing k-means algorithm using Pyspark.\n\u2022 Explored and Extracted data from source XML in HDFS, preparing data for exploratory analysis using data munging.\n\u2022 Responsible for different Data mapping activities from Source systems to Teradata\n\u2022 Handled importing data from various data sources, performed transformations using Hive, Map Reduce, and loaded data into HDFS\n\u2022 Used R and python for Exploratory Data Analysis, A/B testing, Anova test and Hypothesis test to compare and identify the effectiveness of Creative Campaigns.\n\u2022 Created clusters to classify Control and test groups and conducted group campaigns.\n\u2022 Analyzed and calculated the lifetime cost of everyone in the welfare system using 20 years of historical data.\n\u2022 Developed LINUX Shell scripts by using NZSQL/NZLOAD utilities to load data from flat files to Netezza database.\n\u2022 Developed triggers, stored procedures, functions and packages using cursors and ref cursor concepts associated with the project using Pl/SQL\n\u2022 Created various types of data visualizations using R, python and Tableau.\n\u2022 Used Python, R, SQL to create Statistical algorithms involving Multivariate Regression, LogisticRegression, PCA, Random forest models, Decision trees, Support Vector Machine for estimating the risks of welfare dependency.\n\u2022 Identified and targeted welfare high-risk groups with Machine learning algorithms.\n\u2022 Conducted campaigns and run real-time trials to determine what works fast and track the impact of different initiatives.\n\u2022 Developed Tableau visualizations and dashboards using Tableau Desktop.\n\u2022 Used Graphical Entity-Relationship Diagramming to create new database design via easy to use, graphical interface.\n\u2022 Created multiple custom SQL queries in Teradata SQL Workbench to prepare the right data sets for Tableau dashboards\n\u2022 Perform analyses such as regression analysis, logistic regression, discriminant analysis, cluster analysis using SAS programming.\n\u2022 Used Meta data tool for importing metadata from repository, new job categories and creating new data elements.\n\u2022 Scheduled the task for weekly updates and running the model in workflow. Automated the entire process flow in generating the analysis and reports.\n\nEnvironment:R3.x, HDFS, Hadoop2.3, Pig, Hive, Linux, R-Studio, Tableau 10, SQL Server, Ms Excel, Pypark.', u""Data Scientist\nLifetime HealthCare - Rochester, NY\nDecember 2014 to March 2016\nDescription: The Lifetime Healthcare Companies is a family of companies that provide health coverage and health care services to more than 1.6 million people. Our $6 billion nonprofit company is headquartered in Rochester, N.Y., and employs nearly 6,000 workers throughout Upstate New York through nationally recognized businesses.\n\nResponsibilities:\n\u2022 Involved in Design, Development and Support phases of Software Development Life Cycle (SDLC)\n\u2022 Performed data ETL by collecting, exporting, merging and massaging data from multiple sources and platforms including SSIS (SQL Server Integration Services) in SQL Server.\n\u2022 Worked with cross-functional teams (including data engineer team) to extract data and rapidly execute from MongoDB through MongDB connector for Hadoop.\n\u2022 Performed data cleaning and feature selection using MLlib package in PySpark.\n\u2022 Performed partitional clustering into 100 by k-means clustering using Scikit-learn package in Python where similar hotels for a search are grouped together.\n\u2022 Used Python to perform ANOVA test to analyze the differences among hotel clusters.\n\u2022 Implemented application of various machine learning algorithms and statistical modeling like Decision Tree, Naive Bayes, Logistic Regression and Linear Regression using IBM SPSS to determine the accuracy rate of each model.\n\u2022 Determined the most accurately prediction model based on the accuracy rate.\n\u2022 Used text-mining process of reviews to determine customers' concentrations.\n\u2022 Delivered analysis support to hotel recommendation and providing an online A/B test.\n\u2022 Designed Tableau bar graphs, scattered plots, and geographical maps to create detailed level summary reports and dashboards.\n\u2022 Developed hybrid model to improve the accuracy rate.\n\u2022 Delivered the results to operation team for better decisions and feedbacks.\n\nEnvironment: Python, PySpark, Tableau, MongoDB, Hadoop, SQL Server, SDLC, ETL, SSIS, recommendation systems, Machine Learning Algorithms, text-mining process, A/B test"", u""Assoc Data Scientist\nBank of America - Wilmington, DE\nApril 2013 to November 2014\nDescription: Bank of America is a multinational banking and financial services corporation. It is ranked 2nd on the list of largest banks in the United States by assets. As of 2016, Bank of America was the 26th largest company in the United States by total revenue.\n\nResponsibilities:\n\u2022 Participated in all phases of research including data collection, data cleaning, data mining, developing models and visualizations.\n\u2022 Collaborated with data engineers and operation team to collect data from internal system to fit the analytical requirements.\n\u2022 Redefined many attributes and relationships and cleansed unwanted tables/columns using SQL queries.\n\u2022 Utilized Spark SQL API in PySpark to extract and load data and perform SQL queries.\n\u2022 Performed data imputation using Scikit-learn package in Python.\n\u2022 Performed data processing using Python libraries like Numpy and Pandas.\n\u2022 Worked with data analysis using ggplot2 library in R to do data visualizations for better understanding of customers' behaviors.\n\u2022 Visually plotted data using Tableau for dashboards and reports.\n\u2022 Implemented statistical modeling with XGBoost machine learning software package using R to determine the predicted probabilities of each model.\n\u2022 Delivered the results with operation team for better decisions.\n\nEnvironment:Python, R, SQL, Tableau, Spark, Machine Learning Software Package, recommendation systems."", u""Programmer Analyst\nWipro Limited - Bengaluru, Karnataka\nNovember 2011 to March 2013\nDescription: Wipro Limited is an Indian Information Technology Services corporation headquartered in Bengaluru, India. In 2013, Wipro demerged its non-IT businesses into separate companies to bring in more focus on independent businesses\n\nResponsibilities:\n\u2022 Worked with team of developers on Python applications for RISK management.\n\u2022 Designed the database schema for the content management system.\n\u2022 Designed and developed the UI of the website using HTML, XHTML, AJAX, CSS and JavaScript.\n\u2022 Involved in development of Web Services using SOAP for sending and getting data from the external interface in the XML format.\n\u2022 Used PHP to write dynamically generated pages quickly.\n\u2022 Wrote Python routines to log into the websites and fetch data for selected options.\n\u2022 Web pages on the internet are generated by servers running LINUX.\n\u2022 Worked on middle tier and persistence layer. Created service and model layer classes and Value objects/POJO to hold values between java classes and database fields.\n\u2022 Exported/Imported data between different data sources using SQL Server Management Studio. Maintained program libraries, users' manuals and technical documentation.\n\u2022 Responsible for debugging and troubleshooting the web application.\n\u2022 Worked PHP as a server side scripting language, for web pages.\n\u2022 Successfully migrated all the data to the database while the site was in production\n\u2022 Implemented the validation, error handling and caching framework with Oracle Coherence cache.\n\u2022 Worked on scripts for setting up the discovery client with attribute data..\n\nEnvironment: Python, Oracle, Eclipse, MySQL, Linux, HTML, CSS, AJAX, JavaScript."", u""Programmer Analyst\nFramatome Connectors OEN\nApril 2009 to October 2011\nDescription: Framatome Connectors OEN is an Indian company engaged in the manufacture and marketing of Professional Grade Connectors. The company is a collaborative effort between the French company FCI France and its Indian partner OEN. FCI France presently holds 61.5% of the total equity stake of the Indian company.\n\nResponsibilities:\n\u2022 Involved in development, design and implementation of front end part widget based application.\n\u2022 Developed the User Interactive web pages in a professional manner with using web technologies like HTML, XHTML, and CSS as per company's standards.\n\u2022 Developed presentation-tier JSP pages in HTML, Implemented jQuery Data Grid control, Validation control and other Widget controls.\n\u2022 Developed Web forms/user interfaces using Struts MVC, jQuery and JSP.\n\u2022 Used Ajax Controls, Web forms, JavaScript and HTML for Commission, Payment and Inventory reports.\n\u2022 Worked on AJAX controls like Update Panel to manage the post back of the web page to server, AJAX Script Manager and Script Manager Proxy controls to register JavaScript and web service files.\n\u2022 Used JavaScript accordingly for validation purpose, browser detection and controls.\n\u2022 Developed various AJAX controls and widgets to build a rich User Interface for the application.\n\u2022 Created Windows services to create automation processing of XML.\n\u2022 Used Cascading Style Sheets (CSS) to maintain design consistency across all web forms.\n\u2022 Involved in developing jQuery and AJAX wrapper classes for fast retrieval of data and for animations.\n\u2022 Created Model objects using Entity Data Model with Entity framework using Hibernate.\n\u2022 Worked with XPath, XML Data documents to synchronize with dataset.\n\nEnvironment: HTML, JavaScript, CSS, jQuery, XML, XPath, AJAX, JSP, MVC, Junit, Windows..""]",[u'Bachelor of Computer Science in TOOLS AND TECHNOLOGIES'],[u'Informatica Power Centre'],degree_1 : Bachelor of Compter Science in TOOLS AND TECHNOLOGIES
0,https://resumes.indeed.com/resume/1c7d6fbd3fcbd01a,"[u'Graduate Teaching Assistant\nUniversity of Connecticut - Hartford, CT\nJanuary 2018 to Present\n\uf0a7 Graded over 240 assignments about optimization models and macros programming in Excel.\n\uf0a7 Tutored students and provide support about sensitive analysis and linear programming in class.', u'Data Analyst / Business Development Intern\nMenefee Associates Consulting, LLC - Manchester, CT\nNovember 2017 to Present\nAssist start-up companies in Connecticut to develop business plans. Conduct market and industry research; use data analytics and visual tools to help companies achieve growth in sales, revenue, and customer service skills.\n\u2022 Developed project plans for designing social reward system in fitness app, identify tasks and resources, assign deadlines and responsibilities, track and report status.\n\u2022 Executed and directed the coordination of implementation tasks involving IT team, new partnerships as well as provide\nconsultation to clients on fitness application social rewards system.\n\u2022 Built interactive sales data visualization Tableau dashboard for Howard K. Hill Funeral Services, LLC to evaluate current business performance.\n\u2022 Compiled research and analyzed data on over 60 Banquet Hall companies to develop business start-up plans and net present\nvalue (NPV) analysis for client.', u'Data Scientist Intern\nPublicis Groupe - Shanghai, CN\nApril 2017 to September 2017\nBuilt database warehouse using structured and unstructured data sets and ETL processes. Ensured data underwent processing, cleansing, and verification of integrity. Supplied teams with advertising investment strategies using television rating predictions.\n\u2022 Built up database warehouse in SqlServer to store data and connect to tableau for visualization.\n\u2022 Scraped drama content data from webs using Python for analysis.\n\u2022 Built Ridge and Lasso regression models in R to predict 2017 TV ratings over 2000 different target audience markets and achieved above 0.8 accuracy rate at 5% margin of error.\n\u2022 Utilized time series forecasting (Holt-Winter, ARIMA models) to predict ratings for 70 different TV channels.\n\u2022 Generated advertising investment strategies from TV rating predictions for planning teams.\n\u2022 Analyzed influential factors of cosmetics brand image from marketing surveys (Structural Equation Model) to increase brand\nawareness.', u'Marketing Research Intern\nStarbucks - Changsha, CN\nFebruary 2015 to August 2015\n\u2022 Organized customer focus groups and design questionnaires to study customer satisfaction for Starbucks.\n\u2022 Analyzed over 2,700 questionnaires to relay advice from customer experiences and promotional campaigns.']","[u'Master of Science in Business', u'Bachelor of Business Administration in Marketing']","[u'University of Connecticut School of Business Hartford, CT\nMay 2018', u'Hunan University\nJune 2016']","degree_1 : Master of Science in Bsiness, degree_2 :  Bachelor of Bsiness Administration in Marketing"
0,https://resumes.indeed.com/resume/68ea0cbc0dc75d6e,"[u'Data Scientist\nDeep-R - San Francisco, CA\nJune 2017 to September 2017\n\u25cf Developed web scraping tool using Python, Scrapy and Selenium to crawl 100+ government and private web pages to collect commodity and stock data from commercial and agricultural industries.\n\u25cf Cleaned 20 years\u2019 worth of yearly and monthly data (4,000+ data points) with Python and Pandas to analyze and visualize in Plotly graphs designed to predict monthly oil demand with 90% accuracy.\n\u25cf Researched 10 potential machine learning models such as linear regression and ARIMA for forecasting oil demand.', u'Associate Engineer\nCity of Palo Alto - Palo Alto, CA\nJanuary 2016 to December 2016\n\u25cf Documented over 600 utility mapping jobs in GIS database with data verification from field engineers.', u'Design Engineer\nBKF Engineers - Redwood City, CA\nMarch 2015 to January 2016\n\u25cf Documented over 600 utility mapping jobs in GIS database with data verification from field engineers.', u'Project Engineer\nGEI Consultants - Sacramento, CA\nMay 2012 to September 2013\n\u25cf Crafted Java UI for more than 100 users to input, output, and update one of 1000 rainfall gauge stations for historical documentation of California rainfall.\n\u25cf Generated dynamic plots for Java UI and dashboard to showcase intensity-duration frequency curve using logarithmic regression prediction for rainfall return period.\n\u25cf Translated 10 rainfall models for imputing and validating 50GB of data from R to Java using JRI library for historical and newly uploaded rainfall.\n\u25cf Extracted 10GB of LIDAR data in Java Jzy3D to create 3D visualizations of terrain contours with 10 meter moving average to estimate levee height.', u'Data Analyst\nGEI Consultants - Sacramento, CA\nOctober 2010 to April 2012\n\u25cf Extracted 5000+ files of daily and hourly rainfall data using VBA from 10+ excel formats for rainfall Oracle database.\n\u25cf Programmed data cleaning on extracted rainfall data in VBA to document and impute faulty data with 99% accuracy.\n\u25cf Programmed data transfer, upload, and download for 50GB of clean rainfall data using MySQL hosted on Oracle DB.\n\u25cf Created data pipeline flowchart to document end-to-end processes of extraction, transformation, and uploading data.']","[u'Masters of Science in Civil Engineering', u'Bachelors of Science in Civil Engineering']","[u'University of California - Davis Davis, CA\nSeptember 2009 to September 2010', u'University of California - Davis Davis, CA\nSeptember 2004 to December 2008']","degree_1 : Masters of Science in Civil Engineering, degree_2 :  Bachelors of Science in Civil Engineering"
0,https://resumes.indeed.com/resume/6c34ed1536204d3f,"[u'Data Scientist\nFIU Applied Research Center - Miami, FL\nSeptember 2016 to Present\nUSA\n\u2022 Worked in a cross-functional team to build predictive models for cyber security using large scale data (over 10 G\nstructured & unstructured).\n\u2022 Developed a RNN approach without manual feature engineering, which provided an accuracy rate of 98.6%,\nbeating ML approach (random forest, gradient boosting, etc.) by around 8%.\n\u2022 Implemented end-to-end prediction pipeline and data visualization components in our cyber security system.\n\u2022 Performed R&D on feature engineering, identifying the most important features related to malware & phishing\ndetection.\n\u2022 Techniques used: Python, R, SQL server, Spark, RNN, LSTM, predictive model (Random Forest, Gradient\nBoosting, etc.), NLP (word embedding, n-gram, TF-IDF), feature engineering.', u'Statistical Consultant\nFlorida International University - Miami, FL\nSeptember 2013 to September 2016\nUSA\n\u2022 Explained complex statistical ideas to non-statisticians including professors, staffs and students in our University.\n\u2022 Performed data cleaning, experimental design, hypothesis testing and statistical model building for our clients\nusing R and SPSS.\n\u2022 Techniques used: PCA, Factor Analysis, Logistic Regression, SVM, KNN, Naive Bayes, Probabilistic Graphical\nModel, Linear Model, A/B Test, etc.', u'Mechanical Engineer\nBombardier Transportation Ltd\nJune 2010 to August 2012\nDesigned mechanical models using AutoCAD and SolidWords.']","[u'Master in Computer Science', u'Master in Statistics', u'Bachelor in Vehicle Engineering']","[u'Florida International University Miami, FL\nJanuary 2015 to January 2017', u'Florida International University Miami, FL\nJanuary 2013 to January 2015', u'Southwest Jiaotong University\nJanuary 2006 to January 2010']","degree_1 : Master in Compter Science, degree_2 :  Master in Statistics, degree_3 :  Bachelor in Vehicle Engineering"
0,https://resumes.indeed.com/resume/e26d6e1e7cc8f40c,"[u'Business Intelligence Analyst II\nJP Morgan Chase\nNovember 2017 to Present\n\u2022 Develop analyses to uncover process inefficiencies and provide recommendations to increase productivity\n\u2022 Collaborate with team members to identify patterns and trend in the mortgage banking space\n\u2022 Deliver highly technical ad hoc analyses to business owners to provide deep insights at an actionable level', u'Director of Data Sciences\nMECLABS Institute\nJuly 2015 to October 2017\n\u2022 Drove forward the data and statistical testing methods resulting in more thorough and robust data analyses\n\u2022 Oversaw the launch of an in-house online testing tool complete with state of the art statistics\n\u2022 Trained and developed new data team members guaranteeing high quality work from them\n\u2022 Implemented the scrum process ensuring seamless workflow across teams and impactful project deliverable', u'Data Scientist and Data Manager\nMECLABS Institute\nJuly 2013 to June 2015\n\u2022 Maintained a high level of statistical rigor in the web space delivering trusting analysis\n\u2022 Reviewed and provided feedback to team members resulting in a higher level of output quality\n\u2022 Performed descriptive and predictive data analyses providing deep insights into processes and flow']","[u'Master of Science in ABD', u'Bachelor of Arts in Mathematics Education']","[u'Georgia Southern University\nJanuary 2011 to January 2012', u'Georgia Southern University\nJanuary 2002 to January 2007']","degree_1 : Master of Science in ABD, degree_2 :  Bachelor of Arts in Mathematics Edcation"
0,https://resumes.indeed.com/resume/2afaae6d9a05718b,[u'Data Scientist\nNovember 2011 to Present'],"[u""Bachelor's"", u""Master's""]","[u'UC Berkeley', u'UC Davis']","degree_1 : ""Bachelors"", degree_2 :  ""Masters"""
0,https://resumes.indeed.com/resume/0fd2faff1ab83ac8,"[u'Data Scientist\nCiti - Tampa, FL\nFebruary 2016 to Present\n\u2022 Created data conversion requirements based on existing business requirements\n\u2022 Creating SQL code for the data extraction\n\u2022 Maintained a Python/Django web application\n\u2022 Used SQL Developer to alter/insert/delete entries from sterling database.\n\u2022 Wrote SQL queries to insert, delete, update store tables.\n\u2022 Responsible for preparing the existing SQL platform for upgrades that were installed as soon as they were released. Assisted in creating and presenting informational reports for management based on SQL data.', u'Data Scientist\nHome Depot - Alpharetta, GA\nJune 2006 to January 2016\nWrote a python based SQL generator that helped speed up a weekly reporting from several days\neffort to just running the automation and getting the report done in a few hours.\n\u2022 Maintained SQL scripts to create and populate tables in data warehouse for daily reporting across departments.\n\u2022 Worked on a data set that predicts the length of stay in dentist office based on claim records\n\u2022 Created, coded, tested, modified and installed programs for a host of application\n\u2022 Used SQL language to write queries inside the SQL server database.\n\u2022 Experience with Microsoft servers 2005, 2008, 2012']",[],[],degree_1 : 
0,https://resumes.indeed.com/resume/3606fb0922007082,"[u""Chief Data Scientist\nXO Group, Inc\nJanuary 2013 to Present\nWe build and maintain the performance of several real-time deep-learning recommendation\nAPIs. Our APIs have increased our vendor lead conversion rates 11% on mobile platforms and user engagement as well as click-throughs on theknot.com with similar results. These\nAPIs:\n\u2022 Match theknot.com members with local wedding vendors, using Tensorflow among other neural-net frameworks\n\u2022 Predict users' interests and styles and provide personalized real weddings content\nusing gradient boosting trees\n\u2022 Provide similar vendors using a branch-weighted taxonomy\n\u2022 Perform probabilistic n-gram record linkage of vendor profiles for matching and de- duplication\n\nOur team also does things like Image auto-tagging using collaborative and image recognition\ntechniques like convolutional neural nets and wedding registry product recommendation using\ncollaborative filtering and non-negative matrix factorization. All of our APIs are powered by Python/Flask, employ numpy/sci-kit learn models or custom gradient descent solvers, typically\nuse pandas and get packed into Docker instances, monitored with the Kong API gateway and are provisioned on AWS EC2 instances.\n\nWe have developed machine-learning models and algorithms for entity resolution (fuzzy\nduplicate detection) on large (>500K) mongo collections in the Python environment.\nScalability up to any significant size requires inverted indices, n-gram shingling, predicate\nblocking and frequency pruning or some other statistically sound search space reduction over the brute force (binomial coefficient with k=2) space.\n\nOur team works extensively with various mixture models in the classifier and regression\nsettings and all that implies: model selection, model fitting, proper acquisition and ETL of training data, multi-collinearity detection, manifold learning using tSNE and multi-fold cross-\nvalidation to monitor test MSE and find the proper low-bias/low-variance model.\n\nImage similarity analysis and clustering with stuff like Randomized PCA, Haar cascades,\nCanny edge detection, key point matching, and many other techniques to do things like image\nclustering, wedding design board auto-generation, or general metric space projection for image data. These are the kinds of things that tend to drive reverse image search, for\nexample.\n\nDeveloped description similarity measurements using Python's natural language toolkit as well as some general metrics for various classes of strings.\n\nParallelization with Apache Spark\nI organize and present data science workshops for XO Group. I try to make it accessible for those without a heavy math and stats background. Ipython notebooks available on request."", u""Principal Scientist\nTicom Geomatics, Inc\nJanuary 2011 to January 2013\nDesigner and developer lead of large-scale Red Hat Enterprise Linux C++ software for RF\nsignal collection systems using T/FDOA Geolocation. Principal scientist overseeing sensor\nsoftware design and interfaces team.\n\u25cf Delivered and demoed TGI's first transition-ready National/Tactical wideband power\nspectral density collection and geolocation system. C++ multi-process backend\ncollected I/Q and generated PSD using the sliding-DFT approach. Sub-band tuned\nsnapshots were generated in real-time based on tips from a Digital Receiver\nTechnology, Inc. radio (www.drti.com). These were stored and retrieved using mysql.\nProtocol buffers over sockets were used for command, control, and config, tomcat\nservlet with Ozone Widgets browser components were front-end. Real-time\nprocessing and storage. Received PM recognition for planning and execution of an intrinsically complex problem and feature-set within legacy software and hardware\narchitecture constraints on-time and in budget.\n\u25cf Designed remote sensor diagnostics python/mysql/sqlite3 project under budget on firm\nfixed-price contract.\n\u25cf Developed node.js front-end context switcher to legacy servers running in VBoxHeadless\n\u25cf Provided oversight on development of node.js radio interface.\nPosition required heavy Linux C++/Python development in a highly distributed system, 4- person team management, build management (gmake, cmake, ant, maven), release\nmanagement, branch management, issue tracking using JIRA/Bugzilla, cross-team\ncollaborative work with Java frontend components and team, multi-spiral agile development\napproaches and frequent scrums. Also required periodic interviewing of candidates. Roughly\n25 candidates were pre-screened and interviewed across team leads. Adapted to technical\nchallenges of migrating from small engineering firm to mid-size company."", u'Principal Scientist\nTicom Geomatics, Inc - Austin, TX\nJanuary 2000 to January 2013', u'Senior Scientist II, III\nTicom Geomatics, Inc\nJanuary 2000 to January 2011\nDeveloped both Linux and Windows software for airborne RF SIGINT systems. Primary\narchitect responsible for development of signal data collection framework. Borland C++\nBuilder, RHEL\n\u25cf Designed and incorporated slim-transitional C++ network application framework for migrating legacy codebase to boost.asio. The framework was a dependency breaker\nthat loosened the coupling and increased the cohesion of modules to make such large- scale refactoring possible.\n\u25cf Created Python/wxWidgets front-end and server for tracking system config over multiple clients.\n\u25cf Author of several C++ single-threaded multi-process networked sub-systems and multi- threaded single-process sub-systems (boost.thread, pthreads, select I/O multiplexing,\nboost.asio)\n\u25cf Highly-proficient in C++ boost/stl', u'Developer\nAutogas Systems, Inc\nJanuary 1998 to January 1999\nDeveloped C++ software used in a large-scale POS system deployed on both Windows and OS/2. Primary developer migrating and maintaining embedded Motorola 6809 assembly\ncontroller to C++.', u'Developer\nAutogas Systems, Inc - New Braunfels, TX\nJanuary 1997 to January 1999', u'Technical Support, Tools Developer\nAutogas Systems, Inc\nJanuary 1997 to January 1998\nReclaimed over $250K in lost credit card revenue caused by a firmware defect. Originally\ndone by visual inspection, I scripted a probabilistic record linkage with minimal error solution\nusing high-correlation n-gram matching to validate (card number, amount, timestamp) tuples against known good sets.\nOther Recognition\n\n\u25cf Ticom Geomatics Quality Award\n\u25cf Ticom Geomatics SPOT Award\n\u25cf Two-time recipient of graduate departmental award for academic excellence (4.0 GPA)\n\u25cf Alpha Mu Gamma\n\nGraduate Course Work\n\u25cf Implemented a JVM\n\u25cf Developed an Ethernet simulation - This project alllowed me to create an asynchronous communications library with the vision of replacing our very poor legacy\nframework for Ticom Geomatics. It took several years of long-term technical and political effort due to lack of budget allocation for legacy refactoring, but eventually\nentire codebase was migrated to an event-driven model using boost.asio\n\u25cf Wrote a checkers playing program - I used the Qt libraries with custom widgets for the front end. The engine implemented a well-known AI heuristic search technique:\nminimax with alpha-beta pruning. It offered customization of the search parameters,\nselection of algorithm flavor, and a set of weighted-heuristic static evaluation functions.\n\u25cf Created a cache simulator - Developed UI with C++/Qt for the purpose of creating,\nconfiguring, and visualizing set-associative cache performance using temporal and spatial locality of reference.\n\u25cf Developed a diagnostic expert system - Used forward-backward chaining techniques\nand implemented a rule generation framework using Boost.Lambda.\n\u25cf Designed and presented a novel routing technique for improving transfer reliability over disadvantaged sensor networks.\n\u25cf Developed PHP/MySQL book search engine. Used web-scraping techniques for non- service sites and AWS for amazon.com\n\u25cf Wrote IEEE 730-2002 compliant software QA plan']","[u'MS in Computer Science', u'', u'BS in Computer Science']","[u'Texas State University\nJanuary 2006 to January 2009', u'UCLA Extension - Digital Signal Processing\nJanuary 2001', u'Texas Lutheran University\nJanuary 1995 to January 1999']","degree_1 : MS in Compter Science, degree_2 :  , degree_3 :  BS in Compter Science"
0,https://resumes.indeed.com/resume/35efcb1ecd62ebf4,"[u'Machine Learning Engineer\nEntefy - Palo Alto, CA\nSeptember 2017 to Present\n\u2022 Increasing false positives, decreasing true negatives & false negatives to accommodate spelling correction\n\u2022 Zipf\'s law to check the training data quality and coverage rates to check testing data quality\n\u2022 Implemented Hunspell and improved its performance by additional probabilistic layers\n\u2022 Predictions from Hunspell are re-adjusted based on ranking and bi-gram language models (context)\n\u2022 Fat finger approach (swapping, adding, replacing characters) and deleting random characters to inject artificial noise into the sentences for training and testing.\n\u2022 Implemented n-gram language model to incorporate context to predictions\n\u2022 Implemented Sequence 2 Sequence model (Encoder-Decoder Architecture - Recurrent Neural Networks) to incorporate ""Spelling Correction"" module into company product', u""Data Scientist\nFord Motor Company - Global Data Insights and Analytics (GDIA) - Dearborn, MI\nMarch 2017 to September 2017\nOff-Road Performance Index, Machine Learning:\n\u2022 Built a predictive model for identifying the sections of people suitable and interested in driving rugged off-road vehicles, but didn't purchase\n\u2022 Performed SMOTING and TOMEK Link to handle unbalanced data, row level standardization to handle different scales, Principal Component Variable clustering for dimensionality reduction\n\u2022 Understanding brand choices and segment the market into understandable, homogeneous group with regards to brand by implementing K-means clustering on Personal & Vehicle Attitudes, Values, Domestic/Import etc.\n\u2022 Sentiment Analysis on the global portfolio survey data. Built a Neural Network to identify the customer's positive/negative opinions on features and usage."", u'Data Analyst\nLexisNexis, Reed Elsevier Technology Services, RELX Group - Dayton, OH\nMay 2016 to August 2016\n\u2022 Implemented Lagging 4hr Average instead of 75th percentile for inferences on application traffic data\n\u2022 KPI reports to measure customer traffic behaviour and provide actionable insights\n\u2022 Capacity planning for workloads of 8 mobile applications of LexisNexis', u'Assistant-Analytics\nBowling Green State University - Bowling Green, OH\nAugust 2015 to May 2016\n\u2022 Response model for Home Warranty of American Home Shield & other Analytics at Learning Commons\n\u2022 Developed the predictive model using Logistic Regression. Applied Weight of Evidence (WOE) and Information Value (IV) for dimension reduction within and between attributes.\n\u2022 Missing values bin sale percent is compared with other bins and imputed.', u'Predictive Modelling Analyst\nLodestone - IN\nApril 2015 to August 2015\nIndia\n\u2022 Predictive Modelling on customer response to the marketing campaign and purchasing a policy\n\u2022 Synthetic Minority Over-Sampling Technique (SMOTE) & TOMEK Link to overcome unbalanced data issue by synthesizing new minority class samples and establishing a border between minority & majority classes to clear the confusion and better fit\n\u2022 Implemented Random Forest Algorithm considering Recall values & ability to predict potential customers\n\u2022 Parameter tuning to achieve more ""Recall"" than Accuracy. Trade-off between False Positives and False\nNegatives.', u'Data Scientist\nAISIN Group\nJune 2013 to April 2015\nIndia\n\u2022 Predicting the behaviour of customer in purchase of an automobile by Logistic regression & Random forest.\n\u2022 Business understanding, Data preparation, Modelling & Deployment of this pilot project (CRISP-DM).\n\u2022 Learnt Japanese to work effectively with Japanese teammates and communicate with clients in Japan.', u'Data Analyst\nEqic - IN\nJuly 2012 to November 2012\nIndia\n\u2022 Generated insights using Exploratory Data Analysis to interpret the change in shift causing low productivity.']",[u'Master of Science in Applied Statistics'],[u'Bowling Green State University'],degree_1 : Master of Science in Applied Statistics
0,https://resumes.indeed.com/resume/1253a1b15548c9b6,"[u'Data Scientist\nIndependent Contract - Dallas-Fort Worth, TX\nOctober 2017 to Present\nPerformed exploratory research for Data Science.\nTested innovative potential applications outside of mainstream usage.\nCompared hyperparameters\u2019 tuning and predictive models\u2019 results among different statistical softwares.', u'Data Analyst\nWageWorks - Irving, TX\nJune 2017 to September 2017\nAssessed client and system issues including technical and application quickly and handled with error and exception in order to maintain data quality.\nAnalyzed and monitored data updates to ensure data integrity with system application and used SQL scripts to execute successfully or resolve users\u2019 problems and data patch.\nProcessed (including load/test/process) inbound and outbound files in accordance with established Service Level Agreements (SLA\u2019s).\nTechnology: Microsoft SQL Server Management Studio, Power Query of MS Excel, internal production support system.', u'Data Analyst \u2013 Business System\nKintetsu Global I.T., Inc. - Dallas-Fort Worth, TX\nDecember 2016 to May 2017\nReviewed, analyzed, and evaluated global logistic business systems and user needs.\nTested data mapping with Python for methods and implementations.\nTested reporting functions to make sure SAP Crystal Report output correctly and prepared tested reports for the data verification between new version and old version.\nPerformed PL/SQL scripts like merge, join tables to do debugging to support the enterprise systems and applications in department of system development and production support in large amounts of data.\nCompleted about 100 tickets for production support, tested and processed 30 reports (including multinational technology companies) and 20 new functionality of the new system development and web application.\nResolved users\u2019 problems by dealing with data patch (data correction) and work-around application and providing advanced instruction and documentation.', u'Data Analyst - Compliance\nGenpact - Richardson, TX\nMay 2016 to November 2016\nApplied broad compliance knowledge to develop KYC/AML operations such as data enrichment and data remediation for top 5 banking client.\nPerformed data enrichment by confirming data meets or exceeds all minimum compliance requirements including searching and saving relevant PDF files for fraud prevention by using internal and external databases.\nPerformed data Remediation through research and analysis of required supplementary information and prepared daily Excel reports.\nProcessed about 1500 cases of KYC/AML in total during that time.\nPrepared KYC/AML validation for different data types (geographic data, third-party data and customer information data).', u'Operation Analyst\nDimerco Express (U.S.A.) Corp - Grapevine, TX\nJune 2014 to March 2016\nJuly 2014-May 2015\nOperated air logistics, especially international trades to Asia, including global semiconductor companies. Prepared PDF files from clients to do operations for international shipments.\nApplied cloud concepts to collaborate with all users in a faster and more efficient way and prepared daily Excel reports like Tableau.\nPrepare and tested data requirements for business intelligence and analytics solutions.']","[u'Master of Science in Information Technology and Management', u'Master of Arts in Political Economy', u'Bachelor of Arts in Business Administration']","[u'The University of Texas at Dallas Dallas, TX\nMay 2015', u'National Cheng Kung University\nJanuary 2008', u'National Sun Yat-sen University\nJune 2005']","degree_1 : Master of Science in Information Technology and Management, degree_2 :  Master of Arts in Political Economy, degree_3 :  Bachelor of Arts in Bsiness Administration"
0,https://resumes.indeed.com/resume/4a1a650f8086e54c,"[u""Data Scientist\nWest Corporation - Omaha, NE\nMarch 2017 to Present\nDescription: Lead in database maintenance and data visualization. Conducted end-to-end statistical\ndata analysis and modeling, including querying, model building, forecasting, visualization, and implementation.\n\nResponsibilities:\n\u2022 Performed Data Profiling to learn about behavior with various features such as traffic pattern, location,\ntime, Date and Time etc. Evaluated models using CrossValidation, Logloss function, ROCcurves and used AUC for feature selection.\n\u2022 Detected the near-duplicated news by applying NLP methods(word2vec) and developing machine\nlearning models like label spreading, clustering.\n\u2022 Collected data needs and requirements by Interacting with the other departments.\n\u2022 Used Principal Component Analysis in feature engineering to analyze high dimensional data.\n\u2022 Used clustering technique K-Means to identify outliers and to classify unlabeled data.\n\u2022 Ensured that the model has low False Positive Rate.\n\u2022 Created and designed reports that will use gathered metrics to infer and draw logical conclusions of past and future behavior.\n\u2022 Used MLlib, Spark'sMachinelearning library to build and evaluate different models.\n\u2022 Created Data Quality Scripts using SQL and Hive to validate successful data load and quality of the data. Created various types of data visualizations using Python and Tableau.\n\u2022 Developed MapReduce pipeline for feature extraction using Hive.\n\u2022 Application of various machine learning algorithms and statistical modeling like decisiontrees,\nregressionmodels, neuralnetworks, SVM, clustering to identify Volume using scikit-learn package in python, Matlab.\n\u2022 Performed data analysis by using Hive to retrieve the data from Hadoop cluster, Sql to retrieve data\nfrom Oracle database.\n\u2022 Performed DataCleaning, featuresscaling, featuresengineering using pandas and numpy packages in python.\n\u2022 Implemented rule based expertise system from the results of exploratory analysis and information\ngathered from the people from different departments.\n\u2022 Analyze traffic patterns by calculating autocorrelation with different time lags.\n\u2022 Addressed overfitting by implementing of the algorithm regularization methods like L2 and L1.\n\u2022 Performed Multinomial Logistic Regression, Randomforest, DecisionTree, SVM to classify package\nis going to deliver on time for the new route.\n\u2022 Communicated the results with operations team for taking best decisions.\n\nEnvironment: Impala, Linux, Spark, Tableau Desktop, Python 2.x, CDH5, HDFS, Hadoop 2.3, Hive, SQL\nServer 2012, Microsoft Excel, NLP"", u'Data Scientist\nImanage - Chicago, IL\nOctober 2015 to February 2017\nDescription: Identifying, gathering and analyzing complex multi-dimensional datasets utilizing a variety of tools while parallelly implementing them using SCRUM framework.\n\nResponsibilities:\n\u2022 Conducted campaigns and run real-time trials to determine what works fast and track the impact of different initiatives.\n\u2022 Implemented public segmentation using unsupervised machine learning algorithms by implementing k- means algorithm using Pyspark.\n\u2022 Used Python, R, SQL to create Statistical algorithms involving Multivariate Regression,\nLinearRegression, LogisticRegression, PCA, Random forest models, Decision trees, Support Vector\nMachine for estimating the risks of welfare dependency.\n\u2022 Scheduled the task for weekly updates and running the model in workflow. Automated the entire\nprocess flow in generating the analysis and reports.\n\u2022 Used R and python for Exploratory Data Analysis, A/Btesting, Anova test and Hypothesis test to compare and identify the effectiveness of Creative Campaigns.\n\u2022 Created various types of data visualizations using R, python and Tableau.\n\u2022 Created clusters to classify Control and test groups and conducted group campaigns.\n\u2022 sExplored and Extracted data from source XML in HDFS, preparing data for exploratory analysis\nusing data munging.\n\u2022 Identified and targeted welfare high-risk groups with Machinelearningalgorithms. Leveraged Twitter\nAPI and Google Natural language processing API to work on large amount of data and build our\ncomparison dataset.\nEnvironment: NLP, Pig, Hive, Linux, R 3.x, HDFS, Hadoop 2.3, SQL Server,Pypark, R-Studio, Tableau 10,\nMs Excel.', u'Data Scientist/Data Analyst\nAssurant Specialty Property - Santa Ana, CA\nDecember 2014 to September 2015\nDescription: Assurant partners with leaders in mortgage lending, manufactured housing, multifamily housing\nand other industries to protect client and consumer property. Used python data structures for data sampling and validation.\n\nResponsibilities:\n\u2022 Developed Python scripts to automate data sampling process. Ensured the data integrity by checking for completeness, duplication, accuracy, and consistency\n\u2022 Worked on model selection based on confusion matrices, minimized the TypeII error\n\u2022 Generated data analysis reports using Matplotlib, Tableau, successfully delivered and presented the results for C-level decision makers\n\u2022 Worked on data cleaning and reshaping, generatedsegmented subsets using Numpy and Pandas in Python\n\u2022 Continuously collected business requirements during the whole projectlifecycle.\n\u2022 Generated cost-benefit analysis to quantify the model implementation comparing with the former\nsituation.\n\u2022 Identified the variables that significantly affect the target\n\u2022 Applied various machine learning algorithms and statistical modeling like decision tree, logistic\nregression, GradientBoostingMachine to build predictive model using scikit-learn package in Python\n\u2022 Conducted model optimization and comparison using stepwise function based on AIC value\n\u2022 Wrote and optimized complex SQL queries involving multiple joins and advanced analytical functions\nto perform data extraction and merging from large volumes of historical data stored in Oracle 11g,\nvalidating the ETL processed data in target database\n\nEnvironment: Numpy, Pandas, Tableau 7, Python 2.6.8, Matplotlib, Oracle 10g, SQL,Scikit-Learn, MongoDB,', u""Data Architect/Data Modeler\nPitney Bowes Inc - Stamford, CT\nNovember 2013 to November 2014\nDescription: PITNEY BOWES (PB) is the manufacturer of copiers, faxes and other office automation. Pitney\nBowes's operations are aligned under three lines of business, Global Mailing Systems (GMS) - It is the company's core mail automation and shipping business. Extracting data and making them refined for transfer and load while managing big data are some of my tasks.\n\nResponsibilities:\n\u2022 Develop Integrations jobs to transfer data from source system to Hadoop.\n\u2022 Installation of TalendStudio.\n\u2022 Technical design documents for Transformation processes.\n\u2022 Application of business rules on the data being transferred.\n\u2022 Task allocation for the ETL and Reporting team.\n\u2022 Communicate effectively with client and their internal development team to deliver product functionality\nrequirements.\n\u2022 Architecting and design of data warehouse ETL processes.\n\u2022 Demo of POC built for the prospective customer and provide guidance and gather the feedback to backend ETL testing on SQLServer 2008 using SSIS.\n\u2022 Create Integration Jobs to backup a copy of data in network file system.\n\u2022 Design and implement the ETL Data model and create staging, source and Target tables in SQL\n\u2022 server database.\n\u2022 Gathering and analysis requirements definition meetings with business users and document meeting\noutcomes.\n\nEnvironment: Hadoop, MS Office,Talend Studio, ETL, ODS, OLAP ,SQL Server 2008."", u'Data Analyst/Data Modeler\nNestle - IN\nFebruary 2011 to October 2013\nDescription: The Nestle is a Swiss transnational food and drink company. Building and maintaining the company website while parallelly visualizing the data using Tableau.\n\nResponsibilities:\n\u2022 Implemented a job which leads an electronic medical record, extract data into OracleDatabase and generate an output. Analyze the data and provide the insights about the customers using Tableau.\n\u2022 Designed, implemented and automated modeling and analysis procedures on existing and experimentally created data.\n\u2022 Created dynamic linear models to perform trend analysis on customer transactional data in Python.\n\u2022 Increased pace & confidence of learning algorithm by combining state of the art technology and statistical methods.\n\u2022 Parseddata, producing concise conclusions from rawdata in a clean, well-structured and easily\nmaintainable format. Developed clustering models for customer segmentation using Python.\n\u2022 Developed entire frontend and backend modules using Python on Django Web Framework.\n\u2022 Implemented the presentation layer with HTML, CSS and JavaScript.\n\u2022 Involved in writing stored procedures using Oracle.\n\u2022 Optimized the database queries to improve the performance.\n\u2022 Designed and developed data management system using Oracle..\n\nEnvironment: Python 2.x, Tableau, Oracle, MySQL 5.x, ORACLE, HTML5, CSS3, JavaScript, Shell, Linux & Windows, Django.', u'Data Analyst\nSyntel - Pune, Maharashtra\nJuly 2009 to January 2011\nDescription: My main role is to analyze data, check their functionalities and build the requirements with in the given time frame.\n\nResponsibilities:\n\u2022 Applied Business Objects best practices during development with a strong focus on reusability and better\nperformance.\n\u2022 Developed and executed load scripts using Teradata client utilities MULTILOAD, FASTLOAD and BTEQ.\n\u2022 Responsible for development and testing of conversion programs for importing Data from text files into map Oracle Database utilizing PERL shell scripts &SQL*Loader.\n\u2022 Developed Tableauvisualizations and dashboards using Tableau Desktop.\n\u2022 Used Graphical Entity-Relationship Diagramming to create new database design via easy to use,\ngraphical interface.\n\u2022 Formatting the data sets read into SAS by using Format statement in the data step as well as Proc\nFormat.\n\u2022 Worked with the ETL team to document the Transformation Rules for DataMigration from OLTP to Warehouse Environment for reporting purposes.\n\u2022 Used GraphicalEntity-Relationship Diagramming to create new database design via easy to use,\ngraphical interface.\n\u2022 Co-ordinate with various business users, stakeholders and SME to get Functional expertise, design and business test scenarios review, UAT participation and validation of financial data.\n\u2022 Responsible for development and testing of conversion programs for importing Data from text files into map OracleDatabase,utilizing,PERL,shellscripts&SQL*Loader.\n\nEnvironment: Business Objects, Oracle SQL Developer, PL/SQL, MS SQL Server, TOAD, Tableau,\nInformatica, SQL*PLUS, SQL*LOADER, XML.']","[u""Bachelor's""]",[u''],"degree_1 : ""Bachelors"""
0,https://resumes.indeed.com/resume/1e6d7bdb67e2b673,"[u'Principal Data Scientist\nPilot Flying J - Knoxville, TN\nJanuary 2014 to January 2018\nCreated several business solutions , Managed a\nteam of data scientist and cross-functional teams,\ndeveloped guest segmentation, behavioral\nattributes, provided posts test analysis,represented\nthe business intelligence department to\nexecutives, including present strategic products,\ndomain expert in creating Innovative visualizations\ntelling the story, expert in UE and meaningful\nreports leading business to the answers, lead the\ndevelopment of a new loyalty program saving 35\nmillion,including engineering the incorporation of\nCRM tools such as Salesforce, KRUX, and guest\ndecision trees.']","[u'Masters Intelligence Studies in Intelligence minor in Counterintelligence Analytics', u""Master's""]","[u'American Military University West Virginia\nJanuary 2012 to December 2014', u'']","degree_1 : Masters Intelligence Stdies in Intelligence minor in Conterintelligence Analytics, degree_2 :  ""Masters"""
0,https://resumes.indeed.com/resume/67daa1d598d26237,"[u'Data Scientist/ Machine Learning\nApple, Inc - Cupertino, CA\nFebruary 2017 to Present\nDescription:\nResponsibilities:\n\u2022 Built models using Statistical techniques like Bayesian HMM and Machine Learning classification models like XG Boost, SVM, and Random Forest.\n\u2022 A highly immersive Data Science program involving Data Manipulation & Visualization, Web Scraping, Machine Learning, Python programming, SQL, GIT, Unix Commands, NoSQL, MongoDB, Hadoop.\n\u2022 Designing and develop Tableau Reports, Documents, Dashboards for specified requirements and timelines.\n\u2022 Setup storage and data analysis tools in Amazon Web Services cloud computing infrastructure.\n\u2022 Used pandas, numpy, seaborn, scipy, matplotlib, scikit-learn, NLTK in Python for developing various machine learning algorithms.\n\u2022 Installed and used Caffe Deep Learning Framework\n\u2022 Worked on different data formats such as JSON, XML and performed machine learning algorithms in Python.\n\u2022 Worked as Data Architects and IT Architects to understand the movement of data and its storage and ER Studio 9.7.\n\u2022 Purchasing, Setting up and configuring a Tableau Server and MS-SQL 2008 R2 server for Data warehouse purpose.\n\u2022 Preparing Dashboards using calculations, parameters in Tableau.\n\u2022 Designed, developed and implemented Tableau Business Intelligence reports.\n\u2022 Participated in all phases of data mining; data collection, data cleaning, developing models, validation, visualization and performed Gap analysis.\n\u2022 Data Manipulation and Aggregation from different source using Nexus, Toad, Business Objects, Power BI and Smart View.\n\u2022 Implemented Agile Methodology for building an internal application.\n\u2022 Focus on integration overlap and Informatica newer commitment to MDM with the acquisition of Identity Systems.\n\u2022 Good knowledge of Hadoop Architecture and various components such as HDFS, JobTracker, Task Tracker, NameNode, DataNode, Secondary NameNode, and MapReduce concepts.\n\u2022 As Architect delivered various complex OLAP databases/cubes, scorecards, dashboards and reports.\n\u2022 Programmed a utility in Python that used multiple packages (scipy, numpy, pandas)\n\u2022 Implemented Classification using supervised algorithms like Logistic Regression, Decision trees, KNN, Naive Bayes.\n\u2022 Used Teradata15 utilities such as Fast Export, MLOAD for handling various tasks data migration/ETL from OLTP Source Systems to OLAP Target Systems\n\u2022 Experience in Hadoop ecosystem components like Hadoop MapReduce, HDFS, HBase, Oozie, Hive, Sqoop, Pig, Flume including their installation and configuration.\n\u2022 Updated Python scripts to match training data with our database stored in AWS Cloud Search, so that we would be able to assign each document a response label for further classification.\n\u2022 Data transformation from various resources, data organization, features extraction from raw and stored.\n\u2022 Validated the machine learning classifiers using ROC Curves and Lift Charts.\n\u2022 Extracted data from HDFS and prepared data for exploratory analysis using data munging.\nEnvironment: ER Studio 9.7, Tableau 9.03, AWS, Teradata 15, MDM, GIT, Unix, Python 3.5.2, , Machine learning, MLLib, SAS, regression, logistic regression, Hadoop, NoSQL, Teradata, OLTP, random forest, OLAP, HDFS, ODS, NLTK, SVM, JSON, XML, MapReduce.', u""Data Architecture/Data Modeler\nHewlett Packard Enterprise - Palo Alto, CA\nDecember 2015 to January 2017\nDescription:\nResponsibilities:\n\u2022 Coded R functions to interface with Caffe Deep Learning Framework.\n\u2022 Working in Amazon Web Services cloud computing environment\n\u2022 Used Tableau to automatically generate reports, Worked with partially adjudicated insurance flat files, internal records, 3rd party data sources, JSON, XML and more.\n\u2022 Interacting with business stake holders, gathering requirements and managing the delivery, covering the entire Tableau development life cycle.\n\u2022 Created BI interactive Dashboards and submitting to the server using Tableau Publisher.\n\u2022 Worked with several R packages including knitr, dplyr, SparkR, CausalInfer, spacetime.\n\u2022 Implemented end-to-end systems for Data Analytics, Data Automation and integrated with custom visualization tools using R, Mahout, Hadoop and MongoDB.\n\u2022 Gathering all the data that is required from multiple data sources and creating datasets that will be used in analysis.\n\u2022 Performed Exploratory Data Analysis and Data Visualizations using R, and Tableau.\n\u2022 Perform a proper EDA, Univariate and bi-variate analysis to understand the intrinsic effect/combined effects.\n\u2022 Worked with Data governance, Data quality, data lineage, Data architect to design various models and processes.\n\u2022 Independently coded new programs and designed Tables to load and test the program effectively for the given POC's using with Big Data/Hadoop.\n\u2022 Designed data models and data flow diagrams using Erwin and MS Visio.\n\u2022 As an Architect implemented MDM hub to provide clean, consistent data for a SOA implementation.\n\u2022 Developed, Implemented & Maintained the Conceptual, Logical & Physical Data Models using Erwin for Forward/Reverse Engineered Databases.\n\u2022 Established Data architecture strategy, best practices, standards, and roadmaps.\n\u2022 Lead the development and presentation of a data analytics data-hub prototype with the help of the other members of the emerging solutions team\n\u2022 Performed data cleaning and imputation of missing values using R.\n\u2022 Worked with Hadoop eco system covering HDFS, HBase, YARN and Map Reduce\n\u2022 Take up ad-hoc requests based on different departments and locations\n\u2022 Used Hive to store the data and perform data cleaning steps for huge datasets.\n\u2022 Created dash boards and visualization on regular basis using ggplot2 and Tableau\n\u2022 Creating customized business reports and sharing insights to the management\n\u2022 Worked with BTEQ to submit SQL statements, import and export data, and generate reports in Teradata.\n\u2022 Interacted with the other departments to understand and identify data needs and requirements and work with other members of the IT organization to deliver data visualization and reporting solutions to address those needs.\nEnvironment: R 3.0, Erwin 9.5, Tableau 8.0, MDM, QlikView, MLLib, PL/SQL, HDFS, Teradata 14.1, JSON, HADOOP (HDFS), MapReduce, PIG, Spark, R Studio, MAHOUT, JAVA, HIVE, AWS."", u'Java Developer\nEagle Trading Systems - Princeton, NJ\nNovember 2012 to January 2014\nDescription: Eagle Trading Systems Inc. is a financial investment advisory firm headquartered in Princeton, New Jersey. The firm manages 5 accounts totaling an estimated $481 Million of assets under management.\n\nResponsibilities:\n\u2022 Designed & developed the application using Spring Framework\n\u2022 Developed class diagrams, sequence and use case diagrams using UML Rational Rose.\n\u2022 Designed the application with reusable J2EE design patterns\n\u2022 Developed test cases for Unit testing using JUnit and performed integration and system testing\n\u2022 Involved in coding for the presentation layer using Struts Framework, JSP, AJAX, XML, XSLT and JavaScript\n\u2022 Closely worked and supported the creation of database schema objects (tables, stored procedures, and triggers) using Oracle SQL.\n\u2022 Designed DAO objects for accessing RDBMS\n\u2022 Designed & developed Data Transfer Objects to carry the data between different layers\n\u2022 Developed web pages using JSP, HTML, DHTML and JSTL\n\u2022 Designed and developed a web-based client using Servlets, JSP, Tag Libraries, JavaScript, HTML and XML using Struts Framework.\n\u2022 Developed views and controllers for client and manager modules using Spring MVC and Spring Core.\n\u2022 Used Spring Security for securing the web tier Access.\n\u2022 Business logic is implemented using Hibernate.\n\u2022 Developed and modified database objects as per the requirements.\n\u2022 Involved in Unit integration, bug fixing, acceptance testing with test cases, Code reviews.\n\u2022 Interaction with customers and identified System Requirements and developed Software Requirement Specifications.\n\u2022 Implemented Java design patterns wherever required.\n\u2022 Implemented Multi-threading concepts.\n\nEnvironment: Java, PL/SQL, SQL, HTML, CSS, Java Script, hibernate, Middleware Technologies, Ajax, Servlets, JSP, Web logic, JBoss, WebSphere, XML, XHTML, Eclipse, JMS, Oracle11g, EJB.', u'Java Developer\nFirst Indian Corporation Private Limited - Hyderabad, Telangana\nFebruary 2011 to October 2012\nDescription: First Indian Corporation Private Limited, a member of The First American family of companies, provides specialized offshore transaction services, technology services and analytics to the mortgage industry.\n\nResponsibilities:\n\u2022 Participating in system design, planning, estimation, and implementation.\n\u2022 Involved in developing Use case diagrams, Class diagrams, Sequence diagrams and process flow diagrams for the modules using UML and Rational Rose.\n\u2022 Developed the presentation layer using JSP, AJAX, HTML, XHTML, CSS and client validations using JavaScript.\n\u2022 Developed and implemented the MVC Architectural Pattern using Spring Framework.\n\u2022 Effective usage of J2EE Design Patterns Namely Session Facade, Factory Method, Command, and Singleton to develop various base framework components in the application.\n\u2022 Developed various EJBs (session and entity beans) for handling business logic.\n\u2022 Developed Session Beans and DAO classes for Accounts and other Modules.\n\u2022 Worked on generating the web services classes by using WSDL, UDDI, and SOAP.\n\u2022 Consumed Web Services using WSDL, SOAP, and UDDI from the third party for authorizing payments to/from customers.\n\u2022 Designed and developed systems based on JEE specifications and used Spring Framework with MVC architecture.\n\u2022 Used Spring Roo Framework Design/Enterprise Integration patterns and REST architecture compliance for design and development of applications.\n\u2022 Involved in the application development using Spring Core, Spring Roo, Spring JEE, Spring Aspects modules and Java web-based technologies such as Web Service (REST /SOA /micro services) including micro services implementations and Hibernate ORM.\n\u2022 Used LDAP and Microsoft active directory series for authorization and authentication services.\n\u2022 Implemented different design patterns such as singleton, Session Fa\xe7ade, Factory, and MVC design patterns such as Business delegate, session fa\xe7ade and DAO design patterns.\n\u2022 Used JPA - Object Mapping for the backend data persistence.\n\nEnvironment: R 3.0, Erwin 9.5, Tableau 8.0, MDM, QlikView, MLLib, PL/SQL, HDFS, Teradata 14.1, JSON, HADOOP (HDFS), MapReduce, PIG, Spark, R Studio, MAHOUT, JAVA, HIVE, AWS.']","[u""Master's""]",[u''],"degree_1 : ""Masters"""
0,https://resumes.indeed.com/resume/b776465705d23fa0,"[u""Data Scientist\nSykes Enterprises, FL\nDecember 2017 to Present\n\u2022 Planning strategic workforce scaling by implementing employee attrition (Microsoft Azure, SQL, and R) predictive model; recommend analytical strategies on retaining best employees\n\u2022 Built and deployed a recommender system on Microsoft Azure to reduce agent's average handle time of issue by 16%\n\u2022 Predicting type of customer service request by applying deep learning techniques on call center telemetric data to reduce an employee's average request handling time when assisting customers"", u""Data Science Consultant\nLimra International Insurance\nJuly 2017 to December 2017\n\u2022 Identified trends that affect length of service(LOS) of agents and estimated LOS using Linear regression; suggested business proposals to improve LOS of insurance agents\n\u2022 Drawn insights about agents that drive high premium collections and designed analytical models to predict premium collected by agents of various insurance organizations\n\u2022 Constructed employee churn model on insurance agents' data; Identified characteristics of good agents; proposed business ideas to alleviate effect of new rule from Department of Labor(DOL)."", u'Data Analyst\nIBM - IN\nJanuary 2015 to December 2016\n\u2022 Created classification model (Logistic Regression, SQL, and R) with 90% accuracy to analyze customer complaints on incorrect orders and performed Root Cause Analysis; reduced incorrect orders by 75%\n\u2022 Delivered Financial Solution to fix incorrect monthly bill cycle and correctly billed approximately $1 million towards client and won recognition from Subject Matter Expert\n\u2022 Designed interactive dashboards in Tableau to track Key Performance Indicators (KPIs) that assist in meeting service level agreement; proposed business strategies leading to increase of team performances by 15%\n\u2022 Extracted data(SQL), prepared reports and identified performance hindering metrics; fall in percent of aged incidents by 60% in 4 months']",[u'in Business'],[u'University of Connecticut School of Business\nMay 2018'],degree_1 : in Bsiness
0,https://resumes.indeed.com/resume/cfa9627a4b73c08d,"[u'Data Scientist\nJornaya\nJanuary 2017 to Present\nPerform R&D for new products and product enhancements\nAutomate the building and management of predictive models\nAid the customer success and sales teams with ad-hoc client analysis', u'Data Analyst\nJornaya\nJanuary 2016 to January 2017\nPerform ad-hoc analysis for proof of concepts for new products as well as internal tools\nBuild predictive models combining multiple data sources in order to help clients better manage their marketing lead strategies']",[u'Bachelor of Science in Actuarial Mathematics'],"[u'The University of Pittsburgh Pittsburgh, PA\nApril 2015']",degree_1 : Bachelor of Science in Actarial Mathematics
0,https://resumes.indeed.com/resume/d9d03ffc138433b8,"[u'Business Analyst\nESIS\nJanuary 2018 to Present\n\u2022 Developing a predictive claim analysis model that outputs severity score\n\u2022 Improve claim handling activities and reduce cost by working on factors causing severity', u'Associate Data Scientist\nUnitedHealth Group - Noida, Uttar Pradesh\nJune 2015 to July 2017\nIndia\n\u2022 Built a Predictive Model using Logistic Regression and Gradient Boosting to recognize claims having high risk of appeals filing and reprocessing them to save the cost of appeal processing by $200k\n\u2022 Identified the bottlenecks in appeals processing using mining tools like ProM and Celonis to help make the process more efficient\n\u2022 Built inventory prioritization tool from the identified areas causing delay in claim processing which increased the timely processing of claims from 96.2% to 97.1%\n\u2022 Aggregated all the new data sources and automated the complete claim lifecycle to be used directly for process mining\n\u2022 Collaborated with a cross functional team to effectively establish a single claim database for analytical purposes that substantially increased efficiency\n\u2022 Identified erroneous cases through segmentation (CART) and communicated it to operations team to help prevent rework']","[u'Master of Science in Business Analytics in MSBA', u'in Electronics']","[u'W. P. Carey School of Business at Arizona State University Tempe, AZ\nMay 2018', u'Delhi Technological University Delhi, Delhi\nMay 2015']","degree_1 : Master of Science in Bsiness Analytics in MSBA, degree_2 :  in Electronics"
0,https://resumes.indeed.com/resume/d377556bdf404e83,"[u'Quality Assurance Analyst\nRideCharge/TaxiMagic\nApril 2014 to October 2014\nMonitored and directed development of the iOS/Android Apps: TaxiMagic, Curb, and Sedan Mag- ic.\n\u2022 Applied Tsallis entropy models to optimize client-server queuing in mobile software for ground trans- portation.\n\u2022 Contributed strategies and workflow processes to the QA team.\n\u2022 Advised Software Engineers on code structure and mathematical methodology.\n\u2022 Maintained Regression and Functional testing of a Ruby on Rails server and MySQL database.\n\u2022 Debugged .apk files with command line tools for Android.\n\u2022 Automated testing for iOS apps with Ruby scripts in Appium.', u'ical Chemistry\nJanuary 2014 to January 2014\n2014): 669-679.\n\nSnyder, Greg, et al. ""Development of microbial-derived inhibitory peptides using structural studies of mi- crobial TIR proteins TcpB, TcpC and host adapters TIRAP and MyD88.(INM9P. 449)."" The Journal of Im-\nmunology 192.1 Supplement (2014): 189-2.\n\nCoursework\n\u2022 Bioinformatics - used Perl to analyze amino acid and nucleotide sequences in conjunction with alignment based search tools and NCBI databases\n\u2022 Statistical Computing - Monte Carlo integration, Statistical techniques such as the Bootstrap and Jackknife, programmed with R\n\u2022 Vector Calculus- applied Matlab for multivariate optimization by methods such as\nLagrange Multipliers and operators on partial differential equations.\n\u2022 Linear algebra- covered abstract algebra, state models, symmetry groups,\ntopology, and bitmaps. Thoroughly explored the biological applications in popu- lation genetics and molecular spectroscopy.\n\u2022 Discrete Math- Combinatorial enumeration, Formal Logic, Graph theory\n\u2022Computer Science- Object oriented java, UML, Ant/xml, Eclipse, JUnit. \u2022Compu- tational Chemistry -fourier analysis with matlab, Java Gui application for molecular\nmechanics, Markov Chain simulation of nucleic acid folding.', u'Data Mining Engineer\nXerox/PARC- DARPA\nAugust 2012 to March 2013\nHelped to maintain code and implement algorithms mathematically.\n\u2022Applied Maximum Entropy Methods to Scoring Functions for Feature Detection in Sentiments Analysis\n(preprocessing of sentiment features)\n\u2022Debugging SQL, java, python, and R; with linux shell scripts.\n\u2022Writing MySQL queries for Big Data Extraction, Transformation, and Loading.\n\u2022Assessing the sensitivity of output data with respect to privacy issues.\n\u2022Preparing documentation of program functionality and runtime conditions\n\u2022Uploading results with SVN, via SCP.\n\nCredited in the final paragraph of this paper:\n\nPatil, Akshay, et al. ""Modeling Attrition in Organizations from Email Communication."" Social Computing\n(SocialCom), 2013 International Conference on. IEEE, 2013.\n\nNational Institutes of Health, National Institute of Allergy and Infectious Disease', u'Undergraduate Research Scientist\nNovember 2009 to October 2010\nInvestigated Integral Membrane Protein Structure and Immune System Signal Transduction.\n\u2022 Cultured and transfected E.Coli to produce and purify immune signaling proteins TIRAP and MYD88\n\u2022 Designed and implemented assays for the discovery of protein crystals.\n\u2022 Assisted in the analysis and interpretation of protein structural biochemistry based on data from X-ray\nspectroscopy\n\u2022 Modeled protein structures against homologs using python scripts in a Unix environment\n\u2022 Designed and transfected vectors for yeast hybrid assays\n\u2022 Managed Laboratory Data with Excel\n\nListed as a contributing author on these papers:\n\nSnyder, Greg A., et al. ""Crystal Structures of the Toll/Interleukin-1 Receptor (TIR) Domains from the Bru- cella Protein TcpB and Host Adaptor TIRAP Reveal Mechanisms of Molecular Mimicry."" Journal of Biolog-']","[u'B.S.', u'']","[u'University of Maryland Baltimore County Baltimore, MD\nJanuary 2014', u'Montgomery College Maryland Transfer\nFebruary 2011']","degree_1 : B.S., degree_2 :  "
0,https://resumes.indeed.com/resume/e819c4e246b943a5,"[u'Data Scientist of team\nUHAI\nJanuary 2017 to Present\nBuilt deep learning algorithm to predict breast cancer from mammogram images Switzerland and USA', u'Data Scientist Intern\nE\xf6tv\xf6s Lor\xe1nd University - Budapest, HU\nJune 2016 to August 2016\nHungary\n\u2022 Built the theoretical foundation of new research projects by detecting patterns in times series graph dataset\n\u2022 Proved several evolution features of hierarchical graphs by building a classification model on imbalanced data', u'Junior Data Scientist\nLynx Analytics - Hong Kong, HK\nOctober 2015 to December 2015\n\u2022 Increased project team size from 4 to 30 by converting a pilot project to a long-term business deal\n\u2022 Presented the insights and recommendations directly to the CEO and the Executive Board of the client', u'Data Scientist Intern\nLynx Analytics - Singapore\nMay 2015 to October 2015\n\u2022 Developed an interactive visualization tool for analyzing the SMS-Voice-Data usage of the customers.\n\u2022 Analytical and programming support for consultant teams in Indonesia, Philippines, Singapore\n\u2022 Drove revenue increase of 9% by creating a price elasticity model and pricing strategy for a mobile package', u'Support Intern\nMicrosoft - Budapest, HU\nJuly 2013 to August 2013\nHungary\n\u2022 Drove productivity increase of 20% by providing MS Dynamics software support to client companies\n\nAnalytics projects']","[u'MS in Analytics in Business Analytics Track', u'Bachelor of Electrical Engineering in Electrical Engineering', u'in Science & Technology Track', u'in Electrical Engineering']","[u'Georgia Institute of Technology Atlanta, GA\nAugust 2017 to August 2018', u'Budapest University of Technology and Economics Budapest, HU\nJanuary 2012 to January 2017', u'Delft University of Technology Delft, MN\nJanuary 2016', u'National University of Singapore\nJanuary 2015']","degree_1 : MS in Analytics in Bsiness Analytics Track, degree_2 :  Bachelor of Electrical Engineering in Electrical Engineering, degree_3 :  in Science & Technology Track, degree_4 :  in Electrical Engineering"
0,https://resumes.indeed.com/resume/8a82b75ccd46f155,"[u'Graduate Data Assistant\nPurdue University - West Lafayette, IN\nSeptember 2016 to Present\nIMA provides graduate student information and analytical support for departmental and University-wide data requests and reporting\n\u2022 Compiling, integrating, and analyzing graduate student data to meet reporting needs by building queries using SQL\nand SAS for preparation of accurate and meaningful reports within assigned deadlines\n\u2022 Responding efficiently and accurately to internal / external requests and surveys and to help streamline and improve data collection/reporting using Tableau', u'Senior Associate\nDelhivery Pvt Ltd - Gurgaon, Haryana\nAugust 2015 to June 2016\nDelhivery is a leading logistics provider in India operating in over 175 cities Line Haul Network Design\n\u2022 Automated preparation and Data Cleaning required for filling State tax forms in 28 States across country using VBA\n\u2022 Carried out What-If Analysis to identify operations and processes prone to failure in case of network changes\nDeveloped strategies to plan for extra storage, manpower and fleet requirements in case of increased demand\n\u2022 Achieved 10% increase in Service level and 15% increase in Vehicle Utilization resulting in Operational Cost Savings', u'Trainee Decision Scientist\nMu Sigma Consulting Pvt. Ltd - Bengaluru, Karnataka\nAugust 2014 to August 2015\nMu Sigma is a Chicago based sales and technology consulting company\n\u2022 Led a team of 3 and engaged directly with marketing team in US to create sales plan for Consumer and Business segment undergoing business changes\n\u2022 Developed statistical Time Series forecasting model(ARIMAX) and Machine learning based regression models in SAS and worked with client to optimally utilize their Call Centre resources resulting in estimated savings at $300k annually']","[u'Master of Science in Industrial Engineering in Industrial Engineering', u'Bachelors of Technology in Production and Industrial Engineering']","[u'Purdue University West Lafayette, IN\nAugust 2016 to May 2018', u'Indian Institute of Technology Roorkee Roorkee, Uttarakhand\nMay 2014']","degree_1 : Master of Science in Indstrial Engineering in Indstrial Engineering, degree_2 :  Bachelors of Technology in Prodction and Indstrial Engineering"
0,https://resumes.indeed.com/resume/8ecbaedc34becaaa,"[u'DATA SCIENTIST INTERN\nProLytics LLC - Charlotte, NC\nJanuary 2018 to Present\n\u2022 Perform advanced statistical analysis and predictive analytics onMLB, NBA Draft data (3 years of NBA data) and NCAA college stats\n\u2022 Develop Long short term memory(LSTM) recurring neural network to predict the best players to be drafted for an upcoming game\n\u2022 Develop Machine Learning algorithms to predict the players and their match up analysis based on their position, historical nba data\nTechnologies: Python, Jupyter Notebook, H2O, XGBoost, LSTM RNN, Cosine Similarity, Classification Models', u'DATABASE DEVELOPER\nAccenture - Hyderabad, Telangana\nMarch 2016 to December 2016\n\u2022 Worked as an Associate Software Engineer with Global Resource Management project for client: Microsoft with Agile methodology\n\u2022 Modified the web test scripts according to the API changes and created pipelines in Azure Data Factory(ADF), SQL Jobs\n\u2022 Executed constant/load tests in azure which included performance monitoring, performance test analysis, performance tuning\nTechnologies: SQL Server Management Studio, Microsoft Visual Studio, Microsoft Azure, C#', u'SYSTEMS ENGINEER TRAINEE\nInfosys Limited - Mysore, Karnataka\nJune 2015 to November 2015\n\u2022 Trained on PYTHON, JAVA and web technologies like HTML, CSS3, JavaScript, D3.js\n\u2022 Trained and developed a SQL Database system for an internal Business Enterprise Application\nTechnologies: Python, Oracle SQL, Java']","[u'Master of Science', u'Bachelor of Engineering in Electronics and Communications']","[u'University of North Carolina at Charlotte Charlotte, NC\nJanuary 2017 to May 2018', u'Osmania University Hyderabad, Telangana\nAugust 2011 to May 2015']","degree_1 : Master of Science, degree_2 :  Bachelor of Engineering in Electronics and Commnications"
0,https://resumes.indeed.com/resume/21f1c7d7c47b481e,"[u'Principal Data Scientist\nEos Energy Storage, LLC\nFebruary 2015 to Present\nDesign and develop new algorithms for data analysis and predictions based on machine learning and statistical; application to battery performance data.\n\u25e6 Drive the collection of new data to improve existing methodologies\n\u25e6 Communicate results and analysis to business and technical teams\n\u25e6 Work cross-teams on daily basis to improve analysis tools and provide feedbacks to improve processes.\n\u25e6 Involved in the design and development of battery management system.\n\u25e6 Planning and execution of software development using agile life cycle.\n\u25e6 Writing specifications and requirement for the battery management system.\n\nProgramming tools: Python, C++, C#, Source Control, Sql, NoSql, Amazon cloud, and Matlab.', u'Principal Scientist\nAMETEK, Inc\nSeptember 2012 to February 2015\nDesign and develop automatic defects inspection algorithms using large data set able to discriminate between tire defects. This software uses sheet of light profiles of the geometry of the tire data.\n\u25e6 Participate in the design and development of new laser 3-D sensors.\n\u25e6 Drive data collection to improve existing tools\n\u25e6 Planning and execution of software development using agile life cycle\n\u25e6 Develop new concepts that lead to new commercial products\n\u25e6 Participate in pre and post-sales meetings to communicate and present the new developed software tools.\n\nProgramming tools: C++, C#, Source Control, CPU, GPU, Matlab, and Python.', u""Image Analysis Consultant\nGE Healthcare JV\nJuly 2011 to September 2012\nDesign and develop computer-aided tool for the automatic detection, classification, and diagnosis of cancerous diseases found in pathological images integrated into Omnyx's pathology system.\n\u25e6 Drive the data collection to test and improve the limitations of existing\n\u25e6 Participate in code review and design documentation review\n\u25e6 Participate in writing test cases for testing and validation of the software\n\u25e6 Release Planning and execution using agile software life cycle\n\u25e6 Communicate and present results to customers\n\nProgramming tools: C++, C#, f#, Source Control, CPU, GPU, and Matlab."", u'Senior Biomedical Imaging Specialist\nOhio State Medical Center\nJanuary 2008 to June 2011\nDesign and develop new image analysis, classification and pattern recognition techniques to create new computer-aided tools the diagnosis and prognostic of cancerous diseases such follicular lymphoma, breast, and prostate in histopathology images.\n\u25e6 Design and Develop new image analysis, classification and pattern recognition techniques for the diagnosis and monitoring of the Osteoarthritis Arthris disease in MRI images\n\u25e6 Design and develop Computer Aided-Diagnosis tool for the detection and classification of Cutaneous Lymphoma skin disease.\n\u25e6 Design and Develop new classification and pattern recognition techniques to track the growth of breast cancerous cells in volumetric images obtained from confocal microscopy.\n\nProgramming tools: C++, C#, Source Control, and Matlab.', u'Senior Research Scientist\nKestrel Corporation\nFebruary 2005 to December 2007\nPrincipal investigator in several biomedical research and development projects supported by SBIR programs, of interest:\n* Modeling, design, and develop new computer-aided detection (CAD)\nSoftware; screening tools for diabetic retinopathy.\n* Modeling, design, and develop new 3D-CAD to detect and track the progress of Glaucoma.\n\u25e6 Lead design, development, and testing of new image quality software for retinal and facial images.\n\u25e6 Participate in the design of a hyperspectral instrument.\n\u25e6 Project management including; budget, schedule, team orientation, report writing and proposals writing.\n\u25e6 Customer interactions and supports\n\nProgramming tools: C/C++, Matlab, Source Control, and Mathematica.', u'Senior Research Engineer\neMotion, Inc\nApril 2000 to December 2004\nInvolved in the design and development of Mediapartner, multi-tiers clients/server software for the management of digital media such as still images, audio, and video files.\n\u25e6 Bring new ideas to enhance the functionality and performance of the system.\n\u25e6 Participated in in-house customer meetings and presentations.\n\nProgramming tools: Perl, JavaScript, and SQL under Windows and UNIX.']","[u'PhD in Electrical Engineering', u'MS in Electrical Engineering in Electrical Engineering', u'BS in Electrical Engineering']","[u'Pierre and Marie Curie University Paris, FR\nJanuary 1997', u'Pierre and Marie Curie University\nJanuary 1992', u'Algiers University\nJanuary 1990']","degree_1 : PhD in Electrical Engineering, degree_2 :  MS in Electrical Engineering in Electrical Engineering, degree_3 :  BS in Electrical Engineering"
0,https://resumes.indeed.com/resume/05cbcc11081cb05b,"[u'Data Scientist\nCisco Systems - Austin, TX\nJune 2016 to December 2017\nCisco is the largest networking company that develops and manufactures networking hardware as well as software in the security, automation and network analytics domain. Cyber security data analytics was one of the impactful projects I worked on. I introduced Machine Learning approach for DDoS classification and improved the accuracy by 7-8%. I also worked on predictive analytics for costumer churn problem for cisco. As a member of Chief Technology Office (CTO) organization, I actively worked in testing, validating and incubating data analytic products and analyzed standard and coherence for Merger and Acquisition. Filed a patent in microservice machine learning domain in US Patent office.\n\nResponsibilities:\n\u2022 Research, use case development, prototyping and POC in network security, performance and user experience domain in collaboration with network engineers and data scientists.\n\u2022 Used python and R libraries on ensemble based classification using Naive Bayes, Random Forests, XGBoost, Neural Network & Deep Learning algorithms to improve DDoS attack classification accuracy by 7% with potential impact of millions of dollar.\n\u2022 Predictive analytics for customer churn use cases.\n\u2022 Used association rule mining method to investigate unusual bottlenecks in network congestion problems.\n\u2022 Research and development in microservice machine learning and edge computing.\n\u2022 Network Telemetry: Research and development in SNMP/IOS-XR Telemetry data for pipelining, automation and orchestration using Pipeline, Grafana, PNDA etc. SNMP and BGP data analytics for different use cases using different methods including association rule mining method.\n\u2022 Laboratory Management: Manage laboratory with several rack servers and routers. Installed and used virtualization tools, hypervisors, software such as docker and other open-sourced and commercial software to study network flow.\nEnvironment: Python, R Studio, C/C++, SQL, Perl, wireshark, pipeline, Grafana, PNDA, Hypervisors (VMWare), Linux, Unix', u'Data Scientist\nEli Lilly and Company - Indianapolis, IN\nJune 2015 to January 2016\nEli Lilly and Company is among the top 10 pharmaceutical companies in the world and pioneering in its field to discovery of new vaccines. I am involved in multiple statistical projects; one of them is to predict the effect of variation in DNA sequence to enable precision medicine. I have also successfully deployed a machine-learning model to forecast next 5 years revenue from anti-diabetic vaccine sales in production, built on R and Shiny. Extensively used cluster computing to handle large-scale data aggregation and computation\n\nResponsibilities:\n\u2022 Built data pipelines from multiple data sources by performing necessary ETL tasks.\n\u2022 Performed Exploratory Data Analysis using R and Apache Spark.\n\u2022 Performed Data Cleaning, features scaling, features engineering.\n\u2022 Performed natural language processing to extract features from text data.\n\u2022 Performed text analysis, tf-idf analysis.\n\u2022 Visualized bigrams networks to investigate individual importance.\n\u2022 Built a forecasting model to predict future sales for anti-diabetes vaccines in global market.\n\u2022 Built multiple time-series models like ARIMA, ARIMAX (Dynamic Regression), TBATS, ETS.\n\u2022 Evaluated models performance on multiple test metrics such as MAPE, MAE & MASE.\n\u2022 Developed a shiny app to highlight Bayesian analysis and performed visualizations with ggplot2.\n\nEnvironment: EMC Isilon, Oracle, HADOOP (HDFS), Apache Spark, R Studio, Python, JAVA, Tableau, SQL.', u""Data Scientist\nGenuine Parts - Atlanta, GA\nJune 2013 to May 2014\nGenuine Parts Company (GPC) is an American service organization engaged in the distribution of automotive replacement parts, industrial replacement parts, office products and electrical/electronic materials, I have contributed in multiple data science projects; recently I worked on a project that asked for building a predictive model that utilizes text as features to predict repair hours for trucks. Further, we also formulated a recommender system, which facilitated an enhanced approach for configuration of trucks\n\nResponsibilities:\n\u2022 Examined the existing database, collected statistics to learn about user behavior.\n\u2022 Merged user data from multiple data sources.\n\u2022 Performed Exploratory Data Analysis using R and Hive on Hadoop HDFS.\n\u2022 Prototype machine learning algorithm for POC (Proof of Concept).\n\u2022 Performed Data Cleaning, features scaling, features engineering.\n\u2022 Formulated a novel approach to build machine-learning algorithm and implemented it in production environment.\n\u2022 In real-time association rules were implemented which uses prior probabilities.\n\u2022 Performed Data Mining in R (TM package, LSA package) using SAP HANA platform.\n\u2022 Established dimensionality reduction by SVD, 1500 data codes were transitioned into 24 different unique features.\n\u2022 Developed Performance metrics to evaluate Algorithm's performance.\n\u2022 Performed data visualization on the front end by using Tableau\nEnvironment: TERADATA, Oracle, HADOOP (HDFS), R Studio, Python, SAP HANA, JAVA, HIVE, Tableau."", u""Data Scientist\nWal-Mart - Bentonville, AR\nJune 2012 to May 2013\nWal-Mart has millions of customers who shop in store as well as online over a range of thousands of products. It produces enormous amount of data which can provides insights about the products, which are in demand, consumer habits, consumer demands, marginal profits from the sales etc. Our aim is to make wise use of such data to develop a recommender engine, which can learn from past data of the customer transactions and recommend relevant options of new products to the customers. This enhances the customer experience as well as increases sales for the Walmart. By implementing this project, we were able to achieve 5% increase in overall sales revenue.\n\nResponsibilities:\n\u2022 Examined the existing database, collected statistics to learn about user behavior.\n\u2022 Merged user data from multiple data sources by writing SQL queries.\n\u2022 Used Collaborative Filtering with Latent Factors model to build a recommender engine.\n\u2022 Performed extensive implicit as well as explicit data collection.\n\u2022 Performed Exploratory Data Analysis using R and Hive on Hadoop HDFS.\n\u2022 Prototype machine learning algorithm for POC (Proof of Concept).\n\n\u2022 Performed Data Cleaning, handled missing data, outliers, features scaling, and features engineering.\n\u2022 Developed Performance metrics to evaluate Algorithm's performance.\n\u2022 Calculated RMSE score, F-SCORE, PRECISION, RECALL, and A/B testing to evaluate recommender's performance.\n\u2022 Addressed the over-fitting by adding regularization (lasso / ridge) term in the algorithm.\n\u2022 Fine-tuned low bias and high variance trade off.\n\u2022 Performed post-hoc data analysis (tukey test) in R.\n\nEnvironment: TERADATA, Oracle, HADOOP (HDFS), PIG, MySQL, R Studio, Python, MAHOUT, JAVA, HIVE."", u'Data Scientist\nTransamerica - Baltimore, MD\nJune 2010 to April 2012\nResponsibilities:\n\u2022 Work independently and collaboratively throughout the complete analytics project lifecycle including data extraction/preparation, design and implementation of scalable machine learning analysis and solutions, and documentation of results.\n\u2022 Performed statistical analysis to determine peak and off-peak time periods for ratemaking purposes\n\u2022 Conducted analysis of customer data for the purposes of designing rates.\n\u2022 Identified root causes of problems, and facilitated the implementation of cost effective solutions with all levels of management.\n\u2022 Application of various machine learning algorithms and statistical modeling like decision trees, regression models, clustering, SVM to identify Volume using scikit-learn package in R.\n\u2022 Worked on different data formats such as JSON, XML and performed machine learning algorithms in Python.\n\u2022 Hands on experience in implementing Naive Bayes and skilled in Random Forests, Decision Trees, Linear and Logistic Regression, SVM, Clustering, Principle Component Analysis.\n\u2022 Performed K-means clustering, Regression and Decision Trees in R.\n\u2022 Worked on Na\xefve Bayesian algorithms for Agent Fraud Detection using R.\n\u2022 Have knowledge on A/B Testing, ANOVA, Multivariate Analysis, Association Rules and Text Analysis using R.\n\u2022 Developed Regression Models based on data provided by the client.\n\u2022 Work independently or collaboratively throughout the complete analytics project lifecycle including data extraction/preparation, design and implementation of scalable machine learning analysis and solutions, and documentation of results.\n\u2022 Partner with technical and non-technical resources across the business to leverage their support and integrate our efforts.\n\u2022 Partner with infrastructure and platform teams to configure, tune tools, automate tasks and guide the evolution of internal big data ecosystem; serve as a bridge between data scientists and infrastructure/platform teams.\n\u2022 Worked on Text Analytics and Naive Bayes creating word clouds and retrieving data from social networking platforms.\n\u2022 Pro-actively analyze data to uncover insights that increase business value and impact.\n\u2022 Support various business partners on a wide range of analytics projects from ad-hoc requests to large-scale cross-functional engagements\n\u2022 Prepared Data Visualization reports for the management using R\n\u2022 Approach analytical problems with an appropriate blend of statistical/mathematical rigor with practical business intuition.\n\u2022 Hold a point-of-view on the strengths and limitations of statistical models and analyses in various business contexts and is able to evaluate and effectively communicate the uncertainty in the results.\n\u2022 Application of various machine learning algorithms and statistical modeling like decision trees, regression models, SVM, clustering to identify Volume using scikit-learn package in python, Matlab.\n\u2022 Worked on different data formats such as JSON, XML and performed machine learning algorithms in Python.\n\u2022 Approach analysis in multiple ways in order to evaluate approaches and compare results.\nEnvironment: Oracle 10g, SQL, MySQL, R Studio, Python, MAHOUT, JAVA, JSON, XML, UNIX', u'Jr. Data Scientist\nWells Fargo - Minneapolis, MN\nJune 2009 to May 2010\nResponsibilities:\n\u2022 Converted various SQL statements into stored procedures thereby reducing the number of database accesses.\n\u2022 Responsible for architecting analytic frameworks for data mining, ETL, analysis, and reporting under the supervision of the Manager.\n\u2022 Prepared regular patient reports by collecting samples of Diagnosed Patients using Excel spreadsheets.\n\u2022 Cleaned data by analyzing and eliminating duplicate and inaccurate data (outliers) using R.\nWorked in Agile Environment.\n\u2022 Ensure that there are no missing values in the dataset and can be used for further Analysis.\n\u2022 Trained in Basics of Data Scientist and implemented those software applications in collecting and managing patient data in Excel/SPSS.\n\u2022 Assisted in performing statistical analysis of the data and storing them in a database.\n\u2022 Worked with Quality Control Teams to develop Test Plan and Test Cases.\n\u2022 Involved in designing and implementing the data extraction (XML DATA stream) procedures.\n\u2022 Generated graphs and reports using ggplot in R Studio for analyzing models.\n\u2022 Generating the Results and predicting the Accuracy.\n\u2022 Preparing the Final Documents and ensure delivery to the Client before EOD.\nEnvironment: Oracle 9i/10g, SQL, XML, MS Excel, SPSS, UNIX']",[u'MS in Statistics in Statistics and Data Sciences'],"[u'University of Texas at Austin Austin, TX']",degree_1 : MS in Statistics in Statistics and Data Sciences
0,https://resumes.indeed.com/resume/ea4866843221f7ae,"[u""Data Scientist/Data Analyst/SQL Database Developer\nJanuary 2012 to Present\n\u2022 Performed data extraction and manipulation over large relational datasets using SQL, Python, and other analytical tools\n\u2022 Performed business analysis, technical analysis, system design, software programming, data modeling and report using quantitative data and texts,\n\u2022 Performed data de-normalization and normalization based on first, second, and third normal forms\n\u2022 Performed data analysis on large relational datasets using SQL commands such as DML (insert, update,\ndelete, merge) and DDL (create, alter, truncate, drop), Triggers, Views, User Defined Functions and\nComplex Stored Procedures\n\u2022 Coordinated with clients, software developers, QA analysts, and other internal departments to establish a technical platform managing hundreds and thousands of references and related data,\ntranslating client's business needs to technical problems to be solved, providing evaluations on development of the platform in various stages, and explaining complex technical issues to clients who\ndo not have the relevant technical background in a clear and effective way\n\u2022 Managed and coordinated timely completion of multiple concurrent projects, meeting deadlines and production obligations with required work quality\n\u2022 Performed data analysis on large relational datasets using optimized diverse SQL queries, including\nJoins such as inner join, full join, left join, right join, and self-join, group functions such as count, max,\ndistinct, avg, and sum, string functions such as char, charindex, len, left, ltrim, and substring, data\nconversions, and ranking functions\n\u2022 Implemented data analysis with various analytical tools such as Python/SQL, Pandas, NumPy,\nMatplotlib, Statsmodels, SciKit-Learn, Jupiter Notebook, and Anaconda\n\u2022 Developed predictive models and statistical analysis on selective subject matters in technical fields\n\u2022 Analyzed text data of technical references using supervised and unsupervised learning techniques\n\u2022 Used Python libraries and SQL queries/subqueries to create a number of datasets which produced\nstatistic statistics, tables, figures, charts and graphs\n\u2022 Wrote data queries to obtain lists of references from online databases for analysis\n\u2022 Provided data analysis and data support on patient information using constituent management\nplatform, managing data collection, data entry and deduping in the data environment\n\u2022 Provided consulting service on data management, data integration and data exploration of a large\nscale of patient information and data modeling based on text analysis"", u'Data Analyst\nJanuary 2007 to January 2011\n\u2022 Managed a large scale of biorepository database with patient information and medical assessment\n\u2022 worked on research and preclinical projects, collecting data, analyzing data, report analytical result\n\u2022 Led or participated in data improvement initiatives for continuous quality improvements; recommend\nopportunities to enhance data collection, processing and analysis\n\u2022 Investigated and interpreted data to evaluate the effectiveness of drugs on experimental disease\nmodels\n\u2022 Collaborated with a number of pharmaceutical companies on research projects and preclinical\ninitiatives\n\u2022 Queried data from online databases of biomedical research references and products\n\u2022 Designed quantitative tools in analytical research project of pharmacogenetics in disease models\n\u2022 Drafted manuscript, project report, grant application, and presentation development\n\u2022 Provided training, assistance and advice to interns, junior employees, and research staff on data\nanalysis, analysis techniques and the use of the analysis system', u'Researcher/Analyst\nJanuary 2003 to January 2007\nDesigned and conducted scientific research, generated and collected experimental data, analyzed large\nscales of data over scientific hypotheses using statistical models and other quantitative approaches\n\u2022 Reported the analytical results and proposed additional research plan to further explore the relevant\ndisease mechanisms\n\u2022 Queried data from online databases of biomedical references and products\n\u2022 Designed protocols for automated research and analysis to evaluate biomedical hypothesis\n\u2022 Conducted sophisticated statistical analysis over large amount of real-time data in experimental\ndisease models\n\u2022 Trained student interns and junior technician data collection, analysis, and reporting skills\n\u2022 Presented research and analytical findings in a number of international and national conferences']",[],[],degree_1 : 
0,https://resumes.indeed.com/resume/4670c5d560719366,"[u'Solutions Data Architect\nNovartis Pharmaceuticals Inc - Cambridge, MA\nAugust 2013 to January 2018\n- The role was a combination of strategic, leadership and hands-on\n\u27a2 Successfully spearheaded collaborrative intiative to implement data management and data quality best practices for companion diagnostics based clincal trials\n\u27a2 Developed automated Biomarker data quality utilities in Python to support assay development team uncover data insights and anomalies for specific clinical trials\n\u25e6 Worked very closely with:\n* CROs to implement database for data transfer and delivery standardization\n* Assay development scientists to understand the data analysis needs and nuances on an interactive and continuous mode with constant improvement of the data analysis process.\n\u27a2 The utilities merged biomarker data with clinical outcomes across multiple assay methods and technologies to enable unified patient per sample view\n\u27a2 Developed biomarker specific data models to retrofit clincial standards meta-data repository for data integrity. The data model was adapted to implement Medidata -Rave(TM) system for electronic data collection (eCRF) for clinical trial\n\u27a2 Applied the data standards - CDISC - SDTM, CDASH & ADaM to the data models\n\u27a2 Designed and implemented database systems to support multiple FDA submissions and Biostatistics analysis in Oracle and MySQL database platforms. The studies were based on NGS (Next Generation Sequencing), IHC (Immuno Histo-Chemistry) and PCR based technologies.\n\u25e6 Played a lead role, end-to-end through collaborrating with scientists (Assay development), regulatory, quality, biostatistics team to develop requirements, translate requirements to specifications, developed deliverable roadmap, hands-on implementation and managed development team on need basis\n\u25e6 The data architecture solutions supported variety of assay technology based data structures such as - IHC, NGS and PCR.\n\u27a2 Provided support for a large scale ETL PoC, multi study data pooling solution using Apache Spark on AWS datalake platform to enable machine learning and exploratory analysis\n\u27a2 Provided architecture support for developing Biomarker exploratory analysis PoC. Developed initial TIBCO Spotfire realizations to biomarker views across multiple studies', u'Data Scientist and Architect\nFinancial Engineering and Risk management\nMay 2013 to May 2013\n* Gaussian Copula model for CDOs, Black Derman Toy for valuation of fixed income instruments and derivatives, Monte Carlo simulation and Bootstrapping algorithm portfolio management, PCA (principal component analysis), Pattern recognition and Kernel regression techniques on time series data for equity asset selection, conditional maximization and maximum likelihood algorithms for estimation required for asset valuation and portfolio construction, Portfolio optimization - Quadratic and Simulated annealing, Trading strategies - Momentum, Sector rotation modelling, Pairs trading strategies, Time series - ACF (Auto Correlation Function) and ARMA (Auto Regressive Moving Average), Control system modelling methods for portfolio management\n\u27a2 Certificate in Financial Engineering and Risk management (Columbia University, NY), May 2013\n\u27a2 B.E(Bachelor of Engineering) - Electrical and Electronics Engineering, Govt. College of Technology, Bharathiar University, India']",[u'Graduate Certificate in Quantitative Methods in Finance and Risk Management in Statistical Methods'],[u'Stanford University\nDecember 2014'],degree_1 : Gradate Certificate in Qantitative Methods in Finance and Risk Management in Statistical Methods
0,https://resumes.indeed.com/resume/46468408c0ede087,"[u'Data Scientist\nCaprinomics - Denver, CO\nApril 2017 to Present\nData mining using state-of-the-art methods. Processing, cleansing, and verifying the integrity of data\nused for analysis.', u'Design Support Consultant\nEngineering Consultancies, CO\nApril 2006 to April 2017\nManaged projects consisting of multiple disciplines and lead teams of individuals from single disciplines.\nDesign of refineries, chemical plants, separation facilities, and industrial piping systems.', u'Business Intelligence Analyst\nBBQ Rub, Inc - Real, Texas, US\nJanuary 2002 to April 2006\nDeveloped revenue predicting linear models. Planned and oversaw new marketing initiatives.\nResearched organizations and individuals online to identify new leads and potential new markets.']","[u'Certificate', u'B.B.A. in Economics']","[u'University of Colorado Denver, CO', u'Texas A&M University Corpus Christi, TX']","degree_1 : Certificate, degree_2 :  B.B.A. in Economics"
0,https://resumes.indeed.com/resume/437323109d80d749,"[u'Data Scientist Intern\nGartner, Inc - Champaign, IL\nApril 2017 to Present\n\u2022 Researched on various text retrieval projects from Business Research team using tools such as Python, SQL, Linux, etc.\n\u2022 Adapted NLP and Machine Learning techniques to build classification and clustering models\n\u2022 Delivered demo every two weeks about project updates in Agile methodology and introduced new interns to old projects']","[u'M.S. in Statistics in Statistics', u'B.S. in Applied Mathematics in Applied Mathematics']","[u'University of Illinois at Urbana-Champaign Urbana-Champaign, IL\nMay 2018', u'Dongbei University of Finance & Economics\nJuly 2016']","degree_1 : M.S. in Statistics in Statistics, degree_2 :  B.S. in Applied Mathematics in Applied Mathematics"
0,https://resumes.indeed.com/resume/ca3283df37213def,"[u'Data Analyst\nHALLIBURTON - Houston, TX\nJanuary 2015 to January 2017\n\u2022 Imported / Exported and Loaded a wide array of Geoscience and Petrotechnical data using OpenWorks software\n\u2022 Accountable for recommending improvements which streamline workflow processes and standardization efforts\n\u2022 Tested and Validated Geoscience, Drilling, Environmental and Petrotechnical data (Data Mining)\n\u2022 Performed and implemented complex test cases, workflows and scripts which map to requirements', u'Data Analyst / Product Owner\nCONOCOPHILLIPS - Houston, TX\nJanuary 2012 to January 2015\n\u2022 Acted as the Global Lead Technical Subject Matter Expert for the Core and Log Databases\n\u2022 Oversaw the performance of external Consultants and Data Leads to ensured quality of work\n\u2022 Developed, implemented and supported document Data Management processes including setting corporate standards and procedures\n\u2022 Provided direction and guidance, support and mentoring for the geoscience technicians on data management workflows and data governance standards\n\u2022 Managed data in various Database Management Systems (DBMS) including iPoint, Recall and PARS. Ensured the quality, consistency and integrity of all data\n\u2022 Executed continuous massive data capturing projects by transferring data from vendor websites to internal databases saving the Company over $35K year over year\n\u2022 Spearheaded the implementation of technology best practices by directing the transition from SQL to Oracle which allowed the Company to work more efficiently\n\u2022 Performed root cause analysis studies and led ""Lessons Learned"" discussions. Coached employees on what went wrong and how to mitigate issues in the future', u'HALLIBURTON - Houston, TX\nJanuary 2007 to January 2012\nQuality Assurance Geoscientist (October 2010 - November 2012)\n\u2022 Developed outstanding relationships internally with Managers and Key Stakeholders to support customer needs\n\u2022 Created user workflows and test cases\n\u2022 Led the creation and management of keyword-based automation test cases\n\u2022 Conceptualized new ways of working and liaison with developers to determine how to implement best practices', u'Data Management Consultant\nHALLIBURTON\nJanuary 2007 to October 2010\n\u2022 Oversaw several on-site client application portfolios and performed R5000 upgrades and migrations\n\u2022 Conducted well, environmental and seismic data quality control\n\u2022 Led the conversion of large amounts of data using GeoFrame and OpenWorks\n\u2022 Partnered with Project Managers to develop data analytics for special projects\n\u2022 Extracted, loaded and developed quality standards for well interpretation data', u'Environmental Scientist\nSECOR INTERNATIONAL INCORPORATED - Midland, TX\nJanuary 2005 to January 2006\n\u2022 Inspected and maintained equipment for groundwater well purging, sampling and collection\n\u2022 Collect, analyze, investigate, document and report the results of water samples and air samples\n\u2022 Responsible for implementing and monitoring site specific, health, safety, and environmental (HSE) operations\n\u2022 Knowledgeable of state and federal regulations (e.g., TCEQ, EPA, etc.)\n\u2022 Due diligence, site assessment, remediation and monitoring\n\u2022 Synchronized day to day operations between Chevron and SECOR management\n\u2022 Environmental Impact Assessments']","[u'Master of Science in Environmental Management', u'', u'Bachelor of Science in Geology']","[u'University of Houston at Clear Lake Houston, TX', u'University of Edinburgh Edinburgh', u'Baylor University Waco, TX']","degree_1 : Master of Science in Environmental Management, degree_2 :  , degree_3 :  Bachelor of Science in Geology"
0,https://resumes.indeed.com/resume/4d53443178498304,"[u'Data Scientist\nAsk Zuma - Los Angeles, CA\nAugust 2016 to Present\nRelevant Skills: Data-Scraping Beautiful Soup, Data Metric Analysis, Data Munging, Clustering and Regression Models\nTrained K-Nearest-Neighbor classification models to detect similarities in existing customers and single female candidates.\nScraped population data from U.S. census to measure and confirm new hypothesis about customer attributes/features.\nConducted A/B tests using targeted FB ads to make a new product roadmap and made regression predictions about user growth.', u'iOS, Data Engineer\nfraichediscovery.com - Los Angeles, CA\nJanuary 2016 to March 2017\nRelevant Skills: Swift iOS Dev, A/B Testing, Product Road Map Construction, Data Scraping, System Design\nSummary: iOS platform that aggregated video stories to locations to help users understand the ambiance of popular locations.\n\u2022 Developed client-facing app using Material library, animations with the Pop framework; camera libraries from Cocoa-Cont.\n\u2022 Backend built on Django/MySQL stack hosted on AWS as well as using HLS streaming and UGC uploads through CF&ET.\n\u2022 Increased the user-base to 500+MAU and 2,000+ aggregated content uploads.', u'Data Science Intern\nHourglass - Los Angeles, CA\nMarch 2015 to September 2015\nRelevant Skills: Python, Data-Scraping, Data Aggregation/Munging, Na\xefve-Bayes, Product Road-Map, Market Analytics\n\u2022 Used Seaborn to create statistical visualizations like Pearson Correlations in order to detect multi-collinearity in the data.\n\u2022 Ran A/B analytics tests on Facebook ads and identified new customer groups through cluster analysis of their similar attributes.\n\u2022 Wrote a Python web scraping script using BS4 to scrape 1,000,000+ university emails as potential marketable customers.', u'iOS, Product Engineer\nRoastMe - Los Angeles, CA\nNovember 2014 to February 2015\nRelevant Skills: iOS Dev, Firebase Database API, Charter Council Product Dev, User-Interviews, Product Design\nSummary: iOS application that presents fun questions to poke fun and ""Roast"" friends.\n\u2022 Coded the platform on XCode and designed in Sketch, the backend powered by Google\'s Firebase framework.\n\u2022 Conducted individual user interviews and focus groups to test customer willingness against the preexisting product.']","[u'Certification', u'B.S. in Computer Science and Business Administration']","[u'Stanford University on Coursera Mountain View, CA\nOctober 2017 to February 2018', u'University of Southern California Los Angeles, CA\nAugust 2013 to June 2017']","degree_1 : Certification, degree_2 :  B.S. in Compter Science and Bsiness Administration"
0,https://resumes.indeed.com/resume/66e6c6520a1514b3,"[u""Machine Learning Engineer\nBMCHP - Boston, MA\nDecember 2016 to Present\nBMCHP is a HealthCare service provider company that provides services to the states of Massachusetts and New Hampshire. BMCHP's Mission is to serve Boston Medical Center and to assist in providing and enhancing access to effective, efficient medical care among low income, undeserved, disabled and elderly and other vulnerable populations. I worked on a Project named SCO (Senior Care Options) which provides specialty services and privileges for Senior Citizens in Massachusetts and New-Hampshire states\n\nResponsibilities:\n\u2022 Conducted analysis in assessing the behavioral patterns of the customers with RMF analysis; applied segmentation in the customers' data with using clustering algorithms such as K-means, and Hierarchical Clustering.\n\u2022 Determined customer satisfaction and helped enhance customer using NLP. Predicted the claim severity to understand future loss and ranked the importance of features.\n\u2022 Surveyed exclamatory wording difficulty of ESL users, investigated relationship between verb-phrase and word usage, designed machine learning algorithm utilizing verb-phrase features in distinguishing similar emotion words\n\u2022 Created Plots such as bar plots, line charts, scatter plots and also histograms for visualizing the data and the frequency of occurrences of certain entities using Matlpotlib.\n\u2022 Conducted analysis in assessing the behavioral patterns of the customers with RMF analysis; applied segmentation in the customers' data with using clustering algorithms such as K-means, and Hierarchical Clustering.\n\u2022 Optimized SQL queries to perform data extraction and merging from Oracle.\nCoordinated the execution of A or B tests to measure the effectiveness of a personalized recommendation system.\n\u2022 Developed logistic regression models to predict subscription response rate based on customer's variables like past transactions, promotions, response to prior mailings, demographics, interests, and hobbies, etc.\n\u2022 Predicted the claim severity to understand future loss and ranked the importance of features.\n\u2022 Used Python and Spark to implement different machine learning algorithms, including Generalized Linear Model, Random Forest, SVM, Boosting and Neural Network.\n\u2022 Worked on data cleaning, data preparation, and feature engineering with Python, including Numpy, Scipy, Matplotlib, Seaborn, Pandas, and Scikit-learn.\n\u2022 Determined customer satisfaction and helped enhance customer using NLP.\n\u2022 Developed Online Slot booking system for Assessment test.\n\u2022 Developing new products in the site with client's specification and solving the issues in the Freshersworld.com site.\n\u2022 Adding Google analytics code in site.\n\u2022 Testing the compatibility and functionality of websites in different browsers and mobile.\n\u2022 DB design and Maintenance.\n\u2022 Extracted Statistics for performance metrics analysis for various components on load testing.\n\u2022 Applied association rule mining & chain model to identify hidden patterns and rules in remedy ticket analysis which aid in decision making.\n\u2022 Segmenting ABO population and developing demographic profile against each fragment.\n\nEnvironment: Erwin r9.0, Informatica 9.0, ODS, OLTP, Team Foundation Server, Oracle 10g, Hive, OLAP, MySQL, Metadata, MS Excel, Mainframes MS Visio, Rational Rose, Requisite Pro, Hadoop, PL/SQL, etc.."", u""Data Scientist\nOptum - Atlanta, GA\nJune 2015 to October 2016\nOptum provides physician services to various hospitals and it get patient information in the form of ADT and MR's in HL7 data from the facilities. Insurance Eligibility request in the form EDI 270 is sent to the 3rd party end. EDI 837 is send to clearing house. Optum gets the electronic remittances in the form EDI 835. There is an integration application that gets ADT and MR over FTP and the integration application pushes the data to SQL server database. Being a part of a team.\nResponsibilities:\n\u2022 Used python libraries like Numpy, Scipy, and Pandas to create a visualization for the data frames. Performed data parsing, data manipulation and data preparation with methods like regex, split and combine, merge as requested from data scientist team.\n\u2022 Implemented Character Recognition using Support vector machine for performance optimization.\n\u2022 Image compression and reconstruction using Principle component analysis.\n\u2022 Collaborated with distributed cross-functional teams having common goals.\n\u2022 Innovate and leverage machine learning, data mining and statistical techniques to create new, scalable solutions for business problems\n\u2022 Analyze a large amount of data classify data.\n\u2022 Create a model for forecast revenue.\n\u2022 Developing new products in the site with client's specification and solving the issues in the Freshersworld.com site.\n\u2022 Testing the compatibility and functionality of websites in different browsers and mobile.\n\u2022 Extracted Statistics for performance metrics analysis for various components on load testing.\n\u2022 Applied association rule mining & chain model to identify hidden patterns and rules in Service-now ticket analysis which aid in decision making.\n\u2022 Segmenting ABO population and developing demographic profile against each fragment.\n\u2022 Isolating customer behavioral patterns by analyzing millions of customer data records over a period of time and correlating multiple customers' attributes.\n\u2022 Worked on various strategic projects (Customer engagement, SA closures, Locker surrender etc.) & Adhoc proactive Analysis with product teams to provide insights and trends using SAS & Excel.\n\n\u2022 Performed data parsing, data manipulation and data preparation with methods like regex, split and combine, merge as requested from data scientist team. Performed statistical analytics using respective plots and tools using the linear algebraic concepts.\n\u2022 Actively worked as part of the team with managers and other staff to meet delivery timelines and improve the agile process for more predictable delivery.\n\nEnvironments: Apache, Spark MLlib, TensorFlow, Oryx 2, Accord.NET, Amazon Machine Learning (AML), Python, Django, Flask, ORM, Jinja 2, Mako, Naive Bayes, SVM, K- means, ANN, Regression."", u'UI Developer\nJNET - Hyderabad, Telangana\nAugust 2011 to May 2014\nJNET is a software development company offering a full range of custom software application development, web development, mobility, QA services and Professional Staffing Solutions. I worked on multiple projects which include local and international companies.\nResponsibilities:\n\u2022 Communicated with team members to create impactful modules in Python for Predictive Analytics using Machine learning methods like Na\xefve Bayes, Clustering Algorithms.\n\u2022 Created external tables based on flat files and use them to load data into the final target tables. Wrote complex queries to load the data in the desired format.\n\u2022 Used Python and Spark to implement different machine learning algorithms, including\n\u2022 Generalized Linear Model, Random Forest, SVM, Boosting and Neural Network. NLP using NLTK library to analyze consumer feedback classification by Na\xefveByes\n\u2022 Responsible for data collection, cleansing, and ANOVA. Designed technical solution roadmap to deal with noise in sales data.\n\u2022 Worked on loading the data from MySQL to H Base where necessary using Sqoop.\n\u2022 Knowledge in Business Intelligence tools and visualization tools such as Business Objects, Tableau, ChartIO, etc.\n\u2022 Deployed GUI pages by using JSP, JSTL, HTML, DHTML, XHTML, CSS, JavaScript, and AJAX.\n\u2022 Configured the project on Web Sphere 6.1 application servers.\n\u2022 Implemented the online application by using Core Java, JDBC, JSP, Servlets and EJB 1.1, Web Services, SOAP, WSDL.\n\u2022 Monitoring the automated loading processes; Communicated with other Health Care info by using Web Services with the help of SOAP, WSDL JAX-RPC.\n\u2022 Used SAX and DOM parsers to parse the raw XML documents\n\u2022 Used Log4J logging framework to write Log messages with various levels.\n\u2022 Involved in fixing bugs and minor enhancements for the front-end modules.\n\u2022 Implemented Microsoft Visio and Rational Rose for designing the Use Case Diagrams, Class model, Sequence diagrams, and Activity diagrams for SDLC process of the application.\n\u2022 Maintenance in the testing team for System testing/Integration/UAT.\n\u2022 Guaranteeing quality in the deliverables.\n\u2022 Conducted Design reviews and Technical reviews with other project stakeholders.\n\nEnvironment: R 3.0, Erwin 9.5, Python, Tableau 8.0, MDM, IBM Cognos, sklearn, PL/SQL, HDFS, Teradata 14.1, JSON, HADOOP (HDFS), MapReduce, PIG, Spark, R Studio, HIVE, AWS.']","[u""Master's""]",[u''],"degree_1 : ""Masters"""
0,https://resumes.indeed.com/resume/de268735df253c51,"[u'Data Scientist Intern\nBeijing QiHe Data Ctd - Beijing, CN\nJune 2017 to August 2017\n- Related Skills: Python (numpy, pandas, matplotlib), Tensorflow, R, Java\n- Applied deep learning platform in classification of road images through web scraping, data\ncleaning, visualization']","[u'M.S. in Health Data Science', u'B.S. in Biological Science and Mathematics']","[u'Harvard University Cambridge, MA\nAugust 2017 to December 2018', u'China Agricultural University Beijing, CN\nSeptember 2013 to June 2017']","degree_1 : M.S. in Health Data Science, degree_2 :  B.S. in Biological Science and Mathematics"
0,https://resumes.indeed.com/resume/96e86f603603798f,"[u'Data Scientist\nKELLOGG - Elmhurst, IL\nJuly 2017 to Present\nDescription:The Kellogg Company is an American multinational food manufacturing company headquartered in Battle Creek, Michigan, United States.\n\nResponsibilities:\n\u27a2 Developed MapReduce/Spark Python modules for predictive analytics & machine learning in Hadoop on AWS.\n\u27a2 Application of various machine learning algorithms and statistical modeling like decision trees, text analytics, natural language processing (NLP), supervised and unsupervised, regression models, social network analysis, neural networks, deep learning, SVM, clustering to identify Volume using scikit-learn.\n\u27a2 package in python, MATLAB.\n\u27a2 Experienced in working with NLP frameworks such as Apache NLP, Stanford Core NLP for \u27a2 processing natural language text.\n\u27a2 Implemented a Python-based distributed random forest via Python streaming.\n\u27a2 Built models using Statistical techniques like Bayesian HMM and Machine Learning classification models like SVM, XG, SVM, Boost and Random Forest.\n\u27a2 Worked with data governance team to maintain data models, data compliance teams, Metadata, data Dictionaries, define source fields and its definitions.\n\u27a2 Data analysis tools and Setup storage in Amazon Web Services cloud computing infrastructure.\n\u27a2 A highly immersive Data Science program involving Visualization & Data Manipulation, Web Scraping, Machine Learning, Python programming, SQL, Unix Commands, API Git, MongoDB, NoSQL, Hadoop.\n\u27a2 Transformed Logical Data Model to Erwin, Physical Data Model Foreign Key relationships in PDM and ensuring the Primary Key, Consistency of definitions of Data Attributes and Primary Index Considerations.\n\u27a2 Developed Oracle11g stored packages, functions, procedures and database triggers using PL/SQL for ETL process, data handling, logging, archiving and to perform Oracle back-end validations for batch processes.\n\u27a2 Documented logical, physical, relational and dimensional data models.\n\u27a2 Designed the Data Marts in dimensional data modeling using star and snowflake schemas.\n\u27a2 Worked as Data Architects and IT Architects to understand the movement of data and its storage and ER Studio 9.x\n\u27a2 Coordinate with the UNIX team and installed TIDAL job scheduler on QA and Production Netezza environment.\n\u27a2 Handled importing data from various data sources, performed transformations using MapReduce, Hive and loaded data into HDFS.\n\u27a2 Worked with BTEQ to submit SQL statements, generate reports in Teradata and import, export data\n\u27a2 Designed and documented Use Cases, Activity Diagrams, Sequence Diagrams, OOD (Object Oriented Design) using UML and Visio.\n\u27a2 Created Hive queries that helped analysts spot emerging trends by comparing fresh data with historical metrics and EDW reference tables and processed the data using HQL (like SQL) on top of Map-reduce.\n\u27a2 Hand on development and maintenance using Oracle SQL, SQL Loader, PL/SQL and Informatica Power Center9.1.\n\u27a2 Designed the ETL process to Extract translates and load data from OLTP Oracle database system to Teradata data warehouse.\n\u27a2 Created tables, sequences, synonyms, joins, functions and operators in Netezza database.\n\u27a2 Created and implemented MDM data model for Consumer/Provider for HealthCare MDM product from Variant.\n\u27a2 Hands on Oracle External Tables feature to read the data from flat files into Oracle staging tables.\n\u27a2 Analyzed the web log data using the HiveQL to extract number of unique visitors per day, visit duration, page views, and most purchased product on website and managed and reviewed Hadoop log files.\n\u27a2 Created SSIS Packages using Pivot Transformation, Execute SQL Task, Data Flow Task, etc to import data into the data warehouse.\n\u27a2 Developed and implemented SSRS, SPSS, SAS, SSIS and SSAS application solutions for various business units across the organization.\n\nEnvironment: Hadoop, HDFS, Pig, Hive, MapReduce, Erwin r 9.x, Teradata, Oracle11g, PL/SQL, UNIX, Informatica Power Center, MDM, SQL Server, Netezza, DB2, Tableau, Aginity, Architecture, SAS/Graph, SAS/SQL, Python, Tableau, SAS/Connect and SAS/Access.', u""Data Scientist\nAmazon, CA\nApril 2016 to June 2017\nDescription: Amazonis an American electronic commerce and cloud computing company based in Seattle, Washington that was founded by Jeff Bezos on July 5, 1994.\n\nResponsibilities:\n\u27a2 Involved in development of data warehouse environment and design, liaison to business users and technical teams gathering requirement specification documents and presenting and identifying data sources, targets and report generation.\n\u27a2 Designed an Industry standard data Model specific to the company with group insurance offerings, Translated the business requirements into detailed production level using Workflow Diagrams, Activity Diagrams, Sequence Diagrams and Use Case Modelling\n\u27a2 Conceptualized the most-used product module (Research Center) after building a business case for approval, gathering requirements and designing the User Interface\n\u27a2 Successfully managed projects using Agile development methodology\n\u27a2 Developed predictive models using Decision Tree, Random Forest, Vector Machines and Naive Bayes and collaborating with marketing and dev-ops teams for production deployment.\n\u27a2 A team member of analytical Group and assisted in designing and development of statistical models for the end clients.\n\u27a2 Handle with end users for designing and implementation of e-commerce analytics solutions as per project proposals.\n\u27a2 Conducted market research for client; designed sampling methodologies and developed, and analyzed the survey data for pricing and availability of clients' products. Investigated product feasibility by performing analyses that include market sizing, competitive analysis and positioning.\n\u27a2 Successfully optimized codes in Python to solve a variety of purposes in data mining and machine learning in Python.\n\u27a2 Created Data QualityScripts using SQL and Hive to validate successful data load and quality of the data. Created various types of data visualizations using Python and Tableau.\n\u27a2 Facilitated sprint reviews to drive project completion and stakeholder meetings.\n\u27a2 Project experience in Data mining, segmentation analysis, business forecasting and association rule mining using Large Data Sets with Machine Learning.\n\u27a2 Automated Diagnosis of Blood Loss during Accidents and Applied Machine Learning algorithms to diagnose blood loss from vital signs (HF, GSR, and ECG).\n\u27a2 Trained organization-wide employees for Financial domain certification & Business Analysis Certification exams\n\u27a2 Prepared graphs and reports using GGplot2 library for an overview of the analytical models and results.\n\u27a2 Developed Shiny and R application showcasing machine learning for improving business forecasting.\n\nEnvironment: R, SQL Server 2012/2014, SQL, Oracle 10g, Windows XP/NT/2000, DB2, MS Excel, Mainframes MS Visio, Rational Rose, Requisite Pro, Crystal Reports 9, Python, R Studio, Shiny, Excel 2013."", u'Data Scientist\nFermilab - Batavia, IL\nDecember 2014 to March 2016\nDescription: Fermi National Accelerator Laboratory (Fermilab), located just outside Batavia, Illinois, near Chicago, is a United States Department of Energy national laboratory specializing in high-energy particle physics.\n\nResponsibilities:\n\u27a2 Provided the architectural leadership in shaping strategic, business technology projects, with an emphasis on application architecture.\n\u27a2 Utilized domain knowledge and application portfolio knowledge to play a key role in defining the future state of large, business technology programs.\n\u27a2 Participated in all phases of data mining, data collection, data cleaning, developing models, validation, and visualization and performed Gap analysis.\n\u27a2 Performed Source System Analysis, database design, data modeling for the warehouse layer using MLDM concepts and package layer using Dimensional modeling.\n\u27a2 Used Pandas, SciPy, NumPy, Matplotlib, Scikit-learn, seaborn, NLTK in Python for developing various machine learning algorithms and utilized machine learning algorithms such as linear regression, multivariate regression, naive Bayes, Random Forests, K-means&KNN for data analysis.\n\u27a2 Demonstrated experience in design and implementation of Statistical models, Predictive models, enterprise data model, metadata solution and data life cycle management in both RDBMS, Big Data environments.\n\u27a2 Designed and implemented system architecture for Amazon EC2 based cloud-hosted solution for client.\n\u27a2 Tested Complex ETL Mappings and Sessions based on business user requirements and business rules to load data from source flat files and RDBMS tables to target tables.\n\u27a2 Analyzed large data sets apply machine learning techniques and develop predictive models, statistical models and developing and enhancing statistical models by leveraging best-in-class modeling techniques.\n\u27a2 Worked on customer segmentation using an unsupervised learning technique - clustering.\n\u27a2 Worked with various Teradata15 tools and utilities like Teradata Viewpoint, ARC, MultiLoad, Teradata Administrator, BTEQ and other Teradata Utilities.\n\u27a2 Utilized Spark, Scala, Hadoop, HBase, Matlab, MLlib, Spark Streaming, Python, Kafka a broad variety of machine learning methods including classifications, regressions, dimensionally reduction etc.\n\nEnvironment: Java, Hadoop, Hive, Teradata, random forest, OLAP, Azure, MariaDB, SSRS, Tableau, MLlib, regression, Cluster analysis, Scala NLP, Spark, Kafka, MongoDB, logistic regression, SAP CRM, HDFS, ODS, NLTK, SVM, JSON, Tableau, XML, Cassandra, MapReduce, AWS.', u""Data Scientist/ Data Analyst\nTrust Company\nApril 2013 to November 2014\nDescription: The TCW Group is an asset management firm with a broad range of products across fixed income, equities, emerging markets and alternative investments. With nearly five decades of investment experience, TCW today manages over $194 billion in client assets.\n\nResponsibilities:\n\u27a2 Responsible for technical data governance, enterprise wide data modeling and database design.\n\u27a2 Used Model Mart of ERwin for effective model management of sharing, dividing and reusing model information and design for productivity improvement.\n\u27a2 Conducted detailed and comprehensive Business Analysis by working with the IT staff, SME's, Business Staff, and other stakeholders to identify the system, operational requirements and process improvements.\n\u27a2 Designed an Industry standard data Model specific to the company with group insurance offerings, Translated the business requirements into detailed production level using Workflow Diagrams, Activity Diagrams, Sequence Diagramsand Use Case Modeling\n\u27a2 Involved in design and development of data warehouse environment, liaison to business users and technical teams gathering requirement specification documents and presenting and identifying data sources, targets and report generation.\n\u27a2 Worked with Development DBA to assist and support developers with SQL performance tuning, query tuning and code reviews.\n\u27a2 Developed Python programs for manipulating the data reading from various Teradata, update the Content in the database tables.\n\u27a2 Responsible for evaluating various RDBMS like OLTP modeling, documentation, and metadata reporting tools including Erwin, developed logical/ physical data models using Erwin tool across the subject areas based on the specifications and established referential integrity of the system.\n\u27a2 Extensively worked on Source to Target mapping for business need and documentation purpose.\n\u27a2 Created snapshots, views, and database indexes for improving the query performance.\n\u27a2 Created and maintained Logical Data Model (LDM) / Physical Data Modeling for the insurance system.\n\u27a2 Created dimensional model for the reporting system by identifying required dimensions and facts using Erwinr8.\n\u27a2 Perform updating data by weekly and and maintain and monthly, manipulating the data for database management. Used the SAS/MACROS for the monthly production.\n\u27a2 Worked with Comparison between Data Model Vs Database and generate difference Report.\n\nEnvironment: Windows XP/NT/2000, Erwin r8, Informatica, SQL Server 2005/2008, SQL, Oracle8i/10g, DB2, MS Excel, Mainframes MS Visio, Rational Rose, Requisite Pro."", u'Data Analyst\nDatawise - Hyderabad, Telangana\nNovember 2011 to March 2013\nDescription: The DATAWISE Group is a leading Research and Analytics services Company that leverages data to derive meaningful insights that help its global customers to make better decisions.\n\nResponsibilities:\n\u27a2 Developed ETL processes for data conversions and construction of data warehouse using IBM InfoSphere DataStage.\n\u27a2 Responsible for writing stored procedures, triggers etc., in MS SQL Server.\n\u27a2 participated in group technology reviews to critique work of self and others.\n\u27a2 Used Star Schema and designed Mappings between sources to operational staging targets.\n\u27a2 Provided On call Support for the project and manage knowledge transfer for the clients.\n\u27a2 Used Rational Application Developer (RAD) for version control.\n\u27a2 Handle performancetuning.\n\u27a2 Reused python scripts to fetch data from sandra\n\u27a2 Analyzing Problems in the system and making recommendations for system improvement.\n\u27a2 Developed custom reporting solutions using SQL Server.\n\u27a2 Developed transformations using jobs such as Filter, Aggregator, Lookup, Join, Transformer and Dataset.\n\u27a2 Develop components of database, data queries, data storage, data schema, data transformations, and data warehousing applications.\nEnvironment: Microsoft Excel Macros, Pivot Tables, vlookups, Match/Index, Pivot Tables and other advanced functions to leverage raw data', u'Data Analyst\nKarvy Analytics - Hyderabad, Telangana\nApril 2009 to October 2011\nDescription: Karvy Analytics Limited is a new age company and a modern arm of the leading Karvy Conglomerate. Led by visionary management, the young and forward thinking team is building world class solutions for the global analytics universe.\n\nResponsibilities:\n\u27a2 Worked with internal architects, assisting in the development of current and target state data architectures.\n\u27a2 Worked with project team representatives to ensure that logical and physical ER/Studio data models were developed in line with corporate standards and guidelines.\n\u27a2 Involved in defining the business/transformation rules applied for sales and service data.\n\u27a2 Implementation of Metadata Repository, Transformations, Maintaining Data Quality, Data Standards, Data Governance program, Scripts, Stored Procedures, triggers and execution of test plans\n\u27a2 Define the list codes and code conversions between the source systems and the data mart.\n\u27a2 Involved in defining the source to business rules, target data mappings, data definitions.\n\u27a2 Responsible for defining the key identifiers for each mapping/interface.\n\u27a2 Responsible for defining the functional requirement documents for each source to target interface.\n\u27a2 Responsible for defining the key identifiers for each mapping/interface.\n\u27a2 Performed data quality in Talend Open Studio.\n\u27a2 Enterprise Metadata Library with any changes or updates.\n\u27a2 Document data quality and traceability documents for each source interface.\n\u27a2 Establish standards of procedures.\n\u27a2 Coordinated meetings with vendors to define requirements and system interaction agreement documentation between client and vendor system.\n\nEnvironment: Windows Enterprise Server 2000, SSRS, SSIS, Crystal Reports, DTS, SQL Profiler, and Query Analyze.']","[u""Bachelor's""]",[u''],"degree_1 : ""Bachelors"""
0,https://resumes.indeed.com/resume/50d1fd5471a049f0,"[u'Research and Development Intern\nGraphen - New York, NY\nFebruary 2018 to Present\n\u2022 Implement PageRank and SimRank algorithm to build news recommendation system on knowledge graph.\n\u2022 Use Breadth-first-search (BFS) and Katz measure to find all paths of two nodes in a graph and calculate the similarity score.', u'Zillow Housing Price Prediction for Washington State - New York, NY\nFebruary 2018 to March 2018\nStacking Model, Ridge Regression Model, Gradient Boosting Regression Tree, Random Forest Regression, Scikit-learn, Pandas\n\u2022 Detected outliers, imputed missing values, label encoded categorical variables and box cox transformed skewed features in the feature engineering part before modeling the data.\n\u2022 Stacked base models (ridge regression, XGBoost, random forest regression) to predict the housing price, the prediction accuracy was\nabout 93%.', u""Crime Prediction in the City of New York - New York, NY\nSeptember 2017 to December 2017\nTime-series Model, Ridge Regression Model, Gradient Boosting Regression Tree, Python, R shiny\n\u2022 Combined a regression model and a time-series model to predict future occurrence of crimes in different locations and time.\n\u2022 Built an interactive web application using R's Shiny framework that shows these values as a chloropleth map overlaid on the city."", u'Lead Data Scientist Intern\nSizung Inc - New York, NY\nJune 2017 to December 2017\n\u2022 Built a personalized recommendation system for conversations using content-based method and natural language processing models.\n\u2022 Applied support vector machines (SVM), random forest, logistic regression, Naive Bayes classification to segment conversations\ninto different groups.\n\u2022 Implemented ridge regression, lasso regression and gradient boosting regression for price prediction.', u'Personalized Recommendation on Conversations - New York, NY\nJune 2017 to December 2017\nNatural Language Processing, Recommendation System, SQL Server, Cosmos Database, Python, Azure\n\u2022 Calculated the similarity between two conversations by combining natural language processing and TF-IDF method using Python.\n\u2022 Built content based recommendation model and combined the structure of conversations to achieve personalized recommendation.', u'Twitter Economic & Sentiment Analysis on Donald Trump - New York, NY\nMarch 2017 to April 2017\nSentiment Analysis, Word Cloud, R, Tableau, ggplot2\n\u2022 Studied the connection among economic characters and sentiment trends based on geography locations.\n\u2022 Generated word cloud from tweets to find key words and trends when people mentioned Donald Trump.\n\u2022 Conducted a sentiment analysis to evaluate whether a twitter post is positive, negative, or neutral towards Donald Trump.', u'Data Engineer II\nMerkle Inc - Nanjing, CN\nNovember 2014 to May 2015\nHonors: Delivery Excellence Award; Project Contribution Award\n\u2022 Designed and implemented database schemas in different levels (e.g. individual, household and product levels) on SQL Server.\n\u2022 Interacted with various data sources and built Extract/Transform/Load (ETL) processes on Netezza data warehouse.', u""DELL - Nanjing, CN\nMay 2013 to May 2015\nData Pipeline, SQL Server, Data Warehouse, Customer Relationship Management (CRM)\n\u2022 Analyzed order and transaction information to extract top features to target potential customers for campaigns.\n\u2022 Built feedback models to analyze the results of campaigns, and generated reports to improve customers' loyalty.\n\u2022 Built and maintained big data infrastructures and pipelines to ensure a smooth dataflow from 12 different sources to the data\nwarehouse."", u'Data Engineer I\nMerkle Inc - Nanjing, CN\nMay 2013 to November 2014\nHonors: Flawless Delivery Award\n\u2022 Implemented data pipelines including data validation and cleansing processes for different data sources.\n\u2022 Created ad-hoc campaigns and analyzed feedback to understand customers better and improve their loyalty.\n\u2022 Developed feedback models and attribution models of customers or products for better personalized campaigns.']","[u'M.S. in Data Science in Data Science', u'B.S. in Biomedical Engineering in Biomedical Engineering']","[u'Columbia University New York, NY\nSeptember 2016 to February 2018', u'Huazhong University of Science & Technology (HUST)\nSeptember 2009 to June 2013']","degree_1 : M.S. in Data Science in Data Science, degree_2 :  B.S. in Biomedical Engineering in Biomedical Engineering"
0,https://resumes.indeed.com/resume/efb584924e0ea8ff,"[u'Data Scientist\nSantander Bank - Holmdel, NJ\nJune 2017 to Present\nSantander Bank is based in Boston and its principal market is the northeastern United States. The Bank offers financial services and products including retail banking, mortgages, corporate banking, cash management, credit card, capital markets, trust and wealth management, and insurance.\n\nThe project was to build predictive models for the identification/detection of fraudulent transactions by applying machine learning methods, principle component analysis, and logistic regression on large dataset.\n\nResponsibilities:\n\u2022 Participated in all phases of data acquisition, data cleaning, developing models, validation, and visualization to deliver data science solutions.\n\u2022 Worked on fraud detection analysis on payments transactions using the history of transactions with supervised learning methods.\n\u2022 Collected data in Hadoop and retrieved the data required for building models using Hive.\n\u2022 Developed Spark Python modules for machine learning & predictive analytics in Hadoop.\n\u2022 Used Pandas, Numpy, Seaborn, Matplotlib, Scikit-learn in Python for developing various machine learning models and utilized algorithms such as Decision Trees, Logistic regression, Gradient Boosting, SVM and KNN.\n\u2022 Used cross-validation to test the models with different batches of data to optimize the models and prevent overfitting.\n\u2022 Used PCA and other feature engineering techniques for high dimensional datasets while maintaining the variance of most important features.\n\u2022 Created Transformation Pipelines for preprocessing large amount of data with methods such as imputing, scaling, selecting, etc.\n\u2022 Ensemble methods were used to increase the accuracy of the training model with different Bagging and Boosting methods.\n\nEnvironment:\nHadoop 2.x, HDFS, Hive, Pig Latin, Python 3.x (Numpy, Pandas, Scikit-learn, Matplotlib), Jupyter, GitHub, Linux', u""Data Analyst/Data Scientist\nSCIO Health Analytics - Hartford, CT\nApril 2015 to May 2017\nSCIO Health Analytics provides analytics solutions and services that turns data into actionable insights for health care providers in the United States and globally. Services also include medical and pharmacy claims auditing, inpatient data pursuits, care gaps closure, and commercial analytics.\n\nI was part of the team that worked with Subrogation claims of Healthcare Providers such as Humana. The objective was to load data, analyze, and provide monthly reports for the predictions on a claim's potential of a third-party recovery. Tableau and SSRS were used to build claim and recovery reports.\n\nResponsibilities:\n\u2022 Assembled a Predictive Modelling module by using supervised learning for Subrogation Claim Prediction to identify which claims would be classified as having Subrogation potential.\n\u2022 Implemented models such as Logistic Regression and Na\xefve Bayes, in Python using scikit-learn, to predict the claim potential outcome.\n\u2022 Dimensionality Reduction techniques applied to refine the attribute lists and feature selection applied to rank selected features to generate accurate results.\n\u2022 Gathered requirements and business rules from business users to implement Predictive Modelling.\n\u2022 Designed and developed ETL packages using SSIS to create Data Warehouses from different tables and file sources like Flat and Excel files, with different methods in SSIS such as derived columns, aggregations, Merge joins, count, conditional split and more to transform the data.\n\u2022 Designed reporting solutions for different stakeholders from mock-up till deployment in different areas such as Potential Subrogation claims, Monthly Revenue from Subrogation & Transactions.\n\u2022 Performed data visualization and designed dashboards with Tableau, and provided complex reports, including charts, summaries, and graphs to interpret the findings for Adjustors to view various claim information.\n\u2022 Optimized queries in T-SQL by removing unnecessary columns and redundant data, normalized tables, established joins and indices; developed complex SQL queries, stored procedures, views, functions and reports that meet customer requirements.\n\nEnvironment:\nPython 3.x (Scikit-learn, Matplotlib), Jupyter, SQL Server 2012, MS SQL Server Management Studio, MS BI Suite (SSIS/SSRS), T-SQL, Visual Studio BIDS, Tableau"", u'SQL BI Developer\nADP - Chennai, Tamil Nadu\nFebruary 2012 to March 2015\nADP is a leading provider of human resources management software and services worldwide.\n\nThis project was done for an internal business unit (ADP France) to comply with French statutory requirements for employee training. Goal was to develop a web-based SQL application built upon a baseline HRMS application to generate/support the Report development for training plans, budget preparation, cost tracking and 2483 reporting.\n\nResponsibilities:\n\u2022 Collected requirements from business users, and designed report models to meet business requirements.\n\u2022 Directed and managed meetings with clients, tracked document changes and ensured sign-off from clients.\n\u2022 Maintained and developed complex SQL queries, stored procedures, views, functions and reports that meet customer requirements using Microsoft SQL Server 2008 R2.\n\u2022 Created Views and Table-valued Functions, Common Table Expression (CTE), joins, complex subqueries to provide the reporting solutions.\n\u2022 Optimized the performance of queries with modification in T-SQL queries, removed the unnecessary columns and redundant data, normalized tables, established joins and created index.\n\u2022 Created, managed, and delivered interactive web-based reports to support daily operations.\n\u2022 Validated reports and resolve issues in a timely manner.\n\u2022 Developed and implemented several types of Reports (Training Reports, Schedules, Costs Summary Reports and Annual 2483 Report) by using features of SSRS such as sub-reports, drill down reports, summary reports and parameterized reports.\n\u2022 Designed and developed new reports and maintained existing reports for the Human Resource Management System Dashboards using Tableau, Qlikview and Microsoft Excel to support the business strategy and management.\n\u2022 Identified process improvements that significantly reduce workloads or improve quality.\n\nEnvironment:\nSQL Server 2008 R2, MS SQL Server Management Studio, SSRS, T-SQL, Visual Studio BIDS, Tableau, Qlikview']",[u'Master of Science in Business & Information Systems in Business & Information Systems'],"[u'New Jersey Institute of Technology Newark, NJ']",degree_1 : Master of Science in Bsiness & Information Systems in Bsiness & Information Systems
0,https://resumes.indeed.com/resume/68739c97613a2595,"[u""Data Scientist - Analytics Team Lead\nLinkedIn and Google Adwords - Chicago, IL\nJuly 2016 to Present\nChicago, IL\n\u2022 Formulated strategical qualitative and quantitative research plans, and helped to standardize 7/2016 - Present\nresearch process. Built web scraping tools by using API and Python to automate the process.\n\u2022 Introduced and applied statistical analysis on client's primary data and website analytics data\nsuch as A/B test, Logistic Regression and Linear Regression to generate data-driven insights.\n\u2022 Presented insights to the senior team and participated in generating marketing strategies.\n\u2022 Executed and optimized digital marketing campaigns on variety platforms such as Facebook\nad manager, LinkedIn and Google Adwords; Analyzed campaign data by using SQL, Pivot Table and provided insights and recommendations for the next stage strategy.\n\u2022 Averagely increased 300% in profitable monthly lead volume through digital marketing campaigns\nand decreased 49% in cost per lead for clients in CPG, technology and business service industries."", u'Research Assistant\nIllinois Institute of Technology - Chicago, IL\nJanuary 2016 to July 2016\nChicago, IL\n(Research topic: How price affected consumer ratings on Amazon tablet category) 1/2016 - 7/2016\n\u2022 Used SAS to process 45k+ records to generate statistics; improved data quality and increased\ndata integrity to 96% by auditing data and optimizing categorizing rules.\n\u2022 Selected products and variables for analyzing, used PROC SQL and PROC TIMESERIES statements\nto transform data into applicable time series format.\n\u2022 Improved predictive model performance by testing and adjusting VARMAX models based on different assumptions and generated insights from the results.', u'Nurse Executives | Data Analyst\nAmerican Organization of Nurse Executives - Chicago, IL\nAugust 2015 to December 2015\nChicago, IL\n\u2022 Processed data preparation to improve the quality of 50k+ records of demographic and fiscal 8/2015 - 12/2015\ndata, further optimized database schema in 3rd normal form (3NF) by using MySQL.\n\u2022 Executed cluster and revenue attribution analysis in SPSS Modeler by building K-means, RFM\nand linear regression models to identify attributes for valuable customers.\n\u2022 Offered strategical recommendations on retaining old customers, as well as acquiring new\ncustomers; Reduced customer acquisition cost by 15%.', u""Intern PR\nClinique PR Department, Est\xe9e Lauder Companies Inc - Shanghai, CN\nMarch 2012 to June 2012\nShanghai, China\n\u2022 Prepared materials to fulfill media feature requests given by 40+ media such as Elle and Vogue. 3/2012 - 6/2012\n\u2022 Coordinated with the Legal Department for PR events' contract confirmation.""]","[u'M.S. in Marketing Analytics in Qualitative and Quantitative Research', u'B.A. in Public Relations in Public Relations']","[u'Illinois Institute of Technology Chicago, IL\nMay 2016', u'Donghua University Shanghai, CN\nJuly 2013']","degree_1 : M.S. in Marketing Analytics in Qalitative and Qantitative Research, degree_2 :  B.A. in Pblic Relations in Pblic Relations"
0,https://resumes.indeed.com/resume/a4f52759c9788e03,"[u""Data Scientist\nRodan and fieilds - San Francisco, CA\nJune 2017 to Present\nDescription:\nRodan & Fields, LLC manufactures skincare products. Its products include eye cloths, multi-function eye creams, regimens, hand treatment regimens, Vitamin D supplements, body moisturizers, body sunscreens, gauze pads, micro-dermabrasion pastes, and mineral peptides brushes. The company offers its products online, as well as through independent consultants and independent business owners across the United States\n\nResponsibilities:\n\u2022 Performed DataProfiling to learn about behavior with various features such as traffic pattern, location, and time, Date and Time etc.\n\u2022 Used Principal Component Analysis in feature engineering to analyze high dimensional data.\n\u2022 Created and designed reports that will use gathered metrics to infer and draw logical conclusions of past and future behavior.\n\u2022 Performed dataintegrity checks, datacleaning, exploratory analysis and feature engineer using R and Python\n\u2022 Responsible for design and development of advanced R/Python programs to prepare transform and harmonize data sets in preparation for modeling\n\u2022 Application of various machine learning algorithms and statistical modeling like decision trees, regression models, neural networks, SVM, clustering to identify Volume using scikit-learn package in python, Matlab.\n\u2022 Optimizing the search relevance for recommender system in accordance with the user behavior and NLP.\n\u2022 Used Python and Spark to implement different machine learning algorithms including Generalized Linear Model, SVM, Random Forest, Boosting and Neural Network\n\u2022 Created Data Quality Scripts using SQL and Hive to validate successful data load and quality of the data. Created various types of data visualizations using Python and Tableau.\n\u2022 Communicated the results with operations team for taking best decisions.\n\u2022 Used R, Python and Spark to develop variety of models and algorithms for analytic purposes\n\u2022 Performed Multinomial Logistic Regression, Random forest, Decision Tree, SVM to classify package is going to deliver on time for the new route.\n\u2022 Performed data analysis by using Hive to retrieve the data from Hadoop cluster, Sql to retrieve data from Oracle database.\n\u2022 Utilized Spark, Scala, Hadoop, HBase, Kafka, Spark Streaming, MLLib, python, a broad variety of machine learning methods including classifications, regressions, dimensionally reduction etc.\n\u2022 Used Natural Language Processing (NLP) for response modeling and fraud detection efforts for credit cards\n\u2022 Used MLlib, Spark's Machine learning library to build and evaluate different models.\n\u2022 Implemented rule based expertise system from the results of exploratory analysis and information gathered from the people from different departments.\n\u2022 Developed MapReduce/SparkPython modules for machine learning & predictive analytics in Hadoop. Implemented a Python-based distributed random forest via Python streaming\n\u2022 Performed Data Cleaning, features scaling, features engineering using pandas and numpy packages in python.\n\u2022 Developed MapReduce pipeline for feature extraction using Hive.\n\u2022 Determined customer satisfaction and helped enhance customer experience using NLP\n\u2022 Collected data needs and requirements by Interacting with the other departments.\nEnvironment:\n\nR/R studio, SAS, Python, C++, Hive, Hadoop, MS Excel, Perl, MS SQL Server, Power BI, Tableau, T-SQL, ETL, MS Access, XML, JSON, MS office 2010, Outlook"", u'Data Scientist\nCBRE - Dallas, TX\nMarch 2016 to May 2017\nDescription:\nCBRE, Inc. provides commercial real estate services. It provides advisory services and outsourcing services operations.\n\nResponsibilities:\n\u2022 Involved in loading data from RDBMS and web logs into HDFS using Sqoop and Flume.\n\u2022 Worked on loading the data from MySQL to HBase where necessary using Sqoop.\n\u2022 Developed Hive queries for Analysis across different banners.\n\u2022 Coded R functions to interface with CaffeDeep Learning Framework\n\u2022 Analyzed the partitioned and bucketed data and compute various metrics for reporting.\n\u2022 Worked with different sources such as Oracle, Teradata, SQL Server2012 and Excel, Flat, Complex Flat File, Cassandra, MongoDB, HBase, and COBOL files.\n\u2022 Working in Amazon WebServices cloud computing environment\n\u2022 Implemented public segmentation using unsupervised machinelearning algorithms by implementing k-means algorithm using Pyspark.\n\u2022 Explored and Extracted data from source XML in HDFS, preparing data for exploratory analysis using data munging.\n\u2022 Have experience working in Rapid miner studio and Python.\n\u2022 Used R and python for Exploratory Data Analysis, A/B testing, Anovatest and Hypothesis test to compare and identify the effectiveness of CreativeCampaigns.\n\u2022 Created DataQuality Scripts using SQL and Hive to validate successful data load and quality of the data. Created various types of data visualizations using Python and Tableau.\n\u2022 Used R and Python for programming for improvement of model.\n\u2022 Used Spark for test data analytics using MLLib and Analyzed the performance to identifybottlenecks.\n\u2022 Worked on different data formats such as JSON, XML and performed machine learning algorithms in R.\n\u2022 Worked on Linuxshell scripts for business process and loading data from different interfaces to HDFS.\n\u2022 Created MDM, OLAP data architecture, analytical data marts, and cubes optimized for reporting.\n\u2022 Developed MapReduce/SparkPython modules for machine learning & predictive analytics in Hadoop\n\u2022 Determined customer satisfaction and helped enhance customer experience using NLP.\n\u2022 Performed K-means clustering, Multivariate analysis and Support Vector Machines in R.\n\u2022 Used Python, R, SQL to create Statistical algorithms involving Multivariate Regression, Linear Regression, Logistic Regression, PCA, Random forest models, Decisiontrees, Support Vector Machine for estimating the risks of welfare dependency.\n\u2022 Identified and targeted welfare high-risk groups with Machinelearning algorithms.\n\u2022 Lead the development and presentation of a data analytics data-hub prototype with the help of the other members of the emerging solutions team.\nEnvironment:\n\nApache, Spark MLlib, TensorFlow, Oryx 2, Accord.NET, Amazon Machine Learning (AML)Python, Django, Flask, ORM, Jinja 2, Mako, Naive Bayes, NLP, SVM, K- means, ANN, Regression', u""Data Scientist\nDana Corporation - Humboldt, TN\nNovember 2014 to February 2016\nDescription:\nDana Incorporated is an American worldwide supplier of drivetrain, sealing, and thermal-management technologies. Founded in 1904 and based in Maumee, Ohio, the company employs nearly than 29,000 people in 34 countries on six continents\n\nResponsibilities:\n\u2022 Worked on loading the data from MySQL to HBase where necessary using Sqoop.\n\u2022 Responsible for developing, support and maintenance for the ETL (Extract, Transform and Load) processes using Informatica Power Center 8.5.\n\u2022 Establish scalable, efficient, automated processes for large scale data analyses, model development, model validation and model implementation.\n\u2022 Designed data profiles for processing, including using python for Data Acquisition and DataIntegrity which consists of Datasets Comparing and Dataset schema checks\n\u2022 Integrated Teradata with R for BI platform and also implemented corporate business rules\n\u2022 Develop necessary connectors to plug ML software into wider data pipeline architectures.\n\u2022 Understanding the client business problems and analyzing the data by using appropriate Statisticalmodels to generate insights.\n\u2022 Applied association rule mining & chaid model to identify hidden patterns and rules in remedy ticket analysis which aid in decision making.\n\u2022 Participated in Business meetings to understand the business needs & requirements.\n\u2022 Arrange and chair Data Workshops with SME's and related stake holders for requirement data catalogue understanding.\n\u2022 Design Logical Data Model which will fit and adopt the Teradata Financial Logical Data Model (FSLDM11) using Erwin data modeler tool.\n\u2022 Present and approve designed Logical Data Model in Data Model Governance Committee (DMGC).\n\u2022 Responsible for data collection, cleansing and ANOVA. Designed technical solution roadmap to deal with noise in sales data.\n\u2022 Used Python to apply time series models, clusteringalgorithm and other data mining methods to explore the fast growth opportunities of our clients\n\u2022 Wrote SQL-Overrides and used filter conditions in source qualifier thereby improving the performance of the mapping.\n\u2022 Formulate and test hypotheses, extract signals from peta-byte scale, unstructured data sets, and ensure that our display advertising business delivers the highest standards of performance.\n\u2022 Lead a multi-functional project team.\n\u2022 Designed and developed mappings using Source Qualifier, Expression, Lookup, Router, Aggregator, Filter, Sequence Generator, Stored Procedure, Update Strategy, joiner and Rank transformations.\n\u2022 Managed the Metadata associated with the ETLprocesses used to populate the Data Warehouse.\n\nEnvironment:\nErwin r, Informatica, ODS, OLTP, Oracle 11g, Hive, OLAP, DB2, Metadata, MS Excel, Mainframes MS Visio, Rational Rose, Requisite Pro, NLP, Hadoop, PL/SQL."", u""Data Analyst\nCompuware Corp - Houston, TX\nMarch 2013 to October 2014\nDescription:\nCompuware is the only software company solely focused on mainframe innovation. We leverage Agile development and DevOps best practices to accelerate customer collaboration and deliver meaningful innovations every 90 days.\n\nResponsibilities:\n\u2022 Developing propensity models for Retail liability products to drive proactive campaigns.\n\u2022 Built predictive scorecards for Cross-selling Car loan, Life Insurance, TD and RD.\n\u2022 Provide guidance and mentoring to team members.\n\u2022 Arrange and chair Data Workshops with SME's and related stake holders for requirement data catalogue understanding.\n\u2022 Responsible for defining the key identifiers for each mapping/interface.\n\u2022 Extraction and tabulation of data from multiple data sources using R, SAS.\n\u2022 Data cleansing, transformation and creating new variables using R.\n\u2022 Responsible for defining the functional requirement documents for each source to target interface.\n\u2022 Scoring predictive models as per regulatory requirements & ensuring deliverables with PSI.\n\u2022 Data modeling and formulation of statistical equations using advanced statistical forecasting techniques.\n\u2022 Design Logical Data Model which will fit and adopt the Teradata Financial Logical Data Model (FSLDM11) using Erwin data modeler tool.\n\u2022 Work with users to identify the most appropriate source of record and profile the data required for sales and service.\n\u2022 Identifying the Customer and account attributes required for MDM implementation from disparate sourcesand preparing detailed documentation.\n\u2022 Document, clarify, and communicate requests for change requests with the requestor and coordinate with the development and testing team.\n\u2022 Document the complete process flow to describe program development, logic, testing, and implementation, application integration, coding.\n\u2022 Present and approve designed Logical Data Model in Data Model Governance Committee (DMGC).\nEnvironment:\n\nR/R Studio, SQL Enterprise Manager, SAS, Microsoft Excel, Microsoft Access, outlook"", u'Python Developer\nDPS Infotech Pvt Ltd - Hyderabad, Telangana\nOctober 2011 to February 2013\nDescription:\nInfinite healthcare technology and IT company provides custom business software solutions, healthcare IT Services, healthcare IT consulting, next-gen mobility solutions and product engineering services\n\nResponsibilities:\n\u2022 Implemented user interface guidelines and standards throughout the development and maintenance of the website using the HTML, CSS, JavaScript and JQuery.\n\u2022 Used Hive queries for data analysis to meet the business requirements.\n\u2022 Involved with advanced CSS concepts and building table-free layouts.\n\u2022 Used Numpy for Numerical analysis for Insurance premium.\n\u2022 Worked on rebranding the existing web pages to clients according to the type of deployment.\n\u2022 Created UI using JavaScript and HTML5/CSS.\n\u2022 Managed a small team of programmers using a modified version of the agile development.\n\u2022 Worked on Jenkins continuous integration tool for deployment of project.\n\u2022 Written backend programming in Python, Used JavaScript and XML to update a portion of a webpage.\n\u2022 Developed and tested many features for dashboard using Python, Bootstrap, CSS, and JavaScript.\n\u2022 Used advanced packages like Mock, patch and beautiful soup (b4) to perform unit testing.\n\u2022 Used Pandas library for statistics Analysis.\n\u2022 Performed troubleshooting, fixed and deployed many Python bug fixes of the two main applications that were a main source of data for both customers and internal customer service team.\n\u2022 Worked on updating the existing clipboard to have the new features as per the client requirements.\n\u2022 Used Python and Django to interface with the JQueryUI and manage the storage and deletion of content.\n\u2022 Performed Unit testing, Integration Testing, GUI and web application testing using Selenium.\nEnvironment:\n\nPython 2.7, Django , HTML5, CSS, XML, Kafka, MySQL, JavaScript, Angular JS, Backbone JS, Nginix server, Amazon s3, Jenkins, Beautiful soup, JavaScript, Eclipse, Git, GitHub, Linux, and MAC OSX.', u""Python Developer\nACN Infotech Pvt Ltd - Hyderabad, Telangana\nMarch 2009 to September 2011\nDescription:\nACN Infotech is a leading worldwide provider of Business Process Management (BPM) Solutions, Business Process Outsourcing (BPO) Solutions and IT Services to small, medium, and large enterprise businesses worldwide\n\nResponsibilities:\n\u2022 Business logic implementation, data exchange, XML processing and graphics creation has been done using Python and Django\n\u2022 Developed UI using CSS, HTML, JavaScript, AngularJS, JQuery and JSON.\n\u2022 Used web applications development using Django/Python, Flask/Python and JQuery, Ajax while using HTML/CSS/JS for server-side rendered application.\n\u2022 Develop UNIX shell scripts and XML configuration files.\n\u2022 Views and Templates were developed with Python and to create a user-friendly website interface Django's view controller and template language is used.\n\u2022 Developed user interface using, CSS, HTML, JavaScript and JQuery&Ruby on rails.\n\u2022 Was involved in environment, code installation as well as the SVN implementation.\n\u2022 Build all database mapping classes using Django models and Cassandra.\n\u2022 A Django dashboard with custom look and feel for end user has been created after a careful study of the Django admin site and dashboard.\n\u2022 JIRA was used to build an environment for development.\n\u2022 Used Pandas API to put the data as time series and tabular format for east timestamp data manipulation and retrieval.\n\u2022 Cleaned data and processed third party spending data into maneuverable deliverables within specific formats with Excel macros and python libraries.\n\u2022 Different testing methodologies like unit testing, Integration testing, web application testing, selenium testing were performed. Used Django framework for application development.\n\u2022 Developed, tested and debugged software tools utilized by clients and internal customers.\n\u2022 Used several python libraries like wxPython, numPY and matPlotLib.\n\u2022 Worked on continuous integration and automation using Jenkins.\n\u2022 Designed and developed data management system using MySQL.\n\u2022 Creating unit test/regression test framework for working/new code.\n\u2022 Coded test programs and evaluated existing engineering processes.\n\u2022 Designed and configured database and back end applications and programs.\n\nEnvironment:\nPython 2.7, Django, Java Script, SQL Server, HTML, DHTML, CSS, Linux, Sub Version, Wing, AJAX.""]",[u'Bachelor of Computer Science in TOOLS AND TECHNOLOGIES'],[u'Informatica Power Center\nJuly 2010'],degree_1 : Bachelor of Compter Science in TOOLS AND TECHNOLOGIES
0,https://resumes.indeed.com/resume/7245658d4ca076b8,"[u""Data Scientist, Customer Insights\nMacy's - New York, NY\nApril 2016 to Present\nDevelop model on customer engagement and loyalty to support marketing strategy\n\u2022 Led POS offer campaign: Build response rate model using logistic regression; Score customers and optimize offer targeting; Evaluate campaign performance; Present campaign performance to different business users using Tableau.\n\u2022 Led customer loyalty segmentation project: Select customer omni-channel shopping behavior features using Variable Clustering; cluster customers based on business rules and k-means algorithm; Score all active customers in Hive; Set up dashboard in Tableau profiling segmentation and monitoring its migration using Sankey Diagram.\n\u2022 Led market basket analysis: Apply association rules to transaction data to find frequent combination of customer purchase and set up the dashboard in R Shiny.\n\u2022 Direct mail model: Apply logistic regression to predict direct mail response and linear regression to predict customer spend."", u'Junior Commerce Analyst\nZiff Davis, LLC - New York, NY\nFebruary 2015 to March 2016\nAnalyze and manipulate data to identify trends, discover anomalies, evaluate shortcomings and highlight successes for over 8 domains\n\u2022 Held commerce weekly meeting to provide business insights and campaign performance to internal commerce team and direct business partners.\n\u2022 Set up dashboards for ""PC Magazine"" to track its daily web performance (Pageviews, Clicks and CTR) using Google Analytics data. Automated data extraction in Python using Google Analytics reporting API and stored data in mysql. Created dashboards using Tableau.\n\u2022 Created database in MySQL storing daily data of all merchants (Commissions, Sales, Clicks) and automated data import from different data source including API, emails and FTP in python.', u""Data Analytics Intern\nGoEnnounce - New York, NY\nNovember 2014 to January 2015\n\u2022 Worked with internal team to understand analytical needs, including identify critical metrics and KPIs needed to support the company's ongoing growth and promoting data initiatives.\n\u2022 Managed data in mysql and regularly queried for business requests.\n\u2022 Provided campaign analysis for all digital advertising campaigns and marketing programs, using Google Analytics and Kiss Metrics.""]","[u'M.A. in Statistics', u'B.S. in Economics']","[u'Columbia University, Graduate School of Arts and Sciences\nSeptember 2012 to February 2014', u'Zhejiang University, College of Economics\nSeptember 2008 to June 2012']","degree_1 : M.A. in Statistics, degree_2 :  B.S. in Economics"
0,https://resumes.indeed.com/resume/01eaacca156ee465,"[u""Data Scientist\nJ B Hunt - Lowell, AR\nJanuary 2017 to Present\nDescription:J.B. Hunt Transport Services, Inc. is a trucking and transportation company that was founded by Johnnie Bryan Hunt,and based in the Northwest Arkansas city of Lowell. J.B. Hunt Transport Services, Inc. was incorporated in Arkansas on August 10, 1961 and originally started with five trucks and seven refrigerated trailers to support the original rice hull business. By 1983, J.B. Hunt had grown into the 80th largest trucking firm in the U.S.\n\nResponsibilities:\n\u2022 This project was focused on customer segmentation based on machine learning and statistical modelingeffort including building predictive models and generates data products to support customer segmentation.\n\u2022 Used Python to visualize the data and implemented machine learning algorithms.\n\u2022 Used R Programming for more statistical analysis\n\u2022 Develop a pricing model for various product & services bundled offering to optimize and predict the gross margin.\n\u2022 Built price elasticity model for various product and services bundled offering.\n\u2022 Developed predictive causal model using annual failure rate and standard cost basis for the new bundled service offering.\n\u2022 Design and develop analytics, machine learning models, and visualizations that drive performance and provide insights, from prototyping to production deployment and product recommendation and allocation planning;\n\u2022 Application of various machine learning algorithms and statistical modeling like decision trees, regression models, neural networks, SVM, clustering to identify Volume using package in Python.\n\u2022 Performed data imputation using Scikit-learn package in Python.\n\u2022 Performed data processing using Python libraries like Numpy and Pandas.\n\u2022 Worked with data analysis using ggplot2 library in R to do data visualizations for better understanding of customers' behaviors.\n\u2022 Experience in using AWS Cloud Services\n\u2022 Experience in using data science relevant technologies likeJupyter Hub\n\u2022 Performed data analysis by using Hive to retrieve the data from Hadoop cluster, SQL to retrieve data from Oracle database.\n\u2022 Experience in Big Data Hadoop, HIVE, PySpark and HDFS\n\u2022 Experience in using Database like MSSQL, Postgres.\n\u2022 Written complex Hive and SQL queries for data analysis to meet business requirements.\n\u2022 Hands on experience in implementing Naive Bayes and skilled in Random Forests, Decision Trees, Linear and Logistic Regression, SVM, Clustering, neural networks, Principle Component Analysis.\n\u2022 Performed K-means clustering, Multivariate analysis, and Support Vector Machines in Python.\n\u2022 Written complex SQL queries for implementing business requirements\n\u2022 Performed Data Cleaning, features scaling, features engineering using pandas and numpy packages in python.\n\u2022 Developed MapReduce pipeline for feature extraction using Hive.\n\u2022 Developed entire frontend and backend modules using Python on Django Web Framework.\n\u2022 Implemented the presentation layer with HTML, CSS, and JavaScript.\n\u2022 Created Data Quality Scripts using SQL and Hive to validate successful data load and quality of the data. Created various types of data visualizations using Python and Tableau.\n\u2022 Prepared Data Visualization reports for the management using R, Tableau, and Power BI.\n\u2022 Work independently or collaboratively throughout the complete analytics project lifecycle including data extraction/preparation, design and implementation of scalable machine learning analysis and solutions, and documentation of results.\n\nEnvironment: R/R studio, SAS, Python, Hive, Hadoop, MS Excel, MS SQL Server, Power BI, Tableau, T-SQL, ETL, MS Access, XML, JSON, MS office 2007, Outlook."", u""Data Scientist\nKelloggs company - Elmhurst, IL\nNovember 2015 to December 2016\nDescription: The Kellogg Company (also Kellogg's, Kellogg, and Kellogg's of Battle Creek) is an American multinational food manufacturing company headquartered in Battle Creek, Michigan, United States. This is the North American Retail Cereal business that includes many of the Company's popular brands such as Kellogg's Special K, Frosted Flakes, and Kashi, our natural brand.\nResponsibilities:\n\n\u2022 Perform Data Profiling to learn about user behavior and merge data from multiple data sources.\n\u2022 Implemented big data processing applications to collect, clean and normalization large volumes of open data using Hadoopecosystems such as PIG, HIVE, and HBase.\n\u2022 Designing and developing various machine learning frameworks using Python, R, and Matlab.\n\u2022 Integrate R into Micro Strategy to expose metrics determined by more sophisticated and detailed models than natively available in the tool.\n\u2022 Worked on different data formats such as JSON, XML and performed machinelearningalgorithms in Python.\n\u2022 Worked as Data Architects and IT Architects to understand the movement of data and its storage and ERStudio9.7\n\u2022 Processed huge datasets (over billion data points, over 1 TB of datasets) for data association pairing and provided insights into meaningful data association and trends\n\u2022 Developed cross-validation pipelines for testing the accuracy of predictions\n\u2022 Enhanced statistical models (linear mixed models) for predicting the best products for commercialization using Machine Learning Linear regression models, KNN and K-means clustering algorithms\n\u2022 Participated in all phases of datamining,datacollection, datacleaning, developingmodels, validation, visualization and performed Gapanalysis.\n\u2022 data manipulation and Aggregation from adifferent source using Nexus, Toad, BusinessObjects, PowerBI, and SmartView.\n\u2022 Independently coded new programs and designed Tables to load and test the program effectively for the given POC's using with Big Data/Hadoop.\n\u2022 Develop documents and dashboards of predictions in Microstrategy and present it to the business intelligence team.\n\u2022 Developed various QlikViewDataModels by extracting and using the data from various sources files, DB2, Excel, Flat Files and Bigdata.\n\u2022 Good knowledge of HadoopArchitecture and various components such as HDFS, JobTracker, TaskTracker, NameNode, DataNode, SecondaryNameNode, and MapReduce concepts.\n\u2022 As Architect delivered various complex OLAPdatabases/cubes, scorecards, dashboards and reports.\n\u2022 Programmed a utility in Python that used multiple packages (scipy, numpy, pandas)\n\u2022 Implemented Classification using supervised algorithms like LogisticRegression, Decisiontrees, KNN, NaiveBayes.\n\u2022 Used Teradata15 utilities such as FastExport, MLOAD for handling various tasks data migration/ETL from OLTP Source Systems to OLAP Target Systems\n\u2022 Handled importing data from various data sources, performed transformations using Hive, MapReduce, and loadeddata into HDFS.\n\u2022 Collaborate with data engineers to implement ETL process, write and optimized SQL queries to perform data extraction from Cloud and merging from Oracle 12c.\n\u2022 Collect unstructured data from MongoDB 3.3 and completed data aggregation.\n\u2022 Perform data integrity checks, data cleaning, exploratory analysis and feature engineer using R 3.4.0.\n\u2022 Conducted analysis of assessing customer consuming behaviors and discover thevalue of customers with RMF analysis; applied customer segmentation with clustering algorithms such as K-MeansClustering and Hierarchical Clustering.\n\u2022 Work on outliers identification with box-plot, K-means clustering using Pandas, NumPy.\n\u2022 Participate in features engineering such as feature intersection generating, feature normalize and Label encoding with Scikit-learn preprocessing.\n\u2022 Use Python 3.0 (numPy, sciPy, pandas, sci-kit-learn, Seaborn, NLTK) and Spark 1.6 / 2.0 (PySpark, MLlib) to develop avariety of models and algorithms for analytic purposes.\n\u2022 Analyze Data and Performed Data Preparation by applying thehistoricalmodel to the data set in AZUREML.\n\u2022 Perform data visualization with Tableau 10 and generate dashboards to present the findings.\n\u2022 Determine customer satisfaction and help enhance customer experience using NLP.\n\u2022 Work on Text Analytics, Na\xefveBayes, Sentiment analysis, creating word clouds and retrieving data from Twitter and other social networking platforms.\n\u2022 Use Git 2.6 to apply version control. Tracked changes in files and coordinated work on the files among multiple team members.\n\nEnvironment: ER Studio 9.7, Tableau 9.03, AWS, Teradata 15, MDM, GIT, Unix, Python 3.5.2,MLlib, SAS, regression, logistic regression, QlikView."", u'Data Scientist\nCentral Business Solutions Inc - Newark, CA\nJanuary 2014 to October 2015\nDescription: Central Business Solutions product FieldWorkMobility is a very handy and feature-rich mobile CRM application coupled with a powerful auto dialer for Android and iOS Phones. The application with its cloud-based centralized remote workforce management system is also a field service or sales management solution for any organization.\n\nResponsibilities:\n\u2022 Worked with data investigation, discovery and mapping tools to scan every single data record from many sources.\n\u2022 Designed the prototype of the Data mart and documented possible outcome from it for end-user.\n\u2022 Involved in business process modeling using UML\n\u2022 Developed and maintained data dictionary to create metadata reports for technical and business purpose.\n\u2022 Created SQL tables with referential integrity and developed queries using SQL, SQL*PLUS and PL/SQL\n\u2022 Experience in maintaining database architecture and metadata that support the Enterprise Dataware house.\n\u2022 Design, coding, unit testing of ETL package source marts and subject marts using Informatica ETL processes for Oracle database.\n\u2022 Developed various QlikView Data Models by extracting and using the data from various sources files, DB2, Excel, Flat Files and Bigdata.\n\u2022 Handled importing data from various data sources, performed transformations using Hive, Map Reduce, and loaded data into HDFS.\n\u2022 Interaction with Business Analyst, SMEs, and other Data Architects to understand Business needs and functionality for various project solutions\n\u2022 Researched, evaluated, architected, and deployed new tools, frameworks, and patterns to build sustainable Big Data platforms for the clients.\n\u2022 Identifying and executing process improvements, hands-on in various technologies such as Oracle, Informatica, Business Objects.\n\u2022 Designed both 3NF data models for ODS, OLTP systems and dimensional data models using Star and Snowflake Schemas.\n\u2022 Co-ordinate with various business users, stakeholders, and SME to get Functional expertise, design, and business test scenarios review, UAT participation, and validation of financial data.\n\u2022 Worked very close with Data Architects and DBA team to implement data model changes in the database in all environments.\n\u2022 Created PL/SQL packages and Database Triggers and developed user procedures and prepared user manuals for the new programs.\n\u2022 Designed and developed Use Case, Activity Diagrams, Sequence Diagrams, OOD (Object-oriented Design) using UML and Visio.\n\nEnvironment:r9.0, Informatica 9.0, ODS, OLTP, Oracle 10g, Hive, OLAP, DB2, Metadata, MS Excel, Mainframes MS Visio, Rational Rose, Requisite Pro. Hadoop, PL/SQL, etc.', u""Data Analyst\nlifeScan Inc - Chesterbrook, PA\nOctober 2012 to December 2013\nDescription: LifeScan, Inc. is a Johnson & Johnson company headquartered in Chesterbrook, PA. LifeScan manufactures and markets Blood Glucose Monitoring Systems like the OneTouch Ultra family for home and hospital use.\n\nResponsibilities:\n\u2022 Interacted with business users to identify and understand business requirements and identified the scope of the projects.\n\u2022 Identified and designed business Entities and attributes and relationships between the Entities to develop a logical model and later translated the model into physical model.\n\u2022 Developed normalized Logical and Physical database models for designing an OLTP application.\n\u2022 Enforced Referential Integrity (R.I) for consistent relationship between parent and child tables. Work with users to identify the most appropriate source of record and profile the data required for sales and service.\n\u2022 Involved in defining the business/transformation rules applied for ICP data.\n\u2022 Define the list codes and code conversions between the source systems and the data mart.\n\u2022 Developed the financing reporting requirements by analyzing the existing business objects reports\n\u2022 Utilized Informatica toolset (Informatica Data Explorer, and Informatica Data Quality) to analyze legacy data for data profiling.\n\u2022 Reverse Engineered the Data Models and identified the Data Elements in the source systems and adding new Data Elements to the existing data models.\n\u2022 Created XSD's for applications to connect the interface and the database.\n\u2022 Compare data with original source documents and validate Data accuracy.\n\u2022 Used reverse engineering to create Graphical Representation (E-R diagram) and to connect to existing database. \\the new reporting needs based on the user with the existing functionality\n\u2022 Also Worked on some impact of low quality and/or missing data on the performance of data warehouse client\n\u2022 Worked with NZ Load to load flat file data into Netezza tables.\n\u2022 Good understanding about Netezza architecture.\n\u2022 Executed DDL to create databases, tables and views.\n\u2022 Generated comprehensive analytical reports by running SQL queries against current databases to conduct data analysis.\n\u2022 Involved in Data Mapping activities for the data warehouse\n\u2022 Created and Configured Workflows, Work lets, and Sessions to transport the data to target warehouse Netezza tables using Informatica Workflow Manager.\n\u2022 Extensively worked on Performance Tuning and understanding Joins and Data distribution.\n\u2022 Experienced in generating and documenting Metadata while designing application.\n\u2022 Coordinated with DBAs and generated SQL codes from data models.\n\u2022 Generate reports for better communication between business teams.\n\nEnvironment: SQL/Server, Oracle9i, MS-Office, Embarcadero, Crystal Reports, Netezza, Teradata, Enterprise Architect, Toad, Informatica, ER Studio, XML, Informatica, OBIEE."", u'J2EE Developer\nMastech Info Trellis - Chennai, Tamil Nadu\nJanuary 2011 to September 2012', u'Java Developer\nCyient Ltd - Hyderabad, Telangana\nJune 2009 to December 2010\nDescription: Cyient is a global solutions provider focused on engineering, manufacturing, data analytics, and networks & operations. Infotech Enterprises Ltd.\n\nResponsibilities:\n\u2022 Involved in Analysis and Design of the project, which is based on MVC (Model-View-Controller) Architecture and Design Patterns.\n\u2022 Created UML Use Cases, Sequence diagrams, class diagrams and page flow diagrams using Rational Rose.\n\u2022 Designed and developed UI using HTML, CSS, JSP and Struts where users have all the items listed for auctions.\n\u2022 Involved in developing prototypes of the product.\n\u2022 Involved in writing Detail Design Documents with UML Specifications.\n\u2022 Implemented Socket Programming to communicate with all the customers.\n\u2022 Developed Authentication and Authorization modules where authorized persons can only access the inventory related operations.\n\u2022 Developed Controller Servlets, Action and Form objects for process of interacting with Oracle ADF database and retrieving dynamic data.\n\u2022 Responsible for coding SQL Statements and Stored procedures for back end communication using JDBC\n\u2022 Used Net BeansIDE to develop the application.\n\u2022 Wrote JavaScript validations on the client side.\n\u2022 Involved in unit testing and system testing and also responsible for preparing test scripts for the system testing in UNIX Environment.\n\u2022 Responsible for packaging and deploying components in to the JBoss Application Server.\n\nEnvironment: Java, Java Beans, JSP, JavaScript, Servlets, JDBC, Net Beans, JBoss, XML, HTML, Struts, WSDL, Oracle.']","[u""Bachelor's""]",[u''],"degree_1 : ""Bachelors"""
0,https://resumes.indeed.com/resume/9842275b1982e3df,"[u'Manager\nLucky Carwash\nApril 2017 to Present\n\u2022 Took over day-to-day handling of the self-service car wash, including:\n\u25e6 Inventory management, finance, customer service, and contractor management\n\u2022 Had record breaking months from June to August', u'Data Scientist\nTrueAccord\nNovember 2015 to March 2017\n\u2022 Implemented monitoring/controls system using a combination of Python, Google BigQuery (GBQ), and DataDog\n\u25e6 Reduced outbound communication system debugging from ~1.5 weeks to a few hours\n\u25e6 Surfaced compliance risk issues - leading to post mortems/deep dives that led to product improvements (including email tagging/handling for white labeled products)\n\u2022 Designed, implemented, and analyzed experiments\n\u25e6 Worked with product team on content experiments, leading to 25% increase customer engagement\n\u25e6 Did major work on the effects of different payment offers (e.g. payment plan lengths) on collections at various time periods\n\u2022 Extended HeartBeat, the decision making software (written in Python), to support product improvements', u""Solutions Analyst\nLeanTaaS\nJune 2014 to September 2015\n\u2022 Leading Hospital's Risk Management Team\n\u25e6 Collaborated with designers/engineers to create mockups and prototypes\n\u25e6 Created the specs and product roadmap for the first development cycle (~14 weeks), culminating in a successful launch\n\n\u2022 Call Center for Pharmaceutical Services Company\n\u25e6 Created and maintained forecasting scripts (in R) to manage the # of call center service reps\n\u25e6 Wrote python scripts for data transformation/cleaning that were part of the ETL jobs\n\n\u2022 Top 10 Children's Clothes Retailer\n\u25e6 Wrote python script to calculate ~20 financial metrics to reduce QA time""]",[u'B.A. in Applied Mathematics in Economics'],"[u'University of California, Berkeley Berkeley, CA\nMarch 2010 to September 2013']",degree_1 : B.A. in Applied Mathematics in Economics
0,https://resumes.indeed.com/resume/54dfd144626446a3,"[u""Data Scientist\nPillow Homes\nMay 2016 to Present\nDeveloped a deep similarity network using TensorFlow based on VGG and ResNet to solve in-class\nsimilarity problem from two groups of real world images, tuned the model to reach an f1 score close to 0.85 by using image augmentation and up-sampling.\n\u2022 Built parallel crawlers with proxy network to efficiently crawl 98% of all public listings (about 100\nmillion pages) from several short-term rental platforms and uploaded data into AWS RDS\n\u2022 Built a a price calculator to recommend the best rental price for Pillow's customers\n\u2022 Use LDA to automatic generate a large set of labeled images from their descriptions\n\u2022 Perform sentiment analysis for review data and rank the reviews"", u""Data Scientist\nIterative Inc\nJanuary 2015 to April 2016\n\u2022 Built automatic decision making system for retail stores(Verizon) to make management decisions with location based sensor data, increased the sales of accessories by 30%\n\u2022 Machine learning: Developed Location based Model to predict user behavior over 500,000 minutes of raw\nsensor data and 400 days of sales data with ENN, achieving AUC 0.72\n\u2022 Feature engineering: Performed detailed feature engineering for data sources from customer's position,\npath and interactions with sales representatives to address significant factors for transitions.\nCourses & Projects\nSelf-Driving Car Engineer Nano Degree - Udacity\nLearned and deployed code to control a real self-driving car in a test track\nSkills: Tensorflow, Keras, Yolo, ROS, UKF, Particle Filter, PID, behavior planning, FCNN\nData Analyst Nano Degree - Udacity\nData wrangle Open street Maps Data use MongoDB, visualization with D3, A/B test\nSkills: Python, R, MongoDB, Machine Learning, A/B Test, HTML, CSS, D3\n\n\u2022 Database Management Created online post forum 'Quill And Inkpot' with MySQL\n\u2022 Operating System Implemented Priority Scheduling in Nachos using Java\n\u2022 Distributed System Implemented a secure distributed ATM systems with Java\n\u2022 Computer Vision Compared image compression algorithm using structural similarity\n\nHonors and Awards\nOct. 2009 Champion, RoboGame Competition, USTC (rank 1)\nSep. 2007 First Prize, Chinese Physics Olympiad (provincial level), China (top 0.1%))""]","[u'M.S. in Statistical Physics', u'B.S. in Physics']","[u'University of Georgia Athens, GA\nAugust 2013 to December 2015', u'University of Science and Technology of China Hefei, CN\nSeptember 2008 to July 2012']","degree_1 : M.S. in Statistical Physics, degree_2 :  B.S. in Physics"
0,https://resumes.indeed.com/resume/371fa5dddd9328ea,"[u'Senior Data Scientist\nMetabiota - San Francisco, CA\nJanuary 2016 to Present\n- Experienced in the creation of stochastic catalogs for several categories of infectious disease\n\n- Streamlined technical processing of millions of simulations, resulting in over 100+ TB of data\n\n- Prepared agile prototyping of product features using R-Shiny\n\n- Mined public health literature and publicly available line-lists to fit probability distributions to key model parameters', u""Statistician\nState Farm - Bloomington, IL\nJanuary 2012 to January 2016\n- Strong programmer who finished as one of the selected winners of State Farm's Hack Day, a programming competition with over 400 participants\n\n- Most experienced in multivariate regression modeling (logistic and glm) and DOE, as well as cluster analysis, classification & decision trees, discriminant analysis, principal components, quasi-experimental designs"", u'Actuarial Analyst\nMercer Health & Benefits - Chicago, IL\nJune 2010 to January 2012\nConsulted large employers on their group medical, pharmaceutical, and dental plans.']","[u'Master of Science in Statistics', u'Bachelor of Science in Actuarial Science']","[u'University of Illinois Urbana, IL\nJanuary 2013', u'University of Nebraska Lincoln, NE\nJanuary 2010']","degree_1 : Master of Science in Statistics, degree_2 :  Bachelor of Science in Actarial Science"
0,https://resumes.indeed.com/resume/f55fd2778ed4f2e8,"[u'Data Scientist\nCitiGroup Center - Tampa, FL\nNovember 2017 to Present\nProject Name-: Case Review Automation(CRA)\n\u2022 Worked on the CRA(Case Review Automation) Project with Citi Bank to identify AML transactions.\nPackage used was Tensorflow.\u2022\u2022\n\u2022 Involved in Data Extraction activities for the analysis using SAS, SQL Developer.\u2022\n\u2022 \u2022\u2022\u2022 Developed the code in the R software to identify the transactions and classified them as structuring and Non-Structuring pattern. Packages used are NumPy, Pandas and Scikit.\u2022\nDeveloped the code in Python software to connect the database with the Google APIs to extract the coordinates of the location of the transaction using Anaconda tool. Package used was JSON.\u2022\n\u2022 \u2022Developed the Apriori Algorithm in R software to identify the relationship between the\nAmount of the transaction and the type of the transaction. Timeseries Algortihm is also used.\u2022', u'Data Scientist at Kaligia Biosciences, Florida, US\nNon-Invasive Glucose Device\nJune 2017 to October 2017\nProject Name-: Non-Invasive Glucose Device\n\u2022 Develop a model to predict the Glucose levels in the human body through Raman Spectroscopy on the basis of Patient vital characteristics and spectroscopic data. The signals collected at the\ndifferent wavelengths are preprocessed and the intensities are calculated. The softwares used are\nR, Python and Excel for data cleaning and data analysis.\n\u2022 Worked on the Linear and Non-Linear models and used techniques for feature selection like PCA,\nRFE and Ensembling to improve the accuracy of the model.\n\u2022 Developed intricate algorithms based on deep-dive statistical analysis and predictive data modeling\nthat were used to deepen relationships, strengthen longevity and personalize interactions with customers.\n\u2022 Developed audience extension models relying on decision trees, random forest, Support\nVector, clustering.', u'Data Science Intern at Kaligia Biosciences, Florida US\nJanuary 2017 to May 2017\n\u2022 Analyzed and processed complex data sets using advanced SAS, querying, Tableau and analytics tools.\u2022\u2022\n\u2022 \u2022 Identified, measured and recommended improvement strategies for KPIs across all business areas.\u2022\u2022\n\u2022 Mentored organization on large scale data and analytics using advanced statistical and machine\nlearning models.\u2022\u2022\nProject: RAPDRP (Restructured and Accelerated Power Development and Reforms Programme) \u2022', u'Senior Data Scientist\nHCL Technologies - IN\nAugust 2014 to December 2015\n\u2022 Developed dashboards, ad-hoc reports, drill-down reports, standalone jobs, optimized queries, fine- \u2022 Tuned backend application using Oracle-SQL, SAS9.4, Excel and Tableau, Qlikview.\u2022\n\u2022 Involved in Data extraction from the application system and rectifying the data, Data blending as \u2022 per the business requirements using Oracle SQL, PL/SQL, SAS9.4, MS-Excel and Tableau.\u2022\n\u2022 \u2022 Transformed and manipulated data for building models using SAS-EM, R and Python.\u2022\n\u2022 Created and analyzed reports, data models to provide business decisions. Technologies used\nare SAS, MS-Excel, R.\u2022', u'Data Scientist\nHCL Technologies - IN\nJanuary 2013 to July 2014\nUnderstand the data and design models using SAS, R and Python.\u2022\n\u2022 \u2022 Analyzed and Cleaned the data before selecting the model using SAS, R and Python.\u2022\n\u2022 Provided Root Cause Analysis, hot fixes, cut-over tasks during go-live phases accompanying the AGILE methodologies. Technologies used are PL/SQL languages.\u2022']","[u'Master of Science in Management Information Systems', u'Bachelor of Technology in Computer Science']","[u'University of South Florida Tampa, FL\nMay 2017', u'Northern India Engineering College, Guru Gobind Singh Indraprastha University\nMay 2012']","degree_1 : Master of Science in Management Information Systems, degree_2 :  Bachelor of Technology in Compter Science"
0,https://resumes.indeed.com/resume/54346a442b57bf2d,"[u'Big Data Developer\nEquifax Inc - Alpharetta, GA\nAugust 2015 to Present\nWork closely with team at Chile and Ireland to develop, delivery and support batch/online big data platform solutions for credit bureau data exchange at Equifax.\n\u2022 Batch layer platform solutions for credit data ETL, attribution, and scoring using Spark, Hadoop, Cassandra, Scala, Java.\n\u2022 Online layer platform solutions for real-time data ingestion using Kafka, Zookeeper, Avro, Hadoop.\n\u2022 Developed a spark job server (REST API, Spring boot, ORACLE DB) and spark job shell for spark job submission, job profile storage, job data (HDFS) query/monitoring.\n\u2022 Integration tests for Spark job server for job submission based on groovy Spock framework. Developed unit tests cases for Spark job shell development.\n\u2023 PCI compliance, sensitive data security/protection, developed/delivered a tokenization API solution using Protegrity platforms for Equifax BUs in US.\n\u2023 Master of ThoughtWorks GoCD server for automated CD and Testing pipelines in SDLC, continuous delivery strategy, blue/green (zero downtime) deployment.', u'Data Scientist/Physicist\nGeorgia State University - Atlanta, GA\nSeptember 2012 to August 2015\nCollaboratively work with an international team of physicists on the RHIC-PHENIX experiment for data acquisition, ETL, detector R&D, detector construction/beam test. Linux system administrator for the nuclear data center and web server at GSU, Atlanta.\n\u2023 Develop/implement software modules/APIs based on a large-scale software framework for nuclear data production and analysis. Write small code/macros for nuclear data ETL, data visualization, and graphical presentation.\n\u2023 Nuclear data statistic analysis and detector Monto Carlo simulation based on regressions, pattern recognition, and classification. Work with team to construct nuclear detector and perform beam test at Fermi Lab.\n\u2023 Nuclear data center linux system administration (RHEL/CentOS), distributed computing, apache/mysql web server system administration.', u'Visiting Scientist\nBrookhaven National Lab - Upton, NY\nSeptember 2009 to June 2011\nWork with an international team of physicists on the RHIC-STAR experiment for nuclear data acquisition/QA, ETL, particle classification, real-time data sampling/monitoring.\n\u2023 Develop, support High Level Trigger (HLT) software sampling tools for real-time nuclear data selection, ETL, data visualization/monitoring.\n\u2023 Analyze (particle reconstruction/classification) the antimatter nuclear data selected by the High Level Trigger, write technique note, and publish. Observed 18 antimatter helium-4 nucleus (heaviest antimatter discovered) the first time in the world, and the results was published by Nature Magazine (PhD Thesis).']","[u'Ph. D in nuclear physics', u'BS in physics']","[u'Chinese Academy of Science \u4e0a\u6d77\u5e02\nJanuary 2007 to January 2012', u'Shandong University\nJanuary 2003 to January 2007']","degree_1 : Ph. D in nclear physics, degree_2 :  BS in physics"
0,https://resumes.indeed.com/resume/ff5a816adf8cabde,"[u'Data Scientist\nAviall - Dallas, TX\nJanuary 2017 to Present\nResponsibilities:\n\u2022 Played key role in optimizing and benchmarking the Classification models in order to standardize the results across different departments.\n\u2022 Applied advanced statistical and predictive modeling techniques to build, maintain, and improve on multiple real-time decision systems. Conducted advanced data analysis and developed complex algorithms.\n\u2022 Built models using Statistical techniques and Machine Learning classification models like XG Boost, SVM, and Random Forest. Developed and design advanced predictive analysis models. Model and frame business scenarios that are meaningful and impact critical business processes and/or decisions.\n\u2022 Worked with Big Data Technologies such Hadoop, Hive, MapReduce. Extracted data from HDFS and prepared data for exploratory analysis using data munging. Designed experiments, tested hypothesis, and built models.\n\u2022 Developed MapReduce/Spark Python modules for machine learning & predictive analytics in Hadoop on AWS. Implemented a Python-based distributed random forest via Python streaming.\n\u2022 Worked extensively with AWS services like EC2, S3, VPC, ELB, AutoScalingGroups, Route 53, IAM, CloudTrail, CloudWatch, CloudFormation, CloudFront, SNS, and RDS.\n\u2022 Wrote Ansible Playbooks with Python SSH as the Wrapper to Manage Configurations of AWS nodes and Tested Playbooks on AWS instances using Python.\n\u2022 Perform data/systems analysis to determine best BI solution (reports, dashboards, scorecards, data cubes, etc) using Tableau.\n\u2022 Develop load scripts for extracting, transforming and loading data into Tableau applications.\n\u2022 Design and develop new interface elements and objects as required, developed Macros, SET ANALYSIS to provide custom functionality using Tableau\n\u2022 Wrote scripts in Python using Apache Spark and ElasticSearch engine for use in creating dashboards\n\u2022 Developed and presented clear concise recommendations outlining alternatives and key decision criteria. Prepared graphs using GGplot library and Tableau for an overview of the analytical models and results.\n\u2022 Handled importing data from various data sources, performed transformations using Hive, Map Reduce, and loaded data into HDFS.\nEnvironment: AWS (S3, EC2), Python (Scikit Learn), Tableau, Tensorflow, Linux Systems(Ubantu), Hive, MongoDB, SQL, Apache Spark, Apache Hadoop. Major Model tested: Neural Networks, SVM, Logistic Regression, k-Nearest Neighbor (kNN): Decision Tree. Ensemble Trees: Random Forest, GBMboost, XGboost', u'Data Scientist\nRare Genomics Institute - Hanover, MD\nJune 2014 to December 2016\nResponsibilities:\n\u2022 Developed computational and data science solutions for the storage, management, analysis, and visualization of genomic data.\n\u2022 Leveraged existing tools and publicly available genomics data to develop, test, or implement bioinformatics pipelines.\n\u2022 Extracted patent text and numerical features with python library Beautiful Soup, created Decision Tree algorithm to predict the patent classification on their Diseases.\n\u2022 Detected the near-duplicated news by applying NLP methods (e.g. word2vec) and developing machine learning models like label spreading, clustering\n\u2022 Provided expertise in statistical methods or machine learning with the goal of applying these techniques to health data.\n\u2022 Worked with Mobile Science 2.0, Mobile App teams to build a Classifier for Mobile App users that could be used by the digital marketing team to tailor specific messages to groups of users.\n\u2022 Used regulatory genomics/epigenetics & computational approaches in genetics and Patient data to perform clustering to group patients with similar diseases.\n\u2022 Algorithms implemented in Python, SQLite, Hadoop, MapReduce, MongoDB, R.\n\u2022 Worked with Big Data Technologies such Hadoop, Hive, MapReduce.\n\u2022 Handled importing data from various data sources, performed transformations using Hive, MapReduce, and loaded data into HDFS.\n\u2022 Created complex formulas and calculations within Tableau to meet the needs of complex business logic.\n\u2022 Combined Tableau visualizations into Interactive Dashboards using filter actions, highlight actions etc., and published them to the web.\n\u2022 Developed various data connections from data source to Tableau Desktop for report and dashboard development\nEnvironment: NLP, Python, Hadoop, MapReduce, Tableau, Spark, Hive, R. Major models tested: K-Means Clustering, SVM, Decision Tree based models: CART, CHAID, Information Gain, Random Forest', u'Data Scientist\nCeridian HCM Inc - Minneapolis, MN\nJuly 2012 to May 2014\nResponsibilities:\n\u2022 Worked on data cleaning and reshaping, generated segmented subsets using numpy and Pandas in Python. Developed Python scripts to automate data sampling process. Ensured the data integrity by checking for completeness, duplication, accuracy, and consistency.\n\u2022 Worked on model selection based on confusion matrices, minimized the Type II error. Generated cost-benefit analysis to quantify the model implementation comparing with the former situation.\n\u2022 Wrote and optimized complex SQL queries involving multiple joins and advanced analytical functions to perform data extraction and merging from large volumes of historical data stored in Oracle 11g, validating the ETL processed data in target database.\n\u2022 Conducted model optimization and comparison using stepwise function based on AIC value.\n\u2022 Applied various machine learning algorithms and statistical modeling like decision tree, logistic regression, Gradient Boosting Machine to build predictive model using scikit-learn package in Python.\n\u2022 Continuously collected business requirements during the whole project life cycle. Identified the variables that significantly affect the target\nEnvironment: Decision Tree, Logistic regression, Hadoop, Teradata, Python, MLLib, SAS, random forest, OLAP, HDFS, NLTK, SVM, JSON and XML', u'Data Analyst\nHornet InfoTech - Hyderabad, Telangana\nJuly 2010 to June 2012\nResponsibilities:\n\u2022 Ran SQL queries in Oracle database to analyze and manipulate data. Wrote SAS programs to performed ad-hoc analysis and data manipulation.\n\u2022 Created various SAS Reports, Tables, Graphs and Summary analysis on PMS systems being used in these properties.\n\u2022 Transferred data from Oracle database as well as MS Excel into SAS for analysis and used filters based on the analysis.\n\u2022 Used SAS Import/Export Wizard as well as SAS Programming techniques to extract data from Excel.Used SAS Base programming as well as SAS Enterprise Guide 4.0 to produce various reports, charts and graphs\n\u2022 Participated in the technology support team meeting to coordinate, review and determine appropriate hotel property software system for hotel property\nEnvironment: SAS, SQLite, Hadoop, MapReduce, SQL, MS Excel.', u'Java Developer\nSunGard Solutions - Hyderabad, Telangana\nMarch 2008 to June 2010\nResponsibilities:\n\u2022 Actively participated in all the phases of SDLC including Requirements Collection, Design & Analysis of the Customer Specifications, Development and Customization of the application.\n\u2022 Developed the application using Agile/Scrum methodology which involves daily stand ups. Test driven development, continuous integration, demos and test automations.\n\u2022 Strong hands-on knowledge of Core JAVA, Web-Based Application, and OOPS concepts.\n\u2022 Developed Client Side technologies using HTML, CSS, and Java Script. Developed Server Side technologies using spring, Hibernate, Servlets/JSP, Multithreading.\n\u2022 Extensively worked with the retrieval and manipulation of data from the Oracle database by writing queries using SQL and PL/SQL. Web application development by Setting up an environment, configuring an application and Web Logic Application Server.\n\u2022 Hands on Experience in coding, unit testing, Integration testing and Bug fixing.\nEnvironment: Oracle/SQL Server and PL/SQL, spring, Hibernate, Ant, Apache, Tomcat, JBOSS, Web logic, UNIX, RDBMS, HTML, CSS, Java Script, JDBC, Eclipse, Multithreading.']","[u'Master of Science in Computer Science', u'Bachelor of Technology in Computer Science and Engineering']","[u'Indiana State University', u'JNTU']","degree_1 : Master of Science in Compter Science, degree_2 :  Bachelor of Technology in Compter Science and Engineering"
0,https://resumes.indeed.com/resume/5aee562216d4962a,"[u""Freelance Data Scientist\nHarrisburg University\nApril 2017 to Present\nTools and techniques used: R, Python, Hypothesis Testing, Unsupervised Learning, Supervised Learning, PCA, Sentiment Analysis, Deep Learning, Exploratory Data Analysis\nAccomplishments:\n\u2022 Developed a deep neural net using TensorFlow in Python for detection of exotic particle signals from background noise on a Higgs Boson simulated dataset with test set accuracy of 70 percent.\n\u2022 Performed feature selection and correlation analysis and identified the feature variables critical to detect exotic particle signals from background noise & developed a deep neural net using H2O API library on Higgs Boson dataset with model accuracy of 70 percent.\n\u2022 Performed data pre-processing and built an artificial neural network model using Keras framework in Python for a bank dataset to predict the ratio of incoming and outgoing customers, with a model accuracy of 85 percent.\n\u2022 Developed clustering and multilinear regression model on crime dataset and found negative relationship between influx of immigrants and increase in crime rates in the US communities.\n\u2022 Developed a model using Syuzhet package in R to compare the emotional variability of William Shakespeare's novels and identified the most common emotion and its variance over time.\n\u2022 Calculated lexical diversity, positive and negative ratings of speeches of present and former first ladies of the US using VADER module from the NLTK and performed visual representations of these ratings using matplotlib in Python.\n\u2022 Identified the scope of research on parameters influencing energy consumption of individual houses with the help of Nest sensor.\n\u2022 Performed hypothesis testing and developed a linear regression model in R to explore the critical variables associated with energy consumption."", u'Lead Process Engineer\nIBM T.J Watson Research Center\nJuly 2015 to March 2017\nTools and Techniques used: MS Excel, Optical Modelling, Data Collection, Data Preprocessing, Exploratory Data Analysis, Predictive Analysis, Operations Management\nResponsibilities and accomplishments:\n\u2022 Modified wafer thickness measurement models for new products by identifying outliers and false positives and achieved 100 percent model accuracy for measurements of resistive memory devices.\n\u2022 Led the process optimization of wafers for 3-D integration and quantum processors, through accurate measurements, identifying outliers and quantitative data analysis and obtained 99 percent reproducibility rate within 6 months of operation and implementation.\n\u2022 Responsible for documentation and modification of process flow charts and recipes at Materials Research Laboratory.\n\u2022 Mentored and trained new employees on research projects and equipment troubleshooting and helped them getting assimilated into the company culture more effectively.\n\u2022 Supervised waste and expired stocks in the labs and coordinated with waste removal team which resulted in cost cutting on future inventories.', u""Research Consultant\nBess Technologies, LLC - Albany, NY\nAugust 2014 to July 2015\nTools and Techniques used: MS Excel, Problem Identification, Graph Analytics, Data Generation, Spectral Modelling, Predictive Analysis\nResponsibilities and accomplishments:\n\u2022 Designed process improvement projects for a cutting-edge battery technology development and performed experimentation, data collection, data pre-processing, data visualization and qualitative analysis, and increased the life-time of Li-ion batteries by 40 percent in 6 months.\n\u2022 Facilitated the process development steps through quantitative data analysis of spectral models obtained during spectroscopic experiments.\n\u2022 Led the company's vision in process reproducibility and innovation and initiated the prototype to production process.\n\u2022 Primary point of contact for the development team and management team."", u'Graduate Scientist\nSUNY Polytechnic Institute - Albany, NY\nAugust 2012 to August 2014\nTools and Techniques used: Problem Identification, Weibull ++, Origin, Curve Fitting, Hypothesis Testing, Data Generation, Inferential Analysis\nResponsibilities and accomplishments:\n\u2022 Identified the product failure problem associated with transistors and conducted frequent meetings with device manufacturer, lab co-workers, other tool owners and device characterization experts, arriving with a way to design experiments for solving failure issues in new generation semiconductor products.\n\u2022 Identified the reliability problem associated with new generation semiconductor devices and drafted a problem statement followed by experimentation with novel parametric control and techniques.\n\u2022 Conducted experiments for electrical and reliability characterization and gathered data, performed data cleaning, feature selection and quantitative analysis for hypothesis testing.\n\u2022 Found the physical mechanism of product failure by formulating new constants to fit experimental data into reliability models and interpreting reliability physics using data visualization.\n\u2022 Coordinated with technical team and client and liaised them to come with an action plan for joint research project on solder encapsulation.', u'Graduate Research Assistant\nLouisiana Tech University - Ruston, LA\nSeptember 2011 to May 2012\nTools and Techniques used: Productionalization, Hypothesis Testing, Problem Identification and Exploration\nResponsibilities and accomplishments:\n\u2022 Assisted professors in supporting 80-students in organic and physical chemistry labs, assisting them in experimentation, identifying issues in the laboratory and finding resolution for each problem along the way.\n\u2022 Assisted students in assessing potential risks in the experimental process, informing them of potential problems and dangers, while teaching them how to manage these problems in avoiding problematic situations and minimizing risk in the laboratory space.\n\u2022 Developed a technique from white paper to enhance the functionality of gold nanoparticles, which finds its applications in medical imaging technology.', u'Data Scientist\nNational Institute of Technology - Raipur, Chhattisgarh\nJuly 2009 to June 2011\nSkills used: Project Development, Data Collection, Data Visualization, Data Analysis, Patent Application, Research Paper Draft and Publication\nKey Accomplishments:\n\u2022 Designed and developed an analytical and chemical technique for extraction, purification and characterization of bioactive compounds from coconut extract.\n\u2022 Applied for a patent and published two research papers for discovery of bioactive compounds from waste products of coconut.\n\u2022 Demonstrated leadership and management skills, representing class in department for 3 consecutive years, advocating for student needs and concerns.']","[u'MS in Materials Science', u'BTech in Biotechnology', u'MS']","[u'University at Albany Albany, NY\nJanuary 2014', u'National Institute of Technology Raipur, Chhattisgarh\nJanuary 2011', u'Harrisburg University Harrisburg, PA']","degree_1 : MS in Materials Science, degree_2 :  BTech in Biotechnology, degree_3 :  MS"
0,https://resumes.indeed.com/resume/36bb982c719590e6,"[u'Data Scientist\nIndependent - Seattle, WA\nJune 2017 to Present\n\u2022 Performed support and data analysis for independent projects and the research into machine learning and data visualization\n\u2022 Ranked Top 9% for 2017 Kaggle Featured Prediction Competition:Sberbank Russian Housing Market', u'Supply Chain Analyst\nHerald Datanetics Limited - Guangdong, CN\nSeptember 2013 to May 2015\nChina\n\u2022 Gathered historical supply chain data and analyzed results to identify trends and root cause for 2 products\n\u2022 Collaborated with internal and external teams to schedule transportation for indirect materials and manage inventory levels\n\u2022 Initiated strategies to improve customer satisfaction, safety, quality, efficiency and on-time delivery in a team of 5 people']","[u'MS in Information in Management', u'BS in Business Administration in Logistics Management and Economics']","[u'University of Washington, Information School Seattle, WA\nSeptember 2015 to June 2017', u'Ohio State University, Max M. Fisher College of Business Columbus, OH\nSeptember 2010 to August 2013']","degree_1 : MS in Information in Management, degree_2 :  BS in Bsiness Administration in Logistics Management and Economics"
0,https://resumes.indeed.com/resume/2c4dcf039cfbc982,"[u""Chief Data Scientist\nEli Lilly and Company - Indianapolis, IN\nMarch 2013 to Present\nEli Lilly and Company is an American global pharmaceutical company with annual revenue of over $28 billion. Lilly is considered in the top 10 pharmaceutical companies across the globe for pharma sales. Being a pharma company, most of the statisticians deal with the clinical trial data for different types of drugs development. Worked as a Visualization specialist solving various visualization problems as well as worked on several projects with Machine Learning approach as well as Bayesian approach (Automatic Diabetic Retinopathy Detection, BATMAN, Prior-Elicitation, Referral network, Treatment Specify Develop Tool (typical subgroup problem))\n\nAdvanced Analytics and CMDR:\nAdvance Analytics team to improve integration of statistical tools and systems obtains and maintains breadth and depth of expertise in tools and technologies in the statistical analysis space, focusing on visual analytics and it use in the evaluation of clinical data.\nChatBot for FAQ's:\nDeveloped FAQ's chatbot in python using Ask - Tell Model using NLU (Natural Language Understanding).\nThe implementation of this bot is built on RASA- NLU.\n\nRecommendation Systems:\nDeep ranking systems architecture using candidate generation and ranking.\n\nBATMAN:\ninvolved the development of tool for performing Bayesian Network Meta-Analysis in a regulated environment. The tool has a capability of analyzing most of the generalized linear modeling framework for pairwise and network meta-analysis of randomized controlled trials. Lilly internal R package is been developed in agile scrum environment. The tool is been used by most of the statisticians in the third phase of a drug development life cycle. This project helps to save approximately 6 months of the drug development process, which can help to reduce the time frame for patent filing and eventually in billions of profits.\nOne of the other projects I worked on involved building a referral network shiny app using neural networks. This project was initiated to help marketing team to push Lilly product to key leaders in the regions. This helped Lilly to improve their sales by 60%. Same project is been used to increase sales of different products.\n{More details cannot be provided due to company policies and confidentiality purposes}\n\nAutomatic Diabetic Retinopathy Detection:\nDiabetic retinopathy is an eye disease affecting diabetes suffers. It has few early-warning signs and left untreated it can cause blindness. Regular screening is recommended for at risk patients. The current standard is manual grading of retinal fundus images. However, there is a shortage of trained professionals and very high variability. If ophthalmologist graders do manual grading on retinal images only 60-65% of consistency can be achieved.\nAutomatic detection of diabetic retinopathy could have benefits increasing diagnostic availability to patients, decreasing human variability in outcomes, and lowering the time for diagnotic analysis. Applied the task automating diabetic retinopathy grading using a neural network model. Neural networks are powerful models that can be trained to perform a wide variety of tasks. They are well suited to leanring from large amounts of data like images. This network is based on a convolutional neural network architecture and trained on over 30,000 hand-labeled images. Used TensorFlow, Keras and Python tools for implementation.\n\nResponsibilities:\n\u2022 Went through data cleaning and manipulation phase on labeled and unlabeled image data set.\n\u2022 Handled unbalanced data set problem such as models were not learning and label imbalance issues.\n\u2022 Several resampling methods were implemented.\n\u2022 Overfitting issue was present when model failed to generalize using resampled data. Data augmentation, batch normalization, 12 norm, dropout helped to overcome this issue.\n\u2022 Resnet algorithm was used which uses smaller network\n\u2022 Used Keras for implementation and trained using cyclic learning rate schedule.\n\u2022 Using cyclic learning rate automatic schedule was implemented for training in three cycles for about 20 hours of training time.\n\u2022 Accuracy, Kappa, precision and F1 score were calculated for comparing the results of four different algorithms: Na\xefve, Resampled, weighted and Resnet. About 80% accuracy was achieved using Resnet\n\u2022 Developed SDTM Code List Conversion Tool for its intended use as per the Computer Systems and Electronic Records; Electronic Signatures (LQS302) procedure.\n\u2022 Build dynamic UI using React js..\n\u2022 Performed Exploratory Data Analysis using R.\n\u2022 Developed numerous dashboards and stories using tableau.\n\u2022 Prototype statistical models for POC (Proof of Concept).\n\u2022 Performed Data Cleaning, features scaling, features engineering.\n\u2022 Participated in design, development, and optimization of code in R.\n\u2022 In real-time association rules were implemented which uses prior probabilities.\n\u2022 Performed Data Mining in R (TM package, LSA package) using SAP HANA platform.\n\u2022 Developed backend APIs and services for internal and external consumption.\n\u2022 Developed Performance metrics to evaluate Algorithm's performance.\n\u2022 Performed unit testing, functional and user-acceptance testing.\n\u2022 Generated several reports in RMarkdown for submissions.\n\u2022 Performed data manipulation and data cleaning using R and Python.\n\u2022 Improved the efficiency and performance of the application by discovering the memory leaks & redundant codes and implementing multi-threading at various critical places.\n\u2022 Developed many Shiny Applications.\n\u2022 Implemented Spring batch with quartz scheduler framework.\n\u2022 Implemented Role-Based authentication using Spring Security.\n\u2022 Generated reports using Altova Mapforce.\n\u2022 Dynamically generating graphical PDF reports using IText1.1 and excel reports using apache POI.\n\u2022 Developed a business process to dynamically generate data using Oracle BI publisher.\n\u2022 Generated reports from the database using PL/SQL and SQL.\n\u2022 Created analytics reporting charts utilizing d3.js\n\u2022 Developed and modified Sharepoint user privileges as directed and in compliance with all standard operating procedures.\n\u2022 Assisted in designing and development of requirements and statistical models.\n\u2022 Went through the data cleaning and manipulation phase on clinical trial data sets of different drugs\n\u2022 R functions were written for data piping and manipulation before data was feed inside The Bayesian models for meta-analysis\n\u2022 Included several likelihood models such as Normal, Binomial, TTE, cLogLog, survival models, Poisson etc.\n\u2022 Data parsing was done using DOCOPT package.\n\u2022 MCMC sampling was implemented using JAGS sampler.\n\u2022 WINBUGS code was included for data processing and model implementation\n\u2022 Several visualizations (density plots, forest plots, leverage plots, network plots, covariant adjustment plots etc) were made using packages such as GGPLOT2, GGMCMC, animation etc\n\u2022 Customized reports and presentations were generated autonomously using tool for different models using r packages e.g. rmarkdown, animation, knitr, ReporteRs etc\n\u2022 Eventually everything was put in a package for Lilly internal use.\n\u2022 Tool was tested under system testing and user acceptance testing in a regulated environment.\n\nEnvironment: TensorFlow, Keras, Python, HPC, R, Matlab, HPC, Java Script, JAVA, SQL, C++, R shiny, HDFS, AZURE, Docker, D3JS, Tableau, Spotfire"", u""Principal Data Scientist\nMercedez\nJanuary 2011 to March 2013\n\u2022 Successfully delivered multiple NLP projects like building a chatbot that assists a customer to trouble shoot vehicle problems and recommend actions, further the bot could handle questions asked in natural language, related to common issues with the vehicle e.g. when my car is due for oil change, brake pads, how to sync my phone with Bluetooth etc.\n\u2022 Extracted data from multiple sources like vehicle sensor data, vehicle previous claims & services/repairs performed\n\u2022 Performed data pre-processing like data cleaning, text preprocessing, noise removal, lexicon normalization, object standardization\n\u2022 Perform featuring engineering like Word Embedding using word2vec models\n\u2022 Build seq2seqmodels using structured data & word embedding. Seq2Seq model take an input and returns as desired output for e.g. it can take a question as an input and returns an answer. The benefit is it can take any arbitrary length question and returns and answers in natural language. It uses a recurrent neural network (LSTM/Memory Network) at the back-end.\n\u2022 Used multiple evaluation matrix to validate models performance.\n\nRecommender system to configure new truck\n\u2022 Build a recommender system to help a customer configure a new truck from features and based on the historical data containing configuration selections with the help of association rules in SAP HANA, a recommender system was implemented.\n\u2022 Performed data profiling to learn user behavior, data sourcing and performed EDA using R and HIVE on Hadoop\n\u2022 HDFS, Involved in all aspects of data pre-processing. QlikView for dynamically displaying results.\n\u2022 Developed novel approach to build machine learning algorithm and implement it in production environment\n\u2022 Performed Data Cleaning, features scaling, features engineering, string_agg the 1500 datacodes were transformed into 24 different unique features. Historical data for past 5yrs was used.\n\n\u2022 In real-time association rules were implemented which uses prior probabilities, market basket analysis\n\u2022 Prototype machine learning algorithm for POC (Proof of Concept) SAP HANA platform was used for implementation which provides several mining algorithms, associated rules etc. SAP Lumira implementation for frontend.\n\u2022 By Developed Performance metrics to evaluate Algorithm's performance. Addressed the cold start problem and created visualizations for top 10 customers."", u'Data Analyst\nICICI - Mumbai, Maharashtra\nJuly 2008 to December 2010\nThe project was to build smart categories that accurately capture customer interactions. Based on, the existing data available on all historical conversations and other available variables from discrete applications. Further, in 2nd phase of the project we developed a learning algorithm that process all events and interactions between ICICI and its customer and provide a one page story about a customer. This write up helps ICICI agents & customer service partner to enrich customer experience\nResponsibilities:\n\u2022 Data analysis and visualization (Python, R,)\n\u2022 Designed, implemented and automated modeling and analysis procedures on existing and experimentally created data\n\u2022 Increased pace & confidence of learning algorithm by combining state of the art technology and statistical methods; provided expertise and assistance in integrating advanced analytics into ongoing business processes\n\u2022 Parsed data, producing concise conclusions from raw data in a clean, well-structured and easily maintainable format\n\u2022 Implemented Topic Modelling, PASSIVE AGGRESSIVE & other linear classifier models\n\u2022 Perform tfidf weighting, normalize\n\u2022 Performed scheduled and adhoc data driven statistical analysis, supporting existing processes\n\u2022 Developed clustering models for customer segmentation using R\n\u2022 Created dynamic linear models to perform trend analysis on customer transactional data in R\n\u2022 Performed Topic modeling']","[u'Master of Analytics in Analytics', u'Master of Information Systems in Information Systems', u'Bachelor of Management in Management']","[u'Harrisburg University Harrisburg, PA', u'Stratford University Falls Church, VA', u'Mangalore University']","degree_1 : Master of Analytics in Analytics, degree_2 :  Master of Information Systems in Information Systems, degree_3 :  Bachelor of Management in Management"
0,https://resumes.indeed.com/resume/aac37326d8467b5b,"[u'Data Scientist Intern\nConversica - Seattle, WA\nFebruary 2018 to Present\n* Working with data scientists and engineers to build customer intent models and email parsers using Natural Language Processing and Machine Learning (Python, Keras)\n--- Increased F1 score of email parser from 0.76 to 0.93 using boosted trees and LSTM networks', u'Computation Student Intern\nLawrence Livermore National Laboratory - Livermore, CA\nJune 2017 to September 2017\n* Performed a novel, model-agnostic influence analysis using Graph Signal Processing to quantify parameter and data\nsample influence on the performance of a High Performance Computing application (Python)\n--- Non-black-box method whose results aligned with the intuition of domain experts\n--- Validated results with Neural Networks, Gradient Boosting, Random Forests (Keras, scikit-learn, TensorFlow)\n\n* Identified critical samples in ImageNet and Kaggle datasets using AlexNet\u2019s latent space (with/without fine-tuning)\n\n* Selected to present posters at the SoCal Machine Learning Symposium 2017 and Supercomputing conference SC 2017', u'Decision Scientist\nMu Sigma - Bangalore, Karnataka\nAugust 2014 to April 2016\n* Devised and implemented a solution to recommend the optimal marketing expenditure for a leading Australian insurance company using multiplicative models, regression and nonlinear optimization (in R)\n--- Projected savings of ~7% ($2M) of annual marketing budget (recommendations were tested from Jul \u201816)\n--- Won the \u2018D-WOW\u2019 award, a highly selective recognition awarded by the CEO to outstanding projects\n--- Awarded the \u201cImpact Award\u201d for showing utmost ownership, dependability and grit in all endeavors (2015)\n\n* Developed a solution to predict fraudulent insurance claims and increase the efficiency of insurance claims investigators using Exploratory Data Analysis, Logistic Regression and clustering techniques (in R)\n\n* Evaluated performance of insurance claims investigators by creating a dashboard using D3.js\n\n* Served as Teaching Assistant for our hands-on introduction to data science program (for new employees)']","[u""Master's in Computer Science""]","[u'University of California-Irvine Irvine, CA\nSeptember 2016 to December 2017']","degree_1 : ""Masters in Compter Science"""
0,https://resumes.indeed.com/resume/3d3faf747afce335,"[u'SOFTWARE ENGINEER &DATA SCIENTIST\nFreeWheel, A Comcast Company - New York, NY\nOctober 2016 to Present\n\u2022 Design and develop web applications for linear addressable TV products using Python Flask framework\n\u2022 Improve product features and accuracy by implementing predictive models with Machine Learning techniques such as Random Forest\n\u2022 Maintain existing internal libraries and back-end RESTful APIs', u'DATA SCIENTIST & ANALYST\nAltice USA - New York, NY\nJune 2015 to September 2016\n\u2022 Implement predictive models based on viewer-ship data using Regression and Classification algorithm in Python and R\n\u2022 Create customer segmentation based on behavior data using supervised and unsupervised techniques\n\u2022 Construct churning model to identify contributing factors associated with customer loss', u'VICE PRESIDENT OF COMMUNICATIONS\nCenter of Data Science, New York University - New York, NY\nSeptember 2015 to May 2016\nOrganized program events and internal and external communications', u""DATA ANALYTICS INTERN\nAltice USA - New York, NY\nSeptember 2015 to December 2015\n\u2022 Identify KPIs and applied Statistical skillets such as A/B testing to help optimize Hillary Clinton's Campaign and fund raising strategy\n\u2022 Conducted keyword trend analysis to track the social media performance"", u""DATA SCIENTIST INTERN\nNamely - New York, NY\nJanuary 2015 to May 2015\n\u2022 Worked with Data Engineering Director to understand employee's churning pattern\n\u2022 Explored Econometric techniques to predict wage level based on demographic and behavior data"", u'CO-WRITER\nNamely\nJanuary 2015 to May 2015\n\u2022 Performed Multi-class classification (five-class) using Random Forest, One v.s One and One v.s Rest SVM\n\u2022 Reduced 200,000 dimensional feature into 4000 dimensions and achieved an accuracy score of 65%', u'RESEARCH ASSISTANT AND CO-AUTHOR\nBard Summer Research Institute and Levy Institute - Annandale-on-Hudson, NY\nJanuary 2013 to July 2013\n\u2022 Assisted Professor Giovannoni with time-series data processing by employing regression analysis in STATA and Python\n\u2022 Conducted spatial regression analysis in GeoDa to study the impact of geographic factor endowments on economy', u'CHINESE TUTOR\nBard Prison Initiative - Annandale-on-Hudson, NY\nJanuary 2012 to May 2012\nVolunteered to tutor inmates Chinese at Eastern Correctional Facility on weekly base']","[u'MASTER OF SCIENCE in DATA SCIENCE', u'BACHELOR OF ART in ECONOMICS']","[u'New York University New York, NY\nSeptember 2014 to May 2016', u'Bard College Annandale-on-Hudson, NY\nSeptember 2010 to May 2014']","degree_1 : MASTER OF SCIENCE in DATA SCIENCE, degree_2 :  BACHELOR OF ART in ECONOMICS"
0,https://resumes.indeed.com/resume/97cdf083142e11f7,"[u""Data Analyst\nAllegro Finance - Carlsbad, CA\nJuly 2017 to December 2017\n\u2022 Constructed pricing and operation models by using Excel and VBA for Allegro's financial products\n\u2022 Applied machine learning algorithms (Decision Tree and Logistical Regression) by using R and Python to design clients'\ndefault risk models\n\u2022 Assisted in re-designing Relation Model in order to improve Allegro's relational database management\n\u2022 Wrote SQL queries to extract data from relational database by using Microsoft SQL Server\n\u2022 Aggregated and analyzed data from outside data sources by using Excel Vlookup, Index and Pivot Table"", u""Data Scientist Summer Intern\nPing'An Health Insurance Co., LTD - Hefei, CN\nMay 2016 to August 2016\n\u2022 Preprocessed unstructured data, and applied exploratory data analysis techniques before model construction\n\u2022 Developed logistic regression and decision tree to analysis insurance fraud problem\n\u2022 Developed linear regression, lasso regression and ridge regression models to predict premium ratio"", u""Financial Modeling Summer Intern\nHua'an Securities Co., Ltd - Hefei, CN\nMay 2015 to August 2015\n\u2022 Worked on Fama-French three-factor model and analysis portfolio performance by computing Sharpe ratio, Treynor\nratio, Information ratio, Sortino ratio, VaR and ES\n\u2022 Worked on time series model(GARCH) to analysis and estimate stock market's volatility\n\nCOLLEGE PROJECT""]","[u'Master of Science in Financial Mathematics', u'Bachelor of Science in Corporate Finance']","[u'Worcester Polytechnic Institute (WPI) Worcester, MA\nMay 2017', u'Colorado State University (CSU) Fort Collins, CO\nMay 2015']","degree_1 : Master of Science in Financial Mathematics, degree_2 :  Bachelor of Science in Corporate Finance"
0,https://resumes.indeed.com/resume/5dd3689875b1fcfd,"[u'Data Scientist Intern\nCloud Connect - MY\nJuly 2017 to August 2017\n- Worked with RapidMiner on the cleansing process of unstructured data\n- Applied ROC Curve to compare the performance of Decision Tree, Naive Bayes and Rule induction operator on Training Data\n- Acquired the knowledge of Supervised and Unsupervised Learning throughout Machine Learning processes', u'Data Analyst Intern\nIowa United Nations Association - Iowa City, IA\nJanuary 2017 to May 2017\n- Forecasted total employment by economic activity based on historical data and nonlinear approach\n- Engendered Data screening methods through multiple Search Engine Optimization\n- Verified Iowa Division local interests and case studies for upcoming research projects', u'Undergraduate Researcher - Data Interpreter\nThe University of Iowa - Iowa City, IA\nAugust 2013 to December 2013\n- Collaborated with undergraduate Mechanical Engineering students to oversee the prototype designs\n- Quantified Solar Insulation Data to reinforce the parameters interpretations\n- Monitored Cooking performances using fundamental research methodologies and proper tests calculations']",[u'Bachelors of Science in Statistics'],"[u'The University of Iowa Iowa City, IA\nDecember 2017']",degree_1 : Bachelors of Science in Statistics
0,https://resumes.indeed.com/resume/552205cf6bb5a614,"[u'Data scientist\nEpisona, Inc - Pasadena, CA\nOctober 2017 to Present\n\xd8 Analysis of methylation levels in the sperm epigenome using\nNext Gen Sequencing data\n\xd8 Examination of differences in the methylome of euploid and aneuploid products of conception', u'Post-doctoral fellow\nBoston University - Boston, MA\nNovember 2015 to August 2017\nOptimization of laboratory protocols (DNA extraction, PCR\namplification and capillary electrophoresis) for generation of STR\nprofiles from single cell samples', u'Intern\nNational Institute of Science and Technology - Gaithersburg, MD\nJuly 2014 to August 2014\nCalculation of summary statistics of population allele frequency\ndatabases and Next Gen Sequencing data']","[u'PhD in Computational and Integrative Biology', u'in Technology']","[u'State University of New Jersey Camden, NJ\nJanuary 2011 to January 2015', u'Anna University Chennai, Tamil Nadu\nJanuary 2007 to January 2011']","degree_1 : PhD in Comptational and Integrative Biology, degree_2 :  in Technology"
0,https://resumes.indeed.com/resume/d38e408e90d83cac,"[u'Data Scientist\nTransamerica - Baltimore, MD\nMay 2016 to November 2017\n\u2022 Work independently and collaboratively throughout the complete analytics project lifecycle including data extraction/preparation, design and implementation of scalable machine learning analysis and solutions, and documentation of results.\n\u2022 Responsible for launching Amazon EC2 instances using Amazon Web Services (Windows & Linux).\n\u2022 Created roles for EC2, S3 and EBS resources to communicate within the team using IAM.\n\u2022 Responsible for S3 bucket creation, policies and the IAM role based policies and creating alarms and notifications for EC2 hosts using Cloud Watch.\n\u2022 Build and configure Virtual Data Center in AWS cloud to support EDW hosting including Virtual Private Cloud (VPC), Public and Private Subnets, Security Groups, Route Tables and launching EC2, RDS instances in the defined virtual private connection.\n\u2022 Installed Rstudio server on AWS Linux AMI as a free tier.\n\u2022 Application of various machine learning algorithms and statistical modeling like decision trees, regression models, clustering, SVM to identify Volume using scikit-learn package in R and Python.\n\u2022 Hands on experience in implementing Naive Bayes and skilled in Random Forests, Decision Trees, Linear and Logistic Regression, SVM, Clustering, Principle Component Analysis.\n\u2022 Have knowledge on A/B Testing, ANOVA, Multivariate Analysis, Association Rules and Text Analysis using R.\n\u2022 Have Knowledge on Hadoop ecosystem framework, Pig and Hive.\n\u2022 Extensive data cleansing and analysis, using pivot tables, formulas (V-lookup and others), data validation, conditional formatting, and graph and chart manipulation using excel.\n\u2022 Collaborate with technical and non-technical resources across the business to leverage their support and integrate our efforts.\n\u2022 Training and Testing of Data under data modelling process for each machine learning algorithm.\n\u2022 Created pivot tables and charts using worksheet data and external resources, modified pivot tables, sorted items and group data, and refreshed and formatted pivot tables.\n\u2022 Analyzed the Root cause in a code and incorporated changes in programs as cost-effective solution.\n\u2022 Worked on different data formats such as JSON, XML and performed machine-learning algorithms in Python.\n\u2022 Preparing the Test Documents, Results and delivery assuring completion of the user story.', u""Data Scientist\nComputer Sciences Corporation India Pvt Ltd - Indore, Madhya Pradesh\nJune 2014 to January 2016\nIndore/Hyderabad\n\n\u2022 Responsible for Retrieving data using SQL/Hive Queries and perform Analysis enhancements and documentation of the system.\n\u2022 Used R, SAS and SQL to manipulate data, and develop and validate quantitative models.\n\u2022 Generated reports of more than 100 Agent Fraud Investigation cases based on the client requirement and making sure the data is accurate.\n\u2022 Worked as a RLC Team Member and undertook user stories (tasks) with strict deadlines in Agile Environment.\n\u2022 Involved in Analyzing system failures, identifying root causes, and recommended course of actions. Documented the systems processes and procedures for future references.\n\u2022 Analysis performed on more than 100000 rows of data in excel sheet to identify the fraud agents in the system.\n\u2022 Scanned and created processes in the Data Warehouse to import retrieve and analyze data from the Cyber Life database.\n\u2022 Heavily involved in Testing and Modelling the data in order to migrate it to production environment.\n\u2022 Successfully implemented migration of client's requirement application from Test/DSS/Model regions to Production.\n\u2022 Have written numerous codes in R and python to retrieve the data from a .csv or .xl file.\n\u2022 Analyzed data collected in stores (JCL jobs, stored-procedures and queries) and provided reports to the Business team by storing the data in excel/SPSS/SAS file.\n\u2022 Performed Analysis and Interpretation of the reports on various findings.\n\u2022 Performing Exploratory Data Analysis on the data provided by the Client.\n\u2022 Prepared Test documents for Zap before and after changes in Model, Test and Production regions.\n\u2022 Responsible for production support Abend Resolution and other production support activities and comparing the seasonal trends based on the data by Excel.\n\u2022 Used advanced Microsoft Excel functions such as Pivot tables and VLOOKUP in order to analyze the data and prepare programs.\n\u2022 Brainstorming sessions and propose hypothesis, approaches and techniques.\n\u2022 Various statistical tests performed for clear understanding to the client.\n\u2022 Provided training to Beginners regarding the CyberLife system and other basics.\n\u2022 Complete support to all regions. (Test/Model/System/Regression/Production).\n\u2022 Actively involved in Analysis, Development and Unit testing of the data.\n\u2022 Complete delivery assurance of the project."", u'Jr. Data Scientist\nCSC Technologies\nMay 2011 to May 2014\nKey Responsibilities:\n\u2022 Responsible for collecting patients data from various sources including hospitals, clinics etc.\n\u2022 Prepared regular patient reports by collecting samples of Diagnosed Patients using Excel spreadsheets.\n\u2022 Ensure that there are no missing values in the dataset and can be used for further Analysis.\n\u2022 Cleaned data by analyzing and eliminating duplicate and inaccurate data (outliers) using R.\n\u2022 Worked in Agile Environment and responsible for designing analytic frameworks for data mining, ETL, analysis, and reporting under the supervision of the Manager.\n\u2022 Trained in Basics of Data Scientist and implemented those software applications in collecting and managing patient data in Excel/SPSS.\n\u2022 Assisted in performing statistical analysis of the data and storing them in a database.\n\u2022 Worked with Quality Control Teams to develop Test Plan and Test Cases.\n\u2022 Involved in designing and implementing the data extraction (XML DATA stream) procedures.\n\u2022 Generated graphs and reports using ggplot, ggplot2 in R Studio for analyzing models.\n\u2022 Generating the Results and predicting the Accuracy.\n\u2022 Preparing the Final Documents and ensure delivery to the Client before EOD.']",[u'Masters in Computer Science'],[u'Indiana State University'],degree_1 : Masters in Compter Science
0,https://resumes.indeed.com/resume/11fef2fa4bf753ea,"[u'Data Scientist\nCitigroup Inc - Chennai, Tamil Nadu\nJuly 2016 to July 2017\n\u2022 Increased customer base by 3% every month by analyzing an building statistical models on market behavior and social trends using semantic analysis on social network data.\n\u2022 Designed marketing campaigns for Citibank Thailand by applying time-series forecasting and classification algorithms like SVM and KNN on spend patterns and other customer demographics.\n\u2022 Developed algorithms to filter and structure unstructured customer behavior data.\n\u2022 Designed and maintained a recommendation engine and campaign management tool with online dash-boarding and customer level performance tracking which helped the marketing teams to be updated with their portfolios.\n\u2022 Awarded the ARC Award in 2016 for developing better campaigns which increased campaign conversion rate by 12% in the last quarter of 2016.', u'Application Developer\nCitigroup Inc - Chennai, Tamil Nadu\nJuly 2015 to July 2016\n\u2022 Developed packages like live trade status monitor, form generators for CitiFXPulse, an online FX trading platform.\n\u2022 Developed JavaScript based APIs for parsing server responses into interactive dashboards that led to a better user\nexperience.\n\u2022 Enhanced rate receive and update service to reduce the latency of FX rate update from 20 microseconds to 8\nmicroseconds.', u'Intern, Test and Development\nFINISAR\nMay 2014 to July 2014\nDoubled the production per station by expediting the temperature settling process of opto-electronic trans-receivers by developing an algorithm to tune the PI parameters of the temperature controller.']","[u'Masters in Electrical and Computer Engineering', u""Bachelor's in Electrical and Electronics Engineering""]","[u'Texas A&M University College Station, TX\nAugust 2017 to May 2019', u'National Institute of Technology Karnataka Mangalore, India\nJuly 2011 to May 2015']","degree_1 : Masters in Electrical and Compter Engineering, degree_2 :  ""Bachelors in Electrical and Electronics Engineering"""
0,https://resumes.indeed.com/resume/2447cb3aecaaeeda,"[u'Data Scientist (Summer Intern)\nFocalCXM\nMay 2017 to August 2017\nAnalysed the Sales Representative dataset and built a model in R by clustering the customers which gives the top 3 Next Best\ncustomers for the Rep to meet depending on the Planned, targeted, executed frequency and distance.\n\u2022 Provided a POJO Java Output by compiling the R code using H2o ai package used in developing an API.\n\u2022 Have attended meetings and provided weekly presentations regarding the work and progress done to the Board of members.\n\u2022 Have built a survey, developed a conjoint analysis and provided recommendations on the factors to be added to improve employee\nsatisfaction.', u'Data Analyst (Spring Intern)\nDominos\nFebruary 2017 to April 2017\n\u2022 Analysed the data using advanced excel functions like Pivot tables, VLOOK up, visualizations to get the descriptive analysis of the data.\n\u2022 Developed an RFM Model in SAS to find the recency, frequency and monitory values of the customers.\n\u2022 Performed cluster analysis and developed association rules based on the customer behavior with in each cluster as to determine the coupons to be given to each household.\n\u2022 Ran different models on the dataset like Regression, Decision Trees, Market Basket Analysis, Forecasting to analyze the customer\nbehavior and predict the output values for different set of customers.\n\u2022 Provided recommendations on incremental sales with and without coupons, ran a logistic regression that gave the most significant\ncoupons to be used for gaining profits.', u'Systems Analyst\nAccenture India\nJune 2015 to July 2016\n\u2022 Developed and maintained ETL mappings using Informatica Designer to extract the data from SQL server and flat files to the staging area, transform the data using SQL and load into the DataMart.\n\u2022 Written and modified SSIS, SSAS, SSRS packages and SQL stored procedures to clean and load data to databases.\n\u2022 Analysed and executed the test cases for various phases of testing - integration, regression and user.\n\u2022 Built a Margin Alert Reports with Net Sales, GP1% values at stock keeping unit to analyze the market and made strategic decisions\nUsing COGNOS Reporting tool.\n\u2022 Application Maintenance and resolved more than 1000 tickets during any failures of jobs which runs 24 by 7.']",[u'Master of Science in Business Analytics'],"[u'The University of Texas at Dallas Dallas, TX\nMay 2018']",degree_1 : Master of Science in Bsiness Analytics
0,https://resumes.indeed.com/resume/6ddf0fb8742bea57,"[u""Data Analyst/ Software Engineer\nUtah State University - Logan, UT\nJanuary 2017 to August 2017\n\u2022 Worked as a Research Assistant with Dr. Colby Tofel-Grehl in collaboration with NSA in creating an App called 'STITCH'.\n\u2022 Performed data analysis which needed cleaning of humungous data received from schools all over US, used machine learning techniques like Na\xefve Bayes,\nRandom Forest and Decision Tree Classification to accurately show where students lack their knowledge and how to increase their interest.\n\u2022 Created an Android App which had all basic information about electronic components and various projects using it. This App is deployed in Google Play Store\nwhich is accessible to all students and teachers."", u""Data Scientist\nCapgemini India Pvt. Ltd - Bengaluru, Karnataka\nAugust 2015 to July 2016\nIndia\n\u2022 Worked with a team size of 60 members for AT&T client where we followed Hadoop, Spark, Hive and Spark MLLib technologies for database development.\n\u2022 Worked in Hadoop Management System that handles creation of clusters, live activity of clusters, number of resources needed for technologies like Spark and Hive.\n\u2022 Handled huge chunks of incoming data by using Spark Streaming and discretizing it into RDD's.\n\u2022 For better and faster response, collaborated Spark with HIVE giving us a massive performance boost.\n\u2022 To provide meaningful insights to the client, we used Spark MLLib and Tableau for providing reliable options out of all available services provided by AT&T.""]","[u""Master's in Computer Science in Computer Science"", u'in Computer Engineering', u'']","[u'Utah State University\nAugust 2016 to Present', u'Mumbai University\nJanuary 2011 to January 2015', u'Education Centre']","degree_1 : ""Masters in Compter Science in Compter Science"", degree_2 :  in Compter Engineering, degree_3 :  "
0,https://resumes.indeed.com/resume/387742471c5a8ad5,"[u'Data Scientist\nPaypal - Sparks, MD\nJanuary 2016 to Present\nDescription: PayPal is an international e-commerce business allowing payments and money transfers to be made through online money transfers serve as electronic alternatives to paying with traditional paper methods, such as checks and money orders.\n\nResponsibilities:\n\u2022 Presented DQ analysis reports and score cards on all the validated data elements and presented -to the business teams and stakeholders.\n\u2022 Involved in defining the Source to business rules, Target data mappings, data definitions.\n\u2022 Extracting data from different databases as per the business requirements using Sql Server Management Studio.\n\u2022 Extensively using MS Excel for data validation.\n\u2022 Create statistical models using distributed and standalone models to build various diagnostics, predictive and prescriptive solution.\n\u2022 Interacting with the ETL, BI teams to understand / support on various ongoing projects.\n\u2022 Providing analytical network support to improve quality and standard work results.\n\nEnvironment:\nData Governance, SQL Server, ETL, MS Office Suite - Excel (Pivot, VLOOKUP), DB2, R, Python, Visio, HP ALM, Agile, Azure, MDM, Share point, Data Quality, Tableau and Reference Data Management.', u""Data Scientist\nAT&T - Middletown, NJ\nJanuary 2015 to December 2015\nDescription:\nThe project is to build a dashboard to display visual data moreover in the form of charts which is used by VP's and senior VP's inside AT&T. After login into secure global page this dashboard will demonstrate the information related to projects which are being monitored under them."", u'Informatica Analyst, MDM\nHP Quality Center - Richmond, VA\nMay 2013 to December 2014\nEnvironment:\nPower Center 9.x/8.1, Informatica Analyst, MDM, MS Excel, Agile, IDD, Data Governance, Oracle 11g, Meta Data, Sql Server, SOA, SSIS, SSRS, IDQ, Data Lineage, ETL, UNIX, T-SQL, HP Quality Center 11, RDM (Reference Data Management)\n\nReynolds Packaging Group, Richmond VA\nDate: May 2013 -Dec 2014\nRole: Data Analyst\n\nDescription: Reynolds Group Holdings Limited is a leading global manufacturer and supplier of consumer beverage and foodservice packaging products. Reynolds Group Holdings Limited is based in Auckland, New Zealand.\n\nResponsibilities:\n\u2022 Worked on data cleaning and reshaping, generated segmented subsets using Numpy and Pandas in Python.\n\u2022 Generated cost-benefit analysis to quantify the model implementation comparing with former situation.\n\u2022 Conducted model optimization and comparison using stepwise function based on AIC value\n\u2022 Worked on model selection based on confusion matrices, minimized the Type II error\n\u2022 Create Integration Jobs to backup a copy of data in network file system.\n\u2022 Identified the variables that significantly affect the target\n\u2022 Continuously collected business requirements during the whole project life cycle.\n\u2022 Generated data analysis reports using Matplotlib, Tableau, successfully delivered and presented the results for C-level decision makers.\n\nEnvironment:\nSQL Server 2008, Teradata 13, Erwin 8, Oracle 9i, PL/SQL, SQL*Loader, ODS, OLAP, SSAS, Informatica Power Center, OLTP.', u'HSBC - New York, NY\nAugust 2012 to April 2013\nRole: Supply Chain Systems (SCS)\n\nDescription:\nHSBC is a financial services and banking company. It has several divisions, with products such as online money transfer, money orders, business payments and commercial services.\n\nResponsibilities:\n\u2022 Developed Forms and Reports for their customer master Management.\n\u2022 Involved in Design and Development of Form to enter the customer Details, Sites details and Address Details using Forms 6i.\n\u2022 Created a Form to manage the customer account profiles using Forms 6i.\n\u2022 Involved in Design & Development of the Total customer report using Reports 6i.\n\u2022 Data loaded from legacy systems (ETL Operations) using PL/SQL and SQL*Loader.\n\u2022 Loaded the data into database tables using SQL*loader and oracle External tables from text and excel files.\n\nEnvironment:\nOracle 9i/10g (SQL*PLUS, SQL, PL/SQL), TOAD, SQL Navigator, Oracle WEB server, Oracle Applications 11, HTML, Erwin Data Model, Apache Web Server, VSS. Oracle Discover 10g.', u'Database Developer\nWipro Limited - Bengaluru, Karnataka\nJanuary 2011 to July 2012\nDescription:\nWipro Limited is an Indian Information Technology Services corporation headquartered in Bengaluru, India. In 2013, Wipro demerged its non-IT businesses into separate companies to bring in more focus on independent businesses\n\nResponsibilities:\n\u2022 Was responsible for performance tuning of the existing oracle scripts. Writing new shell scripts and modifying existing scripts as per need\n\u2022 Created database Triggers for the security purposes.\n\u2022 Designed and developed database tables, cursors, procedures, functions and packages to meet business requirements.\n\u2022 Developed, tested and debugged stored procedures and complex packages using PL/SQL\n\u2022 Created and used DB-Links to connect to other local & QA databases.\n\u2022 Handling exception to debug the PL/SQL code.\n\u2022 Used TOAD, PL/SQL developer tools for faster application design and developments Performance tuning, SQL tuning.\n\u2022 Analyzed requirements, designed, developed, deployed and documented software components.\n\u2022 Loaded the data into database using the SQL*Loader from various sources..\n\nEnvironment:\nOracle database 9i, PL/SQL, SQL Plus, SQL*LOADER, Oracle Form Designer, Toad, Windows 2000 Professional and UNIX', u'Database Developer\nHEWLETTPACKARD - Bengaluru, Karnataka\nMarch 2009 to December 2010\nDescription:\nHewlett Packard Enterprise (HPE) offers worldwide IT, technology & enterprise products, solutions and services\n\nResponsibilities:\n\u2022 Developed functions, procedures and packages as re-usable application programs that translated complex business logic using PL/SQL and SQL to transform data between source staging area to target staging area. Extensively used PL/SQL tables, cursors and exception handling.\n\u2022 Created database objects like Tables, Sequence, Synonyms, Views, and Materialized Views.\n\u2022 Involved in developing interactive forms and customization of screens using Oracle Forms.\n\u2022 Worked on tuning the SQL Queries for better performance and troubleshooting development problems.\n\u2022 Extensively worked on developing KORN shell scripts for scheduling the jobs using CRONTAB and invoking the procedures through shell scripts.\n\u2022 Involved in implementing bug fixes and enhancements within packages, forms, and reports.\n\u2022 Responsible for preparing user manuals and quick reference guides.\n\nEnvironment:\nOracle, Forms, Reports, PL/SQL, SQL Navigator, Oracle Enterprise Manager, SQL, PL/SQL']",[],[],degree_1 : 
0,https://resumes.indeed.com/resume/a27d25c06b13e10b,"[u'DATA SCIENTIST\nCoursera - Mountain View, CA\nJune 2014 to Present\nResponsibilities\n\u2022 A/B tested marketing campaigns, performed market research, designed and ran surveys.\n\u2022 Extracted business insights from Amazon RedShift, MySQL and BigQuery databases.\n\u2022 Presented analytical results to technical and nontechnical audiences.', u'STATISTICAL ANALYST\neBay, Inc - San Jose, CA\nApril 2012 to Present\nProduced actionable insights from very large datasets using tools such as SQL, Python and R.\n\u2022 Employed advanced statistical modeling and quasi-experimental design where A/B testing was not feasible.\n\u2022 Drove business decisions such as partner selection and campaign/creative optimization.\n\u2022 Introduced statistical rigor and a data-driven decision making framework to the Display marketing channel.', u'BUSINESS INTELLIGENCE ANALYST\nMarkMonitor - San Francisco, CA\nJanuary 2012 to April 2012\nUsed advanced statistical methods to transform raw data into valuable business intelligence.\n\u2022 Presented to and interacted with important clients in the music and movie industry.', u'RESEARCH SCIENTIST\nOgle Lab, Arizona State University - Tempe, AZ\nJuly 2011 to January 2012\nDeveloped, coded and presented large complex hierarchical Bayesian models describing ecological systems.\n\u2022 Consulted with scientists in design of experiments and data collection, analysis and interpretation.', u'STATISTICAL CONSULTANT\nUSDA Agricultural Research Service - Cheyenne, WY\nOctober 2010 to July 2011\nDesigned statistical models for evaluating the efficacy of aerial-image based surveying methods.', u'GRADUATE ASSISTANT\nUniversity of Wyoming - Laramie, WY\nAugust 2009 to May 2011\nManaged class of 185 students and four teaching assistants.\n\u2022 Received the Promoting Intellectual Engagement in the First Year award, a student nominated teaching award.', u'ENGLISH LANGUAGE TEACHER\nEastern Europe and the US\nJanuary 2001 to January 2007\nWas responsible for designing new programs and curricula, organizing class times and locations, and hiring, training and managing groups of teachers.', u'PROFESSIONAL CHEF\nEastern Europe and the US\nJanuary 1997 to January 2001\n2007-2009\n\u2022 Consistently delivered delicious and artfully arranged food in a high-stress environment.']","[u'MASTER OF SCIENCE in STATISTICS', u'BS in ECONOMICS']","[u'University of Wyoming Laramie, WY\nJune 2011', u'University of Wyoming Laramie, WY\nJanuary 2009']","degree_1 : MASTER OF SCIENCE in STATISTICS, degree_2 :  BS in ECONOMICS"
0,https://resumes.indeed.com/resume/5de3775ab67c9556,"[u'Data Scientist\n7B Software, Inc - Vero Beach, FL\nFebruary 2016 to Present\nMachine Learning using Neural Networks, LSTM (Time Series) Neural Networks, Bayesian networks, Subject Vector Machines, logistic networks, and linear regression, in healthcare, financial and military environments']","[u'in M. E', u'B.S. in summa']","[u'Cornell University New York, NY', u'Vanderbilt University Nashville, TN']","degree_1 : in M. E, degree_2 :  B.S. in smma"
0,https://resumes.indeed.com/resume/028690d1be72f5ef,"[u'Data Scientist, Instructor\nGalvanize - New York, NY\nMay 2017 to Present\n* Lead cohorts of Data Science Immersive students\n* Provided pedagogically strong lectures across the scope of DSI curriculum\n* Responsible for organizing the schedule, delivery and execution of entire\nprogram']",[u'Bachelor of Science in Computational Biochemistry'],"[u'University of Texas at Austin Austin, TX\nAugust 2008 to December 2013']",degree_1 : Bachelor of Science in Comptational Biochemistry
0,https://resumes.indeed.com/resume/6c4fa72f1fa947ef,[],[],[],degree_1 : 
0,https://resumes.indeed.com/resume/22d7fc634b127c74,"[u'Data Scientist\nSprint - Overland Park, KS\nFebruary 2017 to Present\n\u2022 Handled over 550 million records in data preparation and training the model.\n\u2022 Designing and developing various machine learning frameworks using R.\n\u2022 Provided bi-weekly reviews to director and VP of Sprint.\n\u2022 Implemented Dynamic time wrapping for time series classification.\n\u2022 Implemented SVM for novelty detection.\n\u2022 Handled imbalance datasets with multiclass classification.\n\u2022 Developed personalized products recommendation with Machine Learning algorithms, including Collaborative filtering and Gradient Boosting Tree, to better meet the needs of existing customers and acquire new customers.\n\u2022 Determined customer satisfaction and helped enhance customer experience using NLP.\n\u2022 Recommended and evaluated marketing approaches based on quality analytics of customer consuming behavior.\n\u2022 Coordinated the execution of A/B tests to measure the effectiveness of personalized recommendation system.\n\u2022 Used Git 2.X to apply version control. Tracked changes in files and coordinated work on the files among multiple team members.', u'Data Scientist\nWichita State University - Wichita, KS\nAugust 2015 to December 2016\nMy role was to perform statistical analysis for Bioinformatics research data collected from GWAS, WTCCC, give insights into specific genomes, preparing technical documentation and present results in a simple and clear fashion to a non-statistical audience.\n\u2022 Used R 3.X, R2.X and Spark 1.4 (PySpark, MLlib) to implement different machine learning algorithms including Generalized Linear Model, SVM, Random Forest, Boosting and Neural Network.\n\u2022 Evaluated and optimized performance of models, tuned parameters with K-Fold Cross Validation.\n\u2022 Worked on data cleaning, data preparation and feature engineering with Python 3.X including Numpy, Scipy, Pandas, Matplotlib, Seaborn and Scikit-learn.\n\u2022 Worked with the version control tools, such as Git 2.X, to keep versions attributed from different people and record project at different time points.\n\u2022 A Novel Machine Learning Framework For Phenotype Prediction Based On Genome-Wide DNA Methylation Data. Published IJCNN 2017\n\u2022 Recognizing handwritten digits using artificial neural network\nImplemented a project using python, skit-learn, Numpy, Scipy and UNIX for detecting handwritten digits using artificial neural networks (Natural Language Processing) and convolutional neural networks.\n\u2022 Library Management system\nDeveloped a database using JAVA, SQL, and UNIX for managing a library, with one user as a librarian with limited access and the other as a database administrator.', u'Data Scientist\nSNT Media - Wichita, KS\nMay 2015 to July 2015\nDescription: Developed python scripts for variation and Time Series analysis, of Stock Market, and for Rent updates analysis from passfail.com\n\n\u2022 Prediction of Multiple Regression analysis.\n\u2022 Designed and developed an advanced recommendation system for real estate and finance customers utilizing R and Python.\n\u2022 Utilized SQL to extract data from SQL Server 11.0, and MongoDB, to prepare data for analysis.\n\u2022 classified customers by RFM analysis, clustering and regression model with R 3.0, selected customers with high value and improved their retention rate by sending ads and coupons\n\nEnvironment:\nExcel 2013, R 3.0, Hadoop 1.X, MS SQL Server 2012, Tableau 8.1', u'Data Scientist\nTech Mahindra Inc - Bengaluru, Karnataka\nApril 2012 to July 2014\nDescription: My role was to design and implement A/B Testing for British Telecom, capture data using web analytics using web crawling platform and Optimization, create dashboards using ELK (Elastic search, Logstash and Kibana) framework.\n\n\u2022 Used Python 2.7 to apply time series models, clustering algorithm and other data mining methods to explore the fast growth opportunities of our clients\n\u2022 Analyzed the traffic queries of Baidu search engine using classification algorithm.\n\u2022 Assisted to improve the liquidity of our ads model.\n\u2022 Based on the data of clients and traffic, designed comprehensive analysis to optimize products and explored the strengths and weaknesses of products.\n\u2022 Team Member, Rating Engine Application\nBoosted marketing for client by designing a cell phone data rating engine in Python, PL/SQL, and UNIX/Linux to suggest suitable cell phone plans based on data usage.\n\u2022 Team Lead, Call Center Application\nIncreased efficiency of client customer support by implementing a call center application using R, Python and SQL that tracked customer complaint history and assigned defects accordingly.\n\u2022 Team Member, Open Reach\nEnhanced customer buying experience using Machine learning, R, python and SQL\n\u2022 Team Member/SPOC R50 Release, One Siebel\nEnsured smooth access for customers by developing scripts in Python and Unix for managing the crash reports and log levels of Unix and Windows servers, maintaining server network configuration components using Siebel 8.1 and Oracle, analyzing and providing RCA for server defects, and developing and publishing documents for avoiding defects in future releases; set up Scrum daily between Delivery Managers, Dev Team, Test Team, and Admin Team.\n\nEnvironment:\nExcel 2010, Python 2.7, Hadoop 1.X, Mapreduce, MS SQL Server 2008']","[u'M.S. in Computer Science', u'B.S. in Electronics and Communication Engineering']","[u'Wichita State University Wichita, KS', u'Rao Bahadur Y.Mahabaleshwarappa Engineering College Bellary, Karnataka']","degree_1 : M.S. in Compter Science, degree_2 :  B.S. in Electronics and Commnication Engineering"
0,https://resumes.indeed.com/resume/c8a07f47ab1e7eb9,"[u'QC Scientist\nPfanstiehl\nNovember 2017 to Present\nResponsibilities:\n\u2022 Performed testing on finished products, raw materials, in process samples, stability, and product development.\nThe testing includes, ROIs, LODs, KF, HPLC, GC, UV-Vis, IR, refractometer, polarimeter and various ID tests from major pharmacopeia (USP, JP, and EP) and ACS.', u'Scientist\nMars and Wrigley Confectionaries\nJune 2017 to August 2017\nResponsibilities:\n\u2022 Prepared gums, mints, and hard/soft candies for analysis that included Karl Fischer, HPLC, pH, insoluble testing,\nwater activity, refractive index, and size separation using sieves.', u'Research Assistant/Graduate Student\nUniversity of Illinois at Chicago - Chicago, IL\nJune 2015 to March 2017\nResponsibilities:\n\u2022 HPLC analysis of synthesized compounds\n\u2022 IT-TOF HRMS accurate mass measurements of synthesized compounds\n\u2022 IR analysis of synthesized samples\n\u2022 Synthetic Chemistry', u'Scientist\nFresenius Kabi\nNovember 2014 to June 2015\nResponsibilities:\n\u2022 Prepared stability samples for analysis using wet chemistry/physical testing techniques such as pH, colorimetric,\nvolume check, particulate matter, manual titrations, and Karl Fischer.\n\u2022 Adhering to cGMP, FDA, USP, and EP guidelines for testing\n\u2022 Maintaining a proper laboratory notebook with GDP', u'Assistant Chemist\nIllinois Institute of Technology Research Institute\nAugust 2012 to May 2014\nResponsibilities:\n\u2022 Prepared dose formulations for testing and for HPLC analysis. Specifically, diluted filter membranes, filtered the sample for impurities, and diluted the formulation into an HPLC vial for testing. Agilent Chemstation was the\nprimary program used for the HPLC.\n\u2022 Assisting in method validation and HPLC of trace organic compounds and pharmaceuticals in complex matrices\nfor toxicological studies under FDA and GLP guidelines.\n\u2022 Stability testing for samples. New samples were run on the HPLC against older samples to determine the level\ndegradation of samples with time. The efficacy of standards used in long term projects was also tested in this\nmanner.\n\u2022 Wet Chemistry; pH, reagent preparation of buffers, diluents, mobile phases, titrations, dissolutions, glassware\ncalibration and cleaning.\n\u2022 Adhere to GLP and GDP guidelines.', u'Chemist I\nAgilent Chemstation for HPLC and GC\nMarch 2012 to August 2012\nResponsibilities:\n\u2022 Performed analysis on raw materials and finished products (pills, capsules, powders, liquids) using HPLC and GC\ninstrumentation. Depending on the type of product being tested, fill weights were also determined.\n\u2022 Wet Chemistry; pH, reagent preparation of mobile phases, titrations, UV-Vis, TLC, titrando\n\u2022 Agilent Chemstation for HPLC and GC were the computer programs used.', u'Data Reviewer\nSGS Life Sciences\nMay 2011 to March 2012\nResponsibilities:\n\u2022 Reviewing analyst data to ensure that the SOPs, USP, EP, JP, and FDA guidelines were followed appropriately\n\u2022 Out of Specification (OOS) Investigations\n\u2022 Laboratory Integrity Checks\n\u2022 Review and edited expired SOPs\n\u2022 Communicated with customers regarding testing, results, and other various questions']","[u'Ph.D. in Medicinal Chemistry', u'Diploma in Chemistry']","[u'University of Illinois at Chicago Chicago, IL\nJune 2015 to March 2017', u'Northeastern Illinois University Chicago, IL\nAugust 2007 to May 2011']","degree_1 : Ph.D. in Medicinal Chemistry, degree_2 :  Diploma in Chemistry"
0,https://resumes.indeed.com/resume/ca0ff23319d4e5d3,"[u'Data Scientist Intern\nAlibaba - Hangzhou, CN\nJune 2017 to August 2017\n\u2022 Distinguished different book authors who shared the same abbreviated name by implementing Na\xefve Bayes and SVM with gaussian kernel in R. Training and test accuracy reached 86.1% and 85.0% respectively\n\u2022 Utilized Mahout to perform user-based and item-based recommendations on book sales. Increased books sales volume by 11.6% among teenagers in Guangzhou\n\u2022 Examined large-scale structured and unstructured data sets of over 100 merchants in Taobao utilizing Hive and MySQL', u'Research Assistant\nSun Yat-sen University - Guangzhou, CN\nFebruary 2016 to June 2016\n\u2022 Implemented Xgboost, GBM and SVM with gaussian kernel in R to distinguish pictures of Labrador and fried chicken and found out SVM with gaussian kernel reached high training accuracy while test accuracy was very low\n\u2022 Solved over-fitting problem by performing PCA and test accuracy of Xgboost reached 78.2%, increased by 37.6%\n\u2022 Obtained HoG features from MATLAB, replaced with original features and enhanced training accuracy to 88.3% and test accuracy to 87.5%']","[u'Master of Arts in Statistics', u'Bachelor of Science in Statistics', u'']","[u'Columbia University, Graduate School of Arts and Science New York, NY\nDecember 2017', u'Sun Yat-sen University, School of Mathematics & Computational Science Guangzhou, CN\nJune 2016', u'University of California Berkeley Berkeley, CA\nAugust 2014 to December 2014']","degree_1 : Master of Arts in Statistics, degree_2 :  Bachelor of Science in Statistics, degree_3 :  "
0,https://resumes.indeed.com/resume/a826c44af65b40ed,"[u'Associate Data Scientist\nBusiness Solutions International\nJanuary 2014 to January 2016\nData Science/Analytics experience with R and python (numpy, pandas, matplotlib, scikit-learn):']","[u'Masters of Science in Computer Science', u'Bachelors of Science in Mechanical Engineering']","[u'Bradley University Peoria, IL\nMay 2018', u'Jawaharlal Technological University\nJune 2014']","degree_1 : Masters of Science in Compter Science, degree_2 :  Bachelors of Science in Mechanical Engineering"
0,https://resumes.indeed.com/resume/d326c30fc9dd610d,"[u'Senior Lab Technologist / Data Analyst\nMidi Labs - Newark, DE\nJanuary 2011 to Present\n* Process-improvement and innovation resulting in savings of over $100,000 annually.\n* Project management and leadership writing a Business/Marketing plan for new services.\n* Member of Marketing team, increasing product awareness and sales, customer relations, as well as developing new lab product service offerings to increase business flow and revenue.\n* Developed B2C and web-based training and marketing videos for new products offerings.\n* Innovator, team member, and contributor in various Scrum teams.', u'IT Technician/Computer Teacher\nEastside Charter School - Wilmington, DE\nJanuary 2011 to January 2012\nDeployed innovative equipment allowing underprivileged students access to advanced technology.', u'Data Review Scientist\nAccugenix, Inc - Newark, DE\nJanuary 2008 to January 2011\n* Sole investigator of facility-wide Corrective Action / Preventative Action (CAPA) program.\n* Implementation of innovative software tools to help improve sample throughput and increase profitability.\n* Key roles as member of Social Engagement Team and Company Softball Team Manager strengthened leadership skills and helped to solidify team atmosphere and increase morale.', u'Research Assistant - Molecular Diagnostics Lab and Molecular Biology Core Lab\nAlfred I. DuPont Hospital for Children - Wilmington, DE\nJanuary 2008 to January 2008\n* Liaison between Principle Investigators and CORE Facility, managing on-time reporting and delivery of results.\n* Customer service and trouble shooting for failed or ambiguous results.', u'Public Health Lab Scientist III\nMaryland State Department of Health and Mental Hygiene - Baltimore, MD\nJanuary 2004 to January 2008\n* Principle management of HIV Genotyping/Sequencing lab.\n* Key responsibilities included high throughput of critical patient samples, report generation, and maintenance of critical lab certifications and inspections.']","[u'MBA in Finance', u'B.S. in Microbiology']","[u'Alfred Lerner College of Business and Economics, University of Delaware Newark, DE', u'Penn State University University Park, PA']","degree_1 : MBA in Finance, degree_2 :  B.S. in Microbiology"
0,https://resumes.indeed.com/resume/c676ad2a2c04c022,"[u'Associate Data Scientist\nAuburn University\nJanuary 2015 to December 2017\n\u2022 Designed applications of Machine learning, Statistical Analysis and Data visualizations with challenging large data processing problems resulting in savings more than $1.2M an year.\n\u2022 Worked working with various databases like Oracle, SQL and performed the computations, log transformations, feature engineering, and Data exploration to identify the insights and conclusions from complex data using R- programming in R-studio\n\u2022 Implemented predictive models using machine learning algorithms linear regression and linear boosting algorithms and performed in- depth analysis on the structure of models, compared the performance of all the models and found tree boosting is the best for the prediction.\n\u2022 Applied concepts of R-squared, R.M.S.E, P-value, in the evaluation stage to extract interesting findings through comparisons.\n\u2022 Performed in-depth statistical analysis and data mining methods using R, including Cluster analysis, Logistic Regression, and boosting models that led to reducing variance by 45%\n\u2022 Proficient in the entire CRISP-DM life cycle and actively involved in all the phases of project life cycle including data acquisition, data cleaning, data engineering,\n\u2022 Extensively used Azure Machine Learning to set up the experiments and creating Web services for the predictive analytics\n\u2022 Performed feature scaling, feature engineering and statistical modeling.\n\u2022 Worked on writing complex SQL queries in performing Data analysis using window functions, joins, improving performance by creating partitioned tables,\n\u2022 Prepared multiple dashboards using Tableau to reflect the data behavior over period of time Analyzed and worked with all aspects of regression models (OLS etc.)\n\u2022 Responsible for working with stakeholders to troubleshoot issues, communicate to team members, leadership and stakeholders on findings to ensure models are well understood and optimized.', u'Senior Supply Chain Data Analyst\nDurr Dental - IN\nFebruary 2013 to December 2016\nIndia\n\nResponsibilities:\n\u2022 Designed, modeled, validated and tested statistical algorithms against various data sets including behavioral data and deployed predictive models using R-studio\n\u2022 Performed Data Transformation method for Rescaling and Normalizing variables.\n\u2022 Applied different Machine Learning algorithms/methods on data sets to predict credit risk, fraud detection, customer churn, and target marketing.\n\u2022 Worked on data to increase cross-& up-sell revenues, enhance customer value or reduce non-credit losses.\n\u2022 Contributed implementing models to identify, extract, summarize, and reduce or categorize the relevant qualitative financial input information like sentiment/feedback/news according to specific structures (templates) from a source text (digital news) to support decision making.\n\u2022 Analyzed, transformed, and contextualized a variety of ingested data - social data, GIS data, POI& AOI data, and some consumer behavior data for building direct marketing predictive models.\n\u2022 Analyzed customer consuming behavior and discover value of customers.\n\u2022 Applied customer segmentation with Clustering algorithms and develop geo-demographic customer segmentation models.\n\u2022 Delivered Interactive visualizations/dashboards using ggplot and Tableau to present analysis outcomes in terms of patterns, anomalies and predictions.', u'Business Data Analyst\nSaisons Technocom - IN\nJuly 2011 to February 2013\nIndia\n\u2022 Prepared comprehensive documented observations, analyses and interpretations of results including technical reports, summaries, protocols and quantitative analyses.\n\u2022 Worked closely with marketing team to deliver actionable insights from huge volume of data, coming from different marketing campaigns and customer interaction matrices such as web portal usage, email campaign responses, public site interaction, and other customer specific parameters.\n\u2022 Gathered analyzed & translated business requirements into relevant analytic approaches & shared for peer review.\n\u2022 Contributed to Finance and Risk management, Operations management, and Marketing to maximize ROI using Data Analytics\n\u2022 Design, model, validate and test statistical algorithms against various real-world data sets including behavioral data and deploy models in the backend\n\u2022 Performed Data Transformation method for Normalizing variables.\n\u2022 Applied Business Objects best practices during development with a strong focus on reusability and better performance.\n\u2022 Co-ordinated with various business users, stakeholders and SME to get Functional expertise, design and business test scenarios review, UAT participation and validation of financial data.']","[u""Master's"", u'Masters Industrial Engineering , Major in Advanced Statistical Data Analysis & Quantitative Process Analysis in Advanced Statistical Data Analysis & Quantitative Process Analysis']","[u'', u'Auburn University Auburn, AL']","degree_1 : ""Masters"", degree_2 :  Masters Indstrial Engineering , degree_3 :  Major in Advanced Statistical Data Analysis & Qantitative Process Analysis in Advanced Statistical Data Analysis & Qantitative Process Analysis"
0,https://resumes.indeed.com/resume/67d8018e90df1029,"[u'Statistician in Financial Risk Data Science Team\nDun & Bradstreet\nJune 2015 to Present\nDeveloped Financial Credit Risk models using WOE based scorecards, which improved decision making\nprocess for different financial customers\n\u2022 Built Fraud Detection models in a team and Shell Index independently using XGBoost/Random Forest/AI\nhelping Banks, Telecom companies and US Government to find fraud cases\n\u2022 Developed NLP techniques to improve the matching of the customer data to the D&B database\n\u2022 Collaborated with H2O (a data science platform company) to build machine learning model interpretability for credit risk models\n\u2022 Built best-in-industry credit scores for Origination and Portfolio scenarios of the small business in USA\n\u2022 Worked on feature engineering of the complex data of Small Business Financial Exchange (SBFE) to create\nuseful attributes for machine learning models\n\u2022 Created collections and delinquency models for the major telecom companies to improve collections effort\n\u2022 Skilled in building Reject Inference on financial data using the data available to Credit Bureaus\n\u2022 Implemented a quick and effective solution to perform Stress Test and Sensitivity Analysis used by banks\n\u2022 All new financial and telecom solutions implemented at customer end improved the performance by millions of $ and created big $ revenue and new capabilities for D&B', u'Data Scientist\nEvantage Solutions\nNovember 2014 to May 2015\nClient: Microsoft\n\u2022 Developed XGBoost model in Python to improve the sales of Microsoft Surface tablets\n\u2022 Develop insights and recommendations based on model outputs\n\u2022 Statistical models on Microsoft Surface and Xbox improved the sales and planning strategy by $50 million', u""Statistical Analyst\nRang Technologies\nMay 2014 to October 2014\nClient: JP Morgan Chase\n\n\u2022 Built statistical and economic models (PD and LGD) for Chase's CCAR team\n\u2022 Estimated loan loss reserves and forecast credit loss, under various economic scenarios from Federal Reserve\nBank\n\u2022 Improved the accuracy of the default models by 3%\nMobile: 405-612-0187 | Email: shaileshpatil18@gmail.com | Located at Avenel, NJ and open to re-locate\n\n\u2022 Identified innovative techniques and/or developed utilities that increase the speed and efficiency of the modeling tool set\n\u2022 Efficiently handled and managed massive volumes of data (terabytes of data)"", u'Statistical Modeler /Data Scientist\nExperis US Inc\nSeptember 2012 to May 2014\nClient: Catalina Marketing\n\u2022 Created regression models based on 200,000,000+ Point of Sale data to target new customer and reduce\ncustomer defection\n\u2022 Improved redemption rate of Data Mining print programs by 3% -4% or $700,000 on average\n\u2022 Participated in design and implementation of Data Mining procedures for both new predictive modeling\ntechniques and improvements in processing efficiencies\n\u2022 Scored full database of 200,000,000+ IDs using the desired predictor variables\n\nText mining on unstructured data\n\u2022 Extracted data and web links, by web crawling from a website and used it for data mining\n\u2022 Processed and imported the data in SQL database\n\u2022 Used NLP techniques in R to perform text cleaning and mining on the collected data\n\u2022 Performed different types of clustering to understand the data patterns\n\u2022 Provided insights about the information generated from text mining']","[u'Master of Science in Industrial Engineering & Management', u'Bachelor of Mechanical Engineering in Mechanical Engineering']","[u'Oklahoma State University\nMay 2012', u'University of Pune\nMay 2009']","degree_1 : Master of Science in Indstrial Engineering & Management, degree_2 :  Bachelor of Mechanical Engineering in Mechanical Engineering"
0,https://resumes.indeed.com/resume/4e11a8883cb57023,"[u'Data Scientist\nCanon - Jamesburg, NJ\nJuly 2017 to Present\nDescription:Canon is specializing in the manufacture of imaging and optical products, including cameras, camcorders, photocopiers, steppers, computer printers and medical equipment.\n\nResponsibilities:\n\u2022 Identifying the Customer and account attributes required for MDM implementation from disparate sources and preparing detailed documentation.\n\u2022 Performing data profiling and analysis on different source systems that are required for CustomerMaster.\n\u2022 Worked closely with the DataGovernanceOfficeteam in assessing the source systems for project deliverables.\n\u2022 Used T-SQL queries to pull the data from disparate systems and Data warehouse in different environments.\n\u2022 Used DataQualityvalidation techniques to validate CriticalData Elements (CDE) and identified various anomalies.\n\u2022 Extensively used open source tools - RStudio (R) and Spyder (Python) for statistical analysis and building the machinelearning.\n\u2022 Involved in defining the Source To businessrules, Targetdatamappings, datadefinitions.\n\u2022 Presented DQ analysis reports and score cards on all the validated data elements and presented -to the business teams and stakeholders.\n\u2022 Performing DataValidation / DataReconciliation between disparate source and target systems (Salesforce, Cisco-UIC, Cognos, DataWarehouse) for various projects.\n\u2022 Interacting with the Business teams and Project Managers to clearly articulate the anomalies, issues, findings during data validation.\n\u2022 Writing complexSQL queries for validating the data against different kinds of reports generated by Cognos.\n\u2022 Extracting data from different databases as per the business requirements using SqlServerManagementStudio.\n\u2022 Interacting with the ETL, BIteams to understand / support on various ongoing projects.\n\u2022 Extensively using MSExcel for datavalidation.\n\u2022 Generating weekly, monthly reports for various business users according to the business requirements. Manipulating/mining data from database tables (Redshift, Oracle, DataWarehouse)\n\u2022 Providing analytical network support to improve quality and standard work results.\n\u2022 Create statistical models using distributed and standalone models to build various diagnostics, predictive and prescriptive solution.\n\u2022 Interface with other technology teams to load (ETL), extract and transform data from a wide variety of data sources\n\u2022 Utilize a broad variety of statistical packages like SAS, R, MLIB, Graphs, Hadoop, Spark, MapReduce and others\n\u2022 Provides input and recommendations on technical issues to Business&DataAnalysts, BIEngineers and DataScientists.\n\nEnvironment: Data Governance, SQL Server, ETL, MS Office Suite - Excel (Pivot, VLOOKUP), DB2, R, Python, Visio, HP ALM, Agile, Sypder, Word, Azure, MDM, SharePoint, Data Quality, Tableau and Reference Data Management.', u""Data Scientist\nAmerican Family Insurance - Madison, WI\nApril 2016 to June 2017\nDescription: American Family Insurance, also abbreviated as AmFam, is a private mutual company that focuses on property, casualty, and auto insurance, and also offers commercial insurance, life, health, and homeowners coverage as well as investment and retirement-planning products.\n\nResponsibilities:\n\u2022 Utilized Spark, Scala, Hadoop, HQL, VQL, oozie, pySpark, DataLake, TensorFlow, HBase, Cassandra, Redshift, MongoDB, Kafka, Kinesis, SparkStreaming, Edward, CUDA, MLLib, AWS, Python, a broad variety of machine learning methods including classifications, regressions, dimensionally reduction etc.\n\u2022 Utilized the engine to increase user lifetime by 45% and triple user conversations for target categories.\n\u2022 Application of various machine learning algorithms and statistical modeling like decision trees, text analytics, naturallanguageprocessing (NLP), supervised and unsupervised, regressionmodels, socialnetworkanalysis, neuralnetworks, deeplearning, SVM, clustering to identify Volume using scikit-learnpackage in python, Matlab.\n\u2022 Worked onanalyzing data from GoogleAnalytics, AdWords, Facebook etc.\n\u2022 Evaluated models using CrossValidation, Loglossfunction, ROCcurves and used AUC for feature selection and elastic technologies like ElasticSearch, Kibana.\n\u2022 Performed DataProfiling to learn about behavior with various features such as trafficpattern, location, Date and Time etc.\n\u2022 Categorized comments into positive and negative clusters from different social networking sites using SentimentAnalysis and TextAnalytics\n\u2022 Used Pythonscripts to update content in the database and manipulate files\n\u2022 Skilled in using dplyr and pandas in R and Python for performing exploratory data analysis.\n\u2022 Performed Multinomial LogisticRegression, DecisionTree, Randomforest, SVM to classify package is going to deliver on time for the new route.\n\u2022 Performed data analysis by using Hive to retrieve the data from Hadoopcluster, Sql to retrieve datafrom Oracledatabase and used ETL for data transformation.\n\u2022 Performed DataCleaning, features scaling, features engineering using pandas and numpy packages in python.\n\u2022 Exploring DAG's, their dependencies and logs using AirFlow pipelines for automation\n\u2022 Performed data cleaning and feature selection using MLlib package in PySpark and working with deep learning frameworks such as Caffe, Neon.\n\u2022 Developed Spark/Scala, RPython for regular expression (regex) project in the Hadoop/Hive environment with Linux/Windows for big data resources.\n\u2022 Used clustering technique K-Means to identify outliers and to classify unlabeled data.\n\u2022 Tracking operations using sensors until certain criteria is met using AirFlowtechnology.\n\u2022 Responsible for different Datamapping activities from Source systems to Teradata using utilities like TPump, FEXP, BTEQ, MLOAD, FLOAD etc\n\u2022 Analyze traffic patterns by calculating autocorrelation with different time lags.\n\u2022 Ensured that the model has low FalsePositiveRate and Textclassification and sentiment analysis for unstructured and semi-structured data.\n\u2022 Addressed over fitting by implementing of the algorithm regularization methods like L1 and L2.\n\u2022 Used Principal Component Analysis in feature engineering to analyze high dimensional data.\n\u2022 Used MLlib, Spark'sMachinelearning library to build and evaluate different models.\n\u2022 Implemented rule based expertise system from the results of exploratory analysis and information gathered from the people from different departments.\n\u2022 Created and designed reports that will use gathered metrics to infer and draw logical conclusions of past and future behavior.\n\u2022 Developed MapReduce pipeline for feature extraction using Hive and Pig.\n\u2022 Created DataQuality Scripts using SQL and Hive to validate successful data load and quality of the data. Created various types of data visualizations using Python and Tableau.\n\u2022 Communicated the results with operations team for taking best decisions.\n\u2022 Collected data needs and requirements by Interacting with the other departments."", u'Data Analyst\nLiquidity Services Inc - Washington, DC\nDecember 2014 to March 2016\nEnvironment:Python 2.x, CDH5, HDFS, Hadoop 2.3, Hive, Impala, AWS, Linux, Spark, Tableau Desktop, SQL Server 2014, Microsoft Excel, MATLAB, Spark SQL, Pyspark.\nClient: Liquidity Services Inc, Washington, District of Columbia. Dec 2014 - Mar 2016\nRole: Data Analyst\n\nDescription: Liquidity Services, Inc. operates various online auction marketplaces for surplus and salvage assets in the United States. Its auction marketplaces include liquidation.com, which enables corporations and selected government agencies located in the United States to sell surplus and salvage consumer goods and capital assets.\n\nResponsibilities:\n\u2022 Worked with BI team in gathering the report requirements and also Sqoop to export data into HDFS and Hive.\n\u2022 Involved in the below phases of Analytics using R, Python and Jupyter notebook.\n\u2022 Data collection and treatment: Analysed existing internal data and external data, worked on entry errors, classification errors and defined criteria for missing values\n\u2022 Data Mining: Used cluster analysis for identifying customersegments, Decisiontrees used for profitable and non-profitablecustomers, MarketBasketAnalysis used for customerpurchasingbehaviour and part/product association.\n\u2022 Developed multiple MapReduce jobs in Java for data cleaning and preprocessing.\n\u2022 Assisted with data capacity planning and node forecasting.\n\u2022 Installed, Configured and managed Flume Infrastructure\n\u2022 Administrator for Pig, Hive and HBase installing updates patches and upgrades.\n\u2022 Worked closely with the claims processing team to obtain patterns in filing of fraudulent claims.\n\u2022 Worked on performing major upgrade of cluster from CDH3u6 to CDH4.4.0\n\u2022 Developed MapReduce programs to extract and transform the data sets and results were exported back to RDBMS using Sqoop.\n\u2022 Patterns were observed in fraudulent claims using text mining in R and Hive.\n\u2022 Exported the data required information to RDBMS using Sqoop to make the data available for the claims processing team to assist in processing a claim based on the data.\n\u2022 Developed MapReduce programs to parse the raw data, populate staging tables and store the refined data in partitioned tables in the EDW.\n\u2022 Adept in statisticalprogramminglanguages like Rand Python including BigData technologies like Hadoop, and Hive.\n\u2022 Experience working as DataEngineer, BigDataSparkDeveloper, FrontEndDeveloper and ResearchAssistant.\n\u2022 Created tables in Hive and loaded the structured (resulted from MapReduce jobs) data\n\u2022 Using HiveQL developed many queries and extracted the required information.\n\u2022 Created Hivequeries that helped market analysts spot emerging trends by comparing fresh data with EDW reference tables and historical metrics.\n\u2022 Was responsible for importing the data (mostly log files) from various sources into HDFS using Flume\n\u2022 Enabled speedy reviews and first mover advantages by using Oozie to automate data loading into the HadoopDistributedFile System and PIG to pre-process the data.\n\u2022 Provided design recommendations and thought leadership to sponsors/stakeholders that improved review processes and resolved technical problems.\n\u2022 Managed and reviewed Hadoop log files.\n\u2022 Tested raw data and executed performance scripts.\n\nEnvironment:HDFS, PIG, HIVE, Map Reduce, Linux, HBase, Flume, Sqoop, R, VMware, Eclipse, Cloudera, Python.', u""Python Developer\nMacys.com, SFO, CA\nApril 2013 to November 2014\nDescription: Macy's, Inc. is one of the nation's premier retailers with over 700 departmental stores across the United States and a company which sells a great range of products.\n\nResponsibilities:\n\u2022 Developed a portal to manage and entities in a content management system using Flask\n\u2022 Designed the database schema for the content management system.\n\u2022 Designed email marketing campaigns and also created responsive web forms that saved data into a database usingPython/ Django Framework.\n\u2022 Worked on Hadoopsinglenode, Apachespark, Hiveinstallations\n\u2022 Installation, Configuration, Integration, Tuning, Backup, Crashrecovery, Upgrades, Patching, MonitoringSystemPerformance, System and NetworkSecurity and Troubleshooting of Linux/Unix Servers.\n\u2022 Developed views and templates in Django to create a user-friendly website interface.\n\u2022 Configured Django to manage URLs and application parameters.\n\u2022 Supported MapReduce Programs those are running on the cluster\n\u2022 Worked on CSV files while trying to get input from the MySQL database.\n\u2022 Wrote programs for performance calculations using Numpyandsqlalchemy.\n\u2022 Administered and monitored multi DatacenterCassandracluster based on the understanding of the CassandraArchitecture.\n\u2022 Extensively worked with Informatica in designing/developing ETL process to load data from xml sources to target database\n\u2022 Configured Ansible to manage AWS environments and automate the build process for core AMIs used by all application deployments including Autoscaling, and Cloudformation scripts.\n\u2022 Designed, automated the process of installation and configuration of secure DataStaxEnterpriseCassandra using chef\n\u2022 Wrote Python scripts to parse XML documents and load the data in database.\n\u2022 Worked in stages such as analysis and design, development, testing and debugging.\n\u2022 Built Web pages that are more user-interactive using jQueryplugins for Drag and Drop, AutoComplete, JSON, AngularJS, JavaScript.\n\nEnvironment:Python 2.7, Windows, MySQL, ETL, Ansibleflask and Python Libraries such as Numpy, sqlalchemy, Angular Js, MySQL DB."", u""R & SAS Programmer\nHindalco Industries - Mumbai, Maharashtra\nNovember 2011 to March 2013\nDescription: Hindalco Industries Ltd., an aluminum manufacturing company, is a subsidiary of the Aditya Birla Group. Its headquarters are at Mumbai, Maharashtra, India. It is the Flagship Company of the company in the metals business.\n\nResponsibilities:\n\u2022 Analyzed high volume, high dimensional client and survey data from different sources using SAS and R.\n\u2022 Manipulated large financial datasets, primarily in SQL and R\n\u2022 Used R for large matrix computation\n\u2022 Developed Algorithms (DataMiningQuery's) to extract data from data warehouse & databases to build Rules for the Analyst&Models Team.\n\u2022 Used R to import high volume of data\n\u2022 High level programming efficiency in the use of statistical modeling tools such as SAS, SPSS and R.\n\u2022 Developed predictive models using R to predict customers churn and classification of customers\n\u2022 Worked on Shiny and R application displaying machine learning for improving the forecast of business.\n\u2022 Developed, reviewed, tested & documented SAS programs/macros.\n\u2022 Created Templates by using SAS macro for existing reports to reduce the manual intervention.\n\u2022 Created Self-service tools for Onshore/Offshore team for data retrieval.\n\u2022 Worked on daily reports and used them for further analysis.\n\u2022 Developed/Designed templates for new data extraction requests.\n\u2022 Executed weekly reports for CommercialDataAnalyticsTeam.\n\u2022 Communicated progress to key Business partners and Analysts through status reports and tracked issues until resolution.\n\u2022 Created predictive and other analytically derived models for assessing sales.\n\u2022 Provided support in the design and implementation of ad hoc requests for Sales-RelatedPortfolioData.\n\u2022 Responsible for preparing test case documents and Technical specification documents."", u'SAS Developer/Analyst\nIntelenet Global Services - Mumbai, Maharashtra\nApril 2009 to October 2011\nDescription:Intelenet Global Services is a Global Business Process Outsourcing (BPO) &contact center provider firm headquartered in Mumbai, India, backed by Blackstone Group. The company offers 24/7 services for contact center solutions, transaction processing, finance & accounting, HRO and IT solutions to Fortune 500 companies in the UK, US, Australia, and India.\n\nResponsibilities:\n\u2022 Integrates all transaction data from multiple data sources used by Actuarial into a single repository.\n\u2022 Implemented and executes monthly incremental updates to the data environment.\n\u2022 Interacts with IT and finance and executes data validation tie-out reports.\n\u2022 Developed new programs and modified existing programs passing SAS macro variables to improve ease and efficiency as well as consistency of results.\n\u2022 Created Data transformation and DataLoading (ETL) scripts for DataWarehouses.\n\u2022 Implement fully automated data flow into Actuarial front end (Excel) Models using SAS process.\n\u2022 Creating SAS programs using SASDI Studio.\n\u2022 Validated the entire data process using SAS and BI tools.\n\u2022 Documenting of service requests by business users, developed code documentation, logs and outputs documentation, creating Test Plans and Production Release Notices for QC, QA and Production teams to perform further analysis.\n\u2022 Extensively used PROCSQL for column modifications, field populations on warehouse tables.\n\u2022 Additional responsibilities being Requirements gathering, Designing, Coding and Analysis, Testing, Debugging, Output generations in prescribed formats, extensive documentation of SAS Programs and Macros.\n\u2022 Developed distinct OLAP Cubes from SASDataset and generated results into the excel sheets.\n\u2022 Involved in discussions with business users to define metadata for tables to perform ETL process.\n\nEnvironment:Python 2.7, Windows, MySQL, ETL, Ansibleflask and Python Libraries such as Numpy, sqlalchemy, Angular Js, MySQL DB.']",[u'Bachelor of Computer Science in TOOLS AND TECHNOLOGIES'],[u'Informatica Power Centre'],degree_1 : Bachelor of Compter Science in TOOLS AND TECHNOLOGIES
0,https://resumes.indeed.com/resume/f33b0d66cabe36b7,"[u'Freelance Data scientist\nCNRS, INSERM - Austin, TX\nJanuary 2017 to Present\n\u2022 Analyzing experimental data to characterize neural coding for visual features and motor control two publications in preparation', u'Data Scientist - Postdoc Researcher\nINSERM, French National Institute of Health & Medical Research - Marseille, FR\nApril 2015 to March 2017\n\u2022 Helped a young team of experimentalists moving to IPython notebooks and implementing processing and data visualization tools.\n\u2022 Cleaned, organized, preprocessed and analyzed complex experi-\nmental data (neural recording and behavior tracking raw sets).', u'Data Scientist Junior - Postdoc Researcher\nCNRS, French National Center for Scientific Research - Marseille\nMarch 2013 to February 2015\n\u2022 Worked with a team of scientists on computer vision research as part of the European BrainScales Project.\n\u2022 Investigatedtheoddsofinherentversusobservedtrial-to-trialvari- ability in recorded neuronal spiking data: statistical analysis and modeling results published in Journal of Neurophysiology 2016.', u'Graduate Research Assistant\nLorraine University - Nancy\nOctober 2009 to September 2012\n\u2022 Collaborated with a group of neurophysiologists and theoreticians.\n\u2022 Investigated time discretization and asynchronous evaluation schemes in neural networks results published in ICNAAM 2009 conference, Journal of Neurophysiology-Paris 2011.\n\u2022 Designed and implemented a parsimonious neural network model of visual targets encoding published in Computational Intelligence book 2013, Biological Cybernetics Journal 2015.', u'Part-time Assistant Professor (64 hours/year)\nESSTIN, Sciences and Technologies Engineering School - Nancy\nOctober 2009 to September 2012\n\u2022 Teached CS courses: programming (Java), databases and relational\ndesign (mySQL, UML), web design (PHP, HTML, JavaScript).']","[u'Phd. in Computer Science', u'Master degree in Science & Executive Engineering, Specialization: Information Systems']","[u'University of Lorraine Nancy\nOctober 2009 to September 2012', u'\xc9cole Nationale Sup\xe9rieure des Mines Nancy\nOctober 2006 to September 2009']","degree_1 : Phd. in Compter Science, degree_2 :  Master degree in Science & Exective Engineering, degree_3 :  Specialization: Information Systems"
0,https://resumes.indeed.com/resume/a44a0b5e890bf42c,"[u'Data Scientist\nFuelX Inc - San Francisco, CA\nSeptember 2017 to Present\nCreated machine learning models that used natural language processing to extract contextually important words.\nCreated regression models to predict various business KPIs with more than 85% accuracy.\nDeployed unsupervised models as APIs for assistance in the business operations tasks which improved campaign performances by 15%.\nPresented results of machine learning experiments to stakeholders which included the CEO.', u'Data Science Intern\nFuelX Inc - San Francisco, CA\nJune 2016 to April 2017\nCreated experiments to prove the usability of machine learning approach to solve complex decision problems.\nCreated data pipelines to consume, transform and store data in machine learning ready format.\nAutomated more than 70% of the sales team reporting.\nCreated report and graph formats for the company dashboard.', u'Associate Solution Advisor\nDeloitte - Bangalore Urban, Karnataka\nJuly 2013 to July 2015\nWorked as part of the automation team for an insurance client.']","[u'MS Computer Science in Computer Science', u'BE in Computer Science']","[u'The University of Texas at Dallas Dallas, TX\nJanuary 2015 to August 2017', u'Visvesvaraya Technological University\nJanuary 2009 to June 2013']","degree_1 : MS Compter Science in Compter Science, degree_2 :  BE in Compter Science"
0,https://resumes.indeed.com/resume/463074e76473e087,"[u'Data Scientist\nDST Pharmacy Solutions\nSeptember 2017 to Present\nUtilized advanced analytics and modeling to bring life to pharmacy data\n\u2022 Created predictive models to solve business cases using R and Python\n\u2022 Communicated findings to executives through visualizations and reporting\n\u2022 Led company machine learning development and training program', u'Data Analyst\nDST Pharmacy Solutions\nJanuary 2017 to September 2017\nAnalyzed patient pharmacy claims and population health to discover insights\n\u2022 Utilized R, SQL, and Scala\n\u2022 Served as lead data analyst for Spark for z/OS proof of concept\n\u2022 Collaborated with research and development team to create new solutions', u""Manager of Chapter Administration\nAlpha Phi Omega\nJune 2015 to January 2017\nAnalyzed national membership data to support executive decisions\n\u2022 Published a public facing dashboard to support our organization's mission\n\u2022 Created ad-hoc reports and provided analysis to support several internal customers\n\u2022 Managed chapter and extension group reporting\n\u2022 Upgraded and maintained online reporting process""]","[u'Master of Business Administration', u'Master of Science in Business Intelligence and Analytics', u'Bachelor of Science in Actuarial Science and Applied Statistics']","[u'Rockhurst University Kansas City, MO\nJanuary 2018 to May 2019', u'Rockhurst University Kansas City, MO\nAugust 2016 to August 2018', u'Purdue University West Lafayette, IN\nAugust 2011 to May 2015']","degree_1 : Master of Bsiness Administration, degree_2 :  Master of Science in Bsiness Intelligence and Analytics, degree_3 :  Bachelor of Science in Actarial Science and Applied Statistics"
0,https://resumes.indeed.com/resume/dcae800fc4cd7056,"[u'Principal Data Scientist\nOracle Data Cloud\nFebruary 2016 to Present\n\u2022 Management of three junior data scientists\n\u2022 Built a large-scale matching engine for online and offline identities\n\u2022 Created a detailed quantitative model profiling purchase behavior\n\u2022 Devised quantitative metrics and updated machine learning methods resulting >10% increases across a range of metrics\n\u2022 Set up projects for young data scientists that would scale to their abilities\n\u2022 Coordinated cross-team international technical forum discussions', u'Senior Data Scientist\nAT&T Big Data\nJune 2014 to February 2016\n\u2022 Grew a social network based on AT&T proprietary data\n\u2022 Time-lined and modified projects to fit within the constraints and needs of multiple business units across the company\n\u2022 Designed predictive models for the propagation of churn within the network\n\u2022 Enhanced our knowledge of customer demographics using a network analysis\n\u2022 Developed an anomaly detection algorithm for security threats within a vpn\n\u2022 Mentored young data scientists in the AT&T Mentorship program', u'DESY Fellow\nCERN\nSeptember 2011 to June 2014\n\u2022 Contributed to the discovery of the Higgs Boson, for which the experiment was cited in the 2013 Nobel Prize in Physics, by reviewing machine learning methods\n\u2022 Lead a team of eight grad students and postdocs in the creation of an analysis software framework\n\u2022 Established a novel method to greatly enhance cross-experiment collaboration in top physics\n\u2022 Coordinated 30+ physicists in Northern Germany to define areas of participation at CERN\n\u2022 Communicated results at international conferences as well as national and internal meetings', u'Graduate Research Assistant\nFermi National Accelerator Laboratory\nJune 2005 to June 2011\n\u2022 Improved use of decision trees and minimization of measurement error in Higgs Searches\n\u2022 Generated a new combination algorithm resulting in 15% sensitivity gains\n\u2022 Optimization of the use of Fermi-based distributed computing network\n\u2022 Development of FPGA algorithms', u'Author\nD\xd8 collaboration\nJanuary 2007 to January 2011\nand the CMS collaboration (2012-2015) on over 100 published articles', u'Undergrad Research Assistant\nUniversity of Colorado Supersymmetry Group\nJanuary 2001 to May 2004\n\u2022 Constructed a scintillator-based calorimeter to parameterize models of a full-sized calorimeter\n\u2022 Performed a theoretical analysis of experimental results at a future linear collider']","[u'Ph.D. in High Energy Physics', u'M.S.', u'B.A. in B.A.M']","[u'University of Washington', u'University of Notre Dame. Physics', u'University of Colorado']","degree_1 : Ph.D. in High Energy Physics, degree_2 :  M.S., degree_3 :  B.A. in B.A.M"
0,https://resumes.indeed.com/resume/19052f641b3e7317,"[u""Data Scientist/ Machine Learning\nFIS - Jacksonville, FL\nJanuary 2017 to Present\nDescription: FIS provides financial software, world-class services and global business solutions. Let us help you compete and win in today's chaotic marketplace. Fidelity National Information Services Inc., better known by the abbreviation FIS, is an international provider of financial services technology and outsourcing services. FIS is the world's largest global provider dedicated to financial technology solutions. FIS empowers the financial world with software, services, consulting and outsourcing solutions focused on retail and institutional banking, payments, asset and wealth management, risk and compliance, trade enablement, transaction processing and record-keeping.\n\nResponsibilities:\n\u2022 Extracted data from HDFS and prepared data for exploratory analysis using data munging\n\u2022 Built models using Statistical techniques like Bayesian HMM and Machine Learning classification models like XGBoost, SVM, and Random Forest.\n\u2022 Participated in all phases of data mining, data cleaning, data collection, developing models, validation, visualization and performed Gap analysis.\n\u2022 A highly immersive Data Science program involving Data Manipulation&Visualization, Web Scraping, Machine Learning, Python programming, SQL, GIT, MongoDB, Hadoop.\n\u2022 Setup storage and data analysis tools in AWS cloud computing infrastructure.\n\u2022 Installed and used Caffe Deep Learning Framework\n\u2022 Worked on different data formats such as JSON, XML and performed machine learning algorithms in Python.\n\u2022 Worked as Data Architects and IT Architects to understand the movement of data and its storage and ER Studio 9.7\n\u2022 Used pandas, numpy, seaborn, matplotlib, scikit-learn, scipy, NLTK in Python for developing various machine learning algorithms.\n\u2022 Data Manipulation and Aggregation from different source using Nexus, Business Objects, Toad, Power BI and Smart View.\n\u2022 Implemented Agile Methodology for building an internal application.\n\u2022 Focus on integration overlap and Informatica newer commitment to MDM with the acquisition of Identity Systems.\n\u2022 Coded proprietary packages to analyze and visualize SPCfile data to identify bad spectra and samples to reduce unnecessary procedures and costs.\n\u2022 Programmed a utility in Python that used multiple packages (numpy, scipy, pandas)\n\u2022 Implemented Classification using supervised algorithms like Logistic Regression, Decision trees, Naive Bayes, KNN.\n\u2022 As Architect delivered various complex OLAPdatabases/cubes, scorecards, dashboards and reports.\n\u2022 Updated Python scripts to match training data with our database stored in AWS Cloud Search, so that we would be able to assign each document a response label for further classification.\n\u2022 Used Teradata utilities such as Fast Export, MLOAD for handling various tasks data migration/ETL from OLTP Source Systems to OLAP Target Systems\n\u2022 Data transformation from various resources, data organization, features extraction from raw and stored.\n\u2022 Validated the machine learning classifiers using ROC Curves and Lift Charts.\n\nEnvironment:Unix, Python 3.5.2, MLLib, SAS, regression, logistic regression, Hadoop 2.7.4, NoSQL, Teradata, OLTP, random forest, OLAP, HDFS, ODS, NLTK, SVM, JSON, XML and MapReduce."", u""Data Scientist\nCBRE - Dallas, TX\nOctober 2015 to November 2016\nDescription: CBRE Group, Inc. is the largest commercial real estate services and investment firm in the world. It is based in Los Angeles, California and operates more than 450 offices worldwide and has clients in more than 100 countries.\n\nResponsibilities:\n\u2022 Utilized Spark, Scala, Hadoop, HQL, VQL, oozie, pySpark, Data Lake, TensorFlow, HBase, Cassandra, Redshift, MongoDB, Kafka, Kinesis, Spark Streaming, Edward, CUDA, MLLib, AWS, Python, a broad variety of machine learning methods including classifications, regressions, dimensionally reduction etc.\n\u2022 Utilized the engine to increase user lifetime by 45% and triple user conversations for target categories.\n\u2022 Application of various machine learning algorithms and statistical modeling like decision trees, text analytics, natural language processing (NLP), supervised and unsupervised, regression models, social network analysis, neural networks, deep learning, SVM, clustering to identify Volume using scikit-learn package in python, Matlab.\n\u2022 Worked onanalyzing data from Google Analytics, AdWords, Facebook etc.\n\u2022 Evaluated models using Cross Validation, Log loss function, ROC curves and used AUC for feature selection and elastic technologies like ElasticSearch, Kibana.\n\u2022 Performed Data Profiling to learn about behavior with various features such as traffic pattern, location, Date and Time etc.\n\u2022 Categorized comments into positive and negative clusters from different social networking sites using Sentiment Analysis and Text Analytics\n\u2022 Performed Multinomial Logistic Regression, Decision Tree, Random forest, SVM to classify package is going to deliver on time for the new route.\n\u2022 Performed data analysis by using Hive to retrieve the data from Hadoop cluster, Sql to retrieve datafrom Oracle database and used ETL for data transformation.\n\u2022 Performed Data Cleaning, features scaling, features engineering using pandas and numpy packages in python.\n\u2022 Exploring DAG's, their dependencies and logs using AirFlow pipelines for automation\n\u2022 Performed data cleaning and feature selection using MLlib package in PySpark and working with deep learning frameworks such as Caffe, Neon.\n\u2022 Developed Spark/Scala, R Python for regular expression (regex) project in the Hadoop/Hive environment with Linux/Windows for big data resources.\n\u2022 Used clustering technique K-Means to identify outliers and to classify unlabeled data.\n\u2022 Tracking operations using sensors until certain criteria is met using AirFlowtechnology.\n\u2022 Responsible for different Data mapping activities from Source systems to Teradata using utilities like TPump, FEXP, BTEQ, MLOAD, FLOADetc\n\u2022 Analyze traffic patterns by calculating autocorrelation with different time lags.\n\u2022 Ensured that the model has low False Positive Rate and Text classification and sentiment analysis for unstructured and semi-structured data.\n\u2022 Addressed over fitting by implementing of the algorithm regularization methods like L1 and L2.\n\u2022 Used Principal Component Analysis in feature engineering to analyze high dimensional data.\n\u2022 Used MLlib, Spark's Machine learning library to build and evaluate different models.\n\u2022 Implemented rule based expertise system from the results of exploratory analysis and information gathered from the people from different departments.\n\u2022 Created and designed reports that will use gathered metrics to infer and draw logical conclusions of past and future behavior.\n\u2022 Developed MapReduce pipeline for feature extraction using Hive and Pig.\n\u2022 Created Data Quality Scripts using SQL and Hive to validate successful data load and quality of the data. Created various types of data visualizations using Python and Tableau.\n\u2022 Communicated the results with operations team for taking best decisions.\n\u2022 Collected data needs and requirements by Interacting with the other departments.\n\nEnvironment:Python 2.x, CDH5, HDFS, Hadoop 2.3, Hive, Impala, AWS, Linux, Spark, Tableau Desktop, SQL Server 2014, Microsoft Excel, Matlab, Spark SQL, Pyspark."", u'Data Analyst\nWalgreens - Deerfield, IL\nDecember 2013 to September 2015\nDescription:The Walgreens Companyis an American company that operatesas the second-largest pharmacy store chain in the United States. It specializes in filling prescriptions, health and wellness products, health information, and photo services\n\nResponsibilities:\n\u2022 Worked with BI team in gathering the report requirements and also Sqoop to export data into HDFS and Hive\n\u2022 Involved in the below phases of Analytics using R, Python and Jupyter notebook.\n\u2022 a. Data collection and treatment: Analysed existing internal data and external data, worked on entry errors, classification errors and defined criteria for missing values\nb. Data Mining: Used cluster analysis for identifying customer segments, Decision trees used for profitable and non-profitable customers, Market Basket Analysis used for customer purchasing behaviour and part/product association.\n\u2022 Developed multiple Map Reduce jobs in Java for data cleaning and preprocessing.\n\u2022 Assisted with data capacity planning and node forecasting.\n\u2022 Installed, Configured and managed Flume Infrastructure\n\u2022 Administrator for Pig, Hive and HBase installing updates patches and upgrades.\n\u2022 Worked closely with the claims processing team to obtain patterns in filing of fraudulent claims.\n\u2022 Worked on performing major upgrade of cluster from CDH3u6 to CDH4.4.0\n\u2022 Developed Map Reduce programs to extract and transform the data sets and results were exported back to RDBMS using Sqoop.\n\u2022 Patterns were observed in fraudulent claims using text mining in R and Hive.\n\u2022 Exported the data required information to RDBMS using Sqoop to make the data available for the claims processing team to assist in processing a claim based on the data.\n\u2022 Developed Map Reduce programs to parse the raw data, populate staging tables and store the refined data in partitioned tables in the EDW.\n\u2022 Created tables in Hive and loaded the structured (resulted from Map Reduce jobs) data\n\u2022 Using HiveQL developed many queries and extracted the required information.\n\u2022 Created Hive queries that helped market analysts spot emerging trends by comparing fresh data with EDW reference tables and historical metrics.\n\u2022 Was responsible for importing the data (mostly log files) from various sources into HDFS using Flume\n\u2022 Enabled speedy reviews and first mover advantages by using Oozie to automate data loading into the Hadoop Distributed File System and PIG to pre-process the data.\n\u2022 Provided design recommendations and thought leadership to sponsors/stakeholders that improved review processes and resolved technical problems.\n\u2022 Managed and reviewed Hadoop log files.\n\u2022 Tested raw data and executed performance scripts.\n\nEnvironment:HDFS, PIG, HIVE, Map Reduce, Linux, HBase, Flume, Sqoop, R, VMware, Eclipse, Cloudera, Python.', u""Data Scientist\nJohnson and Johnson - Raritan, NJ\nSeptember 2012 to November 2013\nDescription:Transamerica is the US-based brand of Aegon, a Dutch financial services firm. Aegon is one of the world's leading providers of life insurance, pensions and asset management and is helping approximately 30 million customers globally to achieve a lifetime of financial security.\n\nResponsibilities:\n\u2022 Manipulating, cleansing & processing data using Excel, Access and SQL.\n\u2022 Responsible for loading, extracting and validation of client data.\n\u2022 Modelled clean data into the Kafka servers for use over the spark engine.\n\u2022 Zookeeper along with Kafka was used to stream data and end-to-end client communication.\n\u2022 Performed transformations over the warehoused data using Scala& Python and modelled the data back into the servers for iterative transformations into KAFKA.\n\u2022 Modelled data using Machine learning libraries(Sci-kit learn) apart from SVN and KNN based classificationto create a training dataset for use in a predictive model.\n\u2022 Liaising with end-users and 3rd party suppliers.\n\u2022 Analyzing raw data, drawing conclusions & developing recommendations\n\u2022 Writing T-SQL scripts to manipulate data for data loads and extracts.\n\u2022 Developing data analytical databases from complex financial source data.\n\u2022 Performing daily system checks.\n\u2022 Data entry, data auditing, creating data reports & monitoring all data for accuracy.\n\u2022 Designing, developing and implementing new functionality.\n\u2022 Monitoring the automated loading processes.\n\u2022 Advising on the suitability of methodologies and suggesting improvements.\n\u2022 Carrying out specified data processing and statistical techniques.\n\u2022 Supplying qualitative and quantitative data to colleagues & clients.\n\u2022 Using Informatica& SAS to extract, transform & load source data from transaction\nsystems.\n\u2022 Performed sequential analytics using SAS Enterprise miner using jobs fed by the SAS Grid Manager.\n\u2022 Loaded packages and stored procedures using Base SAS and integrated functional and business requirements using the EBI suite.\n\u2022 Creating data pipelines using big data technologies like Hadoop, spark etc.\n\u2022 Creating statistical models using distributed and standalone models to build various diagnostics, predictive and prescriptive solution.\n\u2022 Utilize a broad variety of statistical packages like SAS, R , MLIB, Graphs, Hadoop, Spark , MapReduce, Pig\n\u2022 Performed a check using quality parameters fed using the SAS QC engine.\n\u2022 Created a UI dashboard for end users and performed prototype testing using Tableau.\n\u2022 Refine and train models based on domain knowledge and customer business objectives\n\u2022 Deliver or collaborate on delivering effective visualizations to support the client business objectives\n\u2022 Communicate to your peers and managers promptly as and when required.\n\u2022 Produce solid and effective strategies based on accurate and meaningful data reports and analysis and/or keen observations.\n\u2022 Establish and maintain communication with clients and/or team members; understand needs, resolve issues, and meet expectations\n\u2022 Developed web applications using .net technologies; work on bug fixes/issues that arise in the production environment and resolve them at the earliest\n\nEnvironment:Cloudera, HDFS, Pig, Hive, Map Reduce, python, Sqoop, Storm, Kafka, LINUX, Hbase, Impala, Java, SQL, Cassandra, MongoDB, SVN."", u'Data Architect/Data Modeler\nInfotech Enterprises IT Services Pvt. Ltd - Hyderabad, Telangana\nDecember 2010 to August 2012\nDescription: Infotech Enterprises IT Services Pvt. Ltd. (Infotech IT), part of the $ 260 million Infotech Enterprises group, leverages its business process knowledge, technological competence, strategic alliances and strong global presence to offer innovative IT solutions to the Retail Industry.\n\nResponsibilities:\n\u2022 Worked with large amounts of structured and unstructured data.\n\u2022 Knowledge in Machine Learning concepts (Generalized Linear models, Regularization, Random Forest, Time Series models, etc.)\n\u2022 Worked in Business Intelligence tools and visualization tools such as Business Objects, Tableau, ChartIO, etc.\n\u2022 Deployed GUI pages by using JSP, JSTL, HTML, DHTML, XHTML, CSS, JavaScript, AJAX.\n\u2022 Configured the project on WebSphere 6.1 application servers\n\u2022 Implemented the online application by using Core Java, Jdbc, JSP, Servlets and EJB 1.1, Web Services, SOAP, WSDL.\n\u2022 Handled end-to-end project from data discovery to model deployment.\n\u2022 Monitoring the automated loading processes.\n\u2022 Communicated with other Health Care info by using Web Services with the help of SOAP, WSDLJAX-RPC\n\u2022 Used Singleton, factory design pattern, DAO Design Patterns based on the application requirements\n\u2022 Used SAX and DOM parsers to parse the raw XML documents\n\u2022 Used RAD as Development IDE for web applications.\n\u2022 Preparing and executing Unit test cases\n\u2022 Used Log4J logging framework to write Log messages with various levels.\n\u2022 Involved in fixing bugs and minor enhancements for the front-end modules.\n\u2022 Implemented Microsoft Visio and Rational Rose for designing the Use Case Diagrams, Class model, Sequence diagrams, and Activity diagrams for SDLC process of the application\n\u2022 Doing functional and technical reviews\n\u2022 Maintenance in the testing team for System testing/Integration/UAT.\n\u2022 Guaranteeing quality in the deliverables.\n\u2022 Conducted Design reviews and Technical reviews with other project stakeholders.\n\u2022 Was a part of the complete life cycle of the project from the requirements to the production support\n\u2022 Created test plan documents for all back-end database modules\n\u2022 Implemented the project in Linux environment.\n\nEnvironment: R 3.0, Erwin 9.5, Tableau 8.0, MDM, QlikView, MLLib, PL/SQL, HDFS, Teradata 14.1, JSON, HADOOP (HDFS), MapReduce, PIG, Spark, R Studio, MAHOUT, JAVA, HIVE, AWS.', u""Data Analyst/ Data Modeler\nGlobalLogic Technologies - Hyderabad, Telangana\nApril 2009 to November 2010\nDescription:GlobalLogic provides experience design, Digital Product Engineering Services, and Agile Software Development to the world's top brands by leveraging UX UI Design, next-gen technologies, and cloud software, with end-to-end solution by the best Software Development Company.\n\nResponsibilities:\n\u2022 Developed Internet traffic scoring platform for ad networks, advertisers and publishers (rule engine, site scoring, keyword scoring, lift measurement, linkage analysis).\n\u2022 Responsible for defining the key identifiers for each mapping/interface.\n\u2022 Clients include eBay, Click Forensics, Cars.com, Turn.com, Microsoft, and Looksmart.\n\u2022 Designed the architecture for one of the first analytics 3.0. online platforms: all-purpose scoring, with on-demand, SaaS, API services. Currently under implementation.\n\u2022 Web crawling and text mining techniques to score referral domains, generate keyword taxonomies, and assess commercial value of bid keywords.\n\u2022 Developed new hybrid statistical and data mining technique known as hidden decision trees and hidden forests.\n\u2022 Reverse engineering of keyword pricing algorithms in the context of pay-per-click arbitrage.\n\u2022 Implementation of Metadata Repository, Maintaining Data Quality, Data Cleanup procedures, Transformations, Data Standards, Data Governance program, Scripts, Stored Procedures, triggers and execution of test plans\n\u2022 Performed data quality in Talend Open Studio.\n\u2022 Coordinated meetings with vendors to define requirements and system interaction agreement documentation between client and vendor system.\n\u2022 Automated bidding for advertiser campaigns based either on keyword or category (run-of-site) bidding.\n\u2022 Creation of multimillion bid keyword lists using extensive web crawling. Identification of metrics to measure the quality of each list (yield or coverage, volume, and keyword average financial value).\n\u2022 Enterprise Metadata Library with any changes or updates.\n\u2022 Document data quality and traceability documents for each source interface.\n\u2022 Establish standards of procedures.\n\u2022 Generate weekly and monthly asset inventory reports.\n\nEnvironment: Erwin r7.0, SQL Server 2000/2005, Windows XP/NT/2000, Oracle 8i/9i, MS-DTS, UML, UAT, SQL Loader, OOD, OLTP, PL/SQL, MS Visio, Informatica.""]","[u""Bachelor's""]",[u''],"degree_1 : ""Bachelors"""
0,https://resumes.indeed.com/resume/b2be9197be83761e,"[u'Junior Data Scientist(Research Assistant)\nNew Jersey Institute of Technology\nJanuary 2017 to August 2017\n\u2022 Analyzed and Visualized Mel-frequency cepstral coefficients (MFCC) for recorded speech samples\n\u2022 Resolved critical issues for Feature Extraction using various ASR specific smoothing and imputation techniques\n\u2022 Technologies: Python, MFCC, Smoothing Techniques, Feature Extraction, Automatic Speech Recognition', u""Data Analyst\nIBM - Hyderabad, Telangana\nDecember 2013 to July 2016\n\u2022 Implemented co-relation model to predict customer churn in a 30 days' time window in Agile Project Methodology\n\u2022 Designed recommendation system to predict best credit card deals for customers using Market Basket Analysis\n\u2022 Developed Teradata Scripts to perform ETL on data received from heterogeneous sources like Mainframe files, APIs, XMLs\n\u2022 Created a distributed ETL pipeline that loaded new data to DWH 20% faster than the existing pipeline for the same task\n\u2022 Built complex SQL query framework to automate the generated scripts process for stored procedures\n\u2022 Designed tableau dashboard to represent analytics for clients' data using complex aggregate and analytical functions\n\u2022 Technologies: ETL, SQL, Tableau, Business Intelligence, Financial Data Analysis, Market Basket Analysis, Agile"", u'Research Analyst\nArdent Collaborations - Kolkata, West Bengal\nOctober 2012 to November 2013\nBuilt macro scripts and VLOOKUP in Excel and analyzed data with pivot tables and validated claim using hypothesis testing\n\u2022']","[u'Master of Science in Information Systems', u'Bachelor of Engineering in Electronics & Communication']","[u'New Jersey Institute of Technology Newark, NJ\nDecember 2017', u'Future Institute of Engineering & Management Kolkata, West Bengal\nJune 2012']","degree_1 : Master of Science in Information Systems, degree_2 :  Bachelor of Engineering in Electronics & Commnication"
0,https://resumes.indeed.com/resume/89b320497ee42164,"[u'Data Scientist - Pricing & Revenue Management\nEventellect - Houston, TX\nJuly 2016 to Present\n\u2022Develop dynamic pricing models and revenue analysis software for professional sports teams and live entertainment properties to optimize ticket yield, revenue, and brand equity.\n\n\u2022Design and test production processes by implementing machine learning and statistical model techniques by creating experiments and distilling the results into actionable outcomes.\n\n\u2022Build interactive analytics dashboards and web applications for both internal and customer use.\n\n\u2022Develop new algorithms for forecasting supply and demand and build price response functions.', u'Data Analyst\nPlunkett Research, Ltd - Houston, TX\nAugust 2015 to July 2016\n\u2022Develop predictive analysis utilizing customer purchase behavior data with R statistical programming for various business analytics.\n\n\u2022 Produced customized reports by querying dimensions and metrics from Google Analytics API.\n\n\u2022 Conduct economic research by quantifying macroeconomic trends and analyzing financial statements of public companies across a variety of industry sectors. Data mining through various economic databases such as FRED, BEA, and BLS.', u""Economic Data Analyst\nGreater Houston Partnership - Houston, TX\nJune 2015 to August 2015\nResponsible for in-depth research and forecasting for the Greater Houston MSA economies and international economies. Responded to client requests and inquiries with general spreadsheet analysis, in collecting and interpreting data for clients. Produced written analysis for the Partnership's publications."", u'Data Analyst\nKingwood, TX\nNovember 2013 to July 2014\nCreated economic models utilizing Google Analytics data with R for A/B testing and various\nweb analytics. Fulfilled successful federal government contract proposals by responding to RFPs through government databases.', u'Policy Analyst\nHinckley Institute - Washington, DC\nJanuary 2012 to May 2012\n\u2022Responded to client requests that required interpreting and collecting research data that included updates to defense appropriations projects with insight into the budget, bill language, and committee markups.\n\n\u2022Attended congressional hearings and produced policy publications for clients that specialized in the defense and intelligence community.', u'Polling Fellow\nUtah State Senator Pat Jones - Salt Lake City, UT\nSeptember 2010 to November 2010\nPolled 400 independent leaning voters prior to the mid-term election for both qualitative and quantitative research with a high 65% positive persuasion rate during GOTV (get out the vote).']","[u'MA in Economics/Econometrics', u'BS in Economics', u'BS in Political Science']","[u'University of Houston Houston, TX\nJanuary 2017', u'University of Houston\nJanuary 2015', u'University of Utah\nJanuary 2013']","degree_1 : MA in Economics/Econometrics, degree_2 :  BS in Economics, degree_3 :  BS in Political Science"
0,https://resumes.indeed.com/resume/19350351f40587db,"[u'Data Scientist/Engineer (Consultant)\nGrowing TV Data Company\nJanuary 2017 to Present\nDevice and Data Identity Graph to provide Insights on TV viewership (first engineer in this role)\n\xa7 Developed identity graph based on disparate data sources outputting unique, singular profile.\n\xa7 Statistically computed a dynamic scoring and ranking mechanism comprising of various\nattributes like IP-address, deviceID, access time, device type, location, contentID etc.\n\xa7 Expanded cross device audience base by many folds and curated new insights for geo, demo,\netc joining with 3rd party identity data.\n\xa7 Defined and developed various data pipeline to onboard, efficiently process, optimize and store\nresults to be accessed by operations teams at scale.\n\xa7 Developing relevance model for TV viewership recommendation system.', u'Member of Technical Staff\nGlobalFoundries\nJanuary 2005 to Present\nPhysicist/Data Science/Engineering roles including sales/client communication in demonstrating\nmeaningful statistical insights from data and influencing product decisions (for manufacturing recipe,\nprocess flow, etc). Some relevant projects:\nMachine Learning Anomaly Detection model and Principal Component Analysis (PCA)\n\xa7 Developed statistical models to predict failures in manufacturing process for various attributes.\n\xa7 Published production ready Electrical Design Rules to optimize yield across technology nodes.\n\xa7 Performed PCA to eliminate correlation among all attributes, thereby enhancing model efficiency.\n\xa7 Responsible for creating and maintaining ETL data flows for failure prediction modules.\nShipped Software Development Kit (SDKs) per unique client requirements\n\xa7 Responsible for developing and managing python based SDK and libraries by applying\nfundamental solid state physics principles (CMOS, inductor, resistor, capacitor, etc).\n\xa7 Automated monitoring and QA processes for various SW flows.\n\xa7 Developed models to predict node performance (correlating yield to process maturity & time).']","[u'Master of Science', u'Master of Technology in Technology', u'Bachelor of Technology in Technology']","[u'SUNY Stony Brook, NY', u'Indian Institute of Technology [IIT Mumbai, Maharashtra', u'Indian Institute of Technology [IIT Mumbai, Maharashtra']","degree_1 : Master of Science, degree_2 :  Master of Technology in Technology, degree_3 :  Bachelor of Technology in Technology"
0,https://resumes.indeed.com/resume/14e99dc7049527b8,"[u""Lead Data Scientist\nMultiChoice\nMay 2016 to Present\nLead collaboration efforts with SAS and Multichoice as the SAS technical lead to successfully implement SAS\xae Marketing Optimization utilized by the Marketing and Retention Department. International speaker at the SAS Global Forum 2018 and author of a SAS paper on Using SAS\xae Marketing Optimization to improve Campaigning at Africa's largest Pay TV service provider.\n\nDeveloped and maintained relationships with key business stakeholders to drive technology initiatives that show immediate sustainable value. Design and build production-ready machine-learning models that added great value to the business. Data mining using state-of-the-art methods. SAS Administrator and system support for all SAS platforms in the Enterprise Information Management department. Provide SAS mentoring and internal training to help build SAS competency within the Enterprise Information Management department."", u'Data Scientist / Analytical Consultant\nT7C Consulting & Trading\nJune 2013 to April 2016\nManaged a SAS consultation team on various projects and internal developments around South Africa and the United Arab Emirates.\nExpanded business into new territories through strong business development by means of building lasting client relationships and performing sales activities.\n\nPartnered with SAS Institute and fulfilled the following professional services:\n\u2022 Successfully trained hundreds of SAS users on various SAS courses ranging from base programming to advance analytics.\n\u2022 Provided SAS Pre-sales consultation at various clients.\n\u2022 Installed and configured SAS software for clients', u'SAS Consultant / Sales & Marketing\nFifth Discipline Group\nMarch 2011 to May 2013\nGrew the business through active sales and marketing efforts. Performed SAS Pre-sales consultation and developed dynamic marketing strategies for the business.\n\nDelivered SAS educational training on various SAS software. Performed business intelligence consultation at Eskom (Electricity Provider in Southern-Africa).']","[u'Master of Science in Business Analytics', u'in Data Science', u'Bachelor of Commerce in Business Management', u'Bachelor of Commerce in Business Management']","[u'North-West University\nJanuary 2014 to May 2015', u'SAS Institute Academy for Data Science\nJanuary 2014 to March 2015', u'University of South Africa\nJanuary 2012 to May 2013', u'University of Pretoria Pretoria, Gauteng\nJanuary 2008 to May 2011']","degree_1 : Master of Science in Bsiness Analytics, degree_2 :  in Data Science, degree_3 :  Bachelor of Commerce in Bsiness Management, degree_4 :  Bachelor of Commerce in Bsiness Management"
0,https://resumes.indeed.com/resume/dcf218fa83e87451,"[u'Data Scientist\nClickTale - Berkeley, CA\nNovember 2016 to Present\n\xb7 Engaged in the design and implementation of statistical and predictive models.\n\xb7 Utilized Python (Pandas, PySpark, scikit-learn) to identify trends and relationships between data.\n\xb7 Implemented data visualization components in Bokeh and d3.js for presentation.\n\xb7 Modeled large survey and \ufb01nancial data with Granger Causality, FrenchFama5, and ARIMA.\n\xb7 Wrote Selenium web crawler to scrape and parse unstructured text data.\n\xb7 Utilized classi\ufb01cation algorithms to identify DOM elements on websites.', u'Research Associate\nGlass Lewis - San Francisco, CA\nFebruary 2016 to June 2016\n\xb7 Examining executive compensation programs of publicly traded companies to diagnose the structure\u2019s link between pay and performance and level of disclosure.\n\xb7 Analyzing the company\u2019s structure of short and long-term incentive programs in relation to market best practices and competitors to develop a voting recommendation for shareholders.\n\xb7 Considering internal guidelines and recommendations on corporate governance matters including executive compensation and equity-based incentive plans.\n\nSkills Used\npython, R, excel', u'Actuarial Analyst\nApplied Underwriters - Foster City, CA\nNovember 2015 to February 2016\nAutomate Excel spreadsheets using VBA for more efficiency and accuracy.\nAssisted in development and maintenance of a rate manual to quote workers compensation plans\nDrafting yearly rate filings for NCCI and Non-Monopolistic states.\nUsed SQL via Microsoft Access on mainframe to generate ad hoc reports as needed.\nAssisted in production of monthly triangle and monthly reserve updates.\n\nAccomplishments\nAutomated Spreadsheets via VBA\n\nSkills Used\nVBA, SQL, LaTeX', u'Econometrics Instructional Assistant\nUniversity of California - Santa Cruz, CA\nDecember 2013 to June 2015\n\u2022 Developed curriculum around topics covered in the classroom.\n\u2022 Clearly and concisely delivered econometric concepts to classes of 12\n\u2022 Taught theory of finance, including many asset optimization and risk modelling\n\u2022 Encouraged students to develop basic modeling techniques with use of R software']","[u'Masters of Science in Applied Economics and Finance', u'Bachelors of Science in Economics and Mathematics']","[u'University of California Santa Cruz, CA\nJanuary 2014 to January 2015', u'University of California Santa Cruz, CA\nJune 2014']","degree_1 : Masters of Science in Applied Economics and Finance, degree_2 :  Bachelors of Science in Economics and Mathematics"
0,https://resumes.indeed.com/resume/d673266a785f11d7,"[u'Data Scientist, Strategy & Operations Analytics\nDeloitte Consulting LLP - Rosslyn, VA\nJuly 2017 to Present\nPart of a center of excellence (InsightStudio) that provides data science and analytics consulting services to a variety of clients across industries', u'Equities Electronic Trading, Analyst\nGoldman Sachs & Co - Boston, MA\nJune 2014 to June 2016\nProvide real-time algorithmic trading support to a variety of institutional asset manager and hedge fund\nclients by monitoring and reacting to pre, intra, and post trade analytics\n\u2022 Responsible for modeling trading patterns for individual clients, suggesting optimal trading solutions, and empirically evaluating performance\n\u2022 Help clients maximize execution quality by examining raw execution data and implementing customizations\nto client algorithm and venue routing logic\n\u2022 Conduct research on US equity market microstructure and analyze new regulations to educate clients, as well as internal teams to continuously improve algorithm performance']","[u'M.A. in Quantitative Methods in the Social Sciences in Thesis titled Predicting stock returns', u'B.S. in Management in Finance and Information Systems']","[u'Columbia University, Graduate School of Arts and Sciences New York, NY', u'Boston College, Wallace E. Carroll School of Management Chestnut Hill, MA']","degree_1 : M.A. in Qantitative Methods in the Social Sciences in Thesis titled Predicting stock retrns, degree_2 :  B.S. in Management in Finance and Information Systems"
0,https://resumes.indeed.com/resume/fca7455e99f96b25,"[u""Data Scientist\nSpringboard - San Francisco, CA\nJune 2017 to Present\nPredicting Monthly Electricity Price of Residential Houses in the United States Time-Series Analysis\nWrangled and cleaned the data, and performed exploratory data analysis\nSet up test harness and utilize ARIMA models for prediction\nConduct model testing and evaluation\n\nFinding Unique Patterns in Dialysis Facilities with Patients' Data using\nunsupervised learning algorithms\nCarried out data wrangling and cleaning, and applied feature selection\nengineering\nDid exploratory data analysis and inferential statistics, and standardized the\nfeatures\nUtilized unsupervised learning algorithms such as KMeans for model fitting\nand clustering\nDid PCA to reduce the dimensionality of the features for data visualization,\nand evaluated the model using Silhouette score\n\nMachine Learning for supervised and unsupervised related projects\nApplied different typed of supervised learning algorithms for different\ndatasets.\nApplied the different types of clustering algorithms\nEvaluated the performance of each clustering algorithms and selected the\nmodel with good score\n\nInferential Statistics based Analysis\nApplied statistical analysis for three different dataset\nDeveloped hypothesis for each dataset and tested them\n\nSQL Analytics: Investigating a Drop in User Engagement dashboards\nSet up different possible hypothesis for the problem\nApplied SQL analytics to test the hypothesis, and identified the possible\nproblem"", u'Data Scientist/Analyst\nBerger Geoscience, LLC - Houston, TX\nJune 2013 to April 2017\nAnalyzed pore pressure related data of deep water oil/gas wells of Gulf of Mexico in real-time future predictions\nPredicted pore pressure in real-time from offset wells best fitted models\nMore than 6 years experience in the oil/gas\nAccomplished exploratory data analysis work on pore pressure-fracture gradient data (PPFG)\nAnalyzed PWD data to detect any anomaly in downhole pressure using Excel', u'Wellsite Geologist and Geosteering Data Analyst\nHorizon Well Logging, LLC - Tulsa, OK\nFebruary 2011 to May 2013\nReviewed and evaluating all aspects of wireline open-hole logging data on site\n\nCorrelated real-time mudlogs and gamma/resisitivity logs with offset well data\n\nCommunicated daily geological drilling information/geosteering updates with an operation geologist', u'Research and Teaching Assistant\nBowling Green State University - Bowling Green, OH\nJanuary 2009 to February 2011\nApplied LandSat based reflectance algorithms to map contaminant sludge concentration\nCreated MODIS Satellite data based algorithms to map the concentration of toxic cyanobacteria in Great Lakes']","[u'Certificate in Data Science', u'Certificate in Data Science Specialization', u'Msc in Geology', u'Bsc in Geology']","[u'Springboard\nJune 2017 to February 2018', u'Johns Hopkins University through Coursera\nSeptember 2016 to March 2017', u'Bowling Green State University-Main Campus Bowling Green, OH\nJanuary 2011', u'University of Asmara Asmara\nAugust 2002']","degree_1 : Certificate in Data Science, degree_2 :  Certificate in Data Science Specialization, degree_3 :  Msc in Geology, degree_4 :  Bsc in Geology"
0,https://resumes.indeed.com/resume/61452b1b7a701877,"[u'Data Scientist\nSignature Bank - New York, NY\nJanuary 2017 to Present\nSignature Bank is a full-service commercial bank which provides service in the tristate area. Worked on optimizing their marketing campaign efficiency for enhanced customer acquisition and retention and also supporting various departments to identify fraudulent transactions using advanced data analytic approaches.\n\nResponsibilities:\n\u2022 Participated in all phases of data mining, data collection, data cleaning, developing models, validation, and visualization to deliver data science solutions.\n\u2022 Built a model to predict direct mail marketing campaign for loan pre-approvals to optimize the number of letters being sent.\n\u2022 Worked on fraud detection analysis on payments transactions using the history of transactions with supervised learning methods.\n\u2022 Collected data using Hadoop tools to retrieve the data required for building models such as Hive and Pig Latin.\n\u2022 Worked on Amazon Web Services cloud virtual machine to do machine learning on big data.\n\u2022 Developed Spark Python modules for machine learning & predictive analytics in Hadoop.\n\u2022 Implemented a Python-based distributed random forest via PySpark.\n\u2022 Used Pandas, Numpy, Seaborn, Matplotlib, Scikit-learn in Python for developing various machine learning models and utilized algorithms such as Linear regression, Logistic regression, Gradient Boosting, SVM and KNN\n\u2022 Used cross-validation to test the models with different batches of data to optimize the models and prevent overfitting.\n\u2022 Used PCA and other feature engineering techniques for high dimensional datasets while maintaining the variance of most important features.\n\u2022 Created Transformation Pipelines for preprocessing large amount of data with methods such as imputing, scaling, selecting and etc.\n\u2022 Performed Ensemble methods to increase the accuracy of the training model with different Bagging and Boosting methods\n\nTechnology Stack: Hadoop 2.x, HDFS, Hive, Pig Latin, PySpark, Python 3.x (Numpy, Pandas, Scikit-learn, Matplotlib), Jupyter, Github, Linux', u'Data Scientist\nMediaOcean - Chicago, IL\nApril 2015 to December 2016\nMediaOcean is an advertising services and software company that delivers media planning and buying services to advertisers and agencies. The project was to build models for targeting advertisement audiences and segmentation using machine learning algorithms and solutions.\n\nResponsibilities:\n\u2022 Designed and developed machine learning models to improve advertising agencies programmatic strategies for optimal biddings of impression opportunities.\n\u2022 Worked on unsupervised segmentation, targeting based on the social network activities and finding clusters of user groups using k-means method\n\u2022 Worked with high dimensional data sets retrieved from users, media agencies or third-party app and used methods such as PCA, LDA and Kernel Approximations.\n\u2022 Used different feature engineering methods in Python (Pandas, Numpy, Matplotlib, Seaborn) to cleanse high dimensional datasets and prepare it for modeling.\n\u2022 Developed and Supervised classification models to predict if the users will click on certain ads. Using algorithms such as Stochastic Gradient Descent (SGD), Logistic Regression, Random Forest, SVM and more.\n\u2022 Analyzed and visualized different segments of users to understand their advertisement behaviors better with Tableau.\n\u2022 Helped the team to generate more ideas and ask new questions about the dataset to improve our accuracies and gain more insights\n\u2022 Worked in an agile environment using Jira for ticketing and confluence for documentation\n\nTechnology Stack: Python 3.x (Pandas, Numpy, Matplotlib, Seaborn), Jira/Confluence, Tableau 9.x, Github, Jupyter, Linux', u'Data Analyst/Data Scientist\nBelvedere Trading - Chicago, IL\nAugust 2013 to April 2015\nBelvedere Trading is a proprietary trading firm that specializes in equity index and commodity derivatives. The project was to extract data from multiple source systems and create reporting solutions using machine learning for optimizing the insurance comparison process.\n\nResponsibilities:\n\u2022 Worked with data scientists and the research team to gain valuable insights\n\u2022 Worked with Amazon EC2 based cloud-hosted architecture systems to provide solutions for client.\n\u2022 Developed a portfolio optimization system for longing strategies using stocks history, budget and number of days for investment as input to offer the most profitable stocks to buy using Python and Scikit-learn\n\u2022 Used Time Series Analysis to gain insight about stocks and derivate investments based on the historic data available\n\u2022 Generated comprehensive analytical reports by running SQL queries against current databases to conduct data analysis.\n\u2022 Used big data technologies to access and extract data with tools such as Apache Hadoop HDFS, Hive and Pig\n\u2022 Good Knowledge in AWS Environment for loading data files from the cloud servers.\n\u2022 Preprocessed the data based on the trading strategy and created new financial features in the dataset.\n\u2022 Collaborated with business leaders to analyze problems optimize processes and build presentation dashboards.\n\u2022 Created ad-hoc reports about stocks, futures, options and other investments in Tableau.\n\u2022 Implemented data refreshes on Tableau Server for biweekly and monthly increments based on business change to ensure that the views and dashboards were displaying the changed data accurately.\n\nTechnology Stack: Apache Hadoop 2, Hive, Pig Latin, Linux, SQL, Tableau, Python 3.2 (Numpy, Pandas, Scikit-learn, Matplotlib), AWS', u""Database/BI Developer\nCarePoint Health - Jersey City, NJ\nJanuary 2012 to July 2013\nCarePoint is a for-profit organization healthcare provider which combines the resources of three area hospitals Bayonne Medical Center, Christ Hospital in Jersey City, and Hoboken University Medical Center. The project was to create reports for claims, members, healthcare centers, finances and develop ETL packages to support necessary data processing and quality management.\n\nResponsibilities:\n\u2022 Became an expert in the company's database and business model and was actively part of gathering user/project requirements from different stakeholders to convert into documentations required for the project in hand.\n\u2022 Extracted data using T-SQL in SQL server to write Queries, Stored procedures, Triggers, Views, Temp Tables and User-Defined Functions (UDFs).\n\u2022 Designed and developed ETL packages using SSIS to create Data Warehouses from different tables and file sources like Flat and Excel files.\n\u2022 Used different methods in SSIS such as derived columns, aggregations, Merge joins, count, conditional split and more to transform the data.\n\u2022 Developed reporting solutions for different stakeholders from mock-up till deployment in different areas such as Claims, Transactions, Supply, Assets and others in SSRS.\n\u2022 Optimized Queries in T-SQL by removing redundancies, retrieving essential data and using SQL methods like Joins efficiently."", u'SQL Server Management Studio, SQL Server Integration Service, SQL Server Reporting Service, Windows\nTechnology Stack\nJanuary 2010 to January 2010\n7, MS Office Suite 2010']",[u'Bachelors in Computer Science'],"[u'University of Illinois Urbana-Champaign Urbana-Champaign, IL']",degree_1 : Bachelors in Compter Science
0,https://resumes.indeed.com/resume/bc7a43517eebeffc,"[u'Data Scientist\nCigna - Fremont, CA\nJuly 2017 to Present\nResponsibilities:\nParticipated in all phases of data mining, data collection, data cleaning, developing models, validation, visualization, and performed Gap analysis, Data Manipulation and Aggregation from a different source using Nexus, Toad, Business Objects, Power BI and Smart View, Data transformation from various resources, data organization, and features extraction from raw and stored.\n\nWorked as Data Architects and IT Architects to understand the movement of data and its storage and ER Studio 9.7, Programmed a utility in Python that used multiple packages (scipy, numpy, & pandas), Updated Python scripts to match training data with our database stored in AWS Cloud Search, so that we would be able to assign each document a response label for further classification.\n\nImplemented Classification using supervised algorithms like Logistic Regression, Decision trees, KNN, Naive Bayes, Setup storage and data analysis tools in Amazon Web Services cloud computing infrastructure, Used pandas, numpy, Seaborn, scipy, matplotlib, sci-kit-learn, NLTK in Python for developing various machine learning algorithms, Installed and used Caffe Deep Learning Framework, Focus on integration overlap and Informatica newer commitment to MDM with the acquisition of Identity Systems.\n\nWorked on different data formats such as JSON, XML and performed machine learning algorithms in Python, Implemented Agile Methodology for building an internal application, Designed both 3NF data\nModels for ODS, OLTP systems and dimensional data models using Star and Snowflake Schemas.\n\nAs Architect delivered various complex OLAP databases/cubes, scorecards, dashboards and reports, Handled importing data from various data sources, performed transformations using Hive, Map Reduce, and loaded data into HDFS.\n\nEnvironment: R 9.0, Informatica 9.0, ODS, OLTP, Hive, OLAP, Requisite Pro, Mainframes MS Vision, Rational Rose, MS Excel, DB2, Metadata, Oracle 10g.', u""Data Scientist\nInternational Paper - Fremont, CA\nApril 2015 to June 2017\nResponsibilities:\nAnalyzed the business requirements of the project by studying the Business Requirement Specification document.\n\nPerformed Exploratory Data Analysis and Data Visualizations using R, and Tableau, Extensively worked on Data Modeling tools Erwin Data Modeler to design the data models, Worked with Data Governance, Data quality, data lineage, Data architect to design various models and processes, Designed tables and implemented the naming conventions for Logical and Physical Data Models in Erwin 7.0.\n\nPerform a proper EDA, Uni-variate and bi-variate analysis to understand the intrinsic effect/combined effects, Independently coded new programs and designed Tables to load and test the program effectively for the given POC's using with Big Data/Hadoop, whenever source data elements were missing in source tables, these were modified/added inconsistency with third normal form based OLTP source database.\n\nWrote stored procedures, triggers utilizing T-SQL, Explained the data model to the other members of the development team.\n\nExplored and Extracted data from source XML in HDFS, preparing data for exploratory analysis using data munging, Designed data models and data flow diagrams using Erwin and MS Visio, wrote XML parsing module that populates alerts from the XML file into the database tables utilizing JAVA, JDBC, BEA WEBLOGIC IDE, and Document Object Model, As an Architect implemented MDM hub to provide clean, consistent data for an SOA implementation.\n\nDesigned a mapping to process the incremental changes that exist in the source table, Created indexes, made query optimizations, implemented & Maintained the Conceptual, Logical & Physical Data Models using Erwin for forwarding/Reverse Engineered Databases.\n\nEnvironment: Hadoop, SSRS, SSIS, Crystal Reports, Query Analyzer, DTS, Windows Enterprise Server 2000, SQL Profiler, and SQL Server 2008R2/2005 Enterprise."", u""Data Scientist\nLSB Industries Inc - Oklahoma City, OK\nDecember 2012 to March 2015\nDescription: LSB Industries, Inc., through its subsidiaries, engages in the manufacture and sale of geothermal and water source heat pumps, air handling products, and chemical products.\n\nResponsibilities:\nData Visualizations using R, and Tableau, Coded R functions to interface with Caffe Deep Learning Framework, Implemented end-to-end systems for Data Analytics, , Performed Exploratory Data Analysis and Created dash boards and visualization on regular basis using ggplot2 and Tableau, Data Automation and integrated with custom visualization tools using R, Mahout, Hadoop, and MongoDB , Worked with several R packages including knitr, dplyr, SparkR, CausalInfer, space-time, Developed, Implemented & Maintained the Conceptual, Logical & Physical Data Models using Erwin for forwarding/Reverse Engineered Databases.\n\nWorked with Data Governance, Data quality, data lineage, Data architect to design various models and processes, Performed data cleaning and imputation of missing values using R, Used Hive to store the data and perform data cleaning steps for huge datasets.\n\nWorked with Hadoop eco system covering HDFS, HBase, YARN, and MapReduce, Independently coded new programs and designed Tables to load and test the program effectively for the given POC's using with Big Data/Hadoop, Designed data models and data flow diagrams using Erwin and MSVisio, Established Data architecture strategy, best practices, standards, and roadmaps\n\nAs an Architect implemented MDM hub to provide clean, consistent data for an SOA implementation, Perform a proper EDA, Uni-variate and bi-variate analysis to understand the intrinsic effect/combined effects, working in Amazon Web Services cloud computing environment, worked with BTEQ to submit SQL statements, import and export data, and generate reports in Teradata.\n\nInteracted with the other departments to understand and identify data needs and requirements and work with other members of the IT organization to deliver data visualization and reporting solutions to address those needs, Lead the development and presentation of a data analytics data-hub prototype with the help of the other members of the emerging solutions team.\n\nEnvironment: Erwin r, Informatica, ODS, OLTP, Oracle 10g, Hive, OLAP, DB2, Metadata, MS Excel, Mainframes MS Visio, Rational Rose, and Requisite Pro, Hadoop, PL/SQL, etc."", u'Data Analyst/Data Modeler\nAccenture - Bengaluru, Karnataka\nApril 2009 to November 2012\nDescription: Accenture PLC is a global management consulting and professional services company that provides strategy, consulting, digital, technology and operations.\n\nResponsibilities:\nCoordinated meetings with vendors to define requirements and system interaction agreement documentation between client and vendor system, Creation of multimillion bid keyword lists using extensive web crawling and text mining techniques to score referral domains, generate keyword taxonomies, and assess commercial value of bid keywords.\n\nImplementation of Metadata Repository, Maintaining Data Quality, Data Cleanup procedures, Transformations, Data Standards, Data Governance program, Scripts, Stored Procedures, triggers and execution of test plans, developed Internet traffic scoring platform for ad networks, advertisers, and publishers (rule engine, site scoring, keyword scoring, lift measurement, linkage analysis),\n\nDeveloped new hybrid statistical and data mining technique known as hidden decision trees and hidden forests, Designed the architecture for one of the first analytics 3.0. Online platforms: all-purpose scoring, with on-demand, SaaS, API services. Currently under implementation,\n\nReverse engineering of keyword pricing algorithms in the context of pay-per-click arbitrage, Identification of metrics to measure the quality of each list (yield or coverage, volume, and keyword average financial value).\n\nAutomated bidding for advertiser campaigns based either on keyword or category (run-of-site) bidding, Responsible for defining the key identifiers for each mapping/interface, Enterprise Metadata Library with any changes or updates, Document data quality and traceability documents for each source interface, Establish standards of procedures, Generate weekly and monthly asset inventory reports.\n\nEnvironment: Erwin r, SQL Server 2000/2005, Windows XP/NT/2000, Oracle, MS-DTS, UML, UAT, SQL Loader, OOD, OLTP, PL/SQL, MS Visio, Informatica.']","[u""Bachelor's""]",[u''],"degree_1 : ""Bachelors"""
0,https://resumes.indeed.com/resume/57e5813c924a9ef1,"[u'Capstone Project Data Scientist\nUptake - Chicago, IL\nMay 2017 to August 2017\nAssessed applications of data science in the IoT security domain.\nPerformed exploratory data analysis to find feature interactions, identify attacks on various sensors and equipment.\nBuilt models to classify the events as attack or non-attack with over 97% F1 Score. XGboost, clustering, timeseries, PCA', u'Senior Programming Analyst\nBarclays Technology Centre India - Pune, Maharashtra\nNovember 2012 to July 2016\nAssisted in maintaining tier 1 and tier 2 applications which processed payments of nearly 350M GBP daily and generated statements worth 900M GBP.\nSpearheaded in-depth analysis of a statement generation batch which was generating the merchant statements incorrectly and devised a work around that saved Barclaycard the growing reputation damage.\nDeveloped scripts to gather daily stats and data which reduced manual efforts by around 10%.']","[u'Master of Data Science in EDUCATION', u'B.E. in Electronics']","[u'Illinois Institute of Technology Chicago, IL\nAugust 2016 to December 2017', u'Vishwakarma Institute of Technology Pune, Maharashtra\nJuly 2008 to May 2012']","degree_1 : Master of Data Science in EDUCATION, degree_2 :  B.E. in Electronics"
0,https://resumes.indeed.com/resume/81821974fff584e6,"[u'Data Scientist - Capstone project\nArizona State University - Chandler, AZ\nDecember 2017 to Present\n\u2022 Created strategies for better negotiation for lift parts on different sites thereby increasing the potential savings by 10%\n\u2022 Built an algorithm to reduce the overall cost by considering key factors like lead time, discount, fill rate, tax etc', u'Data Science Researcher\nArizona State University - Tempe, AZ\nOctober 2017 to Present\nArizona\n\u2022 Developed machine learning models with text mining to improve the energy efficiency for ASU and recommended strategies\n\u2022 Reduced the uncertainty of the component failures in the HVAC system by 30% and improved accuracy by implementing ML\nalgorithms with feature selection', u'Data Miner/ Assistant Manager\nIBM - Chennai, Tamil Nadu\nMarch 2016 to July 2017\nIndia\n\u2022 Built multiple machine learning models and regression analysis to improve the vaccine business for a pharmaceutical client by profiling the customers and identifying the scope of improvement in vaccine operations to increase vaccines sales by 5%.\n\u2022 Worked on advanced analytics projects to improve the client\'s e-commerce channel and recommended strategies based on the insights which helped them to identify all the potential customers and ~30% was converted as customers\n\u2022 Worked on social media analysis for multiple clients to measure the social media sentiment and presented insights and data- driven recommendations to increase positive sentiment\n\u2022 Developed and implement chatbot using IBM Watson which addressed commonly asked a question which reduced agent\'s\ndependency by ~50%\n\u2022 Awarded ""Best performer award"" twice and two clients recommended ""Excellence award"" during my period in IBM for effective\nproject management, putting client success first and communication skills', u""Decision Scientist\nMu Sigma Business Solutions Pvt Ltd - Bengaluru, Karnataka\nJuly 2013 to March 2016\nIndia\n\u2022 Built an assortment optimization algorithm to increase the sales per linear feet for the world's largest retailer and optimized the sales per linear feet by 40%\n\u2022 Worked on end to end supply chain planning to improve promotional item sales for a China-based retailer\n\u2022 Implemented analytics projects in inventory shrinkage, compliance, and social media marketing in analytical problem space\n\u2022 Built statistical models to improve brand and profits for a US-based manufacturing client and increased sales by 10%\n\u2022 Obtained the 'Decision Sciences Beginner Certification' from Mu Sigma by covering courses in Business, Technology,\nMathematics, and Management within 18 months\n\u2022 Awarded Star performer of the team twice for effective project and team management, communication skills and learning curve""]","[u'Master of Science in Business Analytics in MSBA', u'in Velammal Engineering']","[u'Arizona State University -W. P. Carey School of Business Tempe, AZ\nMay 2018', u'Anna University Chennai, Tamil Nadu\nJune 2013']","degree_1 : Master of Science in Bsiness Analytics in MSBA, degree_2 :  in Velammal Engineering"
0,https://resumes.indeed.com/resume/a61b992c41fdaca9,"[u""Data Scientist\nVNB Consulting - Edison, NJ\nJanuary 2018 to February 2018\nWeb scraping clients\u2019 online customer reviews for sentiment analysis using Python and R. Generate customer-oriented report and visualization in Power BI for clients to develop marketing campaign strategy.\n\nDevelop predictive models for sentiment analysis using Azure ML Studio. Best model, logistic regression, achieve 20% improvement in accuracy. Develop predictive models for product profile which uses manufacturing data in SAP system to classify invoice amount into credit or debit.\n\nPredict revenue and order quantity for clients\u2019 company using time series model.\n\nThe reason I left the company is that client decided not to proceed the project, because they're looking for something else."", u'Data Scientist Intern\nVNB Consulting - Edison, NJ\nNovember 2017 to January 2018\nBring stream social media data from Twitter and Facebook, using Python, in to Azure Blob Storage and SQL\ndata Warehouse to predict retail sales for clients.', u'Undergraduate Research Assistant\nIowa State University - Ames, IA\nFebruary 2016 to June 2016\nUsed Matlab to build up theoretical differential equation system for treating Amyotrophic Lateral Sclerosis (ALS) by discovering connections between cell and cytokines that affect ALS, based on an academic paper. Theoretical differential equation quantifies number of cells and amount of cytokines in ALS patients\' bodies.\n\nUsed Matlab to optimize over 70 parameters in 20 differential equations, and simulate data from ""theoretical patients"" using differential equation system']","[u'Master of Arts in Statistics in Statistics', u'', u'Bachelor of Science in Statistics']","[u'Columbia University, Graduate School of Arts and Sciences New York, NY\nDecember 2017', u'Iowa State University, College of Liberal Arts and Sciences, Ames\nAugust 2012 to May 2016', u'University Honors Program']","degree_1 : Master of Arts in Statistics in Statistics, degree_2 :  , degree_3 :  Bachelor of Science in Statistics"
0,https://resumes.indeed.com/resume/0e76ceaf61104d4a,"[u""Data Scientist/ Machine Learning\nFIS - Jacksonville, FL\nJanuary 2017 to Present\nDescription: FIS provides financial software, world-class services and global business solutions. Let us help you compete and win in today's chaotic marketplace. Fidelity National Information Services Inc., better known by the abbreviation FIS, is an international provider of financial services technology and outsourcing services. FIS is the world's largest global provider dedicated to financial technology solutions. FIS empowers the financial world with software, services, consulting and outsourcing solutions focused on retail and institutional banking, payments, asset and wealth management, risk and compliance, trade enablement, transaction processing and record-keeping.\n\nResponsibilities:\n\u2022 Extracted data from HDFS and prepared data for exploratory analysis using data munging\n\u2022 Built models using Statistical techniques like Bayesian HMM and Machine Learning classification models like XGBoost, SVM, and Random Forest.\n\u2022 Participated in all phases of data mining, data cleaning, data collection, developing models, validation, visualization and performed Gap analysis.\n\u2022 A highly immersive Data Science program involving Data Manipulation&Visualization, Web Scraping, Machine Learning, Python programming, SQL, GIT, MongoDB, Hadoop.\n\u2022 Setup storage and data analysis tools in AWS cloud computing infrastructure.\n\u2022 Installed and used Caffe Deep Learning Framework\n\u2022 Worked on different data formats such as JSON, XML and performed machine learning algorithms in Python.\n\u2022 Worked as Data Architects and IT Architects to understand the movement of data and its storage and ER Studio 9.7\n\u2022 Used pandas, numpy, seaborn, matplotlib, scikit-learn, scipy, NLTK in Python for developing various machine learning algorithms.\n\u2022 Data Manipulation and Aggregation from different source using Nexus, Business Objects, Toad, Power BI and Smart View.\n\u2022 Implemented Agile Methodology for building an internal application.\n\u2022 Focus on integration overlap and Informatica newer commitment to MDM with the acquisition of Identity Systems.\n\u2022 Coded proprietary packages to analyze and visualize SPCfile data to identify bad spectra and samples to reduce unnecessary procedures and costs.\n\u2022 Programmed a utility in Python that used multiple packages (numpy, scipy, pandas)\n\u2022 Implemented Classification using supervised algorithms like Logistic Regression, Decision trees, Naive Bayes, KNN.\n\u2022 As Architect delivered various complex OLAPdatabases/cubes, scorecards, dashboards and reports.\n\u2022 Updated Python scripts to match training data with our database stored in AWS Cloud Search, so that we would be able to assign each document a response label for further classification.\n\u2022 Used Teradata utilities such as Fast Export, MLOAD for handling various tasks data migration/ETL from OLTP Source Systems to OLAP Target Systems\n\u2022 Data transformation from various resources, data organization, features extraction from raw and stored.\n\u2022 Validated the machine learning classifiers using ROC Curves and Lift Charts.\n\nEnvironment:Unix, Python 3.5.2, MLLib, SAS, regression, logistic regression, Hadoop 2.7.4, NoSQL, Teradata, OLTP, random forest, OLAP, HDFS, ODS, NLTK, SVM, JSON, XML and MapReduce."", u""Data Scientist\nCBRE - Dallas, TX\nOctober 2015 to November 2016\nDescription: CBRE Group, Inc. is the largest commercial real estate services and investment firm in the world. It is based in Los Angeles, California and operates more than 450 offices worldwide and has clients in more than 100 countries.\n\nResponsibilities:\n\u2022 Utilized Spark, Scala, Hadoop, HQL, VQL, oozie, pySpark, Data Lake, TensorFlow, HBase, Cassandra, Redshift, MongoDB, Kafka, Kinesis, Spark Streaming, Edward, CUDA, MLLib, AWS, Python, a broad variety of machine learning methods including classifications, regressions, dimensionally reduction etc.\n\u2022 Utilized the engine to increase user lifetime by 45% and triple user conversations for target categories.\n\u2022 Application of various machine learning algorithms and statistical modeling like decision trees, text analytics, natural language processing (NLP), supervised and unsupervised, regression models, social network analysis, neural networks, deep learning, SVM, clustering to identify Volume using scikit-learn package in python, Matlab.\n\u2022 Worked onanalyzing data from Google Analytics, AdWords, Facebook etc.\n\u2022 Evaluated models using Cross Validation, Log loss function, ROC curves and used AUC for feature selection and elastic technologies like ElasticSearch, Kibana.\n\u2022 Performed Data Profiling to learn about behavior with various features such as traffic pattern, location, Date and Time etc.\n\u2022 Categorized comments into positive and negative clusters from different social networking sites using Sentiment Analysis and Text Analytics\n\u2022 Performed Multinomial Logistic Regression, Decision Tree, Random forest, SVM to classify package is going to deliver on time for the new route.\n\u2022 Performed data analysis by using Hive to retrieve the data from Hadoop cluster, Sql to retrieve datafrom Oracle database and used ETL for data transformation.\n\u2022 Performed Data Cleaning, features scaling, features engineering using pandas and numpy packages in python.\n\u2022 Exploring DAG's, their dependencies and logs using AirFlow pipelines for automation\n\u2022 Performed data cleaning and feature selection using MLlib package in PySpark and working with deep learning frameworks such as Caffe, Neon.\n\u2022 Developed Spark/Scala,R Python for regular expression (regex) project in the Hadoop/Hive environment with Linux/Windows for big data resources.\n\u2022 Used clustering technique K-Means to identify outliers and to classify unlabeled data.\n\u2022 Tracking operations using sensors until certain criteria is met using AirFlowtechnology.\n\u2022 Responsible for different Data mapping activities from Source systems to Teradata using utilities like TPump, FEXP,BTEQ, MLOAD, FLOADetc\n\u2022 Analyze traffic patterns by calculating autocorrelation with different time lags.\n\u2022 Ensured that the model has low False Positive Rate and Text classification and sentiment analysis for unstructured and semi-structured data.\n\u2022 Addressed over fitting by implementing of the algorithm regularization methods like L1 and L2.\n\u2022 Used Principal Component Analysis in feature engineering to analyze high dimensional data.\n\u2022 Used MLlib, Spark's Machine learning library to build and evaluate different models.\n\u2022 Implemented rule based expertise system from the results of exploratory analysis and information gathered from the people from different departments.\n\u2022 Created and designed reports that will use gathered metrics to infer and draw logical conclusions of past and future behavior.\n\u2022 Developed MapReduce pipeline for feature extraction using Hive and Pig.\n\u2022 Created Data Quality Scripts using SQL and Hive to validate successful data load and quality of the data. Created various types of data visualizations using Python and Tableau.\n\u2022 Communicated the results with operations team for taking best decisions.\n\u2022 Collected data needs and requirements by Interacting with the other departments.\n\nEnvironment:Python 2.x, CDH5, HDFS, Hadoop 2.3, Hive, Impala, AWS, Linux, Spark, Tableau Desktop, SQL Server 2014, Microsoft Excel, Matlab, Spark SQL, Pyspark."", u'Data Analyst\nWalgreens - Deerfield, IL\nDecember 2013 to September 2015\nDescription:The Walgreens Companyis an American company that operatesas the second-largest pharmacy store chain in the United States. It specializes in filling prescriptions, health and wellness products, health information, and photo services\n\nResponsibilities:\n\u2022 Worked with BI team in gathering the report requirements and also Sqoop to export data into HDFS and Hive\n\u2022 Involved in the below phases of Analytics using R, Python and Jupyter notebook.\n\u2022 a. Data collection and treatment: Analysed existing internal data and external data, worked on entry errors,classification errors and defined criteria for missing values\nb. Data Mining: Used cluster analysis for identifying customer segments, Decision trees used for profitable and non-profitable customers, Market Basket Analysis used for customer purchasing behaviour and part/product association.\n\u2022 Developed multiple Map Reduce jobs in Java for data cleaning and preprocessing.\n\u2022 Assisted with data capacity planning and node forecasting.\n\u2022 Installed, Configured and managed Flume Infrastructure\n\u2022 Administrator for Pig, Hive and HBase installing updates patches and upgrades.\n\u2022 Worked closely with the claims processing team to obtain patterns in filing of fraudulent claims.\n\u2022 Worked on performing major upgrade of cluster from CDH3u6 to CDH4.4.0\n\u2022 Developed Map Reduce programs to extract and transform the data sets and results were exported back to RDBMS using Sqoop.\n\u2022 Patterns were observed in fraudulent claims using text mining in R and Hive.\n\u2022 Exported the data required information to RDBMS using Sqoop to make the data available for the claims processing team to assist in processing a claim based on the data.\n\u2022 Developed Map Reduce programs to parse the raw data, populate staging tables and store the refined data in partitioned tables in the EDW.\n\u2022 Created tables in Hive and loaded the structured (resulted from Map Reduce jobs) data\n\u2022 Using HiveQL developed many queries and extracted the required information.\n\u2022 Created Hive queries that helped market analysts spot emerging trends by comparing fresh data with EDW reference tables and historical metrics.\n\u2022 Was responsible for importing the data (mostly log files) from various sources into HDFS using Flume\n\u2022 Enabled speedy reviews and first mover advantages by using Oozie to automate data loading into the Hadoop Distributed File System and PIG to pre-process the data.\n\u2022 Provided design recommendations and thought leadership to sponsors/stakeholders that improved review processes and resolved technical problems.\n\u2022 Managed and reviewed Hadoop log files.\n\u2022 Tested raw data and executed performance scripts.\n\nEnvironment:HDFS, PIG, HIVE, Map Reduce, Linux, HBase, Flume, Sqoop, R, VMware, Eclipse, Cloudera, Python.', u""Data Scientist\nJohnson and Johnson - Raritan, NJ\nSeptember 2012 to November 2013\nDescription:Transamerica is the US-based brand of Aegon, a Dutch financial services firm. Aegon is one of the world's leading providers of life insurance, pensions and asset management and is helping approximately 30 million customers globally to achieve a lifetime of financial security.\n\nResponsibilities:\n\u2022 Manipulating, cleansing & processing data using Excel, Access and SQL.\n\u2022 Responsible for loading, extracting and validation of client data.\n\u2022 Modelled clean data into the Kafka servers for use over the spark engine.\n\u2022 Zookeeper along with Kafka was used to stream data and end-to-end client communication.\n\u2022 Performed transformations over the warehoused data using Scala& Python and modelled the data back into the servers for iterative transformations into KAFKA.\n\u2022 Modelled data using Machine learning libraries(Sci-kit learn) apart from SVN and KNN based classificationto create a training dataset for use in a predictive model.\n\u2022 Liaising with end-users and 3rd party suppliers.\n\u2022 Analyzing raw data, drawing conclusions & developing recommendations\n\u2022 Writing T-SQL scripts to manipulate data for data loads and extracts.\n\u2022 Developing data analytical databases from complex financial source data.\n\u2022 Performing daily system checks.\n\u2022 Data entry, data auditing, creating data reports & monitoring all data for accuracy.\n\u2022 Designing, developing and implementing new functionality.\n\u2022 Monitoring the automated loading processes.\n\u2022 Advising on the suitability of methodologies and suggesting improvements.\n\u2022 Carrying out specified data processing and statistical techniques.\n\u2022 Supplying qualitative and quantitative data to colleagues & clients.\n\u2022 Using Informatica& SAS to extract, transform & load source data from transaction\nsystems.\n\u2022 Performed sequential analytics using SAS Enterprise miner using jobs fed by the SAS Grid Manager.\n\u2022 Loaded packages and stored procedures using Base SAS and integrated functional and business requirements using the EBI suite.\n\u2022 Creating data pipelines using big data technologies like Hadoop, spark etc.\n\u2022 Creating statistical models using distributed and standalone models to build various diagnostics, predictive and prescriptive solution.\n\u2022 Utilize a broad variety of statistical packages like SAS, R , MLIB, Graphs,Hadoop, Spark , MapReduce, Pig\n\u2022 Performed a check using quality parameters fed using the SAS QC engine.\n\u2022 Created a UI dashboard for end users and performed prototype testing using Tableau.\n\u2022 Refine and train models based on domain knowledge and customer business objectives\n\u2022 Deliver or collaborate on delivering effective visualizations to support the client business objectives\n\u2022 Communicate to your peers and managers promptly as and when required.\n\u2022 Produce solid and effective strategies based on accurate and meaningful data reports and analysis and/or keen observations.\n\u2022 Establish and maintain communication with clients and/or team members; understand needs, resolve issues, and meet expectations\n\u2022 Developed web applications using .net technologies; work on bug fixes/issues that arise in the production environment and resolve them at the earliest\n\nEnvironment:Cloudera, HDFS, Pig, Hive, Map Reduce, python, Sqoop, Storm, Kafka, LINUX, Hbase, Impala, Java, SQL, Cassandra, MongoDB, SVN."", u'Data Architect/Data Modeler\nInfotech Enterprises IT Services Pvt. Ltd - Hyderabad, Telangana\nDecember 2010 to August 2012\nDescription: Infotech Enterprises IT Services Pvt. Ltd. (Infotech IT), part of the $ 260 million Infotech Enterprises group, leverages its business process knowledge, technological competence, strategic alliances and strong global presence to offer innovative IT solutions to the Retail Industry.\n\nResponsibilities:\n\u2022 Worked with large amounts of structured and unstructured data.\n\u2022 Knowledge in Machine Learning concepts (Generalized Linear models, Regularization, Random Forest, Time Series models, etc.)\n\u2022 Worked in Business Intelligence tools and visualization tools such as Business Objects, Tableau, ChartIO, etc.\n\u2022 Deployed GUI pages by using JSP, JSTL, HTML, DHTML, XHTML, CSS, JavaScript, AJAX.\n\u2022 Configured the project on WebSphere 6.1 application servers\n\u2022 Implemented the online application by using Core Java, Jdbc, JSP, Servlets and EJB 1.1, Web Services, SOAP, WSDL.\n\u2022 Handled end-to-end project from data discovery to model deployment.\n\u2022 Monitoring the automated loading processes.\n\u2022 Communicated with other Health Care info by using Web Services with the help of SOAP, WSDLJAX-RPC\n\u2022 Used Singleton, factory design pattern, DAO Design Patterns based on the application requirements\n\u2022 Used SAX and DOM parsers to parse the raw XML documents\n\u2022 Used RAD as Development IDE for web applications.\n\u2022 Preparing and executing Unit test cases\n\u2022 Used Log4J logging framework to write Log messages with various levels.\n\u2022 Involved in fixing bugs and minor enhancements for the front-end modules.\n\u2022 Implemented Microsoft Visio and Rational Rose for designing the Use Case Diagrams, Class model, Sequence diagrams, and Activity diagrams for SDLC process of the application\n\u2022 Doing functional and technical reviews\n\u2022 Maintenance in the testing team for System testing/Integration/UAT.\n\u2022 Guaranteeing quality in the deliverables.\n\u2022 Conducted Design reviews and Technical reviews with other project stakeholders.\n\u2022 Was a part of the complete life cycle of the project from the requirements to the production support\n\u2022 Created test plan documents for all back-end database modules\n\u2022 Implemented the project in Linux environment.\n\nEnvironment: R 3.0, Erwin 9.5, Tableau 8.0, MDM, QlikView, MLLib, PL/SQL, HDFS, Teradata 14.1, JSON, HADOOP (HDFS), MapReduce, PIG, Spark, R Studio, MAHOUT, JAVA, HIVE, AWS.', u""Data Analyst/ Data Modeler\nGlobalLogic Technologies - Hyderabad, Telangana\nApril 2009 to November 2010\nDescription:GlobalLogic provides experience design, Digital Product Engineering Services, and Agile Software Development to the world's top brands by leveraging UX UI Design, next-gen technologies, and cloud software, with end-to-end solution by the best Software Development Company.\n\nResponsibilities:\n\u2022 Developed Internet traffic scoring platform for ad networks, advertisers and publishers (rule engine, site scoring, keyword scoring, lift measurement, linkage analysis).\n\u2022 Responsible for defining the key identifiers for each mapping/interface.\n\u2022 Clients include eBay, Click Forensics, Cars.com, Turn.com, Microsoft, and Looksmart.\n\u2022 Designed the architecture for one of the first analytics 3.0. online platforms: all-purpose scoring, with on-demand, SaaS, API services. Currently under implementation.\n\u2022 Web crawling and text mining techniques to score referral domains, generate keyword taxonomies, and assess commercial value of bid keywords.\n\u2022 Developed new hybrid statistical and data mining technique known as hidden decision trees and hidden forests.\n\u2022 Reverse engineering of keyword pricing algorithms in the context of pay-per-click arbitrage.\n\u2022 Implementation of Metadata Repository, Maintaining Data Quality, Data Cleanup procedures, Transformations, Data Standards, Data Governance program, Scripts, Stored Procedures, triggers and execution of test plans\n\u2022 Performed data quality in Talend Open Studio.\n\u2022 Coordinated meetings with vendors to define requirements and system interaction agreement documentation between client and vendor system.\n\u2022 Automated bidding for advertiser campaigns based either on keyword or category (run-of-site) bidding.\n\u2022 Creation of multimillion bid keyword lists using extensive web crawling. Identification of metrics to measure the quality of each list (yield or coverage, volume, and keyword average financial value).\n\u2022 Enterprise Metadata Library with any changes or updates.\n\u2022 Document data quality and traceability documents for each source interface.\n\u2022 Establish standards of procedures.\n\u2022 Generate weekly and monthly asset inventory reports.\n\nEnvironment: Erwin r7.0, SQL Server 2000/2005, Windows XP/NT/2000, Oracle 8i/9i, MS-DTS, UML, UAT, SQL Loader, OOD, OLTP, PL/SQL, MS Visio, Informatica.""]",[],[],degree_1 : 
0,https://resumes.indeed.com/resume/980cb536f4a962c7,"[u'Data Scientist / Software Developer (Python, Java, Scala, Spark)\nFraunhofer Institute IAIS - Bonn, DE\nFebruary 2014 to Present\nImplementing the state of the art Machine Learning and Data Mining methods using Python,\nTensorFlow, pandas, sklearn etc.\n\u2022 Research in time series analysis (user behavior) and point processes (Poisson, Hawkes)\n\u2022 Improved the accuracy of detected anomalies to 95% in time series using Neural Networks\n\u2022 Supervising and managing Master and Bachelor students on different projects', u'Software Developer\nNINEKS Computers\nAugust 2009 to August 2013\nC#, VB.NET, MS SQL Server, MS Access, Android)\n\n\u2022 Developed and maintained desktop as well as mobile applications for accounting offices and pharmacy stores\n\u2022 Worked with database management systems and wrote SQL queries on daily basis\n\u2022 Took requirements and specifications from customers and planed the development and deployment process']","[u'Master of Computer Science in Machine Learning and Data Mining', u'Bachelor of Computer Science and Information Systems in Computer Science and Information Systems']","[u'University of Bonn Bonn, DE\nJanuary 2013 to January 2016', u'SW University ""Neofit Rilski""\nJanuary 2005 to January 2009']","degree_1 : Master of Compter Science in Machine Learning and Data Mining, degree_2 :  Bachelor of Compter Science and Information Systems in Compter Science and Information Systems"
0,https://resumes.indeed.com/resume/216e8b6ad3c36e1a,"[u'Data Scientist\nMu Sigma Business Solutions - Bengaluru, Karnataka\nAugust 2016 to April 2017\nDesigned and performed Marketing & Commercial decision analytics for a Fortune 100 Pharmaceutical Manufacturer\n\u2022 Sales Driver Analysis: (Teradata, R, SAS-Data manipulation, Excel-Reporting, Random Forest)\n+ Worked in a 3-member team analyzing the impact of the sales of newly launched drugs for all brands and products\n+ Developed a random forest model to evaluate the factors affecting the sale of each drug in each state, hospital\n+ Helped the promotional committee effectively manage the marketing strategies while improving RoI by 3%\n\u2022 Dashboard Development: (Teradata, SQL, Tableau)\n+ Tracked marketing activities such as sampling, detailing and calls to cater the needs of the sales force team\n+ Decreased reporting time by 200% and ensured consistency and accuracy in report delivery across business units\n+ This dashboard was refreshed every month with improvisation according to the timely requests by the client\nDecision Analysis for a leading e-commerce marketplace client\n\u2022 Downward Migration of Top Buyers: (Excel, R, Jupyter Notebook, Logistic Regression)\n+ Retained majority of users with high chances of attrition and reduced their losses for the leading retailer\n+ Delivered actionable insights and potential reasons for the downward migration to the client on Jupyter Notebook']",[u'MS in Business in Analytics'],"[u'University of Illinois at Chicago Chicago, IL\nAugust 2012 to May 2016']",degree_1 : MS in Bsiness in Analytics
0,https://resumes.indeed.com/resume/8df06d7cd269b9d7,"[u'Data Analyst/UI Developer\nTalus Analytics - Boulder, CO\nJanuary 2018 to Present\nConduct project-specific research to add information to datasets, synthesize data, and determine the story we want\nto tell with the data.\n\u2022 Collaborate with teammates to develop designs for visualizations and build user interfaces using HTML and D3.js.', u'Data Scientist\nIBM - Austin, TX\nJune 2017 to August 2017\n\u2022 Evaluated multiple recommender systems and machine learning algorithms to make recommendations regarding how new customers should complete tax forms using completed tax forms as training data.\n\u2022 Implemented collaborative filtering algorithms with pandas and Spark and developed a terminal command to make\nrunning recommender algorithms fast and easy.', u""Project Manager\nmDoc - Johannesburg, Gauteng\nJanuary 2017 to January 2017\nResearched and compiled reports on how metastatic breast cancer is currently treated and how mDoc's platform\ncan cater to the metastatic breast cancer community, presented findings to mDoc team.\n\u2022 Created wire frames for an app to manage treatment for metastatic breast cancer."", u'Programmer\nAfrican Institute for Mathematical Sciences - Cape Town, Western Cape\nJune 2016 to July 2016\n\u2022 Organized and analyzed a sample of 2 million tweets from Africa from 2015 to reveal the mood in Africa.\n\u2022 Investigated correlations between countries based on average sentiment and top hashtags in each country and displayed results with maps and covariance matrices.']",[u'in Mathematics'],"[u'Massachusetts Institute of Technology Cambridge, MA\nSeptember 2014 to December 2017']",degree_1 : in Mathematics
0,https://resumes.indeed.com/resume/48cef2f3052cb93f,"[u""Data Scientist\nArgus Information & Advisory Services, LLC - White Plains, NY\nSeptember 2016 to Present\nArgus Information and Advisory Services is a Subsidiary of Verisk Analytics Company and the leading provider of analytics, information and solutions to consumer banks and their regulators. The company's clients range from financial institutions to retailers and tech companies. The project focused on detecting anti-money laundering violation using Big Data and Data Science tools and improving customer's transaction monitoring system.\nResponsibilities:\n\u2022 Collected and analyzed the business requirements, understood the particular Fraud/AML challenges that our client faces.\n\u2022 Participated in Data integration job with Data Engineer team to gather traditional transaction data and external source data together.\n\u2022 Transformed data from SQL Server database to Hadoop Clusters which is set up by using AWS EMR.\n\u2022 Conducted data cleansing and feature engineering job through python NumPy and Pandas.\n\u2022 Implemented Naive Bayes, Logistic Regression, SVM, Random Forest and Gradient boosting with weighted loss function by using Python Scikit-learn.\n\u2022 Implemented mulit-layers Neural Networks by using Google Tensorflow and Spark.\n\u2022 Performed extensive Behavioral modeling and Customer Segmentation to discover behavior patterns of customers by using K-means Clustering.\n\u2022 Managed and scheduled models by using Oozie for batch processing.\n\u2022 Updated and saved Fraud predictions to AWS S3 for application team.\n\u2022 Tested the business performance of the AML models by evaluating detection rate and false positive rate and worked on continuous improvement on model.\n\u2022 Created reports and dashboards, by using Tableau, to explain and communicated data insights, significant features, model's score and performance of new transaction monitoring system to both technical and business teams.\n\u2022 Used GitHub for version control with Data Engineer team and Data Scientists colleagues.\nEnvironment: SQL Server 2014, Hadoop 2.0, Hive 2.0, Spark (PySpark, SparkSQL), Python 3.X, Tensorflow, Oozie 4.2, Tableau 10.X, AWS S3/EC2/EMR, Github"", u'Data Scientist\nCenterLight Health System - Bronx, NY\nApril 2015 to July 2016\nCenterLight Health System, a not-for-profit organization, has evolved into a leader in serving the elderly, chronically ill and disabled. CenterLight is one of the largest long-term care providers in New York State, serving all of New York City, Westchester, Nassau, Rockland and Suffolk Counties. This project aimed to predict the billing cycles and accounting related issues to increase the efficiency of enterprise claim processing.\n\nResponsibilities:\n\u2022 Conducted reverse engineering based on demo reports to understand the data without documentation.\n\u2022 Generated new data mapping documentations and redefined the proper requirements in detail.\n\u2022 Generated different Data Marts for gathering the tables needed (Member info, Claim info, Transaction info, Appointment info, Diagnose info) from SQL Server Database.\n\u2022 Created ETL packages to transform data into the right format and join tables together to get all features required using SSIS.\n\u2022 Processed data using Python pandas to examine transaction data, identify outliers and inconsistencies.\n\u2022 Conducted exploratory data analysis using python NumPy and Seaborn to see the insights of data and validate each feature through different charts and graphs.\n\u2022 Built predictive models including Linear regression, Lasso Regression, Random Forest Regression and Support Vector Regression to predict the claim closing gap by using python scikit-learn.\n\u2022 Used GridSearchCV to evaluate each model and to find best parameters set for each model.\n\u2022 Created reports and an app demo using Tableau to show client how prediction can help the business.\n\u2022 Deployed and hosted our models by using Azure Machine Learning Studio and share an API with application development team.\n\u2022 Used Confluence to share and collaborate on projects with team members, and keep track of up to date documentations.\n\nEnvironment: SQL Server 2012, SQL Server Data Tools 2010, SQL Server Integration Services, Python 2.7/3.3, Tableau 9.4, Azure Machine Learning Studio', u""Junior Data Scientist\nAtlantic Health - Morristown, NJ\nJanuary 2014 to March 2015\nAtlantic Health System is one of the leading non-profit health care systems in New Jersey, providing a wide array of health care services to the residents of Northern and Central regions of the state as well as Pike County, PA, and southern Orange County, NY. Project was to build a predictive model to predict the readmission case. The main objective was to reduce the risk of being wrongly diagnosed and the risk of being involved in the legal disputes.\n\nResponsibilities:\n\u2022 Communicated and coordinated with other departments to gather business requirements.\n\u2022 Gathered data information from multiple sources, and performed resampling method to handle the issue of imbalanced data.\n\u2022 Worked with ETL Team and Doctors to understand the data and define the uniform standard format.\n\u2022 Conducted data cleansing by using advanced SQL queries in SQL Server Database.\n\u2022 Split the data into different smaller dataset based on different diagnoses, in charge of conducting exploratory data analysis for three of diagnoses datasets (Diabetes, cold/flu, allergy).\n\u2022 Created the whole pipeline of data preprocessing (imputing, scaling, label encoding) through python pandas to get data ready to modeling part.\n\u2022 Built predictive models, using python scikit-learn, including Support Vector Machine, Decision tree, Naive Bayes Classifier, Neural Network to predict a potential readmitted case.\n\u2022 Performed Ensemble methods, including Gradient Boosting, Random Forest, customized ensemble method to produce more accurate solutions.\n\u2022 Designed and implemented cross-validation and statistical tests including Hypothesis testing, AVOVA, Chi-square test to verify models' significance.\n\u2022 Created a API by using Flask and shared the idea with application team and help them define the requirements of new application.\n\u2022 Used Agile methodology and Scrum process for project developing.\n\nEnvironment: SQL server 2012, SQL Server Integration Services, Python 2.7, Jupyter notebook, Flask 0.10, SharePoint 2013"", u'BI Developer\nFulton Financial Corporation - Lancaster, PA\nDecember 2012 to October 2013\nFulton is a financial company based in Lancaster, Pennsylvania. They provide a wide range of financial products and personalized services in Pennsylvania, Maryland, Delaware, Virginia and New Jersey. They are comprised of several different banking subsidiaries. The main job of this project was to provide ETL solutions for data migration and provide data quality and micro strategy solutions.\n\nResponsibilities:\n\u2022 Involved in gathering user/project requirements from business users and IT managers, translated it into functional and non-functional specifications needed and created documentations for the project.\n\u2022 Assisted in design and data modeling efforts of Data Marts and Enterprise Data Warehouse.\n\u2022 Used T-SQL in SQL Server to develop complex stored procedures, triggers, clustered index & non-clustered index, Views, and User-defined Functions (UDFs).\n\u2022 Designed SSIS packages to extract, transform and load existing data into SQL Server, used lots of components of SSIS, such as Pivot Transformation, Fuzzy Lookup, Derived Columns, Condition Split, Aggregate, Execute SQL Task, Data Flow Task and Execute Package Task.\n\u2022 Created SSIS Packages that involved dealing with different source formats (Text files, XML, Database Tables)\n\u2022 Debugged and troubleshot the ETL packages by using breakpoint, analyzing process, catching error information by SQL command in SSIS.\n\u2022 Create reports with the use of SSRS to generate different types of reports such as tabular, matrix, drill down and charts reports with accordance with user requirement.\n\u2022 Maintained and updated existing reports, analyzed the SQL queries and logic behind them to improve the performance.\n\u2022 Helped deploy the report with scheduling, subscription, history snapshot configured and set up.\n\u2022 Developed in Agile environment throughout the project.\n\nEnvironment: SQL server 2008/2012, SQL Server Management Studio (SSMS), MS BI Suite (SSIS, SSRS)']",[u'Master of Science in Electrical Engineering'],[u'Stevens Institute of Technology'],degree_1 : Master of Science in Electrical Engineering
0,https://resumes.indeed.com/resume/c101f73379310e4f,"[u'Data Scientist\nNorthrop Grumman - McLean, VA\nJune 2015 to Present', u'Northrop Grumman\nJune 2015 to May 2016\nDeveloped and maintained multiple Java applications that met customer business requirements that was tested\nand delivered on time. Business solutions included manipulating domains and hostnames from MySQL\ndatabase to be scored by an algorithm and assigned a reputation score.\n\u2022 Automated and updated scripts in Bash used to produce Daily Situational Awareness Threat Report and Network Analysis Reports for US-CERT and DHS customers.\n\u2022 Maintained and monitored large MySQL database tables with over 1 billion daily records ingested from various\ncyber threats feed vendors like Akamai, Shadowservers and Team Cymru. Developed scripts in Bash and Python to import and manipulate data from different vendors using vendor APIs, .csv files and web links.\n\u2022 Experience with day-to-day Linux operating tasks like monitoring logs, importing new feeds and creating Cron\njobs.\n\u2022 Developed visual analytic graphs in Tableau for various customer-facing reports capturing daily metrics on feeds and analysis on threat data.', u""Customer Project Consultant\nVerint Systems, Hendon\nJuly 2010 to January 2013\nVA\n\u2022 Developed Voice of the Customer (VOC) programs, like the Annual Customer Relations Survey, Pre-Training\nSurvey and integrated results into CRM systems like SalesForce.\n\u2022 Gathered customer feedback, conducted customer analytics and research on developing customer personas and identifying solutions for customer retention.\n\u2022 Produced data-driven cross-functional dashboards for organization's stakeholders using Business Intelligence,\nVovici software, MS Excel and MS PowerPoint.\n\u2022 Set up and managed organization's Learning Management System (LMS) in turn reducing LMS operational\ncost by +70%.""]","[u'Masters in Data Analytics Engineering', u'Bachelors in Applied Information Technology']","[u'George Mason University Fairfax, VA\nMay 2018', u'George Mason University Fairfax, VA\nMay 2015']","degree_1 : Masters in Data Analytics Engineering, degree_2 :  Bachelors in Applied Information Technology"
0,https://resumes.indeed.com/resume/a1c0ceec8042e4f4,"[u""Data Scientist\nGreen Charge Networks - Santa Clara, CA\nAugust 2015 to August 2015\nStatistical Analysis:\n- Use statistics and machine learning techniques to develop algorithms to forecast site power load profiles. These forecast were used with Model Predictive Control algorithms to manage a the site's energy consumption\n- Develop methodologies to determine forecast uncertainty for the purpose of stochastic optimization.\n- Use existing, and develop new, machine learning techniques in addition to standard statistical methods to forecast building power loads and energy consumption with uncertainty for robust optimization."", u'Visiting Professor\nGonzaga University - Spokane, WA\nAugust 2014 to August 2015\nUndergraduate Physics\nTeach undergraduate physics and mentor students', u'Adjunct Professor\nMesa College - San Diego, CA\nJanuary 2013 to August 2014\nUndergraduate Physics\nTeach undergraduate physics and mentor students', u'Postdoctoral Research Fellow\nUniversty of New South Wales - Sydney, NSW, Australia\nJanuary 2010 to January 2013\nNew South Wales, Australia\nCosmology and Galactic Dynamics\nUse non-linear least squares fitting, Bayesian statistics, and emperical analysis to develop\nmodels consistant with observed phenomena', u'Test Engineer\nTeledyne - Camarillo, CA\nAugust 2006 to December 2009\nInfrared Focal Plane Test\nDevelop algorithms to accurate characterize infrared focal plane arrays to be used on the James Webb Space Telescope', u'Data Scientist\nNorthrop Grumman - Van Nuys, CA\nJune 2004 to August 2006\nSignal Processing, Tracking, and Radar Imaging\nDevelop algorithms using traditional signal processing methodologies, Kalman filters, and Fourier analysis for image processing and target tracking', u'Engineer\nBAE Systems - Nashua, NH\nJanuary 2001 to June 2004\nInfrared Focal Plane Design\nDevelop and use semi-empirical models to engineer focal plane arrays sensitive to customer specified wavelengths.']","[u'Ph.D. in Physics in Physics', u'Masters of Science in Physics in Physics', u'Bachelors of Science in Physics in Physics']","[u'University of California Los Angeles, CA\nMarch 2009', u'University of California Los Angeles, CA\nDecember 2001', u'Oregon State University Corvallis, OR\nJune 1999']","degree_1 : Ph.D. in Physics in Physics, degree_2 :  Masters of Science in Physics in Physics, degree_3 :  Bachelors of Science in Physics in Physics"
0,https://resumes.indeed.com/resume/c08ba244afd09c5b,"[u'Data Scientist\nJohnson Controls, Inc - Syosset, NY\nDecember 2015 to Present\nIn the project ""Smart Grid Analytics and Energy Price Forecasting"", I have been contributing towards the development of smart energy analytics solution portfolio for its utilities and industrial clients to integrate and manage Smart Grid and Renewable Energy infrastructure. Such systems help the clients to reduce the carbon footprint, obtain real-time visibility of demand and supply, and optimize the energy generation, distribution in a cost effective fashion. The overall aim of the project is to collect data in (near) real time and integrate with the existing ERP backend system to facilitate generation planning and control, demand flexibility, dynamic pricing, demand forecasting and, maximize the utilization of renewable sources by predicting overall consumption and supply patterns.\n\nJob Description:\n\u2022 Develop BI and analytics reports on demand and supply optimization trends and KPIs.\n\u2022 Extracted test data from proto-type equipment, weather data from the National Oceanic and Atmospheric Administration and solar radiation data from the National Aeronautics and Space Administration (NASA).\n\u2022 Developed SQL queries for extracting utilities usage data to lower environment for data analysis.\n\u2022 Explored Hive QL for summarizing, querying and analyzing data stored in HDFS.\n\u2022 Explanatory data analysis and data health check.\n\u2022 Performed outlier detection, imputation for missing data, cleaning, transforming, and normalization.\n\u2022 Coordinated with domain consultants and technical team for product development and customization.\n\u2022 Coordinated with client SME\'s to define the blueprint for data and service integration.\n\u2022 Performed literature review and research to identify historical patterns and methods applied.\n\u2022 Clustering to obtain similar type of data based on location and availability of alternate energy source.\n\u2022 Developed various non-linear regression models to study the relationship between variables.\n\u2022 Applied ARIMA method to forecast renewable energy usage.\n\u2022 Built a predictive model to quantify structural reduction in net load as a function of the presence of solar PV, house and household characteristics and weather for a particular zone.\n\u2022 Develop modules for price performance, asset performance and alerts.\n\u2022 Aggregate relevant information and developed tableau dashboard to present the results.\n\nEnvironment: R 3.2.3, Microsoft Excel 2013, Tableau 9.2.2, MySQL 5.5.48, Hadoop, Hive', u'Geospatial Analyst (GIS)\nAir Products and Chemicals, Inc - Allentown, PA\nAugust 2015 to November 2015\nComputational Modeling Center (CMC) group, hired me to develop geospatial models and perform object based image analysis for making informed business decisions. Involved in spatial data collection and munging, creating geodatabases, and performing necessary analytics. Generated progress reports and manuals to communicate with team members and provide guidelines for efficient spatial analysis. The location specific results and reports were used by the project manager and department head for making business decisions.\n\nJob Description:\n\u2022 Spatial data preparation including collecting shapefiles, grid generation, and file creation.\n\u2022 Summary statistics using Analysis tools.\n\u2022 Location data validation using basemap in ArcGIS.\n\u2022 Developed a conceptual framework for location based spatial analysis.\n\u2022 Identified key variables (shapefiles) for the analysis.\n\u2022 Outlier detection and cleaning using proximity analysis tools of ArcGIS.\n\u2022 Merged, modified data and extracted attributes using data management tools.\n\u2022 Analyzed the area and identified locations for detailed investigation.\n\u2022 Developed python scripts for automated file modification and management.\n\u2022 Object based image analysis to identify desired objects using Trimble eCognition software.\n\u2022 Validation of results using KML and KMZ files in ArcGIS and Google Earth.\n\u2022 Geocoding of results for detailed location information using ArcGIS.\n\u2022 Exported the results in different file formats for further validation and presentation.\n\u2022 Streamlined the project by building geoprocessing workflows using model builder in ArcGIS.\n\u2022 Increased efficiency by automating the project using python scripting in arcpy.\n\u2022 Performed geospatial analysis and image processing to identify desired objects in each given locations.\n\nEnvironment: ArcGIS Desktop 10.3, Python for ArcGIS, eCognition 9.1, MS Excel 2013, Google Earth 7.1', u'Research Scientist (Data Analytics)\nAdvanced Material Analytics, LLC - Binghamton, NY\nJune 2014 to August 2015\nThe goal of the project was to develop commercial data acquisition system for material characterization. In this I have been leveraging my expertise in geophysics and data analysis to integrate complex statistical tools with hardware systems. These equipment are used for testing ""materials/products"" for characteristic properties like porosity, permeability & pore-size measurements.\n\nJob Description:\n\u2022 Supervised the research and offshore software team through equipment and software development cycle.\n\u2022 Developed MATLAB programs to identify the proto-type equipment parameters and component selection.\n\u2022 Efficient design of sample chamber and equipment body using PTC Creo.\n\u2022 Obtained multiple dataset from the proto-type equipment for data analysis.\n\u2022 Performed data health check and summary statistics.\n\u2022 Applied regression and correlation methods to illustrate the accuracy, repeatability & reproducibility of the equipment.\n\u2022 Explored automated report generation using python scripting.\n\u2022 Developed SQL queries for client information management and update.\n\u2022 Explored and created tableau dashboard to present the results.\n\u2022 Offered recommendation on the feasibility of the project, based on simulation (of the pycnometer method) and deviations from competitor\'s data.\n\nEnvironment: MS Excel 2010, R 3.1.2, MySQL 5.5.32, Tableau 8.3.4, MATLAB 7.12, PTC Creo 3.0', u'Research Analyst\nDept. of Geology & Geophysics, IIT-Kharagpur - Kharagpur, West Bengal\nAugust 2006 to May 2008\nThis project was to study the gravity anomaly of the Orissa region of India and predict the tectonic activity based on past gravity data. Performed data cleaning and explanatory data analysis. Applied correction to the observed gravity data. Then applied forward modeling of corrected gravity data to model the structure of the basin using MATLAB.\n\nEnvironment: C, MATLAB 6.5, MS Excel 2003, Generic Mapping Tool', u'Data Analytics Trainee\nDept. of Geology & Geophysics, IIT-Kharagpur\nJune 2007 to July 2007\nThis project was mainly focus on gravity data analysis to study sea-floor structure and deformation of Western Continental Margin, India. Purpose of the project was to develop the crustal model using gravity data and develop a model based on the tectonic activity of the region. Applied descriptive statistics using Excel to create graphs, tables, and charts for initial presentation. Pre-processed the data by cleaning, scaling, and applying various gravimetric corrections. Developed forward model to obtain the structure of the region. Generic Mapping Tool (GMT) was used for geographic projection and visualization.\n\nEnvironment: Fortran 77, Generic Mapping Tool, Adobe Illustrator, MATLAB 6.5', u'Data Scientist\nOil and Natural Gas Corporation Ltd - Chennai, Tamil Nadu\nMay 2007 to June 2007\nIndustrial training\nA four week Industrial training at Oil and Natural Gas Corporation Ltd.(ONGC), India, during summer 2007. It included extensive field work (in Bengal & KG-PG Basin) and data processing of the obtained data. This included data pre-processing, seismic forward modelling to understand the influence of geological complexity, and developed vertical velocity model.\n\nEnvironment: Petrel, Seismograph (UL 408), MS Excel']",[u'in Research Assistant'],"[u'Binghamton University Binghamton, NY\nJune 2008 to May 2014']",degree_1 : in Research Assistant
0,https://resumes.indeed.com/resume/78260c17a1fb668c,"[u'Data Scientist/Machine Learning Engineer\nHewlett Packard Enterprise - Madison, WI\nDecember 2016 to Present\nDevelop data driven solutions in Smart cloud services with emphasis on cost reduction, service quality under current trend of automation and data exchange in manufacturing technologies. Leverage Cyber physical systems, cloud Computing and IOT.', u""Data Scientist\nMckesson Specialty Health - The Woodlands, TX\nSeptember 2015 to November 2016\nDescription: As part of Mckesson Health Care Services, a collection advanced diagnosis systems development for different brain disorders and various tumors using fMRI and other scanning systems. Incorporated with realized continuous learning leveraging Machine learning and Artificial Intelligence.\n\nResponsibilities:\n\u2022 Follow data descriptions and gain required information on the medical conditions and classifications within the context.\n\u2022 Performed data analysis, visualization, feature extraction, feature selection, feature engineering using python pandas, Apache Spark etc.\n\u2022 Used Pyspark data frame to read heavy loads of text, csv and image data from S3 and Cassandra.\n\u2022 Applied Spark RDD's transformations and actions on raw data\n\u2022 Derive feature importance using techniques Random Forest (RF) classification and boosting.\n\u2022 Implemented Two Dimensional Fast Fourier Transformations on raw input data from fMRI.\n\u2022 Implemented Deep convolutional neural networks (CNNs) in modeling cortical representation and organization for spatial and visual processing with computer vision\n\u2022 Created Scikit-learn based learning models for POC's on sample dataset.\n\u2022 Applied statistical modeling like decision trees, regression models, and SVM.\n\u2022 Utilized Convolution Neural Networks to implement a machine learning image recognition component. Implemented Backpropagation in generating accurate predictions\n\u2022 Performed Information Extraction using NLP algorithms coupled with Deep Learning (ANN and CNN), Keras and TensorFlow.\n\u2022 Implemented Apache Spark to speedup Convolutional neural networks modeling.\n\u2022 Analyzed sentimental data and detected patterns in customer usage data sets.\n\u2022 Avoid overfitting by following standard practices such as keeping the number of independent parameters less than the data points avoidable in the model.\n\u2022 Loaded data from Hadoop and made it available for modelling in Keras.\n\u2022 Prepared multi-class classification data for modeling using one hot encoding.\n\u2022 Used Keras neural network models with Apache Spark.\n\u2022 Enhanced model performance by calibrating parameters, researching and improving optimization and weights initialization methods\n\u2022 Used Pyspark dataframe to read data from HDFS and S3.\n\nEnvironment: Python, Keras, TensorFlow, Scala, Apache Spark, Jupyter Notes, Anaconda, SciPy, Scikit-Learn, Numpy, pandas, AWS, HDFS, S3, Git, GitHub, REST, NVIDIA, CUDA, tmux, Linux."", u""Data Analyst/Data Scientist\nTema Business Systems, India - IN\nApril 2013 to August 2015\nDescription: Empowering grocery store operations and maintenance using advanced analytics and predictive modeling. Support email companies and store organization using machine learning modeling and statistical analysis in order to best understand and serve customers.\n\nResponsibilities:\n\u2022 Responsible for performing statistics and machine learning techniques classification/regression. Design and develop advanced R/Python modules to process and prepare datasets for modeling.\n\u2022 Analyzed large datasets to answer business questions by generating reports and outcome.\n\u2022 Worked in a team of programmers and data analysts to develop insightful deliverables that support data driven marketing strategies.\n\u2022 Following to best practices for project support and documentation.\n\u2022 Actively participated in meetings, presented findings and hypotheses\n\u2022 Understanding the business problem, build the hypothesis and validate the same using the data analysis and machine learning models.\n\u2022 Managing the Reporting/Dash boarding for the Key metrics of the business.\n\u2022 Collect and manage datasets using panda's data frames, queried RDBMS using python ORMs and session bindings along with data collected from Excel, Spreadsheets and API end points\n\u2022 Performed data visualizations with matplotlib's pyplot and seaborn. Applied transformations on data samples using numPy and sciPy.\n\u2022 Performed descriptive and infernal statistical analysis\n\u2022 Cleaning data with missing value treatments and removing outliers.\n\u2022 Implemented time series analysis by generating fixed frequency dates and time spans, convert time series frequencies.\n\u2022 Executed primary statistical analysis including linear regression, hypothesis testing, kmeans, SVMs random forest and many more.\n\u2022 Visualize and present machine learning hypotheses modeled on proprietary datasets.\n\u2022 Created visualizations and grouped various graph types using matlpltlib and seaboarn.\n\u2022 Determine relationship between random variables using covariance/correlation and plot it with seaborn's heatmaps.\nEnvironment: Python, Ipython Notebook, Scipy, Scikit-Learn, Numpy, pandas, MySQL, Git, REST,\nAWS, Flask, Linux.""]",[u'Master of Science in Information System Technologies'],[u'August 2015 to January 2017'],degree_1 : Master of Science in Information System Technologies
0,https://resumes.indeed.com/resume/fdc5d8ed5e651348,"[u'Data Scientist\nNYC Data Science - New York, NY\nJanuary 2018 to April 2018\nImmersive program focused on data science and machine learning applied skill development\n\u2022 Capstone: Scraped product review data from the web using threaded requests with multiple user agents and proxies to increase efficiency. Used pickle to concatenate disparate files facilitating analysis. (Include NLP)\n\u2022 Machine Learning: Optimized housing price prediction model on highly dimensional data with sophisticated imputation and feature engineering to support rapid iteration of multiple machine learning approaches\n\u2022 Interactive Visualization: Built web app using R Shiny allowing exploration of S&P 500 stock prices, sector returns, correlation plots, news-based word clouds, as well ARIMA based Time Series Analysis\n\u2022 Web scrapping project: Scraped data from BestBuy and deployed web app to enable analysis of Laptop based on price, user review and keyword analysis.', u'Trader (Series 57)\nT3 Trading Group LLC - New York, NY\nJune 2017 to November 2017\nTraded NYSE and NASDAQ equities using Light speed platform applying modern portfolio theory and related technical analyses', u'Trainee\nHong Kong, HK\nFebruary 2015 to March 2015\n\u2022 Trained on-site to gain a basic understanding of the finance industry\n\u2022 Led a group to simulate the fund-raising process of listed companies, made business plan, roadshow video, and financial statements for the simulated company and won first prize in the program', u""Asset Valuation Assistant\nJune 2014 to December 2014\n\u2022 Assisted real estate appraisers with preliminary stages of asset valuation, including collecting market information and creating grading sheets according to location, furnishing, and other price drivers\n\u2022 Developed models to assess various firms' real estate asset values using corporate annual reports and balance sheets from databases such as Wind and CCXI to calculate WACC of commercial real estate\n\nPERSONAL\nHabits: 1. listening to music, joining choir and playing piano. 2.Playing basketball and doing workout""]","[u'BS in Master of Financial Analysis', u'BS in Bachelor of Accounting']","[u'Rutgers, the state university of New Jersey New Jersey\nSeptember 2016 to June 2017', u'Shanghai Li Xin University of Accounting and Finance Shanghai\nSeptember 2012 to July 2016']","degree_1 : BS in Master of Financial Analysis, degree_2 :  BS in Bachelor of Acconting"
0,https://resumes.indeed.com/resume/bfcc38e19e48c9af,"[u""Scan Health Plan - Data Scientist Intern (Marketing)\nSeptember 2017 to March 2018\nExtracted meaningful information from the text, I queried data and aggregated data, researched text mining/\nmachine learning algorithms, designed, programmed, and tested suitable algorithms using R/Python. This\nsolution helped data scientist team to access the specific information for further analysis.\n\n\u2022 Understood the overall picture and problems faced by customers from the big data, I created interactive\nvisualization using tableau to understand the overall picture. This visualizations helped data scientist team to analyze important problems and give priority to solve them.\n\n\u2022 Understood the business needs from consumer insights and marketing team, I worked closely with them to understand problem, provided the solution, and delivered presentation in layman's term. This made easy for\neveryone to follow the project, give suggestion and make effective improvements."", u'Data Intern\nComputer Technology Research Laboratory\nSeptember 2016 to March 2017\nUnstructured data can give high variance and high bias which affect the accuracy of the models, I queried data from the database, wrangled data using R/python. This helped UCLA data team to save time and focus on analysis.', u'Instructor\nBerkeley Chess School\nNovember 2015 to September 2016\nUnderstanding chess problems and logic can be difficult to follow and understand, I mentored children to understand problems and logic. This improved them thinking different ways to solve problems.']",[u'B.Sc. in Statistics'],"[u'University of California Los Angeles Los Angeles, CA\nJune 2018']",degree_1 : B.Sc. in Statistics
0,https://resumes.indeed.com/resume/214d7791193c6135,"[u'\xd8 Data Scientist (Freelancer\nUpwork - Monterey, CA\nDecember 2016 to Present\n- Consulting services for companies in Upwork\n- Tutoring Statistics, R, SQL and Data Science Lessons\n- Machine Learning Projects on Kaggle\nv Marketing Campaigns of a Portuguese Banking Feb 2018 - Ongoing\n\xfc Finding the factor that influences the prediction of client subscription in depositing\n\xfc Automate the prediction of phone call results to sell long-term deposit\n\xfc Logistic Regression, SVM and Neural Networks used for this project', u""\xd8 Master's Student\nU.S. Naval Postgraduate School - Monterey, CA\nSeptember 2015 to December 2016\n- Extensive training and hands on projects related with Data Science, Machine Learning, Optimization and Simulation\n- Conducted research on predictive models\n- Existing predictive models analyzed and applied to GTD data"", u'Data Scientist\nU.S. Naval Postgraduate School\nSeptember 2015 to December 2016\n\xfc Advanced data analysis techniques applied to GTD by using R language\n\xfc Local min and self-exciting hurdle model compared to come up with a better prediction model', u'\xd8 Operations Research Analyst\nArmy - Ankara, TR\nJuly 2012 to September 2015\nAnalyzed and optimized logistics and inventory problem, such as allocation of resources to appropriate\noperation teams\n- By using data analysis, simulation and optimization techniques, helped decision makers to decide\n- Used Excel/VBA, XLMiner, OpenSolver for Advanced Analysis and Optimization\n- Responsible for Quality Analysis of new equipment and arms worth of $20,000 - $160,000 / each', u'Operation Research Analyst\nArmy\nSeptember 2012 to January 2015\nhistorical artillery fires data collected, cleaned, formatted and reshaped\n\xfc Applied machine learning algorithms to predict the best parameters for the live fires\n\xfc ML Model integrated with the fire control system\n\xfc With this model, accuracy of the fires increased %4.078', u'\xd8 Team Leader\nSpecial Forces - Ankara, TR\nJuly 2009 to July 2012\nLed Special Forces team']","[u'', u'Master of Science in Operations Research in education', u'Bachelor of Science in Systems in Systems / Industrial Engineering']","[u'\xd8 Stanford University\nDecember 2016 to October 2017', u'\xd8 Naval Postgraduate School Monterey, CA\nSeptember 2015 to December 2016', u'\xd8 Turkish Military Academy Ankara, TR\nSeptember 2004 to August 2008']","degree_1 : , degree_2 :  Master of Science in Operations Research in edcation, degree_3 :  Bachelor of Science in Systems in Systems / Indstrial Engineering"
0,https://resumes.indeed.com/resume/b4c95bd29d58f2a5,"[u'Sr. Data Scientist\nIBM - Wilmington, DE\nMarch 2017 to Present\nResponsibilities:\n\u2713 This project was focused on customer clustering based on ML and statistical modeling effort including building predictive models and generate data products to support customer classification and segmentation.\n\u2713 Develop a Estimation model for various product & services bundled offering to optimize and predict the gross margin\n\u2713 Built sales model for various product and services bundled offering\n\u2713 Developed predictive causal model using annual failure rate and standard cost basis for the new bundled services.\n\u2713 Design and develop analytics, machine learning models, and visualizations that drive performance and provide insights, from prototyping to production deployment and product recommendation and allocation planning.\n\u2713 Worked with sales and Marketing team for Partner and collaborate with a cross-functional team to frame and answer important data questions.\n\u2713 Proto typing and experimenting ML algorithms and integrating into production system for different business needs.\n\u2713 Application Machine Learning algorithms with Spark Mlib standalone and R/Python\n\u2713 Worked on Multiple datasets containing 2billion values which are structured and unstructured data about web applications usage and online customer surveys\n\u2713 Design, built and deployed a set of python modeling APIs for customer analytics, which integrate multiple machine learning techniques for various user behavior prediction\n\u2713 and support multiple marketing segmentation programs\n\u2713 Segmented the customers based on demographics using K-means Clustering\n\u2713 Used classification techniques including Random Forest and Logistic Regression to quantify the likelihood of each user referring\n\u2713 Designed and implemented end-to-end systems for Data Analytics and Automation, integrating custom visualization tools using R, Tableau\nEnvironment: MS SQL Server, R/R studio, Python, Spark frame work, Redshift, MS Excel, Tableau, T-SQL, ETL, RNN, LSTM MS Access, XML, MS office 2007, Outlook.', u'Sr. Data Scientist\nIBM - Wilmington, DE\nJuly 2015 to February 2017\nResponsibilities:\n\u2713 Responsible for analyzing large data sets to develop multiple custom models and algorithms to drive innovative business solutions.\n\u2713 Perform Data profiling, preliminary data analysis and handle anomalies such as missing, duplicates, outliers, and imputed irrelevant data.\n\u2713 Remove outliers using Proximity Distance and Density based techniques.\n\u2713 Involved in Analysis, Design and Implementation/translation of Business User requirements.\n\u2713 Experienced in using supervised, unsupervised and regression techniques in building models.\n\u2713 Performed Market Basket Analysis to identify the groups of assets moving together and recommended the client their risks\n\u2713 Experience in determine trends and significant data relationships using advanced Statistical Methods.\n\u2713 Implemented techniques like forward selection, backward elimination and step wise approach for selection of most significant independent variables.\n\u2713 Performed Feature selection and Feature extraction dimensionality reduction methods to figure out significant variables.\n\u2713 Used RMSE score, Confusion matrix, ROC, Cross validation and A/B testing to evaluate model performance in both simulated environment and real world.\n\u2713 Performed Exploratory Data Analysis using R. Also involved in generating various graphs and charts for analyzing the data using Python Libraries.\n\u2713 Involved in the execution of multiple business plans and projects Ensures business needs are being met Interpret data to identify trends to go across future data sets.\n\u2713 Developed interactive dashboards, created various Ad Hoc reports for users in Tableau by connecting various data sources.\nEnvironment: Python, SQL server, Hadoop, HDFS, HBase, MapReduce, Hive, Impala, Pig, Sqoop, Mahout, LSTM, RNN, Spark MLLib, MongoDB, Tableau, Unix/Linux.\n\n# Project 3\nConfidential, USA', u'Data Analyst\nSpark SQL\nMarch 2014 to June 2015\nResponsibilities:\n\u2713 Involved in Analysis, Design and Implementation/translation of Business User requirements.\n\u2713 Worked on collection of large sets using Python scripting. Spark SQL\n\u2713 Worked on large sets of Structured and Unstructured data.\n\u2713 Worked on creating DL algorithms using LSTM and RNN.\n\u2713 Actively involved in designing and developing data ingestion, aggregation, and integration in Hadoop environment.\n\u2713 Developed Sqoop scripts to import export data from relational sources and handled incremental loading on the customer, transaction data by date.\n\u2713 Experience in creating Hive Tables, Partitioning and Bucketing.\n\u2713 Performed data analysis and data profiling using complex SQL queries on various sources systems including Oracle 10g/11g and SQL Server 2012.\n\u2713 Identified inconsistencies in data collected from different source.\n\u2713 Worked with business owners/stakeholders to assess Risk impact, provided solution to business owners.\n\u2713 Experienced in determine trends and significant data relationships Analyzing using advanced Statistical Methods.\n\u2713 Carrying out specified data processing and statistical techniques such as sampling techniques, estimation, hypothesis testing, time series, correlation and regression analysis Using R.\n\u2713 Applied various data mining techniques: Linear Regression & Logistic Regression, classification, clustering.\n\u2713 Took personal responsibility for meeting deadlines and delivering high quality work.\n\u2713 Strived to continually improve existing methodologies, processes, and deliverable templates.\nEnvironment: R, SQL server, Oracle, HDFS, HBase, MapReduce, Hive, Impala, Pig, Sqoop, NoSQL, Tableau, RNN, LSTM, Unix/Linux, Core Java.\n\n# Project 4\nConfidential, USA', u'Data Analyst\nOLTP and Analytic\nMarch 2012 to February 2014\nConfidential\nData Analyst\nResponsibilities:\n\u2713 Provide expertise and recommendations for physical database design, architecture, testing, performance tuning and implementation.\n\u2713 Designed logical and physical data models for multiple OLTP and Analytic applications.\n\u2713 Extensively used the Erwin design tool &Erwin model manager to create and maintain the Data Mart.\n\u2713 Designed the physical model for implementing the model into oracle9i physical data base.\n\u2713 Involved with Data Analysis primarily Identifying Data Sets, Source Data, Source Meta Data, Data Definitions and Data Formats\n\u2713 Performance tuning of the database, which includes indexes, and optimizing SQL statements, monitoring the server.\n\u2713 Wrote simple and advanced SQL queries and scripts to create standard and adhoc reports for senior managers.\n\u2713 Collaborated the data mapping document from source to target and the data quality assessments for the source data.\n\u2713 Used Expert level understanding of different databases in combinations for Data extraction and loading, joining data extracted from different databases and loading to a specific database.\n\u2713 Co-ordinate with various business users, stakeholders and SME to get Functional expertise, design and business test scenarios review, UAT participation and validation of financial data.\n\u2713 Worked very close with Data Architects and DBA team to implement data model changes in database in all environments.\n\u2713 Created PL/SQL packages and Database Triggers and developed user procedures and prepared user manuals for the new programs.\n\u2713 Performed performance improvement of the existing Data warehouse applications to increase efficiency of the existing system.\n\u2713 Designed and developed Use Case, Activity Diagrams, Sequence Diagrams, OOD (Object oriented Design)using UML and Visio.\nEnvironment: Erwin r9.0, Informatica 9.0, ODS, OLTP, Oracle 10g, Hive, OLAP, DB2, Metadata, MS Excel, Mainframes MS Visio, Rational Rose, Requisite Pro, Hadoop, PL/SQL, etc.\n\n# Project 5\nConfidential, USA', u'SQL Developer\nFTP and SFTP\nMarch 2010 to February 2012\nResponsibilities:\n\u2713 Extensively experienced working on different dataflow and control flow task, for loop container, sequence container, script task, execute SQL task and Package configuration.\n\u2713 Created new procedures to handle complex logic for business and modified already existing stored procedures, functions, views and tables for new enhancements of the project and to resolve the existing defects.\n\u2713 Loading data from various sources like OLEDB, flat files to SQL Server 2012 database Using SSIS Packages and created data mappings to load the data from source to destination.\n\u2713 Created batch jobs and configuration files to create automated process using SSIS.\n\u2713 Created SSIS packages to pull data from SQL Server and exported to Excel Spreadsheets and vice versa.\n\u2713 Built SSIS packages, to fetch file from remote location like FTP and SFTP, decrypt it, transform it, mart it to data warehouse and provide proper error handling and alerting\n\u2713 Extensive use of Expressions, Variables, Row Count in SSIS packages\n\u2713 Data validation and cleansing of staged input records was performed before loading into Data Warehouse\n\u2713 Automated the process of extracting the various files like flat/excel files from various sources like FTP and SFTP (Secure FTP).\n\u2713 Deploying and scheduling reports using SSRS to generate daily, weekly, monthly and quarterly reports.\nEnvironment: MS SQL Server 2005 & 2008, SQL Server Business Intelligence Development Studio, SSIS-2008, SSRS-2008, Report Builder, Office, Excel, Flat Files, .NET, T-SQL']","[u'Masters in Engineering', u'Bachelors in Engineering']","[u'Anna University\nJanuary 2007', u'JNT University\nJanuary 2004']","degree_1 : Masters in Engineering, degree_2 :  Bachelors in Engineering"
0,https://resumes.indeed.com/resume/295b52057ec949b1,"[u'Data Scientist\nHarley Davidson - Milwaukee, WI\nJanuary 2017 to Present\nResponsibilities:\n\u2022 As an Architect design conceptual, logical and physical models using Erwin and build datamarts using hybrid Inmon and Kimball DW methodologies.\n\u2022 Worked closely with business, datagovernance, SMEs and vendors to define data requirements.\n\u2022 Worked with data investigation, discovery and mapping tools to scan every single data record from many sources.\n\u2022 Designed the prototype of the Data mart and documented possible outcome from it for end-user.\n\u2022 Involved in business process modeling using UML\n\u2022 Developed and maintained data dictionary to create metadata reports for technical and business purpose.\n\u2022 Created SQLtables with referential integrity and developed queries using SQL, SQL*PLUS and PL/SQL\n\u2022 Experience in maintaining database architecture and metadata that support the Enterprise Datawarehouse.\n\u2022 Design, coding, unit testing of ETL package source marts and subject marts using Informatica ETL processes for Oracledatabase.\n\u2022 Developed various QlikView Data Models by extracting and using the data from various sources files, DB2, Excel, Flat Files and Bigdata.\n\u2022 Handled importing data from various data sources, performed transformations using Hive, Map Reduce, and loaded data into HDFS.\n\u2022 Interaction with Business Analyst, SMEs and other Data Architects to understand Business needs and functionality for various project solutions\n\u2022 Researched, evaluated, architected, and deployed new tools, frameworks, and patterns to built sustainable Big Data platforms for the clients\n\u2022 Identifying and executing process improvements, hands-on in various technologies such as Oracle, Informatica, BusinessObjects.\n\u2022 Designed both 3NF data models for ODS, OLTP systems and dimensionaldatamodels using Star and SnowflakeSchemas.\nEnvironment: r9.0, Informatica 9.0, ODS, OLTP, Oracle 10g, Hive, OLAP, DB2, Metadata, MS Excel, Mainframes MS Visio, Rational Rose, Requisite Pro. Hadoop, PL/SQL, etc.', u'Data Scientist\nNBT Bank - Norwich, NY\nJanuary 2015 to December 2016\nResponsibilities:\n\u2022 Worked as a Data Modeler/Analyst to generate Data Models using Erwin and developed relational database system.\n\u2022 Analyzed the business requirements of the project by studying the Business Requirement Specification document.\n\u2022 Extensively worked on DataModeling tools ErwinDataModeler to design the datamodels.\n\u2022 Designedmapping to process the incremental changes that exists in the source table. Whenever source data elements were missing in source tables, these were modified/added in consistency with third normal form based OLTP source database.\n\u2022 Designed tables and implemented the naming conventions for Logical and PhysicalData Models in Erwin 7.0.\n\u2022 Provide expertise and recommendations for physicaldatabasedesign, architecture, testing, performance tuning and implementation.\n\u2022 Designedlogical and physical data models for multiple OLTP and Analytic applications.\n\u2022 Extensively used the Erwin design tool &Erwin model manager to create and maintain the DataMart.\n\u2022 Designed the physical model for implementing the model into oracle9i physical data base.\n\u2022 Involved with DataAnalysis primarily Identifying DataSets, SourceData, Source Meta Data, Data Definitions and Data Formats\n\u2022 Performance tuning of the database, which includes indexes, and optimizing SQL statements, monitoring the server.\n\u2022 Wrote simple and advanced SQLqueries and scripts to create standard and adhoc reports for senior managers.\n\u2022 Collaborated the data mapping document from source to target and the data quality assessments for the source data.\n\u2022 Used Expert level understanding of different databases in combinations for Data extraction and loading, joiningdata extracted from different databases and loading to a specific database.\n\u2022 Co-ordinate with various business users, stakeholders and SME to get Functional expertise, design and business test scenarios review, UAT participation and validation of financial data.\n\u2022 Worked very close with Data Architects and DBA team to implement data model changes in database in all environments.\n\u2022 Created PL/SQL packages and DatabaseTriggers and developed user procedures and prepared user manuals for the new programs.\n\u2022 Performed performance improvement of the existing Data warehouse applications to increase efficiency of the existing system.\n\u2022 Designed and developed UseCase, Activity Diagrams, Sequence Diagrams, OOD (Object oriented Design) using UML and Visio.\n\nEnvironment: SQL Server 2008R2/2005 Enterprise, SSRS, SSIS, Crystal Reports, Windows Enterprise Server 2000, DTS, SQL Profiler, and Query Analyzer.', u""Data Scientist\nConAgra Foods - Omaha, NE\nMay 2013 to December 2014\nResponsibilities:\n\u2022 Coded R functions to interface with CaffeDeepLearning Framework\n\u2022 Working in AmazonWebServices cloud computing environment\n\u2022 Worked with several R packages including knitr, dplyr, SparkR, CausalInfer, spacetime.\n\u2022 Implemented end-to-end systems for DataAnalytics, DataAutomation and integrated with custom visualization tools using R, Mahout, Hadoop and MongoDB.\n\u2022 Gathering all the data that is required from multiple data sources and creating datasets that will be used in analysis.\n\u2022 Performed Exploratory DataAnalysis and DataVisualizations using R, andTableau.\n\u2022 Perform a proper EDA, Univariate and bi-variate analysis to understand the intrinsic effect/combined effects.\n\u2022 Worked with Data governance, Data quality, data lineage, Data architect to design various models and processes.\n\u2022 Independently coded new programs and designed Tables to load and test the program effectively for the given POC's using with Big Data/Hadoop.\n\u2022 Designed data models and data flow diagrams using Erwin and MSVisio.\n\u2022 As an Architect implemented MDM hub to provide clean, consistent data for a SOA implementation.\n\u2022 Developed, Implemented & Maintained the Conceptual, Logical&Physical Data Models using Erwin for Forward/ReverseEngineered Databases.\n\u2022 Established Data architecture strategy, best practices, standards, and roadmaps.\n\u2022 Lead the development and presentation of a dataanalytics data-hub prototype with the help of the other members of the emerging solutions team\n\u2022 Performed datacleaning and imputation of missing values using R.\n\u2022 Worked with Hadoop eco system covering HDFS, HBase, YARN and MapReduce\n\u2022 Take up ad-hoc requests based on different departments and locations\n\u2022 Used Hive to store the data and perform datacleaning steps for huge datasets.\n\u2022 Created dash boards and visualization on regular basis using ggplot2 and Tableau\n\u2022 Creating customized business reports and sharing insights to the management\n\u2022 Worked with BTEQ to submit SQL statements, import and export data, and generate reports in Teradata.\n\u2022 Interacted with the other departments to understand and identify dataneeds and requirements and work with other members of the ITorganization to deliver data visualization and reportingsolutions to address those needs.\nEnvironment: Erwin r9.0, Informatica 9.0, ODS, OLTP, Oracle 10g, Hive, OLAP, DB2, Metadata, MS Excel, Mainframes MS Visio, Rational Rose, Requisite Pro. Hadoop, PL/SQL, etc.."", u'Data Scientist\nDPSG - Plano, TX\nAugust 2012 to April 2013\nResponsibilities:\n\u2022 Supported MapReduce Programs running on the cluster.\n\u2022 Evaluated business requirements and prepared detailed specifications that follow project guidelines required to develop written programs.\n\u2022 Configured Hadoop cluster with Namenode and slaves and formatted HDFS.\n\u2022 Used Oozie workflow engine to run multiple Hive and Pig jobs.\n\u2022 Performed Map Reduce Programs those are running on the cluster.\n\u2022 Developed multiple MapReduce jobs in java for data cleaning and preprocessing.\n\u2022 Analyzed the partitioned and bucketed data and compute various metrics for reporting.\n\u2022 Involved in loading data from RDBMS and web logs into HDFS using Sqoop and Flume.\n\u2022 Worked on loading the data from MySQL to HBase where necessary using Sqoop.\n\u2022 Developed Hive queries for Analysis across different banners.\n\u2022 Extracted data from Twitter using Java and Twitter API. Parsed JSON formatted twitter data and uploaded to database.\n\u2022 Launching Amazon EC2 Cloud Instances using Amazon Images (Linux/ Ubuntu) and Configuring launched instances with respect to specific applications.\n\u2022 Exported the result set from Hive to MySQL using Sqoop after processing the data.\n\u2022 Analyzed the data by performing Hive queries and running Pig scripts to study customer behavior.\n\u2022 Have hands on experience working on Sequence files, AVRO, HAR file formats and compression.\n\u2022 Used Hive to partition and bucket data.\n\u2022 Experience in writing MapReduce programs with Java API to cleanse Structured and unstructured data.\n\u2022 Wrote Pig Scripts to perform ETL procedures on the data in HDFS.\n\u2022 Created HBase tables to store various data formats of data coming from different portfolios.\n\u2022 Worked on improving performance of existing Pig and Hive Queries.\n\nEnvironment: SQL/Server, Oracle 9i, MS-Office, Teradata, Informatica, ER Studio, XML, Business Objects.', u'Data Architect/Data Modeler\nPeople Group - IN\nNovember 2010 to July 2012\nResponsibilities:\n\u2022 Deployed GUI pages by using JSP, JSTL, HTML, DHTML, XHTML, CSS, JavaScript, AJAX\n\u2022 Configured the project on WebSphere 6.1 application servers\n\u2022 Implemented the online application by using Core Java, Jdbc, JSP, Servlets and EJB 1.1, Web Services, SOAP, WSDL\n\u2022 Communicated with other Health Care info by using Web Services with the help of SOAP, WSDL JAX-RPC\n\u2022 Used Singleton, factory design pattern, DAO Design Patterns based on the application requirements\n\u2022 Used SAX and DOM parsers to parse the raw XML documents\n\u2022 Used RAD as Development IDE for web applications.\n\u2022 Preparing and executing Unit test cases\n\u2022 Used Log4J logging framework to write Log messages with various levels.\n\u2022 Involved in fixing bugs and minor enhancements for the front-end modules.\n\u2022 Implemented Microsoft Visio and Rational Rose for designing the Use Case Diagrams, Class model, Sequence diagrams, and Activity diagrams for SDLC process of the application\n\u2022 Doing functional and technical reviews\n\u2022 Maintenance in the testing team for System testing/Integration/UAT\n\u2022 Guaranteeing quality in the deliverables.\n\u2022 Conducted Design reviews and Technical reviews with other project stakeholders.\n\u2022 Was a part of the complete life cycle of the project from the requirements to the production support\n\u2022 Created test plan documents for all back-end database modules\n\u2022 Implemented the project in Linux environment.\n\nEnvironment: R 3.0, Erwin 9.5, Tableau 8.0, MDM, QlikView, MLLib, PL/SQL, HDFS, Teradata 14.1, JSON, HADOOP (HDFS), MapReduce, PIG, Spark, R Studio, MAHOUT, JAVA, HIVE, AWS.', u'Data Analyst/Data Modeler\nInfotech Info - Hyd\nJanuary 2009 to October 2010\nResponsibilities:\n\u2022 Worked with project team representatives to ensure that logical and physical ER/Studio data models were developed in line with corporate standards and guidelines.\n\u2022 Involved in defining the source to target data mappings, business rules, data definitions.\n\u2022 Worked with BTEQ to submit SQL statements, import and export data, and generate reports in Teradata.\n\u2022 Responsible for defining the key identifiers for each mapping/interface.\n\u2022 Responsible for defining the functional requirement documents for each source to target interface.\n\u2022 Document, clarify, and communicate requests for change requests with the requestor and coordinate with the development and testing team.\n\u2022 Work with users to identify the most appropriate source of record and profile the data required for sales and service.\n\u2022 Document the complete process flow to describe program development, logic, testing, and implementation, application integration, coding.\n\u2022 Involved in defining the business/transformation rules applied for sales and service data.\n\u2022 Define the list codes and code conversions between the source systems and the data mart.\n\u2022 Worked with internal architects and, assisting in the development of current and target state data architectures.\n\u2022 Coordinate with the business users in providing appropriate, effective and efficient way to design the new reporting needs based on the user with the existing functionality.\n\u2022 Remain knowledgeable in all areas of business operations in order to identify systems needs and requirements.\n\u2022 Responsible for defining the key identifiers for each mapping/interface.\n\u2022 Implementation of Metadata Repository, Maintaining Data Quality, Data Cleanup procedures, Transformations, Data Standards, Data Governance program, Scripts, Stored Procedures, triggers and execution of test plans\n\u2022 Performed data quality in Talend Open Studio.\n\u2022 Coordinated meetings with vendors to define requirements and system interaction agreement documentation between client and vendor system.\n\u2022 Enterprise Metadata Library with any changes or updates.\n\u2022 Document data quality and traceability documents for each source interface.\n\u2022 Establish standards of procedures.\n\u2022 Generate weekly and monthly asset inventory reports.\n\nEnvironment: Erwin r7.0, SQL Server 2000/2005, Windows XP/NT/2000, Oracle 8i/9i, MS-DTS, UML, UAT, SQL Loader, OOD, OLTP, PL/SQL, MS Visio, Informatica.']","[u""Master's""]",[u''],"degree_1 : ""Masters"""
0,https://resumes.indeed.com/resume/d883abd5aaffd2ca,"[u'Data Scientist\nProquest - MountainView, CA, US\nJuly 2017 to Present\nDescription: ProQuest LLC is an Ann Arbor, Michigan-based global information-content and technology company founded in 1938 as University Microfilms by Eugene B. Power. ProQuest provides solutions, applications, and products for libraries.\n\nResponsibilities:\n\u2022 As an Architect design conceptual, logical and physical models using Erwin and build datamarts using hybrid Inmon and Kimball DW methodologies.\n\u2022 Worked closely with business, datagovernance, SMEs and vendors to define data requirements.\n\u2022 Worked with data investigation, discovery and mapping tools to scan every single data record from many sources.\n\u2022 Designed the prototype of the Data mart and documented possible outcome from it for end-user.\n\u2022 Involved in business process modeling using UML\n\u2022 Developed and maintained data dictionary to create metadata reports for technical and business purpose.\n\u2022 Involved on Prediction model building, Machine Learning, Business process improvements, Visualization & Process implementation with R Programming and DeepSee\n\u2022 Redesigned and developed SAS Applications with Netezza Database to the Netezza Applications reducing run time of Applications from 40 hours to 20 sec using PostgreSQL, nzsql, Aginity Workbench, SAS\n\u2022 Created SQLtables with referential integrity and developed queries using SQL, SQL*PLUS, and PL/SQL.\n\u2022 Formulated procedures for integration of R programming plans with data sources and delivery systems and R language was used for prediction.\n\u2022 Implementing SparkMlib utilities such as including classification, regression, clustering, collaborative filtering and dimensionality reduction.\n\u2022 Design, coding, unit testing of ETL package source marts and subject marts using Informatica ETL processes for Oracledatabase.\n\u2022 Developed Statistical Analysis and Response Modeling for Analytical Database contributors (logistic regression).\n\u2022 Used Pig and Hive in the analysis of data.\n\u2022 Used all complex data types in Pig for handling data.\n\u2022 Created/modified UDF and UDAFs for Hive whenever necessary.\n\u2022 Developed various QlikView Data Models by extracting and using the data from various sources files, DB2, Excel, Flat Files and Bigdata.\n\u2022 Participated in all phases of datamining; datacollection, datacleaning, developingmodels, validation, visualization and performed Gapanalysis.\n\u2022 data manipulation and Aggregation from adifferent source using Nexus, Toad, BusinessObjects, PowerBI, and SmartView.\n\u2022 Handled importing data from various data sources, performed transformations using Hive, Map Reduce, and loaded data into HDFS.\n\u2022 Interaction with Business Analyst, SMEs, and other Data Architects to understand Business needs and functionality for various project solutions\n\u2022 Researched, evaluated, architected, and deployed new tools, frameworks, and patterns to built sustainable Big Data platforms for the clients\n\u2022 Identifying and executing process improvements, hands-on in various technologies such as Oracle, Informatica, BusinessObjects.\n\u2022 Loaded and transformed large sets of structured, semi-structured and unstructured data.\n\u2022 Supported Map Reduce Programs those are running in the cluster.\n\u2022 Managed and reviewed Hadoop log files to identify issues when ajob fails.\n\u2022 Designed both 3NF data models for ODS, OLTP systems and dimensional data models using Star and snowflake schemas.\n\u2022 Developed Pig UDFs for preprocessing the data for analysis.\n\u2022 Involved in writing shell scriptsfor scheduling and automation of tasks.\n\nEnvironment: r9.0, Informatica 9.0, ODS, OLTP, Oracle 12c/11g, Hive, OLAP, DB2, Metadata, MS Excel, Mainframes MS Visio, Rational Rose, Requisite Pro., Hadoop, PL/SQL, SAS etc.', u""Data Scientist\nAgero - Boston, MA\nApril 2016 to June 2017\nDescription: We're driving the next generation of vehicle and mobile technology forward, pushing the limits of big data to transform the entire driving experience. The majority of leading vehicle manufacturers and insurance providers use Agero's innovations to strengthen their businesses and create stronger, lasting connections with their customers. Together, we're making driving smarter and safer for everyone.\n\nResponsibilities:\n\u2022 Perform Data Profiling to learn about user behavior and merge data from multiple data sources.\n\u2022 Implemented big data processing applications to collect, clean and normalization large volumes of open data using Hadoopecosystems such as PIG, HIVE, and HBase.\n\u2022 Designing and developing various machine learning frameworks using Python, R, and Matlab.\n\u2022 Integrate R into Micro Strategy to expose metrics determined by more sophisticated and detailed models than natively available in the tool.\n\u2022 Worked on different data formats such as JSON, XML and performed machinelearningalgorithms in Python.\n\u2022 Worked as Data Architects and IT Architects to understand the movement of data and its storage and ERStudio9.7\n\u2022 Processed huge datasets (over billion data points, over 1 TB of datasets) for data association pairing and provided insights into meaningful data association and trends\n\u2022 Developed cross-validation pipelines for testing the accuracy of predictions\n\u2022 Enhanced statistical models (linear mixed models) for predicting the best products for commercialization using Machine Learning Linear regression models, KNN and K-means clustering algorithms\n\u2022 Participated in all phases of datamining, datacollection, datacleaning, developingmodels, validation, visualization and performed Gapanalysis.\n\u2022 data manipulation and Aggregation from adifferent source using Nexus, Toad, BusinessObjects, PowerBI, and SmartView.\n\u2022 Independently coded new programs and designed Tables to load and test the program effectively for the given POC's using with Big Data/Hadoop.\n\u2022 Develop documents and dashboards of predictions in Microstrategy and present it to the business intelligence team.\n\u2022 Developed various QlikViewDataModels by extracting and using the data from various sources files, DB2, Excel, Flat Files and Bigdata.\n\u2022 Good knowledge of HadoopArchitecture and various components such as HDFS, JobTracker, TaskTracker, NameNode, DataNode, SecondaryNameNode, and MapReduce concepts.\n\u2022 As Architect delivered various complex OLAPdatabases/cubes, scorecards, dashboards and reports.\n\u2022 Programmed a utility in Python that used multiple packages (scipy, numpy, pandas)\n\u2022 Implemented Classification using supervised algorithms like LogisticRegression, Decisiontrees, KNN, NaiveBayes.\n\u2022 Used Teradata15 utilities such as FastExport, MLOAD for handling various tasks data migration/ETL from OLTP Source Systems to OLAP Target Systems\n\u2022 Handled importing data from various data sources, performed transformations using Hive, MapReduce, and loadeddata into HDFS.\n\u2022 Collaborate with data engineers to implement ETL process, write and optimized SQL queries to perform data extraction from Cloud and merging from Oracle 12c.\n\u2022 Collect unstructured data from MongoDB 3.3 and completed data aggregation.\n\u2022 Perform data integrity checks, data cleaning, exploratory analysis and feature engineer using R 3.4.0.\n\u2022 Conducted analysis of assessing customer consuming behaviors and discover thevalue of customers with RMF analysis; applied customer segmentation with clustering algorithms such as K-MeansClustering and Hierarchical Clustering.\n\u2022 Work on outliers identification with box-plot, K-means clustering using Pandas, NumPy.\n\u2022 Participate in features engineering such as feature intersection generating, feature normalize and Label encoding with Scikit-learn preprocessing.\n\u2022 Use Python 3.0 (numPy, sciPy, pandas, sci-kit-learn, Seaborn, NLTK) and Spark 1.6 / 2.0 (PySpark, MLlib) to develop avariety of models and algorithms for analytic purposes.\n\u2022 Analyze Data and Performed Data Preparation by applying thehistoricalmodel to the data set in AZUREML.\n\u2022 Perform data visualization with Tableau 10 and generate dashboards to present the findings.\n\u2022 Determine customer satisfaction and help enhance customer experience using NLP.\n\u2022 Work on Text Analytics, Na\xefveBayes, Sentiment analysis, creating word clouds and retrieving data from Twitter and other social networking platforms.\n\u2022 Use Git 2.6 to apply version control. Tracked changes in files and coordinated work on the files among multiple team members.\n\nEnvironment: ER Studio 9.7, Tableau 9.03, AWS, Teradata 15, MDM, GIT, Unix, Python 3.5.2, MLlib, SAS, regression, logistic regression, QlikView."", u""Data Scientist\nCapital One - Reston, VA\nDecember 2014 to March 2016\nDescription: Capital One Financial Corporation is a bank holding company specializing in credit cards, auto loans, banking and savings products headquartered in McLean, Virginia. Capital One is the eighth-largest commercial bank in the United States when ranked by assets and deposits and is ranked 9th on the list of largest banks in the United States by total assets.\n\nResponsibilities:\n\u2022 Data mining using state-of-the-art methods\n\u2022 Extending company's data with third party sources of information when needed\n\u2022 Enhancing data collection procedures to include information that is relevant for building analytic systems\n\u2022 Processing, cleansing, and verifying the integrity of data used for analysis\n\u2022 Doing ad-hoc analysis and presenting results in a clear manner\n\u2022 Creating automated anomaly detection systems and constant tracking of its performance\n\u2022 Strong command of data architecture and data modelling techniques.\n\u2022 Hands on experience with commercial data mining tools such as Splunk, R, Map reduced, Yarn, Pig, Hive, Floop, Oozie, Scala, HBase, Master HDFS, Sqoop, Spark, Scala (Machine learning tool) or similar software required depending on seniority level in job field.\n\u2022 Developed scalable machine learning solutions within a distributed computation framework (e.g. Hadoop, Spark, Storm etc.).\n\u2022 Utilizing NLP applications such as topic models and sentiment analysis to identify trends and patterns within massive data sets.\n\u2022 Knowledge in ML & Statistical libraries (e.g. Scikit-learn, Pandas).\n\u2022 Having knowledge to build predict models to forecast risks for product launches and operations and help predict workflow and capacity requirements for TRMS operations\n\u2022 Having experience with visualization technologies such as Tableau\n\u2022 Draw inferences and conclusions, and create dashboards and visualizations of processed data, identify trends, anomalies\n\u2022 Generation of TLFs and summary reports, etc. ensuring on-time quality delivery.\n\u2022 Participated in client meetings, teleconferences and video conferences to keep track of project requirements, commitments made and the delivery thereof.\n\u2022 Solved analytical problems, and effectively communicate methodologies and results\n\u2022 Worked closely with internal stakeholders such as business teams, product managers, engineering teams, and partner teams.\n\nEnvironment: Erwin r9.0, Informatica 9.0, ODS, OLTP, Oracle 10g, Hive, OLAP, DB2, Metadata, MS Excel, Mainframes MS Visio, Rational Rose, Requisite Pro. Hadoop, PL/SQL, etc.."", u'Data Scientist\nCoventry Health Care - Minneapolis, MN\nApril 2012 to November 2014\nDescription: Coventry Health Care Management utilizes multiple software systems to support the intake and processing of authorization requests, the exchange of data between the payer and vendors contracted to perform services on our behalf, manage Case and Disease programs, provide robust reporting and decision support, and generally automate and facilitate their business processes.\n\nResponsibilities:\n\u2022 Statistical Modelling with ML to bring Insights in Data under guidance of Principal Data Scientist\n\u2022 Data modeling with Pig, Hive, Impala.\n\u2022 Ingestion with Sqoop, Flume.\n\u2022 Used SVN to commit the Changes into the main EMM application trunk.\n\u2022 Understanding and implementation of text mining concepts, graph processing and semi structured and unstructured data processing.\n\u2022 Worked with Ajax API calls to communicate with Hadoop through Impala Connection and SQL to render the required data through it .These API calls are similar to Microsoft Cognitive API calls.\n\u2022 Good grip on Cloudera and HDP ecosystem components.\n\u2022 Used ElasticSearch (Big Data) to retrieve data into application as required.\n\u2022 Performed Map Reduce Programs those are running on the cluster.\n\u2022 Developed multiple MapReduce jobs in java for data cleaning and preprocessing.\n\u2022 Developed scalable machine learning solutions within a distributed computation framework (e.g. Hadoop, Spark, Storm etc.).\n\u2022 Analyzed the partitioned and bucketed data and compute various metrics for reporting.\n\u2022 Involved in loading data from RDBMS and web logs into HDFS using Sqoop and Flume.\n\u2022 Worked on loading the data from MySQL to HBase where necessary using Sqoop.\n\u2022 Developed Hive queries for Analysis across different banners.\n\u2022 Extracted data from Twitter using Java and Twitter API. Parsed JSON formatted twitter data and uploaded to database.\n\u2022 Launching Amazon EC2 Cloud Instances using Amazon Images (Linux/ Ubuntu) and Configuring launched instances with respect to specific applications.\n\u2022 Exported the result set from Hive to MySQL using Sqoop after processing the data.\n\u2022 Analyzed the data by performing Hive queries and running Pig scripts to study customer behavior.\n\u2022 Have hands on experience working on Sequence files, AVRO, HAR file formats and compression.\n\u2022 Used Hive to partition and bucket data.\n\u2022 Experience in writing MapReduce programs with Java API to cleanse Structured and unstructured data.\n\u2022 Wrote Pig Scripts to perform ETL procedures on the data in HDFS.\n\u2022 Created HBase tables to store various data formats of data coming from different portfolios.\n\u2022 Worked on improving performance of existing Pig and Hive Queries.\n\nEnvironment: SQL/Server, Oracle 9i, MS-Office, Teradata, Informatica, ER Studio, XML, Business Objects, HDFS, Teradata 14.1, JSON, HADOOP (HDFS), MapReduce, PIG, Spark, R Studio, MAHOUT, JAVA, HIVE, AWS..', u'Data Architect/Data Modeler\nSpectra force Technologies India Pvt. Ltd - Hyderabad, Telangana\nNovember 2011 to March 2012\nDescription: Spectra force is a leading global services firm that provides a portfolio of consulting, staffing and outsourcing services and solutions to a broad range of clients and industries worldwide. We are headquartered in Raleigh, NC, USA and have offshore global delivery centers in Pune, Chandigarh, and Hyderabad, India.\n\nResponsibilities:\n\u2022 Worked with large amounts of structured and unstructured data.\n\u2022 Knowledge in Machine Learning concepts (Generalized Linear models, Regularization, Random Forest, Time Series models, etc.)\n\u2022 Worked in Business Intelligence tools and visualization tools such as Business Objects, Tableau, ChartIO, etc.\n\u2022 Deployed GUI pages by using JSP, JSTL, HTML, DHTML, XHTML, CSS, JavaScript, AJAX.\n\u2022 Configured the project on WebSphere 6.1 application servers\n\u2022 Implemented the online application by using Core Java, Jdbc, JSP, Servlets and EJB 1.1, Web Services, SOAP, WSDL.\n\u2022 Handled end-to-end project from data discovery to model deployment.\n\u2022 Monitoring the automated loading processes.\n\u2022 Communicated with other Health Care info by using Web Services with the help of SOAP, WSDL JAX-RPC\n\u2022 Used Singleton, factory design pattern, DAO Design Patterns based on the application requirements\n\u2022 Used SAX and DOM parsers to parse the raw XML documents\n\u2022 Used RAD as Development IDE for web applications.\n\u2022 Preparing and executing Unit test cases\n\u2022 Used Log4J logging framework to write Log messages with various levels.\n\u2022 Involved in fixing bugs and minor enhancements for the front-end modules.\n\u2022 Implemented Microsoft Visio and Rational Rose for designing the Use Case Diagrams, Class model, Sequence diagrams, and Activity diagrams for SDLC process of the application\n\u2022 Doing functional and technical reviews\n\u2022 Maintenance in the testing team for System testing/Integration/UAT.\n\u2022 Guaranteeing quality in the deliverables.\n\u2022 Conducted Design reviews and Technical reviews with other project stakeholders.\n\u2022 Was a part of the complete life cycle of the project from the requirements to the production support\n\u2022 Created test plan documents for all back-end database modules.\n\nEnvironment: R 3.0, Erwin 9.5, Tableau 8.0, MDM, QlikView, MLLib, PL/SQL, HDFS, Teradata 14.1, JSON, HADOOP (HDFS), MapReduce, PIG, Spark, R Studio, MAHOUT, JAVA, HIVE, AWS.', u""Data Analyst/Data Modeler\nInspire Solutions - Hyderabad, Telangana\nApril 2010 to October 2011\nDescription: Inspire is leading IT Company which considers ''IT Services'' as a way to develop business for the clients we work for. With over 150+ IT professionals from varied industries working across locations, we ensure nothing but best products and services with utmost care.\n\nResponsibilities:\n\u2022 Developed Internet traffic scoring platform for ad networks, advertisers and publishers (rule engine, site scoring, keyword scoring, lift measurement, linkage analysis).\n\u2022 Responsible for defining the key identifiers for each mapping/interface.\n\u2022 Clients include eBay, Click Forensics, Cars.com, Turn.com, Microsoft, and Looksmart.\n\u2022 Designed the architecture for one of the first analytics 3.0. online platforms: all-purpose scoring, with on-demand, SaaS, API services. Currently under implementation.\n\u2022 Web crawling and text mining techniques to score referral domains, generate keyword taxonomies, and assess commercial value of bid keywords.\n\u2022 Developed new hybrid statistical and data mining technique known as hidden decision trees and hidden forests.\n\u2022 Reverse engineering of keyword pricing algorithms in the context of pay-per-click arbitrage.\n\u2022 Implementation of Metadata Repository, Maintaining Data Quality, Data Cleanup procedures, Transformations, Data Standards, Data Governance program, Scripts, Stored Procedures, triggers and execution of test plans\n\u2022 Performed data quality in Talend Open Studio.\n\u2022 Coordinated meetings with vendors to define requirements and system interaction agreement documentation between client and vendor system.\n\u2022 Automated bidding for advertiser campaigns based either on keyword or category (run-of-site) bidding.\n\u2022 Creation of multimillion bid keyword lists using extensive web crawling. Identification of metrics to measure the quality of each list (yield or coverage, volume, and keyword average financial value).\n\u2022 Enterprise Metadata Library with any changes or updates.\n\u2022 Document data quality and traceability documents for each source interface.\n\u2022 Establish standards of procedures.\n\u2022 Generate weekly and monthly asset inventory reports.\n\nEnvironment: MS Office suite 2008, MS-SQL Server Management Studio 2000/2005/2008/R2, T-SQL, DTS, Replication, Rational Rose, Windows NT, MS SQL Reporting Services 2008, MS SQL Server Analysis Services 2008, MS SQL Server Integration Services 2008, MS Access, Erwin, SQL Query Analyzer.""]","[u""Bachelor's""]",[u''],"degree_1 : ""Bachelors"""
0,https://resumes.indeed.com/resume/d0626ffffea14f70,"[u'Research Scientist, Machine Learning\nThe Trade Desk - Boulder, CO\nNovember 2017 to Present\n\u2756 Use machine learning models (TensorFlow) to improve bidding strategies and uncover hidden affinities\n\u2756 Incorporate external data sources to augment user information and target advertisements more effectively\n\u2756 In real-world testing, resulting algorithm doubles clicks through rate of advertisements at no additional cost', u'Teaching Assistant\nCU Dept. of Applied Mathematics\nSeptember 2013 to Present\nTeach courses in calculus, differential equations, MATLAB, and Mathematica.', u'Data Scientist\nEntelligent LLC\nNovember 2016 to October 2017\nAnalyze and improve financial model, implement scalable portfolio optimization & risk analytics library\nUse library to construct index and ETF (Smart Climate 500) currently published by Bloomberg\nIndex shows higher returns and lower risk than S&P 500', u'Research Assistant, Statistics & Data Analysis\nNatl. Inst. of Standards & Tech\nMay 2014 to August 2015\nDevelop statistical techniques for analyzing data arising in experimental quantum mechanics\nMethod is shown to be significantly more robust than the current most popular approach']","[u'Ph.D. in Applied Mathematics', u'Bachelor of Arts in Physical Sciences']","[u'University of Colorado Boulder, CO\nAugust 2013 to May 2018', u'Reed College Portland, OR\nAugust 2006 to May 2010']","degree_1 : Ph.D. in Applied Mathematics, degree_2 :  Bachelor of Arts in Physical Sciences"
0,https://resumes.indeed.com/resume/80747520369f945d,"[u'Research Survey Scientist, Data Collection Task Leader, Project Manager\nData Collection Task Leader/Project Manager for a set of large-scale surveys of US medical providers. Responsible for operations and management of six separate data collection tasks and budgets for the project, generating detailed work plans and accompanying budgets for each data collection task and, implementation and monitoring of client-approved plans and budgets. Additionally, managed a secondary add-on study: a survey of medical providers regarding medical practice characteristics. Responsible for protocol development, survey and electronic data capture (EDC) system design, study start up, training program development, and operations and management of three separate data collection tasks and accompanying budgets for the add-on study. Responsible for integration of the secondary study into operations for the primary study.\n\n\u2022 Team leadership within professional & contract staffing models\n\u2022 Managed complex budgets within funding constraints and required\ntimelines\n\u2022 Managed project operations within a research-based call center\nsetting\n\u2022 Generated high volumes of project documentation\n\u2022 Partnered with other task leaders to achieve research goals\n\u2022 Authored/contributed to technical reports', u'Principal Research Scientist, Research Project Manager\nTenure includes work on over 20 research projects, half of which were longitudinal in nature. Fostered client relations across a diverse client landscape. Governed budgets of varying scale within funding parameters and timeline constraints. Led research, system, programming, and data collection teams for multiple data coordination center contracts. Directed projects across extensive research populations: HIV/AIDS patients, adolescents, educators, parents, pregnant women, MSM, cohabiting couples, participants reporting pain conditions, LGBT. Responsible for designing study protocols for multi-mode survey instrument methodology: computer-assisted telephone interviews (CATI), computer-assisted personal interviews (CAPI), audio computer-assisted self-administered interviews (ACASI), SAQ (self-administered qx) including optical scan and web-based questionnaire formats. Directed research teams for single and multi-site studies; spearheaded development and oversight of quality control assessments; oversaw biologic and environmental collection protocols. Directed electronic data capture (EDC) system development to include writing EDC user manuals and designing end user trainings. Coordinated Institutional Review Board submissions.\n\n\u2022 Designed and directed trainings for professional research (principal\ninvestigators, clinicians, SME), data collection and QA/QC staff\n\u2022 Led presentations to and debriefings with clients (University, Federal\nAgency)\n\u2022 Key technical coordinator for 2 Data Coordination Centers\n\u2022 Authored/Contributed to technical reports for clients and funding\nagencies\n\u2022 Project Manager for diverse study protocols and methodologies\n\u2022 Mentored professional staff\n\u2022 Contributed to BD capture and proposal efforts']","[u'B.A. in English', u'A.A. in Applied Agriculture']","[u'North Carolina State University Raleigh, NC', u'North Carolina State University Raleigh, NC']","degree_1 : B.A. in English, degree_2 :  A.A. in Applied Agricltre"
0,https://resumes.indeed.com/resume/a48211b9fcdc8878,"[u'Data Scientist\nOscar Health Insurance - New York, NY\nSeptember 2016 to Present\nOscar Health Insurance is a technology-focused health insurance company founded in 2012 and headquartered in New York City. This project was to support auditing team and claim department to improve accounting accuracy and reduce risk of fraudulent activities via providing machine learning and modeling solutions to identify suspicious insurance claims.\n\nResponsibilities:\n\u2022 Gathered, analyzed, documented and translated application requirements into data models, supported standardization of documentation and the adoption of standards and practices related to data and applications.\n\u2022 Queried and aggregated data from Amazon Redshift to get the sample dataset.\n\u2022 Identified patterns, data quality issues, and leveraged insights by communicating with BI team.\n\u2022 In preprocessing phase, used Pandas to remove or replace all the missing data, and feature engineering to eliminate unrelated features.\n\u2022 Balanced the dataset with Over-sampling the minority label class and Under-sampling the majority label class.\n\u2022 In data exploration stage used correlation analysis and graphical techniques to get some insights about the claim data.\n\u2022 Tested classification algorithms such as Logistic Regression, Gradient Boosting and Random Forest using Pandas and Scikit-learn and evaluated the performance.\n\u2022 Implemented, tuned and tested the model on AWS EC2 with the best algorithm and parameters.\n\u2022 Set up data preprocessing pipeline to guarantee the consistency between the training data and new coming data.\n\u2022 Deployed the model on AWS Lambda, collaborated with develop team to build the business solutions.\n\u2022 Collected the feedback after deployment, retrained the model to improve the performance.\n\u2022 Designed, developed and maintained daily and monthly summary, trending and benchmark reports in Tableau Desktop.\n\nEnvironment:\nAWS EC2, S3, Redshift, Lambda, Linux, Python (Scikit-Learn/Numpy/Pandas/Matplotlib), Machine Learning (Logistic Regression/Gradient Boosting/Random Forest), Tableau', u""Data Scientist\nWSFS Bank - Wilmington, DE\nFebruary 2015 to August 2016\nWSFS bank is a financial service company headquartered in Delaware, providing personal service, small business service and commercial service mainly in Delaware and Pennsylvania. The project was to implement machine learning algorithms to identify fraudulent transactions across all sectors to enhance bank's anti-money laundry process and security management.\n\nResponsibilities:\n\u25cf Collected and analyzed the business requirements, mapped the data from database to fulfill the analysis purpose.\n\u25cf Designed SSIS package to perform extract, transform and load (ETL) data across different platforms and validate the data and achieve the data from database.\n\u25cf Tackled highly imbalanced Fraud dataset using oversampling with SMOTE (Synthetic Minority Over-Sampling Technique) and cost sensitive algorithms with Python Scikit-learn to fulfill the dataset requirement.\n\u25cf Worked on data cleaning and ensure data quality, consistency, integrity using Numpy and Pandas.\n\u25cf Participated in feature engineering such as feature intersection generating, feature normalize and label encoding with Scikit-learn preprocessing.\n\u25cf Implemented Logistic regression, Random forest classification and Gradient boosting classification to compare performance.\n\u25cf Deployed the model on AWS EC2 using Flask.\n\u25cf Created and maintained reports to display the status and performance of deployed model and algorithm with Tableau.\n\nEnvironment:\nMS SQL Server 2012, SSIS, Tableau, Python (Scikit-Learn/Scipy/Numpy/Pandas), Machine Learning (Naive Bayes, KNN, Random Forest, SVM), AWS S3, EC2, SharePoint 2013"", u""Data Analyst/Data Scientist\nNational Lloyds Insurance - Dallas, TX\nMarch 2014 to February 2015\nNational Lloyds Insurance Company is the specialist in Homeowners and Renters Insurance, Dwelling Coverage, Mobile Home Insurance, Fire Insurance, Flood Insurance, Commercial and Business Insurance. This project targeted on implementing a fully automated customer segmentation system using machine learning and data analytic models to support marketing department.\n\nResponsibilities:\n\u25cf Collaborated with database engineers to implement ETL process, wrote and optimized SQL queries to perform data extraction and merging from SQL server database.\n\u25cf Conducted analysis in assessing customer behaviors and discover value of customers, applied customer segmentation with clustering algorithm.\n\u25cf Performed data integrity checks, data cleansing, exploratory analysis and feature engineer using python and data visualization packages such as Matplotlib, Seaborn.\n\u25cf Used Python to develop a variety of models and algorithms for analytic purposes.\n\u25cf Developed logistic regression models to predict subscription response rate based on customer's variables like past transactions, promotions, response to prior mailings, demographics, interests and hobbies, etc.\n\u25cf Used prediction model to rank the importance of features and deliver feature engineering.\n\u25cf Used Python to implement different machine learning algorithms, including Generalized Linear Model, Random Forest, SVM and Gradient Boosting.\n\u25cf Evaluated parameters with K-Fold Cross Validation and optimized performance of models.\n\u25cf Recommended and evaluated marketing approaches based on quality analytics on customer consuming behavior.\n\u25cf Collected and analyzed the customer feedback by using the streaming data from social networks stored in Hadoop system with Hive.\n\u25cf Performed data visualization and Designed dashboards with Tableau, and provided complex reports, including charts, summaries, and graphs to interpret the findings to the team and stakeholders.\n\u25cf Identified process improvements that significantly reduce workloads or improve quality.\n\nEnvironment:\nPython (Scikit-Learn/Scipy/Numpy/Pandas), Linux, Tableau, MS SQL Server 2012, Hadoop, Hive, MS SSIS, Windows 8/XP, JIRA"", u'SQL BI Developer/Data Analyst\nFocus Diagnostics Inc - Herndon, VA\nJanuary 2013 to February 2014\nFocus Diagnostics, Inc. manufactures and distributes molecular and serology products worldwide.\nThis project was to consolidate data from different sources (different laboratory databases), develop SQL codes and maintain packages to generate datasets and migrate data to support the Report develop and Analytics Team.\n\nResponsibilities:\n\u25cf Collected requirements from business users, and designed report models to meet business requirements.\n\u25cf Maintained and developed complex SQL queries, stored procedures, views, functions and reports that meet customer requirements using Microsoft SQL Server 2008 R2.\n\u25cf Created Views and Table-valued Functions, Common Table Expression (CTE), joins, complex subqueries to provide the reporting solutions.\n\u25cf Optimized the performance of queries with modification in T-SQL queries, removed the unnecessary columns and redundant data, normalized tables, established joins and created index.\n\u25cf Created SSIS packages using Pivot Transformation, Fuzzy Lookup, Derived Columns, Condition Split, Aggregate, Execute SQL Task, Data Flow Task and Execute Package Task.\n\u25cf Used SSIS to create ETL packages to Validate, Extract, Transform and Load data into Data Warehouse and Data Mart.\n\u25cf Created, managed, and delivered interactive web-based reports to support daily operations.\n\u25cf Validated reports and resolve issues in a timely manner.\n\u25cf Developed and implemented several types of Financial Reports (Income Statement, Profit & Loss Statement and Ad Hoc Reports) by using SSRS.\n\u25cf Developed parameterized dynamic performance Reports (Gross Margin, Revenue base on geographic regions, Profitability based on web sales and smartphone app sales) and ran the reports every month and distributed them to respective departments through mailing server subscriptions and SharePoint server.\n\u25cf Designed and developed new reports and maintained existing reports using Microsoft SQL Reporting Services (SSRS) and Microsoft Excel to support the business strategy and management.\n\u25cf Created sub-reports, drill down reports, summary reports, parameterized reports, and ad-hoc reports using SSRS.\n\nEnvironment:\nSQL Server 2012, MS SQL Server Management Studio, MS BI Suite (SSIS/SSRS), T-SQL, Visual Studio']",[u'Master of Science in Industrial and Systems Engineering'],[u'Lehigh University'],degree_1 : Master of Science in Indstrial and Systems Engineering
0,https://resumes.indeed.com/resume/e8c9008e1a8a9ee8,"[u'Bookkeeper\nSC Associates - Middletown, DE\nJune 2017 to Present\nProvide Bookkeeping and Payroll services to clients', u'Bookkeeper\nFaw Casson - Dover, DE\nNovember 2016 to June 2017\nWork with Bookkeeping clients using QuickBooks to process payrolls, record and reconcile monthly finances, and quarterly tax reporting for payroll clients. Use QuickBooks on a daily basis, as well as MS Excel.', u'Accounting Assistant\nHeadley Associates - Dover, DE\nOctober 2015 to November 2016\nResponsibilities\nAnswer phones and great clients when they come into the office.\nFiling, typing, faxing, scanning, basic office responsibilities.\nConduct weekly, biweekly, and monthly payroll for clients using QuickBooks.\nEnter information on bank accounts into QuickBooks, reconcile and run financial reports.\nBasic Income Tax Preparation.\n\nAccomplishments\nBecame a Notary Public\n\nSkills Used\nQuickBooks, Word, Excel, office equipment', u'Data Management Coordinator\nJobs for Delaware Graduates - Dover, DE\nAugust 2009 to September 2015\nWork to ensure that Jobs for Delaware Graduates stays on track with contract agreements with external funding sources. As a non-profit organization, it is critical that funding is awarded each year from multiple State agencies and private sector sources. Manage data, making sure numbers are accurate and working with employees to ensure JDG looks the best it can with percentages. Organized and manage electronic and paper files, making the files more easily accessible to retrieve for audits when they are conducted.\n\u2022 Increased validation completion rate by understanding and mainstreaming Excel spreadsheets.\n\u2022 Ready and willing to do work, anything from analyzing data in a database to moving boxes in a storage shed.\n\u2022 JDG 2012-13 Employee of the Year', u'Integrated Science Teacher\nCampus Community High School - Dover, DE\nJanuary 2007 to January 2008\nPlanned, organized, and conducted the teaching of classroom and laboratory science classes for 8th, 10th, and 12th graders. Developed curriculum to meet the Delaware State standards for the science classes for these grade levels.\n\u2022 Incorporated technology in all aspects of the curriculum\n\u2022 Assigned lessons, corrected papers, and heard oral presentations\n\u2022 Kept attendance and grade records in e-School', u'Science Teacher\nPositive Outcomes Charter School - Camden, DE\nJanuary 2006 to January 2007\nTaught science to 9th, 10th, 11th and 12th graders.\n\u2022 Designed curriculum to encompass the various student learning levels in the classroom\n\u2022 Exhibited patience, ability, and desire to work with children who have special needs\n\u2022 Established and implemented a developmentally appropriate curriculum\n\u2022 Prepared course objectives and outlined the course of study following curriculum guidelines and requirements of the state and school\n\u2022 Used e-School to take attendance and report grades', u""Environmental Scientist\nState of Delaware - DNREC - Dover, DE\nJanuary 1996 to January 2006\nAnalyzed Hazardous Chemical Inventory Reports for completeness and accuracy. Maintained the State Emergency Response Commissions web site. Organized and conducted workshops to go over Emergency Planning and Community Right-to-Know requirements and training on reporting software. Wrote queries from relational databases to get data from the databases and put into a user friendly format. Recorded meeting minutes from the State Emergency Response Commission. Worked on the State of Delaware Hazmat team.\n\u2022 Worked with Emergency responders and the Local Emergency Planning Committee's to disseminate hazardous chemical inventory information.\n\u2022 Worked with facility owners and managers to ensure they were in compliance with the Emergency Planning and Community Right-to-Know Act laws and regulations.\n\u2022 EPA award for work in organizing and coordinating the Delaware Online Hazardous Chemical Inventory database.""]","[u'A.A.S. in Accounting', u'Certificate in ARTC Program', u'B.S. in Environmental Sciences']","[u'Delaware Technical & Community College Dover, DE\nJanuary 2013 to January 2016', u'University of Delaware Georgetown, DE\nJanuary 2006', u'Wesley College Dover, DE\nJanuary 1993']","degree_1 : A.A.S. in Acconting, degree_2 :  Certificate in ARTC Program, degree_3 :  B.S. in Environmental Sciences"
0,https://resumes.indeed.com/resume/ff95311f44ce3208,"[u'APPLAB Systems INC\nSeptember 2013 to Present\nAPPLabSystems INC is a Data Consulting firm for the Fortune 500 Clients like Capital Group, Experian, Disney and so on.', u""Data Scientist\nSecurity and Supply Chain - Los Angeles, CA\nJanuary 2013 to Present\nCognizant is one of the world's leading software consulting company.\n\u29eb Applied the Data Mining and Statistical analysis to the Market and Sales\ndata lakes.\n\u29eb Prototyped the Linear Regression model to track the user inactivity.\n\nProject 4 Client Name: Disney, Los Angeles, CA\n\nData Scientist\nWalt Disney Imagineering designs and builds all Disney theme parks,\nresorts, attractions worldwide.\n\u29eb Implemented SVM Model resulting in secure customer transactions.\n\n\u29eb Developed clear and well-structured analytical plans and analyzed large\ndata-sets.\n\u29eb Developed models and reports, shared observations and recommendations with senior executives.\n\nProject 5 Client Name: Experian, Denver, CO\n\nA Credit Services company, Experian unlocks the power of data to create\nopportunities for consumers, businesses and society.My responsibilities\nhere were to data mine the web analytics data on the legacy website to improve the operational and product experience for the customers.\n\u29eb Prototyped the Linear Regression model to track the user inactivity\nusing Scikit-Learn.\n\u29eb Actively expedited the incumbent Market Segmentation analytics\n\u29eb Created customized reports for the executives."", u""Lloyds Bank - London\nJune 2010 to July 2012\nDKRIN is a Data Consulting company that provides a variety of services for the Banking and Financial firms.\n\nProject 6 Client Name: Lloyds Bank\n\nLloyd's bank is the UK's largest retail and commercial bank.\n\u29eb Implemented Logistic Regression Model using Natural Language\nProcessing resulting identify the customer intent and run smart ad\ncampaigns,\n\u29eb Performed text mining to understand the financial behavior and preferences of potential customers."", u'HSBC Bank\nJune 2008 to August 2009\nDataBot Systems is a Software consulting company for the Financial\nfirms.\n\nProject 7 Client Name: HSBC Bank\n\nDatabot system offers a data analytics technology platform.\n\u29eb Designed the schema and Built the SQL database for the internal\nfinancial operations.\n\u29eb Gathered, learning present and historical data in preparation for data\nmining.']","[u'Masters in Computer Science', u'MSc in Product Design']","[u'Texas A & M University\nJanuary 2012', u'University of South Wales\nJanuary 2010']","degree_1 : Masters in Compter Science, degree_2 :  MSc in Prodct Design"
0,https://resumes.indeed.com/resume/c47f7ac095973653,"[u'Data Scientist Intern\nAugust 2017 to November 2017\n\u2022 Prepared customer behavior datasets for classification tasks using SQL and R\n\u2022 Proposed two optional machine learning methods to predict customer purchases.\n\u2022 Built logistic regression model to select features according to its p-values and to predict whether consumers will purchase the products; adopted random forest method to filter features using feature importance, and predicted the purchase probability\nbased on the target of low variance & bias, and achieved a model performance of 93.7% accuracy.\n\u2022 Worked directly with VP, data scientist team and sale team to create a demo and generated business insights based on model\noutput, leading to a 10% increase in consumer purchase\n\u2022 Redesigned interactive visualization graphs in D3.js', u'Web Designer Assistant\nSeptember 2015 to January 2016\n\u2022 Maintained university website (HTML & CSS) including fixing thousands of broke links/images, etc.\n\u2022 Designed the website layout and edited content\n\u2022 Gathered, prioritized, and responded to the user requests with a high customer satisfaction (93% satisfaction rate)', u""UI Designer Assistant\nSchool of Nursing & Health Professions\nAugust 2015 to September 2015\n\u2022 Designed and created nursing course modules on Canvas for USF's online graduate school (courses descriptions, schedules, etc.)\n\u2022 Improved the UI by modulating in HTML and CSS, including adding new functions or buttons, designing new web interfaces, and updating course contents\n\u2022 Edited graphs using Photoshop""]",[u'B.S. in Data Science'],[u'University of San Francisco\nJanuary 2013 to May 2017'],degree_1 : B.S. in Data Science
0,https://resumes.indeed.com/resume/7b45e1a53185a00b,"[u""Data Scientist\n\u2022 Designing of Physical Data Architecture of New system engines.\n\u2022 Worked and extracted data from various database sources like Oracle, SQL Server, DB2, and Teradata.\n\u2022 Well experienced in Normalization &De-Normalization techniques for optimum performance in relational and dimensional database environments.\n\u2022 Regularly accessing JIRA tool and other internal issue trackers for the Project development.\n\u2022 Skilled in System Analysis, E-R/Dimensional Data Modeling, Database Design and implementing RDBMS specific features.\n\u2022 Hands on experience in implementing LDA, Na\xefve Bayes and skilled in Random Forests, Decision Trees, Linear and Logistic Regression, SVM, Clustering, neural networks, Principle Component Analysis (PCA) and good knowledge on Recommender Systems.\n\u2022 Expertise in all aspects of Software Development Life Cycle (SDLC) from requirement analysis, Design, Development Coding, Testing, Implementation and Maintenance.\n\u2022 Hand on working experience in machine learning and statistics to draw meaningful insights from data. I am good at communication and storytelling with data.\n\u2022 Utilize analytical applications/libraries like Plotly, D3JS andTableau to identify trends and relationships between different pieces of data, draw appropriate conclusions and translate analytical findings into marketing strategies that drive value.\n\u2022 Experienced in working with enterprise search platform like Apache Solr and distributed real-time processing system like Storm.\n\u2022 Hands on experience on Spark Mlib utilities such as classification, regression, clustering, collaborative filtering, dimensionality reductions\n\u2022 Strong knowledge of statistical methods (regression, time series, hypothesis testing, randomized experiment), machine learning, algorithms, data structures and data infrastructure.\n\u2022 Experience in Extracting data for creating Value Added Datasets using Data Science, Python, R, SAS, Azure and SQL to analyze the behavior to target a specific set of customers to obtain hidden insights within the data to effectively implement the project Objectives.\n\u2022 Experience in creating Data Visualizations for KPI's as per the business requirements for various departments.\n\u2022 Solid team player, team builder, and an excellent communicator.\n\u2022 Extensive hands-on experience and high proficiency with structures, semi-structured and unstructured data, using a broad range of data science programming languages and big data tools including R, Python, Spark, SQL, Scikit Learn, Hadoop Map Reduce\n\u2022 Expertise in Technical proficiency in Designing, Text Processing, Data Modeling Online Applications, Solution Lead for Architecting Data Warehouse/Business Intelligence Applications.\n\u2022 Skilled in Advanced Regression Modeling, Correlation, Multivariate Analysis, Model Building, Business Intelligence tools and application of Statistical Concepts\n\u2022 Extensive experience working in a Test-Driven Development and Agile-Scrum Development.\n\u2022 Experience in working on both windows, Linux and UNIX platforms including programming and debugging skills in UNIX Shell Scripting.\n\u2022 Flexible with Unix/Linux and Windows Environments, working with Operating Systems like Centos5/6, Ubuntu13/14, Cosmos.\n\u2022 Defining job flows in Hadoop environment-using tools like Oozie for data scrubbing and processing.\n\u2022 Experience in Data migration from existing data stores to Hadoop.\n\u2022 Developed MapReduce programs to perform Data Transformation, Text Processing and analysis.""]",[],[],degree_1 : 
0,https://resumes.indeed.com/resume/a916a51d81fa56ed,"[u'Data Scientist\nIBM - NY\nDecember 2014 to Present\n\u2022 Predicting Employee Engagement using social media data (US Patented).\n\u2022 Engagement analysis of employees using Twitter data\n\u2022 Identifying key drivers and barriers to engagement over time with text mining.\n\u2022 Developing a new hierarchical topic-modeling algorithm.\n\u2022 Developing a new time series prediction algorithm based on nonlinear analysis and machine learning techniques.\n\u2022 Analyzing the level of self-censorship of employees in social media.\n\u2022 Developing a recommendation system using machine-learning techniques for specific target groups to increase their engagement.\n\u2022 Text analysis using Spark.\n\u2022 Develop a new theme extraction algorithm for analysis of social data.', u""Research Assistant\nUniversity of Windsor - Windsor, ON\nSeptember 2009 to November 2014\nInvented a process and system for performing predictive modeling using machine learning techniques (US\nPatented, 61/882863).\n\u2022 Created a software for time series prediction using data mining and machine learning techniques (Implemented by MATLAB and R).\n\u2022 Developed a solution for prediction of financial time series (including successful prediction of Dow Jones\nIndustrial Average Stock index during 2008 financial crisis) using data mining and evolutionary algorithms.\n\u2022 Built an algorithm for prediction of epileptic seizures using nonlinear analysis and optimization (Implemented by R).\n\u2022 Developed several large-scale data mining prototypes for pattern mining by using Python and MySQL.\n\u2022 Designed and implemented a specific platform for analyzing large data sets of epileptic patients using\nMATLAB and Weka.\n\u2022 Developed a rule extraction algorithm using Weka and MATLAB.\n\u2022 Analysis and predicting the monthly records of global temperature anomalies time series using optimization\nalgorithms and nonlinear time series analysis.\n\u2022 Developed an agent-based model for simulating complex systems using C++.\n\u2022 Implemented an efficient database for storing 80 TB of data in database using C++ and MySQL.\n\u2022 Developed a dynamic memory management for handling 200,000 agents simultaneously using C++.\n\u2022 Designed and developed a unique data structure for coding agents' genome using C++."", u'Software Engineer\nScientific Green\nJanuary 2005 to January 2009\nDesigned and developed an operating system (SSCOS) for smart card using C++ (Joint project with\nSAMSUNG SDS).\n\u2022 Implemented test framework for evaluating smart card operating systems.\n\u2022 Created dynamic library for reading data from Epassport using C++.\n\u2022 Developed cryptography algorithms based on the PKCS#15 standard.\n\u2022 Developed an OCX for working with the SAGEM fingerprint reader.\n\u2022 Developed identity card application on Javacards.', u'Instructor\nAzad University\nJanuary 2008 to January 2008\n2008\n\u2022 Software Engineering (Instructor for two semesters).\n\u2022 Programming Language using C++ (Instructor for two semesters).', u'Data Modeler\nIUST\nJanuary 2005 to January 2008\nDeveloped a Neuro-Fuzzy system for character recognition by using C++ and MATLAB.\n\u2022 Created a new Adaptive-Boost-based algorithm for creating ensemble of classifiers.\n\u2022 Implemented a function approximation algorithm using genetic programming implemented by C++.\n\u2022 Built a persian speech recognition system.', u'Web Developer\nTRC\nJanuary 2004 to January 2005\nDeveloped websites using PHP, MySQL, Javascript.']","[u'PhD in Computer Science', u'MSc in Computer Engineering', u'BSc in Computer Engineering']","[u'University of Windsor\nJanuary 2009 to January 2014', u'Iran University of Science and Technology\nJanuary 2005 to January 2008', u'Shahed University\nJanuary 2001 to January 2005']","degree_1 : PhD in Compter Science, degree_2 :  MSc in Compter Engineering, degree_3 :  BSc in Compter Engineering"
0,https://resumes.indeed.com/resume/686b414cb8546e29,"[u'Contextual Data Scientist Internship\nCenter for Innovation Management Studies - Raleigh, NC\nMay 2017 to Present\n\u2022Constructing an IBM based contextual strategic big-data modeling for different agribusiness fields, such as insect control and crops genome editing; proved the model by identifying additional 10% unknown competitors and partners worldwide\n\u2022Leading the development of a new strategic research model based on big-data model and the new research model will close out the dead zone by building up a constant developing knowledge base for the industry to accelerate the market research efficiency by 80% faster and drive the business to take absolute advantage in the competition\n\u2022Scheming real-data based demonstration of visualization-aid functions within IBM Watson. This standardized demonstration is pioneering in IBM Watson Use and optimizing project critical path by saving 10% project duration\n\u2022Providing recommendations to management team on implementing the new research model to allow multiple functional departments and more than 1,000 employees to adopt the new research method\n\u2022Designing and developing Natural Language Process (NLP) approaches to abstract accurate contextual data from the database and open sources to answer strategic questions. This core phase with effectively designed strategies can facilitate our client to take advantages or avoid risks by knowing early research in their business operation\n\u2022Analyzing generated data and concluding weighted metrics for applicable results to respond customer\u2019s tactical questions in a bilingual environment\n\u2022Training and communicating with customers upon requests for the use of new analytics and decision-making tools', u""Contextual Data Scientist Internship\nCenter for Innovation Management Studies - Raleigh, NC\nJanuary 2017 to Present\nBig data-driven decision-making projects for one of the biggest global public biotechnology company)\n\u2022 Constructing an IBM based contextual strategic big-data modeling for different agribusiness fields, such as insect control\nand crops genome editing; proved the model by identifying additional 10% unknown competitors and partners worldwide\n\u2022 Leading the development of a new strategic research model based on big-data model and the new research model will close\nout the dead zone by building up a constant developing knowledge base for the industry to accelerate the market research\nefficiency by 80% faster and drive the business to take absolute advantage in the competition\n\u2022 Scheming real-data based demonstration of visualization-aid functions within IBM Watson. This standardized\ndemonstration is pioneering in IBM Watson Use and optimizing project critical path by saving 10% project duration\n\u2022 Providing recommendations to management team on implementing the new research model to allow multiple functional\ndepartments and more than 1,000 employees to adopt the new research method\n\u2022 Designing and developing Natural Language Process (NLP) approaches to abstract accurate contextual data from the database and open sources to answer strategic questions. This core phase with effectively designed strategies can facilitate\nour client to take advantages or avoid risks by knowing early research in their business operation\n\u2022 Analyzing generated data and concluding weighted metrics for applicable results to respond customer's tactical questions in a bilingual environment\n\u2022 Training and communicating with customers upon requests for the use of new analytics and decision-making tools"", u'Accounting Manager Assistant\nIBX Seafood LLC - Plymouth, NC\nMarch 2016 to June 2016\n\u2022Managed accounts receivable/payable functions relating to customers and suppliers, including invoicing, voucher processing, payment processing with massive in and out cash flows; recorded related transactional inputs', u'Accounting Specialist\nSZ RongXing ID, Ltd - Shenzhen, CN\nJanuary 2009 to January 2010\nCollaborated with German partner company and supported a 1-year German technical team of 8 in payroll filing and tax\npreparation in China\n\u2022 Communicated with German partner to align knowledge and commitment regarding routine working tasks and contingencies; ensured on time information exchange for foreign technician team\n\nAdditional Information']","[u'MBA in Entrepreneurship and Technology Commercialization', u'Master of Global Innovation Management in MGIM', u'Bachelor of Management in Accounting']","[u'North Carolina State University, Jenkins Graduate School of Management, Raleigh\nAugust 2017 to December 2018', u'North Carolina State University, Jenkins Graduate School of Management, Raleigh Aix en Provence, FR\nMay 2017', u'Beijing Institute of Technology Beijing, CN\nJune 2009']","degree_1 : MBA in Entreprenership and Technology Commercialization, degree_2 :  Master of Global Innovation Management in MGIM, degree_3 :  Bachelor of Management in Acconting"
0,https://resumes.indeed.com/resume/4df1590cff2e092e,"[u'Data Science Consultant\nCarlson Analytics Lab - Minneapolis, MN\nJuly 2017 to Present\nClient: Fortune 500 Dairy Manufacturer\n\u2022 Partnered with manufacturer to analyze IoT sensor data to identify factors affecting cheese quality and forming\nprocess controls to decrease failure rate.\n\u2022 Process owner for team with special focus on scrum framework.\nClient: Mall of America\n\u2022 Predicted visitors to Nickelodeon Universe amusement park using Gated Recurrent Neural Networks in Python\nthus saving $163K in staffing costs and improving customer traffic management.\n\u2022 Created cost-effective strategies to enhance customer experience using clustering and Bayes nets in R and Tableau.\nClient: PwC\n\u2022 Led a team of five for profitability analysis of an Auto Insurance company, potentially reducing losses by $5M.\n\u2022 Analyzed hypothesis, created customer clusters in Rapid Miner to identify significant factors causing losses.', u'Decision Scientist\nMU SIGMA Inc - Bengaluru, Karnataka\nJanuary 2017 to April 2017\n\u2022 Led a team of three for R&D in analytics to build innovation pipeline in areas AI, IoT, Digital Worker.\n\u2022 Allocated $1.5M towards use cases in areas of computer vision, NLP, Chatbots to drive analytical\ntransformation in energy industry.\n\u2022 Designed a computer vision framework to streamline and consolidate image analytics projects for client.\n\u2022 Built an image classifier with TensorFlow to identify anomalies in electric utility assets.\n\u2022 Pitched $2M RFP to Fortune 100 energy company by designing proof of concept for grid optimization use-case.\n\u2022 Mentored 10 employees to ease transition from training to client services.', u""Trainee Decision Scientist\nSeptember 2015 to December 2016\nAnalyzed 80 hypothesis across 4 customer segments to identify factors affecting churn. Constructed 19 predictive\nmodels in SAS leading to savings of $6M and 60% reduction in marketing.\n\u2022 Established nuclear analytics division for the team. Defined and created EDA framework for a fuzzy problem to increase efficiency by 1% and reduce fuel usage by $5M.\n\u2022 Eliminated manual evaluation of windmill video data requiring 600-man hours using Hog-SVM and neural nets.\n\nDATA SCIENCE PROJECTS\n\u2022 Created customer segments and performed sentiment analysis using LDA for Sun Country leveraging Big data\ntechnologies like Apache Spark, Hive, EMR and Amazon AWS.\n\u2022 Analyzed Gartner's magic quadrant for BI and created end to end ETL/SQL solution in Datameer.\n\u2022 Built solar saving estimator with geo-visualizations leveraging Python libraries like Geoplotlib, Dash, Plotly.\n\u2022 Predicted high-cost patients with Type 2 Diabetes based on claims history as part of MinneMUDAC challenge.""]","[u'Master of Science in Business Analytics', u'Bachelor of Engineering in Computers']","[u'UNIVERSITY OF MINNESOTA Minneapolis, MN\nMay 2018', u'SAVITRIBAI PHULE PUNE UNIVERSITY Pune, Maharashtra\nMay 2015']","degree_1 : Master of Science in Bsiness Analytics, degree_2 :  Bachelor of Engineering in Compters"
0,https://resumes.indeed.com/resume/407b3d10e232f7b6,"[u'Data Scientist Intern\nCompass Red - Wilmington, DE\nJanuary 2018 to Present\n\u2022 Developed a schedule optimization program for an ecommerce client, that identified 35 wine combinations, among more than 115,000 possible decisions, for each time period for a given week. Model resulted in average sales increase of 11%\n\u2022 Created dashboards in Tableau to summarize 3 Million+ orders, and identified customer preferences and seasonality trends', u'Business Analyst Intern\nCrane Co. - Malvern, PA\nJune 2017 to September 2017\n- Created a financial model in excel that forecasts Crane\u2019s performance across its major product lines globally and enables Business Development to analyze impact of various assumptions. Results were used in Crane\u2019s 2018-2020 Strategic Plan\n- Developed an improved R program for Crane\u2019s Media Network, that statistically tested advertisement campaign performance, by reducing code from 5000 to 500 lines and automating entire program. Reduced actual run time of program by 63 %', u'Junior Project Director\nMSI International - King of Prussia, PA\nMarch 2016 to June 2016\nManaged fieldwork, data processing and client reporting of 3 nationwide surveys for a Fortune 50 telecommunications company.\n\u2022 Summarized respondent attitude towards 8 new product offerings, and identified top offering among customers and prospects', u'Data Processing Spec Writer\nMSI International - King of Prussia, PA\nMay 2015 to March 2016\nCreated tabulation programs, data tables and other electronic outputs within Unix for 30+ nationwide surveys, and enabled Project Directors to develop customer satisfaction and brand equity reports for a Fortune 50 telecommunications company\n\u2022 Verified data accuracy & integrity by writing Quantum programs for 40+ nationwide online and phone surveys, and ensured\nsuccessful execution of back-end deliverables']","[u""Master's in Business Analytics"", u'BS in Business Administration in Marketing', u'BS in Leisure and Sport Studies']","[u'Drexel University LeBow College of Business Philadelphia, PA\nSeptember 2016 to March 2018', u'Kutztown University of Pennsylvania Kutztown, PA\nJanuary 2011 to May 2015', u'Kutztown University Kutztown, PA\nJanuary 2011 to May 2015']","degree_1 : ""Masters in Bsiness Analytics"", degree_2 :  BS in Bsiness Administration in Marketing, degree_3 :  BS in Leisre and Sport Stdies"
0,https://resumes.indeed.com/resume/e1c59a6d9f74bfec,"[u""Data Scientist\nArgus Information & Advisory Services, LLC - White Plains, NY\nSeptember 2016 to Present\nArgus Information and Advisory Services is a Subsidiary of Verisk Analytics Company and the leading provider of analytics, information and solutions to consumer banks and their regulators. The company's clients range from financial institutions to retailers and tech companies. The project focused on detecting anti-money laundering violation using Big Data and Data Science tools and improving customer's transaction monitoring system.\nResponsibilities:\n\u2022 Collected and analyzed the business requirements, understood the particular Fraud/AML challenges that our client faces.\n\u2022 Participated in Data integration job with Data Engineer team to gather traditional transaction data and external source data together.\n\u2022 Transformed data from SQL Server database to Hadoop Clusters which is set up by using AWS EMR.\n\u2022 Conducted data cleansing and feature engineering job through python NumPy and Pandas.\n\u2022 Implemented Naive Bayes, Logistic Regression, SVM, Random Forest and Gradient boosting with weighted loss function by using Python Scikit-learn.\n\u2022 Implemented mulit-layers Neural Networks by using Google Tensorflow and Spark.\n\u2022 Performed extensive Behavioral modeling and Customer Segmentation to discover behavior patterns of customers by using K-means Clustering.\n\u2022 Managed and scheduled models by using Oozie for batch processing.\n\u2022 Updated and saved Fraud predictions to AWS S3 for application team.\n\u2022 Tested the business performance of the AML models by evaluating detection rate and false positive rate and worked on continuous improvement on model.\n\u2022 Created reports and dashboards, by using Tableau, to explain and communicated data insights, significant features, model's score and performance of new transaction monitoring system to both technical and business teams.\n\u2022 Used GitHub for version control with Data Engineer team and Data Scientists colleagues.\nEnvironment: SQL Server 2014, Hadoop 2.0, Hive 2.0, Spark (PySpark, SparkSQL), Python 3.X, Tensorflow, Oozie 4.2, Tableau 10.X, AWS S3/EC2/EMR, Github"", u'Data Scientist\nCenterLight Health System - Bronx, NY\nApril 2015 to July 2016\nCenterLight Health System, a not-for-profit organization, has evolved into a leader in serving the elderly, chronically ill and disabled. CenterLight is one of the largest long-term care providers in New York State, serving all of New York City, Westchester, Nassau, Rockland and Suffolk Counties. This project aimed to predict the billing cycles and accounting related issues to increase the efficiency of enterprise claim processing.\n\nResponsibilities:\n\u2022 Conducted reverse engineering based on demo reports to understand the data without documentation.\n\u2022 Generated new data mapping documentations and redefined the proper requirements in detail.\n\u2022 Generated different Data Marts for gathering the tables needed (Member info, Claim info, Transaction info, Appointment info, Diagnose info) from SQL Server Database.\n\u2022 Created ETL packages to transform data into the right format and join tables together to get all features required using SSIS.\n\u2022 Processed data using Python pandas to examine transaction data, identify outliers and inconsistencies.\n\u2022 Conducted exploratory data analysis using python NumPy and Seaborn to see the insights of data and validate each feature through different charts and graphs.\n\u2022 Built predictive models including Linear regression, Lasso Regression, Random Forest Regression and Support Vector Regression to predict the claim closing gap by using python scikit-learn.\n\u2022 Used GridSearchCV to evaluate each model and to find best parameters set for each model.\n\u2022 Created reports and an app demo using Tableau to show client how prediction can help the business.\n\u2022 Deployed and hosted our models by using Azure Machine Learning Studio and share an API with application development team.\n\u2022 Used Confluence to share and collaborate on projects with team members, and keep track of up to date documentations.\n\nEnvironment: SQL Server 2012, SQL Server Data Tools 2010, SQL Server Integration Services, Python 2.7/3.3, Tableau 9.4, Azure Machine Learning Studio', u""Junior Data Scientist\nAtlantic Health - Morristown, NJ\nJanuary 2014 to March 2015\nAtlantic Health System is one of the leading non-profit health care systems in New Jersey, providing a wide array of health care services to the residents of Northern and Central regions of the state as well as Pike County, PA, and southern Orange County, NY. Project was to build a predictive model to predict the readmission case. The main objective was to reduce the risk of being wrongly diagnosed and the risk of being involved in the legal disputes.\n\nResponsibilities:\n\u2022 Communicated and coordinated with other departments to gather business requirements.\n\u2022 Gathered data information from multiple sources, and performed resampling method to handle the issue of imbalanced data.\n\u2022 Worked with ETL Team and Doctors to understand the data and define the uniform standard format.\n\u2022 Conducted data cleansing by using advanced SQL queries in SQL Server Database.\n\u2022 Split the data into different smaller dataset based on different diagnoses, in charge of conducting exploratory data analysis for three of diagnoses datasets (Diabetes, cold/flu, allergy).\n\u2022 Created the whole pipeline of data preprocessing (imputing, scaling, label encoding) through python pandas to get data ready to modeling part.\n\u2022 Built predictive models, using python scikit-learn, including Support Vector Machine, Decision tree, Naive Bayes Classifier, Neural Network to predict a potential readmitted case.\n\u2022 Performed Ensemble methods, including Gradient Boosting, Random Forest, customized ensemble method to produce more accurate solutions.\n\u2022 Designed and implemented cross-validation and statistical tests including Hypothesis testing, AVOVA, Chi-square test to verify models' significance.\n\u2022 Created a API by using Flask and shared the idea with application team and help them define the requirements of new application.\n\u2022 Used Agile methodology and Scrum process for project developing.\n\nEnvironment: SQL server 2012, SQL Server Integration Services, Python 2.7, Jupyter notebook, Flask 0.10, SharePoint 2013"", u'BI Developer\nFulton Financial Corporation - Lancaster, PA\nDecember 2012 to October 2013\nFulton is a financial company based in Lancaster, Pennsylvania. They provide a wide range of financial products and personalized services in Pennsylvania, Maryland, Delaware, Virginia and New Jersey. They are comprised of several different banking subsidiaries. The main job of this project was to provide ETL solutions for data migration and provide data quality and micro strategy solutions.\n\nResponsibilities:\n\u2022 Involved in gathering user/project requirements from business users and IT managers, translated it into functional and non-functional specifications needed and created documentations for the project.\n\u2022 Assisted in design and data modeling efforts of Data Marts and Enterprise Data Warehouse.\n\u2022 Used T-SQL in SQL Server to develop complex stored procedures, triggers, clustered index & non-clustered index, Views, and User-defined Functions (UDFs).\n\u2022 Designed SSIS packages to extract, transform and load existing data into SQL Server, used lots of components of SSIS, such as Pivot Transformation, Fuzzy Lookup, Derived Columns, Condition Split, Aggregate, Execute SQL Task, Data Flow Task and Execute Package Task.\n\u2022 Created SSIS Packages that involved dealing with different source formats (Text files, XML, Database Tables)\n\u2022 Debugged and troubleshot the ETL packages by using breakpoint, analyzing process, catching error information by SQL command in SSIS.\n\u2022 Create reports with the use of SSRS to generate different types of reports such as tabular, matrix, drill down and charts reports with accordance with user requirement.\n\u2022 Maintained and updated existing reports, analyzed the SQL queries and logic behind them to improve the performance.\n\u2022 Helped deploy the report with scheduling, subscription, history snapshot configured and set up.\n\u2022 Developed in Agile environment throughout the project.\n\nEnvironment: SQL server 2008/2012, SQL Server Management Studio (SSMS), MS BI Suite (SSIS, SSRS)']",[u'Master of Science in Electrical Engineering'],[u'Stevens Institute of Technology'],degree_1 : Master of Science in Electrical Engineering
0,https://resumes.indeed.com/resume/1fefdffa72078718,"[u'Data Scientist- Intern\nSCHNEIDER ELECTRIC\nSeptember 2017 to Present\nDesigned and programmed an outlier detection process for real-time data based on KDE (Kernel Density\nEstimation) algorithm leveraging R and C#, thereby automating and streamlining the manual task\n\u2022 Investigated and explored methods for modelling a limited history time-series data for single signal prediction,\nthereby proposing solutions based on LSTM RNN, ARIMA and Exponential Smoothing models\n\u2022 Predicted performance of hardware equipment by implementing LSTM-Recurrent Neural Network(RNN) model,\nleveraging deep learning libraries - Tensorflow, Keras in Python', u'IT Application - Intern\nCHAMBERLAIN GROUP INC\nMay 2016 to August 2016\nAnalyzed and translated warehouse and logistics business processes into SAP system, thus reducing time\nlogistics/warehouse activities by 30% and cost by 12%\n\u2022 Streamlined goods receiving and shipping data through automation and enhancement in RF barcode process,\nhelping the project achieve its goal\n\u2022 Presented my project to the Executive Leadership Team', u'Business Analyst\nTATA CONSULTANCY SERVICES - Gandhinagar, Gujarat\nAugust 2013 to July 2015\n\u2022 Key resource for Material & Warehouse Management (MM/WM) module of SAP ECC 6.0 in multiple projects across different geographies\n\u2022 Worked as a liaison between client and development team, translating business requirements into technical\ndesign- DB tables, scripts, forms, reports and data flow as a part of the project and regular support activities\n\u2022 Well-acquainted with data extraction capabilities of Excel including VLOOKUP and pivot tables, in course of data cleansing task and preparing monthly reconciliation reports\n\u2022']","[u'Masters in Management Information Systems', u'Bachelor of Engineering in Computer Science', u'in architecture']","[u'ILLINOIS INSTITUTE OF TECHNOLOGY Chicago, IL\nMay 2017', u'MAHARAJA SAYAJIRAO UNIVERSITY OF VADODARA Vadodara, Gujarat\nJune 2013', u'visit university gym']","degree_1 : Masters in Management Information Systems, degree_2 :  Bachelor of Engineering in Compter Science, degree_3 :  in architectre"
0,https://resumes.indeed.com/resume/f8a73dc973101f55,"[u""Data Scientist Intern\nWilliams Sonoma\nAugust 2013 to Present\nAnalytics Group)\n\u2022 Project to analyze customer retention rate for one of William Sonoma's brands by modeling customer behaviors with Machine Learning Algorithm through Python's Sklearn and R's Glm package\n\u2022 Worked in Teradata (SQL) environment for data gathering (data set consists of 60 tables, 300 fields, and one million records)\n\u2022 Performed statistical analysis using R's statistical packages (car, ggplot2, e1071)\n\u2022 Search Engine Optimization by improving recall and ranking using Machine Learning Algorithms"", u'Test Engineer\nAreva\nAugust 2012 to August 2013\nTest Engineer (Contract)\n\u2022 Generated material characterization for solar coating with spectroscopy (FTIR, Vi-IR, LIBS)\n\u2022 Implemented and upkeep an SQL database of all coating related test results and performed data analysis using MATLAB and data visualization with Tableau\n\u2022 Worked in a cross functional and international research and manufacturing team consist of French Scientists, Chinese Solar Manufacturer, and American scientists', u'Product Development Engineer\nAvery Dennison\nSeptember 2011 to August 2012\n\u2022 Engineering lead in the development of a consumer product ($10M Project)\n\u2022 Worked in an international cross functional team consists of Procurement, Marketing, Sales, and Finance\n\u2022 Communicated in Mandarin Chinese with Chinese manufacturers and in English with U.S. Marketing team', u'Intern\nLawrence Berkeley National Laboratory\nJune 2009 to January 2010\nWorked with a femtosecond laser to develop an autocorrelator to measure pulse length and conducted Helium Ion Imaging experiments\n\nAdditional Information\nU.S. Green Card Holder, Native in English and Chinese (Mandarin)\nSAS Certified Base Programmer for SAS 9\nProgramming Languages: Python, R, MATLAB, SQL, SAS,\nAnalytics Techniques: Machine Learning, Time Series, Linear Regression, Logistic Regression, Text Mining, Map Reduce, Teradata, Mongodb, Tableau, Multiprocessing']","[u'Master of Science in Analytics', u'B.S. in Chemical Engineering']","[u'University of San Francisco\nJune 2014', u'University of California, Berkeley Berkeley, CA']","degree_1 : Master of Science in Analytics, degree_2 :  B.S. in Chemical Engineering"
0,https://resumes.indeed.com/resume/7ba7de1c96f6cf40,"[u'Data Scientist\nAviall\nJune 2017 to Present\nResponsibilities:\n\u2022 Played key role in optimizing and benchmarking the Classification models in order to standardize the results across different departments.\n\u2022 Applied advanced statistical and predictive modeling techniques to build, maintain, and improve on multiple real-time decision systems. Conducted advanced data analysis and developed complex algorithms.\n\u2022 Built models using Statistical techniques and Machine Learning classification models like XG Boost, SVM, and Random Forest. Developed and design advanced predictive analysis models. Model and frame business scenarios that are meaningful and impact critical business processes and/or decisions.\n\u2022 Worked with Big Data Technologies such Hadoop, Hive, MapReduce. Extracted data from HDFS and prepared data for exploratory analysis using data munging. Designed experiments, tested hypothesis, and built models.\n\u2022 Developed MapReduce/Spark Python modules for machine learning & predictive analytics in Hadoop on AWS. Implemented a Python-based distributed random forest via Python streaming.\n\u2022 Worked with statistical models for data analysis, predictive modelling, machine learning approaches, recommendation and optimization algorithms.\n\u2022 Leveraged Sales/Customer/Marketing Analytics to increase company revenue.\n\u2022 Developed and presented clear concise recommendations outlining alternatives and key decision criteria. Prepared graphs using GGplot library and Tableau for an overview of the analytical models and results.\n\u2022 Handled importing data from various data sources, performed transformations using Hive, Map Reduce, and loaded data into HDFS.\nEnvironment: SAS, Python (Scikit Learn), Tableau, Tensorflow, Linux Systems(Ubantu), Hive, MongoDB, SQL, Apache Spark, Apache Hadoop. Major Model tested: Neural Networks, SVM, Logistic Regression, k-Nearest Neighbor (kNN): Decision Tree. Ensemble Trees: Random Forest, GBMboost, XGboost', u'Data Scientist\nRare Genomics Institute\nJune 2015 to May 2017\nResponsibilities:\n\u2022 Developed computational and data science solutions for the storage, management, analysis, and visualization of genomic data.\n\u2022 Leveraged existing tools and publicly available genomics data to develop, test, or implement bioinformatics pipelines.\n\u2022 Provided expertise in statistical methods or machine learning with the goal of applying these techniques to health data.\n\u2022 Worked with Mobile Science 2.0, Mobile App teams to build a Classifier for Mobile App users that could be used by the digital marketing team to tailor specific messages to groups of users.\n\u2022 Used regulatory genomics/epigenetics & computational approaches in genetics and Patient data to perform clustering to group patients with similar diseases.\n\u2022 Algorithms implemented in Python, SQLite, Hadoop, MapReduce, MongoDB, R.\n\u2022 Worked with Big Data Technologies such Hadoop, Hive, MapReduce.\n\u2022 Handled importing data from various data sources, performed transformations using Hive, MapReduce, and loaded data into HDFS\nEnvironment: Python, Hadoop, MapReduce, Spark, Hive, R. Major models tested: K-Means Clustering, SVM, Decision Tree based models: CART, CHAID, Information Gain, Random Forest', u'Jr. Data Scientist\nCeridian HCM, Inc\nFebruary 2013 to May 2015\nResponsibilities:\n\u2022 Worked on data cleaning and reshaping, generated segmented subsets using numpy and Pandas in Python. Developed Python scripts to automate data sampling process. Ensured the data integrity by checking for completeness, duplication, accuracy, and consistency.\n\u2022 Worked on model selection based on confusion matrices, minimized the Type II error. Generated cost-benefit analysis to quantify the model implementation comparing with the former situation.\n\u2022 Wrote and optimized complex SQL queries involving multiple joins and advanced analytical functions to perform data extraction and merging from large volumes of historical data stored in Oracle 11g, validating the ETL processed data in target database.\n\u2022 Conducted model optimization and comparison using stepwise function based on AIC value.\n\u2022 Applied various machine learning algorithms and statistical modeling like decision tree, logistic regression, Gradient Boosting Machine to build predictive model using scikit-learn package in Python.\n\u2022 Continuously collected business requirements during the whole project life cycle. Identified the variables that significantly affect the target\n\u2022 Environment: Decision Tree, Logistic regression, Hadoop, Teradata, Python, MLLib, SAS, random forest, OLAP, HDFS, NLTK, SVM, JSON and XML', u'Data Analyst\nHornet Infotech - IN\nJune 2009 to December 2010\nResponsibilities:\n\u2022 Ran SQL queries in Oracle database to analyze and manipulate data. Wrote SAS programs to performed ad-hoc analysis and data manipulation.\n\u2022 Created various SAS Reports, Tables, Graphs and Summary analysis on PMS systems being used in these properties.\n\u2022 Transferred data from Oracle database as well as MS Excel into SAS for analysis and used filters based on the analysis.\n\u2022 Used SAS Import/Export Wizard as well as SAS Programming techniques to extract data from Excel.Used SAS Base programming as well as SAS Enterprise Guide 4.0 to produce various reports, charts and graphs\n\u2022 Participated in the technology support team meeting to coordinate, review and determine appropriate hotel property software system for hotel property\nEnvironment: SAS, SQLite, Hadoop, MapReduce, SQL, MS Excel.', u'Java Developer\nSunGard Solutions\nJune 2008 to June 2009\nResponsibilities:\n\u2022 Actively participated in all the phases of SDLC including Requirements Collection, Design & Analysis of the Customer Specifications, Development and Customization of the application.\n\u2022 Developed the application using Agile/Scrum methodology which involves daily stand ups. Test driven development, continuous integration, demos and test automations.\n\u2022 Strong hands-on knowledge of Core JAVA, Web-Based Application, and OOPS concepts.\n\u2022 Developed Client Side technologies using HTML, CSS, and Java Script. Developed Server Side technologies using spring, Hibernate, Servlets/JSP, Multithreading.\n\u2022 Extensively worked with the retrieval and manipulation of data from the Oracle database by writing queries using SQL and PL/SQL. Web application development by Setting up an environment, configuring an application and Web Logic Application Server.\n\u2022 Hands on Experience in coding, unit testing, Integration testing and Bug fixing.\nEnvironment: Oracle/SQL Server and PL/SQL, spring, Hibernate, Ant, Apache, Tomcat, JBOSS, Web logic, UNIX, RDBMS, HTML, CSS, Java Script, JDBC, Eclipse, Multithreading.']","[u'Master of Science in Computer Science', u'Bachelor of Technology in Computer Science and Engineering']","[u'Indiana State University', u'JNTU']","degree_1 : Master of Science in Compter Science, degree_2 :  Bachelor of Technology in Compter Science and Engineering"
0,https://resumes.indeed.com/resume/fbfc5ef09daa8641,"[u""Data Scientist\nMomentum Travel Group - Bellevue, WA\nMarch 2017 to Present\n\u2022 Machine learning fraud prevention system design and development.\n\n\u2022 Enabled office-wise SQL analytic capability and provided data source for machine learning by developing and maintaining company's first analytic-facing MySQL database using Python SQLAlchemy, which integrates and cleans raw prod JSON events and API payloads from over 5 sources into one database and easy-to-understand table structures.\n\n\u2022 Established pinpoint rules by building, visualizing and understanding decision tree outcomes through python scikit-learn and simulating future impacts through RDBMS query, reducing review rate by 30% and auto reject rate by 50%.\n\n\u2022 Developed FraudNet, an xgboost-based fraud prevention machine learning model with less than 0.001% confirmed false positives and over 0.985 AUC, which corrects analyst's false decisions, provides QA suggestions for QA team, and is able to reduce review rate by 50%.\n\n\u2022 Docker implementation for machine learning production system deployment."", u'Data Scientist\nCardinalCommerce - Cleveland, OH\nJune 2015 to January 2017\n\u2022 Built the machine learning model, which can optimize the conversion rate of transactions, recover over 80% of stuck transactions and increase sales by 5% for merchants ($50 million potential increase for Newegg), using Random Forest with H2O.ai.\n\n\u2022 Improved classification of bank authentication protocols by 20% by applying clustering methods on transaction data using Python Scikit-learn locally, and Spark MLlib on production level.\n\n\u2022 Modeled risk-based banks\u2019 history behavior of processing time on transactions to automatically create labeled challenge behaviors data using kernel density estimation.\n\n\u2022 Reduced customer friction during shopping by 50% and increased liability shift capability up to 25% for merchants by predicting whether the transaction will be challenged before completed using logistic regression and pre-labeled behaviors data.\n\n\u2022 Performed ad-hoc analytic on millions of diverse data, including user agent, latency, geography, devices, and service providers, using Python, Spark and SQL to find out potential areas of optimization.']",[u'Master of Science in Operations Research'],"[u'Case Western Reserve University Cleveland, OH\nAugust 2014 to January 2016']",degree_1 : Master of Science in Operations Research
0,https://resumes.indeed.com/resume/7ead7adbf72f9ded,"[u'Teaching Assistant\nUniversity of Virginia - Charlottesville, VA\nJanuary 2018 to Present\n\uf06c Answer questions for first-year graduate students regarding to their study of the course, covering topics of linear regression, binary, ordinal, and multinomial logistic regression and count models;\n\uf06c Run the weekly 2-hour lab and providing feedback on weekly problem sets. The software package used is Stata.', u'Data Scientist Intern\nChinese Academy of Science - Shenzhen\nJune 2017 to August 2017\n\uf06c Developed contactless sleep monitoring system with deep learning and biomedical engineering team;\n\uf06c Researched on the relationship between cardiac arrhythmia and heartbeat, expiration, body movement data;\n\uf06c Developed algorithms to extract features such as R-R interval, hypopnea index from filtered data with Python;\n\uf06c Authored reports references for transplanting the traditional ECG based cardiac arrhythmia recognition models to pressure signal based machine learning model.']","[u'MS in Statistics', u'BS in Applied Mathematics']","[u'University of Virginia Charlottesville, VA\nAugust 2016 to May 2018', u'Sichuan University Chengdu\nSeptember 2012 to June 2016']","degree_1 : MS in Statistics, degree_2 :  BS in Applied Mathematics"
0,https://resumes.indeed.com/resume/b9c5d40cdcb005a7,"[u'Data Scientist\nCapital One\nOctober 2016 to Present\nInvolve in data resource gathering, data scraping, data cleansing, and loading in a cloud database\n\u2022 Analyze existing data repository of 43 million UNIQUE records\n\u2022 Acquired and cleaned data using Talend and structure data from multiple sources\n\u2022 Performed data extraction, manipulation, cleaning, analysis, modeling and data mining using Python\n\u2022 Designed 10+ dashboards in Tableau for sales managers with instant access to personalized analytics portal\n\u2022 Exploratory data analysis using python to diagnose areas of improvement to increase efficiency', u""Systems Analyst\nAbbott Nutrition, Cognizant solutions\nAugust 2015 to September 2016\n\u2022 Delivered Interactive visualizations/dashboards using matplotlib and Tableau to present data analysis outcomes in terms of patterns, anomalies and predictions.\n\u2022 Created multiple workbooks, dashboards, and charts using calculated fields, quick table calculations, Custom\nhierarchies, sets & parameters to meet business needs.\n\u2022 Create and monitor KPIs representing the product usage and user interactions in real time\n\u2022 Led the company's machine learning and statistical modeling effort including building predictive models and generate data products to support customer segmentation, product recommendation and allocation planning;\nprototyping and experimenting ML algorithms and integrating into production system for different business needs"", u'Analyst\nTCS, Farmers Insurance, MassMutual Financials, Indiana State Gov\nJune 2011 to August 2015\nPrepared comprehensive documented observations, analyses and interpretations of results including technical\nreports, summaries, protocols and quantitative analyses\n\u2022 Proficient in research of current process and emerging technologies which need analytic models, data inputs and output, analytic metrics and user interface needs.\n\u2022 Designed easy to follow visualizations using Tableau software and published dashboards on web\n\u2022 Under supervision of Sr. Data Scientist performed Data Transformation and data cleaning and applied Backward -\nForward filling methods on datasets\n\u2022 Performed data management, including creating SQL Server Report Services to develop reusable code']","[u'Master of Science in Computer Science', u'Bachelor of Technology in Information Technology']","[u'Texas A&M University Kingsville', u'SASTRA University']","degree_1 : Master of Science in Compter Science, degree_2 :  Bachelor of Technology in Information Technology"
0,https://resumes.indeed.com/resume/e15814fc30e5a168,"[u'Data Scientist | User Experience Manager\nBYU Analytics - Provo, UT\nSeptember 2017 to Present', u'Marketing Analyst Intern | Data Analytics\nPluralsight - Salt Lake City, UT\nJune 2017 to August 2017\nAnalyzed data and created dashboards for the CMO & Marketing Department\n\u2022 Created real-time dashboard measuring key performance indicators of over $200M revenue marketing campaigns\n\u2022 Forecasted over $10M revenue for the coming quarter by analyzing historical data from the campaign\u2019s results\n\u2022 Increased by 25% the campaigns responses by applying segmentation analysis to target customers and users\n\u2022 Summarized $92.6M revenue and identified trends on the quarter results to support performance meetings\n\u2022 Created and executed daily reports for the CMO and Marketing Department to keep track of marketing results', u'Engineering Consultant | Dam Design Engineer\nKL ENGINEERING - Fortaleza, Brazil\nJune 2014 to June 2016\nDesigned and evaluated 10+ dams (worth +$1B)\n\u2022 Reduced by $1M construction and remodeling cost by improving and implementing new methods and models\n\u2022 Improved forecasted values by 20% for extreme events by using automatic calibration to model spatial data, extremal value analysis (EVA) and time series analysis', u'Product Manager | Data Scientist\nASTEF ENGINEERING CONSULTING - Fortaleza, Brazil\nFebruary 2014 to June 2016\nCreated an innovative consumer-based software and coordinated a cross-functional team of 8 consultants\n\u2022 Increased productivity over 20% by automatizing data acquisition and integrating data visualization\n\u2022 Drove results with accuracy over 90% by applying cross-validation, machine learning and statistical analysis\n\u2022 Brought reliability to the developing product by applying it to real problems and validating the results\n\u2022 Created training material and conducted the preparation course for +50 future users and customers', u'Project Manager | Operations Specialist\nSTATE DEPARTMENT OF WATER RESOURCE MANAGEMENT - Fortaleza, Brazil\nMay 2010 to January 2014\nDesigned and optimized management processes for 6 different teams by using innovative technologies, statistical models and data mining\n\u2022 Modeled 70% of the systems by improving data processing and reducing the time analysis by 90% (9 days)\n\u2022 Reduced the risk of collapse from 70% to 20% and improved processes using data analysis and optimization\n\u2022 Developed a mixed integer linear programming (MILP) model by using linear regression and data transformation\n\u2022 Developed a methodology using statistical analysis and modeling to support the negotiation and drive decisions', u'Product Manager | Data Scientist\nSTATE FOUNDATION FOR METEOROLOGY AND WATER MANAGEMENT - Fortaleza, Brazil\nJanuary 2006 to April 2010\nDeveloped an engineering software and implemented new technologies to improve management processes\n\u2022 Defined requirements for a customer-based B2B/B2C software by using design thinking and market demands\n\u2022 Decreased by 4.5% ($100K) operational annual costs and reduced by 94% the risk of collapse on the water system by using machine learning to a multi-objective optimization of a non-linear model\n\u2022 Reduced by 94% the risk of collapse on the water system by modeling and adjusting operational practices\n\u2022 Developed a linear model using Linear Programming and suggested new operational rules for water systems\n\u2022 Implemented Nash-Sutcliffe model efficient coefficient to evaluate and compare results and time series\n\u2022 Applied clustering analysis to segment solutions and keep diversity in the dataset\n\u2022 International publications in Computational Intelligence (DOI 10.1007/978-3-642-05165-4_3)']","[u'MBA in Marketing and Analytics', u'Master of Science in Computational Intelligence', u'Bachelor of Science in Civil Engineering']","[u'MARRIOTT SCHOOL OF MANAGEMENT - BRIGHAM YOUNG UNIVERSITY Provo, UT\nJanuary 2016 to January 2018', u'FEDERAL UNIVERSITY OF CEARA Fortaleza, CE\nJanuary 2005 to June 2007', u'FEDERAL UNIVERSITY OF CEARA Fortaleza, CE\nJanuary 2000 to December 2004']","degree_1 : MBA in Marketing and Analytics, degree_2 :  Master of Science in Comptational Intelligence, degree_3 :  Bachelor of Science in Civil Engineering"
0,https://resumes.indeed.com/resume/b1346912763b4206,"[u'Data Scientist\nProject - H1B Visa Petition - Syracuse, NY\nMarch 2017 to Present\n\u2756 Devising a predictive model to forecast the trends in H1B selection by mining a dataset containing 3 million records.\n\u2756 Incorporating techniques of scrubbing data, removing anomalies, creating a predictive model and conducting regression tests by using Spark ML and methods such as Random Forests and Linear Regression.', u'Database Analyst\nSyracuse University Lock Shop, iConsult Syracuse University - Syracuse, NY\nNovember 2016 to Present\n\u2756 Executed the task of upgrading the system by migrating the client\u2019s MS-DOS database to Microsoft SQL using SQL Server.\n\u2756 Minimizing the risk of the data\u2019s exposure if intercepted or misrouted using Oracle TDE, to increase security level by 100%.\n\u2756 Creating a secure database to store confidential information of over 50,000 IDs and passwords.\n\u2756 Creating data generation plans to generate realistic and representative test data and conducted regression tests using T-SQL.\n\u2756 Performing QA testing to the database to correct issues and to back track if necessary, thus ensuring data integrity.', u'Project Manager\nAPHASIA LABS, iConsult, Syracuse University - Syracuse, NY\nSeptember 2016 to Present\n\u2756 Oversaw operations in translating business needs into technical terms, chartered the project, prepared work breakdown structures and performed project evaluation and review techniques.\n\u2756 Pioneered the project implementing AGILE and delivered the project ahead of schedule, decreasing project time by 20%.\n\u2756 Administered high-quality work employing LEAN Development, and increased work allocation efficiency by 40%.', u'Supply Chain Intern\nNokia Technologies - Chennai\nMay 2015 to July 2015\n\u2756 Coordinated the strategic sourcing and planning of more than 100,000 transistors and diodes in imports from countries including Philippines, China, and Myanmar into the South Indian headquarters.\n\u2756 Developed Balance Sheets & Income Statements and assisted in the preparation and interpretation of other financial documents.\n\u2756 Created sales reports and calculated sets for preparing dashboards of 175,000 records of log data using Tableau.\n\u2756 Prepared detailed technical documentation such as workflows, scripts and diagrams in coordination with researchers.']","[u""Master's in Information Management"", u""Bachelor's in Electronics and Communication""]","[u'Syracuse University Syracuse, NY\nAugust 2016 to May 2018', u'Srm university Chennai, TN\nAugust 2012 to May 2016']","degree_1 : ""Masters in Information Management"", degree_2 :  ""Bachelors in Electronics and Commnication"""
0,https://resumes.indeed.com/resume/d57096677932469b,"[u""Data Scientist\nwellsfargo Des Moines,Lowa\nMarch 2017 to Present\nDescription: Wells Fargo is forever linked with the image of a six-horse stagecoach thundering across the American West, loaded with gold. The full history, over more than 160 years, is rich in detail with great events in America's history. From the Gold Rush to the early 20th Century, through prosperity, depression and war, Wells Fargo earned a reputation of trust due to its attention and loyalty to customers.\n\nResponsibilities:\n\u27a2 Extracted data from HDFS and prepared data for exploratory analysis using data munging\n\u27a2 Built models using Statistical techniques like Bayesian HMM and Machine Learning classification models like XGBoost, SVM, and Random Forest.\n\u27a2 Participated in all phases of data mining, data cleaning, data collection, developing models, validation and visualization and performed Gap analysis.\n\u27a2 A highly immersive Data Science program involving Data Manipulation Visualization, Web Scraping, Machine Learning, Python programming, SQL, GIT, Mongo DB, Hadoop.\n\u27a2 Setup storage and data analysis tools in AWS cloud computing infrastructure.\n\u27a2 Installed and used Caffe Deep Learning Framework\n\u27a2 Worked on different data formats such as JSON, XML and performed machine learning algorithms in Python.\n\u27a2 Worked as Data Architects and IT Architects to understand the movement of data and its storage and ER Studio 9.7\n\u27a2 Used pandas, numpy, seaborn, matplotlib, scikit-learn, scipy, NLTK in Python for developing various machine learning algorithms.\n\u27a2 Data Manipulation and Aggregation from different source using Nexus, Business Objects, Toad, Power BI and Smart View.\n\u27a2 Implemented Agile Methodology for building an internal application.\n\u27a2 Focus on integration overlap and Informatica newer commitment to MDM with the acquisition of Identity Systems.\n\u27a2 Coded proprietary packages to analyze and visualize SPCfile data to identify bad spectra and samples to reduce unnecessary procedures and costs.\n\u27a2 Programmed a utility in Python that used multiple packages (numpy, scipy, pandas)\n\u27a2 Implemented Classification using supervised algorithms like Logistic Regression, Decision trees, Naive Bayes, KNN.\n\u27a2 As Architect delivered various complex OLAP databases/cubes, scorecards, dashboards and reports.\n\u27a2 Updated Python scripts to match training data with our database stored in AWS Cloud Search, so that we would be able to assign each document a response label for further classification.\n\u27a2 Used Teradata utilities such as Fast Export, MLOAD for handling various tasks data migration/ETL from OLTP Source Systems to OLAP Target Systems\n\u27a2 Data transformation from various resources, data organization, features extraction from raw and stored.\n\u27a2 Validated the machine learning classifiers using ROC Curves and Lift Charts.\nEnvironment: Unix, Python 3.5.2, MLLib, SAS, regression, logistic regression, Hadoop 2.7.4, NoSQL, Teradata, OLTP, random forest, OLAP, HDFS, ODS, NLTK, SVM, JSON, XML and MapReduce."", u""Data Scientist\nEllucian - Malvern, PA\nOctober 2015 to February 2017\nDescription: Ellucian's technology solutions are designed for the modern student specifically to meet the needs of higher education. Our software and services help students, staff, and faculty achieve their goals.\n\nResponsibilities:\n\u27a2 Utilized Spark, Scala, Hadoop, HQL, VQL, oozie, pySpark, Data Lake, TensorFlow, HBase, Cassandra, Redshift, MongoDB, Kafka, Kinesis, Spark Streaming, Edward, CUDA, MLLib, AWS, Python, a broad variety of machine learning methods including classifications, regressions, dimensionally reduction etc.\n\u27a2 Utilized the engine to increase user lifetime by 45% and triple user conversations for target categories.\n\u27a2 Application of various machine learning algorithms and statistical modeling like decision trees, text analytics, natural language processing (NLP), supervised and unsupervised, regression models, social network analysis, neural networks, deep learning, SVM, clustering to identify Volume using scikit-learn package in python, Matlab.\n\u27a2 Worked onanalyzing data from Google Analytics, AdWords and Facebook etc.\n\u27a2 Evaluated models using Cross Validation, Log loss function, ROC curves and used AUC for feature selection and elastic technologies like ElasticSearch, Kibana.\n\u27a2 Performed Data Profiling to learn about behavior with various features such as traffic pattern, location, Date and Time etc.\n\u27a2 Categorized comments into positive and negative clusters from different social networking sites using Sentiment Analysis and Text Analytics\n\u27a2 Performed Multinomial Logistic Regression, Decision Tree, Random forest, SVM to classify package is going to deliver on time for the new route.\n\u27a2 Performed data analysis by using Hive to retrieve the data from Hadoop cluster, Sql to retrieve datafrom Oracle database and used ETL for data transformation.\n\u27a2 Performed Data Cleaning, features scaling, features engineering using pandas and numpy packages in python.\n\u27a2 Exploring DAG's, their dependencies and logs using Air Flow pipelines for automation\n\u27a2 Performed data cleaning and feature selection using MLlib package in PySpark and working with deep learning frameworks such as Caffe, Neon.\n\u27a2 Developed Spark/Scala,R Python for regular expression (regex) project in the Hadoop/Hive environment with Linux/Windows for big data resources.\n\u27a2 Used clustering technique K-Means to identify outliers and to classify unlabeled data.\n\u27a2 Tracking operations using sensors until certain criteria is met using Air Flow technology.\n\u27a2 Responsible for different Data mapping activities from Source systems to Teradata using utilities like TPump, FEXP,BTEQ, MLOAD, FLOAD etc\n\u27a2 Analyze traffic patterns by calculating autocorrelation with different time lags.\n\u27a2 Ensured that the model has low False Positive Rate and Text classification and sentiment analysis for unstructured and semi-structured data.\n\u27a2 Addressed over fitting by implementing of the algorithm regularization methods like L1 and L2.\n\u27a2 Used Principal Component Analysis in feature engineering to analyze high dimensional data.\n\u27a2 Used MLlib, Spark's Machine learning library to build and evaluate different models.\n\u27a2 Implemented rule based expertise system from the results of exploratory analysis and information gathered from the people from different departments.\n\u27a2 Created and designed reports that will use gathered metrics to infer and draw logical conclusions of past and future behavior.\n\u27a2 Developed MapReduce pipeline for feature extraction using Hive and Pig.\n\u27a2 Created Data Quality Scripts using SQL and Hive to validate successful data load and quality of the data. Created various types of data visualizations using Python and Tableau.\n\u27a2 Communicated the results with operations team for taking best decisions.\n\u27a2 Collected data needs and requirements by Interacting with the other departments.\n\nEnvironment: Python 2.x, CDH5, HDFS, Hadoop 2.3, Hive, Impala, AWS, Linux, Spark, Tableau Desktop, SQL Server 2014, Microsoft Excel, Matlab, Spark SQL, Pyspark."", u""Data Scientist\nWalgreens - Deerfield, IL\nDecember 2014 to September 2015\nDescription: The Walgreens Companyis an American company that operatesas the second-largest pharmacy store chain in the United States. It specializes in filling prescriptions, health and wellness products, health information, and photo services\n\nResponsibilities:\n\u27a2 Involved in Design, Development and Support phases of Software Development Life Cycle (SDLC)\n\u27a2 Performed data ETL by collecting, exporting, merging and massaging data from multiple sources and platforms including SSIS (SQL Server Integration Services) in SQL Server.\n\u27a2 Worked with cross-functional teams (including data engineer team) to extract data and rapidly execute from MongoDB through MongDB connector for Hadoop.\n\u27a2 Create Hive Internal tables with appropriate partitioning and bucketing and write complex queries for data Analysis. Configured Oozie work flows to automate data flow.\n\u27a2 Performed data cleaning and feature selection using MLlib package in PySpark.\n\u27a2 Performed partitional clustering into 100 by k-means clustering using Scikit-learn package in Python where similar hotels for a search are grouped together.\n\u27a2 Used Python to perform ANOVA test to analyze the differences among hotel clusters.\n\u27a2 Implemented application of various machine learning algorithms and statistical modeling like Decision Tree, Text Analytics, Sentiment Analysis, Naive Bayes, Logistic Regression and Linear Regression using Python to determine the accuracy rate of each model.\n\u27a2 Determined the most accurately prediction model based on the accuracy rate.\n\u27a2 Used text-mining process of reviews to determine customers' concentrations.\n\u27a2 Delivered analysis support to hotel recommendation and providing an online A/B test.\n\u27a2 Designed Tableau Bar Graphs, Scatter Plots, and Geographical maps to create detailed level summary reports and dashboards.\n\u27a2 Developed hybrid model to improve the accuracy rate.\n\u27a2 Delivered the results to operation team for better decisions and feedbacks.\n\nEnvironment: Python, SAS, Tableau, MongoDB, Hadoop, SQL Server, SDLC, ETL, SSIS, Recommendation Systems, Machine Learning Algorithms, Text-mining Process, A/B test"", u""Data analyst\nTransamerica - Plano, TX\nNovember 2013 to November 2014\nDescription: Transamerica is the US-based brand of Aegon, a Dutch financial services firm. Aegon is one of the world's leading providers of life insurance, pensions and asset management and is helping approximately 30 million customers globally to achieve a lifetime of financial security.\n\nResponsibilities:\n\u27a2 Worked with BI team in gathering the report requirements and also Sqoop to export data into HDFS and Hive\n\u27a2 Involved in the below phases of Analytics using R, Python and Jupyter notebook.\n\u27a2 a. Data collection and treatment: Analysed existing internal data and external data, worked on entry errors, classification errors and defined criteria for missing values\nb. Data Mining: Used cluster analysis for identifying customer segments, Decision trees used for profitable and non-profitable customers, Market Basket Analysis used for customer purchasing behaviour and part/product association.\n\u27a2 Developed multiple Map Reduce jobs in Java for data cleaning and preprocessing.\n\u27a2 Assisted with data capacity planning and node forecasting.\n\u27a2 Installed, Configured and managed Flume Infrastructure.\n\u27a2 Administrator for Pig, Hive and HBase installing updates patches and upgrades.\n\u27a2 Worked closely with the claims processing team to obtain patterns in filing of fraudulent claims.\n\u27a2 Worked on performing major upgrade of cluster from CDH3u6 to CDH4.4.0\n\u27a2 Developed Map Reduce programs to extract and transform the data sets and results were exported back to RDBMS using Sqoop.\n\u27a2 Patterns were observed in fraudulent claims using text mining in R and Hive.\n\u27a2 Exported the data required information to RDBMS using Sqoop to make the data available for the claims processing team to assist in processing a claim based on the data.\n\u27a2 Developed Map Reduce programs to parse the raw data, populate staging tables and store the refined data in partitioned tables in the EDW.\n\u27a2 Created tables in Hive and loaded the structured (resulted from Map Reduce jobs) data\n\u27a2 Using HiveQL developed many queries and extracted the required information.\n\u27a2 Created Hive queries that helped market analysts spot emerging trends by comparing fresh data with EDW reference tables and historical metrics.\n\u27a2 Was responsible for importing the data (mostly log files) from various sources into HDFS using Flume\n\u27a2 Enabled speedy reviews and first mover advantages by using Oozie to automate data loading into the Hadoop Distributed File System and PIG to pre-process the data.\n\u27a2 Provided design recommendations and thought leadership to sponsors/stakeholders that improved review processes and resolved technical problems.\n\u27a2 Managed and reviewed Hadoop log files.\n\u27a2 Tested raw data and executed performance scripts.\n\nEnvironment: HDFS, PIG, HIVE, Map Reduce, Linux, HBase, Flume, Sqoop, R, VMware, Eclipse, Cloudera and Python."", u'Data analyst\nHitachi Consulting - Pune, Maharashtra\nFebruary 2011 to October 2013\nDescription: Hitachi corporate statement, ""Inspire the Next"" expresses Hitachi\'s determination to breathe new life into the next era. With its social innovation businesses, Hitachi strives to become the ""Best Solutions Partner"" and help create a comfortable and abundant society.\n\nResponsibilities:\n\u27a2 Collaborating with business and technology teams.\n\u27a2 Data Analysis-Data collection, data transformation and data loading the data using different ETL systems like SSIS and Informatica.\n\u27a2 Performed source to target mapping as part of data migration from JD Edwards system to Agile PDM system.\n\u27a2 Data Migration testing and implementation activities using SSIS and SSRS tools of Microsoft SQL Server 2008.\n\u27a2 Responsible for accuracy of the data collected and stored in the corporate support system.\n\u27a2 Performed data review, evaluate, design, implement and maintain company database.\n\u27a2 Involved in construction of data flow diagrams and documentation of the processes.\n\u27a2 Interacted with end users for requirements study and analysis by JAD (Joint Application Development).\n\u27a2 Performed gap analysis between the present data warehouse to the future data warehouse being developed and identified data gaps and data quality issues and suggested potential solutions.\n\u27a2 Participated in system and use case modeling like activity and use case diagrams.\n\u27a2 Analyzed user requirements & worked with data modelers to identify entities and relationship for data modeling.\n\u27a2 Actively participated in the design of data model like conceptual, logical models using Erwin. Used Exception handling application block for checking errors/exceptions across the website.\n\u27a2 Developed Report Component, so that it retrieves the data by executing Stored Procedures throw Data Access component.\n\nEnvironment: Windows, Oracle, MS Excel, SSIS, Informatica, GAP Analysis, ERWIN', u'Associate Software Engineer\nEchidna Software pvt ltd - Bengaluru, Karnataka\nJuly 2009 to January 2011\nDescription: Echidna began when a small group of ecommerce leaders knew there had to be a better way to do ecommerce. So they branched off and created a new kind of agency one that combines amazing UX, enterprise-level technology implementation, and value-added marketing and analytics services.\n\nResponsibilities:\n\u27a2 Assisted in development and testing of various interior installations and instrumentation.\n\u27a2 Documented the technical specification for the reports and tested the generated reports.\n\u27a2 Gathered user requirements and created the business requirements documents.\n\u27a2 Used the technical document to design tables.\n\u27a2 Prepared user manual and technical support manuals.\n\u27a2 Prepared test plans for various modules.\n\u27a2 Created and managed Databases.\n\u27a2 Optimized the SQL queries for improved performance.\n\u27a2 Created Database triggers to maintain the audit data in the tables.\n\nEnvironment: Oracle 9i, SQL* Loader, PL/SQL, SQL.']","[u""Bachelor's""]",[u''],"degree_1 : ""Bachelors"""
0,https://resumes.indeed.com/resume/4270478d53117554,"[u'Data Scientist Immersive\nGeneral Assembly - Boston, MA\nApril 2017 to Present\n\u2022 Completed data science projects to established metrics, performed lab work applying data modeling to data sets of a wide variety in a predominately Python environment. Worked independently and managed\nprojects across the team.\n\u2022 Programming included using many libraries such as Numpy, Pandas, Matplotlib, Seaborn, SciPy, and Scikit-learn.\n\u2022 Became familiar with Scrum methodology in software development across teams.', u""Technical Writer\nIndependent & Freelance - Watertown, MA\nAugust 2013 to Present\n\u2022 Write content and design documents that serve as collateral documents, video scripts, instruction sets, customer resources, blog articles and create business correspondence for area small to medium businesses.\n\u2022 Draft and design technical publications, primarily for analytics startups.\n\u2022 Demonstrate consistent autonomy, management of deadlines, technical mastery and knowledge of the marketplace's current drivers."", u'Data Integrity Lead\nVisible Technologies - Burlington, MA\nJune 2012 to July 2013\n\u2022 Worked in the socialytics/enterprise social intelligence area of market research services, verifying integrity of data gathered from traditional online and social media sources for a major PC manufacturer and services client as well as competitors.\n\u2022 Developed expertise of technology marketplace for enterprise clients in the PC production and services industries. Used proprietary web-based applications to push and pull online media documents, articles and posts that were tagged with search queries and terms of interest for real-time business analysis.', u""Field Technology Specialist\nFedEx\nNovember 2003 to May 2011\n\u2022 Managed all technology assets for 18 busy downtown Boston/Cambridge locations as part of FedEx Office's diverse field technology division. Installed, configured, maintained, and troubleshot the technology resources, networked devices, and peripherals.""]","[u'Bachelor of Science in Engineering Technology', u'Associate of Applied Science in Electronic Engineering Technology', u'Certificate in Technical Communications']","[u'Wentworth Institute of Technology', u'Wentworth Institute of Technology', u'Wentworth Institute of Technology']","degree_1 : Bachelor of Science in Engineering Technology, degree_2 :  Associate of Applied Science in Electronic Engineering Technology, degree_3 :  Certificate in Technical Commnications"
0,https://resumes.indeed.com/resume/5b5122913525c578,"[u'Data Scientist\ntheDifference Consulting - Minneapolis, MN\nJanuary 2017 to Present\n\xb7 Optimized productivity with United Health Group, The World Bank, The Kauffman Foundation\no UHG\n\uf0a7 Text Classification using Python (Multinomial Na\xefve Bayes);\n\uf0a7 Pandas, Seaborn, NetworkX, iGraph, D3.js, Plotly, Bokeh for wrangling and visualizations \u2013 Created groupings/event design/social engineering to optimize collaboration for corporate sponsored events.\n\uf0a7 HQL (HiveQL) to query distributed database clusters\n\uf0a7 Spark through Jupyter Notebook service\no The World Bank\n\uf0a7 Text Classification\n\uf0a7 Twitter API mining to find network connections and extended networks for node attributes/insights\n\uf0a7 created a grouping algorithm that leveraged network metrics to foster investment in 3rd world economies.\no The Kauffman Foundation\n\uf0a7 3-day event in foster/identifying opportunities for entrepreneurship\n\uf0a7 created a live Twitter streamer which extracted key-words from the entire API specifically related to the event and converged them as a network representation. This allowed us to see emerging topics of importance that were key insights in determining the event\u2019s success.\n\uf0a7 Using twitter-based keywords, created predictive models to identify key attributes in future contributors to the foundation goals\n\xb7 Network Analysis by mining various APIs such as twitter, Crunchbase, LinkedIn, Crystal Knows, Instagram, Pintrest, Zillow\no Used mined data to leverage network analysis metrics\n\uf0a7 Twitter feed classifications helped identify peoples\u2019 areas of interests, LinkedIn webscraping allowed us to identify skills of individuals, Crystal Knows\u2019 email scanning and LinkedIn summary scanning API allowed us to identify personality types, created image recognition (CNN) scripts for Instagram/Pintrest feeds, Zillow API allowed us to approximate wealth\no All this data put together allowed us to create nodal attributes for individuals in a network. Attaching weights to these variables, we could create predictive models for certain outcome\no Created specialized metrics to analyze the network to identify emerging cultures, communities, POI, corporate silos, cultural choke points, and cultural resistors\n\xb7 Text Classification systems using Convolutional Neural Networks and Multinomial Na\xefve Bayes methods\no Our software platform for grouping/sorting of teams and corporate structures is located at Collaboration.Ai\n\uf0a7 I am responsible for deploying the Community Detection algorithm which uses Network data to identify, mathematically, distinct communities that exist in a network.\n\uf0a7 I am also responsible for creating the text classification system that replaced our dependency on Google API/Monkey Learn/Ingenia', u'Data Science Consultant\nUdacity - Mountain View, CA\nMay 2015 to Present\n\xb7 Have performed over 6.000 project reviews, helping students develop Job-Ready project portfolios and meet their career development goals\n\xb7 Tutored students through several Data Science Foundations\no Programming Foundations\n\uf0a7 Data Structures\n\uf0a7 Algorithms\n\uf0a7 Functional Programming\n\uf0a7 Object Oriented Programming\n\uf0a7 Mathematical Foundations \u2013 Linear Algebra, Calculus\n\uf0a7 Regular Expressions\no Inferential/Exploratory Statistics\no Python Programming Foundations \u2013 Pandas, Sklearn, MLib (Spark), tensorflow, Seaborn, Bokeh\no R Programming Foundations \u2013 stringr, ggplot2, knitr, googlevis, car\no Machine Learning Foundations\n\uf0a7 Linear Regression \u2013 Housing prices\n\uf0a7 Logistic Regression \u2013 Hotel Review Sentiment\n\uf0a7 Clustering Analysis \u2013 Wikipedia Article Sorting\n\uf0a7 Artificial Neural Nets \u2013 Image Recognition (CNN); Voice Recognition (RNN)\no Big Data\n\uf0a7 SQL\n\uf0a7 Scala\n\uf0a7 Hadoop (Hive, Pig, Spark)\n\uf0a7 Basic Cluster Creation on AWS', u'Data Science Teaching Assistant\nJohns Hopkins University\nFebruary 2014 to Present\n\xb7 Created a text-completion app for Swiftkey using mined Twitter data\nhttp://rpubs.com/theDurphy/wordpredpres\n\xb7 Mining of Open Street Map database, conversion into MongoDB, and analysis\nhttps://github.com/thedurphy/Project2.OSM/raw/master/project.pdf\n\xb7 Analysis of severe weather damages from 1950-2011 classifying source and the subsequent damage to property and people.\nhttp://rpubs.com/theDurphy/RepDataPA2\n\xb7 Exploration of Wine Quality with consideration of its Physiochemical Properties\nhttp://rpubs.com/theDurphy/vihno', u'Data Science Community Manager\nSpringboard - San Francisco, CA\nOctober 2016 to April 2017\n\u2022 Facilitate coding boot camps, Kaggle Competitions\n\u2022 Daily consultations on career development, project ideas\n\u2022 Curriculum revisions and additions concerning established practices']","[u'Certification in Deep Learning', u'Certification in Data Science', u'Certification in Data Analyst', u'Certification in Machine Learning']","[u'deeplearning.ai\nAugust 2017 to December 2017', u'Johns Hopkins University\nFebruary 2015', u'Udacity', u'Udacity']","degree_1 : Certification in Deep Learning, degree_2 :  Certification in Data Science, degree_3 :  Certification in Data Analyst, degree_4 :  Certification in Machine Learning"
0,https://resumes.indeed.com/resume/e632fd659b0d052e,"[u""Data Scientist\nnone - Massachusetts\nFebruary 2018 to Present\nSoftware Engineer Intern\n\u2022 Worked as an Intern in BHEL( Bharath Heavy Electricals Limited), where I got to learn the codes used for automation of machines used in manufacturing of Heavy engineering machines.\nPROJECT:\n\u2022 Worked on a project of Predicting death analysis in Titanic using R- programming language as a part of my Masters program.\n\u2022 Worked on a project to find out the analysis between experience and salary of an employee as a part of my master's program.\n\u2022 Worked on a algorithm to analyze the reviews of a restaurant using natural language processing.\n\u2022 Worked on a project to choose the best advertisement from a available list using Machine Learning.\n\u2022 Worked on the code used in developing Self-driving cars using Artificial Inteligence.\nEXTRA-CIRICULAR ACTIVITIES:\n\u2022 Participated in organizing an event of POLARIS drive during the Cultural fest at Under-graduation.""]","[u""master's in Computer Science"", u""Bachelor's in Mechanical Engineering""]","[u'Fitchburg State University', u'GITAM University']","degree_1 : ""masters in Compter Science"", degree_2 :  ""Bachelors in Mechanical Engineering"""
0,https://resumes.indeed.com/resume/12667bbd80c4666d,"[u'Product Development Intern\nADP, LLC - Roseland, NJ\nJune 2017 to August 2017\nDeveloped master client portal for internal management of clients across the three primary proprietary\ndivisions: Payroll, Insurance, and Retirement Services\n\u2022 Communicated shared front-end and back-end responsibilities and productivity to Agile scrum team\n\u2022 Integrated JavaScript(React) and NodeJS(Express) technologies to connect database with seamless UI', u'Data Scientist - Labor Economist Intern\niCIMS, Inc - Matawan, NJ\nJanuary 2017 to May 2017\n\u2022 Analyzed proprietary data to yield fresh insights on emerging trends in US labor market\n\u2022 Assisted publication of quarterly reports on hiring trends and semi-annual reports on ad hoc labor topics\n\u2022 Supported development of software that evaluate client performance relative to industry benchmarks\n\u2022 Visualized data to print and broadcast media, industry conferences, and clients in collaboration with data\nscientists, software developers, and marketing executives', u'Data Analyst Intern\nUrban Science - Woodcliff Lake, NJ\nMay 2016 to January 2017\n\u2022 Generated automated quality assurance solutions for monthly data processing, reducing runtime by 95%\n\u2022 Collaborated with product developers to design and maintain databases for proprietary software\n\u2022 Performed market studies and ad hoc analyses for dealer network optimization via statistical analysis\n\u2022 Validated and screened large, raw data sets to verify accuracy and presentation of final products']","[u""Master's of Science in Computer Science"", u'Bachelor of Science in Statistics', u'CS']","[u'Carnegie Mellon University, School of Computer Science Pittsburgh, PA\nSeptember 2018', u'Rutgers University, School of Arts and Sciences Honors Program New Brunswick, NJ\nMay 2018', u'Rutgers University']","degree_1 : ""Masters of Science in Compter Science"", degree_2 :  Bachelor of Science in Statistics, degree_3 :  CS"
0,https://resumes.indeed.com/resume/e524d2497f2fb755,"[u'Data Scientist\nAmerican Express - Little Rock, AR\nJuly 2017 to Present\nResponsibilities:\nInteraction with Business Analyst, SMEs, and other Data Architects to understand Business needs and functionality for various project solutions, Researched, evaluated, architected, and deployed new tools, frameworks, and patterns to build sustainable Big Data platforms for the clients\n\nParticipated in all phases of data mining, data collection, data cleaning, developing models, validation, visualization, and performed Gap analysis, Data Manipulation and Aggregation from a different source using Nexus, Toad, Business Objects, Power BI and Smart View, Worked as Data Architects and IT Architects to understand the movement of data and its storage and ER Studio 9.7, Data transformation from various resources, data organization, features extraction from raw and stored.\n\nSetup storage and data analysis tools in Amazon Web Services cloud computing infrastructure, Used pandas, numpy, Seaborn, scipy, matplotlib, sci-kit-learn, NLTK in Python for developing various machine learning algorithms, Installed and used Caffe Deep Learning Framework, Implemented Classification using supervised algorithms like Logistic Regression, Decision trees, KNN, Naive Bayes.\n\nWorked on different data formats such as JSON, XML and performed machine learning algorithms in Python, Implemented Agile Methodology for building an internal application, Designed both 3NF data\nmodels for ODS, OLTP systems and dimensional data models using Star and Snowflake Schemas, Focus on integration overlap and Informatica newer commitment to MDM with the acquisition of Identity Systems.\n\nAs Architect delivered various complex OLAP databases/cubes, scorecards, dashboards and reports, Handled importing data from various data sources, performed transformations using Hive, Map Reduce, and loaded data into HDFS.\n\nProgrammed a utility in Python that used multiple packages (scipy, numpy, & pandas), Updated Python scripts to match training data with our database stored in AWS Cloud Search, so that we would be able to assign each document a response label for further classification.\n\nIdentifying and executing process improvements, hands-on in various technologies such as Oracle, Informatica, and Business Objects.\n\nEnvironment: R 9.0, Informatica 9.0, ODS, OLTP, Oracle 10g, Hive, OLAP, DB2, Metadata, MS Excel, Mainframes MS Vision, Rational Rose.', u""Data Scientist\nWalmart - Bentonville, AR\nApril 2016 to June 2017\nResponsibilities:\nAnalyzed the business requirements of the project by studying the Business Requirement Specification document.\n\nPerformed Exploratory Data Analysis and Data Visualizations using R, and Tableau, Extensively worked on Data Modeling tools Erwin Data Modeler to design the data models, Worked with Data Governance, Data quality, data lineage, Data architect to design various models and processes, Developed, Implemented & Maintained the Conceptual, Logical & Physical Data Models using Erwin for forwarding/Reverse Engineered Databases.\n\nDesigned tables and implemented the naming conventions for Logical and Physical Data Models in Erwin 7.0, Explored and Extracted data from source XML in HDFS, preparing data for exploratory analysis using data munging, Designed data models and data flow diagrams using Erwin and MS Visio.\n\nPerform a proper EDA, Uni-variate and bi-variate analysis to understand the intrinsic effect/combined effects, Independently coded new programs and designed Tables to load and test the program effectively for the given POC's using with Big Data/Hadoop, Designed a mapping to process the incremental changes that exist in the source table. Whenever source data elements were missing in source tables, these were modified/added inconsistency with third normal form based OLTP source database.\n\nCreated indexes, made query optimizations. Wrote stored procedures, triggers utilizing T-SQL, Explained the data model to the other members of the development team. Wrote XML parsing module that populates alerts from the XML file into the database tables utilizing JAVA, JDBC, BEA WEBLOGIC IDE,\nAnd Document Object Model, As an Architect implemented MDM hub to provide clean, consistent data for an SOA implementation.\n\nEnvironment: SQL Server 2008R2/2005 Enterprise, SSRS, SSIS, Crystal Reports, Hadoop, Windows Enterprise Server 2000, DTS, SQL Profiler, and Query Analyzer."", u""Data Scientist\nLSB Industries Inc - Oklahoma City, OK\nDecember 2014 to March 2016\nResponsibilities:\nGathering all the data that is required from multiple data sources and creating datasets that will be used in the analysis.\n\nImplemented end-to-end systems for Data Analytics, Data Automation and integrated with custom visualization tools using R, Mahout, Hadoop, and MongoDB, Performed Exploratory Data Analysis and Data Visualizations using R, and Tableau, Coded R functions to interface with Caffe Deep Learning Framework, Worked with several R packages including knitr, dplyr, SparkR, CausalInfer, space-time, Developed, Implemented & Maintained the Conceptual, Logical & Physical Data Models using Erwin for forwarding/Reverse Engineered Databases.\n\nWorked with Data Governance, Data quality, data lineage, Data architect to design various models and processes, Performed data cleaning and imputation of missing values using R, Used Hive to store the data and perform data cleaning steps for huge datasets.\n\nIndependently coded new programs and designed Tables to load and test the program effectively for the given POC's using with Big Data/Hadoop, Worked with Hadoop eco system covering HDFS, HBase, YARN, and MapReduce, Designed data models and data flow diagrams using Erwin and MSVisio, Created dash boards and visualization on regular basis using ggplot2 and Tableau, Established Data architecture strategy, best practices, standards, and roadmaps\n\nPerform a proper EDA, Uni-variate and bi-variate analysis to understand the intrinsic effect/combined effects, Working in Amazon Web Services cloud computing environment, As an Architect implemented MDM hub to provide clean, consistent data for an SOA implementation, Worked with BTEQ to submit SQL statements, import and export data, and generate reports in Teradata.\n\nLead the development and presentation of a data analytics data-hub prototype with the help of the other members of the emerging solutions team\n\nInteracted with the other departments to understand and identify data needs and requirements and work with other members of the IT organization to deliver data visualization and reporting solutions to address those needs.\n\nEnvironment: Erwin r, Informatica, ODS, OLTP, Oracle 10g, Hive, OLAP, DB2, Metadata, MS Excel, Mainframes MS Visio, Rational Rose, and Requisite Pro, Hadoop, PL/SQL, etc."", u'Data Analyst/Data Modeler\nAccenture - Bengaluru, Karnataka\nApril 2009 to October 2013\nResponsibilities:\nDeveloped Internet traffic scoring platform for ad networks, advertisers, and publishers (rule engine, site scoring, keyword scoring, lift measurement, linkage analysis), Implementation of Metadata Repository, Maintaining Data Quality, Data Cleanup procedures, Transformations, Data Standards, Data Governance program, Scripts, Stored Procedures, triggers and execution of test plans.\n\nDeveloped new hybrid statistical and data mining technique known as hidden decision trees and hidden forests, Designed the architecture for one of the first analytics 3.0. Online platforms: all-purpose scoring, with on-demand, SaaS, API services. Currently under implementation, Web crawling and text mining techniques to score referral domains, generate keyword taxonomies, and assess commercial value of bid keywords.\n\nReverse engineering of keyword pricing algorithms in the context of pay-per-click arbitrage, coordinated meetings with vendors to define requirements and system interaction agreement documentation between client and vendor system, Creation of multimillion bid keyword lists using extensive web crawling. Identification of metrics to measure the quality of each list (yield or coverage, volume, and keyword average financial value).\n\nAutomated bidding for advertiser campaigns based either on keyword or category (run-of-site) bidding, Responsible for defining the key identifiers for each mapping/interface, Enterprise Metadata Library with any changes or updates, Document data quality and traceability documents for each source interface, Establish standards of procedures, Generate weekly and monthly asset inventory reports.\n\nEnvironment: Erwin r, SQL Server 2000/2005, Windows XP/NT/2000, Oracle, MS-DTS, UML, UAT, SQL Loader, OOD, OLTP, PL/SQL, MS Visio, Informatica.']","[u""Bachelor's""]",[u''],"degree_1 : ""Bachelors"""
0,https://resumes.indeed.com/resume/f09d67805e7d9fc3,"[u'Data Scientist\nMonJa - Oakland, CA\nJune 2017 to January 2018\n(Data Transformation, Data Mining & Data Modeling) Build, extend and maintain MonJa\u2019s data stores for market data and propose new data structures for data storage and code that will lead to more effective and efficient use of market data by MonJa\u2019s product software and modeling toolkit\n(Statistical Predictive Modeling) Research, experiment, and build a model to score each loan in auctions based on the borrower\u2019s financial background using machine learning methodologies and previous loan data of Marketplace Lending platforms so that our investors can maximize the rate of return.\n(Report Generation & Analysis) Prepare slice-and-dice reports of modeled data to support senior research members\u2019 deep analysis of Marketplace Lending borrower credit behavior, loan delinquency, default, and investor expected return', u'Database Administrator Intern\nCommonwealth FX - San Francisco, CA\nApril 2017 to July 2017\nBuilt, cleaned, and managed databases so that each client\u2019s data is clean and organized for analyzing and future references.\nAnalyzed clients\u2019 data to identify patterns and behaviors to help the marketing team with the email campaigns']","[u'MBA in Information Systems', u'Bachelor of Arts in Business Economics']","[u'San Francisco State University San Francisco, CA\nJanuary 2016 to December 2017', u'University of California Los Angeles, CA\nSeptember 2012 to December 2015']","degree_1 : MBA in Information Systems, degree_2 :  Bachelor of Arts in Bsiness Economics"
0,https://resumes.indeed.com/resume/56eb145488cd73cc,"[u""Data Scientist\nMorgan Stanley, TimesSquare, NY\nFebruary 2017 to Present\nDescription: Morgan Stanleyis a financial holding company. The Company is engaged in global financial services. The Company, through its subsidiaries and affiliates, advises, and originates, trades, manages and distributes capital for governments, institutions and individuals. The Company's segments include Institutional Securities, Wealth Management and Investment Management. Through its subsidiaries and affiliates, the Company provides a range of products and services to a group of clients and customers, including corporations, governments, financial institutions and individuals.\n\nResponsibilities:\n\u2022 Long Short-Term Memory Recurrent Neural Networks (LSTM RNNs) learnt using Deep Learning techniques applied to Problem X.\n\u2022 LSTM RNNs applied to Problem Y.\n\u2022 Improving Fraud Detection using Digital Links at Amazon, Seattle.\n\u2022 Scaled upto Machine Learning pipelines: 4600 processors, 35000 GB memory achieving 5-minute execution.\n\u2022 Deployed GUI pages by using JSP, JSTL, HTML, DHTML, XHTML, CSS, JavaScript, AJAX.\n\u2022 Configured the project on WebSphere 6.1 application servers\n\n\u2022 Designed a new Machine Learning pipeline to replace existing prod: AUC perf. increase from 83% to 90%.\n\u2022 Handled 2+ TB data with graphs upto130 GB (50M nodes, 100M edges) using single-node in-disk scaling.\n\u2022 Developed a Machine Learning test-bed with 24 different model learning and feature learning algorithms.\n\u2022 By thorough systematic search, demonstrated performance surpassing the state-of-the-art (deep learning).\n\n\u2022 Upto 10 times more accurate predictions over existing state-of-the-art algorithms.\n\u2022 Developed in-disk, huge (100GB+), highly complex Machine Learning models.\n\u2022 Used SAX and DOM parsers to parse the raw XML documents\n\u2022 Used RAD as Development IDE for web applications.\n\n\u2022 Demonstrated performances comparable to other state-of-the-art deep learning models.\n\u2022 Applied Machine Learning algorithms to diagnose blood loss from vital signs (ECG, HF, GSR, etc.)\n\u2022 Devised and implemented a Vehicle Speed Detector using low-power LEDs and field-tested for robustness.\n\u2022 National Highways Authority (Govt. of India) is evaluating the design for installations across the country.\n\u2022 IIT Madras has installed the speed detectors across the institute for permanent speed limit enforcement.\n\u2022 Developed & tested feature tracking algorithms for Intelligent Transportation Systems Computer Vision.\n\u2022 Analyzed SIFT feature descriptors and their resilience to changes in illumination.\n\u2022 Devised a novel machine learning algorithm for classification of ECG abnormalities.\n\nEnvironment: R 9.0, Informatic a 9.0, ODS, OLTP, Oracle 10g, Hive, OLAP, DB2, Metadata,MS Excel, Mainframes MS Vision, Rational Rose."", u""Data Scientist\nState Street Bank - Boston, MA\nDecember 2015 to February 2017\nDescription:State Street Corporation, is an American worldwide financial services company. State Street was founded in 1792 and is the second oldest financial institution in the United States of America. It is one of the largest asset management companies in the world with $2.45 trillion (USD) under management and $28 trillion (USD) under custody and administration, which represents 11% of the world's total financial assets. State Street is a Fortune 500 company with headquarters at One Lincoln Street in Boston and has offices in 30 countries around the world.\n\nResponsibilities:\n\u2022 Manipulating, cleansing & processing data using Excel, Access and SQL.\n\u2022 Responsible for loading, extracting and validation of client data.\n\u2022 Liaising with end-users and 3rd party suppliers.\n\u2022 Analyzing raw data, drawing conclusions & developing recommendations\n\u2022 Writing T-SQL scripts to manipulate data for data loads and extracts.\n\u2022 Developing data analytical databases from complex financial source data.\n\u2022 Performing daily system checks.\n\u2022 Data entry, data auditing, creating data reports & monitoring all data for accuracy.\n\u2022 Designing, developing and implementing new functionality.\n\u2022 Monitoring the automated loading processes.\n\u2022 Advising on the suitability of methodologies and suggesting improvements.\n\u2022 Carrying out specified data processing and statistical techniques.\n\u2022 Supplying qualitative and quantitative data to colleagues & clients.\n\u2022 Using Informatica& SAS to extract, transform & load source data from transaction\nsystems.\n\u2022 Creating data pipelines using big data technologies like Hadoop, spark etc.\n\u2022 Creating statistical models using distributed and standalone models to build various diagnostics, predictive and prescriptive solution.\n\u2022 Utilize a broad variety of statistical packages like SAS, R , MLIB, Graphs,Hadoop, Spark , MapReduce, Pig and others\n\u2022 Refine and train models based on domain knowledge and customer business objectives\n\u2022 Deliver or collaborate on delivering effective visualizations to support the client business objectives\n\u2022 Communicate to your peers and managers promptly as and when required.\n\u2022 Produce solid and effective strategies based on accurate and meaningful data reports and analysis and/or keen observations.\n\u2022 Establish and maintain communication with clients and/or team members; understand needs, resolve issues, and meet expectations\n\u2022 Developed web applications using .net technologies; work on bug fixes/issues that arise in the production environment and resolve them at the earliest\nEnvironment: SQL Server 2008R2/2005 Enterprise, SSRS, SSIS, Crystal Reports, Hadoop, Windows Enterprise Server 2000, DTS, SQL Profiler, and Query Analyzer."", u""Data Scientist\neBay Inc - San Jose, CA\nApril 2014 to November 2015\nDescription:eBayis a multinational e-commerce corporation, facilitating online consumer-to consumer and business-to-consumer sales. It is headquartered in San Jose, California. eBay was founded by Pierre Omidyar in 1995, and became a notable success story of the dot-com bubble. Today it is a multibillion-dollar business with operations in about 30 countries.\nResponsibilities:\n\u2022 Data mining using state-of-the-art methods\n\u2022 Extending company's data with third party sources of information when needed\n\u2022 Enhancing data collection procedures to include information that is relevant for building analytic systems\n\u2022 Processing, cleansing, and verifying the integrity of data used for analysis\n\u2022 Doing ad-hoc analysis and presenting results in a clear manner\n\u2022 Creating automated anomaly detection systems and constant tracking of its performance\n\u2022 Strong command of data architecture and data modelling techniques.\n\u2022 Hands on experience with commercial data mining tools such as Splunk, R, Map reduced, Yarn, Pig,Hive, Floop, Oozie, Scala, HBase, Master HDFS, Sqoop, Spark, Scala (Machine learning tool) or similar software required depending on seniority level in job field.\n\u2022 Developed scalable machine learning solutions within a distributed computation framework (e.g. Hadoop, Spark, Storm etc.).\n\u2022 Utilizing NLP applications such as topic models and sentiment analysis to identify trends and patterns within massive data sets.\n\u2022 Knowledge in ML& Statistical libraries (e.g. Scikit-learn, Pandas).\n\u2022 Having knowledge to build predict models to forecast risks for product launches and operations and help predict workflow and capacity requirements for TRMS operations\n\u2022 Having experience with visualization technologies such as Tableau\n\u2022 Draw inferences and conclusions, and create dashboards and visualizations of processed data, identify trends, anomalies\n\u2022 Generation of TLFs and summary reports, etc. ensuring on-time quality delivery.\n\u2022 Participated in client meetings, teleconferences and video conferences to keep track of project requirements, commitments made and the delivery thereof.\n\u2022 Solved analytical problems, and effectively communicate methodologies and results\n\u2022 Worked closely with internal stakeholders such as business teams, product managers, engineering teams, and partner teams.\n\u2022 Created automated metrics using complex databases.\n\u2022 Foster culture of continuous engineering improvement through mentoring, feedback, and metrics.\nEnvironment: Erwin r9.0, Informatica 9.0, ODS, OLTP, Oracle 10g, Hive, OLAP, DB2, Metadata, MS Excel, Mainframes MS Visio, Rational Rose, Requisite Pro. Hadoop, PL/SQL, etc."", u""Data Scientist\nEagle Trading Systems - Princeton, NJ\nMay 2013 to March 2014\nDescription:Eagle Trading Systems Inc. is a financial investment advisory firm headquartered in Princeton, New Jersey. The firm manages 5 accounts totaling an estimated $481 Million of assets under management. Eagle Trading Systems Inc.'s 17 employees help advice 1-10 clients.\n\nResponsibilities:\n\u2022 Statistical Modelling with ML to bring Insights in Data under guidance of Principal Data Scientist\n\u2022 Data modeling with Pig, Hive, Impala.\n\u2022 Ingestion with Sqoop, Flume.\n\u2022 Used SVN to commit the Changes into the main EMM application trunk.\n\u2022 Understanding and implementation of text mining concepts, graph processing and semi structured and unstructured data processing.\n\u2022 Worked with Ajax API calls to communicate with Hadoop through Impala Connection and SQL to render the required data through it. TheseAPI calls are similar to Microsoft Cognitive API calls.\n\u2022 Good grip on Cloudera and HDP ecosystem components.\n\u2022 Used Elasticsearch (Big Data) to retrieve data into application as required.\n\u2022 Performed Map Reduce Programs those are running on the cluster.\n\u2022 Developed multiple MapReduce jobs in java for data cleaning and preprocessing.\n\u2022 Analyzed the partitioned and bucketed data and compute various metrics for reporting.\n\u2022 Involved in loading data from RDBMS and web logs into HDFS using Sqoop and Flume.\n\u2022 Worked on loading the data from MySQL to HBase where necessary using Sqoop.\n\u2022 Developed Hive queries for Analysis across different banners.\n\u2022 Extracted data from Twitter using Java and Twitter API. Parsed JSON formatted twitter data and uploaded to database.\n\u2022 Launching Amazon EC2 Cloud Instances using Amazon Images (Linux/ Ubuntu) and Configuring launched instances with respect to specific applications.\n\u2022 Exported the result set from Hive to MySQL using Sqoop after processing the data.\n\u2022 Analyzed the data by performing Hive queries and running Pig scripts to study customer behavior.\n\u2022 Have hands on experience working on Sequence files, AVRO, HAR file formats and compression.\n\u2022 Used Hive to partition and bucket data.\n\u2022 Experience in writing MapReduce programs with Java API to cleanse Structured and unstructured data.\n\u2022 Wrote Pig Scripts to perform ETL procedures on the data in HDFS.\n\u2022 Created HBase tables to store various data formats of data coming from different portfolios.\n\u2022 Worked on improving performance of existing Pig and Hive Queries.\n\nEnvironment: SQL/Server, Oracle 9i, MS-Office, Teradata, Informatica, ER Studio, XML, Business Objects, HDFS, Teradata 14.1, JSON, HADOOP (HDFS), MapReduce, PIG, Spark, R Studio, MAHOUT, JAVA, HIVE, AWS."", u""Data Architect/Data Modeler\nFirst Indian Corporation Private Limited - Hyderabad, Telangana\nFebruary 2011 to April 2013\nDescription:First Indian Corporation Private Limited, a member of The First American family of companies, provides specialized offshore transaction services, technology services and analytics to the mortgage industry. First Indian Corporation's primary focus is on the Title Insurance, Property Tax, Flood Certification, Default Management Services, Credit and Real Estate Information segments.\n\nResponsibilities:\n\u2022 Worked with large amounts of structured and unstructured data.\n\u2022 Knowledge in Machine Learning concepts (Generalized Linear models, Regularization, Random Forest, Time Series models, etc.)\n\u2022 Worked in Business Intelligence tools and visualization tools such as Business Objects, Tableau, ChartIO, etc.\n\u2022 Deployed GUI pages by using JSP, JSTL, HTML, DHTML, XHTML, CSS, JavaScript, AJAX.\n\u2022 Configured the project on WebSphere 6.1 application servers\n\u2022 Implemented the online application by using Core Java, Jdbc, JSP, Servlets and EJB 1.1, Web Services, SOAP, WSDL.\n\u2022 Handled end-to-end project from data discovery to model deployment.\n\u2022 Monitoring the automated loading processes.\n\u2022 Communicated with other Health Care info by using Web Services with the help of SOAP, WSDLJAX-RPC\n\u2022 Used Singleton, factory design pattern, DAO Design Patterns based on the application requirements\n\u2022 Used SAX and DOM parsers to parse the raw XML documents\n\u2022 Used RAD as Development IDE for web applications.\n\u2022 Preparing and executing Unit test cases\n\u2022 Used Log4J logging framework to write Log messages with various levels.\n\u2022 Involved in fixing bugs and minor enhancements for the front-end modules.\n\u2022 Implemented Microsoft Visio and Rational Rose for designing the Use Case Diagrams, Class model, Sequence diagrams, and Activity diagrams for SDLC process of the application\n\u2022 Doing functional and technical reviews\n\u2022 Maintenance in the testing team for System testing/Integration/UAT.\n\u2022 Guaranteeing quality in the deliverables.\n\u2022 Conducted Design reviews and Technical reviews with other project stakeholders.\n\u2022 Was a part of the complete life cycle of the project from the requirements to the production support\n\u2022 Created test plan documents for all back-end database modules\n\u2022 Implemented the project in Linux environment.\n\nEnvironment: R 3.0, Erwin 9.5, Tableau 8.0, MDM, QlikView, MLLib, PL/SQL, HDFS, Teradata 14.1, JSON, HADOOP (HDFS), MapReduce, PIG, Spark, R Studio, MAHOUT, JAVA, HIVE, AWS."", u""Data Analyst/Data Modeler\nDELTA Technologies & Managements Services - Hyderabad, Telangana\nAugust 2009 to January 2011\nDescription: Delta Technology's vision is to be an organization of value, respect and transparency for its people to continuously innovate, improve and deliver efficient and effective business solutions.\n\nResponsibilities:\n\u2022 Developed Internet traffic scoring platform for ad networks, advertisers and publishers (rule engine, site scoring, keyword scoring, lift measurement, linkage analysis).\n\u2022 Responsible for defining the key identifiers for each mapping/interface.\n\u2022 Clients include eBay, Click Forensics, Cars.com, Turn.com, Microsoft, and Looksmart.\n\u2022 Designed the architecture for one of the first analytics 3.0. online platforms: all-purpose scoring, with on-demand, SaaS, API services. Currently under implementation.\n\u2022 Web crawling and text mining techniques to score referral domains, generate keyword taxonomies, and assess commercial value of bid keywords.\n\u2022 Developed new hybrid statistical and data mining technique known as hidden decision trees and hidden forests.\n\u2022 Reverse engineering of keyword pricing algorithms in the context of pay-per-click arbitrage.\n\u2022 Implementation of Metadata Repository, Maintaining Data Quality, Data Cleanup procedures, Transformations, Data Standards, Data Governance program, Scripts, Stored Procedures, triggers and execution of test plans\n\u2022 Performed data quality in Talend Open Studio.\n\u2022 Coordinated meetings with vendors to define requirements and system interaction agreement documentation between client and vendor system.\n\u2022 Automated bidding for advertiser campaigns based either on keyword or category (run-of-site) bidding.\n\u2022 Creation of multimillion bid keyword lists using extensive web crawling. Identification of metrics to measure the quality of each list (yield or coverage, volume, and keyword average financial value).\n\u2022 Enterprise Metadata Library with any changes or updates.\n\u2022 Document data quality and traceability documents for each source interface.\n\u2022 Establish standards of procedures.\n\u2022 Generate weekly and monthly asset inventory reports.\n\nEnvironment: Erwin r7.0, SQL Server 2000/2005, Windows XP/NT/2000, Oracle 8i/9i, MS-DTS, UML, UAT, SQL Loader, OOD, OLTP, PL/SQL, MS Visio, Informatica.""]",[],[],degree_1 : 
0,https://resumes.indeed.com/resume/52a4b19262ccb452,"[u""Data Scientist\nFIS, FL 32204 USA\nJanuary 2016 to Present\nDescription:\nFIS provides financial software, world-class services and global business solutions. Let us help you compete and win in today's chaotic marketplace. Fidelity National Information Services Inc., better known by the abbreviation FIS, is an international provider of financial services technology and outsourcing services. FIS is the world's largest global provider dedicated to financial technology solutions. FIS empowers the financial world with software, services, consulting and outsourcing solutions focused on retail and institutional banking, payments, asset and wealth management, risk and compliance, trade enablement, transaction processing and record-keeping.\nResponsibilities:\n\n\u2022 As an Architect design conceptual, logical and physical models using Erwin and build datamarts using hybrid Inmon and Kimball DW methodologies.\n\u2022 Worked closely with business, datagovernance, SMEs and vendors to define data requirements.\n\u2022 Worked with data investigation, discovery and mapping tools to scan every single data record from many sources.\n\u2022 Designed the prototype of the Data mart and documented possible outcome from it for end-user.\n\u2022 Involved in business process modeling using UML\n\u2022 Developed and maintained data dictionary to create metadata reports for technical and business purpose.\n\u2022 Created SQLtables with referential integrity and developed queries using SQL, SQL*PLUS and PL/SQL\n\u2022 Experience in maintaining database architecture and metadata that support the Enterprise Datawarehouse.\n\u2022 Design, coding, unit testing of ETL package source marts and subject marts using Informatica ETL processes for Oracledatabase.\n\u2022 Developed various QlikView Data Models by extracting and using the data from various sources files, DB2, Excel, Flat Files and Bigdata.\n\u2022 Handled importing data from various data sources, performed transformations using Hive, Map Reduce, and loaded data into HDFS.\n\u2022 Interaction with Business Analyst, SMEs and other Data Architects to understand Business needs and functionality for various project solutions\n\u2022 Researched, evaluated, architected, and deployed new tools, frameworks, and patterns to built sustainable Big Data platforms for the clients\n\u2022 Identifying and executing process improvements, hands-on in various technologies such as Oracle, Informatica, BusinessObjects.\n\u2022 Designed both 3NF data models for ODS, OLTP systems and dimensionaldatamodels using Star and SnowflakeSchemas.\nEnvironment: r9.0, Informatica 9.0, ODS, OLTP, Oracle 10g, Hive, OLAP, DB2, Metadata, MS Excel, Mainframes MS Visio, Rational Rose, Requisite Pro. Hadoop, PL/SQL, etc."", u'Data Scientist\nCBRE - Dallas, TX\nJanuary 2015 to December 2015\nDescription: CBRE Group, Inc. is the largest commercial real estate services and investment firm in the world. It is based in Los Angeles, California and operates more than 450 offices worldwide and has clients in more than 100 countries.\nServices provided by the company include facilities management services to occupiers of commercial real estate as well as property management, leasing, capital markets, appraisal, and brokerage services to owners of commercial real estate.\n\nResponsibilities:\n\u2022 Worked as a Data Modeler/Analyst to generate Data Models using Erwin and developed relational database system.\n\u2022 Analyzed the business requirements of the project by studying the Business Requirement Specification document.\n\u2022 Extensively worked on DataModeling tools ErwinDataModeler to design the datamodels.\n\u2022 Designedmapping to process the incremental changes that exists in the source table. Whenever source data elements were missing in source tables, these were modified/added in consistency with third normal form based OLTP source database.\n\u2022 Designed tables and implemented the naming conventions for Logical and PhysicalData Models in Erwin 7.0.\n\u2022 Provide expertise and recommendations for physicaldatabasedesign, architecture, testing, performance tuning and implementation.\n\u2022 Designedlogical and physical data models for multiple OLTP and Analytic applications.\n\u2022 Extensively used the Erwin design tool &Erwin model manager to create and maintain the DataMart.\n\u2022 Designed the physical model for implementing the model into oracle9i physical data base.\n\u2022 Involved with DataAnalysis primarily Identifying DataSets, SourceData, Source Meta Data, Data Definitions and Data Formats\n\u2022 Performance tuning of the database, which includes indexes, and optimizing SQL statements, monitoring the server.\n\u2022 Wrote simple and advanced SQLqueries and scripts to create standard and adhoc reports for senior managers.\n\u2022 Collaborated the data mapping document from source to target and the data quality assessments for the source data.\n\u2022 Used Expert level understanding of different databases in combinations for Data extraction and loading, joiningdata extracted from different databases and loading to a specific database.\n\u2022 Co-ordinate with various business users, stakeholders and SME to get Functional expertise, design and business test scenarios review, UAT participation and validation of financial data.\n\u2022 Worked very close with Data Architects and DBA team to implement data model changes in database in all environments.\n\u2022 Created PL/SQL packages and DatabaseTriggers and developed user procedures and prepared user manuals for the new programs.\n\u2022 Performed performance improvement of the existing Data warehouse applications to increase efficiency of the existing system.\n\u2022 Designed and developed UseCase, Activity Diagrams, Sequence Diagrams, OOD (Object oriented Design) using UML and Visio.\n\nEnvironment: SQL Server 2008R2/2005 Enterprise, SSRS, SSIS, Crystal Reports, Windows Enterprise Server 2000, DTS, SQL Profiler, and Query Analyzer.', u""Data Scientist\nSouthwest Airlines - Dallas, TX\nMay 2013 to December 2014\nDescription:The mission of Southwest Airlines is dedication to the highest quality of customer service delivered with a sense of warmth, friendliness, individual pride, and company spirit.\nResponsibilities:\n\n\u2022 Coded R functions to interface with CaffeDeepLearning Framework\n\u2022 Working in AmazonWebServices cloud computing environment\n\u2022 Worked with several R packages including knitr, dplyr, SparkR, CausalInfer, spacetime.\n\u2022 Implemented end-to-end systems for DataAnalytics, DataAutomation and integrated with custom visualization tools using R, Mahout, Hadoop and MongoDB.\n\u2022 Gathering all the data that is required from multiple data sources and creating datasets that will be used in analysis.\n\u2022 Performed Exploratory DataAnalysis and DataVisualizations using R, andTableau.\n\u2022 Perform a proper EDA, Univariate and bi-variate analysis to understand the intrinsic effect/combined effects.\n\u2022 Worked with Data governance, Data quality, data lineage, Data architect to design various models and processes.\n\u2022 Independently coded new programs and designed Tables to load and test the program effectively for the given POC's using with Big Data/Hadoop.\n\u2022 Designed data models and data flow diagrams using Erwin and MSVisio.\n\u2022 As an Architect implemented MDM hub to provide clean, consistent data for a SOA implementation.\n\u2022 Developed, Implemented & Maintained the Conceptual, Logical&Physical Data Models using Erwin for Forward/ReverseEngineered Databases.\n\u2022 Established Data architecture strategy, best practices, standards, and roadmaps.\n\u2022 Lead the development and presentation of a dataanalytics data-hub prototype with the help of the other members of the emerging solutions team\n\u2022 Performed datacleaning and imputation of missing values using R.\n\u2022 Worked with Hadoop eco system covering HDFS, HBase, YARN and MapReduce\n\u2022 Take up ad-hoc requests based on different departments and locations\n\u2022 Used Hive to store the data and perform datacleaning steps for huge datasets.\n\u2022 Created dash boards and visualization on regular basis using ggplot2 and Tableau\n\u2022 Creating customized business reports and sharing insights to the management\n\u2022 Worked with BTEQ to submit SQL statements, import and export data, and generate reports in Teradata.\n\u2022 Interacted with the other departments to understand and identify dataneeds and requirements and work with other members of the ITorganization to deliver data visualization and reportingsolutions to address those needs.\nEnvironment: Erwin r9.0, Informatica 9.0, ODS, OLTP, Oracle 10g, Hive, OLAP, DB2, Metadata, MS Excel, Mainframes MS Visio, Rational Rose, Requisite Pro. Hadoop, PL/SQL, etc.."", u'Data Scientist\nCoventry Health Care - Minneapolis, MN\nAugust 2012 to April 2013\nDescription: Coventry Health Care Management utilizes multiple software systems to support the intake and processing of authorization requests, the exchange of data between the payer and vendors contracted to perform services on our behalf, manage Case and Disease programs, provide robust reporting and decision support, and generally automate and facilitate their business processes.\n\nResponsibilities:\n\u2022 Supported MapReduce Programs running on the cluster.\n\u2022 Evaluated business requirements and prepared detailed specifications that follow project guidelines required to develop written programs.\n\u2022 Configured Hadoop cluster with Namenode and slaves and formatted HDFS.\n\u2022 Used Oozie workflow engine to run multiple Hive and Pig jobs.\n\u2022 Performed Map Reduce Programs those are running on the cluster.\n\u2022 Developed multiple MapReduce jobs in java for data cleaning and preprocessing.\n\u2022 Analyzed the partitioned and bucketed data and compute various metrics for reporting.\n\u2022 Involved in loading data from RDBMS and web logs into HDFS using Sqoop and Flume.\n\u2022 Worked on loading the data from MySQL to HBase where necessary using Sqoop.\n\u2022 Developed Hive queries for Analysis across different banners.\n\u2022 Extracted data from Twitter using Java and Twitter API. Parsed JSON formatted twitter data and uploaded to database.\n\u2022 Launching Amazon EC2 Cloud Instances using Amazon Images (Linux/ Ubuntu) and Configuring launched instances with respect to specific applications.\n\u2022 Exported the result set from Hive to MySQL using Sqoop after processing the data.\n\u2022 Analyzed the data by performing Hive queries and running Pig scripts to study customer behavior.\n\u2022 Have hands on experience working on Sequence files, AVRO, HAR file formats and compression.\n\u2022 Used Hive to partition and bucket data.\n\u2022 Experience in writing MapReduce programs with Java API to cleanse Structured and unstructured data.\n\u2022 Wrote Pig Scripts to perform ETL procedures on the data in HDFS.\n\u2022 Created HBase tables to store various data formats of data coming from different portfolios.\n\u2022 Worked on improving performance of existing Pig and Hive Queries.\n\nEnvironment: SQL/Server, Oracle 9i, MS-Office, Teradata, Informatica, ER Studio, XML, Business Objects.', u'Data Architect/Data Modeler\nPeople Tech Group - IN\nJanuary 2011 to July 2012\nDescription: Founded in 2006, People Tech is an emerging leader in the Enterprise Applications and IT Services marketplace. People Tech draws its expertise from strategic partnerships with technology leaders like Microsoft, Oracle and SAP and combines that with the deep understanding of its employees.\n\nResponsibilities:\n\u2022 Deployed GUI pages by using JSP, JSTL, HTML, DHTML, XHTML, CSS, JavaScript, AJAX\n\u2022 Configured the project on WebSphere 6.1 application servers\n\u2022 Implemented the online application by using Core Java, Jdbc, JSP, Servlets and EJB 1.1, Web Services, SOAP, WSDL\n\u2022 Communicated with other Health Care info by using Web Services with the help of SOAP, WSDL JAX-RPC\n\u2022 Used Singleton, factory design pattern, DAO Design Patterns based on the application requirements\n\u2022 Used SAX and DOM parsers to parse the raw XML documents\n\u2022 Used RAD as Development IDE for web applications.\n\u2022 Preparing and executing Unit test cases\n\u2022 Used Log4J logging framework to write Log messages with various levels.\n\u2022 Involved in fixing bugs and minor enhancements for the front-end modules.\n\u2022 Implemented Microsoft Visio and Rational Rose for designing the Use Case Diagrams, Class model, Sequence diagrams, and Activity diagrams for SDLC process of the application\n\u2022 Doing functional and technical reviews\n\u2022 Maintenance in the testing team for System testing/Integration/UAT\n\u2022 Guaranteeing quality in the deliverables.\n\u2022 Conducted Design reviews and Technical reviews with other project stakeholders.\n\u2022 Was a part of the complete life cycle of the project from the requirements to the production support\n\u2022 Created test plan documents for all back-end database modules\n\u2022 Implemented the project in Linux environment.\n\nEnvironment: R 3.0, Erwin 9.5, Tableau 8.0, MDM, QlikView, MLLib, PL/SQL, HDFS, Teradata 14.1, JSON, HADOOP (HDFS), MapReduce, PIG, Spark, R Studio, MAHOUT, JAVA, HIVE, AWS.', u'Data Analyst/Data Modeler\nInnova Infotech - Bengaluru, Karnataka\nMarch 2009 to December 2010\nDescription: SYSINNOVA Infotech is an offshore software services and IT consulting company based in Bangalore, India. As a committed outsourcing partner and an IT vendor, our goal is to ensure cost effective, technical excellence and on-time deliveries. While we take care of their end-to-end programming and consulting needs, our clients focus on core business activities which correlate directly to their revenues and profitability. Strategic partnership with us gives our clients the access to latest technology, skilled manpower and scalable team which ultimately results in lower risk and higher ROI.\n\nResponsibilities:\n\u2022 Worked with project team representatives to ensure that logical and physical ER/Studio data models were developed in line with corporate standards and guidelines.\n\u2022 Involved in defining the source to target data mappings, business rules, data definitions.\n\u2022 Worked with BTEQ to submit SQL statements, import and export data, and generate reports in Teradata.\n\u2022 Responsible for defining the key identifiers for each mapping/interface.\n\u2022 Responsible for defining the functional requirement documents for each source to target interface.\n\u2022 Document, clarify, and communicate requests for change requests with the requestor and coordinate with the development and testing team.\n\u2022 Work with users to identify the most appropriate source of record and profile the data required for sales and service.\n\u2022 Document the complete process flow to describe program development, logic, testing, and implementation, application integration, coding.\n\u2022 Involved in defining the business/transformation rules applied for sales and service data.\n\u2022 Define the list codes and code conversions between the source systems and the data mart.\n\u2022 Worked with internal architects and, assisting in the development of current and target state data architectures.\n\u2022 Coordinate with the business users in providing appropriate, effective and efficient way to design the new reporting needs based on the user with the existing functionality.\n\u2022 Remain knowledgeable in all areas of business operations in order to identify systems needs and requirements.\n\u2022 Responsible for defining the key identifiers for each mapping/interface.\n\u2022 Implementation of Metadata Repository, Maintaining Data Quality, Data Cleanup procedures, Transformations, Data Standards, Data Governance program, Scripts, Stored Procedures, triggers and execution of test plans\n\u2022 Performed data quality in Talend Open Studio.\n\u2022 Coordinated meetings with vendors to define requirements and system interaction agreement documentation between client and vendor system.\n\u2022 Enterprise Metadata Library with any changes or updates.\n\u2022 Document data quality and traceability documents for each source interface.\n\u2022 Establish standards of procedures.\n\u2022 Generate weekly and monthly asset inventory reports.\n\nEnvironment: Erwin r7.0, SQL Server 2000/2005, Windows XP/NT/2000, Oracle 8i/9i, MS-DTS, UML, UAT, SQL Loader, OOD, OLTP, PL/SQL, MS Visio, Informatica.']","[u""Bachelor's""]",[u''],"degree_1 : ""Bachelors"""
0,https://resumes.indeed.com/resume/197f4a83a1ad2365,"[u'Data Science Analyst\nFord Motors Company (Global Data Insight Analytics) - Dearborn, MI\nDecember 2017 to Present\n\u25cf Integrate various data sources and process billions of records for Feature engineering using Hive and Spark.\n\u25cf Built different customer preference models using advanced Machine Learning to support Ford market strategies for global market.\n\u25cf Built automated scripts for data blending and ML workflows for advanced analytics using Alteryx.', u'Research/Teaching Assistant\nUniversity of North Carolina, NC - Dearborn, MI\nAugust 2016 to December 2017\n\u25cf Cleaned the unstructured data (NoSQL) from the logs provided by \u201cWALLMART.COM\u201d.\n\u25cf Analyzed customer behavior from 70+ factors including demographics, price, marketing channels, search etc. to identify trends.\n\u25cf Regrouping items into various categories using Market Basket Analysis and Clustering according to requirements.\n\u25cf Reduced Dimensionality with PCA, built models to predict customer purchase using XGBoost and Ensemble models.\n\u25cf Customers Segmentation for improving Retention rate based on Survival Analysis and Customer LTV.', u'Data Scientist\nBerkshire Hathaway (Oriental Trading Company) - Omaha, NE\nMay 2017 to August 2017\n\u25cf 0.5% increment in the summer sales in identified key areas by improving the Conversion Rate.\n\u25cf Mined 75 million records, identified crucial factors from 150+ factors using Regression and RF that drives Conversion Rate.\n\u25cf Saved 5000+ man-hours annually by building an automated classifier using Text mining (NLP) for user complaint & product review.\n\u25cf Built response models to Rank customers based on logistic regression to predict the probability of purchasing items.', u'Data Science Analyst\nToyota North America (Cognizant Technology Solution) - IN\nAugust 2015 to June 2016\n\u25cf Developed weekly reports for senior management for Customer Segmentation and Retention using RFM analysis.\n\u25cf Saved 500+ man-hours annually by developing an application using Shiny & D3.js for data visualization for executive leadership.\n\u25cf Predicted sales growth of the next quarter using Time Series Analysis to carry out an A/B testing.\n\u25cf Designed a targeted E-mail promotion strategy to drive sales growth by segmenting customers into high & low-value customers.', u'Data Analyst Intern\nAshok Leyland Ltd - IN\nMay 2014 to January 2015\n\u25cf Gathered, cleaned, and analyzed millions of records for doing Root Cause Analysis of the faults in the plant.\n\u25cf Saved 500+ man-hours by developing a R- shiny based KPI dashboard to visualize real time production metrics and performance of each department for executive leadership.\n\u25cf Improved plant efficiency by identifying faults in the production line and departments using Predictive models and Text Mining.']","[u'MS in Computer Science', u'BS in Electrical Engineering']","[u'University of North Carolina at Charlotte Charlotte, NC\nAugust 2016 to January 2017', u'VIT University Vellore Vellore, Tamil Nadu\nMay 2011 to January 2015']","degree_1 : MS in Compter Science, degree_2 :  BS in Electrical Engineering"
0,https://resumes.indeed.com/resume/b1a7ad9533205734,"[u'Corporate student consultant (Data Analyst)\nJanuary 2018 to Present\nData modeling using R and Tableau to mitigate food insecurity, benefiting smallholder farmers.\n\u2022 Proactive data collection, market research, trend analysis, Supply value chain analysis.\nSupply Chain Risk Analysis:\n\u2022 Analyzed resilience to interruptions in Supply Chain design of retail sales using R for Data cleaning, imputation, dimensionality\nreduction, Data modeling using random forest, SVM and neural network\n\u2022 Stepwise regression for multicollinearity handling, Feature selection using random forest and Boruta, feature engineering using\nARIMA time series analysis, Holtswinters model and Visualization using GGPlot2 and caret packages\n\u2022 Model prediction improvisation using stacking and ensembling, increased accuracy to 94%.\nBusiness Continuity plans- Emergency Preparedness:\n\u2022 Forecasted consumer buying patterns and optimized the network by synchronizing various elements of supply chain network.\n\u2022 Provided indicator as a percentage of capacity measure to predict demand during steady state and emergency.\n\u2022 Arena simulation Model for restocking and replenishment cycles using quick response strategy\nSupply Chain network optimization-Integrated planning:\n\u2022 Modeled an Omni-Channel supply network for a leading kitchenware firm using MILP approach and optimization in CPLEX\n\u2022 Incorporated reverse logistics for environmental sustainability and improved SC visibility, customer acquisition.\n\u2022 Resulted in 15% total cost reduction.', u'Data Scientist\nProducts and Services, Panasonic Automotive Systems of America\nJune 2017 to December 2017\nData collection from company databases, preprocessing of structured & unstructured\ndata, analysis for insights, a proposal for business strategy recommendation that saved SKILLS:\n$50000 per product channel per year.\n\u2022 R, Python, Tableau, SQL, C++\n\u2022 Data visualization using Tableau, GGPlot2, Interactive excel dashboard presented \u2022 Arena, Excel (VBA, Power BI, through PowerPoint to the team, Manager and Vice president. Gurobi Optimization)\n\u2022 Data mining for customer behavior pattern recognition and classification algorithms, \u2022 Demand forecasting\nidentifying 40 % incorrect data to save unrealized revenue. Text analytics with (Natural \u2022 Time series analysis\nLanguage Processing) NLP of customer complaints data to identify root cause and \u2022 Predictive modeling\nreduce response time to customers. \u2022 Machine Learning\n\u2022 Automated reporting using web scraping and Excel VBA to reduce the time taken from \u2022 Statistical Analysis\n4hours/day to 10 seconds/day for two employees.\n\u2022 Project Management\n\u2022 Developed web interface using R-Shiny for demand Forecasting machine learning \u2022 Supply Chain Analytics\nalgorithm hierarchical forecasting, random forest and model ensembling and lifecycle \u2022 Data Visualization\npredication with statistical analysis and models using random forest, neural network, \u2022 INFORMS Communications VP\nclustering. \u2022 Captain- Volleyball\nOutcomes:\nWeb interface for customer insights and demand forecasting integrated into department database.\nRevenue generated by warranty data correction, process improvement, and proposed strategy.\nProgrammed Macros for quick reporting reduce employee time on day-to-day tasks.\nValidation algorithm for shipment quantity\nPROJECTS:']","[u'MS in Industrial Engineering', u'B. Tech in Production Engineering']","[u'Purdue University\nJanuary 2016 to May 2018', u'National Institute of Technology Tiruchchirappalli, Tamil Nadu\nJanuary 2012 to January 2016']","degree_1 : MS in Indstrial Engineering, degree_2 :  B. Tech in Prodction Engineering"
0,https://resumes.indeed.com/resume/c978bf1733e3d85a,"[u'Chairman\nV TRUST - Chennai, Tamil Nadu\nSeptember 2007 to Present\nDesigned, launched and managed social welfare projects for underprivileged people\n\u25cf Work closely with other trustees on budgeting to implement new activities, policies and regulations\n\nAwards and Recognition\n\u25cf Small Step Award - IGATE - For finding business solution and developing a model that saves $14,000 annually\n\u25cf CEO Award - Prestigious award in IGATE, selected from among 150 employees, for organizing and managing CSR activities', u'Data Scientist III\nZions Bancorporation\nJune 2015 to April 2016\n\u2022 Perform cleanup of general ledger data monthly and reconcile it using the SQL program developed for each division\n\u2022 Prepare programs using SAS to cleanup, prepare, analyze GL data and predict future rates\n\u2022 Test the CCAR results every quarter of Zions Bancorporation\n\u2022 Analyze the General Ledger data and create the reports for ALCO reports', u'Senior Software Engineer\nIGATE GLOBAL SOLUTIONS - Chennai, Tamil Nadu\nJune 2010 to December 2013\nLearned ACBS model and contributed to process improvements\n\u25cf Managed a team of 7 members who reported directly to me\n\u25cf Analyzed and optimized tasks creating a model which reduced a process from 90 to 2 minutes using SQLs\n\u25cf Performed time series analysis to predict the future expected loans by new customers\n\u25cf Prepared weekly reports using Business Objects from the data warehouse of our project and presented to the clients']","[u'MBA in Management Information Systems', u'MBA in Financial Management', u'BACHELORS OF ENGINEERING in Electronics and Instrumentation Engineering']","[u'OKLAHOMA STATE UNIVERSITY, SPEARS SCHOOL OF BUSINESS\nJuly 2015', u'LOYOLA INSTITUTE OF VOCATIONAL EDUCATION Chennai, Tamil Nadu\nMay 2010', u'SATHYABAMA UNIVERSITY Chennai, Tamil Nadu\nApril 2010']","degree_1 : MBA in Management Information Systems, degree_2 :  MBA in Financial Management, degree_3 :  BACHELORS OF ENGINEERING in Electronics and Instrmentation Engineering"
0,https://resumes.indeed.com/resume/51fe2342ebc4d195,"[u'Data Scientist\nNFPA - Quincy, MA\nJanuary 2017 to Present\nSupervised and Unsupervised Learning\n\u2022 Built \u201cProperty Inspection Prioritization\u201d tool to detect high risk properties using logistic regression and random forest. Collaborated with software engineer to optimize data collection efficiency and deploy prediction model on website.\n\u2022 Collaborated with fire domain experts to conduct 30,000+ fire departments clustering to establish resource, performance and budget benchmark within similar groups of fire departments.\n\u2022 Detected geocoded fire incident anomalies using DBSCAN.\n\nNatural Language Processing (NLP)\n\u2022 Applied text mining in Austin fire incident narratives. Classified fire incident type using tf-idf and random forest and generated topics using Latent Dirichlet Allocation (LDA).\n\u2022 Detected invalid fire incident address using POS tagging.\n\nData Pipeline\n\u2022 Implemented NFPA enhanced geocoding system, achieving high geocoding accuracy and low cost in 20 million fire incident records from National Fire Reporting System (NFIRS).\n\u2022 Conducted records screening using HIVE and Pyspark.\n\u2022 Benchmarked commercial geocoders performance based on hypothesis testing (t-test, chi-square test).\n\u2022 Created geocoded fire incidents data warehouse based on MySQL and MongoDB. Interacted with HERE geocoding REST API to retrieve enhanced geocoded records. Visualized geocoded records using Tableau.', u'Data Scientist Intern (Graduate Quality Project)\nDana-Farber Cancer Institute - Brookline, MA\nJanuary 2017 to May 2017\n\u2022 Develop a platform that compares a new patient to a known patient cohort by matching clinical and genomic profiles to recommend treatment strategies. Conducted feature engineering from gene mutation and cancer specimen datasets using similarity metrics. Conducted pointwise learning to rank strategy to find genomic-similar patients using random forest, SVM and GBM.', u'Data Scientist and Python Developer Intern\nViaSat Inc - Quincy, MA\nMay 2016 to August 2016\n\u2022 Design and implement prefetching hints for high-latency web browser. Developed a novel algorithm based on entropy minimization to improve hints performance based on PostgreSQL, python and pyspark. Maintained python research and production coding library. The final deliverable is filed as US Patent.', u'Research Assistant\nJoint Research Centre for Biomedical Engineering, CUHK - Hong Kong, HK\nAugust 2010 to May 2014\n\u2022 Developed algorithm based on Compressed Sensing for increasing Magnetic Resonance Imaging (MRI) speed by 4 times. Developed a novel quantitative metric for evaluating medical image reconstruction quality.\n\u2022 Published research results in over 10 journal and international conference papers. Presented research results in three international conferences.']","[u'Master of Science in Data Science in Data Science', u'Doctor of Philosophy in Electronic Engineering in Electronic Engineering', u'Master of Science in Biomedical Engineering in Biomedical Engineering', u'Bachelor of Engineering in Biomedical Engineering in Biomedical Engineering']","[u'Worcester Polytechnic Institute (WPI) Worcester\nAugust 2015 to May 2017', u'The Chinese University of Hong Kong (CUHK) Hong Kong\nAugust 2010 to May 2014', u'Southeast University Nanjing\nSeptember 2007 to March 2010', u'Tianjin Medical University Tianjing\nSeptember 2002 to July 2007']","degree_1 : Master of Science in Data Science in Data Science, degree_2 :  Doctor of Philosophy in Electronic Engineering in Electronic Engineering, degree_3 :  Master of Science in Biomedical Engineering in Biomedical Engineering, degree_4 :  Bachelor of Engineering in Biomedical Engineering in Biomedical Engineering"
0,https://resumes.indeed.com/resume/5845301591038a1d,"[u""Senior software Engineer/Data Engineer\nBank of America - Pennington, NJ\nJanuary 2017 to Present\nRefactor the architecture of MongoDB to make the environment highly available and fault tolerant by a. adding multiple secondary nodes to the cluster in different data centers.\nb. adding delayed secondary to recover from accidental deletion of data\n\u2022 Lead the end-to-end implementation MongoDB upgrade project in order to ensure more stability in the environment.\n\u2022 Design and automate the deployment of the following in cloud platform.\n\u2022 NoSQL Database (MongoDB, Elasticsearch) cluster for the corporate cloud platform.\n\u2022 Version control repository (Bit bucket Datacenter) for the corporate cloud platform\n\u2022 Hadoop cluster to archive Elasticseach indexes.\n\u2022 Migrate and upgrade jFrog artifactory to ensure compliance with the standards at the Bank.\n\u2022 Design and Architect Machine Learning capabilities for the corporate cloud platform, which will be used to forecast the capacity.\n\u2022 Develop Dashboard on the system's metric and generate alert and take preventive action to avoid unplanned downtime. Reduced more than 95% percent unplanned downtime.\n\u2022 Automate the Deployment of new features to the cloud platform. Reduced the 60-70% manual deployment by automation"", u'Data Engineer/Analyst\nCisco Systems - Research Triangle Park, NC\nOctober 2015 to December 2016\n\u2022 Built a metrics gathering, storing and reporting framework for the Management team to consume and make data driven decision to plan the workload on the developers of different teams.\n\u2022 Automated the building and configuration (using ConcourseCI) of private repository for different kinds of artifactories such Yum, Docker etc. in Jfrog Artifactory.\n\u2022 Achieved 60% reduction of manual testing by developing automated functional tests to verify the functionality of Kafka, RabbitMQ, and Streamsets.\n\u2022 Achieved 95 % reduction on unplanned downtime by developing an automated predictive alert system to ensure on time preventive maintenance of servers.\n\u2022 Delivered a predictive model by using random forest classification model to predict possibility of denial of service attack. Improved the accuracy to 95% by using grid search technique', u'Data Scientist Intern\nLear Corporations - Southfield, MI\nMay 2015 to August 2015\n\u2022 Led an initiative to deliver an analytical data model to help improve the product development cycle, integration, data transparency and better collaboration tools to all the functional teams engaged and analyze the impact of system change to downstream systems.\n\u2022 Created a linear regression model with 95% accuracy to predict the demand for a given month to plan for the required inventory level.', u'Data Analyst\nMichigan State University - East Lansing, MI\nJanuary 2015 to May 2015\n\u2022 Achieved more than 95% accuracy on finding the appropriate criteria to hire a celebrity for product endorsements. This was achieved by extracting the celebrity posts and tweets from social media websites and training a classification model.\n\u2022 Attained 94% accuracy on a liner regression model by using residual plot analysis, box-cox transformation. The model predicted the expected popularity of a celebrity in the long run.\n\u2022 Recommended the business the combinations of products to cross-sale to increase (expected 15%) the average order value by using association analysis (apriori).', u'Data Engineer/Data Analyst\nJohnson and Johnson\nJuly 2013 to December 2014\n\u2022 Delivered Global Business Intelligence Framework that provides 360-degree view of the performance of the company across the Globe. Analyzed the average selling price(s) of top 20 products across US and EMEA to set the optimum prices for those products in those regions.\n\n\u2022 Transformed data into visually appealing stories for 500+ users using data visualization tools to facilitate business decision making. The reports and dashboards reduced manual workload.\n\n\u2022 Led a project to deliver more than 200 Data Integration (ETL) processes to build the Data Mart/Data Warehouse by extracting, transforming and loading the data into Data Mart/Data warehouse from different source systems.', u'Data Engineer/Data Analyst\nFebruary 2009 to July 2013\n\u2022 Built 150 + Data Integration processes to extract data from source systems and developed reports to display different key performance indicators that help business to improve operational efficiency.\n\u2022 Re-engineered and optimized existing Informatica Data Integration processes (100+) to reduce manual workload. Optimized the query performance by removing row-chaining and gathering statistics.\n\u2022 Reduced more than 30% of manual workload by delivering an automated process to refresh more than 100 cubes and publish those cubes to web portal for consumption.', u""Data Engineer/Data Analyst\nIBM Cognos\nAugust 2005 to February 2009\n\u2022 Developed and Designed an Enterprise Data Warehouse, which reduced the dependency of the business on the IT to run complex queries to generate reports.\n\u2022 Migrated existing legacy reporting solution to the IBM Cognos to ensure efficient and consistent reporting across the organization i.e. single version of truth.\n\u2022 Automated the execution of more than 200+ ETL processes by writing UNIX shell\n\nADDITIONAL PROJECTS\nKAGGLE COMPETITONS\nKernel Link: https://www.kaggle.com/iamchanchal/kernels\n\u2022 Data Science for Good: Kiva Crowdfunding: The objective is to pair Kiva's data with additional data sources to estimate the welfare level of borrowers in specific regions, based on shared economic and demographic characteristics.\n\u2022 House Prices: Advanced Regression Techniques: The aim of this project is to accurately predict the price of homes.\n\u2022 Predicting Red Hat Business Value: The aim of this project is to classify customer potential by analyzing the behavior of individual customer.""]","[u'Master of Science in Business Analytics', u'Bachelor of Technology in Computer Science and Engineering']","[u'Michigan State University East Lansing, MI\nJanuary 2015 to December 2015', u'Kalyani Government Engineering College Kalyani, West Bengal\nJuly 2001 to July 2005']","degree_1 : Master of Science in Bsiness Analytics, degree_2 :  Bachelor of Technology in Compter Science and Engineering"
0,https://resumes.indeed.com/resume/99c14fb3b840e55f,"[u'Data Scientist\nLARSEN AND TOUBRO INFOTECH - Houston, TX\nNovember 2016 to Present\nNatural Language Processing and Time Series Analysis for Investment Portfolio Management: Developed a model to analyze the news for prediction of stock trends on a 6-month basis to re-balance stock portfolios. Explored various algorithmic trading theories and ideas. This product is being marketed to portfolio management companies as part of Prime Technology Group services. Architected new office 365 plan for system consisting of over 20,000 users.\n\nExpertise in applying data mining techniques and optimization techniques in B2B and B2C industries and proficient in Machine Learning, Data/Text Mining, Statistical Analysis and Predictive Modeling.\nUtilized MapReduce/Spark Python modules for machine learning & predictive analytics in Hadoop on AWS.\nPredictive modeling using state-of-the-art methods\nUtilizing Informatica toolset (Informatica Data Explorer, and Informatica Data Quality) to analyze legacy data for Data Profiling.\nInvolved in upgrading DTS packages to SSIS packages (ETL).\nBroad knowledge of programming, and scripting (especially in R / Java / Python)\nProven experience building sustainable and trustful relationships with senior leaders.\nMigrating Informatica mappings from SQL Server to Netezza Foster culture of continuous engineering improvement through mentoring, feedback, and metrics\nDeveloping and maintaining Data Dictionary to create metadata reports for technical and business purpose.\nBuild and maintain dashboard and reporting based on the statistical models to identify and track key metrics and risk indicators.\nExtracting the source data from Oracle tables, MS SQL Server, sequential files and excel sheets.\nParse and manipulate raw, complex data streams to prepare for loading into an analytical tool.\nInvolved in defining the source to target data mappings, business rules, and data definitions.\nPerforming an end to end Informatica ETL Testing for these custom tables by writing complex SQL Queries on the source database and comparing the results against the target database.', u'Data Scientist\nPOLARIS INDUSTRIES, INC - Medina, MN\nJune 2015 to October 2016\nPolaris Industries designs, manufactures and markets innovative, high-quality, high-performance motorized products for recreation and utility use to the international market through global distribution channels.\nDeveloped analytical approaches to strategic business decisions\nMigrating Informatica mappings from SQL Server to Netezza Foster culture of continuous engineering improvement through mentoring, feedback, and metrics\nExpertise in applying data mining techniques and optimization techniques in B2B and B2C industries and proficient in Machine Learning, Data/Text Mining, Statistical Analysis and Predictive Modeling.\nImplemented Event Task for executing an Application Automatically.\nInvolved in defining the source to target data mappings, business rules, and data definitions.\nAssist in continual improvement of AWS data lake environment\nBuild and maintain dashboard and reporting based on the statistical models to identify and track key metrics and risk indicators.\nUsing HP Quality Center for defect tracking of issues.\nProven experience building sustainable and trustful relationships with senior leaders.\nDefining the list codes and code conversions between the source systems and the data mart using Reference Data Management (RDM).\nExtracting the source data from Oracle tables, MS SQL Server, sequential files and excel sheets.\nPerform analysis using predictive modeling, data/text mining, and statistical tools\nDeveloped MapReduce/Spark Python modules for machine learning & predictive analytics in Hadoop on AWS.\nCollaborate cross-functionally to arrive at actionable insights\nDeveloping and maintaining Data Dictionary to create metadata reports for technical and business purpose.\nPredictive modeling using state-of-the-art methods\nParse and manipulate raw, complex data streams to prepare for loading into an analytical tool.\nSynthesize analytic results with business input to drive measurable change\nUtilizing Informatica toolset (Informatica Data Explorer, and Informatica Data Quality) to analyze legacy data for Data Profiling.\nInvolved in upgrading DTS packages to SSIS packages (ETL).\nAdvanced analytics skills, and proficient at integrating and preparing large, varied datasets, architecting specialized database and computing environments, and communicating results.\nPerforming an end to end Informatica ETL Testing for these custom tables by writing complex SQL Queries on the source database and comparing the results against the target database.\nInvolved in developing Patches & Updates Module.\nBroad knowledge of programming, and scripting (especially in R / Java / Python)\nPerforming data profiling on various source systems that are required for transferring data to ECH using', u'Data Scientist\nCERIDIAN HCM, Inc - Minneapolis, MN\nJanuary 2014 to May 2015\nResponsible for management of analytics projects aimed at solving business problems. My role involves identifying/understanding business requirements, scope definition, developing machine learning models, presenting results to business and following up through implementation.\n\nUse of advanced analytics, data mining and statistical techniques on a variety of industries and using a diverse set of tools to bring insights out of complex data. Assess information from a range of data stored in disparate systems, integrating data and providing data mining to answer specific business questions as well as identifying unknown trends and relationships in data.\nWorked with internal architects and, assisting in the development of current and target state data architectures.\nPerformed data quality in Talend Open Studio.\nInvolved in defining the source to target data mappings, business rules, data definitions.\nRemain knowledgeable in all areas of business operations in order to identify systems needs and requirements.\nGenerate weekly and monthly asset inventory reports.\nInvolved in defining the business/transformation rules applied for sales and service data.\nResponsible for defining the key identifiers for each mapping/interface.\nResponsible for defining the functional requirement documents for each source to target interface.\nResponsible for defining the key identifiers for each mapping/interface.\nDocument the complete process flow to describe program development, logic, testing, and implementation, application integration, coding.\nDefine the list codes and code conversions between the source systems and the data mart.\nImplementation of Metadata Repository, Maintaining Data Quality, Data Cleanup procedures, Transformations, Data Standards, Data Governance program, Scripts, Stored Procedures, triggers and execution of test plans\nEnterprise Metadata Library with any changes or updates.\nCoordinate with the business users in providing appropriate, effective and efficient way to design the new reporting needs based on the user with the existing functionality.\nCoordinated meetings with vendors to define requirements and system interaction agreement documentation between client and vendor system.\nWorked with BTEQ to submit SQL statements, import and export data, and generate reports in Teradata.', u'Data Scientist\nGENERAL MOTORS - Detroit, MI\nJanuary 2013 to December 2013\nAnalysis focuses on quality control and automated testing, process control and Industrial automation, data acquisition and instrumentation including vision systems and motion control. \xb7 Data Science & Big Data \xb7 Simulation & Modeling \xb7\nData analysis using regressions, data cleaning, excel v-look up, histograms and TOAD client and data representation of the analysis and suggested solutions for investors\nImplemented Agile Methodology for building an internal application.\nHadoop Data Lake Implementation and HADOOP Architecture for client business data management.\nParticipated in all phases of data mining; data collection, data cleaning, developing models, validation, visualization and performed Gap analysis.\nUsed pandas, NumPy, seaborn, SciPy, matplotlib, Scikit-Learn, NLTK in Python for utilizing various machine learning algorithms.\nWorked on different data formats such as JSON, XML and performed machine learning algorithms in Python.\nData Manipulation and Aggregation from different source using Nexus, Toad, Business Objects, Power BI and Smart View.\nExtracted data from HDFS and prepared data for exploratory analysis using data munging.\nWorked as Data Architects and IT Architects to understand the movement of data and its storage and ER Studio\nFocus on integration overlap and Informatica newer commitment to MDM with the acquisition of Identity Systems.\nSetup storage and data analysis tools in Amazon Web Services cloud computing infrastructure.\nRapid model creation in Python using Pandas, NumPy, SKLearn, and plot.ly for data visualization. These models are then implemented in SAS where they are interfaced with MSSQL databases and scheduled to update on a timely basis.\nDevelopment and Deployment using Google Dialogflow Enterprise.', u'Data Scientist\nJC PENNEY - Plano, TX\nAugust 2011 to December 2012\nThis large volume retailer uses data science to manage inventory, increase and maintain market share and ensure a continuous understanding of market drivers. The project involved designing, developing and implementing data analysis using cloud architecture and applying data science techniques for data exploration.\nData ingestion is done using Flume with source as Kafka Source & sink as HDFS.\nImplementing YARN Resource pools to share resources of cluster for YARN jobs submitted by users.\nResponsible for creating Hive tables, loading the structured data resulted from MapReduce jobs into the tables and writing Hive queries to further analyze the logs to identify issues and behavioral patterns.\nAutomated Sqoop Jobs in a timely manner for Data Migration from Existing RDBMS to HDFS using Shell Scripting.\nPerformed import and export of large data set transfer between traditional databases and HDFS using Sqoop.\nResponsible for handling Hive queries using Spark SQL that integrates with Spark environment.\nKeep current with latest technologies to help automate tasks and implement tools and processes to manage the environment.\nWorked with cloud services like Amazon Web Services (AWS) and involved in ETL, Data Integration and Migration.\nPerformed transformations using Spark and then loaded data into HBase tables.\nCreated Hive external tables and designed data models in hive.\nProactively monitored systems and services, manage backup and disaster recovery systems and procedure.\nImplemented Hive Generic UDFs to handle business logic.', u""Data Analyst\nMACY'S TECHNOLOGY - Atlanta, GA\nMay 2010 to July 2011\nResponsible for extracting, sorting, and cleansing data; analyzing raw data and providing analysis and reports which can be used practically in business strategy.\nResponsible for loading, extracting and validation of client data.\nAnalyzing raw data, concluding, developing recommendations and manipulate data for data loads and extracts.\nData output - Made data chart presentations and coded variables from original data, conducted statistical analysis as and when required and provided summaries of analysis.\nExperience in Creating visually impactful dashboards in Excel and Tableau for data reporting by using pivot tables and VLOOKUP.\nCarried out data manipulation, data cleaning, dealt with missing records, purged and consolidated data for analysis and selected the appropriate visualization techniques; developing reports and communicating insights to stakeholders.\nDeveloped Enterprise applications in the Client/Server, Web application using PHP, Word Press, JavaScript, Jquery, AJAX, HTML5, CSS, MySQL & SQL Server database.\nDesigning and developing dynamic web pages using PHP and WordPress. Effectively used Tableau, Excel, jQuery, HTML, CSS and AJAX interactions.\nDesigning the database, creating Tables, Stored Procedures, Views, Function, and Triggers & Automated test cases to safeguard against regression defects and an efficient way of unit testing to exercise 70% of Test coverage.\nRegular follow-up meetings with Project Manager to update them about project progress & demo after each sprint.\nDocumentation, Release Note Preparation, and Deployment""]","[u""Bachelor's in Physics""]","[u'University of Texas at Dallas Dallas, TX\nMay 2015']","degree_1 : ""Bachelors in Physics"""
0,https://resumes.indeed.com/resume/575662539e25627d,"[u'Data Scientist\nFarmers Insurance - Somerville, NJ\nAugust 2015 to Present\nDescription: Farmers Insurance, the wholly-owned subsidiary Swiss company Zurich Insurance Group Ltd, is an American insurer group of automobiles, homes and small businesses and also provides other insurance and financial services products.\n\nResponsiblities:\n\u2022 Performed statistical analysis of data using SAS and SPSS. Applied descriptive and inferential methodologies to identify disease trends & warning signals and to undertake impact assessment\n\u2022 Monitor, collate, and synthesize information to produce reports and program e-related documents\n\u2022 Performed other administrative tasks like drafting & designing informational materials\n\u2022 Discover and track customer behavior data to identify trends and business impact.\n\u2022 Leveraging data using BI tools Tableau for Data visualization and storytelling.\n\u2022 Profile raw data sets across platforms and develop KPI/dashboard to measure product performance.\n\u2022 Collaborate with executive management team to rapidly prototype business dashboards and KPIs.\n\u2022 Perform quantitative analysis of product sales trends to recommend pricing decisions.\n\u2022 Monitored and enhanced performance of existing Sales and Marketing models including customer life time value models and retention models using statistical techniques.\n\u2022 Develop and implement data collection systems and other strategies that optimized statistical efficiency and data quality.\n\u2022 Worked on Noise Reduction methods exponential smoothing and Fast Fourier Transformation methods and made comparison with regression methods.\n\u2022 Assisted in logical and physical Modeling for OLAP and OLTP systems.\n\u2022 Extensively worked on the ETL mapping, source data analysis, source to target mapping and documentation of OLAP reports requirements.\n\u2022 Carried out Regression Analysis with R/SAS, investigated on the model for problems like goodness of fit, over-fitting, Multi co linearity, residual normality, etc. Established our linear model for forecasting.\n\u2022 Performed regular research and gathered new statistical evidence at every opportunity.\n\u2022 Data analyzed for scientists, research scholars, medical doctors, and also to post.\n\u2022 Utilized SPSS and Minitab soft ware to randomize, analyze and interpretation of data.\n\u2022 Summarized findings, created reports, & maintained database of statistical information.\n\u2022 Used advanced excel to create pivot tables, charts, used VLOOKUP, and other functions.\nEnvironment: Erwin R 9.0, R Studio, Machine learning, Informatica 9.0, ODS, OLTP, Oracle 10g, Hive, OLAP, DB2, Metadata, MS Excel, Mainframes MS Vision, Rational Rose.', u'Data Scientist\nEricsson - Dallas, TX\nMarch 2013 to July 2015\nDescription: My role was to design and implement A/B Testing for Ericsson Telecom, capture data using web analytics using web crawling platform and Optimization, create dashboards using ELK (Elastic search, Logstash and Kibana) framework.\n\nResponsibilities:\n\u2022 Responsible for performing Machine-learning techniques regression/classification to predict the outcomes.\n\u2022 Responsible for design and development of advanced R/Python programs to prepare transform and harmonize data sets in preparation for modeling.\n\u2022 Developed large data sets from structured and unstructured data. Perform data mining.\n\u2022 Partnered with modelers to develop data frame requirements for projects.\n\u2022 Performed Ad-hoc reporting/customer profiling, segmentation using R/Python.\n\u2022 Tracked various campaigns, generating customer profiling analysis and data manipulation.\n\u2022 Provided R/SQL programming, with detailed direction, in the execution of data analysis that contributed to the final project deliverables. Responsible for data mining.\n\u2022 Analyzed large datasets to answer business questions by generating reports and outcome.\n\u2022 Worked in a team of programmers and data analysts to develop insightful deliverables that support data- driven marketing strategies.\n\u2022 Used Python 2.7 to apply time series models, clustering algorithm and other data mining methods to explore the fast growth opportunities of our clients\n\u2022 Analyzed the traffic queries of Baidu search engine using classification algorithm.\n\u2022 Assisted to improve the liquidity of our ads model.\n\u2022 Based on the data of clients and traffic, designed comprehensive analysis to optimize products and explored the strengths and weaknesses of products.\n\u2022 Involved in fixing bugs and minor enhancements for the front-end modules.\n\u2022 Implemented Microsoft Visio and Rational Rose for designing the Use Case Diagrams, Class model, Sequence diagrams, and Activity diagrams for SDLC process of the application\n\u2022 Maintenance in the testing team for System testing/Integration/UAT.\n\u2022 Guaranteeing quality in deliverables.\n\u2022 Conducted Design reviews and Technical reviews with other project stakeholders.\n\u2022 Was a part of the complete life cycle of the project from the requirements to the production support.\n\u2022 Involved in loading data from RDBMS and web logs into HDFS using Sqoop and Flume.\n\u2022 Worked on loading the data from MySQL to HBase where necessary using Sqoop.\n\u2022 Developed Hive queries for Analysis across different banners.\n\u2022 Extracted data from Twitter using Java and Twitter API. Parsed JSON formatted twitter data and uploaded to database.\n\u2022 Performed performance improvement of the existing Data warehouse applications to increase efficiency of the existing system.\n\nEnvironment: R 3.0, Erwin 9.5, Tableau 8.0, MDM, QlikView, MLLib, PL/SQL, HDFS, Teradata 14.1, JSON, MapReduce, MySQL, Spark, R Studio, MAHOUT.', u'Data /System Analyst\nANZ Bank\nMarch 2009 to February 2013\nDescription: Australia and New Zealand Banking Group Ltd., commonly known as ANZ Bank, provides retail and commercial banking services, cash management, consumer and commercial lending, investment alternatives, insurance and estate planning solutions through online and mobile devices.\n\nResponsibilities:\n\u2022 Converted various SQL statements into stored procedures thereby reducing the number of database accesses.\n\u2022 Handled importing data from various data sources, performed transformations using Hive, Map Reduce, and loaded data into HDFS.\n\u2022 Interaction with Business Analyst, SMEs and other Data Architects to understand Business needs and functionality for various project solutions\n\u2022 Identifying and executing process improvements, hands-on in various technologies such as Oracle, Informatica, Business Objects.\n\u2022 Created data governance and privacy policies.\n\u2022 Manipulating/mining data from database tables (Redshift, Oracle, Data Warehouse)\n\u2022 Handle conversation with Country DM team, Finance team, BI team and regional team to get sign off for the products logic.\n\u2022 Responsible for architecting analytic frameworks for data mining, ETL, analysis, and reporting under the supervision of the Manager.\n\u2022 Cleaned data by analyzing and eliminating duplicate and inaccurate data (outliers) using R.\n\u2022 Worked in Agile Environment.\n\u2022 Ensure that there are no missing values in the dataset and can be used for further Analysis.\n\u2022 Trained in Basics of Data Scientist and implemented those software applications in collecting and managing patient data in Excel/SPSS.\n\u2022 Assisted in performing statistical analysis of the data and storing them in a database.\n\u2022 Worked with Quality Control Teams to develop Test Plan and Test Cases.\n\u2022 Involved in designing and implementing the data extraction (XML DATA stream) procedures.\n\u2022 Generated graphs and reports using ggplot in RStudio for analyzing models.\n\u2022 Generating the Results and predicting the Accuracy.\n\u2022 Preparing the Final Documents and ensure delivery to the Client before EOD.\n\nEnvironment: SQL/Server, Oracle 9i, MS-Office, Teradata, Informatica, ER Studio, XML, Business Objects, HDFS, Teradata 14.1, JSON, MapReduce, PIG, Spark, R Studio, MAHOUT, JAVA, HIVE,']","[u'Masters in Information Technology in Professional Computing', u'Bachelors in Electronics and Communications Engineering in Electronics and Communications Engineering']","[u'Swinburne University', u'Anna University']","degree_1 : Masters in Information Technology in Professional Compting, degree_2 :  Bachelors in Electronics and Commnications Engineering in Electronics and Commnications Engineering"
0,https://resumes.indeed.com/resume/4a7cb61bca016011,"[u'DATA SCIENTIST INTERN\nVALOR WATER ANALYTICS, A XYLEM INC. COMPANY - San Francisco, CA\nOctober 2017 to Present\nImproved existing statistical model to find out under-registration water meters.\nBuilt machine learning and deep learning models on time series data to classify meter under- registration.\nAutomated the generation of labeled data by introducing third-party APIs. Boosted the efficiency of the labeling process by 20x.\nTechnical stack: Python, SQL, AWS Redshift, Tensorflow, machine learning, deep learning.']","[u'Master of Data Science in Data Science', u'Bachelor of Science in Biology']","[u'University of San Francisco San Francisco, CA\nJuly 2018', u'University of California La Jolla, CA\nJuly 2016']","degree_1 : Master of Data Science in Data Science, degree_2 :  Bachelor of Science in Biology"
0,https://resumes.indeed.com/resume/98182a630b61614d,[u'IT/Information Security Director and Data Scientist\nCobham Advanced Electronic Solutions\nJanuary 2001 to Present\nRemote Worker'],[],[],degree_1 : 
0,https://resumes.indeed.com/resume/da468f039f93c268,"[u""Data Scientist Intern\nSoothsayer Analytics - Dearborn, MI\nJanuary 2018 to Present\nApply statistical modeling and Natural Language Processing methods in Python to understand consumer shopping habits,\nand predict product demand from online text conversation datasets\n\u2022 Categorize all these text conversations into business-relevant topics using Latent Dirichlet Allocation\n\u2022 Use lexicon based approach to extract sentiment and people's emotion in text"", u'Research Assistant\nWearable and Signal Processing Lab, University of Michigan - Dearborn, MI\nMay 2017 to December 2017\nCollected, cleansed and provided modeling and analyses of structured and unstructured data sets using MATLAB,\ndesigned Deep Learning convolutional neural networks with different convolutional, pooling and dropout architectures\n\u2022 Examined batch normalization procedures for model regularization to avoid overfitting, delivered research papers', u'Software Engineer\nProhaktiv Inc - Pune, Maharashtra\nAugust 2014 to December 2016\n\u2022 Migrated application from .NET to Laravel 5.2 backend, and Angular Js Frontend, created RESTful API\n\u2022 Integrated application with Google Analytics and Google AdWords to download the analytics data\n\u2022 Built App that communicates with RESTful services, integrated Cordova into an existing web application\n\u2022 Interacted with clients and business subject matter experts for gathering requirements for the project\n\u2022 Wrote custom modules in Drupal using Drupal APIs such as Node API Hooks, and Forms API Hooks']","[u'Master of Science in Computer and Information Science in Advanced Artificial Intelligence', u'in Technology']","[u'The University of Michigan Dearborn, MI\nJanuary 2017 to Present', u'Uttar Pradesh Technical University Ghaziabad, Uttar Pradesh\nAugust 2009 to June 2013']","degree_1 : Master of Science in Compter and Information Science in Advanced Artificial Intelligence, degree_2 :  in Technology"
0,https://resumes.indeed.com/resume/c50aa9c86b828c4a,"[u'Data Scientist Intern\nAnheuser-Busch InBev - Champaign, IL\nSeptember 2017 to Present\n\u2022 Generated different excitable pricing suggestions in America and Mexico based on statistical modeling\n\u2022 Built and evaluated regression, classification and clustering models to analyze beer price elasticity\n\u2022 Applied PCA to compute weights of different demographic indicators and visualized results in R Shiny', u'Data Scientist Intern\nDow AgroScience - Champaign, IL\nJune 2017 to August 2017\n\u2022 Selected well-performed corn hybrids using machine learning algorithms instead of artificial cultivation\n\u2022 Built a more advanced regional model to predict yield of corn using large-scale genetic data with 60 million observations and 8,000 features, and compared prediction results with original model\n\u2022 Increased 7% prediction accuracy by tuning parameters of Artificial Neural Network model on Keras\n\u2022 Cleaned, grouped and visualized geographical data using Python and Power BI', u'Student Researcher, Agricultural Products Price Forecasting System\nLab for Information Systems at Renmin University of China - Beijing, CN\nAugust 2015 to January 2016\n\u2022 Built and evaluated time series and machine learning models to predict agricultural product prices\n\u2022 Filtered around 300 agricultural products news every day and extracted information related to price changes using word-category disambiguation automatically in Java\n\u2022 Adding sentiment analysis to improve prediction accuracy using TF-IDF and Word2Vec models\n\u2022 Mined potential topics in text data using Latent Dirichlet Allocation model']","[u'M.S. in Statistics in Statistics', u'B.S. in Mathematics and Applied Mathematics in Mathematics and Applied Mathematics']","[u'University of Illinois Urbana-Champaign, IL\nMay 2018', u'Renmin University of China Beijing, CN\nJune 2016']","degree_1 : M.S. in Statistics in Statistics, degree_2 :  B.S. in Mathematics and Applied Mathematics in Mathematics and Applied Mathematics"
0,https://resumes.indeed.com/resume/59681bd2ad675157,"[u""Data Scientist\nAT&T - Atlanta, GA\nAugust 2015 to Present\nObjective: Modelling to determine best customer experience against competitors like Xfinity to target customers for personalized marketing strategies. AT&T supports sales Online (Ecommerce) and other sales channels (Retail stores, POS, OPUS).\n\nResponsibilities:\n\u2022 Working as a part of Sales Catalog Data Management Team.\n\u2022 Project involves integrating Wireline (EDW) and Wireless (eCDW) data.\n\u2022 Closely work with the IT, Sales and Marketing leads to formulate hypothetical insights, predictive analysis and work through delivering these insights to production.\n\u2022 Closely work in providing data for the TDATA personalisation engine for better and advanced customer experience while shopping online.\n\u2022 Personalization for registered users, Returning visitors and Anonymous visitors.\n\u2022 Worked in gathering the required events (System Events, User Events) for reporting to provide a better application GUI.\n\u2022 Inflection points tracking, Target Marketing, Customer profiling and Segmentation.\n\u2022 Worked on model building using machine-learning algorithms like logistic regression, naive bayes, random forest, SVM, SVR.\n\u2022 Worked on a POC project for NLP (Natural Language processing) with Review Data.\n\u2022 Propensity to convert modelling using the visit sequence and conversion sequence mapping.\n\u2022 Zeroth problem solution for Anonymous visitor, returning visitor and registered users\n\u2022 Perform text analytics for the CHAT application via review data machine learning technique in python using NLTK to improve better recommendations for the Reps when advising a customer.\n\u2022 Used UCB & Thomson Sampling Intuition for Multi Bandit Testing and A/B testing to perform ad banner choices and product visual choices to support customer from different geographic locations and off different classes (Student, Employee, IRU's, CRUS's).\n\u2022 Market basket Analysis using APRIOR for purchase transactions data at store level to recommend customers with the different combinations of the products like better internet speed with the best DIRECT TV packages as a bundle. Custom Dashboards product category wise.\n\u2022 Sequence mining for the conversion paths for different segments.\n\u2022 Profiling based on the content access reports, event trigger reports and navigation reports. Profiles are then used for target marketing and segmentation.\n\u2022 Custom Website strategy for segments.\n\u2022 Site/Page optimization, Promotional campaigns assessment, Customer segmentation and assessing revenue against targets at regular intervals.\n\u2022 Developed visualizations using sets, Parameters, Calculated Fields, Dynamic sorting, Filtering, Parameter driven analysis, gathered data from different data marts.\n\u2022 Reporting designs based on the business specific problems, Reporting implementation on Tableau to suggest business ways to improve their customer experience both in terms of sales and user interface.\n\u2022 Advanced charts, drill downs and intractability are incorporated in the reporting for different stakeholders and integrating the publishing of reports to the clients SharePoint infrastructure."", u'Data Scientist\nState Farm - Bloomington, IL\nJanuary 2012 to May 2015\nObjective: Modelling Individual and Family Insurance rate based on Income, Demography and competitors strategy for Market campaigning.\n\nResponsibilities:\n\u2022 Performed predictive analysis and developed models to provide insight to business on the rates for the insurance plans based on several parameters (Demography, Competitors market, different Customer classes).\n\u2022 Participated in all phases of data mining- data collection, data cleaning, developing models, validation, and visualisation.\n\u2022 Performed Gap analysis to detail the performance of the application against the business requirements.\n\u2022 Providing Ad hoc analysis for statistical modelling and analytic reports to answer specific business query.\n\u2022 Captured Modelling requirements from Senior Stakeholders to Bench functional requirements for SAS/ R Python\n\u2022 Performed Data Manipulation and Aggregation from Various source including HDFS.\n\u2022 Creating various B2B Predictive and descriptive analytics using R and Tableau to improve the experience of Small Business Insurance Plans.\n\u2022 Used pandas, numpy, seaborn, scipy, matplotlib, scikit-learn in Python for developing various machine-learning algorithms.\n\u2022 Designed and tested Predictive Algorithms using Historical Data to predict better future prices and promotional campaigns.\n\u2022 Utilized machine learning algorithms such as Decision Tree, linear regression, multivariate regression, Naive Bayes, Random Forests, K-means, & KNN to understand the sales of various insurance plans and also to get an insight of claims based on different conditions like demography, customer income, natural disasters.\n\u2022 Parsing data, producing concise conclusions from raw data in a clean, well-structured and easily maintainable format.\n\u2022 Responsible for Big data initiatives and engagement including analysis, brainstorming, POC, and architecture.\n\u2022 Worked on different data formats from different vendor systems with data formats like JSON, XML.\n\u2022 Worked on Map Reduce/Spark Python modules for machine learning & predictive analytics in Hadoop on AWS.\n\u2022 Worked with (Tableau) Report Writers to Test, Validate Data Integrity of Reports', u'Modelling Analyst\nAccenture Private limited\nAugust 2010 to December 2011\nObjective: Modelling Individual and Family Insurance rate based on Income and Demography for Market campaigning.\n\nResponsibilities:\n\u2022 Prepared financials reports, analysis of month on month variance and provided reports to management on a weekly, monthly and quarterly basis\n\u2022 Scrutiny of General Ledger on monthly basis and do a financial analysis of the same. Provide the information in an appropriate manner to the management\n\u2022 Monitored costs & trend analysis and providing updates on variances to respective stakeholders\n\u2022 Prepared annual budget and periodic forecast/outlook and Generating Insights\n\u2022 Partnered with Leadership to understand the requirement and provide reports, Analyze business units\n\u2022 Recommended solutions for areas of concern with financial data.\n\u2022 Provided ad-hoc reports as per requirement from time to time']",[u'Masters in Computer Science Engineering'],[u'St Cloud State University'],degree_1 : Masters in Compter Science Engineering
0,https://resumes.indeed.com/resume/95aaad1204aa397f,"[u'Data Scientist\nWalgreens - Deerfiled, IL\nFebruary 2017 to Present\nDescription:The Walgreen Company is an American company that operates as the second-largest pharmacy store chain in the United States behind CVS Health. It specializes in filling prescriptions, health and wellness products, health information, and photo services.\nResponsibilities:\n\u2022 Involved in defining the Source To business rules, Target data mappings, data definitions.\n\u2022 Performing Data Validation / Data Reconciliation between disparate source and target systems (Salesforce, Cisco-UIC, Cognos, Data Warehouse) for various projects.\n\u2022 Worked closely with the Data Governance Office team in assessing the source systems for project deliverables.\n\u2022 Performing data profiling and analysis on different source systems that are required for Customer Master.\n\u2022 Extracting data from different databases as per the business requirements using Sql Server Management Studio.\n\u2022 Identifying the Customer and account attributes required for MDM implementation from disparate sources and preparing detailed documentation.\n\u2022 Used T-SQL queries to pull the data from disparate systems and Data warehouse in different environments.\n\u2022 Presented DQ analysis reports and score cards on all the validated data elements and presented -to the business teams and stakeholders.\n\u2022 Used Data Quality validation techniques to validate Critical Data elements (CDE) and identified various anomalies.\n\u2022 Interacting with the ETL, BI teams to understand / support on various ongoing projects.\n\u2022 Extensively using MS Excel for data validation.\n\u2022 Interacting with the Business teams and Project Managers to clearly articulate the anomalies, issues, findings during data validation.\n\u2022 Writing complex SQL queries for validating the data against different kinds of reports generated by Cognos.\n\u2022 Generating weekly, monthly reports for various business users according to the business requirements. Manipulating/mining data from database tables (Redshift, Oracle, Data Warehouse).\n\u2022 Providing analytical network support to improve quality and standard work results.\n\u2022 Utilize a broad variety of statistical packages like SAS, R , MLIB, Graphs, Hadoop, Spark , Map Reduce and others\n\u2022 Create statistical models using distributed and standalone models to build various diagnostics, predictive and prescriptive solution.\n\u2022 Interface with other technology teams to load (ETL), extract and transform data from a wide variety of data sources\n\u2022 Provides input and recommendations on technical issues to Business & Data Analysts, BI Engineers and Data Scientists.\nEnvironment: Data Governance, SQL Server, ETL, MS Office Suite - Excel(Pivot, VLOOKUP), DB2, R, Visio, HP ALM, Agile, Azure, Data Quality, Tableau and Reference Data Management.', u'Data Scientist\nSales force , Indianapolis\nDecember 2015 to January 2017\nDescription:: Opened in 1990 near Monument Circle in Indianapolis, Salesforce Tower is the tallest building in the U.S. state of Indiana. It surpassed the AUL Tower in Indianapolis for the distinction.\nResponsibilities:\n\u2022 Analyzed and solved business problems, and found patterns and insights within structured and unstructured data.\n\u2022 Collaborated with business to understand company needs and devise possible solutions\n\u2022 Cleaned, analyzed and selected data to gauge customer experience.\n\u2022 Worked as a liaison between multiple teams to gather and document requirements and developed data science platform designed to cover the end-to-end Machine learning workflow: manage data, train, evaluate, and deploy models, make predictions, and monitor predictions using different machine learning methodologies like Regression, Bayesian, Decision Trees, Random Forests, SVM, Kernel SVM Clustering, Instance based methods, Association Rules, Dimensionality Reduction etc.\n\u2022 Involved in defining the Source To business rules, Target data mappings, data definitions.\n\u2022 Performing Data Validation / Data Reconciliation between disparate source and target systems for various projects.\n\u2022 Presented proposals and results in a clear manner backed by data and coupled with actionable conclusions to drive business decisions\n\u2022 Performing data profiling and analysis on different source systems that are required for Customer Master.\n\u2022 Created meaningful data visualizations to communicate findings and relate them back to how they create business impact.\n\u2022 Utilized a diverse array of technologies and tools as needed, to deliver insights such as R, SAS, Matlab, Tableau and more.\n\u2022 Implemented new statistical and mathematical methodologies as needed for specific models or analysis.\n\u2022 Used algorithms and programming to efficiently go through large datasets and apply treatments, filters, and conditions as needed.\n\u2022 Identifying the Customer and account attributes required for MDM implementation from disparate sources and preparing detailed documentation.\n\u2022 Presented DQ analysis reports and score cards on all the validated data elements and presented -to the business teams and stakeholders.\n\u2022 Used Data Quality validation techniques to validate Critical Data elements (CDE) and identified various anomalies.\n\u2022 Used T-SQL queries to pull the data from disparate systems and Data warehouse in different environments.\n\u2022 Worked closely with the Data Governance Office team in assessing the source systems for project deliverables.\n\u2022 Extracting data from different databases as per the business requirements using Sql ServerManagement Studio.\n\u2022 Interacting with the ETL, BI teams to understand / support on various ongoing projects.\n\u2022 Extensively using MS Excel for data validation.\n\u2022 Interacting with the Business teams and Project Managers to clearly articulate the anomalies, issues, findings during data validation.\n\u2022 Writing complex SQL queries for validating the data against different kinds of reports generated by Cognos.\n\u2022 Generating weekly, monthly reports for various business users according to the business requirements. Manipulating/mining data from database tables (Redshift, Oracle, and Data Warehouse).\n\u2022 Create statistical models using distributed and standalone models to build various diagnostics, predictive and prescriptive solution.\n\u2022 Interface with other technology teams to load (ETL), extract and transform data from a wide variety of data sources\n\u2022 Providing analytical network support to improve quality and standard work results.\n\u2022 Provides input and recommendations on technical issues to Business & Data Analysts, BI Engineers and Data Scientists.\n\nEnvironment:Data Governance, SQL Server, ETL, MS Office Suite - Excel(Pivot, VLOOKUP), DB2, R, Visio, HP ALM, Agile, Azure, MDM , Share point, Data Quality, Tableau and Reference Data Management.', u""Data Analyst\nGE Capital Treasury - Stamford, CT\nFebruary 2014 to November 2015\nDescription:GE Capital, is the financial services unit of the American multinational conglomerate General Electric. It provides commercial lending and leasing, as well as a range of financial services for commercial aviation, energy, and support for GE's industrial business units.\n\nResponsibilities:\n\u2022 Partner with analysts and live producers to identify strategic business questions, key metrics, and actionable insights.\n\u2022 Conducted feasibility study and performed GAP and Impact analysis for the proposed state.\n\u2022 Communication channel and project's review for management.\n\u2022 Conducted JAD sessions to allow different stakeholders to communicate their perspective with each other, resolve any issues and come to an agreement quickly.\n\u2022 Managed direct relationship with third party vendors and off-shore development teams, created\n\u2022 Collaborated with ProjectArchitect/Designer to detail technical requirements and refine the solution design.\n\u2022 Created use case scenarios and documented workflow and business processes.\n\u2022 Prepared Business Requirement Document (BRD) and Functional Specification Document (FSD).\n\u2022 Created executive summary with the risks and mitigation plan for the projects.\n\u2022 Collaborated with the QA team to ensure adequate testing on software, maintained quality procedures, and ensured that appropriate documentation was in place.\n\u2022 Queried the database using complex SQL queries.\n\u2022 Utilized data analytics to evaluate workload performance data and recommend changes.\n\u2022 Conducted peer review meetings periodically to keep track of the project's milestones.\n\u2022 Facilitated training sessions for internal and external teams for smooth transition.\n\u2022 Assisted the Project Manager with creating detailed project plans and in developing, scheduling and tracking project timelines.\n\u2022 Led Defect triage sessions with client during UAT phase, and was the point of contact on Defect management for the project team and led the change management process for the PMO.\n\u2022 Reviewed business requirements documented by other business analysts to scope level of effort in testing functionality and identifying possible inter-dependencies.\n\nEnvironment: HTML5, CSS3, AJAX, JSON, jQuery, MySQL, NumPy, SQL Alchemy, Matplotlib, Hadoop, Pig Scripts."", u'Data Modular\nEquifax - Alpharetta, GA\nNovember 2012 to January 2014\nDescription: Equifax Inc. is a consumer credit reporting agency. Equifax collects and aggregates information on over 800 million individual consumers and more than 88 million businesses worldwide. Offering credit and demographic related data and services to business, Equifax sells credit monitoring and fraud-prevention services directly to consumers.\n\nResponsibilities:\n\u2022 Implemented user interface guidelines and standards throughout the development and maintenance of the website using the HTML, CSS, JavaScript and JQuery.\n\u2022 Used Django to interface with the JQueryUI and manage the storage and deletion of content.\n\u2022 Used Hive queries for data analysis to meet the business requirements.\n\u2022 Involved with advanced CSS concepts and building table-free layouts.\n\u2022 Used advanced packages like Mock, patch and beautiful soup (b4) to perform unit testing.\n\u2022 Used Pandas library for statistics Analysis.\n\u2022 Used Numpy for Numerical analysis for Insurance premium.\n\u2022 Worked on rebranding the existing web pages to clients according to the type of deployment.\n\u2022 Created UI using JavaScript and HTML5/CSS.\n\u2022 Developed and tested many features for dashboard using Bootstrap, CSS, and JavaScript.\n\u2022 Managed a small team of programmers using a modified version of the agile development.\n\u2022 Worked on Jenkins continuous integration tool for deployment of project.\n\u2022 Worked on updating the existing clipboard to have the new features as per the client requirements.\n\u2022 Performed Unit testing, Integration Testing, GUI and web application testing using Selenium.\nEnvironment: Django , HTML5, CSS, XML, Kafka, MySQL, JavaScript, Angular JS, Backbone JS, Nginix server, Amazon s3, Jenkins, Beautiful soup, JavaScript, Eclipse, Git, GitHub, Linux, and MAC OSX.', u'Data Modular\nInfinite Solutions - Hyderabad, Telangana\nFebruary 2011 to October 2012\nDescription: Infinite healthcare technology and IT company provides custom business software solutions, healthcare IT Services, healthcare IT consulting, next-gen mobility solutions and product engineering services.\nResponsibilities:\n\u2022 Business logic implementation, data exchange, XML processing and graphics creation has been done using Django.\n\u2022 Developed UI using CSS, HTML, JavaScript, AngularJS, JQuery and JSON.\n\u2022 Developed user interface using, CSS, HTML, JavaScript and JQuery&Ruby on rails.\n\u2022 A Django dashboard with custom look and feel for end user has been created after a careful study of the Django admin site and dashboard.\n\u2022 JIRA was used to build an environment for development.\n\u2022 Used web applications development using Django, Flask and JQuery, Ajax while using HTML/CSS/JS for server-side rendered application.\n\u2022 Develop UNIX shell scripts and XML configuration files.\n\u2022 Was involved in environment, code installation as well as the SVN implementation.\n\u2022 Build all database mapping classes using Django models and Cassandra.\n\u2022 Used Pandas API to put the data as time series and tabular format for east timestamp data manipulation and retrieval.\n\u2022 Different testing methodologies like unit testing, Integration testing, web application testing, selenium testing were performed. Used Django framework for application development.\n\u2022 Worked on continuous integration and automation using Jenkins.\n\u2022 Developed, tested and debugged software tools utilized by clients and internal customers.\n\u2022 Coded test programs and evaluated existing engineering processes.\n\u2022 Designed and configured database and back end applications and programs.\n\u2022 Designed and developed data management system using MySQL.\n\u2022 Creating unit test/regression test framework for working/new code.\n\nEnvironment: Django, Java Script, SQL Server, HTML, DHTML, CSS, Linux, Sub Version, Wing, AJAX.', u'SQL Developer\nHoneywell - Hyderabad, Telangana\nAugust 2009 to January 2011\nDescription: Honeywell International Inc. is an American multinational conglomerate company that produces a variety of commercial and consumer products, engineering services and aerospace systems for a wide variety of customers, from private consumers to major corporations and governments.\nResponsibilities:\n\u2022 Monitored and visualized the trends at daily/weekly/monthly/quarterly levels in Tableau\n\u2022 Extensively used SQL queries to translate data into valuable information for decision making.\n\u2022 Created adhoc database reports using advanced SQL queries.\n\u2022 Maintained the data of the institution in MySQL on cloud and built in a recommendation system and a web application.\n\u2022 Created measures for tracking the actual performance\n\u2022 Performed extensive analysis on Pricing, Market trends and Yearly data of the institution to estimate the performance.\n\u2022 Estimated the inventory for the in-house students based on the available resources which led to effective revenue management in the boarding section\n\u2022 Prepared research reports, qualitative and quantitative analysis.\n\u2022 Used algorithms like regression, clustering and decision trees to analyze the data on SAS JMP.\n\nEnvironment: SQL Server 2005, SQL, MySQL, DTS Designer, MS-Office, MS-Excel, VSS.']","[u""Bachelor's""]",[u''],"degree_1 : ""Bachelors"""
0,https://resumes.indeed.com/resume/310bdc7a9b1954ab,"[u'Senior Business Intelligence Analyst\nManulife and John Hancock - Boston, MA\nAugust 2017 to February 2018\n* Involved in extraction of data from various sources (SQL Server, Oracle, flat files) into SQL server for reporting purposes.\n* Created and maintained weekly cycle-time Scorecard reports using excel for Corporate\nInsurance Operation Forecasting as per user requirements.\n* Created and maintained weekly Replacement Report using Oracle SQL Developer report\nfunction.\n* Designed and built standard reports and dashboard reports and created documents in MicroStrategy.\n* Created MDX SSAS cubes with fact and dimension tables with SQL server Analysis', u""Data Specialist\nBrigham and Women's Hospital - Boston, MA\nJune 2015 to July 2017\nCreated and maintained various regular and ad hoc reports including table reports, matrix\nreports, charts in different forms and formats such as cascading parameter reports,\ndashboard reports with drill down and drill through function using SSRS.\n* Developed cubes for various measure groups and dimensions in SQL Analysis Services\n(SSAS) and created dashboard using MDX and SQL to address business needs.\nPerformed data analysis using Data Mining Algorithms in SSAS to support client's\ndecision making.\n* Developed dynamic ETL packages in SSIS Visual Studio 2012 to load various sources\nof data (i.e. EMR, pharmacy and lab) into database and data mart using multiple tasks (Data\nflow task, Execute SQL task, Execute package task, File system task, script task, etc.),\ncontainers (Foreach loop, Sequence), and various transforms in data flow task (Derived\ncolumn, Lookup, Data conversion, Conditional split) and variables/parameters. Performed\ndata extraction and analysis utilized SQL Server applications/tools for reporting\nanalysis/development or for customer delivery.\n* Analyzed data for both reporting development as well as trend identification, event impact\nanalysis, process measurement/improvement, and observation/summarization for senior\nmanagement attention.\n* Gathered business requirements and functionally transformed them into technical\nrequirements.\n* Documented for ETL development, report creation and report maintenance and created\nuser-friendly manual for colleague share.\n* Performed data-analysis on diagnosis and antiretroviral treatment of HIV patients, and on pre-clinical screening of HIV infected cells that developed resistance to such treatment."", u'Data Analyst\nNortheastern University - Boston, MA\nSeptember 2014 to May 2015\nDesigned and constructed Access databases to organize and store graduate application\ndata. Later on created SQL Server databases and migrated data from access to SQL Server\ndatabases to meet the changing needs of education activity and maintained security of databases containing student-level data. Documented database standards for SQL Server\ndatabases.\n* Created reports and summaries in excel and access to assist management decision making\nin recruiting students and supporting the overall administrative goals of the University.\n* Configured SQL Server 2008 to adjust memory usage and file sizes. Created database\ntables using various constraints including primary key and foreign key.\n* Taught Kinetics and Unit Operation lab courses for undergraduate students.', u'Database Specialist/Scientist\nWorcester Polytechnic Institute - Worcester, MA\nAugust 2013 to August 2014\nPerformed data analysis for pre-clinical study of nanoparticle enhanced radiation therapy\nof tumor-bearing mice. The study design included pharmacologically effective dose range\n(i.e., optimal biological dose and minimally effective dose); dose schedule optimization;\ncomparison of therapeutic efficacy of maximum tolerated dose (MTD) with low-dose\nmetronomic regimen; evaluation of uptake and distribution of nanoparticles in tissue; and genotoxicity and cytotoxicity evaluations of radiation induced DNA damage.\n* Created reports with Excel and Crystal Report for project reporting and management.\n* Installed and configured SQL Server 2008 to adjust memory usage and file sizes. Created\nSQL Server databases and imported data (flat files and excel spreadsheets) into respective\ntables; continuous improved overall database performance; performed backup and restoration of databases.', u'Scientist/Data Analyst\nUniversity of Central Florida - Orlando, FL\nAugust 2007 to July 2013\nDesigned MS 2007/2010 Access databases, imported data from flat files and Excel\nspreadsheets into MS Access databases.\n* Created over 100 reports in Access and Excel formats for project reporting and operation;\nworked extensively with Excel (VLOOKUP, complex calculations, pivot tables\ncreation/manipulation, external data links, macros)\n* Installed and configured SQL Server 2008 database parameter files to adjust memory usage\nand file sizes. Created database tables in SQL Server using various constraints including\nprimary key and foreign key. Loaded data from flat data files into SQL Server 2008\ndatabase tables using bcp, bulk insert, and table export/import. Extracted data from SQL\nServer 2008 database tables into flat data files and Excel sheets.\n* Developed single cell array method for DNA damage and repair assay. Developed a VBA\n(Visual Basic for Applications) macro script to derive fluorescence intensity of DNA.\n* Performed data analysis for pre-clinical toxicity studies of nanoparticles at cellular level.\nThe study included the effects of size, shape, surface charge and surface chemistry of nanoparticle on toxicity in cultured cells. Performed various kinds of data modeling using\nadvanced excel features and related modeling software.\n* Performed data analysis for pre-clinical studies of nanoparticle enhanced cancer therapy.\nThe study design included conjugation and encapsulation of anticancer agents inside nanoparticles, targeted drug delivery, and controlled drug releasing.']","[u'Ph. D in Chemistry in Chemistry', u'Bachelor of Engineering in MicroStrategy']","[u'Duke University Durham, NC', u'Tianjin University Tianjin, CN']","degree_1 : Ph. D in Chemistry in Chemistry, degree_2 :  Bachelor of Engineering in MicroStrategy"
0,https://resumes.indeed.com/resume/bb68342645d0d66a,"[u""Data Scientist\nMass Mutual - Springfield, MA\nFebruary 2017 to Present\nDescription:\nMassachusetts Mutual Life Insurance Company operates as a mutual life insurance company in the United States. It offers life, disability income, and long-term care insurance.\n\nResponsibilities:\n\u2022 Performed Data Profiling to learn about behavior with various features such as traffic pattern, location, time, Date and Time etc.\n\u2022 Application of various machine learning algorithms and statistical modeling like decision trees, regressionmodels, neural networks, SVM, clustering to identify Volume using scikit-learn package in python, Matlab.\n\u2022 Utilized Spark, Scala, Hadoop, HBase, Cassandra, MongoDB, Kafka, Spark Streaming, MLLib, Python, a broad variety of machine learning methods including classifications, regressions, dimensionally reduction etc. and Utilized the engine to increase user lifetime by 45% and triple user conversations for target categories.\n\u2022 Developed Spark/Scala, Python for regular expression (regex) project in the Hadoop/Hive environment with Linux/Windows for big data resources. Used clustering technique K-Means to identify outliers and to classify unlabeled data.\n\u2022 Evaluated models using Cross Validation, Log loss function, ROC curves and used AUC for feature selection.\n\u2022 Design and develop state-of-the-art deep-learning / machine-learning algorithms for analyzing the image and video data among others.\n\u2022 Develop and implement innovative AI and machine learning tools that will be used in the Risk.\n\u2022 Analyze traffic patterns by calculating autocorrelation with different time lags.\n\u2022 Ensured that the model has low False Positive Rate.\n\u2022 Addressed overfitting by implementing of the algorithm regularization methods like L2 and L1.\n\u2022 Used Principal Component Analysis in feature engineering to analyze high dimensional data.\n\u2022 Created and designed reports that will use gathered metrics to infer and draw logical conclusions of past and future behavior.\n\u2022 Performed Multinomial Logistic Regression, Randomforest, Decision Tree, SVM to classify package is going to deliver on time for the new route.\n\u2022 Performed data analysis by using Hive to retrieve the data from Hadoopcluster, Sql to retrieve data from Oracle database.\n\u2022 Used MLlib, Spark's Machine learning library to build and evaluate different models.\n\u2022 Implemented rule based expertise system from the results of exploratory analysis and information gathered from the people from different departments.\n\u2022 Performed Data Cleaning, features scaling, features engineering using pandas and numpy packages in python.\n\u2022 Automatic Categorization on drug efficacy and side effect extraction were performed. Counter intuitive predictors were identified using machine-learning methods.\n\u2022 Developed MapReduce pipeline for feature extraction using Hive.\n\u2022 Created Data QualityScripts using SQL and Hive to validate successful data load and quality of the data. Created various types of data visualizations using Python and Tableau.\n\u2022 Communicated the results with operations team for taking best decisions.\n\u2022 Collected data needs and requirements by Interacting with the other departments.\n\nEnvironment: Python 2.x, CDH5, HDFS, Hadoop 2.3, Hive, Impala, Linux, Spark, Tableau Desktop, SQL Server 2012, Microsoft Excel, MATLAB, Spark SQL, PySpark, Machine learning."", u'Data Scientist\nBank Of Tokyo Jersey city, New Jersey - Jersey City, NJ\nDecember 2015 to January 2017\nDescription:\nThe Bank of Tokyo-Mitsubishi UFJ, Ltd. provides various banking and financial services to individuals and corporate customers in the Americas, Europe, the Middle East, Africa, Asia, and Oceania. The company operates through Retail Banking Business Unit, Corporate Banking Business Unit, Global Business Unit, Global Markets Unit, and Other Units segments. It offers corporate and investment banking services, including syndication and cross border syndication loans.\n\nResponsibilities:\n\u2022 Provided Configuration Management and Build support for more than 5 different applications, built and deployed to the production and lower environments.\n\u2022 Implemented public segmentation using unsupervised machine learning algorithms by implementing k-means algorithm using Pyspark.\n\u2022 Explored and Extracted data from source XML in HDFS, preparing data for exploratory analysis using data munging.\n\u2022 Responsible for different Data mapping activities from Source systems to Teradata\n\u2022 Handled importing data from various data sources, performed transformations using Hive, Map Reduce, and loaded data into HDFS\n\u2022 Used R and python for Exploratory Data Analysis, A/B testing, Anova test and Hypothesis test to compare and identify the effectiveness of Creative Campaigns.\n\u2022 Created clusters to classify Control and test groups and conducted group campaigns.\n\u2022 Analyzed and calculated the lifetime cost of everyone in the welfare system using 20 years of historical data.\n\u2022 Identify and assess available machine learning and statistical analysis libraries (including repressors, classifiers, statistical tests, and clustering algorithms).\n\u2022 Innovate and leverage machine learning, data mining and statistical techniques to create new, scalable solutions for business problems\n\u2022 Developed LINUXShell scripts by using NZSQL/NZLOAD utilities to load data from flat files to Netezza database.\n\u2022 Developed triggers, stored procedures, functions and packages using cursors and ref cursor concepts associated with the project using Pl/SQL\n\u2022 Created various types of data visualizations using R, python and Tableau.\n\u2022 Used Python, R, SQL to create Statistical algorithms involving Multivariate Regression, LinearRegression, PCA, Random forest models, Decision trees, Support Vector Machine for estimating the risks of welfare dependency.\n\u2022 Identified and targeted welfare high-risk groups with Machinelearningalgorithms.\n\u2022 Conducted campaigns and run real-time trials to determine what works fast and track the impact of different initiatives.\n\u2022 Developed Tableau visualizations and dashboards using Tableau Desktop.\n\u2022 Used Graphical Entity-Relationship Diagramming to create new database design via easy to use, graphical interface.\n\u2022 Created multiple custom SQLqueries in TeradataSQLWorkbench to prepare the right data sets for Tableau dashboards\n\u2022 Perform analyses such as regression analysis, logistic regression, discriminant analysis, cluster analysis using SAS programming.\n\u2022 Used Meta data tool for importing metadata from repository, new job categories and creating new data elements.\n\u2022 Scheduled the task for weekly updates and running the model in workflow. Automated the entire process flow in generating the analysis and reports.\n\nEnvironment:R 3.x, HDFS, Hadoop 2.3, Pig, Hive, Linux, R-Studio, Tableau 10, SQL Server, Ms Excel, PySpark, Machine learning.', u""Data Scientist\nTripAdvisor - New York, NY\nFebruary 2014 to November 2015\nDescription: TripAdvisor, Inc. is an American travel website company providing reviews of travel-related content. It also includes interactive travel forums. TripAdvisor was an early adopter of user-generated content. The website services are free to users, who provide most of the content, and the website is supported by an advertising business model.\n\nResponsibilities:\n\u2022 Involved in Design, Development and Support phases of Software Development Life Cycle (SDLC)\n\u2022 Performed data ETL by collecting, exporting, merging and massaging data from multiple sources and platforms including SSIS (SQL Server Integration Services) in SQL Server.\n\u2022 Worked with cross-functional teams (including data engineer team) to extract data and rapidly execute from MongoDB through MongDB connector for Hadoop.\n\u2022 Performed data cleaning and feature selection using MLlib package in PySpark.\n\u2022 Performed partitional clustering into 100 by k-means clustering using Scikit-learn package in Python where similar hotels for a search are grouped together.\n\u2022 Used Python to perform ANOVA test to analyze the differences among hotel clusters.\n\u2022 Implemented application of various machine learning algorithms and statistical modeling like Decision Tree, Naive Bayes, Logistic Regression and Linear Regression using Python to determine the accuracy rate of each model.\n\u2022 Created Machine Learning and statistical methods, (SVM, CRF, HMM, sequential tagging) or willingness to intensely learn.\n\u2022 Develop and implement innovative AI and machine learning tools that will be used in the Risk.\n\u2022 Determined the most accurately prediction model based on the accuracy rate.\n\u2022 Used text-mining process of reviews to determine customers' concentrations.\n\u2022 Delivered analysis support to hotel recommendation and providing an online A/B test.\n\u2022 Designed Tableau bar graphs, scattered plots, and geographical maps to create detailed level summary reports and dashboards.\n\u2022 Developed hybrid model to improve the accuracy rate.\n\u2022 Delivered the results to operation team for better decisions and feedbacks.\n\nEnvironment:Python, PySpark, Tableau, MongoDB, Hadoop, SQL Server, SDLC, ETL, SSIS, recommendation systems, Machine Learning Algorithms, text-mining process, A/B test, Machine learning."", u""Data Scientist\nBank of America - Wilmington, DE\nNovember 2012 to January 2014\nDescription: Bank of America is a multinational banking and financial services corporation. It is ranked 2nd on the list of largest banks in the United States by assets. As of 2016, Bank of America was the 26th largest company in the United States by total revenue.\n\nResponsibilities:\n\u2022 Participated in all phases of research including data collection, data cleaning, data mining, developing models and visualizations.\n\u2022 Collaborated with data engineers and operation team to collect data from internal system to fit the analytical requirements.\n\u2022 Redefined many attributes and relationships and cleansed unwanted tables/columns using SQL queries.\n\u2022 Utilized Spark SQL API in PySpark to extract and load data and perform SQL queries.\n\u2022 Performed data imputation using Scikit-learn package in Python.\n\u2022 Performed data processing using Python libraries like Numpy and Pandas.\n\u2022 Worked with data analysis using ggplot2 library in R to do data visualizations for better understanding of customers' behaviors.\n\u2022 Visually plotted data using Tableau for dashboards and reports.\n\u2022 Implemented statistical modeling with XGBoost machine learning software package using R to determine the predicted probabilities of each model.\n\u2022 Delivered the results with operation team for better decisions.\n\nEnvironment:Python, R, SQL, Tableau, Spark, Machine Learning Software Package, recommendation systems."", u""Python Developer\nCenvien Technologies - Hyderabad, Telangana\nFebruary 2011 to October 2012\nDescription: Cenvien technologies gather the requirements by listening and understanding to the client's business requirement to deliver quality products. It is highly qualified and strongly dedicated developing team that produces unique solutions.\n\nResponsibilities:\n\u2022 Developed entire frontend and backend modules using Python on Django Web Framework.\n\u2022 Implemented the presentation layer with HTML, CSS and JavaScript.\n\u2022 Involved in writing stored procedures using Oracle.\n\u2022 Optimized the database queries to improve the performance.\n\u2022 Designed and developed data management system using Oracle.\n\nEnvironment:MySQL, ORACLE, HTML5, CSS3, JavaScript, Shell, Linux & Windows, Django, Python"", u'Programmer Analyst\nPennar Industries Limited - Hyderabad, Telangana\nAugust 2009 to January 2011\nDescription:As a backend developer of web applications and data science infrastructure. The main area of focus is to come up with comprehensive solutions that need massive capacity and throughput.\n\nResponsibilities:\n\u2022 Effectively communicated with the stakeholders to gather requirements for different projects\n\u2022 Used MySQL db package and Python-MySQL connector for writing and executing several MYSQL database queries from Python.\n\u2022 Created functions, triggers, views and stored procedures using My SQL.\n\u2022 Worked closely with back-end developer to find ways to push the limits of existing Web technology.\n\u2022 Involved in the code review meetings.\n\nEnvironment:Python, MySQL.']",[u'Bachelor of Computer Science in TOOLS AND TECHNOLOGIES'],[u'Informatica Power Centre'],degree_1 : Bachelor of Compter Science in TOOLS AND TECHNOLOGIES
0,https://resumes.indeed.com/resume/9eeead187c8b402e,"[u'Data Scientist\nSears Holdings - Chicago, IL\nAugust 2017 to Present\nDescription: Sears Holdings. The Sears Holdings Corporation is an American holding company headquartered in Hoffman Estates, Illinois, a suburb of Chicago.\n\nResponsibilities:\n\u2022 Design, Develop and implement Comprehensive Data Warehouse Solution to extract, clean, transfer, load and manage quality/accuracy of data from various sources to EDW Enterprise Data Warehouse.\n\u2022 Architect framework for data warehouse solutions to bring data from source system to EDW and provide data mart solutions for Order/Sales operation, Salesforce activity, Inventory tracking, in depth data mining and analysis for market projection etc.\n\u2022 Utilized ApacheSpark with Python to develop and execute Big Data Analytics and Machine learning applications, executed machine learning use cases under Spark ML and Mllib.\n\u2022 Handled importing data from various data sources, performed transformations using Hive, Map Reduce, and loaded data into HDFS.\n\u2022 Utilized Spark, Scala, Hadoop, HBase, Cassandra, MongoDB, Kafka, Spark Streaming, MLLib, Python, a broad variety of machine learning methods including classifications, regressions, dimensionally reduction etc. and Utilized the engine to increase user lifetime by 45% and triple user conversations for target categories.\n\u2022 Designed and developed NLP models for sentiment analysis.\n\u2022 Designed and provisioned the platform architecture to execute Hadoop and machine learning use cases under Cloud infrastructure, AWS, EMR, and S3.\n\u2022 Developed and configured on Informatica MDM hub supports the Master Data Management (MDM), Business Intelligence (BI) and Data Warehousing platforms to meet business needs.\n\u2022 Transforming staging area data into a STAR schema (hosted on Amazon Redshift) which was then used for developing embedded Tableau dashboards\n\u2022 Worked on machine learning on large size data using Spark and Map Reduce.\n\u2022 Let the implementation of new statistical algorithms and operators on Hadoop and SQL platforms and utilized optimizations techniques, linear regressions, K-means clustering, Native Bayes and other approaches.\n\u2022 Developed Spark/Scala, Python for regular expression (regex) project in the Hadoop/Hive environment with Linux/Windows for big data resources.\n\u2022 Proficiency in SQL across a number of dialects (we commonly write MySQL, PostgreSQL, Redshift, Teradata, and Oracle)\n\u2022 Responsible for full data loads from production to AWSRedshift staging environment and Worked on migrating of EDW to AWS using EMR and various other technologies.\n\u2022 Worked on TeradataSQL queries, Teradata Indexes, Utilities such as Mload, Tpump, Fast load and FastExport.\n\u2022 Application of various machine learning algorithms and statistical modeling like decision trees, regression models, neural networks, SVM, clustering to identify Volume using scikit-learn package in python, Matlab.\n\u2022 Worked on data pre-processing and cleaning the data to perform feature engineering and performed data imputation techniques for the missing values in the dataset using Python.\n\u2022 Created Data Quality Scripts using SQL and Hive to validate successful data load and quality of the data. Created various types of data visualizations using Python and Tableau.\n\u2022 Worked with BTEQ to submit SQL statements, import and export data, and generate reports in Teradata.\n\u2022 Build and maintain scalable data pipelines using the Hadoop ecosystem and other open source components like Hive and HBase.\n\u2022 Created Hive architecture used for real time monitoring and HBase used for reporting and worked for map reduce and query optimization for Hadoop hive and HBase architecture.\n\u2022 Involved in Teradata utilities (BTEQ, Fast Load, Fast Export, Multiload, and Tpump) in both Windows and Mainframe platforms.\n\u2022 Built analytical data pipelines to port data in and out of Hadoop/HDFS from structured and unstructured sources and designed and implemented system architecture for Amazon EC2 based cloud-hosted solution for client.\n\nEnvironment:Erwin9.6.4, Oracle 12c, Python, Pyspark, Spark, Spark MLLib, Tableau, ODS, PL/SQL, OLAP, OLTP, AWS, Hadoop, Map Reduce, HDFS, Python, MDM, Teradata 15, Hadoop, Spark, Cassandra, SAP, MS Excel, Flat files, Tableau, Informatica, SSIS, SSRS, AWS EC2, AWS EMR, Elastic Search.', u""DataScientist\nAmerican college of cardiology - Washington, DC\nMay 2016 to July 2017\nDescription:The American College of Cardiology is a 54,000-member medical society that is the professional home for the entire cardiovascular care team.\nResponsibilities:\n\n\u2022 Provide data architecture support to enterprise data management efforts, such as the development of the enterprise data model and master and reference data, as well as support to projects, such as the development of physical data models, data warehouses and data marts.\n\u2022 Create new data designs and make sure they fall within the realm of the overall Enterprise BI Architecture and Building relationships and trust with key stakeholders to support program delivery and adoption of enterprise architecture.\n\u2022 Implemented HQL Scripts in creating Hive tables, loading, analyzing, merging, binning, backfilling, cleansing using hive.\n\u2022 Used R, SQL to create Statistical algorithms involving Multivariate Regression, Linear Regression, Logistic Regression, PCA, Random forest models, Decision trees, Support Vector Machine for estimating the risks.\n\u2022 Used R for Exploratory Data Analysis, A/B testing, Anova test and Hypothesis test to compare and identify the effectiveness of Creative Campaigns.\n\u2022 Providing technical leadership, mentoring throughout the project life-cycle, developing vision, strategy, architecture and overall design for assigned domain and for solutions and responsible for the development of target data architecture, design principles, quality control, and data standards for the organization\n\u2022 Developed and maintains data models and data dictionaries, data maps and other artifacts across the organization, including the conceptual and physical models, as well as metadata repository\n\u2022 Performed extensive Data Validation, Data Verification against Data Warehouse and performed debugging of the SQL-Statements and stored procedures for business scenarios.\n\u2022 Used Spark Data frames, Spark-SQL, Spark MLLib extensively and developing and designing POC's using Scala, Spark SQL and MLlib libraries.\n\u2022 Working on a Map RHadoop platform to implement Bigdata solutions using Hive, Map reduce, shell scripting and Pig.\n\u2022 Worked with cloud based technology like Redshift, S3, AWS, EC2 Machine, etc. and extracting the data from the Oracle financials and the Redshift database.\n\u2022 Used Teradata OLAP functions like RANK, ROW_NUMBER, QUALIFY, CSUM and SAMPLE.\n\u2022 Involved in designing and developing Data Models and Data Marts that support the Business Intelligence Data Warehouse.\n\u2022 Extensively used Aginity Netezza work bench to perform various DML, DDLetc operations on Netezza database.\n\u2022 Developed multiple MapReduce jobs in java for Data Cleaning and pre-processing analyzing data in PIG.\n\u2022 Worked on predictive and what-if analysis using R from HDFS and successfully loaded files to HDFS from Teradata, and loaded from HDFS to HIVE.\n\u2022 Designed the schema, configured and deployed AWS Redshift for optimal storage and fast retrieval of data.\n\u2022 NLTK, Stanford NLP, RAKE to preprocess the data, entity extraction and keyword extraction.\n\u2022 Transforming staging area data into a STAR schema (hosted on Amazon RedShift) which was then used for developing embedded Tableau dashboards\n\u2022 Developed SQL scripts for loading data from staging area to Target tables and worked on SQL and SAS script mapping.\n\u2022 Performed transformations of data using Spark and Hive according to business requirements for generating various analytical datasets.\n\u2022 Performed Multinomial Logistic Regression, Random forest, Decision Tree, SVM to classify package is going to deliver on time for the new route and Performed data analysis by using Hive to retrieve the data from Hadoop cluster, Sql to retrieve data from Oracle database.\n\u2022 Proposed the EDW data design to centralize the data scattered across multiple datasets and Worked on migrating of EDW to AWS using EMR and various other technologies.\n\u2022 Worked on the development of Data Warehouse, Business Intelligence architecture that involves data integration and the conversion of data from multiple sources and platforms.\n\u2022 Let the implementation of new statistical algorithms and operators on Hadoop and SQL platforms and utilized optimizations techniques, linear regressions, K-means clustering, Native Bayes and other approaches.\n\u2022 Created mapreduce running over HDFS for data mining and analysis using R and Loading & Storage data to Pig Script and R for MapReduce operations.\n\u2022 Worked on Teradata SQL queries, Teradata Indexes, Utilities such as Mload, Tpump, Fast load and Fast Export.\n\u2022 Used Metadata tool for importing metadata from repository, new job categories and creating new data elements and proficiency in SQL across a number of dialects (we commonly write MySQL, PostgreSQL, Redshift, SQL Server, and Oracle).\n\nEnvironment:Oracle 12c, SQL Plus, Erwin 9.6, MS Visio, SAS, Source Offsite (SOS), Hive, PIG, Windows XP, AWS, QC Explorer, Share point workspace, Teradata, Oracle, Agile, PostgreSQL, Data Stage, MDM , Netezza, IBM Infosphere, SQL, PL/SQL, IBM DB2, SSIS, Power BI, AWS Redshift, Business Objects XI3.5, COBOL, SSRS, QuickData, Hadoop, MongoDB, HBase, Hive, Cassandra, JavaScript."", u'Data Analyst\nFleetCor Technologies Inc - Norcross, GA\nJanuary 2015 to April 2016\nDescription:FleetCor Technologies, Inc. provides specialized payment products and services to Dataes, commercial fleets, oil companies, petroleum marketers, and government entities in North America, Europe, South Africa, and Asia.\nResponsibilities:\n\n\u2022 Created new reports based on requirements. Responsible in Generating Weekly ad-hoc Reports\n\u2022 Planned, coordinated, and monitored project levels of performance and activities to ensure project completion in time.\n\u2022 Automated and scheduled recurring reporting processes using UNIXshellscripting and Teradata utilities such as MLOAD, BTEQ and FastLoad.\n\u2022 Worked in a ScrumAgileprocess&WritingStories with two week iterations delivering product for eachiteration.\n\u2022 Worked on transferring the data files to vendor through SFTP&FTPprocess.\n\u2022 Involved in defining and Constructing the customer to customer relationships based on Association to an account & customer\n\u2022 Created action filters, parameters and calculatedsets for preparing dashboards and worksheets in Tableau.\n\u2022 Experience in performing Tableauadministering by using tableau admin commands.\n\u2022 Worked with architects and, assisting in the development of current and target state enterprise level data architectures\n\u2022 Worked with project team representatives to ensure that logical and physical data models were developed in line with corporate standards and guidelines.\n\u2022 Involved in defining the source to target data mappings, business rules and data definitions.\n\u2022 Responsible for defining the key identifiers for each mapping/interface.\n\u2022 Performed data analysis and data profiling using complex SQL on various sources systems including Oracle and Teradata.\n\u2022 Migrated three critical reporting systems to BusinessObjects and WebIntelligence on a Teradata platform\n\u2022 Created Excelcharts and pivot tables for the Adhocdatapull\n\nEnvironment: Teradata 13.1, Informatica 6.2.1, Ab Initio, Business Objects, Oracle 9i, PL/SQL, Microsoft Office Suite (Excel, Vlookup, Pivot, Access, Power Point), Visio, VBA, Micro Strategy, Tableau, UNIX Shell Scripting ERWIN.', u'Data Analyst\nCopartInc - Fairfield, CA\nMay 2013 to December 2014\nDescription:Copart, Inc. provides online auctions and vehicle remarketing services in the United States, Canada, and the United Kingdom.\nResponsibilities:\n\n\u2022 Experienced in developing business reports by writing complex SQLqueries using views, volatile tables\n\u2022 Experienced in Automating and Scheduling the TeradataSQLScripts in UNIX using KornShell scripting.\n\u2022 Wrote several TeradataSQLQueries using TeradataSQLAssistant for AdHocDataPullrequest.\n\u2022 Extensive experience in working with TableauDesktop, TableauServer, and TableauReader in various versions of Tableau7.0, 8.0, 8.3, 9.0 and 10 as a Developer and Analyst.\n\u2022 Analysis of functional and non-functional categorized data elements for data profiling and mapping from source to target data environment. Developed working documents to support findings and assign specific tasks.\n\u2022 Design and prototype of accurate and scalableprediction algorithms using R/RStudio.\n\u2022 Analyzed different types of data to derive insights about relationships between locations, statistical measurements and qualitatively assess the data using R/RStudio.\n\u2022 Data Profiling to help identify patterns in the source data using SQL and Informatica and thereby help improve quality of data and help business to understand the converted data better to come up with accurate business rules.\n\u2022 Involved with data profiling for multiple sources and answered complex business questions by providing data to business users.\n\u2022 Implemented Indexes, CollectingStatistics, and Constraints while creating table\n\u2022 Created action filters, parameters and calculated sets for preparing dashboards and worksheets in Tableau.\n\u2022 Design and deploy rich Graphicvisualizations with DrillDown and Dropdown menu option and Parameterized using Tableau.\n\u2022 Created side-by-sidebars, ScatterPlots, StackedBars, HeatMaps, FilledMaps and SymbolMaps according to deliverable specifications.', u""DataAnalyst\nFirst Indian Corporation Private Limited - Hyderabad, Telangana\nDecember 2011 to April 2013\nDescription:First Indian Corporation Private Limited, a member of The First American family of companies, provides specialized offshore transaction services, technology services, and analytics to the mortgage industry.\n\nResponsibilities:\n\u2022 Activating lease contracts to generate invoices for customer payments in a system, Pyramid.\n\u2022 Assigning work to the team members and handling in-house workflow system.\n\u2022 Handling review calls with front-end customers on process updates.\n\u2022 Being involved in the quality check process for peers\n\u2022 Work closely with the deal desk team and other business partners like corporate systems ops and finance, to ensure accurate quotes and contracts are quoted and booked.\n\u2022 Involved with various customers for the quick resolution of pending deals or orders; preparing operational metrics for the process.\n\u2022 Maintaining relationship with clients to achieve quality service norms by resolving their service related critical issues.\n\u2022 Handling escalation cases in SFDC (VmStarApplication of Vmware) with quality.\n\u2022 Preparing control reports for the team and self are accurate.\n\u2022 Looking after US&Domesticcustomers and following TAT.\n\u2022 Maintaining daily trackers of the team in processing orders.\n\u2022 Providing updates on University'sclosure and the action to be taken by CSR to understand the delivery time to reach customer.\n\u2022 Identify root cause analysis of all complaints and standardize processes aspect wise &recommend process improvements. Conduct or coordinate gaps in internal business process are verified and fixed. Identify critical processes, schedule and audit processes/practices for compliance and performance, generate report, follow-up for closing of gaps\n\u2022 Supporting backend processes and ensuring that the orders are dispatched in warehouse and finally to end customer.\n\u2022 Ensuring SLA and TAT for all the process, which includes Singapore and US customers as well.\n\nEnvironment: Oracle 10g, MS-Office, Teradata, Tableau."", u'MS-Office, Teradata, Tableau\nJanuary 2013 to January 2013', u'System Analyst\nAccenture - Bengaluru, Karnataka\nMay 2009 to November 2011\nDescription: Accenture PLC is a global management consulting and professional services company that provide strategy, consulting, digital, technology and operations.\n\nResponsibilities:\n\u2022 OABS Analytics team is mainly into reporting and creation of dashboards and Digitization of reports for all the Practices.\n\u2022 Design and develop the Headcountreports for worldwide regions and for all the practices.\n\u2022 Validation of data to check the accuracy of it.\n\u2022 Analytics team is mainly into reporting and creation of dashboards and Digitization of reports.\n\u2022 Automate report to pull data from database directly to excel and make the formatting and other changes to the report.\n\u2022 Design and develop the sales standard reports for APJ regions and area across the client locations.\n\u2022 Prepare the Weekly monthly and fortnight sales report.\n\u2022 Validate the report in order to check the accuracy of it.\n\u2022 Identify trends and present results in chart format.\n\u2022 Manage, organize & update relevant data to support business activities\n\u2022 Check source data in order to validate completeness & accuracy\n\u2022 Involved in automation of reports, was a go to person for all automation on all MSofficetools.\n\u2022 Worked on VBAprojects as integration of Excel-PowerPoint and Excel-Access etc.\n\u2022 Used charts, Pivots and other complex excel function to automate and improve the productivity of the team.\n\u2022 Frequent Adhoc request to support key business requirement.\n\u2022 Providing Content and Data Managementservices to Ford of Europe being in Marketing Sales & Service team.\n\u2022 Involved in report writing and business process development.\n\u2022 Modified reports and make proper evaluations.\n\u2022 Validated data to ensure the quality, validity and accuracy of content.\n\u2022 Claim processing for both Indian and European market (Retail Claims, Fleet Claims, and Warranty Claims etc.).\n\nEnvironment:Adhoc, VBA, SQL/Server, Oracle 9i, MS-Office, Teradata.']",[],[],degree_1 : 
0,https://resumes.indeed.com/resume/3e62d95d64d9002c,"[u""Data Scientist\nActive Health Management - Alpharetta, GA\nSeptember 2016 to Present\nAlpharetta, GA September 2016- Present\nActive Health Management is a global leader in the creation of innovative prescription medicines for all health, mental health, and anesthesia - products that contribute to the health of people and their quality of life. Its objective is to be a reliable partner of health care professionals and patients by providing products and services for the worldwide improvement of human health and quality of life.\nThe project mostly consists of data processing activities and analyzing data to build patient and treatment models, capture patient and treatment behaviors and aggregate rules for customer treatment classification there by creating customer treatment recommendations.\nDesignation: Data Scientist\n\u2022 Experience working in Data Requirement analysis for transforming data according to business requirements.\n\u2022 Applied Forward Elimination and Backward Elimination for data sets to identify most statically significant variables and to remove insignificant variables for Data analysis and to get better predictive insights.\n\u2022 Utilized Label Encoders in Python to convert non-numerical significant variables to numerical significant variables to identify their impact on pre-acquisition and post acquisitions by using 2 sample paired t test.\n\u2022 Worked with ETL SQL Server Integration Services (SSIS) for data investigation and mapping to extract data and applied fast parsing and enhanced efficiency by 17%.\n\u2022 Developed Data Science content involving Data Manipulation and Visualization, Web Scraping, Machine Learning, Python programming, SQL, GIT and ETL for Data Extraction.\n\u2022 Created A/B testing and Multivariate testing to check the performance of Patient's applications.\n\u2022 Developed Analytical systems, data structures, gather and manipulate data, using statistical techniques.\n\u2022 Designing suite of Interactive dashboards, which provided an opportunity to scale and measure the statistics of the HR dept. which was not possible earlier and schedule and publish reports.\n\u2022 Provided and created data presentation to reduce biases and telling true story of people by pulling millions of rows of data using SQL and performed Exploratory Data Analysis.\n\u2022 Applied breadth of knowledge in programming (Python, R), Descriptive, Inferential, and Experimental Design statistics, advanced mathematics, and database functionality (SQL, Hadoop).\n\u2022 Migrated data from Heterogeneous Data Sources and legacy system (DB2, Access, Excel) to centralized SQL Server databases using SQL Server Integration Services (SSIS).\n\u2022 Applied Descriptive statistics and Inferential Statistics on varies data attributes using SPSS to draw better insights of data and to provide products and services for patients.\n\u2022 Developed Machine learning algorithms such as Collaborative filtering, Neural Network models, Hybrid recommendation model and NLP for analyzing most significant variables to get better predictive insights.\n\u2022 Rapidly evaluated Machine learning algorithms and Deep Learning frameworks, tools, techniques and approaches for deployment in AWS and consumption of data science teams.\n\u2022 Utilized NLP algorithms with help of NLTK and Genism libraries to recognize text from the patient's reviews.\n\u2022 Utilized data reduction techniques such as Factor analysis to identify most correlated values to underlying factors of the data and categorized the variable according to factors.\n\u2022 Applied Wilcoxon sign test to patient and treatment data for pre-acquisition and post-acquisition for different sectors to find the statistical significance in R programming.\n\u2022 Performance Tuning: Analyze the requirements and fine tune the stored procedures/queries to improve the performance of the application.\n\u2022 Designed and built scalable infrastructure to support real- time analytics with the help of Scala.\n\u2022 Used Spark Streaming to divide streaming data into batches for batch processing.\n\u2022 Developed various Tableau9.4 Data Models by extracting and using the data from various sources files, DB2, Excel, Flat Files and Bigdata.\n\u2022 Handled importing data from various data sources, performed transformations using Hive, Map Reduce, and loaded data into HDFS by using HQL queries in Hadoop.\n\u2022 Utilized Amazon Web Services (AWS) S3, EC2, EMR and RDS, Redshift to setup storage for deploying different developed machine learning models.\n\u2022 Setting up EC2 instances and deployment of patient applications and treatment records.\n\u2022 Interaction with Business Analyst, SMEs and other Data Architects to understand Business needs and functionality for various project solutions.\n\u2022 Identifying and executing process improvements, hands-on in various technologies such as Oracle, Informatica, and Business Objects.\n\u2022 Designed both 3NF data models for ODS, OLTP systems and dimensional data models using Star and Snowflake Schemas.\n\nEnvironment: Python, Jupyter, Tensor Flow, Keras, Theano, R Programming, SPSS, SQL Server 2014, SSRS, SSIS, SSAS, Spark, Scala, AWS, Microsoft office, SQL Server Management Studio, Business Intelligence Development Studio, MS Access, Informatica, SAP Business Objects and Business Intelligence."", u""Data Scientist\nMISO - Carmel, IN\nJune 2015 to August 2016\nCarmel, IN June 2015- August 2016\nMISO is a cost-effective delivery of electric power consumption in Carmel, IN that specializes in power service. The company wanted to enhance the business to find new ways of bringing more customers.\nThe project involves data extraction and applying data integrity and analytical techniques for story telling from the data. The major key performance indicators such as power reliability and improved interconnected transmission data etc. using supervised and unsupervised machine learning techniques.\nDesignation: Data Scientist\n\u2022 Involved in gathering, analyzing and translating business requirements into analytic approaches.\n\u2022 Worked with Machine learning algorithms like Neural network models, Linear Regressions (linear, logistic etc.), SVM's, Decision trees for classification of groups and analyzing most significant variables.\n\u2022 And utilized SAS for developing Pareto Chart for identifying highly impacting categories in modules to find the work force distribution and created various data visualization charts.\n\u2022 Performed univariate, bivariate and multiple analysis of approx. 4890 tuples using bar charts, box plots and histograms.\n\u2022 Participated in features engineering such as feature creating, feature scaling and One-Hot encoding with Scikit-learn.\n\u2022 Converted raw data to processed data by merging, finding outliers, errors, trends, missing values and distributions in the data.\n\u2022 Generated detailed report after validating the graphs using R and adjusting the variables to fit the model.\n\u2022 Worked on Clustering and factor analysis for classification of data using machine learning algorithms.\n\u2022 Developed Descriptive statistics and inferential statistics for Logistics optimization, Value throughput data to at 95% confidence interval.\n\u2022 Imported data by using Power Query in MS Excel from API's and Web API's then created relationship between data tables by using Power Pivot.\n\u2022 Used Power Map and Power View to represent data very effectively to explain and understand technical and non-technical users.\n\u2022 Written MapReduce code to process and parsing the data from various sources and storing parsed data into HBase and Hive using HBase - Hive Integration.\n\u2022 Created SQL tables with referential integrity and developed advanced queries using stored procedures and functions using SQL server management studio.\n\u2022 Used TensorFlow, Keras, Theano, Pandas, NumPy, SciPy, Scikit-learn, NLTK in Python for developing various machine learning algorithms such as Neural network models, Linear Regression, multivariate regression, na\xefve Bayes, random Forests, decision trees, SVMs, K-means and KNN for data analysis.\n\u2022 Responsible for developing data pipeline with AWS S3 to extract the data and store in HDFS and deploy implemented all machine learning models.\n\u2022 Used Spark and Spark-SQL/Streaming for faster testing and processing of data.\n\u2022 Used packages like dplyr, tidyr and ggplot2 in R Studio for data visualization and generated scatter plot and high low graph to identify relation between different variables.\n\u2022 Worked on Business forecasting, segmentation analysis and Data mining and prepared management reports defining the problem; documenting the analysis and recommending courses of action to determine the best outcomes.\n\u2022 Experience with risk analysis, root cause analysis, cluster analysis, correlation and optimization and K-means algorithm for clustering data into groups.\n\u2022 Coordinated with data scientists and senior technical staff to identify client's needs.\n\nEnvironment: SQL Server 2012, Python, Jupyter, R 3.1.2, MATLAB, SSRS, SSIS, SSAS, MongoDB, HBase, HDFS, Hive, Pig, SAS, Power Query, Power Pivot, Power Map, Power View, Microsoft office, SQL Server Management Studio, Business Intelligence Development Studio, MS Access."", u""Data Analyst\nEducational Testing Service - Princeton, NJ\nJanuary 2014 to May 2015\nPrinceton, NJ January 2014- May 2015\nEducational Testing Service is a leading testing service organization. It provided and maintained all testing information of test takers. The project consisted of data preprocessing activities from OLTP server using ETL and analyzing data to build testing report and test models, capture test taker behaviors and aggregate rules for test takers by their recommendations.\nDesignation: Data Analyst\n\u2022 Developed complex SQL queries using group by, join, where clause to answer tester questions.\n\u2022 Communicated and coordinated with other departments to collect Business Requirement Analysis and developed data flow mapping to load OLAP server for analysis of policies details.\n\u2022 Worked on missing value imputation, outlier's identification using Random Forest and Box Plots.\n\u2022 Tackled highly imbalanced dataset using under sampling with ensemble methods, oversampling and cost sensitive algorithms.\n\u2022 Improved prediction performance by using random forest and gradient boosting for feature selection with the help of Scikit-learn library in Python.\n\u2022 Utilized Parametric and Non-Parametric test in SPSS to draw insights from data for making business decisions.\n\u2022 Implemented machine learning models (logistic regression, XGboost) with the help of Scikit-learn in Python.\n\u2022 Validated and selected models using k-fold cross validation, confusion matrices and worked on optimizing models for high recall rate.\n\u2022 Involved with data profiling for multiple sources and answered complex business questions by providing data to business users.\n\u2022 Participated in Agile planning process and daily scrums, providing details and discuss with team lead, Data Scientist, Data Analyst, Data Engineer and others.\n\u2022 Experience with routine DBA activities like Query Optimization, Performance Tuning and Effective SQL Server configuration for better performance and cost reduction.\n\u2022 Developed Tabular Reports, Sub Reports, Matrix Reports, drill down Reports and Charts using SQL Server Reporting Services (SSRS).\n\u2022 Designed rich data visualizations with Tableau 9.4 and dynamic dashboards for business analysis.\n\nEnvironment: SQL Server 2012, R programming, Python, MATLAB, SSRS, SSIS, SSAS, SPSS, Tableau, Minitab, Microsoft office, SQL Server Management Studio, Business Intelligence Development Studio, MS Access."", u""Data Analyst\nDecision Craft - Bengaluru, Karnataka\nSeptember 2012 to December 2013\nBangalore, India September 2012- December 2013\nDecision Craft is a provider of information technology (IT) services. The Company delivers a range of IT services through globally integrated onsite and offshore delivery locations. It offers its services to customers through industry-focused practices, including insurance, manufacturing, financial services, healthcare and telecommunications, and through technology-focused practices.\nThe project consisted of extracting data from various sources and transforming the data according to the business requirements and loading data to target destination. The SQL Server Integration Services (SSIS) is used to create mapping for automating data pipelines from OLTP to OLAP. The Data cleansing and Data integrity checks are performed on data and customized business report are developed and deployed on to SQL Server Reporting Services (SSRS) server for making Business Decisions.\nDesignation: Data Analyst\n\u2022 Gathered requirements and documented requirements with Use cases in Requisite Pro and created different traceability views by MS Visio.\n\u2022 Worked with Data Compliance teams to identify the most suitable source of record and outlined the data.\n\u2022 Functioned with project team representatives to ensure that logical and physical ER/Studio data models were established in line with business standards and guidelines.\n\u2022 Deeply analyzed the clients' data by using SQL Server Analytic Services (SSAS).\n\u2022 Completed data cleaning process to discover like taking care of missing values by utilizing strategies, like, supplanting by mean, forward/backward fill, evacuating whole rows/columns/values, expelling outliers and errors, normalizing, and scaling information in data set.\n\u2022 Implemented metadata repository, maintained data quality, data cleanup procedures, transformations, data standards, data governance program, scripts, stored procedures, triggers and executed test plans.\n\u2022 Worked with team by Extracting Mainframe Flat Files (Fixed or CSV) onto UNIX Server and then converting them into Teradata Tables using SQL Server Integration Services (SSIS).\n\u2022 Responsible for report generation using SQL Server Reporting Services (SSRS) and Crystal Reports based on business requirements and connect with Teradata base for generating daily reports.\n\u2022 Developed visualizations and dashboards using Tableau to present analysis outcomes in terms of patterns, anomalies, and predictions use of bar charts, Scatter Plots, 3D plots, and histograms.\n\u2022 Documented the complete process flow to describe program development, logic, testing, implementation, application integration, and coding by using SQL Server Reporting Services (SSRS).\n\u2022 Created customized SQL Queries using SQL Server 2008/2008R2 Enterprise to pull specified data for analysis and report building in conjunction with Crystal Reports.\n\u2022 Designed & developed various Ad hoc reports for different teams in Business (Teradata and MS ACCESS, MS EXCEL).\n\nEnvironment: SSRS, SSIS, SSAS, SQL Server 2008/2008 R2 Enterprise, Tableau, MS Visio, MS Excel, MS Project, Teradata, Crystal Reports, ER Studio, Crystal reports, and Business Objects.""]",[u'Bachelor of Technology in Technology'],[u'Jawaharlal Nehru technological university'],degree_1 : Bachelor of Technology in Technology
0,https://resumes.indeed.com/resume/e3233768beed9c5b,"[u'Data Scientist Intern\nTuutkia - San Francisco Bay Area, CA\nJanuary 2018 to Present\n\u2022 Collecting and analyzing sales lead patterns and generating visualization charts of leads contacts profiles\n\u2022 Analyzing patterns to differentiate trusted and fraud leads and enabling voice recognition features using Python\n\u2022 Enabled the profanity filter for the voice recording using Google Voice recognition API\n\u2022 Collected the reviews about the company on various sites using Web Scraping', u'Data Science Intern\nRolyte - San Francisco Bay Area, CA\nJanuary 2018 to Present\n\u2022 Developed AI models by deriving results from patient\u2019s data and medical records of similar symptoms\n\u2022 Developing Deep learning models for detecting the skin related diseases using Image Classification\n\u2022 Scraped the data from json files and resized the images to pass in to CNN\n\u2022 Developing a platform to detect Skin Cancer based on Images of the tumors using Deep Learning methodologies', u'Data Scientist\nAccenture\nJuly 2014 to July 2016\n\u2022 Reduced dataset dimensionality by performing Principal Component Analysis using Python and R\n\u2022 Improved prediction accuracy by 10% implementing a regression algorithm called RandomForestRegressor\n\u2022 Achieved a recall rate of 93% by building a model to determine fraud transactions using SVM classifications\n\u2022 Predicted house prices by developing a predictive analytics platform using Python and Tableau\n\u2022 Rectified a defect by correcting the logic in 2 procedures and 6 functions using Dynamic SQL and queries']","[u""Master's in Computer Science in Computer Science"", u""Bachelor's in Computer Science in Cloud Computing for Data Analysis""]","[u'University of North Carolina at Charlotte Charlotte, NC\nDecember 2017', u'Vellore Institute of Technology\nMay 2014']","degree_1 : ""Masters in Compter Science in Compter Science"", degree_2 :  ""Bachelors in Compter Science in Clod Compting for Data Analysis"""
0,https://resumes.indeed.com/resume/5b02b61ca458f2ff,"[u'Lead Data Scientist\nTEKsystems\nOctober 2017 to Present\nAs a lead data scientist, analyzing video user experience, to identify causes contributing to degraded customer experience across different backend systems, applications and devices.\n\n\u2022 Gathered structured and semi-structured data from different sources and analyzed large datasets\n\u2022 Created different visualizations for analyzing Time to First Frame, Buffering and Errors.\n\u2022 Explored and developed various clustering models using machine learning algorithms for analyzing latency for across devices.\n\nTools Used: Spark, Scala, Python (scikit-learn, numpy, pandas, scipy & Matplotlib), AWS, Tableau, Data Bricks. Zeppelin, Oracle, Apache Kafka.', u'Lead Developer / Data Scientist\nBank of America\nJanuary 2016 to September 2017\nAs part of CPRT (Consolidated, PnL, Risk, and Trades) initiative, involved in building Enterprise wide risk framework using Quartz platform as the repository for Global markets trade, Risk and PnL calculations to replace legacy systems. Downstream consumers including Sub ledgers, Market Risk, Credit Risk, Liquidity Risk, Independent Price Verifications, Client Valuations and PnL Attribution.\n\u2022 Design & developed using agile various risk measures & scenarios for securities such as Muni Bond, IR Swap, CDS & TRS using tools such as Python, J2EE, NoSQL & Sandra DB\n\u2022 Worked with quant group for developing pricing model for various derivatives.\n\u2022 Developed PoC for creating various risk buckets for Risk Managers with various risk measures & scenarios using clustering algorithm & created various data visualizations.\n\u2022 Extensively used Python modules such as scikit-learn, numpy, pandas, scipy & Matplotlib.', u""Data Scientist\nCSC\nJanuary 2014 to December 2015\nResponsible for implementing data mining and statistical machine learning solutions to various business problems for FIS's clients.\n\u2022 Used transaction histories to develop new behavioral models of retail bank customers, who are at risk of switching.\n\u2022 Developed customer segmentation using clustering algorithm to promote cross selling.\n\u2022 Using 18 months period web server log to analyzed online behavior of customers to improve marketing campaigns.\n\u2022 Experienced in Regression models, PCA, Decision Tree Analysis, Clustering & Statistical methods.\n\u2022 Practical experience with tool such as R, Python, Hadoop, AWS, Azure & NoSQL.\n\u2022 Adept and deep understanding of Statistical modeling, Multivariate Analysis, model testing, problem analysis, model comparison and validation."", u'Implementation Lead\nCSC\nJanuary 2011 to December 2013\nManage online banking product (FIS Profile) implementation / upgrade for several banking clients such as GE Capital, Barclays, Schwab. Plan and coordinate all aspects of a core banking product implementation from initiation through delivery using J2EE architecture & NoSQL database.\n\u2022 Manage product customizations and upgrades. Conduct GAP analysis and define development methodology & testing strategy for the product upgrade / implementation.\n\u2022 Lead technical design and development of feed files for Prime AML (Anti Money Laundering) system to export customer, account, and activity information into BSA (Bank Secrecy Act) Reporter.\n\u2022 Integrated the real-time interface with RFC (Risk, Fraud and Compliance) system for new customer identity verification and authorization using Web Services.\n\u2022 Integrated ACH Risk Manager process to assess fraud risk for external fund transfers\n\u2022 Involved in implementing SOAP Web Services using Xpress product.\n\u2022 Manage testing scope, approach, resources, and environments for the application testing.\n\u2022 Manage regression testing cycles using automated test suite and co-ordinate interface testing with upstream and downstream systems.\n\u2022 Lead development of Touch Point Sales and Service product using agile methodology & J2EE architecture and lead team of developers, Business Analysts and testers', u'Tech Lead\nCSC\nOctober 2000 to December 2010\nAs a hands-on tech lead, successfully delivered several J2EE based projects such as OTC Bond Option Processing system, Cash Settlement system, Interest Claims Manager, Inbound Queue Manager, Cash Manager reporting in the back office fixed income and derivative group for Merrill Lynch.\n\n\u2022 Analyzed and implemented Straight Through Processing (STP) for option processing systems that reduced user time by 50%.\n\u2022 Recommended and implemented a reporting system using open source tool Jasper Reports that significantly reduced costs.\n\u2022 Interacted with traders and business users to gather business requirements, and performed gap analysis with existing systems.\n\u2022 Implemented technically complex projects using J2EE/CORBA.\n\u2022 Contributed to design and development of Graphical User Interfaces (GUI) using Swing for applications such as OPUS and JCash.\n\u2022 Designed generalized Trade Capture for OTC Bond Option system. Coordinated with the upstream and downstream systems, and designed feed files and messaging layers.\n\u2022 Handled cash reconciliations among different systems. Participated in creation of logical and physical database design and migrating exiting data into the new system.\n\u2022 Implemented peer code review process that helped to identify defects in the early stage.\n\u2022 Scheduled tasks with clear priorities and directions for offshore teams, producing excellent results in delivery.\n\u2022 Handled various knowledge transfer sessions for offshore teams.\n\u2022 Assigned technical tasks and milestones, measured progress and maintained project metrics. Conducted periodic team meetings with project stakeholders, and resolved outstanding issues in a timely manner.\n\u2022 Planned and executed training sessions including project management, business analysis and various technical subjects. Motivated cross-training sessions with teams, improving employee understanding of the client\'s business. Organized monthly social events that improved employee cohesiveness.\n\u2022 Conducted quarterly performance reviews with realistic, measurable and specific goal setting that enhanced employee performance. Reshaped teams from flat to well structured and integrated, creating more opportunities for employees to move up to leadership roles.\n\u2022 Developed ongoing consulting relationships with clients as part of retention and growth strategy of account base.\n\u2022 Involved in development of OTC Derivatives settlement system using industry centralized DTCC settlement system. Generated and parsed SWIFT messages.\n\u2022 Contributed in database and managed storage sizing for application with the SA and DBA. Handled SQL performance tuning.\n\u2022 Involved in reengineering a Cash Settlement application from C++ to Java. Designed framework for Java applications.\n\u2022 Managed application servers and software upgrades to utilize new features and maximize application performance in a timely fashion.\n\u2022 Instituted strict coding standards and detailed documentations which improved better understanding of the system for newcomers.\n\u2022 Enforced code reviews that improved system performance and identified ""bugs"" in the early development cycle.\n\u2022 Guided projects to on-time completion within budget, achieving target objectives, and led technical team through relevant phases of the SDLC.\n\u2022 Participated in contingency & Recovery Testing, systemized SOX compliance & auditing requirements for applications in accordance with policy, and represented audits.\n\u2022 Provided industry standard, cost-effective and optimized solutions resulting in long-term client relationships.\n\u2022 Managed through all release cycles of the application using well defined quality processes and procedures, and coordinating QA, system and user acceptance testing.\n\u2022 Involved in presales activities, handled RFI/RFP responses and SOW / contract negotiations for numerous clients.', u""Lead Developer\nCSC\nSeptember 1998 to September 2000\nWorked for a dotcom client, Medsite.com, to launch different websites such as Journal Tracker, Order processing, EDI, Credit card processing & Med Calendar and handled technology integration for the client's M&A activities. Facilitated Medsite.com in launching a range of products, and handled technology integration for its M&A activities. Handled EDI & credit card integration for MedBookStore.com. Enabled Medsite.com to launch a premier e-commerce product, Medical Journal Tracker, in record time, helping the client to grow massively in the dot com sector. Guided a reporting project for Customer Service that improved customer response time by 50%."", u'Associate\nCognizant Technology Solutions - Chennai, Tamil Nadu\nJanuary 1996 to January 1998\nLed 12-person development team to design and develop products for Radio Spot Buying System, worked on various projects for Neilsen Media Research, developed test plans and cases, and maintained ISO 9001 compliant project documents, and represented audits.', u'Software Engineer\nEco Tech - Chennai, Tamil Nadu\nJanuary 1995 to January 1996\nDesigned a product for the textile and garment exports industry using client server technology. Involved in GUI and database development of process planning and subcontracting modules using Power Builder and Sybase.', u'Software Engineer\nAbacus Systems - Bengaluru, Karnataka\nJanuary 1992 to January 1995\nSoftware requirement analysis, functional design and prototyping. Designed applications for manufacturing, financial accounting, banking and transportation management.']","[u'MBA in Finance & Business', u'BS in Computer Science & Engineering']","[u'Rutgers School of Business Management', u'Bharathiar University']","degree_1 : MBA in Finance & Bsiness, degree_2 :  BS in Compter Science & Engineering"
0,https://resumes.indeed.com/resume/ae6d6f71c7c95efb,"[u""Data Scientist\nVerizon, TX\nFebruary 2017 to Present\nDescription: Verizon Wireless is an American telecommunications company, a wholly owned subsidiary of Verizon Communications, which offers wireless products and services. Verizon Wireless is the largest wireless telecommunications provider in the United States.\nThe company is headquartered in Basking Ridge, New Jersey. It was founded in 2000 as a joint venture of American telecommunications firm Bell Atlantic, which would soon become Verizon Communications, and British multinational telecommunications company Vodafone.\nResponsibilities:\n\n\u2022 Responsible for working with various teams on a project to develop analytics based solution to target roaming subscribers specifically.\n\u2022 Leading a team of 4 data analysts and created multi-dimensional segmentation to define specific cohorts of subscribers.\n\u2022 Preparing the travel prediction model that can predict subscribers' future travel behavior up to a month in advance.\n\u2022 Combination of these elements (travel prediction & multi-dimensional segmentation) would enable operators to conduct highly targeted and personalized roaming services campaigns leading to significant subscriber uptake.\n\u2022 Scaled up to Machine Learning pipelines: 4600 processors, 35000 GB memory achieving 5-minute execution.\n\u2022 Deployed GUI pages by using JSP, JSTL, HTML, DHTML, XHTML, CSS, JavaScript, AJAX.\n\u2022 Configured the project on WebSphere 6.1 application servers\n\n\u2022 Developed a Machine Learning test-bed with 24 different model learning and feature learning algorithms.\n\u2022 By thorough systematic search, demonstrated performance surpassing the state-of-the-art (deep learning).\n\n\u2022 Up to 10 times more accurate predictions over existing state-of-the-art algorithms.\n\u2022 Developed in-disk, huge (100GB+), highly complex Machine Learning models.\n\u2022 Used SAX and DOM parsers to parse the raw XML documents\n\u2022 Used RAD as Development IDE for web applications.\n\n\u2022 Develop and plan required analytic projects in response to business needs.\n\u2022 In conjunction with data owners and department managers, contribute to the development of data models and protocols for mining production databases.\n\u2022 Develop new analytical methods and/or tools as required.\n\u2022 Contribute to data mining architectures, modeling standards, reporting, and data analysis methodologies.\n\u2022 Conduct research and make recommendations on data mining products, services, protocols, and standards in support of procurement and development efforts.\n\u2022 Work with application developers to extract data relevant for analysis.\n\u2022 Collaborate with unit managers, end users, development staff, and other stakeholders to integrate data mining results with existing systems.\n\u2022 Provide and apply quality assurance best practices for data mining/analysis services.\n\nEnvironment: R 9.0, Informatica 9.0, ODS, OLTP, Oracle 10g, Hive, OLAP, DB2, Metadata, MS Excel, Mainframes MS Vision, Rational Rose."", u'Data Scientist\nMarvell Technology Group - Santa Clara, CA\nDecember 2015 to January 2017\nDescription: Marvell Technology Group Limited is a producer of storage, communications and consumer semiconductor products. Marvell\'s U.S. operating headquarters is located in Santa Clara, California, and the company operates design centers in places including Canada, Europe, Israel, India, Singapore and China. Marvell is a ""fabless"" manufacturer of semiconductors. Its market segments include the enterprise, cloud, automotive, industrial and consumer markets.\n\nResponsibilities:\n\u2022 Developed the prediction model for crop yield, based on different kinds of field, weather and imagery data.\n\u2022 Exploratory data analysis and Feature engineering to best fit the regression model.\n\u2022 Designed a static pipeline in MS Azure for data ingestion and dashboarding. Used MS ML Studio for modeling and MS Power BI for dash boarding.\n\u2022 Analyze large datasets to provide strategic direction to the company.\n\u2022 Perform quantitative analysis of product sales trends to recommend pricing decisions.\n\u2022 Conduct cost and benefits analysis on new ideas.\n\u2022 Scrutinize and track customer behavior to identify trends and unmet needs.\n\u2022 Develop statistical models to forecast inventory and procurement cycles.\n\u2022 Assist in developing internal tools for data analysis.\n\u2022 Advising on the suitability of methodologies and suggesting improvements.\n\u2022 Carrying out specified data processing and statistical techniques.\n\u2022 Supplying qualitative and quantitative data to colleagues & clients.\n\u2022 Using Informatica & SAS to extract transform & load source data from transaction systems.\n\u2022 Creating data pipelines using big data technologies like Hadoop, spark etc.\n\u2022 Creating statistical models using distributed and standalone models to build various diagnostics, predictive and prescriptive solution.\n\u2022 Utilize a broad variety of statistical packages like SAS, R , MLIB, Graphs, Hadoop, Spark , MapReduce, Pig and others\n\u2022 Refine and train models based on domain knowledge and customer business objectives\n\u2022 Deliver or collaborate on delivering effective visualizations to support the client business objectives\n\u2022 Communicate to your peers and managers promptly as and when required.\n\u2022 Produce solid and effective strategies based on accurate and meaningful data reports and analysis and/or keen observations.\n\u2022 Establish and maintain communication with clients and/or team members; understand needs, resolve issues, and meet expectations\n\u2022 Developed web applications using .net technologies; work on bug fixes/issues that arise in the production environment and resolve them at the earliest\nEnvironment: SQL Server 2008R2/2005 Enterprise, SSRS, SSIS, Crystal Reports, Hadoop, Windows Enterprise Server 2000, DTS, SQL Profiler, and Query Analyzer.', u""Data Scientist\nEbay Inc - San Jose, CA\nFebruary 2014 to November 2015\nDescription: Ebay is a multinational e-commerce corporation, facilitating online consumer-to consumer and business-to-consumer sales. It is headquartered in San Jose, California. eBay was founded by Pierre Omidyar in 1995, and became a notable success story of the dot-com bubble.\nResponsibilities:\n\u2022 Developed scalable machine learning solutions within a distributed computation framework (e.g. Hadoop, Spark, Storm etc.).\n\u2022 Utilizing NLP applications such as topic models and sentiment analysis to identify trends and patterns within massive data sets.\n\u2022 Creating automated anomaly detection systems and constant tracking of its performance\n\u2022 Strong command of data architecture and data modelling techniques.\n\u2022 Knowledge in ML & Statistical libraries (e.g. Scikit-learn, Pandas).\n\u2022 Having knowledge to build predict models to forecast risks for product launches and operations and help predict workflow and capacity requirements for TRMS operations\n\u2022 Having experience with visualization technologies such as Tableau\n\u2022 Draw inferences and conclusions, and create dashboards and visualizations of processed data, identify trends, anomalies\n\u2022 Generation of TLFs and summary reports, etc. ensuring on-time quality delivery.\n\u2022 Participated in client meetings, teleconferences and video conferences to keep track of project requirements, commitments made and the delivery thereof.\n\u2022 Solved analytical problems, and effectively communicate methodologies and results\n\u2022 Worked closely with internal stakeholders such as business teams, product managers, engineering teams, and partner teams.\n\u2022 Data mining using state-of-the-art methods\n\u2022 Extending company's data with third party sources of information when needed\n\u2022 Enhancing data collection procedures to include information that is relevant for building analytic systems\n\u2022 Processing, cleansing, and verifying the integrity of data used for analysis\n\u2022 Doing ad-hoc analysis and presenting results in a clear manner.\n\u2022 Created automated metrics using complex databases.\n\u2022 Foster culture of continuous engineering improvement through mentoring, feedback, and metrics.\nEnvironment: Erwin r9.0, Informatica 9.0, ODS, OLTP, Oracle 10g, Hive, OLAP, DB2, Metadata, MS Excel, Mainframes MS Visio, Rational Rose, Requisite Pro. Hadoop, PL/SQL, etc."", u'Data Scientist\nEagle Trading Systems - Princeton, NJ\nNovember 2012 to January 2014\nDescription: Eagle Trading Systems Inc. is a financial investment advisory firm headquartered in Princeton, New Jersey. The firm manages 5 accounts totaling an estimated $481 Million of assets under management.\n\nResponsibilities:\n\u2022 Worked with Ajax API calls to communicate with Hadoop through Impala Connection and SQL to render the required data through it .These API calls are similar to Microsoft Cognitive API calls.\n\u2022 Good grip on Cloudera and HDP ecosystem components.\n\u2022 Used ElasticSearch (Big Data) to retrieve data into application as required.\n\u2022 Performed Map Reduce Programs those are running on the cluster.\n\u2022 Developed multiple MapReduce jobs in java for data cleaning and preprocessing.\n\u2022 Statistical Modelling with ML to bring Insights in Data under guidance of Principal Data Scientist\n\u2022 Data modeling with Pig, Hive, Impala.\n\u2022 Ingestion with Sqoop, Flume.\n\u2022 Used SVN to commit the Changes into the main EMM application trunk.\n\u2022 Understanding and implementation of text mining concepts, graph processing and semi structured and unstructured data processing.\n\u2022 Analyzed the partitioned and bucketed data and compute various metrics for reporting.\n\u2022 Involved in loading data from RDBMS and web logs into HDFS using Sqoop and Flume.\n\u2022 Worked on loading the data from MySQL to HBase where necessary using Sqoop.\n\u2022 Developed Hive queries for Analysis across different banners.\n\u2022 Extracted data from Twitter using Java and Twitter API. Parsed JSON formatted twitter data and uploaded to database.\n\u2022 Launching Amazon EC2 Cloud Instances using Amazon Images (Linux/ Ubuntu) and Configuring launched instances with respect to specific applications.\n\u2022 Exported the result set from Hive to MySQL using Sqoop after processing the data.\n\u2022 Analyzed the data by performing Hive queries and running Pig scripts to study customer behavior.\n\u2022 Have hands on experience working on Sequence files, AVRO, HAR file formats and compression.\n\u2022 Used Hive to partition and bucket data.\n\u2022 Experience in writing MapReduce programs with Java API to cleanse Structured and unstructured data.\n\u2022 Wrote Pig Scripts to perform ETL procedures on the data in HDFS.\n\u2022 Created HBase tables to store various data formats of data coming from different portfolios.\n\u2022 Worked on improving performance of existing Pig and Hive Queries.\n\nEnvironment: SQL/Server, Oracle 9i, MS-Office, Teradata, Informatica, ER Studio, XML, Business Objects, HDFS, Teradata 14.1, JSON, HADOOP (HDFS), MapReduce, PIG, Spark, R Studio, MAHOUT, JAVA, HIVE, AWS..', u'Data Architect/Data Modeler\nFirst Indian Corporation Private Limited - Hyderabad, Telangana\nFebruary 2011 to October 2012\nDescription: First Indian Corporation Private Limited, a member of The First American family of companies, provides specialized offshore transaction services, technology services and analytics to the mortgage industry.\n\nResponsibilities:\n\u2022 Worked with large amounts of structured and unstructured data.\n\u2022 Knowledge in Machine Learning concepts (Generalized Linear models, Regularization, Random Forest, Time Series models, etc.)\n\u2022 Worked in Business Intelligence tools and visualization tools such as Business Objects, Tableau, ChartIO, etc.\n\u2022 Deployed GUI pages by using JSP, JSTL, HTML, DHTML, XHTML, CSS, JavaScript, AJAX.\n\u2022 Configured the project on WebSphere 6.1 application servers\n\u2022 Implemented the online application by using Core Java, Jdbc, JSP, Servlets and EJB 1.1, Web Services, SOAP, WSDL.\n\u2022 Handled end-to-end project from data discovery to model deployment.\n\u2022 Monitoring the automated loading processes.\n\u2022 Communicated with other Health Care info by using Web Services with the help of SOAP, WSDL JAX-RPC.\n\u2022 Used Singleton, factory design pattern, DAO Design Patterns based on the application requirements\n\u2022 Used SAX and DOM parsers to parse the raw XML documents\n\u2022 Used RAD as Development IDE for web applications.\n\u2022 Preparing and executing Unit test cases\n\u2022 Used Log4J logging framework to write Log messages with various levels.\n\u2022 Involved in fixing bugs and minor enhancements for the front-end modules.\n\u2022 Implemented Microsoft Visio and Rational Rose for designing the Use Case Diagrams, Class model, Sequence diagrams, and Activity diagrams for SDLC process of the application\n\u2022 Doing functional and technical reviews\n\u2022 Maintenance in the testing team for System testing/Integration/UAT.\n\u2022 Guaranteeing quality in the deliverables.\n\u2022 Conducted Design reviews and Technical reviews with other project stakeholders.\n\u2022 Was a part of the complete life cycle of the project from the requirements to the production support\nEnvironment: R 3.0, Erwin 9.5, Tableau 8.0, MDM, QlikView, MLLib, PL/SQL, HDFS, Teradata 14.1, JSON, HADOOP (HDFS), MapReduce, PIG, Spark, R Studio, MAHOUT, JAVA, HIVE, AWS.', u'Data Analyst/Data Modeler\nAccenture - Bengaluru, Karnataka\nAugust 2009 to January 2011\nDescription: Accenture PLC is a global management consulting and professional services company that provides strategy, consulting, digital, technology and operations.\n\nResponsibilities:\n\u2022 Developed Internet traffic scoring platform for ad networks, advertisers and publishers (rule engine, site scoring, keyword scoring, lift measurement, linkage analysis).\n\u2022 Responsible for defining the key identifiers for each mapping/interface.\n\u2022 Clients include eBay, Click Forensics, Cars.com, Turn.com, Microsoft, and Looksmart.\n\u2022 Designed the architecture for one of the first analytics 3.0. online platforms: all-purpose scoring, with on-demand, SaaS, API services. Currently under implementation.\n\u2022 Web crawling and text mining techniques to score referral domains, generate keyword taxonomies, and assess commercial value of bid keywords.\n\u2022 Developed new hybrid statistical and data mining technique known as hidden decision trees and hidden forests.\n\u2022 Reverse engineering of keyword pricing algorithms in the context of pay-per-click arbitrage.\n\u2022 Implementation of Metadata Repository, Maintaining Data Quality, Data Cleanup procedures, Transformations, Data Standards, Data Governance program, Scripts, Stored Procedures, triggers and execution of test plans\n\u2022 Performed data quality in Talend Open Studio.\n\u2022 Coordinated meetings with vendors to define requirements and system interaction agreement documentation between client and vendor system.\n\u2022 Automated bidding for advertiser campaigns based either on keyword or category (run-of-site) bidding.\n\u2022 Creation of multimillion bid keyword lists using extensive web crawling. Identification of metrics to measure the quality of each list (yield or coverage, volume, and keyword average financial value).\n\u2022 Enterprise Metadata Library with any changes or updates.\n\u2022 Document data quality and traceability documents for each source interface.\n\u2022 Establish standards of procedures.\n\u2022 Generate weekly and monthly asset inventory reports.\n\nEnvironment: Erwin r7.0, SQL Server 2000/2005, Windows XP/NT/2000, Oracle 8i/9i, MS-DTS, UML, UAT, SQL Loader, OOD, OLTP, PL/SQL, MS Visio, Informatica.']","[u""Bachelor's""]",[u''],"degree_1 : ""Bachelors"""
0,https://resumes.indeed.com/resume/5ffd8526d6e2b021,"[u""Data Scientist /Software Engineer\nActive Network, LLC - Dallas, TX\nJune 2015 to February 2018\nDeveloped a python Asset_Prediction_API classification engine, predicted topics from unstructured text uti- lizing NLP techniques (TFIDF etc) and ML algorithms (SVM, LR, NN etc), reduced the workload of market team\nby 80% and saved 200+ thousand dollars each year.\n\u2022 Built YMCA retention model (a Generalized Linear model implemented by R) ranking the customer's\nprobability to retain membership. Increased customers' membership retention rate considerably.\n\u2022 Developed unified-profile-api (an entity resolution and record linkage solution). Deployed a python-based bot- tle-gunicorn-nginx web framework on linux server to get users' RFM scores in JSON format.\n\u2022 Hands on experience on Deep Learning frameworks (TensorFlow, Keras etc).\n\u2022 Performed daily data cleaning, dimension reduction, classification/regression/clustering tasks.\n\u2022 Finished various SQL stored procedures, performed statistical analysis and present to C-level leaders.""]","[u'Master of Science in Computer Science', u'Bachelor of Science in Software Engineering in Software Engineering', u'certificate']","[u'The University of Texas at Dallas Richardson, TX\nMay 2015', u'University of Electronic Science and Technology of China Chengdu, CN\nJuly 2012', u'Andrew Ng Stanford University\nJanuary 2012']","degree_1 : Master of Science in Compter Science, degree_2 :  Bachelor of Science in Software Engineering in Software Engineering, degree_3 :  certificate"
0,https://resumes.indeed.com/resume/b1a00e2cc603fe48,"[u'Senior Data Scientist\nDunnhumby, Gurgaon, Haryana - IN\nAugust 2013 to December 2016\nDeployed Machine Learning algorithms like Logistic Regression, Random Forest, Support Vector Machine at scale in order\nto determine relevant customers for coupon distribution which increased the ROI from 4 to 6.\n* Implemented a sampling methodology based on Neyman Allocation to minimize the Control size. As a result, more\nnumber of customers were targeted which increased the revenue by $106MM for the client.\n* Migrated solutions from SAS to PySpark (Spark Python API) where data were housed in Hadoop. This significantly reduced\ncost incurred to company and also run-time by ~12 hours on average.\n\nSenior Analyst\n* Implemented Market Mix Modeling using logistic and linear regression to evaluate the effectiveness of television\ncampaigns for leading CPGs in USA. Increased the revenue from $300K to $1MM.\n* Created ANCOVA based media evaluations for digital campaigns that generated $1MM.', u'Business Analyst\nMu Sigma Business Solutions - Bengaluru, Karnataka\nJune 2011 to July 2013\nImplemented Random Forest algorithm in python to identify drivers for purchasing life insurance for a leading US based\nInsurance company, which improved their targeting methodology, thereby increasing the revenue by 5%\n* Created KPI dashboards in the form of VBA tools which tracked the performance of various drugs for a US based\npharmaceutical company generating $5 MM revenue.']","[u'in Business', u'Bachelor of Engineering in Biotechnology in Biotechnology']","[u'University of Connecticut School of Business Hartford, CT\nMarch 2018', u'Birla Institute of Technology\nJanuary 2011']","degree_1 : in Bsiness, degree_2 :  Bachelor of Engineering in Biotechnology in Biotechnology"
0,https://resumes.indeed.com/resume/af879f78fdac8e82,"[u'Data Scientist\nCigna Healthcare, NY. - Dallas, TX\nJune 2018 to Present\nDescription: Cigna is an American worldwide health services organization based in suburban Bloomfield, Connecticut. Its insurance subsidiaries are major providers of medical, dental, disability, life and accident insurance and related products and services, the majority of which are offered through employers and other groups', u'Data Scientist\nCharles Schwab, Denver, Colorado. - Dallas, TX\nMarch 2017 to Present\nDescription: The Charles Schwab Corporation is a bank and brokerage firm, based in San Francisco, California. It was founded in 1971 by Charles R. Schwab and is one of the largest banks in the United States and is one of the largest brokerage firms in the United States.', u'Big Data Developer\nSPRINT - Kansas City, MO - Dallas, TX\nNovember 2016 to Present\nDescription: Sprint Corporation, commonly referred to as Sprint is the fourth largest mobile network operator in the United States. The objective is to provide large-scale programs that integrate with technology.']","[u""Bachelor's""]",[u''],"degree_1 : ""Bachelors"""
0,https://resumes.indeed.com/resume/a29c3bfbfb7a4f58,"[u'Data Scientist\nCapital Group - CA\nFebruary 2017 to Present\nA private equity firm, Capital Group provides investment management services for long-term investors to deliver superior, long-term investment results and service.I am a Data scientist responsible to help the Digital Marketing and Sales team.\nResponsibilities\n\u2022 Developed an NLP model using NLTK library in Python to classify the content in the investment portfolio database with 75 percent accuracy.\n\u2022 Implemented Linear Regression Algorithm resulting to identify the most valuable customers.\n\u2022 Implement data mining and statistical machine learning solutions to various business problems.\n\u2022 Develop customer segmentation algorithm to score sales and lead to increase market share.\n\u2022 Implement and monitor the fraud detection models for the Nadia mobile Marketing and Sales.\n\u2022 Develop large scale machine learning models for real time fraud detection using big data.\n\u2022 Implement demand forecasting models which improved upon forecast accuracy successfully.\n\u2022 Manage and work on setting up the data platform for analytic through integrating data sources.\n\u2022 Create and present executive dashboards to show the trends and to operationalize churn model.\nTools and Techniques used:Linear Regression, Scikit Learn, Python, Numpy, Spark, Data Mining, FeatureExtraction, Hypothesis Testing, Normalization, PCA, SQL, NLP, TensorFlow, Reporting Results', u'Data Scientist\nExperian\nAugust 2015 to December 2016\nA fortune 500 company, Experian unlocks the power of data to create opportunities for consumers, businesses and society.I was a part of the Application Security team responsible to implement data mining techniques to help protect the customer data by identifying the fraudulent transactions.\nResponsibilities\n\u2022 Built the fraudulent forecasting classification model using Logistic Regression algorithm.\n\u2022 Provided technical leadership in a team that developed systems to analyze large scale data.\n\u2022 Used Python to analyze and identify the Security data lakes for the fraudulent transactions.\n\u2022 Used NLP to text mine the customer service data lake for analyze the intent.\n\u2022 Monitored and provided analytical insights and predictive modeling for mobile games: price\n\u2022 Modeling, LTV, churn prediction, experimental design.\n\u2022 Developed Regression algorithms using advanced data mining algorithms successfully.\nTools and Techniques used: Logistic Regression, NeuralNetwork, Python, NLP, Pandas, SQL, MatplotLib, Tableau, TensorFlow, Scikit Learn.', u'Data Scientist\nDisney\nJune 2014 to July 2015\nWalt Disney Imagineering is the unique, creative force behind Walt Disney Parks and Resorts that dreams up, designs and builds all Disney theme parks, resorts, attractions worldwide.I was a part of the data science team and my role was to leverage the history of park & resorts financial transaction data to build the insights for the efficient and secure transactions.\nResponsibilities\n\u2022 Data engineered and defined the scope, features for the data set.\n\u2022 Processed and analyzed large data sets to identify fraudulent transactions using trading patterns;\n\u2022 Build the Decision Tree classification model to track the fraudulent activity;\n\u2022 Developed clear and well-structured analytical plans and analyzed large data-sets.\n\u2022 Identified, evaluated, and documented potential data sources in support of project requirements.\n\u2022 Built recommender system to help users to choose rides based on their previous selections.\n\u2022 Predicted labels of articles using Neural Network and used machine learning algorithms.\n\u2022 Developed models and reports, shared observations and recommendations with senior Management, executives, and stakeholders for further consideration\nTools and Techniques used: Decision Tree, Neural Network Algorithms, Python, Numpy, Pandas, SQL, MatplotLib, Tableau, Regression Analysis.', u'Data Analyst\nExperian\nSeptember 2013 to May 2014\nA Fortune 500 company, Experian unlocks the power of data to create opportunities for consumers, businesses and society.My responsibilities here were to data mine the web analytics data to help the product and development teams reduce the customer churn rate in the sales transaction funnel.\nResponsibilities\n\u2022 Applied the Data Mining techniques using Pandas and Numpy.\n\u2022 Prototyped the Linear Regression model to track the user inactivity using Scikit-Learn;\n\u2022 Preprocessed complex financial/regulatory documents and conducted keyword analysis;\n\u2022 Managed data operations team and collaborated with data warehouse developers to meet business\n\u2022 Developed ETL pipelines in the production for reporting marketing / operations teams.\n\u2022 Actively expedited the incumbent Market Segmentation analytics by 30% over the previous\n\u2022 Model by optimizing the statistical model successfully.\n\u2022 Programmed and tested data ingestion from SQL, HDFS to an ETL workflow.\n\u2022 Created customized reports and revealed fraudulent patterns using SQL and Tableau.\nTools and Techniques used:Linear Regression, Python, Machine Learning, Numpy, Pandas, SQL, Spark, No SQL, MatplotLib, Tableau.', u'Data Analyst\nDKRIN, UK\nOctober 2010 to July 2012\nA private Data Consulting company that provides a variety of services including implementation, process improvement, and software customization solutions.Analyzed the social media datasets twitter to build the customer behavioral insights.\nResponsibilities\n\u2022 Built Behavior insights using Natural Language Processing(NLP) to find ideal customer intent, allowing clients to reach their ideal customers with an informed understanding of the demographic, market, and behavioral patterns;\n\u2022 Designed the technology architecture for the current state of business strategy;\n\u2022 Interpreted complex simulation data using statistical methods as per requirements.\n\u2022 Architected and implemented analytics and visualization components for data analysis.\n\u2022 Developed cash-flow analytics for raw transaction data and implemented targeted features\n\u2022 Performed text mining to understand the spending patterns of potential customers.\n\u2022 Analyzed and created dimensional data modeling to meet OLAP needs.\n\u2022 Extracted, transformed and loaded present and historical data in preparation for data mining.\n\u2022 Conducted data analysis and developed complex designs algorithm\nTools and techniques used: Data Mining, Machine Learning, Regression Analysis, Text Mining, NLP, Python, Numpy, Pandas, SQL, MatplotLib, Tableau, Visualization.', u'Data Analyst\nDataBot Systems - IN\nMay 2008 to October 2009\nDatabot system offers a data analytics technology platform to help the start-ups by providing insights to create data-driven personas.\nResponsibilities\n\u2022 Extracted, transformed and loaded present and historical data in preparation for data mining.\n\u2022 Conducted data analysis and developed complex designs algorithm\n\u2022 Performed Text mining to better understand the buying patterns of potential customers;\n\u2022 Gathered, learning present and historical data in preparation for data mining;\nTools and Techniques used: Data Analysis, Data Mining, Python, Numpy, Pandas, SQL, ETL, Visualization Tools.']","[u'Masters in Computer Science', u'Masters in Embedded Systems', u'Bachelors in Computer Science']","[u'Texas A & M University\nJanuary 2013', u'University of South Wales\nJanuary 2010', u'JNTU\nJanuary 2008']","degree_1 : Masters in Compter Science, degree_2 :  Masters in Embedded Systems, degree_3 :  Bachelors in Compter Science"
0,https://resumes.indeed.com/resume/0b9feff4b7deb330,"[u'Sr Bioinformatics Scientist\nGenomic Health, Inc - Redwood City, CA\nJanuary 2016 to Present\nAlgorithm development and bioinformatics pipeline implementation for molecular\nDiagnostics\n\u2022 Improved the algorithm and implementation of data analysis pipeline to obtain\nclinically actionable information for cancer diagnostics\n\u2022 Biomarker development and PCR design for molecular diagnostic platforms', u'Staff Scientist\nBio-Rad, Inc. - Pleasanton, CA\nApril 2015 to April 2016\nNGS and single cell algorithm development and data analysis pipeline\nimplementation\n\u2022 Led and collaborated in cross-functional teams to design and improve the singlecell\ntechnology platform.\n\u2022 Prototyped and implemented end-to-end data analysis pipeline for single cell\ntechnology in AWS cloud computing environment\n\u2022 Customized assay design for digital droplet PCR (ddPCR)', u'Research Scientist / Data Scientist\nDuPont Pioneer - Des Moines, IA\nNovember 2011 to April 2015\nNGS data analysis and statistical modeling; Web-app design and development\n\u2022 Developed a Genotyping by Sequencing (GBS) pipeline and implemented on HighPerformance Computing (HPC) system and Hadoop system.\n\u2022 Completed various trait and genome-wide biomarker design projects\n\u2022 Led and collaborated to perform genome assembly and annotation for key germplasm', u'Postdoctoral Researcher\nOak Ridge National Laboratory - Oak Ridge, TN\nAugust 2010 to November 2011\nGenomic and transcriptomic data analysis; gene family characterization\n\u2022 454 sequencing analysis and De novo assembly of the genomic sequence from RNA- seq.\n\u2022 Developed a new approach for gene prediction, especially short open reading frame, in plant species and contributed 201 putative new genes to Arabidopsis genome.', u'Graduate Research Assistant\nSyracuse University\nJanuary 2006 to January 2010\nGenome-wide transcription regulator study performed computationally and in the wet lab']","[u'Ph.D in Computational Biology', u'M.S. in Applied Statistics']","[u'Syracuse University Syracuse, NY', u'Syracuse University Syracuse, NY']","degree_1 : Ph.D in Comptational Biology, degree_2 :  M.S. in Applied Statistics"
0,https://resumes.indeed.com/resume/dda63bc78ed7e6fa,"[u'Data Scientist\nCanon - Jamesburg, NJ\nJuly 2017 to Present\nDescription:Canon is specializing in the manufacture of imaging and optical products, including cameras, camcorders, photocopiers, steppers, computer printers and medical equipment.\n\nResponsibilities:\n\u2022 Identifying the Customer and account attributes required for MDM implementation from disparate sources and preparing detailed documentation.\n\u2022 Performing data profiling and analysis on different source systems that are required for CustomerMaster.\n\u2022 Worked closely with the DataGovernanceOfficeteam in assessing the source systems for project deliverables.\n\u2022 Used T-SQL queries to pull the data from disparate systems and Data warehouse in different environments.\n\u2022 Used DataQualityvalidation techniques to validate CriticalData Elements (CDE) and identified various anomalies.\n\u2022 Extensively used open source tools - RStudio (R) and Spyder (Python) for statistical analysis and building the machinelearning.\n\u2022 Involved in defining the Source To businessrules, Targetdatamappings, datadefinitions.\n\u2022 Presented DQ analysis reports and score cards on all the validated data elements and presented -to the business teams and stakeholders.\n\u2022 Performing DataValidation / DataReconciliation between disparate source and target systems (Salesforce, Cisco-UIC, Cognos, DataWarehouse) for various projects.\n\u2022 Interacting with the Business teams and Project Managers to clearly articulate the anomalies, issues, findings during data validation.\n\u2022 Writing complexSQL queries for validating the data against different kinds of reports generated by Cognos.\n\u2022 Extracting data from different databases as per the business requirements using SqlServerManagementStudio.\n\u2022 Interacting with the ETL, BIteams to understand / support on various ongoing projects.\n\u2022 Extensively using MSExcel for datavalidation.\n\u2022 Generating weekly, monthly reports for various business users according to the business requirements. Manipulating/mining data from database tables (Redshift, Oracle, DataWarehouse)\n\u2022 Providing analytical network support to improve quality and standard work results.\n\u2022 Create statistical models using distributed and standalone models to build various diagnostics, predictive and prescriptive solution.\n\u2022 Interface with other technology teams to load (ETL), extract and transform data from a wide variety of data sources\n\u2022 Utilize a broad variety of statistical packages like SAS, R, MLIB, Graphs, Hadoop, Spark, MapReduce and others\n\u2022 Provides input and recommendations on technical issues to Business&DataAnalysts, BIEngineers and DataScientists.\n\nEnvironment: Data Governance, SQL Server, ETL, MS Office Suite - Excel (Pivot, VLOOKUP), DB2, R, Python, Visio, HP ALM, Agile, Sypder, Word, Azure, MDM, SharePoint, Data Quality, Tableau and Reference Data Management.', u""Data Scientist\nAmerican Family Insurance - Madison, WI\nApril 2016 to June 2017\nDescription: American Family Insurance, also abbreviated as AmFam, is a private mutual company that focuses on property, casualty, and auto insurance, and also offers commercial insurance, life, health, and homeowners coverage as well as investment and retirement-planning products.\n\nResponsibilities:\n\u2022 Utilized Spark, Scala, Hadoop, HQL, VQL, oozie, pySpark, DataLake, TensorFlow, HBase, Cassandra, Redshift, MongoDB, Kafka, Kinesis, SparkStreaming, Edward, CUDA, MLLib, AWS, Python, a broad variety of machine learning methods including classifications, regressions, dimensionally reduction etc.\n\u2022 Utilized the engine to increase user lifetime by 45% and triple user conversations for target categories.\n\u2022 Application of various machine learning algorithms and statistical modeling like decision trees, text analytics, naturallanguageprocessing (NLP), supervised and unsupervised, regressionmodels, socialnetworkanalysis, neuralnetworks, deeplearning, SVM, clustering to identify Volume using scikit-learnpackage in python, Matlab.\n\u2022 Worked onanalyzing data from GoogleAnalytics, AdWords, Facebook etc.\n\u2022 Evaluated models using CrossValidation, Loglossfunction, ROCcurves and used AUC for feature selection and elastic technologies like ElasticSearch, Kibana.\n\u2022 Performed DataProfiling to learn about behavior with various features such as trafficpattern, location, Date and Time etc.\n\u2022 Categorized comments into positive and negative clusters from different social networking sites using SentimentAnalysis and TextAnalytics\n\u2022 Used Pythonscripts to update content in the database and manipulate files\n\u2022 Skilled in using dplyr and pandas in R and Python for performing exploratory data analysis.\n\u2022 Performed Multinomial LogisticRegression, DecisionTree, Randomforest, SVM to classify package is going to deliver on time for the new route.\n\u2022 Performed data analysis by using Hive to retrieve the data from Hadoopcluster, Sql to retrieve datafrom Oracledatabase and used ETL for data transformation.\n\u2022 Performed DataCleaning, features scaling, features engineering using pandas and numpy packages in python.\n\u2022 Exploring DAG's, their dependencies and logs using AirFlow pipelines for automation\n\u2022 Performed data cleaning and feature selection using MLlib package in PySpark and working with deep learning frameworks such as Caffe, Neon.\n\u2022 Developed Spark/Scala, RPython for regular expression (regex) project in the Hadoop/Hive environment with Linux/Windows for big data resources.\n\u2022 Used clustering technique K-Means to identify outliers and to classify unlabeled data.\n\u2022 Tracking operations using sensors until certain criteria is met using AirFlowtechnology.\n\u2022 Responsible for different Datamapping activities from Source systems to Teradata using utilities like TPump, FEXP, BTEQ, MLOAD, FLOAD etc\n\u2022 Analyze traffic patterns by calculating autocorrelation with different time lags.\n\u2022 Ensured that the model has low FalsePositiveRate and Textclassification and sentiment analysis for unstructured and semi-structured data.\n\u2022 Addressed over fitting by implementing of the algorithm regularization methods like L1 and L2.\n\u2022 Used Principal Component Analysis in feature engineering to analyze high dimensional data.\n\u2022 Used MLlib, Spark'sMachinelearning library to build and evaluate different models.\n\u2022 Implemented rule based expertise system from the results of exploratory analysis and information gathered from the people from different departments.\n\u2022 Created and designed reports that will use gathered metrics to infer and draw logical conclusions of past and future behavior.\n\u2022 Developed MapReduce pipeline for feature extraction using Hive and Pig.\n\u2022 Created DataQuality Scripts using SQL and Hive to validate successful data load and quality of the data. Created various types of data visualizations using Python and Tableau.\n\u2022 Communicated the results with operations team for taking best decisions.\n\u2022 Collected data needs and requirements by Interacting with the other departments.\n\nEnvironment:Python 2.x, CDH5, HDFS, Hadoop 2.3, Hive, Impala, AWS, Linux, Spark, Tableau Desktop, SQL Server 2014, Microsoft Excel, MATLAB, Spark SQL, Pyspark."", u'Data Analyst\nLiquidity Services Inc - Washington, DC\nDecember 2014 to March 2016\nDescription: Liquidity Services, Inc. operates various online auction marketplaces for surplus and salvage assets in the United States. Its auction marketplaces include liquidation.com, which enables corporations and selected government agencies located in the United States to sell surplus and salvage consumer goods and capital assets.\n\nResponsibilities:\n\u2022 Worked with BI team in gathering the report requirements and also Sqoop to export data into HDFS and Hive.\n\u2022 Involved in the below phases of Analytics using R, Python and Jupyter notebook.\n\u2022 Data collection and treatment: Analysed existing internal data and external data, worked on entry errors, classification errors and defined criteria for missing values\n\u2022 Data Mining: Used cluster analysis for identifying customersegments, Decisiontrees used for profitable and non-profitablecustomers, MarketBasketAnalysis used for customerpurchasingbehaviour and part/product association.\n\u2022 Developed multiple MapReduce jobs in Java for data cleaning and preprocessing.\n\u2022 Assisted with data capacity planning and node forecasting.\n\u2022 Installed, Configured and managed Flume Infrastructure\n\u2022 Administrator for Pig, Hive and HBase installing updates patches and upgrades.\n\u2022 Worked closely with the claims processing team to obtain patterns in filing of fraudulent claims.\n\u2022 Worked on performing major upgrade of cluster from CDH3u6 to CDH4.4.0\n\u2022 Developed MapReduce programs to extract and transform the data sets and results were exported back to RDBMS using Sqoop.\n\u2022 Patterns were observed in fraudulent claims using text mining in R and Hive.\n\u2022 Exported the data required information to RDBMS using Sqoop to make the data available for the claims processing team to assist in processing a claim based on the data.\n\u2022 Developed MapReduce programs to parse the raw data, populate staging tables and store the refined data in partitioned tables in the EDW.\n\u2022 Adept in statisticalprogramminglanguages like Rand Python including BigData technologies like Hadoop, and Hive.\n\u2022 Experience working as DataEngineer, BigDataSparkDeveloper, FrontEndDeveloper and ResearchAssistant.\n\u2022 Created tables in Hive and loaded the structured (resulted from MapReduce jobs) data\n\u2022 Using HiveQL developed many queries and extracted the required information.\n\u2022 Created Hivequeries that helped market analysts spot emerging trends by comparing fresh data with EDW reference tables and historical metrics.\n\u2022 Was responsible for importing the data (mostly log files) from various sources into HDFS using Flume\n\u2022 Enabled speedy reviews and first mover advantages by using Oozie to automate data loading into the HadoopDistributedFile System and PIG to pre-process the data.\n\u2022 Provided design recommendations and thought leadership to sponsors/stakeholders that improved review processes and resolved technical problems.\n\u2022 Managed and reviewed Hadoop log files.\n\u2022 Tested raw data and executed performance scripts.\n\nEnvironment:HDFS, PIG, HIVE, Map Reduce, Linux, HBase, Flume, Sqoop, R, VMware, Eclipse, Cloudera, Python.', u""Python Developer\nMacys.com, SFO, CA\nApril 2013 to November 2014\nDescription: Macy's, Inc. is one of the nation's premier retailers with over 700 departmental stores across the United States and a company which sells a great range of products.\n\nResponsibilities:\n\u2022 Developed a portal to manage and entities in a content management system using Flask\n\u2022 Designed the database schema for the content management system.\n\u2022 Designed email marketing campaigns and also created responsive web forms that saved data into a database usingPython/ Django Framework.\n\u2022 Worked on Hadoopsinglenode, Apachespark, Hiveinstallations\n\u2022 Installation, Configuration, Integration, Tuning, Backup, Crashrecovery, Upgrades, Patching, MonitoringSystemPerformance, System and NetworkSecurity and Troubleshooting of Linux/Unix Servers.\n\u2022 Developed views and templates in Django to create a user-friendly website interface.\n\u2022 Configured Django to manage URLs and application parameters.\n\u2022 Supported MapReduce Programs those are running on the cluster\n\u2022 Worked on CSV files while trying to get input from the MySQL database.\n\u2022 Wrote programs for performance calculations using Numpyandsqlalchemy.\n\u2022 Administered and monitored multi DatacenterCassandracluster based on the understanding of the CassandraArchitecture.\n\u2022 Extensively worked with Informatica in designing/developing ETL process to load data from xml sources to target database\n\u2022 Configured Ansible to manage AWS environments and automate the build process for core AMIs used by all application deployments including Autoscaling, and Cloudformation scripts.\n\u2022 Designed, automated the process of installation and configuration of secure DataStaxEnterpriseCassandra using chef\n\u2022 Wrote Python scripts to parse XML documents and load the data in database.\n\u2022 Worked in stages such as analysis and design, development, testing and debugging.\n\u2022 Built Web pages that are more user-interactive using jQueryplugins for Drag and Drop, AutoComplete, JSON, AngularJS, JavaScript.\n\nEnvironment:Python 2.7, Windows, MySQL, ETL, Ansibleflask and Python Libraries such as Numpy, sqlalchemy, Angular Js, MySQL DB."", u""R & SAS Programmer\nHindalco Industries - Mumbai, Maharashtra\nNovember 2011 to March 2013\nDescription: Hindalco Industries Ltd., an aluminum manufacturing company, is a subsidiary of the Aditya Birla Group. Its headquarters are at Mumbai, Maharashtra, India. It is the Flagship Company of the company in the metals business.\n\nResponsibilities:\n\u2022 Analyzed high volume, high dimensional client and survey data from different sources using SAS and R.\n\u2022 Manipulated large financial datasets, primarily in SQL and R\n\u2022 Used R for large matrix computation\n\u2022 Developed Algorithms (DataMiningQuery's) to extract data from data warehouse & databases to build Rules for the Analyst&Models Team.\n\u2022 Used R to import high volume of data\n\u2022 High level programming efficiency in the use of statistical modeling tools such as SAS, SPSS and R.\n\u2022 Developed predictive models using R to predict customers churn and classification of customers\n\u2022 Worked on Shiny and R application displaying machine learning for improving the forecast of business.\n\u2022 Developed, reviewed, tested & documented SAS programs/macros.\n\u2022 Created Templates by using SAS macro for existing reports to reduce the manual intervention.\n\u2022 Created Self-service tools for Onshore/Offshore team for data retrieval.\n\u2022 Worked on daily reports and used them for further analysis.\n\u2022 Developed/Designed templates for new data extraction requests.\n\u2022 Executed weekly reports for CommercialDataAnalyticsTeam.\n\u2022 Communicated progress to key Business partners and Analysts through status reports and tracked issues until resolution.\n\u2022 Created predictive and other analytically derived models for assessing sales.\n\u2022 Provided support in the design and implementation of ad hoc requests for Sales-RelatedPortfolioData.\n\u2022 Responsible for preparing test case documents and Technical specification documents."", u'SAS Developer/Analyst\nIntelenet Global Services - Mumbai, Maharashtra\nApril 2009 to October 2011\nDescription:Intelenet Global Services is a Global Business Process Outsourcing (BPO) &contact center provider firm headquartered in Mumbai, India, backed by Blackstone Group. The company offers 24/7 services for contact center solutions, transaction processing, finance & accounting, HRO and IT solutions to Fortune 500 companies in the UK, US, Australia, and India.\n\nResponsibilities:\n\u2022 Integrates all transaction data from multiple data sources used by Actuarial into a single repository.\n\u2022 Implemented and executes monthly incremental updates to the data environment.\n\u2022 Interacts with IT and finance and executes data validation tie-out reports.\n\u2022 Developed new programs and modified existing programs passing SAS macro variables to improve ease and efficiency as well as consistency of results.\n\u2022 Created Data transformation and DataLoading (ETL) scripts for DataWarehouses.\n\u2022 Implement fully automated data flow into Actuarial front end (Excel) Models using SAS process.\n\u2022 Creating SAS programs using SASDI Studio.\n\u2022 Validated the entire data process using SAS and BI tools.\n\u2022 Documenting of service requests by business users, developed code documentation, logs and outputs documentation, creating Test Plans and Production Release Notices for QC, QA and Production teams to perform further analysis.\n\u2022 Extensively used PROCSQL for column modifications, field populations on warehouse tables.\n\u2022 Additional responsibilities being Requirements gathering, Designing, Coding and Analysis, Testing, Debugging, Output generations in prescribed formats, extensive documentation of SAS Programs and Macros.\n\u2022 Developed distinct OLAP Cubes from SASDataset and generated results into the excel sheets.\n\u2022 Involved in discussions with business users to define metadata for tables to perform ETL process.\n\nEnvironment:Python 2.7, Windows, MySQL, ETL, Ansible flask and Python Libraries such as Numpy, sqlalchemy, Angular Js, MySQL DB.']",[u'Bachelor of Computer Science in TOOLS AND TECHNOLOGIES'],[u'Informatica Power Centre'],degree_1 : Bachelor of Compter Science in TOOLS AND TECHNOLOGIES
0,https://resumes.indeed.com/resume/4a81b985720777d1,"[u'Freelance Data Scientist\nself-employed - Los Angeles, CA\nNovember 2017 to Present\n-Maximizing Product Sales:\nMy client, a clothing retailer, is looking for recommendations to maximize their sale. Having their product sales history for the past three years, I\u2019m using Python and a combination of supervised and unsupervised (PCA) learning to build a predictive model to understand the importance of product features in number of sales.\n\n- Algorithmic Prediction of Soccer Matches:\nUsing Python and a SQLite database of team and player attributes and more than 25,000 matches, I\u2019m trying to analyze and extract patterns leading to winning a soccer match.', u'Soil Data Analyst\nArup\nSeptember 2013 to June 2017\n\u2022 Performed data analysis on variety of soil properties datasets in order to predict soil\nbehavior under various loading conditions.\n\u2022 Used inter-connected Excel spreadsheets to analyze and manage foundation design for multiple large-scale infrastructure and tunneling projects.\n\u2022 Worked as part of a diverse team of designers and managers to take a project from the bidding stage to construction in a high-pressure environment.\n\u2022 Wrote technical reports to communicate project specifics to clients and contractors.']","[u'Data Science Immersive - Certificate of Completion in Data Science', u'Master of Science in Civil and Geotechnical Engineering', u'Bachelor of Science in Civil and Environmental Engineering']","[u'General Assembly Santa Monica, CA\nJune 2017 to October 2017', u'University of California Los Angeles, CA', u'Sharif University of Technology']","degree_1 : Data Science Immersive - Certificate of Completion in Data Science, degree_2 :  Master of Science in Civil and Geotechnical Engineering, degree_3 :  Bachelor of Science in Civil and Environmental Engineering"
0,https://resumes.indeed.com/resume/af67ae446b1c8b7a,"[u""Data Scientist\nTeachers Pay Teachers - New York, NY\nJuly 2015 to Present\nLead data scientist for developing buyer personas. Implemented an exploratory factor analysis (EFA)\nfollowed by a hierarchical agglomerative clustering algorithm to identify groups of buyers with distinct\ncharacteristics for marketing teams insights.\n\u2022 Using python's Airflow assisted in the development of an ETL process with the end goal of storing all\ncompany data in Bigquery such that it could be easily accessed and analyzed.\n\u2022 Assisted with the development of our Trending Module including writing backend code for detecting\ndaily trending search terms, completing AB tests to assess performance, and further analyzing specific\nuser behavior with the module.\n\u2022 While embedded on the Personalization team developed a Collaborative Filtering pipeline for serving\nrecommendations to users. Evaluated performance of three libraries: libmf (C++), Implicit (Python),\nand mllib (PySpark) before deciding to use mllib. Worked on backend code in Python and created a stable production-ready pipeline using Airflow.\n\u2022 Perform AB test analysis for Search, Personalization, and Conversion Rate Optimization teams"", u'Statistician\nAmplify Education - Brooklyn, NY\nJune 2012 to June 2015\n\u2022 Developed software using Python that implemented a Markov Chain Monte Carlo procedure with the goal of automating student scoring for Item Response Theory models\n\u2022 Conducted statistical analyses using R on assessment data for predictive modeling, student scoring,\nand test validation\n\u2022 Implemented a data pipeline for extracting, processing, and analyzing click-stream data']","[u'Ph.D. in Quantitative Psychology', u'B.A. in Psychology & Statistics']","[u'University of North Carolina at Chapel Hill Chapel Hill, NC\nAugust 2012', u'The College of New Jersey Ewing, NJ\nMay 2004']","degree_1 : Ph.D. in Qantitative Psychology, degree_2 :  B.A. in Psychology & Statistics"
0,https://resumes.indeed.com/resume/14835462042e5847,"[u""Data Scientist\nHomelink Real Estate Agency Co., Ltd - Beijing, CN\nJanuary 2017 to Present\nEmploying Data Mining and Statistical Analysis to uncover and identify key markets that management hadn't previously considered in their analysis for a major inter-continental expansion. Reporting directly to the Division Director and VP of Operations, providing all aspects of data modeling and data analysis services to the North American Division of the PRC's Largest Real Estate Company with over 120,000 associates."", u""Data Analyst/Consultant\nVirtual Rehabilitation, Inc - San Francisco, CA\nJanuary 2014 to January 2017\nData modeling and forecasting for startups to use in their pitch decks to various stages of funding rounds, including Seed, Series A, B, C, and Venture Capital. Working with and advising to various startups at SOSV's IndieBio and HAX incubators on Jessie in SF. Led development of TBI (Trauma c Brain Injury) and Pharma initiatives. Integral to the introduction and deployment of firm's VR-based vision therapy system into Stanford University Medical Clinics."", u'Data Analyst/Project Manager\nA\xb2 ANALYTICS, Sea le - San Francisco, CA\nJanuary 2001 to January 2014\nDuring this period, held various contracts. Responsible for analyzing institutional portfolio holdings, aiding with the acquisition and operations/management of institutional investment portfolios, financial database development, new business development, marketing and promotional work, independent portfolio management, staff coaching, team development, and other industry-related functions.', u'Research Analyst - Mergers\nMOSS ADAMS ADVISORY SERVICES, Sea le, Washington\nJanuary 1998 to January 2001\nDuring tenure with company, worked on a myriad of diverse projects. Projects encompassed such areas as mergers and acquisi ons, buyouts, insurance audits, and raising capital for public companies, private companies, and government\nagencies.']","[u'', u'']","[u'BELLEVUE COLLEGE Bellevue, WA', u'MONTEREY PENINSULA COLLEGE Monterey, CA']","degree_1 : , degree_2 :  "
0,https://resumes.indeed.com/resume/654e43156e03cb67,"[u'Data Scientist Intern\nMilliman - Seattle, WA\nMarch 2017 to September 2017\n\u2022 Conducted exploratory data analysis on policyholder\u2019s churn and withdrawal behaviors; Successfully performed missing value imputation with generalized low rank model and customer segmentation with entropy weighting Kmeans algorithm on a client dataset (about 5 million rows and 1000+ variables)\n\u2022 Improved the accuracy of the GLM customer churn model by 11% through data enrichment, integrating internal policyholder data with third party data (macro-economic, census, credit, consumer behavior, etc.); Identified credit score as the major variable increasing model accuracy\n\u2022 Created informative visualization dashboards using Tableau and R Shiny and presented to stakeholders\n\u2022 Implemented downloading feature in the Oct 2017 release of Milliman RECON Utilization Explorer web app', u'Graduate Research Assistant\nUniversity of Cincinnati - Cincinnati, OH\nJanuary 2011 to January 2016\n\u2022 Designed and conducted factorial experiments to assess the performance of oil dispersants, performed inferential and regression analysis in R, built data visualizations using ggplot2; Resulted in three peer-reviewed publications\n\u2022 Initiated and managed two top-level EPA research projects with 9 members in total; Key responsibilities include writing quality assurance plans, monitoring project progress, and delivering data summarizing reports; Presented at four international conferences, won 1 travel award and 2 poster awards']","[u'Master of Science in Data Science', u'Ph.D. in Environmental Engineering']","[u'University of Washington Seattle, WA\nSeptember 2016 to March 2018', u'University of Cincinnati Cincinnati, OH\nSeptember 2010 to August 2016']","degree_1 : Master of Science in Data Science, degree_2 :  Ph.D. in Environmental Engineering"
0,https://resumes.indeed.com/resume/734c29582c67ecec,"[u'Data Scientist\nSFO ITT - San Francisco, CA\nOctober 2016 to Present\nResponsibilities:\n\u2022 Built K-means model designed to reduce over-authorization of readers assigned to employees, improve aviation workflow and reduce security threats by reclassifying access classes and their assignment to employees.\n\u2022 Performed descriptive and exploratory analysis of network equipment data based on different data sources. Classified issues by related types and categories and used this classification for failure rate and mean time to failure prediction for purpose of raising overall service quality of equipment managed by SFO ITT.\n\u2022 Analyzed SFO airport taxi traffic patterns and built model for managing taxi traffic congestion i.e. reducing taxis oversupply and freeing parking garage spaces.\n\u2022 Applied NLP for reducing data redundancy and uncertainty in existing data bases as part of creating efficient data catalog that improved accuracy and enabled efficient functionality of data services.\n\u2022 Designed and created externally facing web portal that accessed SFO noise data in real time and visualized noise data collectors by location in Tableau.', u'Data Scientist Consultant\nPinterest - San Francisco, CA\nJuly 2016 to October 2016\nResponsibilities:\n\u2022 Built data quality checking and monitoring tools for off-line production processing pipeline that caught data integrity, correctness and accuracy issues as soon as possible.\n\u2022 Performed metric quality analysis and drove investigation to fix issues that influenced quality of production workflows and ad-hoc analysis.', u'Data Scientist Consultant\nClinical Persona Inc - East Palo Alto, CA\nDecember 2015 to July 2016\nResponsibilities:\n\u2022 Ported shell scripts that are managing AWS hadoop cluster into Python.\n\u2022 Performed genomic data analysis on AWS hadoop cluster.', u'Data Scientist Fellow\nGalvanize Inc - San Francisco, CA\nJune 2015 to September 2015\nResponsibilities:\n\u2022 Intensive 12 weeks Data Science bootcamp covering the fundamentals of exploratory data analysis, machine learning, NLP and data visualization.\n\u2022 Capstone Project: Human Activity Recognition Using Smartphones UCI Data Set - machine learning methods (Support Vector Machine, Random Forest, AdaBoost, XGBoost) applied on standing, sitting, laying, walking, walking upstairs and walking downstairs data set. Link: https://github.com/gornes/Human_Activity_Recognition', u'Senior Product Development Engineer\nKLA-Tencor\nJune 2013 to December 2014\nResponsibilities:\n\u2022 Collected measurements on laser based silicon wafers scanning inspection system and performed detected defects data analysis using proprietary software that lead to customers issues resolution and new features development.\n\u2022 Effectively worked with cross-functional teams in analyzing time sequential change in the number of defects and correlation between the number of defects and manufacturer process anomalies, resulting in cost reduction improvements.', u'KLA-Tencor - Milpitas, CA\nApril 2005 to December 2014\n3 Billion global semiconductor leader in defect inspection and metrology products and solutions)', u'Senior Applications Development Engineer\nKLA-Tencor\nNovember 2008 to June 2013\nResponsibilities:\n\u2022 Successfully developed method for optimizing optical aperture that improved rough film sensitivity and met customer requirements and deadlines, leading to multiple system orders.\n\u2022 Performed data mining to determine similar defect signatures and their sources common technologies and processes, presented results and made recommendations to senior management that contributed to winning strategy and plan development.\n\u2022 Participated in market research, developed product specifications, project plans, risk-benefit analysis, assigned resources to project tasks and led characterization involving US-India-China team coordination under time driven business critical situations.', u'Applications Development Engineer\nKLA-Tencor\nApril 2005 to November 2008\nResponsibilities:\n\u2022 Participated in and led small and medium new feature characterization projects, and product demos.\n\u2022 Responded to customer requests for improved defect detection with higher throughput, by performing problem diagnosis and developing problem resolutions.\n\u2022 Received 2006 Business Unit award for Purchasing Order success for North America over $45M of bookings.']","[u'Ph.D. in Electrical Engineering', u'M.S. in Electrical Engineering', u'B.S. in Electrical Engineering']","[u'Santa Clara University', u'University of Belgrade', u'University of Belgrade']","degree_1 : Ph.D. in Electrical Engineering, degree_2 :  M.S. in Electrical Engineering, degree_3 :  B.S. in Electrical Engineering"
0,https://resumes.indeed.com/resume/ca9b4dc88385e1c5,"[u'Data Scientist Intern\nKoninklijke Philips N.V. - Beijing, CN\nJuly 2017 to July 2017\nAided Philips in gaining insight into customer support for precision medicine, and on methods required to launch precision medicine products.\n\u2022 Analyzed radiomics data with Python and MATLAB using Support Vector Machine and Convolutional Neural Networks to provide help of diseases diagnosis and potential treatment plan suggestions.\n\u2022 Developed Convolutional Neural Network to predict stages of Lymphoma and utilized multiple methods of feature selection, data cleaning, and innovative methods of concatenating data to optimize the performance of the model to an accuracy of 95% .\n\u2022 Conducted research on radiomics, and analyzed industrial reports of mathematical models and algorithms for managers to develop data models of health care products.\n\u2022 Cooperated with other Clinical Science departments and assisted in developing innovative MRI image recovery using compressive sensing and numerical methods.\n\u2022 Presented project progress, researched methodologies, and industrial report analysis across with other departments in clinical science to upper management in weekly conferences.']","[u'Master of Financial Mathematics in Financial Mathematics', u""Bachelor's in Economics"", u""Bachelor's in Mathematics - Specialized in Computer Applications""]","[u'University of Minnesota-Twin Cities Minneapolis, MN\nAugust 2017 to May 2019', u'University of Minnesota-Twin Cities Minneapolis, MN\nJanuary 2013 to May 2017', u'University of Minnesota-Twin Cities Minneapolis, MN']","degree_1 : Master of Financial Mathematics in Financial Mathematics, degree_2 :  ""Bachelors in Economics"", degree_3 :  ""Bachelors in Mathematics - Specialized in Compter Applications"""
0,https://resumes.indeed.com/resume/e85734f87811f1e1,"[u'TEACHING ASSISTANT (UIUC)\nBIG DATA ANALYTICS\nAugust 2017 to December 2017\nR, Python, Weka)\n\u2022 Developed a course on advanced regression and classification for a professional audience.\n\u2022 Created exercises and solutions for regression and classification lectures of the course.\n\u2022 Mentored student groups on final machine learning project and guided them in improving their model efficiencies.', u""RESEARCH ASSISTANT\nDATA INTELLIGENCE, UIUC\nJanuary 2017 to May 2017\nInsurance Analytics - Application of Machine learning in Insurance - Big Data\n\u2022 Data Cleansing and transformation to prepare raw data for statistical analysis.\n\u2022 Implemented Generalized Linear Regression models with Poisson response to estimate the claim counts.\n\u2022 Utilized K-Means clustering technique to analyze customer groups and model each group individually.\n\u2022 Implemented Gaussian Mixture Models with 'Expectation Maximization' algorithm to study claims across customer groups.\n\u2022 Explored a variety of models including ARIMA, exponential smoothing with bagging, vector autoregression, STL\ndecomposition, and used cross-validation to select best model.\n\u2022 Successfully identified unique customer cohorts and calculated premiums for optimum profits.\n\u2022 Developed project report involving machine learning algorithms/techniques for insurance domain.\nPROJECTS"", u""Data Scientist\nPython\nJanuary 2017 to May 2017\nProject undertaken for an MIT affiliated course)\n\u2022 Created an ANN from scratch with two hidden layers having 4 hidden units each and utilized this network for image classification.\n\u2022 Compared the results for different activation functions - 'Tanh', 'ReLU', 'Sigmoid'.\n\u2022 Improved the model performance using advanced regularization techniques (L2, drop-out)\n\u2022 Optimized the execution time using RMSprop, Adam (Adaptive moment estimation) techniques for gradient\ndescent.\n\nANALYSIS OF TELEMARKETING DATA IN BANKING JAN 2017 - MAY 2017\n(R)\n\u2022 Data cleansing and transformation for the data collected from UC Berkeley data repository.\n\u2022 Determined the probability of a customer to subscribe a term deposit conditional to the most recent marketing\ncampaign.\n\u2022 Designed and compared predictive models - Decision trees (and Random forests), Support Vector Machines\n(both linear and radial kernels) for classifying the customer based on subscription.\n\u2022 Achieved prediction accuracy of 91% on training set and 89.5% on test set, our best model being the SVM (linear\nkernel)."", u'Data Scientist\nPython/Hadoop\nAugust 2016 to December 2016\nProject undertaken for an MIT affiliated course)\n\u2022 Analyzed the data for all the cars that were produced in the year 2004 in USA to understand the impact of car type,\nmanufacturer and other characteristics on profit margins.\n\u2022 Identified clusters based on car weight, horse power and number of cylinders and studied the differences in profits among these clusters.\n\u2022 Successfully predicted profits for the following year using regression models (Lasso) and classified cars based on number of cylinders using discriminant analysis.', u""Data Scientist\nPython/Hadoop\nAugust 2016 to December 2016\nR)\n\u2022 Used Shiny to generate interactive plots like Histograms, Boxplots, Scatterplots etc. for Letter Recognition dataset\n\u2022 Built predictive model and compared the results of classification algorithms such as Random Forest, SVM and\nKNN in R\n\nPRICING & HEDGING OF GUARANTEED MINIMUM MATURITY BENEFIT RIDER Jan 2016 - May 2016\n(MATLAB)\n\u2022 Derived and verified a general formula for the time-t risk-neutral value of the GMMB rider (10 years' time)\n\u2022 Developed an algorithm based on Monte-Carlo simulations to calculate:\n\u2756 The probability of the contract making a profit.\n\u2756 The 90% conditional tail expectation of the net liability.\n\u2756 The 90th percentile of the net liability\n\u2022 Developed a Delta-Hedging portfolio which begins with zero money and is entirely funded with fee incomes,\nborrowing money from a money market account."", u'BUSINESS ANALYST\nACCENTURE SERVICES PVT. LTD\nJuly 2014 to December 2015\nClient: Berkshire Hathaway Specialty Insurance\n\u2022 Translated user, business requirements into functional requirements and design specification documents.\n\u2022 Analyzed historical customer data and designed predictive models to establish and compare relationships between various customer profile attributes and policy purchase trends.\n\u2022 Development and implementation of big data solutions for Hadoop platform with expertise in MapReduce, HDFS,\nHive, Pig\n\u2022 Designed a Logistic Regression model in R to classify drivers based on insurance claims history, driving habits and driver profiles.\n\u2022 Ensured product quality to meet the functional requirements and designed test scenarios to evaluate the product.\n\u2022 Extensively participated in Agile Scrum Development Methodology with Test Driven Development process, from the development phase to testing phase and production phase.\n\u2022 Successfully led multiple projects in Agile development methodologies, to deliver complex data management\nsolutions, acting as liaison between internal and external stakeholders.\n\u2022 Responsible for Knowledge transfer, technical documentation and handing over the product to super users.\n\u2022 Received Accenture Excellence Award for being among top 1% performers in the practice.', u'DATA ANALYST\nCOMPUTER SCIENCE CORPORATION\nJune 2012 to July 2014\n\u2022 Analyzed large datasets with millions of bank transaction records to extract actionable inferences.\n\u2022 Research, evaluate, architect, and deploy new tools, frameworks, and patterns to build sustainable Big Data\nplatforms for our clients.\n\u2022 Installed, configured, and maintained Apache Hadoop clusters for application development and major components of Hadoop Ecosystem: Hive, Pig, HBase, Sqoop.\n\u2022 Created KPI dashboards using SQL SRSS and QlikSense/Tableau, presented results to client leadership.\n\u2022 Designed and implemented complex highly scalable statistical models and solutions by collaborating with the lead\nData Scientist.\n\u2022 Conducted quantitative and statistical testing (hypothesis, A/B, UAT) to validate and evaluate the models.\n\u2022 Created data pipelines to collect and store the data which was later be utilized for analysis.\n\u2022 Created interactive data visualizations to efficiently convey the results of data analysis using Tableau.']","[u'Master of Science in Applied Mathematics', u'Master of Business Administration in Insurance and Risk Management', u'Bachelor of Technology in Electronics and Communications']","[u'University of Illinois Urbana, IL\nJanuary 2016 to December 2017', u'Birla Institute of Management Technology\nJune 2012 to April 2014', u'Nagarjuna University\nJune 2008 to April 2012']","degree_1 : Master of Science in Applied Mathematics, degree_2 :  Master of Bsiness Administration in Insrance and Risk Management, degree_3 :  Bachelor of Technology in Electronics and Commnications"
0,https://resumes.indeed.com/resume/19caaf6e9afef1cf,"[u'Data Scientist\nDataShine LLC - Wilton, CT\nOctober 2017 to Present\n\u2022 Used machine learning and probabilistic graphic techniques developing a web application (www.dipine.com) which can automatically make diagnosis based on patient\u2019s symptoms.\n\u2022 Used Python and natural language processing techniques retrieving information from medical journals to diagnose more complex medical cases.\n\u2022 Build a website predicting stock prices using deep learning and time series analysis techniques.', u'Technical Specialist\nICF - Atlanta, GA\nMay 2007 to October 2017\n\u2022 Worked for various projects in roles of data scientist, data engineer or statistician. Almost all the projects are funded by the US federal government agencies such as CDC, NIH and SAMHSA. The projects include child mental health, child mental trauma, youth suicide prevention, cervical cancer vaccine, flu vaccine, smoke cessation etc.\n\u2022 Analyzed dozens of data sets using SAS, SPSS, or Stata and reported the findings. Statistical models he used include linear regression, logistic regression, ordered probit, multilevel modelling, survival analysis, cluster analysis and factor analysis.\n\u2022 Wrote C++ code using parallel computing algorithm to simulate the epidemiology of cervical cancer vaccination and then ran the code on the US Department of Defense supercomputers and Amazon Web Services clusters.\n\u2022 Simulated the epidemiology of smoking cessation in Matlab and studied the cost-effectiveness of smoking cessation.\n\u2022 Wrote VBA code automating numerous data-intensive reports in Access, Powerpoint, Excel and Word, saving huge amount of human labors and reducing human-made errors.\n\u2022 Developed a VB.net application that allows user to select criteria and then generate a very complex Powerpoint report (with more than 100 data-intensive slides) on demand online.\n\u2022 Won the Special Project Contribution Award in 2008 for using his outstanding technical skills solving a major technical problem.', u""Research Assistant\nUniversity of Minnesota - Minneapolis-Saint Paul, MN\nFebruary 2002 to May 2007\n\u2022 Studied the relationship between market competition and nursing home quality of care, nursing home adoption of sprinkler systems, and nursing homes' reactions to deficiency citations.\n\u2022 Conducted data analyses and literature reviews for different projects, including quality-based payment of Minnesota nursing homes; the University of Minnesota's self-insurance plan; the health insurance program for the Commonwealth of Puerto Rico; a mortality comparison of health maintenance organization (HMO) and fee-for-service, selection of Sterling Option I versus private fee-for-service of the Medicare patients; HMO's bidding behavior; electronic medical record adoption; and immigrant health.\n\u2022 Evaluated the outcome of the Adolescent Parenting Program for the Healthy Families Home Visiting Program and designed a Microsoft Access database that automated the work process and generated monthly reports automatically.\n\u2022 Evaluated the Minnesota Family Investment Program (MFIP). Conducted a survival analysis, studying factors affecting when a welfare recipient left welfare, and a cost-effectiveness analysis, studying whether the MFIP was worthwhile.\n\u2022 Migrated birth and death data of the State of Minnesota population from the 1940s to the 2000s into an Oracle server.""]","[u'PhD in Health Services Research', u'MBA', u'Bachelor of Medicine in Health Management']","[u'University of Minnesota Minneapolis-Saint Paul, MN', u'China Europe International Business School Shanghai, CN', u'Fudan University Shanghai, CN']","degree_1 : PhD in Health Services Research, degree_2 :  MBA, degree_3 :  Bachelor of Medicine in Health Management"
0,https://resumes.indeed.com/resume/34dd1ad99dd8a195,"[u'Data Scientist\nAmeri100 - Richardson, TX\nAugust 2017 to Present\nResponsibilities:\n\u2022 Created SQL server database to store data, such as smart meters, customer usage, and service supplement data source and extracted the data elements based on business requirement\n\u2022 Developed and maintained SQL scripts to generate purpose-built data mart for identifying disconnection for Non-Pay accounts\n\u2022 Preprocessed the dataset, filled the missing data, encoded the region name and normalized the load\n\u2022 Applied the Gradient Boosted Regressor by XGBoost, and optimized the input parameter by Cross Validation to forecast the trading power price in the day-ahead energy market\n\u2022 Summarized the report by Tableau bar graphs, scattered plots, and geographical maps to stakeholders, and validated the results with the official predictions', u'Research Data Assistant\nThe Ohio State University - Columbus, OH\nMay 2016 to August 2016\nProject: House sale price prediction based on high dimension dataset (Kaggle competition)\n\u2022 Designed a data preprocessing pipeline to remove/fill the missing data line, categorize string data by the One-Hot Encoder, standardize the dataset by feature scaling, smooth the distribution of sale price\n\u2022 Reduced the dataset dimension by the Principal Component Analysis (PCA), to extract the most relevant features\n\u2022 Established the Ridge Regression Model due to the multi-factor dataset, and optimized input parameter of the model using the Cross Validation\n\u2022 Implemented the Gradient Boosted Decision Trees by XGBoost to improve the predictions by 5%\n\u2022 Created informative visualization dashboards using Tableau and presented to stakeholders', u'Reasearch Data Assistant\nThe Ohio State University - Columbus, OH\nJune 2015 to September 2015\nProject: The stock market prediction from daily news using Natural Language Processing (Kaggle competition)\n\u2022 Crawled top 25 historical daily news headlines of 2008-2016 from Reddit Worldnews Channel, and Dow Jones Industrial Average (DJIA)\n\u2022 Tokenized the raw text, then obtained the daily word list by stemming and removing stop words\n\u2022 Built Bag of Words Model to implement Na\xefve Bayes Classifier and Support Vector Classifier, consequently achieved the accuracy of 70% and 73% separately']",[u'Doctor of Philosophy in Geodetic Science'],"[u'The Ohio State University Columbus, OH\nSeptember 2013 to August 2017']",degree_1 : Doctor of Philosophy in Geodetic Science
0,https://resumes.indeed.com/resume/23b4899bdf42ad3d,"[u'Consulting Data Scientist\nPROSPECT RESOURCES INC - Skokie, IL\nMay 2017 to August 2017\n\u2022 Analyzed existing predictive analytics tool for layered hedging in energy procurement market.\n\u2022 Analyzed and forecasted impact on returns because of delayed hedging client responses.\n\u2022 Implemented a buying strategy based on Moving Average (MACD) and Deep Learning.', u'Consultant\nDELOITTE CONSULTING - Bengaluru, Karnataka\nJuly 2013 to July 2016\n\u2022 Designed data warehouse to house the product/portfolio health benefits data and customer segmentation to report for Obama Care HIX, to help Anthem Insurance increase sales and determine pricing strategies.\n\u2022 Integrated drug sales information into Veeva CRM (salesforce.com) and created visualizations to help business users of\nBiogen Idec with decision making, and intuitive analysis of patients, products and health plans.\n\u2022 Led a mid-size team to build an authoritative system for managing master data for PayPal Inc.\n\u2022 Consulted as a subject matter expert in Teradata Parallel Transporter. Presented several use-cases on advanced\nanalytics to clients as a part of request for proposals.', u'Data Analyst\nTATA CONSULTANCY SERVICES (TCS) LIMITED - Kolkata, West Bengal\nNovember 2010 to July 2013\nAnalyzed, cleansed and transformed insurance data for Sunlife Financials for risk reporting and portfolio management\nsystem to facilitate management and operations of its asset management activities on ETL/RDBMS framework.\n\u2022 Involved in requirement gathering, technical solutions and process improvement for BlackRock Inc. Created analytical\nreports based on Portfolio which facilitated a highly automated, consistent investment process.']","[u'Master of Sciences in Data Science', u'Bachelor of Technology in Information Technology']","[u'ILLINOIS INSTITUTE OF TECHNOLOGY Chicago, IL\nSeptember 2016 to December 2017', u'WEST BENGAL UNIVERSITY OF TECHNOLOGY Kolkata, West Bengal\nAugust 2006 to June 2010']","degree_1 : Master of Sciences in Data Science, degree_2 :  Bachelor of Technology in Information Technology"
0,https://resumes.indeed.com/resume/769b349bba78956a,"[u""Full-Stack Software Developer and Data Scientist\nRisk Management Department, Tokio Marine Technologies LLC - Duluth, GA\nMarch 2018 to Present\n\u2022 Designed, developed, troubleshot, debugged and tested website compatibility via .NET, C#, and SQL Server\n\u2022 Performed K-Means to cluster customers' background profiles building upon on case history for customized insurance"", u""Data Scientist and Invest Risk Quant Intern\nPython - Manhattan, NY\nJune 2017 to August 2017\n\u2022 Built statistical models to perform valuation for convertible and exchangeable bonds using C++, Python and R\n\u2022 Performed financial analyses for over $6.08 billion commissioned credit facilities\n\u2022 Cooperated with senior quantitative analysts in implementing collaborative filtering recommender system via Python\n\u2022 Developed five programs in VBA to automate risk evaluation for foreign exchange derivatives, bonds, securities and funds\n\u2022 Assisted account officers with text mining on over 3000 clients' reviews via deep learning algorithms such as LSTM\nMACHINE LEARNING APPLICATIONS""]",[u'B.S. in Computer Science and Mathematics in Computer Science and Mathematics'],"[u'Georgia Institute of Technology Atlanta, GA\nJanuary 2014 to December 2017']",degree_1 : B.S. in Compter Science and Mathematics in Compter Science and Mathematics
0,https://resumes.indeed.com/resume/1663d5747f412c72,"[u'Data Scientist - GIS Analyst\nGeospatial Center BCC - New York, NY\nOctober 2017 to October 2017\n\u2022 Developed applications using ESRI ArcGIS and QGIS.\n\u2022 Wrote Python programs to automate GIS applications in Geospatial workshops.\n\u2022 Cleaned and managed the large volumes of infrastructure data sets using ArcGIS, R, SQL and Excel.', u'QA Analyst\nMyKlovr - New York, NY\nAugust 2017 to October 2017\n\u2022 Performed functional testing on MyKlovr virtual career counselor Website, Android and iOS Mobile Application.\n\u2022 Execution of Selenium scripts and reporting defects using Trello.\n\u2022 Worked on agile model.\n\u2022 Participated in weekly test planning, project status meetings, scrum calls and reviews.', u'Software Developer Intern\nRSA Infotech - Pune, Maharashtra\nJanuary 2012 to May 2014\nHome Automation \u2013 Remote Access to Your Devices\n\u2022 Gained good knowledge of software development life cycles (SDLC), in depth knowledge of requirement gathering, analysis, reporting and documentation.\n\u2022 Developed and implemented system to remotely control devices in the home.\n\u2022 Created Remote Android Application to access the user\u2019s home appliances.\n\u2022 Wrote a JAVA client application which ran on Amazon EC2 t2.micro instance.\n\u2022 Published paper - www.ijarcsse.com/docs/papers/Volume_5/3_March2015/V5I3-0571.pdf']","[u'Masters in Information Technology Management in Information Technology Management', u'Bachelor of Engineering in Computer Engineering in Advanced Database Management']","[u'Illinois Institute of Technology Chicago, IL\nMay 2017', u'University of Pune Pune, Maharashtra\nMay 2015']","degree_1 : Masters in Information Technology Management in Information Technology Management, degree_2 :  Bachelor of Engineering in Compter Engineering in Advanced Database Management"
0,https://resumes.indeed.com/resume/aab1da7da057b923,"[u'Data Scientist\nPine Biotech - New Orleans, LA\nDecember 2017 to Present\n\u2022 Assist in development of the T-BioInfo bioinformatics cloud platform (https://server.t-bio.info)\n\u2022 Develop computational methods for behavioral monitoring and analysis of mice for a DARPA project\n\u2022 Develop web-based educational material for bioinformatics in collaboration with the Tauber Bioinformatics Research Center (https://edu.t-bio.info)', u'Research Associate\nDept. of Biology, University of New Orleans - New Orleans, LA\nMay 2017 to Present\n\u2022 Develop a simulation of transposable element activity in an evolving human cell lineage (https://github.com/atallah-lab/sim-develop and https://github.com/atallah-lab/analyses)\n\u2022 Perform genomic analyses using the Bioconductor package for R, and other open-source bioinformatics tools (BLAST, BEDTools, SAMtools)', u'Research Associate\nLADC-GEMM Gulf Ecological Monitoring and Modeling - New Orleans, LA\nJune 2016 to Present\n\u2022 Design spectral analysis and machine learning routines to analyze long-term passive acoustic monitoring data (https://github.com/jackgle/ladc-gemm-scripts)\n\u2022 Develop statistical models of cetacean population densities\n\u2022 Perform dynamical system modeling of bioacoustic signals', u'Computer Scientist\nNaval Oceanographic Office - Stennis Space Center, MS\nApril 2016 to November 2016\n\u2022 Developed software for GIS database support and multibeam sonar data validation on a Linux system using C/C++ and shell scripting\n\u2022 Upgraded network software for hydrographic data processing on an oceanographic survey vessel', u'Research Assistant\nDepartment of Physics, University of New Orleans - New Orleans, LA\nMay 2015 to January 2016\n\u2022 Applied unsupervised clustering to the optimization of ocean transmission loss modeling\n\u2022 Developed and tested statistical algorithms for internal cluster validation\n\u2022 Wrote scripts for structuring and visualizing large binary datasets in MATLAB', u'Research Assistant\nDept. of Earth and Environmental Sciences, Univ. of New Orleans - New Orleans, LA\nJanuary 2015 to December 2015\n\u2022 Developed MATLAB programs for coastal processes research involving statistics, signal and image processing, and vector transformations\n\u2022 Processed hydrodynamic data and applied results to coastal dynamics models\n\u2022 Developed GIS data conversion programs', u'Software Design Lead \u2013 Team Maxwell\nLouisiana Space Grant Consortium\nSeptember 2014 to March 2015\n\u2022 Led a team of four in the development of a high-altitude atmospheric radiation detection system\n\u2022 Wrote Arduino IDE software for microcontroller communication and data flow\n\u2022 Co-authored a critical design review of the system\n\u2022 Implemented the device and presented results at NASA\u2019s Columbia Scientific Balloon Facility']","[u""Master's in Applied Physics"", u'Bachelor of Science in Physics']","[u'University of New Orleans New Orleans, LA\nDecember 2017', u'University of New Orleans, College of Sciences New Orleans, LA\nDecember 2015']","degree_1 : ""Masters in Applied Physics"", degree_2 :  Bachelor of Science in Physics"
0,https://resumes.indeed.com/resume/8a8ca0b038259099,"[u'Data Scientist\nSantander Bank - Holmdel, NJ\nJune 2017 to Present\nSantander Bank is based in Boston and its principal market is the northeastern United States. The Bank offers financial services and products including retail banking, mortgages, corporate banking, cash management, credit card, capital markets, trust and wealth management, and insurance.\n\nThe project was to build predictive models for the identification/detection of fraudulent transactions by applying machine learning methods, principle component analysis, and logistic regression on large dataset.\n\nResponsibilities:\n\u2022 Participated in all phases of data acquisition, data cleaning, developing models, validation, and visualization to deliver data science solutions.\n\u2022 Worked on fraud detection analysis on payments transactions using the history of transactions with supervised learning methods.\n\u2022 Collected data in Hadoop and retrieved the data required for building models using Hive.\n\u2022 Developed Spark Python modules for machine learning & predictive analytics in Hadoop.\n\u2022 Used Pandas, Numpy, Seaborn, Matplotlib, Scikit-learn in Python for developing various machine learning models and utilized algorithms such as Decision Trees, Logistic regression, Gradient Boosting, SVM and KNN.\n\u2022 Used cross-validation to test the models with different batches of data to optimize the models and prevent overfitting.\n\u2022 Used PCA and other feature engineering techniques for high dimensional datasets while maintaining the variance of most important features.\n\u2022 Created Transformation Pipelines for preprocessing large amount of data with methods such as imputing, scaling, selecting, etc.\n\u2022 Ensemble methods were used to increase the accuracy of the training model with different Bagging and Boosting methods.\n\nEnvironment:\nHadoop 2.x, HDFS, Hive, Pig Latin, Python 3.x (Numpy, Pandas, Scikit-learn, Matplotlib), Jupyter, GitHub, Linux', u""Data Analyst/Data Scientist\nSCIO Health Analytics - Hartford, CT\nApril 2015 to May 2017\nSCIO Health Analytics provides analytics solutions and services that turns data into actionable insights for health care providers in the United States and globally. Services also include medical and pharmacy claims auditing, inpatient data pursuits, care gaps closure, and commercial analytics.\n\nI was part of the team that worked with Subrogation claims of Healthcare Providers such as Humana. The objective was to load data, analyze, and provide monthly reports for the predictions on a claim's potential of a third-party recovery. Tableau and SSRS were used to build claim and recovery reports.\n\nResponsibilities:\n\u2022 Assembled a Predictive Modelling module by using supervised learning for Subrogation Claim Prediction to identify which claims would be classified as having Subrogation potential.\n\u2022 Implemented models such as Logistic Regression and Na\xefve Bayes, in Python using scikit-learn, to predict the claim potential outcome.\n\u2022 Dimensionality Reduction techniques applied to refine the attribute lists and feature selection applied to rank selected features to generate accurate results.\n\u2022 Gathered requirements and business rules from business users to implement Predictive Modelling.\n\u2022 Designed and developed ETL packages using SSIS to create Data Warehouses from different tables and file sources like Flat and Excel files, with different methods in SSIS such as derived columns, aggregations, Merge joins, count, conditional split and more to transform the data.\n\u2022 Designed reporting solutions for different stakeholders from mock-up till deployment in different areas such as Potential Subrogation claims, Monthly Revenue from Subrogation & Transactions.\n\u2022 Performed data visualization and designed dashboards with Tableau, and provided complex reports, including charts, summaries, and graphs to interpret the findings for Adjustors to view various claim information.\n\u2022 Optimized queries in T-SQL by removing unnecessary columns and redundant data, normalized tables, established joins and indices; developed complex SQL queries, stored procedures, views, functions and reports that meet customer requirements.\n\nEnvironment:\nPython 3.x (Scikit-learn, Matplotlib), Jupyter, SQL Server 2012, MS SQL Server Management Studio, MS BI Suite (SSIS/SSRS), T-SQL, Visual Studio BIDS, Tableau"", u'SQL BI Developer\nADP - Chennai, Tamil Nadu\nFebruary 2012 to March 2015\nADP is a leading provider of human resources management software and services worldwide.\n\nThis project was done for an internal business unit (ADP France) to comply with French statutory requirements for employee training. Goal was to develop a web-based SQL application built upon a baseline HRMS application to generate/support the Report development for training plans, budget preparation, cost tracking and 2483 reporting.\n\nResponsibilities:\n\u2022 Collected requirements from business users, and designed report models to meet business requirements.\n\u2022 Directed and managed meetings with clients, tracked document changes and ensured sign-off from clients.\n\u2022 Maintained and developed complex SQL queries, stored procedures, views, functions and reports that meet customer requirements using Microsoft SQL Server 2008 R2.\n\u2022 Created Views and Table-valued Functions, Common Table Expression (CTE), joins, complex subqueries to provide the reporting solutions.\n\u2022 Optimized the performance of queries with modification in T-SQL queries, removed the unnecessary columns and redundant data, normalized tables, established joins and created index.\n\u2022 Created, managed, and delivered interactive web-based reports to support daily operations.\n\u2022 Validated reports and resolve issues in a timely manner.\n\u2022 Developed and implemented several types of Reports (Training Reports, Schedules, Costs Summary Reports and Annual 2483 Report) by using features of SSRS such as sub-reports, drill down reports, summary reports and parameterized reports.\n\u2022 Designed and developed new reports and maintained existing reports for the Human Resource Management System Dashboards using Tableau, Qlikview and Microsoft Excel to support the business strategy and management.\n\u2022 Identified process improvements that significantly reduce workloads or improve quality.\n\nEnvironment:\nSQL Server 2008 R2, MS SQL Server Management Studio, SSRS, T-SQL, Visual Studio BIDS, Tableau, Qlikview']",[u'Master of Science in Business & Information Systems in Business & Information Systems'],"[u'New Jersey Institute of Technology Newark, NJ']",degree_1 : Master of Science in Bsiness & Information Systems in Bsiness & Information Systems
0,https://resumes.indeed.com/resume/4c7e4eda8124ba38,"[u""Data Scientist\nTech Mahindra Ltd - Hyderabad, Telangana\nJanuary 2014 to December 2015\n\u2022 Mahindra Holidays Ltd:\n\u25e6 Worked with the CTO and team to address member referral conversion challenge and to develop a recommender system to advertise holiday packages\n\u25e6 Performed Exploratory Analysis on 3 years historic data to determine the key influencers and capture trends\n\u25e6 Constructed a simulation model based on Linear Regression to predict the outcome based on different referral benefits, utilized the model to prescribe the best suitable Referral and Sign-up benefits\n\u25e6 Achieved a 20% increase in Referral conversions within half-years' time\n\u25e6 Engineered a Recommendation System used to advertise new Holiday Packages to existing members\n\u25e6 Used Items based Collaborative Filtering in the model to determine similar holiday packages\n\u25e6 Benefits to the extent of 10% more Bookings was achieved through this\n\u2022 Mahindra Agriculture Ltd:\n\u25e6 Worked with the Agriculture R&D team to develop a Prescriptive Analytics product for the 'Farm Help' device\n\u25e6 'Farm Help' is an AI device continuously capturing data pertaining to Soil Nutrients, Water and Weather conditions to provide inputs to farmers on Farming actions be undertaken\n\u25e6 Constructed a Prescriptive Data model based on Ideal conditions/ nutrient levels for the Time Series Data provided by the device to provide specific inputs to farmers\n\u25e6 Embedded the model in the AI device, deployed on-site to provide real-time inputs to farmers and succeeded in achieving better yield and reducing cost of farming\n\u2022 Tech Mahindra Analytics Platform (TAP):\n\u25e6 TAP is the in-house Analytics product of Tech Mahindra akin to IBM Watson\n\u25e6 Developed Generalized modules in R for Exploratory Data Analysis and Forecasting\n\u2022 Tech Mahindra Sales Operations:\n\u25e6 Worked with the Tech Mahindra Business Development team to address high Day Sales Outstanding challenge\n\u25e6 Executed very detailed Exploratory Analysis using historic data to identify Root Causes, Trends and Best Practices in different Geographies\n\u25e6 Advised Changes to Payment Clauses in Contractual agreements and increased Sales Collections Oversight, this contributed to 30% decrease in Collection time"", u'Sr. Data Analyst\nTech Mahindra Ltd - Hyderabad, Telangana\nJanuary 2011 to December 2013\n\u2022 Worked as the Chief Data Analyst in the Corporate Operations Unit, closely supporting the Chief Operating Officer\n\u2022 Developed various Performance based Operations KPIs and the Computational Methodology\n\u2022 Collaborated with Business stakeholders in acting on complex, multi-source data to Explore, Generate and Test Business Assumptions\n\u2022 Partnered with other Analysts to Develop Data Infrastructure (data pipelines, reports etc.) and other tools to make Analytics easier and more Effective\n\u2022 Developed Ad-hoc analyses to aid team in understanding Customer Behavior, developed POCs, and drove Decision-Making\n\u2022 Collaborated with Data Architects on changes associated with Data Systems and System Interfaces\n\u2022 Mined Large amounts of Structured Data to Determine how our customers interact with our Products and Service Offerings\n\u2022 Lead teams in Complex Analytical Initiatives - Combining Data from multiple Sources to Extract Meaning, Reconcile assumptions, and Identify Logical paths for Action\n\u2022 Significantly contributed in achieving 7X increase in Net Profit from -3% to 21%, through very efficient Optimization Measures backed by solid Data Analysis\n\u2022 Took part in two major Mergers and Acquisitions worth $1.6B as a key member of the Operations track, worked on Systems and Processes Integration, Headcount Optimization and PMOs Training']","[u""Master's in Data Science"", u""Bachelor's in Computer Science""]","[u'Indiana University Bloomington, IN\nJanuary 2017', u'Jawaharlal Technological University Anantapur, Andhra Pradesh\nJanuary 2010']","degree_1 : ""Masters in Data Science"", degree_2 :  ""Bachelors in Compter Science"""
0,https://resumes.indeed.com/resume/2d5240a86a2b497f,"[u'Senior Data Scientist\nTransUnion - Chicago, IL\nJanuary 2017 to Present', u'Senior Statistician\nPredictive Analytics Department - Webster, MA\nApril 2016 to Present\n\u2022 Responsible for research and development of predictive models and analytical techniques for subjects involving customer retention, fraudulent detection, proactive claim handling, loss prediction, litigation outcome and other related topics utilizing internal and external data\n\u2022 Work closely with IT, Product Management, Actuarial and Claim teams to push forward the projects', u'Statistician II\nPredictive Analytics Department - Webster, MA\nAugust 2013 to April 2016\n\u2022 Effectively understood the business problems to identify the optimal modeling techniques\n\u2022 Analyzed large raw datasets, including data collection, data cleaning, data matching, missing values imputation, outlier treatments, variable selection and so on \u2022 Explored and identified the significant predictors that can be used to predict the future outcomes\n\u2022 Developed various predictive models and communicated the analysis results to marketing, underwriting, claims, enterprise contact center and other internal teams\n\u2022 Worked on implementing the models and tracked the results using modeling monitoring reports', u'Data Analyst II\nPredictive Analytics Department - Webster, MA\nMay 2013 to August 2013\nIdentifying both internal and external data sources, extracting and cleansing data elements, creating variables and datasets and utilizing programming and analytical skills to conduct qualitative and quantitative analysis', u'PMO Access Database Developer\nUniversity of Connecticut - Storrs, CT\nMarch 2012 to May 2013\n\u2022 Successfully created a MS Access database, wrote complex queries, edited macros, created, updated and published online reports\n\u2022 Wrote SQL queries for database access, modifications, and constructions including stored procedures', u'Intern\nHoneywell Environmental Combustion Controls Global Development Center - \u5929\u6d25\u5e02\nJuly 2009 to September 2009\n\u2022 Successfully collaborated with engineers and researchers in US and India to standardized, collect, analyze and document product data using Honeywell Product Data Management Tool\n\u2022 Identified and implemented data processes through research and design with the global ECC teams']","[u'Master of Science in Statistics', u'Master of Management in Management', u'Bachelor of Engineering in Engineering']","[u'University of Connecticut\nMay 2013', u'Tianjin University of Technology\nJune 2011', u'Tianjin University of Technology\nJune 2009']","degree_1 : Master of Science in Statistics, degree_2 :  Master of Management in Management, degree_3 :  Bachelor of Engineering in Engineering"
0,https://resumes.indeed.com/resume/c2195238f46fd2a5,"[u'Data Scientist\nEpisona - Pasadena, CA\nMay 2016 to Present\n\u2022 Identified epigenetic features important in male fertility using mixed effect beta regression models. (R)\n\u2022 Developed a predictive model to classify fertile and infertile patients using epigenetic markers extracted from large human\ngenome methylation data and cost sensitive supervised learning (Weka, R, Python, Big Data).\n\u2022 Developed platform to transform from array technology to a DNA sequencing platform to reduce costs while maintaining the accuracy of the predictive models. (Python, R, Snakemake)\n\u2022 Designed a validation study to assess repeatability, specificity and sensitivity of the 450k array platform and establish guidlines for use in clinical settings. (R)\n\u2022 Developed pipeline to perform quality assessment to detect erroneous data based on previously established guidelines. (R, C++,\nLinux)\n\u2022 Identified genes related to environmental effects in a population of infertile men using DNA mehtylation 450k array data.(R)\n\u2022 Developed pipeline to perform association studies on large genome methylation datasets using statistical models, to gain a\nbetter understanding of the potential new features for the final product. (R, Snakemake, Python)', u'Research Assistant\nUniversity of Southern California - Los Angeles, CA\nAugust 2011 to Present\nDeveloped a multi-level, high throughput tracking and behavioral analysis system to study flying targets using computer vision\nalgorithms, Gaussian mixture models and Markov chains. (Matlab, R)\n\u2022 Developed a semi-manual GUI based object tracking system to generate low-resolution/high-accuracy trajectories in non-trivial\ntracking problems. This system has been used in the behavioral study of flies and ants. (Matlab)\n\u2022 Created an automated tracking system for the study of the behavior of many Drosophila in different environments. The\ncollected data was studied using visualization tools. (Matlab, R)\n\u2022 Collaborated in the development of an extensible visualization platform for tracking of small organisms. This platform has a\nplug-in-based architecture to allow for rapid prototyping of new approaches to tracking. (Python, Qt, Open-CV)\n\u2022 Collaborated in the development of machine learning based system to annotate and model animal behaviors. This system uses\ndifferent machine learning algorithms (SVM, Boosting, ) to train behavior detection classifiers. (Python, Qt, Open-cv)', u'Researcher at Bioinformatics Group\nInstitute for Studies in Theoretical Physics and Mathematics - Tehran, IR\nApril 2010 to April 2011\n\u2022 Developed novel algorithm to predict protein active sites using graph models & approximate sub-graph isomorphism. (Matlab)\n\u2022 Used spatial feature extraction of protein secondary structure to train a classifier for protein classification. (Matlab)']","[u'PhD in Biology & Bioinformatics', u""Master's in Statistics"", u""Master's in Biomedical Engineering"", u""Bachelor's in Electrical Engineering""]","[u'University of Southern California Los Angeles, CA\nJune 2017', u'University of Southern California Los Angeles, CA\nJanuary 2014 to January 2016', u'Sharif University of Technology Tehran\nSeptember 2008 to January 2011', u'Sharif University of Technology Tehran\nSeptember 2004 to September 2008']","degree_1 : PhD in Biology & Bioinformatics, degree_2 :  ""Masters in Statistics"", degree_3 :  ""Masters in Biomedical Engineering"", degree_4 :  ""Bachelors in Electrical Engineering"""
0,https://resumes.indeed.com/resume/b4ee15df644c2405,"[u'Data Engineer / Data Scientist\nThis is Girish Sukhwani (Data Engineer / Scientist). Currently, I am located in Morristown, NJ. My current project is going to end in this week.Actively I am looking job all over the USA(Open to relocate). Please find my attached updated resume and let me know if you need any additional information\n\nI am really good in Hadoop ecosystem tools, Spark, Scala, Machine Learning, Tenser flow, Python, and Java.\n\n\n\n\nBest regards\nGirish']","[u""Bachelor's""]",[u''],"degree_1 : ""Bachelors"""
0,https://resumes.indeed.com/resume/57e714b8549abe16,"[u'Data Scientist\nForeal Spectrum - Fremont, CA\nAugust 2017 to Present\nDesigned multiple real-time prediction models based on both supervised and unsupervised learning.\n\u25e6 Performed image segmentation on food image data using pyrMeanShiftFiltering in OpenCV.\n\u25e6 Implemented fine-tuned InceptionV3 and InceptionResNetV2 models with Keras to do food recognition on school cafeteria food data set with over 100 classes and achieved 93 % accuracy for top 5 classes.\n\u25e6 Applied Faster R-CNN using TensorFlow with VGG16 pre-trained model on food images with multiple dishes\nto detect location of food and recognized food type simultaneously to save time.', u'Assistant Data Analyst\nOverseas Chinese Fund - Hangzhou, CN\nJuly 2016 to August 2016\nWrote SQL queries to get age, investment amount and geographic distribution for investors throughout the whole year. Improved customer experience by analyzing collected information.\n\u25e6 Found potential investors of all ages to help company further expand the business regions.\n\u25e6 Determined the peak time to invest every day to help engineering team to make proper arrangement.\n\u25e6 Found the most popular range of investment amount then eliminated 3 redundant investment options.']","[u'Master of Science in Data Science in Data Science', u'Bachelor of Science in Applied Mathematics in Applied Mathematics']","[u'New York University New York, NY\nSeptember 2015 to May 2017', u'ZheJiang University Hangzhou, CN\nSeptember 2011 to June 2015']","degree_1 : Master of Science in Data Science in Data Science, degree_2 :  Bachelor of Science in Applied Mathematics in Applied Mathematics"
0,https://resumes.indeed.com/resume/27feccea90ddeb8a,"[u'Data Scientist\nCognizant Technology Solutions\nJanuary 2013 to Present\nDeveloped data models, algorithms to implement machine learning solutions as a member of data scientist/data engineer team.\n\u2756 Ingestion of data from different source system via Sqooping/FTPS and perform data enrichment, exploration and enrichment.\n\u2756 Involved in creating/designing Hive tables, and loading and analyzing data using hive queries\n\u2756 Implemented Partitioning, Dynamic Partitions, Buckets in HIVE.\n\u2756 Developed GBM classification model for DNA mapping of more than 475 labels with 95% accuracy.\n\u2756 Chatbot for Document Search Engine using NLP, Tf-idf and web scrapping for keyword searching using cosine similarity in urls and webpage content.\n\u2756 Crawled 6,000 url documents and 2,000 pdf documents from their html links with Beautiful Soup and Goose library.\n\u2756 Applied PCA algorithm to decrease the 2000 dimensions dataset to 120 Dimension with only 0.3% decrease in accuracy.\n\u2756 Exploratory Analysis and feature engineering to best fit the model using Python.\n\u2756 Find best model and parameters using SK-learn Grid Search CV, Cross validation and Metrics.\n\u2756 Applied Ensemble using XGBOOST algorithm to identify the important features and used it to improve the accuracy of the prediction with Random Forest algorithm.\n\u2756 Initiated and Implemented deep-learning works with LSTM for sentiment analysis.\n\u2756 Multivariate analysis to build an algorithm for forecasting of next quarter variance using time series analysis and RNN with an accuracy of 86.2%.\n\u2756 Created framework to build and deploy Jars on cluster built using SBT assembly for different Machine learning modules such as Training, Prediction, Transformation.\n\u2756 Implementation of POCs/POVs with Spark Data Frames and SQL, and Spark Machine Learning (by using objects like Transformer and Estimator for Pipelines, Evaluator, Cross Validator).\n\u2756 Responsible to deliver data driven end-to-end solution, i.e. ingesting raw data, transformation, model training and generating prediction using Cloudera big data ecosystem (Sqoop, Spark, Hue, Hive, Impala).\n\u2756 Expertise in SK-learn, Tensor flow, Keras and H2O libraries.\n\u2756 Implemented data analytics projects and dashboard development on Tableau.', u'ETL Architect\nTata Consultancy Services\nJanuary 2012 to January 2012\nExtensively worked on Informatica, PL/SQL, Linux shell scripting as an ETL architect.\n\u2756 Develop various Oracle PLSQL package, procedure and Unix shell script as per business requirement.\n\u2756 Created the Data Warehouse Data Model to store atomic level data and the summarized data.\n\u2756 Performed SQL performance tuning & optimized queries using Explain plan.\n\u2756 Designed complex mapping architecture for critical applications with data enrichment, mapping design and flow of data.\n\u2756 Architectural design, development & production support of financial applications for Back office IT operation.\n\u2756 Worked on to implement the partitioning and sub partitioning methodologies to store and process high volume data in optimized fashion.\n\u2756 Responsible in gathering requirements from users and designing Use cases, Technical Design and Implementation of end solutions for customers.', u'Software Engineer\nBirlasoft\nJanuary 2009 to January 2011\nDeveloped mapping to extract, transform and load data using Informatica Power center.\n\u2756 RPD, report and dashboard development in OBIEE.']","[u'Master of Technology in Software Engineering', u'Bachelor of Technology in Electronics & Comm', u'Diploma in Mechanical Engineering', u'in Technology', u'']","[u'Birla Institute of Technology', u'Kurushetra University', u'Board of Technical Education', u'Stanford', u'Informatica Power center']","degree_1 : Master of Technology in Software Engineering, degree_2 :  Bachelor of Technology in Electronics & Comm, degree_3 :  Diploma in Mechanical Engineering, degree_4 :  in Technology, degree_5 :  "
0,https://resumes.indeed.com/resume/e615778e0e8cb385,"[u'Data Scientist\nCadence Design Systems\nFebruary 2014 to July 2015\nDescription: Cadence Design Systems, Inc. is an American multinational electronic design automation software and engineering services company, founded in 1988 by the merger of SDA Systems and ECAD, Inc.\n\nResponsibilities:\n\u2022 Responsible for performing Machine-learning techniques regression/classification to predict the outcomes.\n\u2022 Responsible for design and development of advanced R/Python programs to prepare transform and harmonize data sets in preparation for modeling.\n\u2022 Identifying and executing process improvements, hands-on in various technologies such as Oracle, Informatica and Business Objects.\n\u2022 Designed the prototype of the Data mart and documented possible outcome from it for end-user.\n\u2022 Involved in business process modeling using UML\n\u2022 Developed and maintained data dictionary to create metadata reports for technical and business purpose.\n\u2022 Handled importing data from various data sources, performed transformations using Hive, Map Reduce, and loaded data into HDFS.\n\u2022 Interaction with Business Analyst, SMEs and other Data Architects to understand Business needs and functionality for various project solutions.\n\u2022 Created SQL tables with referential integrity and developed queries using SQL, SQL*PLUS and PL/SQL.\n\u2022 Involved with Data Analysis primarily Identifying Data Sets, Source Data, Source Meta Data, Data Definitions and Data Formats\n\u2022 Performance tuning of the database, which includes indexes, and optimizing SQL statements, monitoring the server\n\u2022 Wrote simple and advanced SQL queries and scripts to create standard and adhoc reports for senior managers.\n\u2022 Collaborate the data mapping document from source to target and the data quality assessments for the source data.\n\u2022 Created PL/SQL packages and Database Triggers and developed user procedures and prepared user manuals for the new programs.\n\u2022 Participated in Business meetings to understand the business needs & requirements.\n\u2022 Prepare ETL architect& design document which covers ETL architect, SSIS design, extraction, transformation and loading of Duck Creek data into dimensional model.\n\u2022 Provide technical & requirement guidance to the team members for ETL -SSISdesign.\n\u2022 Participated in Business meetings to understand the business needs & requirements.\n\u2022 Design ETL framework and development.\n\u2022 Design Logical & Physical Data Model using MS Visio 2003 data modeler tool.\n\u2022 Participated in stake holders meetings to understand the business needs & requirements.\n\u2022 Participated in Architect solution meetings & guidance in Dimensional Data Modeling design.\n\u2022 Coordinate and communicate with technical teams for any data requirements.\n\nEnvironment: Python, Spark MLlib, TensorFlow, K- means, ANN, Regression, Oryx 2, Accord.NET, Flask, ORM, Jinja 2, Amazon Machine Learning (AML),Apache,Django, Mako, Naive Bayes, SVM.', u""Data Scientist\nCeridian HCM, Inc - Minneapolis, MN\nAugust 2012 to January 2014\nDescription: Ceridian HCM, Inc. is a provider of human resources software and services with employees in the USA, Canada, Europe and Mauritius.\n\nResponsibilities:\n\u2022 Data modeling and formulation of statistical equations using advanced statistical forecasting techniques.\n\u2022 Document the complete process flow to describe program development, testing, application integration, coding and implementation.\n\u2022 Scoring predictive models as per regulatory requirements & ensuring deliverables with PSI.\n\u2022 Built predictive scorecards for Life Insurance, TD, Cross-selling Car loan and RD.\n\u2022 Mentoring Provide guidance to team members.\n\u2022 Developing propensity models for Retail liability products to drive proactive campaigns.\n\u2022 Transformation, Data cleansing and creating new variables using R.\n\u2022 Responsible for defining the functional requirement documents for each source to target interface.\n\u2022 Identifying the Customer and account attributes required for MDM implementation from disparate sources and preparing detailed documentation.\n\u2022 Approve and Present designed Logical Data Model in Data Model Governance Committee (DMGC)\n\u2022 Tabulation and Extraction of data from multiple data sources using R, SAS.\n\u2022 Work with users to identify the most appropriate source of record and profile the data required for sales and service.\n\u2022 Extracted data from HDFS and prepared data for exploratory analysis using datamunging.\n\u2022 Responsible for defining the key identifiers for each mapping/interface.\n\u2022 Validated the machine learning classifiers using ROC Curves and Lift Charts.\n\u2022 Arrange and chair Data Workshops with SME's and related stake holders for requirement data catalogue understanding.\n\nEnvironment: regression, logistic regression, Hadoop, Teradata, OLTP, Unix, Python, MLLib, SAS, random forest, OLAP, HDFS, NLTK, SVM, JSON and XML."", u'Data Scientist/Data Analyst\nSalesForce - San Francisco, CA\nMay 2008 to December 2010\nDescription: Salesforce.com, inc. develops enterprise cloud computing solutions with a focus on customer relationship management. The company offers Sales Cloud to store data, monitor leads and progress, forecast opportunities, gain insights through relationship intelligence, and collaborate around sales on desktop and mobile devices, as well as solutions for partner relationship management. It also provides Service Cloud, which enables companies to deliver personalized customer service and support, as well as connects their service agents with customers on various devices.\n\nResponsibilities:\n\u2022 Worked on data cleaning and reshaping, generated segmented subsets using Numpy and Pandas in Python.\n\u2022 Developed Python scripts to automate data sampling process. Ensured the data integrity by checking for completeness, duplication, accuracy, and consistency.\n\u2022 Worked on model selection based on confusion matrices, minimized the Type II error\n\u2022 Generated cost-benefit analysis to quantify the model implementation comparing with the former situation.\n\u2022 Wrote and optimized complex SQL queries involving multiple joins and advanced analytical functions to perform data extraction and merging from large volumes of historical data stored in Oracle 11g, validating the ETL processed data in target database.\n\u2022 Conducted model optimization and comparison using stepwise function based on AIC value\n\u2022 Applied various machine learning algorithms and statistical modeling like decision tree, logistic regression, Gradient Boosting Machine to build predictive model using scikit-learn package in Python\n\u2022 Continuously collected business requirements during the whole project life cycle.\n\u2022 Identified the variables that significantly affect the target\n\u2022 Generated data analysis reports using Matplotlib, Tableau, successfully delivered and presented the results for C-level decision makers.\n\nEnvironment: Teradata 13, Erwin 8, SQL Server 2008, Oracle 9i, PL/SQL, OLAP, Informatica Power Center, SQL*Loader, ODS, OLTP, SSAS.', u'Data Modeler\nSalesForce - Brentwood, TN\nMay 2008 to December 2010\nDescription: Sterling Insurance Company is located in Cobleskill, New York. Since 1895 we have been providing quality insurance protection to New Yorkers with an ever expanding array of product offerings.\n\nResponsibilities:\n\u2022 Develop Integrations jobs to transfer data from source system to Hadoop.\n\u2022 Installation of Talend Studio.\n\u2022 Technical design documents for Transformation processes.\n\u2022 Application of business rules on the data being transferred.\n\u2022 Task allocation for the ETL and Reporting team.\n\u2022 Communicate effectively with client and their internal development team to deliver product functionality requirements.\n\u2022 Architecting and design of data warehouse ETL processes.\n\u2022 Demo of POC built for the prospective customer and provide guidance and gather the feedback to backend ETL testing on SQL Server 2008 using SSIS.\n\u2022 Create the Operational manual Document.\n\u2022 Create Integration Jobs to backup a copy of data in network file system.\n\u2022 Design and implement the ETL Data model and create staging, source and Target tables in SQL server database.\n\u2022 Gathering and analysis requirements definition meetings with business users and document meeting outcomes.\n\nEnvironment: ETL, ODS, Hadoop, MS Office, Talend Studio, OLAP , SQL Server 2008.', u'Data Modeler\nSalesForce - IN\nMay 2008 to December 2010\nDescription: Bharti Airtel Limited commonly known as Airtel is an Indian telecommunications company that operates in 20 countries across South Asia, Africa and the Channel Islands. It operates a GSM network in all countries, providing 2G or 3G services depending upon the country of operation. Airtel is the third largest telecom operator in the world with over 243.336 million customers across 20 countries.\n\nResponsibilities:\n\u2022 Developed and implemented predictive models using Natural Language Processing Techniques and machine learning algorithms such as linear regression, classification, multivariate regression, Naive Bayes, Random Forests, K-means clustering, KNN, PCA and regularization for data analysis.\n\u2022 Designed and developed Natural Language Processing models for sentiment analysis.\n\u2022 Applied clustering algorithms i.e. Hierarchical, K-means with help of Scikit and Scipy.\n\u2022 Developed visualizations and dashboards using ggplot, Tableau.\n\u2022 Worked on development of data warehouse, Data Lake and ETL systems using relational and non relationaltools like SQL, NoSQL.\n\u2022 Built and analyzed datasets using Matlab R, SAS and Python (in decreasing order of usage)\n\u2022 Participated in all phases of datamining; datacollection, datacleaning, developingmodels, validation, visualization and performed Gapanalysis.\n\u2022 DataManipulation and Aggregation from different source using Nexus, Toad, Business Objects, PowerBI and Smart View.\n\u2022 Implemented Agile Methodology for building an internal application.\n\u2022 Good knowledge of Hadoop Architecture and various components such as HDFS, JobTracker, Task Tracker, Name Node, Data Node, Secondary Name Node, and MapReduce concepts.\n\u2022 As Architect delivered various complex OLAP databases/cubes, scorecards, dashboards and reports.\n\u2022 Programmed a utility in Python that used multiple packages (scipy, numpy, pandas)\n\u2022 Implemented Classification using supervised algorithms like Logistic Regression, Decision trees, KNN, and Naive Bayes.\n\u2022 Used Teradata15 utilities such as Fast Export, MLOAD for handling various tasks data migration/ETL from OLTP Source Systems to OLAP Target Systems\n\u2022 Maintenance in the testing team for System testing/Integration/UAT.\n\u2022 Involved in preparation & design of technical documents like Bus Matrix Document, PPDM Model, and LDM & PDM.\n\u2022 Understanding the client business problems and analyzing the data by using appropriate Statistical models to generate insights.\n\nEnvironment: MDM, QlikView, MLLib, HADOOP, MapReduce, PIG, MAHOUT, R, Erwin, Tableau, PL/SQL, JAVA, HIVE, AWS, HDFS, Teradata, JSON, Spark, R Studio.', u""Data Analyst\nSalesForce - Bengaluru, Karnataka\nMay 2008 to December 2010\nDescription: Exide Life Insurance Company Limited, is a 100% Indian owned life insurance company, owned by the Exide Industries. Exide Life Insurance distributes its products through multi-channels.\n\nResponsibilities:\n\u2022 Developed Tableau visualizations and dashboards using Tableau Desktop.\n\u2022 Applied Business Objects best practices during development with a strong focus on reusability and better performance.\n\u2022 Developed and executed load scripts using Teradata client utilities FASTLOAD, MULTILOAD and BTEQ.\n\u2022 Generated periodic reports based on the statistical analysis of the data using SQL Server Reporting Services.\n\u2022 Used Graphical Entity-Relationship Diagramming to create new database design via easy to use, graphical interface.\n\u2022 Maintained metadata (data definitions of table structures) and version controlling for the data model.\n\u2022 Designed different type of STAR schemas for detailed data marts and plan data marts in the OLAP environment.\n\u2022 Write SQL scripts to test the mappings and Developed Traceability Matrix of Business Requirements mapped to Test Scripts to ensure any Change Control in requirements leads to test case update.\n\u2022 Responsible for development and testing of conversion programs for importing Data from text files into map Oracle Database utilizing PERL, shell scripts & SQL*Loader.\n\u2022 Utilized Erwin's forward/reverse engineering tools and target database schema conversion process.\n\u2022 Developed SQLscripts for creating tables, Sequences, Triggers, views and materialized views.\n\u2022 Co-ordinate with various business users, stakeholders and SME to get Functional expertise, design and business test scenarios review, UAT participation and validation of financial data.\n\u2022 Worked on creating enterprise wide Model EDM for products and services in Teradata Environment based on the data from PDM. Conceived, designed, developed and implemented this model from the scratch.\n\nEnvironment: Oracle SQL Developer, MS SQL Server, SQL*PLUS, PL/SQL, Business Objects, SQL*LOADER, Tableau, Informatica, XML, Windows XP, TOAD, Business Objects.""]",[],[],degree_1 : 
0,https://resumes.indeed.com/resume/f360b3b1f7763128,"[u""Data Scientist\nArgus Information & Advisory Services, LLC - White Plains, NY\nSeptember 2016 to Present\nArgus Information and Advisory Services is a Subsidiary of Verisk Analytics Company and the leading provider of analytics, information and solutions to consumer banks and their regulators. The company's clients range from financial institutions to retailers and tech companies. The project focused on detecting anti-money laundering violation using Big Data and Data Science tools and improving customer's transaction monitoring system.\nResponsibilities:\n\u2022 Collected and analyzed the business requirements, understood the particular Fraud/AML challenges that our client faces.\n\u2022 Participated in Data integration job with Data Engineer team to gather traditional transaction data and external source data together.\n\u2022 Transformed data from SQL Server database to Hadoop Clusters which is set up by using AWS EMR.\n\u2022 Conducted data cleansing and feature engineering job through python NumPy and Pandas.\n\u2022 Implemented Naive Bayes, Logistic Regression, SVM, Random Forest and Gradient boosting with weighted loss function by using Python Scikit-learn.\n\u2022 Implemented mulit-layers Neural Networks by using Google Tensorflow and Spark.\n\u2022 Performed extensive Behavioral modeling and Customer Segmentation to discover behavior patterns of customers by using K-means Clustering.\n\u2022 Managed and scheduled models by using Oozie for batch processing.\n\u2022 Updated and saved Fraud predictions to AWS S3 for application team.\n\u2022 Tested the business performance of the AML models by evaluating detection rate and false positive rate and worked on continuous improvement on model.\n\u2022 Created reports and dashboards, by using Tableau, to explain and communicated data insights, significant features, model's score and performance of new transaction monitoring system to both technical and business teams.\n\u2022 Used GitHub for version control with Data Engineer team and Data Scientists colleagues.\nEnvironment: SQL Server 2014, Hadoop 2.0, Hive 2.0, Spark (PySpark, SparkSQL), Python 3.X, Tensorflow, Oozie 4.2, Tableau 10.X, AWS S3/EC2/EMR, Github"", u'Data Scientist\nCenterLight Health System - Bronx, NY\nApril 2015 to July 2016\nCenterLight Health System, a not-for-profit organization, has evolved into a leader in serving the elderly, chronically ill and disabled. CenterLight is one of the largest long-term care providers in New York State, serving all of New York City, Westchester, Nassau, Rockland and Suffolk Counties. This project aimed to predict the billing cycles and accounting related issues to increase the efficiency of enterprise claim processing.\n\nResponsibilities:\n\u2022 Conducted reverse engineering based on demo reports to understand the data without documentation.\n\u2022 Generated new data mapping documentations and redefined the proper requirements in detail.\n\u2022 Generated different Data Marts for gathering the tables needed (Member info, Claim info, Transaction info, Appointment info, Diagnose info) from SQL Server Database.\n\u2022 Created ETL packages to transform data into the right format and join tables together to get all features required using SSIS.\n\u2022 Processed data using Python pandas to examine transaction data, identify outliers and inconsistencies.\n\u2022 Conducted exploratory data analysis using python NumPy and Seaborn to see the insights of data and validate each feature through different charts and graphs.\n\u2022 Built predictive models including Linear regression, Lasso Regression, Random Forest Regression and Support Vector Regression to predict the claim closing gap by using python scikit-learn.\n\u2022 Used GridSearchCV to evaluate each model and to find best parameters set for each model.\n\u2022 Created reports and an app demo using Tableau to show client how prediction can help the business.\n\u2022 Deployed and hosted our models by using Azure Machine Learning Studio and share an API with application development team.\n\u2022 Used Confluence to share and collaborate on projects with team members, and keep track of up to date documentations.\n\nEnvironment: SQL Server 2012, SQL Server Data Tools 2010, SQL Server Integration Services, Python 2.7/3.3, Tableau 9.4, Azure Machine Learning Studio', u""Junior Data Scientist\nAtlantic Health - Morristown, NJ\nJanuary 2014 to March 2015\nAtlantic Health System is one of the leading non-profit health care systems in New Jersey, providing a wide array of health care services to the residents of Northern and Central regions of the state as well as Pike County, PA, and southern Orange County, NY. Project was to build a predictive model to predict the readmission case. The main objective was to reduce the risk of being wrongly diagnosed and the risk of being involved in the legal disputes.\n\nResponsibilities:\n\u2022 Communicated and coordinated with other departments to gather business requirements.\n\u2022 Gathered data information from multiple sources, and performed resampling method to handle the issue of imbalanced data.\n\u2022 Worked with ETL Team and Doctors to understand the data and define the uniform standard format.\n\u2022 Conducted data cleansing by using advanced SQL queries in SQL Server Database.\n\u2022 Split the data into different smaller dataset based on different diagnoses, in charge of conducting exploratory data analysis for three of diagnoses datasets (Diabetes, cold/flu, allergy).\n\u2022 Created the whole pipeline of data preprocessing (imputing, scaling, label encoding) through python pandas to get data ready to modeling part.\n\u2022 Built predictive models, using python scikit-learn, including Support Vector Machine, Decision tree, Naive Bayes Classifier, Neural Network to predict a potential readmitted case.\n\u2022 Performed Ensemble methods, including Gradient Boosting, Random Forest, customized ensemble method to produce more accurate solutions.\n\u2022 Designed and implemented cross-validation and statistical tests including Hypothesis testing, AVOVA, Chi-square test to verify models' significance.\n\u2022 Created a API by using Flask and shared the idea with application team and help them define the requirements of new application.\n\u2022 Used Agile methodology and Scrum process for project developing.\n\nEnvironment: SQL server 2012, SQL Server Integration Services, Python 2.7, Jupyter notebook, Flask 0.10, SharePoint 2013"", u'BI Developer\nFulton Financial Corporation - Lancaster, PA\nDecember 2012 to October 2013\nFulton is a financial company based in Lancaster, Pennsylvania. They provide a wide range of financial products and personalized services in Pennsylvania, Maryland, Delaware, Virginia and New Jersey. They are comprised of several different banking subsidiaries. The main job of this project was to provide ETL solutions for data migration and provide data quality and micro strategy solutions.\n\nResponsibilities:\n\u2022 Involved in gathering user/project requirements from business users and IT managers, translated it into functional and non-functional specifications needed and created documentations for the project.\n\u2022 Assisted in design and data modeling efforts of Data Marts and Enterprise Data Warehouse.\n\u2022 Used T-SQL in SQL Server to develop complex stored procedures, triggers, clustered index & non-clustered index, Views, and User-defined Functions (UDFs).\n\u2022 Designed SSIS packages to extract, transform and load existing data into SQL Server, used lots of components of SSIS, such as Pivot Transformation, Fuzzy Lookup, Derived Columns, Condition Split, Aggregate, Execute SQL Task, Data Flow Task and Execute Package Task.\n\u2022 Created SSIS Packages that involved dealing with different source formats (Text files, XML, Database Tables)\n\u2022 Debugged and troubleshot the ETL packages by using breakpoint, analyzing process, catching error information by SQL command in SSIS.\n\u2022 Create reports with the use of SSRS to generate different types of reports such as tabular, matrix, drill down and charts reports with accordance with user requirement.\n\u2022 Maintained and updated existing reports, analyzed the SQL queries and logic behind them to improve the performance.\n\u2022 Helped deploy the report with scheduling, subscription, history snapshot configured and set up.\n\u2022 Developed in Agile environment throughout the project.\n\nEnvironment: SQL server 2008/2012, SQL Server Management Studio (SSMS), MS BI Suite (SSIS, SSRS)']",[u'Master of Science in Electrical Engineering'],[u'Stevens Institute of Technology'],degree_1 : Master of Science in Electrical Engineering
0,https://resumes.indeed.com/resume/e9db4a2c8cb6bb0d,"[u'Data Scientist\nMontefiore Medical Center\nMay 2015 to Present\nUSA\n\u2022 Presently working with Montefiore Medical Center to investigate and build the quantitative models using R.\n\u2022 Design, scope, and drive implementation to test hypothesis of data-driven models and programs to improve the efficiency in identifying the high risk patients.\n\u2022 Worked on building a model by choosing selective behavioral, demographic and transactional attributes of patients from the enterprise data warehouse. Using statistical models, such as, RFM Analysis, Logistic Regression and Decision Trees, to obtain potential customers, who would be receptive to new Health Plan.\n\u2022 Developed efficient pig and hive scripts with joins on datasets using various techniques in a Big Data environment.\n\u2022 Analyzed large data sets to develop custom models and algorithms to drive business solutions.\n\u2022 Created predictive models using structured and unstructured data available within various departments.\n\u2022 Loaded unstructured data in HIVE and analyzed the data for reporting purpose.\n\u2022 Applied advanced analytical methods to improve decision-making.\n\u2022 Analyzed diverse sources of data, and implemented various machine learning algorithms to realize actionable results.\n\u2022 Solved complex decision-making problems using machine learning, statistical analysis, and mathematical optimization\n\u2022 Assessed various machine learning frameworks and packages.\n\u2022 Building the Data Models and working with Operation team to deploy it in Production environment.\n\u2022 Collaborated effectively with other data sciences teams to align on direction and leverage existing tools and knowledge\n\u2022 Assessed the data quality using R scripts and provided the meaningful insights.\n\u2022 Worked on the data analytics project, that provides data driven solutions to improve the cross-selling strategies implemented by the client. The goal is to identify dependent variables that would influence an existing member in deciding to purchase additional Health Plan benefits.\n\u2022 Developed models and algorithms to learn the recommended strategies.\n\u2022 Worked on both Supervised and Unsupervised Learning Techniques including Neural Network.', u'IT Analyst\nUNM IT\nJanuary 2014 to May 2015\nUSA\n\u2022 Worked closely with key business partners to understand business and operations-related problems, and then applied analytical methods to solve them.\n\u2022 Formulated the development strategy and developed the project plan using MS Project.\n\u2022 Assisted the Project Manager in Project Planning and Work Breakdown Structure\n\u2022 Translated business requirements into system and functional requirements.\n\u2022 Analyzed the Patient data for forecasting the impressions of Care Management intervention.\n\u2022 Worked in an Agile environment using JIRA and coordinating with other team members to follow the best practices.\n\u2022 Developed Use Cases, Activity, Sequence Diagrams and system design and database structure.\n\u2022 Designed and created the management and operation reports using SSIS tool.', u'Sr. System Engineer\nIBM Corporation\nJune 2008 to January 2014\nUK & India\n\u2022 Experience in interacting with business users and executives to identify their needs, gathering requirements and authoring Business Requirement Documents (BRD), Use Case Diagrams, Activity Diagrams and Sequence Diagrams using UML modeling.\n\u2022 Coordinated with other technology teams to extract, transform, and load data from a wide variety of data sources using SQL and other ETL solutions\n\u2022 Wrote complex SQL queries PL/SQL function, procedure, packages, cursor and triggers to retrieve the data from sources system and to count and validate the data and data set.\n\u2022 Gathered the Business Requirements from the Stakeholders and created the functional requirement documents.\n\u2022 Created to Source to Target Mapping documents in an ETL project using Informatica Power Center.\n\u2022 Finalized the project scope transferred over to IBM.\n\u2022 Monitored and assessed the Knowledge Transfer, performed gap analysis, and mitigated risk.\n\u2022 Executed the outsourcing strategy of a software testing project from UK to India.']","[u'MBA in Project Management', u'in Electronics and Computer Engineering']","[u'University of New Mexico Albuquerque, NM\nDecember 2015', u'Rajiv Gandhi Technical University\nMay 2008']","degree_1 : MBA in Project Management, degree_2 :  in Electronics and Compter Engineering"
0,https://resumes.indeed.com/resume/83c48df800bd5377,"[u""Associate\nJP Morgan Chase & Co - New York, NY\nJune 2017 to Present\nSupport various tactical and strategic initiatives, including the implementation of FORCE (J.P. Morgan's Op\nRisk tool)\n\u2022 Analyze the Risk Control Self-Assessment (RCSA) related data, support LOBs with RCSA related matters\nusing Python and R\n\u2022 Improve RCSA framework and integrate other risk assessment tools into a sustainable operating model that\nreduces duplication, improves efficiency and produces optimal results using Excel\n\u2022 Identify problems by detect patterns or connections, determining alternative solutions & creates action steps\nto solve the problem. thinks, acts quickly & effectively in a variety of situations\n\u2022 Solve the problem while maintaining a balance between the time invested, the amount of information needed\nand the magnitude of the problem and evaluates both the short & long-term consequences of decisions\n\u2022 Develop, test and evaluate quantitative models in a consulting-style environment, which places a premium on problem solving and client interaction\n\u2022 Partner with JPM quants, technologists and business leaders to model, price, implement trading and risk\nmanagement strategies across all asset class and partner in discussions in data modeling and algorithmic\ndesign\n\u2022 Developed 20+ Tableau dashboards that provide automated & self-service reporting and repeatable insights\non product performances, responsible for data-exploration \u2192 data-aggregation \u2192 Tableau prototyping \u2192\nproduction. End results include Executive-level dashboards (for senior management) and Product-specific\ndashboards (for product managers\n\u2022 Produce firmwide Metrics presentations using data extracted with Tableau and Excel, enabling internal issue\ncontrol and the ability to identify process improvement opportunities\n\u2022 Projection analysis and working on developing products that use real time device-based Machine Learning\nusing R, Review of weekly reconciliations, implementation, variance and projection analysis"", u'Data Scientist / Intern\nEdge Geoscience Inc - Houston, TX\nApril 2017 to June 2017\nData mining, extracted, cleaned, audited and prepared data for analysis and relying on well- structured\nprocedures and maintaining reproducibility of results\n\u2022 Exploring data will occupy the largest portion of attention, and should be second nature in order to deeply\nunderstand the phenomenon being modeled, and the validity and reliability of the inputs, including but not\nlimited to inspecting univariate distributions, exploring bivariate relationships, constructing appropriate\ntransformations, and tracking down the source and meaning of anomalies when and where they arise\n\u2022 Validating models against alternative approaches, expected and observed outcome, and numerous directly and indirectly relevant business defined key performance indicators\n\u2022 Reviewing models of peers for the purpose of reducing and managing risk to the business, and maximizing\nimprovement of business practice and customer experience\n\u2022 Automate scoring using machine learning techniques', u'Data Scientist /Quantitative Research\nMehr Finance and Credit Inc\nJanuary 2011 to November 2014\nResearch Execution, Design and launch surveys and other initiatives that will generate insights to inform\nproduct development, including testing and optimizing concepts, determining appropriate pricing, estimating\nproduct trial and usage, defining the appropriate customer target market, etc. using both traditional and innovative quantitative research techniques\n\u2022 Prepare and Communicate Insights, Analyze research results, prepare report summaries and share research\ninsights with project leads and other colleagues to help guide and refine the development of concepts, products and services\n\u2022 Analyze data to identify opportunities to improve the customer experience and drive actionable insights\n\u2022 Develop, extract and maintain logical and physical data models for data analytics\n\u2022 Design and develop predictive models and machine learning algorithms using advanced methodologies\n\u2022 Identify specific opportunities to leverage new data sources or systems to improve ads delivery or measurement for marketers\n\u2022 Assess the validity and rigor of new data sources and approaches, establishing scalable frameworks for ongoing evaluation as appropriate']","[u'M.S., in Applied Mathematics in Financial Mathematics', u'M.S., in Applied and computational Mathematics in Applied and computational Mathematics', u'B.S., in Applied Mathematics in Applied Mathematics']","[u'University of Houston Houston, TX\nJanuary 2016', u'Semnan University\nJanuary 2014', u'Yasuj University\nJanuary 2011']","degree_1 : M.S., degree_2 :  in Applied Mathematics in Financial Mathematics, degree_3 :  M.S., degree_4 :  in Applied and comptational Mathematics in Applied and comptational Mathematics, degree_5 :  B.S., degree_6 :  in Applied Mathematics in Applied Mathematics"
0,https://resumes.indeed.com/resume/8d65e416b68e27b7,"[u""DATA SCIENTIST\nerawtnegA\nJanuary 2002 to January 2002\nsdradnats )PSAWO(\ntcejorP ytiruceS noitacilppA beW nepO ot demrofnoc stcudorp lla gnirusne, snoitacilppa SAPR dna SAPR rof tcatnoC fo tnioP ytiruceS AQ sa devreS\n.troper tcefed ni sgnidnif yna gnitnemucod ylraelc dna gnidoc erofeb\nsksir laitnetop gninialpxe, sreenignE erawtfoS htiw ylesolc yrev dekroW\n.gnidoc ot roirp tnetnoc dna egarevoc\ntset rof snoitadnemmocer edam dna sweiveR ngiseD ni detapicitraP\n.rorre resu rof ecnahc gnicuder yltaerg dna, emit ni noitcuder %07\nni gnitluser, ssecorp eht detamotua dna troper tcefed ylkeew depoleveD\n)1102 - 7002( tsylanA AQ roineS\n\n.snoitseuq dedne-desolc dna -nepo gniksa\ndna sisylana sdeen remotsuc hguorht sdeen cificeps 'sremotsuc derevocnU cisaB - hsinapS\n.sessecorp dna stnemeriuqer s'tneilc etaidemretnI - nailatI\nno desab, snoitacilppa gninnalP liateR dezimotsuc tliub dna dengiseD tneulF - hcnerF\n)1102( tnatlusnoC noitacilppA lapicnirP: segaugnaL ngieroF\n1102 - 7002 AG, atnaltA, elcarO\nSEGAUGNAL NGIEROF\nECNEIREPXE LANOISSEFORP\nDATA SCIENTIST""]",[u'BACHELOR OF ARTS in General Assembly'],"[u'OGLETHORPE UNIVERSITY Atlanta, GA']",degree_1 : BACHELOR OF ARTS in General Assembly
0,https://resumes.indeed.com/resume/ffcf8b9318030a1c,"[u'Scientist\nMachine Learning / Data - Menlo Park, CA\nJuly 2017 to Present\nscience frameworks:\n\u25cf created four machine learning models for classification of Scikit-Learn, NumPy, Pandas,\nsmall-angle x-ray scattering spectra using supervised and Tensorflow, Keras, Scipy\nunsupervised learning (Python, Scikit-Learn, Pandas)\nData Visualization: D3,\nhttps://github.com/scattering-central/saxskit\nMatplotlib, Seaborn\n\u25cf created three regression models for prediction of continuous\nvalues for small-angle x-ray scattering spectra Database Management: SQL,\n\u25cf developed API interaction (getting data from a NoSQL database, MySQL, PostgreSQL, NoSQL\nusing of pre-trained models for predictions) database( Citrination)\n\u25cf developed a web-interface for processing small-angle x-ray\nMachine Learning technique:\nscattering (Python, Flask, Bootstrap)\nClustering, Regression,\nhttps://github.com/slaclab/paws-web\nClassification, Deep learning,\n\u25cf developed user documentation for SASXKIT in HTML format\nCNN\nusing Sphinx\nBig Data technology and Concepts: Hadoop,\nNuLEAF Technologies project of The Advanced Studies', u'Intern (member\nLaboratory at NASA - Mountain View, CA\nJune 2015 to August 2015\nBootstrap\n\u25cf Developed a database for keeping track of internal events and Unix command line tools,\nannouncements using MySQL SSH, Git\n\nFoothill College, Los Altos, CA - Office Assistant, Data\nAnalyst\nSchool Year 2013/2014\n\n\u25cf performed data analysis using Excel', u'Specialist, Data Analyst\nDepartment of Distance Education - Saint Petersburg, RU\nSeptember 2007 to January 2012\nSept 2007 - Jan 2012\n\n\u25cf collected and analyzed students performance data using Excel\n\u25cf evaluated students books for corresponding to their education\nprograms\n\n\u25cf evaluated sets of tests for corresponding to students books\n\nRELEVANT COURSEWORK']","[u""Bachelor's Degree in Computer Science in Computer Science"", u'Associated Degree in Computer Science']","[u'UC Santa Cruz\nJune 2017', u'Foothill College Los Altos, CA\nAugust 2015']","degree_1 : ""Bachelors Degree in Compter Science in Compter Science"", degree_2 :  Associated Degree in Compter Science"
0,https://resumes.indeed.com/resume/42d0dec60cacdf0f,"[u'Data Scientist\nBecton Dickinson - San Diego, CA\nNovember 2016 to Present\nResponsibilities:\n\u2022 A highly immersive Data Science program involving Data Manipulation & Visualization, Web Scraping, Machine Learning, Python programming, SQL, GIT, Unix Commands, NoSQL, Mongo DB, Hadoop.\n\u2022 Setup storage and data analysis tools in Amazon Web Services cloud computing infrastructure.\n\u2022 Used pandas, numpy, seaborn, scipy, matplotlib, scikit-learn, NLTK in Python for developing various machine learning algorithms.\n\u2022 Installed and used Caffe Deep Learning Framework\n\u2022 Worked on different data formats such as JSON, XML and performed machine learning algorithms in Python.\n\u2022 Worked as Data Architects and IT Architects to understand the movement of data and its storage and ER Studio 9.7\n\u2022 Participated in all phases of data mining; data collection, data cleaning, developing models, validation, visualization and performed Gap analysis.\n\u2022 Data Manipulation and Aggregation from different source using Nexus, Toad, Business Objects, Power BI and Smart View.\n\u2022 Implemented Agile Methodology for building an internal application.\n\u2022 Focus on integration overlap and Informatica newer commitment to MDM with the acquisition of Identity Systems.\n\u2022 Good knowledge of Hadoop Architecture and various components such as HDFS, JobTracker, Task Tracker, Name Node, Data Node and MapReduce concepts.\n\u2022 As Architect delivered various complex OLAP databases/cubes, scorecards, dashboards and reports.\n\u2022 Programmed a utility in Python that used multiple packages (scipy, numpy, pandas)\n\u2022 Implemented Classification using supervised algorithms like Logistic Regression, Decision trees, KNN, Naive Bayes.\n\u2022 Used Teradata15 utilities such as Fast Export, MLOAD for handling various tasks data migration/ETL from OLTP Source Systems to OLAP Target Systems\n\u2022 Experience in Hadoop ecosystem components like Hadoop MapReduce, HDFS, HBase, Oozie, Hive, Sqoop, Pig, Flume including their installation and configuration.\n\u2022 Updated Python scripts to match training data with our database stored in AWS Cloud Search, so that we would be able to assign each document a response label for further classification.\n\nEnvironment: regression, logistic regression, Hadoop, Teradata, OLTP, Unix, Python, MLLib, SAS, random forest, OLAP, HDFS, NLTK, SVM, JSON and XML.', u'Data Scientist\nEquinix - Newark, NJ\nMarch 2016 to August 2016\nResponsibilities:\n\u2022 Involved in defining the source to target data mappings, business rules, and data definitions.\n\u2022 Coordinated with team on designing and implementing data solutions as per project requirements.\n\u2022 Detected fraudulent transactions quickly and to find or predicting future customers in the customer relationship management market.\n\u2022 Communicated and coordinated with other departments to collect business requirement.\n\u2022 Handled importing data from various data sources, performed transformations using Hive, Map Reduce, and loaded data into HDFS.\n\u2022 Performed data analysis by using Hive to retrieve the data from Hadoop cluster, Sql to retrieve data from Oracle database.\n\u2022 Used pandas, numpy, seaborn, scipy, matplotlib, scikit-learn in Python for developing various machine learning algorithms.\n\u2022 Developed Map Reduce pipeline for feature extraction using Hive.\n\u2022 Used Statistical testing to evaluate Model performance.\n\u2022 Improved fraud prediction performance by using random forest and gradient boosting for feature selection with Python Scikit-learn.\n\u2022 Implemented machine learning model (logistic regression, XGboost) with Python Scikit- learn.\n\u2022 Designed rich data visualizations with Tableau and Python.\n\nEnvironment: Machine learning, Cassandra, NLTK, Spark, HDFS, Hive, Pig, Linux, Python (Scikit-Learn/Scipy/Numpy/Pandas), R, SAS, SPSS, MySQL, PL/SQL, Jupyter notebook, Tableau.', u""Data Analyst/Data Modeler\nInfotech Enterprises IT Services Pvt\nMay 2012 to October 2015\nResponsibilities\n\u2022 Data analysis and reporting using MySQL, MS Power Point, MS Access and SQL assistant.\n\u2022 Involved in MySQL, MS Power Point, MS Access Database design and design new database on Netezza which will have optimized outcome.\n\u2022 Involved in writing T-SQL, working on SSIS, SSRS, SSAS, Data Cleansing, Data Scrubbing and Data Migration.\n\u2022 Involved in writing scripts for loading data to target data Warehouse using Bteq, Fast Load, Multi load\n\u2022 Create ETL scripts using Regular Expressions and custom tools (Informatica, Pentaho, and Sync Sort) to ETL data.\n\u2022 Developed SQL Service Broker to flow and sync of data from MS-I to Microsoft's master database management (MDM).\n\u2022 Involved in loading data between Netezza tables using NZSQL utility.\n\u2022 Worked on Data modeling using Dimensional Data Modeling, Star Schema/Snow Flake schema, and Fact& Dimensional, Physical& Logical data modeling.\n\u2022 Generated Stats pack/AWR reports from Oracle database and analyzed the reports for Oracle wait events, time consuming SQL queries, table space growth, and database growth.\n\nEnvironment: MySQL, MS Power Point, MS Access, MY SQL, MS Power Point, MS Access, Netezza, DB2, T-SQL, DTS, SSIS, SSRS, SSAS, ETL, MDM, Teradata, Oracle, Star Schema and Snow Flake Schema.""]","[u""Bachelor's""]",[u''],"degree_1 : ""Bachelors"""
0,https://resumes.indeed.com/resume/1e6ea92e3359206e,"[u""Data Scientist\nCapstone Solutions, Inc - Redmond, WA\nFebruary 2018 to Present\nLeading a data scientist team of three to improve the timesheet audit process to reduce cycle times and save\nmanual labor hours for a cell phone Infrastructure company.\n\nCreating a software dashboard, using Python and cloud hosted on AWS, to help company administrators prioritize\ntimesheets to audit.\n\nWorking closely with company data administrators to deliver insights into causes of timesheet errors\n\nDesigning normalized SQL database of 7 tables with information on all company employees and timesheets. Interfaces with Intuit's cloud-based Quickbooks TSheets."", u'Data Scientist - Student\nGalvanize, Inc - Seattle, WA\nNovember 2017 to February 2018\nCompleted Data Scientist Certification Program.\n\nDeveloped a recommender system in Python using Collaborative Filtering to recommend Magic: The\nGathering cards.\n\u25cb Utilized Spark ALS matrix factorization of implicit ratings to make live recommendations to online\nusers in under 2 seconds.\n\u25cb Designed and implemented an Amazon RDS PostgreSQL database to store 300,000+ records of web-scraped data across 3 tables.\n\u25cb Published interactive recommendation model at cardstorm.me\n\u25cf Programmed a wide variety of Machine Learning models, including Logistic and Linear Regression,\nGradient Boosting, Collaborative Filtering, Random Forests and Naive Bayes.\n\u25cf Worked in teams on case studies to predict fraud and churn and identify causes with real data sets.', u'Math and Physics Science Tutor\nSelf-Employed - Sammamish, WA\nSeptember 2011 to August 2017\nTutored middle school & high school students in science, algebra, geometry, pre-calculus, calculus and physics.\n\u25cf Worked with parents and struggling students to catch up, and with advanced students to get ahead.', u'Data Scientist & Researcher, Co-Head Author\nUniversity of Washington - Bothell, WA\nFebruary 2017 to June 2017\nAssisted Lead Scientist To Evaluate Data.\n\nDeveloped MATLAB software application code to locate individual crows from their calls within an array of\n4 microphones using difference time of arrival technique.\n\u25cf Presented work at the University of Washington Bothell Spring 2017 Research Symposium.', u'Logistics Data Coordinator\nAduro, Inc - Redmond, WA\nAugust 2013 to March 2014\nCoordinated supply chain vendors, equipment and materials data inventory, and shipping for national corporate wellness screenings program.\n\nhttps://www.linkedin.com/in/bwalzer/']","[u'Certificate in Data Science', u'BS in Mathematics']","[u'Galvanize Data Scientist Progam Seattle, WA\nNovember 2017 to February 2018', u'University of Washington Seattle, WA\nAugust 2017']","degree_1 : Certificate in Data Science, degree_2 :  BS in Mathematics"
0,https://resumes.indeed.com/resume/5e2b7ede746fef84,"[u'Data Scientist\nCompuCom\nMay 2017 to Present\nResponsibilities:\n\u2022 Performed data exploratory, data visualizations, and feature selections using Python and Apache Spark.\n\u2022 Designed a model to predict if a customer will respond to marketing campaign based on customer information.\n\u2022 Collaborated with data engineers and operation team to collect data from internal system to fit the analytical requirements.\n\u2022 Parsing data, producing concise conclusions from raw data in a clean, well-structured and easily maintainable format.\n\u2022 Build analytical solutions and models by manipulating large data sets.\n\u2022 Applied machine learning and statistical techniques to large datasets to find actionable insights.\n\u2022 Data transformation from various resources, data organization, features extraction from raw and stored.\n\u2022 Performed data processing using Python libraries like Numpy and Pandas.\n\u2022 Redefined many attributes and relationships and cleansed unwanted tables/columns using SQL queries.\n\u2022 Utilized Python libraries wxPython, Numpy, Twisted and matPlotLib.\n\u2022 Used Pandas, Numpy, Seaborn, SciPy, MatPlotLib, Scikit-Learn, NLTK in Python for developing various machine learning algorithms.\n\u2022 Developed Random forest and logistic regression models to observe this classification. Fine-tuned models to obtain more recall than accuracy. Tradeoff between False Positives and False Negatives.\n\u2022 Developed and implemented predictive models of user behavior data on websites, URL categorical, social network analysis, social mining and search content based on large-scale Machine Learning.\n\u2022 Worked on different data formats such as JSON, XML and performed machine learning algorithms in R and Python.\n\u2022 Converted pandas dataframe dataset to apache spark dataframe.\n\u2022 Developed scalable machine learning solutions within a distributed computation framework (e.g. Hadoop, Spark, Storm etc.).\n\u2022 Developed Spark/Scala, Python for regular expression (regex) project in the Hadoop/Hive environment with Linux/Windows for big data resources.\n\u2022 Trained model using historical data stored in HDFS and Amazon S3.\n\u2022 Applied unsupervised and supervised learning methods in analyzing high-dimensional data. Proficient use of Python scikit-learn, pandas, and Numpy packages.\n\u2022 Programmed a utility in Python that used multiple packages (SciPy, Numpy, pandas).\n\u2022 Cleaned input text data using Pyspark Machine learning feature exactions API.\n\u2022 Implemented Classification using supervised algorithms like Logistic Regression, Decision trees, KNN, Naive Bayes.\n\u2022 Used Hive to store the data and perform data cleaning steps for huge datasets.\n\u2022 Developed highly scalable classifiers and tools by leveraging machine learning, Apache spark & deep learning.\n\u2022 Handled importing data from various data sources, performed transformations using Hive, MapReduce, and loaded data into HDFS.\n\u2022 Researched, evaluated, architected, and deployed new tools, frameworks, and patterns to build sustainable Big Data platforms for the clients.\n\u2022 Fully automated job scheduling, monitoring, and cluster management without human intervention using airflow.', u""Data Scientist\nDTE Energy - Detroit, MI\nMarch 2016 to May 2017\nResponsibilities:\n\u2022 Responsible for data identification, collection, exploration, and cleaning for modeling, participate in model development.\n\u2022 Visualize, interpret, report findings, and develop strategic uses of data by python Libraries like Numpy, Scikit-learn, MatPlotLib.\n\u2022 Performed Data Cleaning, features scaling, features engineering.\n\u2022 Responsible for loading, extracting and validation of client data.\n\u2022 Creating statistical models using distributed and standalone models to build various diagnostics, predictive and prescriptive solution.\n\u2022 Missing value treatment, outlier capping and anomalies treatment using statistical methods, deriving customized key metrics.\n\u2022 Performed analysis using industry leading text mining, data mining, and analytical tools and open source software.\n\u2022 Understanding and implementation of text mining concepts, graph processing and semi structured and unstructured data processing.\n\u2022 Evaluated models using Cross Validation, Log loss function, ROC curves and used AUC for feature selection.\n\u2022 Setup storage and data analysis tools in Amazon Web Services cloud computing infrastructure.\n\u2022 Dummy variables where created for certain datasets to into the regression.\n\u2022 Developed triggers, stored procedures, functions and packages using cursors and ref cursor concepts associated with the project using Pl/SQL.\n\u2022 Used Meta data tool for importing metadata from repository, new job categories and creating new data elements.\n\u2022 Used MLlib, Spark's Machine learning library to build and evaluate different models.\n\u2022 Worked with several R packages including knitr, dplyr, SparkR, Causal Infer, space-time.\n\u2022 Build multiple features of machine learning using python, Scala and java based on need.\n\u2022 Developed Map Reduce pipeline for feature extraction using Hive.\n\u2022 Creating data pipelines using big data technologies like Hadoop, spark etc.\n\u2022 Strong skills in data visualization like matPlotLib and seaborn library.\n\u2022 Create different charts such as Heat maps, Bar charts, Line charts etc.,\n\u2022 Developed, Implemented & Maintained the Conceptual, Logical & Physical Data Models using Erwin for forwarding/Reverse Engineered Databases.\n\u2022 Collaborating with team to enhance modeling process and develop project.\n\u2022 Developed Hive queries for ad hoc analysis.\n\u2022 Analyzed the partitioned and bucketed data and compute various metrics for reporting.\n\u2022 Exported the result set from Hive to MySQL using Sqoop after processing the data."", u""Data Scientist/Data Analyst\nFrost Bank - Houston, TX\nFebruary 2015 to March 2016\nResponsibilities:\n\u2022 Responsible for performing Machine-learning techniques regression/classification to predict the outcomes.\n\u2022 Responsible for design and development of advanced Python programs to prepare transform and harmonize data sets in preparation for modeling.\n\u2022 Implemented public segmentation using unsupervised machine learning algorithms by implementing k-means algorithm using Pyspark.\n\u2022 Worked on Data Cleaning, features scaling, features engineering.\n\u2022 Utilized Convolution Neural Networks to implement a machine learning image recognition component.\n\u2022 Partnered with modelers to develop data frame requirements for projects.\n\u2022 Visualize, interpret, report findings, and develop strategic uses of data by python Libraries like Numpy, Scikit-learn, MatPlotLib.\n\u2022 Advanced charts, drill downs and intractability are incorporated in reporting for different stakeholders and integrating the publishing of reports to the client's infrastructure.\n\u2022 Performed Data Validation, Data Cleaning, Data Verification and Identifying data mismatch.\n\u2022 Used forward engineering to create a Physical Data Model with DDL that best suits the requirements from the Logical Data Model.\n\u2022 Analyzing raw data, concluding, developing recommendations and manipulate data for data loads and extracts.\n\u2022 Performed data analysis and profiling of source data to better understand the sources.\n\u2022 Developed large data sets from structured and unstructured data. Perform data mining.\n\u2022 Determined the most accurately prediction model based on the accuracy rate.\n\u2022 Perform complex modeling, simulation and analysis of data and processes.\n\u2022 Performed Ad-hoc reporting/customer profiling, segmentation using Python.\n\u2022 Create, maintain, modify and optimize SQL Server databases.\n\u2022 Executed SQL queries from Python on complex table configurations.\n\u2022 Retrieving data from database through SQL as per business requirements.\n\u2022 Used text-mining process of reviews to determine customers' concentrations.\n\u2022 Involved in data analysis with using different analytic techniques and modeling techniques."", u'Data Modeler/Data Analysist\nAdvent Global Solutions - IN\nMarch 2013 to December 2014\nResponsibilities:\n\u2022 Worked on data cleaning and reshaping, generated segmented subsets using Numpy and Pandas in Python.\n\u2022 Data analysis of existing database to understand the data flow and business rule applied to Different databases by SQL.\n\u2022 Worked with large amounts of structured and unstructured data.\n\u2022 Generated cost-benefit analysis to quantify the model implementation comparing with the former situation.\n\u2022 Designed and Developed Oracle PL/SQL and Shell Scripts, Data Import/Export, Data Conversions and Data Cleansing.\n\u2022 Responsible for data governance rules and standards to maintain the consistency of the business element names in the different data layers.\n\u2022 Performed data analysis on the target tables to make sure the data as per the business expectations.\n\u2022 Worked on different data formats such as Flat files, SQL files, Databases, XML schema, CSV files.\n\u2022 Involved in dimensional modeling, identifying the facts and dimensions.\n\u2022 Developed SQL scripts for creating tables, Sequences, Triggers, views and materialized views.\n\u2022 Created package for automated loading of historical and ongoing data into aggregates.\n\u2022 Identified, formulated and documented detailed business rules and Use Cases based on requirements analysis.\n\u2022 Write Python scripts for file transfer and file manipulation.\n\u2022 Divided model into subject areas for reflecting understandable view to business as well as data model reviewers.\n\u2022 Managed and created and altered Databases, Tables, Views, Indexes and Constraints with business rules.\n\u2022 Performed unit testing, System integrated testing for the aggregate tables.', u'Python Developer\nProgress Software - IN\nAugust 2010 to February 2013\nResponsibilities:\n\u2022 Designed the application using Python, HTML, CSS, AJAX, JSON and jQuery. Worked on backend of the application.\n\u2022 Involved in the Design, development, and test and deploy the website.\n\u2022 Designed and developed the UI of the website using HTML, AJAX, CSS, jQuery and JavaScript.\n\u2022 Designed and Implemented the Python in Eclipse PyDev.\n\u2022 Developed entire frontend and backend modules using Python on Django Web Framework.\n\u2022 Performed database operations and queries using MySQL.\n\u2022 Generated property list for every application dynamically using Python.\n\u2022 Used Python and Django to interface with the jQuery UI and manage the storage and deletion of content.\n\u2022 Worked on developing screens for delete, edit, enable and disable users in web application.\n\u2022 Designed and developed components using Python with Django framework. Implemented code in python to retrieve and manipulate data.\n\u2022 Written Python scripts to parse XML documents and load data in database.\n\u2022 Optimized the database queries to improve the Performance.\n\u2022 Designed and developed Forms, templates and mapped URLs using Django guidelines.\n\u2022 Rewrite existing Python/Django module to deliver certain format of data.\n\u2022 Handled all the client-side validation using JavaScript.\n\u2022 Implement code in Python to retrieve and manipulate data.\n\u2022 Responsible for debugging and troubleshooting the web application.\n\u2022 Developed Unit test cases and performed integration and system testing.\n\u2022 Executed various MySQL database queries from Python using MySQL connector and MySQL database package.']",[u'Bachelor of Technology in Computer Science and Engineering'],[u'JNTU'],degree_1 : Bachelor of Technology in Compter Science and Engineering
0,https://resumes.indeed.com/resume/33e9ccb67acf2d74,"[u'JUNIOR DATA SCIENTIST\nAT&T SHANNON LABS\nAugust 2017 to Present\nHelped optimize LSTM model with lead machine learning\ndeveloper to replace legacy name recognition system\nCreated script to automate statistical analysis of chat logs,\nreducing work load up to 50%\nIdentified opportunity & developed Python/bash script to automate data tag checking; efficiency increased by 75%.\nKept Linux server up to date, ran CRON jobs, etc.\nAnalyzed & marked up text for entities', u'LEAD DATA ANALYST\nRutgers University - New Brunswick, NJ\nMay 2016 to August 2016\nManaged a team of 5 individuals focused on:\nTeaching beginning members how to work with Python and sentiment\nanalysis of Tweets (using NLTK)\nThen applying these techniques to creating Twitter/Facebook crawlers\n\nDARPAN\nthat would find themes in media accounts\nCreated a preliminary model of the social medial analysis that we wanted to perform for each potential football recruit\n\nGANATRA\nSet up a pipeline for dashboard developers that took live Twitter feeds to our server and let us perform analysis on the tweets in real time', u'JUNIOR FINANCIAL ANALYST\nRutgers University\nSeptember 2014 to May 2015\nLed a team that conducted qualitative and quantitative\nresearch on stock picks.\nAnalyzed corporate performance metrics using Bloomberg\nterminal, and successfully predicted future stock performance.\nUsed Excel macros to automatically create tables, and keep\nCONTACT analysis up to date from global spreadsheets\nUsed a macro to create a pivot table of all company liquidity\n(after using formula to calculate), ROA, etc. from a global sheet\n845.699.4298\nthat had the information scraped\nGANATRADARPAN@GMAIL.COM\nINTERNATIONAL ECONOMICS HONOR SOCIETY\nLITTLE INVESTMENT BANKERS OF RUTGERS (LIBOR)\nQUANTITATIVE FINANCE CLUB (QFC)\nRUTGERS UNIVERSITY MATHEMATICS CLUB (RUMC)\nBUSINESS ASSOCIATION OF SUPPLY EXPERTISE']",[u'BS in Mathematics'],"[u'Rutgers University-New Brunswick New Brunswick, NJ\nSeptember 2013 to May 2018']",degree_1 : BS in Mathematics
0,https://resumes.indeed.com/resume/3120183f679ccd2c,"[u'Data Scientist/ Machine Learning\nJohnson and Johnson - Raritan, NJ\nFebruary 2017 to Present\nDescription: Johnson & Johnson is an American multinational medical devices, pharmaceutical and consumer packaged goods manufacturing company founded in 1886. Its common stock is a component of the Dow Jones Industrial Average and the company is listed among the Fortune 500. Johnson & Johnson is headquartered in New Brunswick, New Jersey, the consumer division being located in Skillman, New Jersey. The corporation includes some 250 subsidiary companies with operations in 60 countries and products sold in over 175 countries\n\nResponsibilities:\n\u2022 Setup storage and data analysis tools in Amazon Webservices cloud computing infrastructure.\n\u2022 Installed and used Caffe Deep Learning Framework\n\u2022 Worked on different data formats such as JSON, XML and performed machine learning algorithms in R.\n\u2022 Worked as Data Architects and IT Architects to understand the movement of data and its storage and ER Studio 9.7\n\u2022 Participated in all phases of datamining; data collection, data cleaning, developing models, validation, visualization and performed Gap analysis.\n\u2022 Data Manipulation and Aggregation from a different source using Nexus, Toad, Business Objects, PowerBI and SmartView.\n\u2022 Implemented Agile Methodology for building an internal application.\n\u2022 Focus on integration overlap and Informatica newer commitment to MDM with the acquisition of Identity Systems.\n\u2022 Good knowledge of Hadoop Architecture and various components such as HDFS, Job Tracker, Task Tracker, Name Node, Data Node, Secondary Name Node, and MapReduce concepts.\n\u2022 As Architect delivered various complex OLAP databases/cubes, scorecards, dashboards and reports.\n\u2022 Programmed a utility in R that used multiple packages (scipy, numpy, pandas)\n\u2022 Implemented Classification using supervised algorithms like Logistic Regression, Decision trees, KNN, Naive Bayes.\n\u2022 Updated R scripts to match training data with our database stored in AWS Cloud Search, so that we would be able to assign each document a response label for further classification.\n\u2022 Data transformation from various resources, data organization, features extraction from raw and stored.\n\u2022 Handled importing data from various data sources, performed transformations using Hive, MapReduce, and loaded data into HDFS.\n\u2022 Interaction with Business Analyst, SMEs, and other Data Architects to understand Business needs and functionality for various project solutions\n\u2022 Researched, evaluated, architected, and deployed new tools, frameworks, and patterns to built sustainable Big Data platforms for the clients\n\u2022 Identifying and executing process improvements, hands-on in various technologies such as Oracle, Informatica, Business Objects.\n\u2022 Designed both 3NF data models for ODS, OLTP systems and dimensional data models using Star and Snowflake Schemas.\n\nEnvironment: R 9.0, Informatica 9.0, ODS, OLTP, Bigdata, Oracle 10g, Hive, OLAP, DB2, Metadata, MS Excel, Mainframes MS Vision, Rational Rose.', u'R Developer\nHitachi Consulting - Pune, Maharashtra\nDecember 2010 to Present\nEnvironment: SQL/Server, Oracle, MS-Office, Teradata, Informatica, ER Studio, XML, Business Objects, HDFS, Teradata 14.1, JSON, HADOOP (HDFS), MapReduce, PIG, Spark, R Studio, MAHOUT, JAVA, HIVE, AWS.\nClient: Hitachi Consulting - Pune, IN Dec 2010 - Aug 2012\nR Developer\n\nDescription: Hitachi corporate statement, ""Inspire the Next"" expresses Hitachi\'s determination to breathe new life into the next era. With its social innovation businesses, Hitachi strives to become the ""Best Solutions Partner"" and help create a comfortable and abundant society.\n\nResponsibilities:\n\u2022 Worked with large amounts of structured and unstructured data.\n\u2022 Knowledge of Machine Learning concepts (Generalized Linear models, Regularization, Random Forest, Time Series models, etc.)\n\u2022 Worked in Business Intelligence tools and visualization tools such as Business Objects, ChartIO, etc.\n\u2022 Deployed GUI pages by using JSP, JSTL, HTML, DHTML, XHTML, CSS, JavaScript, and AJAX.\n\u2022 Configured the project on WebSphere 6.1 application servers\n\u2022 Implemented the online application by using Core Java, JDBC, JSP, Servlets and EJB 1.1, Web Services, SOAP, WSDL.\n\u2022 Handled end-to-end project from data discovery to model deployment.\n\u2022 Monitoring the automated loading processes.\n\u2022 Communicated with other Health Care info by using Web Services with the help of SOAP, WSDL JAX-RPC\n\u2022 Used Singleton, factory design pattern, DAO Design Patterns based on the application requirements\n\u2022 Used SAX and DOM parsers to parse the raw XML documents\n\u2022 Used RAD as Development IDE for web applications.\n\u2022 Preparing and executing Unit test cases\n\u2022 Used Log4J logging framework to write Log messages with various levels.\n\u2022 Involved in fixing bugs and minor enhancements to the front-end modules.\n\u2022 Implemented Microsoft Visio and Rational Rose for designing the Use Case Diagrams, Class model, Sequence diagrams, and Activity diagrams for SDLC process of the application\n\u2022 Doing functional and technical reviews\n\u2022 Maintenance in the testing team for System testing/Integration/UAT.\n\u2022 Guaranteeing quality in the deliverables.\n\u2022 Conducted Design reviews and Technical reviews with other project stakeholders.\n\u2022 Was a part of the complete life cycle of the project from the requirements to the production support\n\u2022 Created test plan documents for all back-end database modules\n\u2022 Implemented the project in Linux environment.\n\nEnvironment: R, Erwin,MDM, QlikView, Machine learning, MLlib, PL/SQL, HDFS, Teradata, JSON, HADOOP (HDFS), MapReduce, PIG, R Studio, MAHOUT, JAVA, HIVE, AWS.', u""Data Scientist\nAIM Specialty Health - Chicago, IL\nOctober 2015 to November 2016\nDescription: AIM Specialty Health\xae (AIM) provides clinical solutions that drive appropriate, safe, and affordable care. Serving more than 50 million members across 50 states, D.C. and U.S. territories, AIM promotes optimal care through use of evidence-based clinical guidelines and real-time decision support for both providers and their patients. The AIM platform delivers significant cost-of-care savings across an expanding set of clinical domains, including radiology, cardiology, oncology, specialty drugs, sleep medicine, musculoskeletal care, and genetic testing.\n\nResponsibilities:\n\u2022 Analyzed the business requirements of the project by studying the Business Requirements Specification document.\n\u2022 Extensively worked on Data Modeling tools Erwin Data Modeler to design the datamodels.\n\u2022 Designeda mapping to process the incremental changes that exist in the source table. Whenever source data elements were missing in source tables, these were modified/added inconsistency with third normal form based OLTP source database.\n\u2022 Worked in Statistical Modeling and Machine Learning techniques (Linear, Logistics, Decision Trees, Random Forest, SVM, K-Nearest Neighbors, Bayesian, XGBoost) in Forecasting/ Predictive Analytics, Segmentation methodologies, Regression-basedmodels, Hypothesis testing, Factor analysis/ PCA, Ensembles.\n\u2022 Designed tables and implemented the naming conventions for Logical and Physical Data Models in Erwin7.0.\n\u2022 Participated inthe conversion of ITS (Immigration Tracking System) Visual Basic client-server application into C#, ASP.NET3-tierIntranet application.\n\u2022 Performed Exploratory Data Analysis and Data Visualizations using R, and Tableau.\n\u2022 Perform a proper EDA, Uni-variate and bi-variate analysis to understand the intrinsic effect/combined effects.\n\u2022 Worked with Data Governance, Data quality, data lineage, Data architect to design various models and processes.\n\u2022 Independently coded new programs and designed Tables to load and test the program effectively for the given POC's using with Big Data/Hadoop.\n\u2022 Designed data models and data flow diagrams using Erwin and MS Visio.\n\u2022 Utilized ADO.Net Object Model to implement middle-tier components that interacted with MSSQL Server 2000database.\n\u2022 Participated in AMS (AlertManagementSystem) JAVA and SYBASE project. Designed SYBASE database utilizing ERWIN. Customized error messages utilizing SP_ADDMESSAGE and SP_BINDMSG. Created indexes, made query optimizations. Wrote stored procedures, triggers utilizing T-SQL.\n\u2022 Explained the data model to the other members of thedevelopment team. Wrote XML parsing module that populates alerts from the XML file into the database tables utilizing JAVA, JDBC, BEAWEBLOGICIDE, Document Object Model.\n\u2022 As an Architect implemented MDMhub to provide clean, consistent data for anSOA implementation.\n\u2022 Developed, Implemented & Maintained the Conceptual, Logical &Physical Data Models using Erwin for forwarding/Reverse Engineered Databases.\n\u2022 Explored and Extracted data from source XML in HDFS, preparing data for exploratory analysis using data mining.\n\nEnvironment: SQL Server 2008R2/2005 Enterprise, SSRS, SSIS, Crystal Reports, Hadoop, Windows Enterprise Server 2000, DTS, Bigdata, R , machine learning,SQL Profiler, and Query Analyzer."", u""Data Analyst\nWalgreens - Deerfield, IL\nDecember 2013 to September 2015\nDescription: The Walgreens Companyis an American company that operatesas the second-largest pharmacy store chain in the United States. It specializes in filling prescriptions, health and wellness products, health information, and photo services\nResponsibilities:\n\u2022 Coded R functions to interface with CaffeDeep Learning Framework\n\u2022 Working in Amazon Web Services cloud computing environment\n\u2022 Worked with several R packages including knitr, dplyr, SparkR, Causal Infer, space-time.\n\u2022 Implemented end-to-end systems for Data Analytics, Data Automation and integrated with custom visualization tools using R, Mahout, Hadoop, andMongoDB.\n\u2022 Gathering all the data that is required from multiple data sources and creating datasets that will be used in theanalysis.\n\u2022 Performed Exploratory Data Analysis and Data Visualizations using R, and Tableau.\n\u2022 Perform a proper EDA, Uni-variate and bi-variate analysis to understand the intrinsic effect/combined effects.\n\u2022 Worked with Data Governance, Data quality, data lineage, Data architect to design various models and processes.\n\u2022 Independently coded new programs and designed Tables to load and test the program effectively for the given POC's using with Big Data/Hadoop.\n\u2022 Designed data models and data flow diagrams using Erwin and MS Visio.\n\u2022 As an Architect implemented MDM hub to provide clean, consistent data for anSOA implementation.\n\u2022 Developed, Implemented &Maintained the Conceptual, Logical&Physical Data Models using Erwin for forwarding/Reverse Engineered Databases.\n\u2022 Established Data architecture strategy, best practices, standards, and roadmaps.\n\u2022 Lead the development and presentation of a dataanalytics data-hub prototype with the help of the other members of the emerging solutions team\n\u2022 Performed datacleaning and imputation of missing values using R.\n\u2022 Worked with Hadoop eco system covering HDFS, HBase, YARN, andMapReduce\n\u2022 Take up ad-hoc requests based on different departments and locations\n\u2022 Used Hive to store the data and perform datacleaning steps for huge datasets.\n\u2022 Created dash boards and visualization on regular basis using ggplot2 and Tableau.\n\u2022 Creating customized business reports and sharing insights to the management.\n\u2022 Worked with BTEQ to submit SQL statements, import and export data, and generate reports in Teradata.\n\u2022 Interacted with the other departments to understand and identify data needs and requirements and work with other members of the IT organization to deliver data visualization and reporting solutions to address those needs.\n\nEnvironment: Erwin r, Informatica, ODS, OLTP, Oracle 10g, Hive, OLAP, DB2, Metadata, MS Excel, Mainframes MS Visio, Rational Rose, and Requisite Pro, Hadoop, PL/SQL, etc."", u""Hadoop Developer\nTransamerica - Plano, TX\nSeptember 2012 to November 2013\nDescription: Transamerica is the US-based brand of Aegon, a Dutch financial services firm. Aegon is one of the world's leading providers of life insurance, pensions and asset management and is helping approximately 30 million customers globally to achieve a lifetime of financial security.\n\nResponsibilities:\n\u2022 Statistical Modeling with ML to bring Insights in Data under guidance of Principal Data Scientist\n\u2022 Data modeling with Pig, Hive, Impala.\n\u2022 Ingestion with Sqoop, Flume.\n\u2022 Used SVN to commit the Changes into the main EMM application trunk.\n\u2022 Understanding and implementation of text mining concepts, graph processing and semi-structured and unstructured data processing.\n\u2022 Worked with Ajax API calls to communicate with Hadoop through Impala Connection and SQL to render the required data through it. TheseAPI calls are similar to Microsoft Cognitive API calls.\n\u2022 Good grip on Cloudera and HDP ecosystem components.\n\u2022 Used Elasticsearch (Big Data) to retrieve data intothe application as required.\n\u2022 Performed Map Reduce Programs those are running on the cluster.\n\u2022 Developed multiple MapReduce jobs in java for data cleaning and preprocessing.\n\u2022 Analyzed the partitioned and bucketed data and compute various metrics for reporting.\n\u2022 Involved in loading data from RDBMS and weblogs into HDFS using Sqoop and Flume.\n\u2022 Worked on loading the data from MySQL to HBase where necessary using Sqoop.\n\u2022 Developed Hive queries for Analysis across different banners.\n\u2022 Extracted data from Twitter using Java and Twitter API. Parsed JSON formatted twitter data and uploaded tothe database.\n\u2022 Launching Amazon EC2 Cloud Instances using Amazon Images (Linux/ Ubuntu) and Configuring launched instances with respect to specific applications.\n\u2022 Exported the result set from Hive to MySQL using Sqoop after processing the data.\n\u2022 Analyzed the data by performing Hive queries and running Pig scripts to study customer behavior.\n\u2022 Have hands-on experience working withSequence files, AVRO, HAR file formats and compression.\n\u2022 Used Hive to partition and bucket data.\n\u2022 Experience in writing MapReduce programs with Java API to cleanse Structured and unstructured data.\n\u2022 Wrote Pig Scripts to perform ETL procedures on the data in HDFS.\n\u2022 Created HBase tables to store various data formats of data coming from different portfolios.\n\u2022 Worked on improvingthe performance of existing Pig and Hive Queries."", u""Java Developer\nGlobalLogic Technologies - Hyderabad, Telangana\nApril 2009 to November 2010\nDescription: GlobalLogic provides experience design, Digital Product Engineering Services, and Agile Software Development to the world's top brands by leveraging UX UI Design, next-gen technologies, and cloud software, with end-to-end solution by the best Software Development Company.\n\nResponsibilities:\n\u2022 Developed Internet traffic scoring platform for ad networks, advertisers, and publishers (rule engine, site scoring, keyword scoring, lift measurement, linkage analysis).\n\u2022 Responsible for defining the key identifiers for each mapping/interface.\n\u2022 Clients include eBay, Click Forensics, Cars.com, Turn.com, Microsoft, and Look smart.\n\u2022 Implementation of Metadata Repository, Maintaining Data Quality, Data Cleanup procedures, Transformations, Data Standards, Data Governance program, Scripts, Stored Procedures, triggers and execution of test plans.\n\u2022 Designed the architecture for one of the first analytics 3.0. Online platforms: all-purpose scoring, with on-demand, SaaS, API services. Currently under implementation.\n\u2022 Web crawling and text mining techniques to score referral domains, generate keyword taxonomies, and assess commercial value of bid keywords.\n\u2022 Developed new hybrid statistical and data mining technique known as hidden decision trees and hidden forests.\n\u2022 Reverse engineering of keyword pricing algorithms in the context of pay-per-click arbitrage.\n\u2022 Coordinated meetings with vendors to define requirements and system interaction agreement documentation between client and vendor system.\n\u2022 Automated bidding for advertiser campaigns based either on keyword or category (run-of-site) bidding.\n\u2022 Creation of multimillion bid keyword lists using extensive web crawling. Identification of metrics to measure the quality of each list (yield or coverage, volume, and keyword average financial value).\n\u2022 Enterprise Metadata Library with any changes or updates.\n\u2022 Document data quality and traceability documents for each source interface.\n\u2022 Establish standards of procedures.\n\u2022 Generate weekly and monthly asset inventory reports.\n\nEnvironment: Erwin r, SQL Server 2000/2005, Windows XP/NT/2000, Oracle, MS-DTS, UML, UAT, SQL Loader, OOD, OLTP, PL/SQL, MS Visio, Informatica.""]",[u'Bachelor of Engineering in Electronics and Communication Engineering in Electronics and Communication Engineering'],"[u""St. Mary's College of Engineering and Technology.\nJanuary 2009""]",degree_1 : Bachelor of Engineering in Electronics and Commnication Engineering in Electronics and Commnication Engineering
0,https://resumes.indeed.com/resume/bcbb2e8d820d3fe0,"[u'Data Scientist\nLARSEN AND TOUBRO INFOTECH - Houston, TX\nNovember 2016 to Present\nNatural Language Processing and Time Series Analysis for Investment Portfolio Management: Developed a model to analyze the news for prediction of stock trends on a 6-month basis to re-balance stock portfolios. Explored various algorithmic trading theories and ideas. This product is being marketed to portfolio management companies as part of Prime Technology Group services. Architected new office 365 plan for system consisting of over 20, 000 users.\n\nExpertise in applying data mining techniques and optimization techniques in B2B and B2C industries and proficient in Machine Learning, Data/Text Mining, Statistical Analysis and Predictive Modeling.\nUtilized MapReduce/Spark Python modules for machine learning & predictive analytics in Hadoop on AWS.\nPredictive modeling using state-of-the-art methods\nUtilizing Informatica toolset (Informatica Data Explorer, and Informatica Data Quality) to analyze legacy data for Data Profiling.\nInvolved in upgrading DTS packages to SSIS packages (ETL)\nBroad knowledge of programming, and scripting (especially in R / Java / Python)\nProven experience building sustainable and trustful relationships with senior leaders.\nMigrating Informatica mappings from SQL Server to Netezza Foster culture of continuous engineering improvement through mentoring, feedback, and metrics\nDeveloping and maintaining Data Dictionary to create metadata reports for technical and business purpose.\nBuild and maintain dashboard and reporting based on the statistical models to identify and track key metrics and risk indicators.\nExtracting the source data from Oracle tables, MS SQL Server, sequential files and excel sheets.\nParse and manipulate raw, complex data streams to prepare for loading into an analytical tool.\nInvolved in defining the source to target data mappings, business rules, and data definitions.\nPerforming an end to end Informatica ETL Testing for these custom tables by writing complex SQL Queries on the source database and comparing the results against the target database.', u'Data Scientist\nPOLARIS INDUSTRIES, INC - Medina, MN\nJune 2015 to October 2016\nPolaris Industries designs, manufactures and markets innovative, high-quality, high-performance motorized products for recreation and utility use to the international market through global distribution channels.\nDeveloped analytical approaches to strategic business decisions\nMigrating Informatica mappings from SQL Server to Netezza Foster culture of continuous engineering improvement through mentoring, feedback, and metrics\nExpertise in applying data mining techniques and optimization techniques in B2B and B2C industries and proficient in Machine Learning, Data/Text Mining, Statistical Analysis and Predictive Modeling.\nImplemented Event Task for executing an Application Automatically.\nInvolved in defining the source to target data mappings, business rules, and data definitions.\nAssist in continual improvement of AWS data lake environment\nBuild and maintain dashboard and reporting based on the statistical models to identify and track key metrics and risk indicators.\nUsing HP Quality Center for defect tracking of issues.\nProven experience building sustainable and trustful relationships with senior leaders.\nDefining the list codes and code conversions between the source systems and the data mart using Reference Data Management (RDM).\nExtracting the source data from Oracle tables, MS SQL Server, sequential files and excel sheets.\nPerform analysis using predictive modeling, data/text mining, and statistical tools\nDeveloped MapReduce/Spark Python modules for machine learning & predictive analytics in Hadoop on AWS.\nCollaborate cross-functionally to arrive at actionable insights\nDeveloping and maintaining Data Dictionary to create metadata reports for technical and business purpose.\nPredictive modeling using state-of-the-art methods\nParse and manipulate raw, complex data streams to prepare for loading into an analytical tool.\nSynthesize analytic results with business input to drive measurable change\nUtilizing Informatica toolset (Informatica Data Explorer, and Informatica Data Quality) to analyze legacy data for Data Profiling.\nInvolved in upgrading DTS packages to SSIS packages (ETL).\nAdvanced analytics skills, and proficient at integrating and preparing large, varied datasets, architecting specialized database and computing environments, and communicating results.\nPerforming an end to end Informatica ETL Testing for these custom tables by writing complex SQL Queries on the source database and comparing the results against the target database.\nInvolved in developing Patches & Updates Module.\nBroad knowledge of programming, and scripting (especially in R / Java / Python)\nPerforming data profiling on various source systems that are required for transferring data to ECH using', u'Data Scientist\nCERIDIAN HCM, Inc - Minneapolis, MN\nJanuary 2014 to May 2015\nResponsible for management of analytics projects aimed at solving business problems. My role involves identifying/understanding business requirements, scope definition, developing machine learning models, presenting results to business and following up through implementation.\n\nUse of advanced analytics, data mining and statistical techniques on a variety of industries and using a diverse set of tools to bring insights out of complex data. Assess information from a range of data stored in disparate systems, integrating data and providing data mining to answer specific business questions as well as identifying unknown trends and relationships in data.\nWorked with internal architects and, assisting in the development of current and target state data architectures.\nPerformed data quality in Talend Open Studio.\nInvolved in defining the source to target data mappings, business rules, data definitions.\nRemain knowledgeable in all areas of business operations in order to identify systems needs and requirements.\nGenerate weekly and monthly asset inventory reports.\nInvolved in defining the business/transformation rules applied for sales and service data.\nResponsible for defining the key identifiers for each mapping/interface.\nResponsible for defining the functional requirement documents for each source to target interface.\nResponsible for defining the key identifiers for each mapping/interface.\nDocument the complete process flow to describe program development, logic, testing, and implementation, application integration, coding.\nDefine the list codes and code conversions between the source systems and the data mart.\nImplementation of Metadata Repository, Maintaining Data Quality, Data Cleanup procedures, Transformations, Data Standards, Data Governance program, Scripts, Stored Procedures, triggers and execution of test plans\nEnterprise Metadata Library with any changes or updates.\nCoordinate with the business users in providing appropriate, effective and efficient way to design the new reporting needs based on the user with the existing functionality.\nCoordinated meetings with vendors to define requirements and system interaction agreement documentation between client and vendor system.\nWorked with BTEQ to submit SQL statements, import and export data, and generate reports in Teradata.', u'Data Scientist\nGENERAL MOTORS - Detroit, MI\nJanuary 2013 to December 2013\nAnalysis focuses on quality control and automated testing, process control and Industrial automation, data acquisition and instrumentation including vision systems and motion control. \xb7 Data Science & Big Data \xb7 Simulation & Modeling \xb7\nData analysis using regressions, data cleaning, excel v-look up, histograms and TOAD client and data representation of the analysis and suggested solutions for investors\nImplemented Agile Methodology for building an internal application.\nHadoop Data Lake Implementation and HADOOP Architecture for client business data management.\nParticipated in all phases of data mining; data collection, data cleaning, developing models, validation, visualization and performed Gap analysis.\nUsed pandas, NumPy, seaborn, SciPy, matplotlib, Scikit-Learn, NLTK in Python for utilizing various machine learning algorithms.\nWorked on different data formats such as JSON, XML and performed machine learning algorithms in Python.\nData Manipulation and Aggregation from different source using Nexus, Toad, Business Objects, Power BI and Smart View.\nExtracted data from HDFS and prepared data for exploratory analysis using data munging.\nWorked as Data Architects and IT Architects to understand the movement of data and its storage and ER Studio\nFocus on integration overlap and Informatica newer commitment to MDM with the acquisition of Identity Systems.\nSetup storage and data analysis tools in Amazon Web Services cloud computing infrastructure.\nRapid model creation in Python using Pandas, NumPy, SKLearn, and plot.ly for data visualization. These models are then implemented in SAS where they are interfaced with MSSQL databases and scheduled to update on a timely basis.\nDevelopment and Deployment using Google Dialogflow Enterprise.', u'Data Scientist\nJC PENNEY - Plano, TX\nAugust 2011 to December 2012\nThis large volume retailer uses data science to manage inventory, increase and maintain market share and ensure a continuous understanding of market drivers. The project involved designing, developing and implementing data analysis using cloud architecture and applying data science techniques for data exploration.\nData ingestion is done using Flume with source as Kafka Source & sink as HDFS.\nImplementing YARN Resource pools to share resources of cluster for YARN jobs submitted by users.\nResponsible for creating Hive tables, loading the structured data resulted from MapReduce jobs into the tables and writing Hive queries to further analyze the logs to identify issues and behavioral patterns.\nAutomated Sqoop Jobs in a timely manner for Data Migration from Existing RDBMS to HDFS using Shell Scripting.\nPerformed import and export of large data set transfer between traditional databases and HDFS using Sqoop.\nResponsible for handling Hive queries using Spark SQL that integrates with Spark environment.\nKeep current with latest technologies to help automate tasks and implement tools and processes to manage the environment.\nWorked with cloud services like Amazon Web Services (AWS) and involved in ETL, Data Integration and Migration.\nPerformed transformations using Spark and then loaded data into HBase tables.\nCreated Hive external tables and designed data models in hive.\nProactively monitored systems and services, manage backup and disaster recovery systems and procedure.\nImplemented Hive Generic UDFs to handle business logic.', u""Data Analyst\nMACY'S TECHNOLOGY - Atlanta, GA\nMay 2010 to July 2011\nResponsible for extracting, sorting, and cleansing data; analyzing raw data and providing analysis and reports which can be used practically in business strategy.\nResponsible for loading, extracting and validation of client data.\nAnalyzing raw data, concluding, developing recommendations and manipulate data for data loads and extracts.\nData output - Made data chart presentations and coded variables from original data, conducted statistical analysis as and when required and provided summaries of analysis.\nExperience in Creating visually impactful dashboards in Excel and Tableau for data reporting by using pivot tables and VLOOKUP.\nCarried out data manipulation, data cleaning, dealt with missing records, purged and consolidated data for analysis and selected the appropriate visualization techniques; developing reports and communicating insights to stakeholders.\nDeveloped Enterprise applications in the Client/Server, Web application using PHP, Word Press, JavaScript, Jquery, AJAX, HTML5, CSS, MySQL & SQL Server database.\nDesigning and developing dynamic web pages using PHP and WordPress. Effectively used Tableau, Excel, jQuery, HTML, CSS and AJAX interactions.\nDesigning the database, creating Tables, Stored Procedures, Views, Function, and Triggers & Automated test cases to safeguard against regression defects and an efficient way of unit testing to exercise 70% of Test coverage.\nRegular follow-up meetings with Project Manager to update them about project progress & demo after each sprint.\nDocumentation, Release Note Preparation, and Deployment""]","[u""Bachelor's in Science in Physics""]","[u'University of Texas at Dallas Dallas, TX\nMay 2015']","degree_1 : ""Bachelors in Science in Physics"""
0,https://resumes.indeed.com/resume/47ad4beabc4bb757,"[u'Head Data Scientist/ Statistician\nBiostatisticsbytheBay - Half Moon Bay, CA\nJanuary 2008 to September 2017\nData Analytics, Business Intelligence, Algorithm Development', u'Statistician\nIntuitive Surgical - Sunnyvale, CA\nJanuary 2005 to December 2007', u'Statistician\nAbbott Laboratories\nJanuary 2004 to December 2005', u'Statistician\nGuidant/Boston Scientific - Minneapolis-Saint Paul, MN\nMay 2001 to December 2003']","[u'PhD in Statistics', u'BS in Neuroscience', u'Certificate in Quantitative Finance in Quantitative Finance', u'Core Finance in Finance']","[u'University of Minnesota-Twin Cities Minneapolis, MN\nJune 1999 to May 2001', u'University of Minnesota-Twin Cities\nSeptember 1995 to May 1999', u'Fitch (Wilmott Group) New York, NY\nDecember 2017', u'Harvard Business School Harvard, MA\nSeptember 2017']","degree_1 : PhD in Statistics, degree_2 :  BS in Neroscience, degree_3 :  Certificate in Qantitative Finance in Qantitative Finance, degree_4 :  Core Finance in Finance"
0,https://resumes.indeed.com/resume/3870a2f9298d5682,"[u'Global Safety Manager\nSelect Genetics, LLC - Aurora, MO\nMarch 2015 to Present\n\u2022 Team leader of company safety and health policies and procedures.\n\u2022 Supervision of 4 safety coordinators\n\u2022 Drafting and revising safety department S.O.Ps\n\u2022 Developing, distributing, and tracking regulatory and supplemental safety training\n\u2022 Budgeting and purchasing supplies\n\u2022 Safety Team Chairman\n\u2022 Conducting risk analysis and leading accident prevention and investigation', u'Poultry Holding QC & Data Scientist\nSelect Genetics, LLC - Aurora, MO\nApril 2014 to March 2015\n\u2022 In charge of operating our poultry holding room.\n\u2022 Ensuring that all the proper environmental controls were up to standard and our product was healthy and comfortable.\n\u2022 Gathered information and data on room variables for control optimizations and trend analysis.\n\n\u3161']",[u'Associate of Science in Engineering'],"[u'Ozarks Technical Community College / Associate of Science Springfield, MO\nJune 2016 to Present']",degree_1 : Associate of Science in Engineering
0,https://resumes.indeed.com/resume/6bb9780103457c03,"[u'Data Scientist / Owner\nAlpha Analysis Consulting LLC - San Jose, CA\nJanuary 2006 to Present\nDesigned factorial type experimental design to maximize mobile app downloads for Android mobile\nphone users for e-commerce company.\n\u2022 Developed predictive model to target users most likely to purchase specific goods for e-commerce\ncompany\'s ""Back to School Campaign"" using display advertising and promotional email campaign.\n\u2022 Designed and built recommendation engine for specific product categories for e-commerce company.\n\u2022 Tested and improved functionality of cloud based machine learning platform for software company.\n\u2022 Developed optimal playing strategy for three card poker casino management group through simulation\ntechniques. All possible scenarios were simulated allowing group to look at extreme win and loss\nprobabilities to maximize gaming revenues.\n\u2022 Developed predictive model to target demographics most likely to purchase health product using\nsurvey data.\n\u2022 Developed predictive model using real estate data to determine factors affecting rents and property\nvalues to maximize rental incomes and sales for corporate real estate company.\n\u2022 Developed, backtested, and implemented statistical model to predict equity prices in the short term\nfollowing an earnings release for 3500 equity universe for investment software company.\n\u2022 Built asset optimizer and risk management tools using R for Boston based investment management\nfirm. Created optimal international, domestic, and global equity and fixed income portfolios with graphics based upon client guidelines on a quarterly basis.\n\u2022 Developed standardized testing templates for technology company\'s demand planning team to test\ncomplex functionality designed into planning software that resulted in increased accuracy.\n\u2022 Advised multiple PhD candidates with statistical component of dissertation research.', u'Platform Data Scientist\nAlpha Analysis Consulting LLC - San Francisco, CA\nSeptember 2012 to July 2013\nDeveloped methodology for customer lifetime value for Internet search product with 140 million+ user\nbase for both Google and Yahoo users globally.\n\u2022 Managed process to move legacy Internet search data from SQL to Hadoop and have current search\ndata fed into Hadoop for Big Data analytics and predictive modeling.\n\u2022 Developed profitability models for search platform business that are used in contract negotiation and determine the optimal price to pay for an install or activation for Internet search product.\n\u2022 Developed predictive models to predict amount of searches per day per geographical region, search\nprovider, and other demographics.\n\u2022 Developed predictive model that determines optimal search provider to send Internet traffic to based\nupon multiple constraints and guidelines.', u'Consultant\nNSTAR Electric and Gas - Westwood, MA\nJune 2006 to August 2006\nDesigned and implemented process for predicting short and long term natural gas demand using\nregression and time series econometric models which allowed company to strategically plan energy\nreserves.', u'Summer Quantitative Analyst\nGeode Capital Management, LLC - Boston, MA\nJune 2005 to August 2005\nDeveloped and implemented process for statistically testing various internal and external optimization\nsoftware packages seeking maximum utility for global market neutral funds using time series of inputs and multiple constraints.', u'Research Officer\nJohn Hancock Advisers, LLC - Boston, MA\nJanuary 2000 to January 2003\nInvestment Analyst\n\n\u2022 Analyzed and actively monitored equity portfolios using risk decompositions, factor exposures, and marginal contribution to active risk scores and communicated active and total risks to investment\nteams.\n\u2022 Researched and presented equity attribution, risk and market data at weekly investment meeting.\n\u2022 Advised quantitative team with model creation, backtesting of portfolios, and data maintenance.\n\u2022 Optimized equity portfolios using time series of inputs and multiple constraints.\n\u2022 Constructed and maintained custom benchmarks which were actively managed for institutional clients.\n\u2022 Created attribution analysis for portfolio managers for global large, mid and small cap portfolios.', u'Performance Analyst\nPioneer Investments - Boston, MA\nJanuary 2000 to January 2000\n\u2022 Created attribution analysis for portfolio managers across domestic and international portfolios.\n\u2022 Constructed and maintained custom benchmarks and peer groups for investment management.\n\u2022 Created ad-hoc requests for investment management.', u'Senior Performance Analyst\nState Street Corporation - Quincy, MA\nJanuary 1997 to January 2000\nPerformance Analyst\nSenior Portfolio Accountant / Auditor\nPortfolio Accountant\n\n\u2022 Created and interpreted domestic and international attribution analysis, universe comparison, and investment performance services to clients and senior consultants.\n\u2022 Developed and implemented risk analysis for retail mutual fund client.\n\u2022 Responsible for training of new analysts.\n\u2022 Member of Euro and Year 2000 Conversion Teams responsible for systems testing within division.\n\u2022 Liaison chosen by senior management to help implement new operating model within division.\n\u2022 Production of extensive client specific reporting including client annual report and financial statements.']",[u'Bachelors of Science in Marketing'],[u'Providence College\nMay 1996'],degree_1 : Bachelors of Science in Marketing
0,https://resumes.indeed.com/resume/f30f969162fc9656,"[u""Data Scientist\nMorgan Stanley, TimesSquare, NY\nFebruary 2017 to Present\nDescription: Morgan Stanleyis a financial holding company. The Company is engaged in global financial services. The Company, through its subsidiaries and affiliates, advises, and originates, trades, manages and distributes capital for governments, institutions and individuals. The Company's segments include Institutional Securities, Wealth Management and Investment Management. Through its subsidiaries and affiliates, the Company provides a range of products and services to a group of clients and customers, including corporations, governments, financial institutions and individuals.\n\nResponsibilities:\n\u2022 Natural Language Processing&Recurrent Neural Networks (LSTM RNNs) learnt using Deep Learning techniques applied to a fraud detection system.\n\u2022 Clean data was migrated from warehouses and streamed into Kafka feeding the Spark engine.\n\u2022 Performed transformations on loaded datasets using Pythonover the spark engine using both batch and streaming data.\n\u2022 Assimilated real-time credit data from monitoring agencies and deployed containers(Dockers) thereby allowing for the generated insights/reports to be deployed into the In-House Database management tool.\n\u2022 Improving the existential Fraud Detection using Digital Links from TransUnion and Experian.\n\u2022 Scaled for Machine Learning pipelines: 4600 processors, 35000 GB memory achieving simultaneous runs.\n\u2022 Deployed GUI pages by using JSP, JSTL, HTML, DHTML, XHTML, CSS, JavaScript, AJAX.\n\u2022 Configured the project on Web Sphere 6.1 application servers\n\n\u2022 Designed a new Machine Learning pipeline to replace existing process. increasing efficiency from 81% to 90%.\n\u2022 Handled 2+ TB data with graphs upto130 GB (50M nodes, 100M edges) using single-node in-disk scaling.\n\u2022 Developed a Machine Learning test-bed with 24 different model learning and feature learning algorithms.\n\u2022 By thorough systematic search, demonstrated performance surpassing the state-of-the-art (deep learning).\n\n\u2022 Upto 10 times more accurate predictions over existing state-of-the-art algorithms.\n\u2022 Developed in-disk, huge (100GB+), highly complex Machine Learning models.\n\u2022 Used SAX and DOM parsers to parse the raw XML documents\n\u2022 Used RAD as Development IDE for web applications.\n\n\u2022 Demonstrated performances comparable to other state-of-the-art deep learning models.\n\u2022 Devised and implemented a Credit monitoring application using Databricks.\n\u2022 Deployed the Application using Spark using data from the predictive model created earlier.\n\u2022 Tested the insights using Na\xefve Bayes Algorithm and simulated a model to predict credit frauds.\n\u2022 Warehoused data into Amazon Redshift Servers and the model is now being tested out by TRANSUNION.\n\u2022 Devised a novel machine learning algorithm for classification of Credit Score prediction models.\n\u2022 Created visualization dashboards and API's were hosted using Tableau.\n\nEnvironment: Hadoop, Spark, OLAP, DB2, Metadata, Scala, Python, Amazon S3, Kafka, CoreML, Automated Logistic regression models, Informatica 9.0, MongoDB"", u""Data Scientist\nState Street Bank - Boston, MA\nDecember 2015 to February 2017\nDescription:State Street Corporation, is an American worldwide financial services company. State Street was founded in 1792 and is the second oldest financial institution in the United States of America. It is one of the largest asset management companies in the world with $2.45 trillion (USD) under management and $28 trillion (USD) under custody and administration, which represents 11% of the world's total financial assets. State Street is a Fortune 500 company with headquarters at One Lincoln Street in Boston and has offices in 30 countries around the world.\n\nResponsibilities:\n\u2022 Manipulating, cleansing & processing data using Excel, Access and SQL.\n\u2022 Responsible for loading, extracting and validation of client data.\n\u2022 Modelled clean data into the Kafka servers for use over the spark engine.\n\u2022 Zookeeper along with Kafka was used to stream data and end-to-end client communication.\n\u2022 Performed transformations over the warehoused data using Scala& Python and modelled the data back into the servers for iterative transformations into KAFKA.\n\u2022 Modelled data using Machine learning libraries(Sci-kit learn) apart from SVN and KNN based classificationto create a training dataset for use in a predictive model.\n\u2022 Assimilated data from Blogs, Feedback systems for NLP based processing models.\n\u2022 Liaising with end-users and 3rd party suppliers.\n\u2022 Analyzing raw data, drawing conclusions & developing recommendations\n\u2022 Writing T-SQL scripts to manipulate data for data loads and extracts.\n\u2022 Developing data analytical databases from complex financial source data.\n\u2022 Performing daily system checks.\n\u2022 Data entry, data auditing, creating data reports & monitoring all data for accuracy.\n\u2022 Designing, developing and implementing new functionality.\n\u2022 Monitoring the automated loading processes.\n\u2022 Advising on the suitability of methodologies and suggesting improvements.\n\u2022 Carrying out specified data processing and statistical techniques.\n\u2022 Supplying qualitative and quantitative data to colleagues & clients.\n\u2022 Using Informaticato extract, transform & load source data from transaction\nsystems.\n\u2022 Loaded packages and stored procedures using Base SAS and integrated functional and business requirements using the EBI suite.\n\u2022 Creating data pipelines using big data technologies like Hadoop, spark etc.\n\u2022 Creating statistical models using distributed and standalone models to build various diagnostics, predictive and prescriptive solution.\n\u2022 Utilize a broad variety of statistical packages like R , MLIB,CoreML,Graphs,Hadoop, Spark , MapReduce, Pig\n\u2022 Created a UI dashboard for end users and performed prototype testing using Tableau.\n\u2022 Refine and train models based on domain knowledge and customer business objectives\n\u2022 Deliver or collaborate on delivering effective visualizations to support the client business objectives\n\u2022 Communicate to your peers and managers promptly as and when required.\n\u2022 Produce solid and effective strategies based on accurate and meaningful data reports and analysis and/or keen observations.\n\u2022 Establish and maintain communication with clients and/or team members; understand needs, resolve issues, and meet expectations\n\u2022 Developed web applications using .net technologies; work on bug fixes/issues that arise in the production environment and resolve them at the earliest\nEnvironment: SQL Server 2008R2/2005 Enterprise, Kafka, Scala, Python, Spark, Hadoop, Crystal Reports,Windows Enterprise Server 2000, MongoDB, SQL Profiler, and Query Analyzer, TensorFlow."", u""Data Scientist\neBayInc - San Jose, CA\nApril 2014 to November 2015\nDescription:eBayis a multinational e-commerce corporation, facilitating online consumer-to consumer and business-to-consumer sales. It is headquartered in San Jose, California. eBay was founded by Pierre Omidyar in 1995, and became a notable success story of the dot-com bubble. Today it is a multibillion-dollar business with operations in about 30 countries.\nResponsibilities:\n\u2022 Data mining using state-of-the-art methods\n\u2022 Extending company's data with third party sources of information when needed\n\u2022 Enhancing data collection procedures to include information that is relevant for building analytic systems\n\u2022 Processing, cleansing, and verifying the integrity of data used for analysis\n\u2022 Doing ad-hoc analysis and presenting results in a clear manner\n\u2022 Creating automated anomaly detection systems and constant tracking of its performance\n\u2022 Strong command of data architecture and data modelling techniques.\n\u2022 Hands on experience with commercial data mining tools such as Splunk, R, Map reduced, Yarn, Pig,Hive, Floop, Oozie, Scala, HBase, Master HDFS, Sqoop, Spark, Scala (Machine learning tool).\n\u2022 Developed scalable machine learning solutions within a distributed computation framework (e.g. Hadoop, Spark, Storm etc.).\n\u2022 Utilizing NLP applications such as topic models and sentiment analysis to identify trends and patterns within massive data sets.\n\u2022 Knowledge in ML Libraries such as Tensorflow & Statistical libraries (e.g. Scikit-learn, Pandas).\n\u2022 Having knowledge to build predict models to forecast risks for product launches and operations and help predict workflow and capacity requirements for TRMS operations\n\u2022 Having experience with visualization technologies such as Tableau.\n\u2022 Draw inferences and conclusions, and create dashboards and visualizations of processed data, identify trends, anomalies\n\u2022 Generation of TLFs and summary reports, etc. ensuring on-time quality delivery.\n\u2022 Participated in client meetings, teleconferences and video conferences to keep track of project requirements, commitments made and the delivery thereof.\n\u2022 Solved analytical problems, and effectively communicate methodologies and results\n\u2022 Worked closely with internal stakeholders such as business teams, product managers, engineering teams, and partner teams.\n\u2022 Created automated metrics using complex databases.\n\u2022 Foster culture of continuous engineering improvement through mentoring, feedback, and metrics.\nEnvironment: Erwin r9.0, Informatica 9.0, ODS, OLTP, Oracle 10g, Hive, OLAP, DB2, Terradata, MS Excel, Hadoop with Python, PL/SQL, Spark with Python,TensorFlow, Deep learning Libraries, Tableau."", u""Data Scientist\nEagle Trading Systems - Princeton, NJ\nMay 2013 to March 2014\nDescription:Eagle Trading Systems Inc. is a financial investment advisory firm headquartered in Princeton, New Jersey. The firm manages 5 accounts totaling an estimated $481 Million of assets under management. Eagle Trading Systems Inc.'s 17 employees help advice 1-10 clients.\n\nResponsibilities:\n\u2022 Statistical Modelling with ML to bring Insights in Data under guidance of Principal Data Scientist\n\u2022 Data modeling with Pig, Hive, Impala.\n\u2022 Ingestion with Sqoop, Flume.\n\u2022 Used SVN to commit the Changes into the main EMM application trunk.\n\u2022 Understanding and implementation of text mining concepts, graph processing and semi structured and unstructured data processing.\n\u2022 Worked with Ajax API calls to communicate with Hadoop through Impala Connection and SQL to render the required data through it. TheseAPI calls are similar to Microsoft Cognitive API calls.\n\u2022 Good grip on Cloudera and HDP ecosystem components.\n\u2022 Used Elasticsearch (Big Data) to retrieve data into application as required.\n\u2022 Performed Map Reduce Programs on nodes running on the cluster.\n\u2022 Developed multiple MapReduce jobs in Scala for data cleaning and preprocessing.\n\u2022 Analyzed the partitioned and bucketed data and compute various metrics for reporting.\n\u2022 Involved in loading data from RDBMS and web logs into HDFS using Sqoopand Flume.\n\u2022 Worked on loading the data from MySQL to HBase where necessary using Sqoop.\n\u2022 Developed Hive queries for Analysis across different banners.\n\u2022 Extracted data from Twitter using Java and Twitter API. Parsed JSON formatted twitter data and uploaded to database.\n\u2022 Launching Amazon EC2 Cloud Instances using Amazon Images (Linux/ Ubuntu) and Configuring launched instances with respect to specific applications.\n\u2022 Exported the result set from Hive to MySQL using Sqoop after processing the data.\n\u2022 Analyzed the data by performing Hive queries and running Pig scripts to study customer behavior.\n\u2022 Have hands on experience working on Sequence files, AVRO, HAR file formats and compression.\n\u2022 Used Hive to partition and bucket data.\n\u2022 Experience in writing MapReduce programs with Java API to cleanse Structured and unstructured data.\n\u2022 Wrote Pig Scripts to perform ETL procedures on the data in HDFS.\n\u2022 Created HBase tables to store various data formats of data coming from different portfolios.\n\u2022 Worked on improving performance of existing Pig and Hive Queries.\n\nEnvironment: SQL/Server, Oracle 9i, MS-Office, Teradata, Informatica, ER Studio, XML, Business Objects, HDFS, Teradata 14.1, JSON, HADOOP (HDFS), MapReduce, PIG, Spark, R Studio, MAHOUT, JAVA, HIVE, AWS."", u""Data Architect/Data Modeler\nFirst Indian Corporation Private Limited - Hyderabad, Telangana\nFebruary 2011 to April 2013\nDescription:First Indian Corporation Private Limited, a member of The First American family of companies, provides specialized offshore transaction services, technology services and analytics to the mortgage industry. First Indian Corporation's primary focus is on the Title Insurance, Property Tax, Flood Certification, Default Management Services, Credit and Real Estate Information segments.\n\nResponsibilities:\n\u2022 Worked with large amounts of structured and unstructured data.\n\u2022 Knowledge in Machine Learning concepts (Generalized Linear models, Regularization, Random Forest, Time Series models, etc.)\n\u2022 Worked in Business Intelligence tools and visualization tools such as Business Objects, Tableau, ChartIO, etc.\n\u2022 Deployed GUI pages by using JSP, JSTL, HTML, DHTML, XHTML, CSS, JavaScript, AJAX.\n\u2022 Configured the project on WebSphere 6.1 application servers\n\u2022 Implemented the online application by using Core Java, Jdbc, JSP, Servlets and EJB 1.1, Web Services, SOAP, WSDL.\n\u2022 Handled end-to-end project from data discovery to model deployment.\n\u2022 Monitoring the automated loading processes.\n\u2022 Communicated with other Health Care info by using Web Services with the help of SOAP, WSDLJAX-RPC\n\u2022 Used Singleton, factory design pattern, DAO Design Patterns based on the application requirements\n\u2022 Used SAX and DOM parsers to parse the raw XML documents\n\u2022 Used RAD as Development IDE for web applications.\n\u2022 Preparing and executing Unit test cases\n\u2022 Used Log4J logging framework to write Log messages with various levels.\n\u2022 Involved in fixing bugs and minor enhancements for the front-end modules.\n\u2022 Implemented Microsoft Visio and Rational Rose for designing the Use Case Diagrams, Class model, Sequence diagrams, and Activity diagrams for SDLC process of the application\n\u2022 Doing functional and technical reviews\n\u2022 Maintenance in the testing team for System testing/Integration/UAT.\n\u2022 Guaranteeing quality in the deliverables.\n\u2022 Conducted Design reviews and Technical reviews with other project stakeholders.\n\u2022 Was a part of the complete life cycle of the project from the requirements to the production support\n\u2022 Created test plan documents for all back-end database modules\n\u2022 Implemented the project in Linux environment.\n\nEnvironment: R 3.0, Erwin 9.5, Tableau 8.0, MDM, QlikView, MLLib, PL/SQL, HDFS, Teradata 14.1, JSON, HADOOP (HDFS), MapReduce, PIG, Spark, R Studio, MAHOUT, JAVA, HIVE, AWS."", u""Data Analyst/Data Modeler\nDELTA Technologies & Managements Services - Hyderabad, Telangana\nAugust 2009 to January 2011\nDescription: Delta Technology's vision is to be an organization of value, respect and transparency for its people to continuously innovate, improve and deliver efficient and effective business solutions.\n\nResponsibilities:\n\u2022 Developed Internet traffic scoring platform for ad networks, advertisers and publishers (rule engine, site scoring, keyword scoring, lift measurement, linkage analysis).\n\u2022 Responsible for defining the key identifiers for each mapping/interface.\n\u2022 Clients include eBay, Click Forensics, Cars.com, Turn.com, Microsoft, and Looksmart.\n\u2022 Designed the architecture for one of the first analytics 3.0. online platforms: all-purpose scoring, with on-demand, SaaS, API services. Currently under implementation.\n\u2022 Web crawling and text mining techniques to score referral domains, generate keyword taxonomies, and assess commercial value of bid keywords.\n\u2022 Developed new hybrid statistical and data mining technique known as hidden decision trees and hidden forests.\n\u2022 Reverse engineering of keyword pricing algorithms in the context of pay-per-click arbitrage.\n\u2022 Implementation of Metadata Repository, Maintaining Data Quality, Data Cleanup procedures, Transformations, Data Standards, Data Governance program, Scripts, Stored Procedures, triggers and execution of test plans\n\u2022 Performed data quality in Talend Open Studio.\n\u2022 Coordinated meetings with vendors to define requirements and system interaction agreement documentation between client and vendor system.\n\u2022 Automated bidding for advertiser campaigns based either on keyword or category (run-of-site) bidding.\n\u2022 Creation of multimillion bid keyword lists using extensive web crawling. Identification of metrics to measure the quality of each list (yield or coverage, volume, and keyword average financial value).\n\u2022 Enterprise Metadata Library with any changes or updates.\n\u2022 Document data quality and traceability documents for each source interface.\n\u2022 Establish standards of procedures.\n\u2022 Generate weekly and monthly asset inventory reports.\n\nEnvironment: Erwin r7.0, SQL Server 2000/2005, Windows XP/NT/2000, Oracle 8i/9i, MS-DTS, UML, UAT, SQL Loader, OOD, OLTP, PL/SQL, MS Visio, Informatica.""]","[u""Bachelor's""]",[u''],"degree_1 : ""Bachelors"""
0,https://resumes.indeed.com/resume/a5a6ec59249d029a,"[u'Research Scientist Intern\nMaana Inc - Bellevue, WA\nJune 2017 to September 2017\nVagrant, Spark, Deep Learning, Deep Learning 4 Java (DL4J), Natural Language Processing (NLP)\n\u2022 Fostered the capability of retrieving information from semi-structured textual data to extract proper nouns and named entities for several clients in Fortune 500 and pioneers of Oil and Gas industries\n\u2022 Leveraged distributed representation of words - Word2Vec and GloVe to train Long-Short Term Memory (LSTM)\nNeural Network using DL4J library to build Named-Entity Recognition solution', u'Data Analyst\nVeermata Jijabai Technical Institute - Mumbai, Maharashtra\nJanuary 2016 to June 2016\nFull Stack Development, Python, Numpy, D3.js\n\u2022 Investigated and provided successful validation of PageRank theory by simulating experiments on Full-Stack Web\nApplication in Cybersecurity network lab to identify most vulnerable nodes in the network\n\u2022 Formulated, designed and developed a comprehensive dashboard solution to increase transparency for consumers\nabout their electricity consumption aimed to reduce the 25% complaints that follow post bill cycle', u'ETL & Data Reporting Developer\nAccenture Services Pvt. Ltd - Mumbai, Maharashtra\nAugust 2014 to April 2016\nInformatica, PL/SQL, Full-Stack, Java, Tableau, D3.js, SAP BO XI\n\u2022 Increased efficiency, robustness of ETL processes and Business Recovery up to 45% by optimizing queries and automating business recovery steps, thereby saving 5 man hours per day across 3 teams of 2-4 members each\n\u2022 Designed and developed various aggregated dashboards across multiple teams to monitor Business KPIs, identify\nfailures across 7 teams in the project, increasing productivity by saving 2 man hours for each team']","[u'Master of Science in Data Science and Analytics', u'Bachelor of Engineering in Information Technology in Computational Mathematics', u'in Technology and Social Change Group']","[u'University of Washington Seattle, WA\nJune 2018', u'University of Mumbai Mumbai, Maharashtra\nMay 2014', u'Information School Research']","degree_1 : Master of Science in Data Science and Analytics, degree_2 :  Bachelor of Engineering in Information Technology in Comptational Mathematics, degree_3 :  in Technology and Social Change Grop"
0,https://resumes.indeed.com/resume/06b35dc01a8e5c77,"[u'Machine Learning Research Intern\nAutodesk Inc - New York, NY\nNovember 2017 to Present\n- New York City, US\n\u2022 Developed Multi-view convolutional neural network for 3D shape recognition using tensorflow and integrated model into machine learning pipeline\n\u2022 Integrated machine learning with 10+ Autodesk products including data lake which provided complete platform for big data\nanalytics\n\u2022 Developed Sentiment Analysis model capable of predicting positive and negative reviews using online tweets\n\u2022 Build a framework for seamless integration of Python based machine learning models in Node js pipeline', u'Data Scientist Intern\nAutodesk Inc - San Francisco, CA\nMay 2017 to November 2017\n- San Francisco, US\n\u2022 Optimized hourly data in Spark SQL with ETL process and build executive dashboard for changing subscription policy\n\u2022 Build data pipeline in big data analytics platform for visualization and reporting in Looker and Tableau\n\u2022 Performed statistical analysis and predictive modeling to improve customer base by targeting industry specific customers\n\u2022 Developed dashboard for showing users and apps metrics by aggregating data in Pyspark using REST APIs', u'System Analyst\nHCL Technologies - Noida, Uttar Pradesh\nJuly 2013 to December 2015\n- Noida, India\n\u2022 Designed web interfaces to provide common platform to access client documents, Standard Operating Procedures (SOPs) and to fill time sheets\n\u2022 Developed transformational releases on 5+ business applications in Java/J2EE for banking clients\n\u2022 Build a Regression model to predict alerts traffic in coming months which assisted the senior management team in making\nstrategies']","[u'Master of Science in Management Sciences and Quantitative Methods', u'Bachelor of Technology in Computer Science Engineering']","[u'The University of Texas at Dallas Dallas, TX\nMay 2018', u'Dr. A.P.J. Abdul Kalam Technical University\nJune 2012']","degree_1 : Master of Science in Management Sciences and Qantitative Methods, degree_2 :  Bachelor of Technology in Compter Science Engineering"
0,https://resumes.indeed.com/resume/15c9e17ac67525a2,"[u""Data Scientist\nSoundOut - Reading\nJanuary 2015 to Present\nLead data science initiatives using varied statistical methods on over 2 million\ncrowd sourced product reviews\n\u2022 Build predictive models, provide trend insights, and unique solutions for clients in consumer led markets and programmatic advertising\n\u2022 Translate findings and collaborate with innovation, product, marketing and development teams to create easy to understand and actionable metrics for\nwritten and oral presentations to industry/non-technical professionals and key\nstakeholders\n\u2022 Manage Soundout DatAcademy, an academic based internship program aimed at a diverse and internal group of students who seek experience in a commercial\ndata based environment\n\u2022 Radio Format Index (RFI) metric: Enhanced the long standing RFI metric used for predicting chart success of popular music in the US radio market.\no Built recursive binary partitioning models, with leave one out cross\nvalidation, on consumer opinion data and music streaming artist activity\no Measured accuracy using area under the receiver operating\ncharacteristic curves (ROC).\n\u2022 Fashion forecasting: Forecast sales and fashion trends using consumer market\nresearch data and historic sales data for major UK and US fashion retailers.\no Used linear regression, logistic regression, and random forest modelling to predict sales, model optimal pricing, and provide ranging insight\n\u2022 Soundout Brand Match: A psychometric tool mapping the closeness of fit between a brand's aspirational/perceived personality and media content.\no Used principal component analysis (PCA) on reviewer data to develop\nnovel measurement tool\no Maps any image, video, audio, celebrity, or competitor brand ratings in a\n5-dimensional space for comparison\no Used to position brand and media content in consumer market\n\n1"", u'Student\nSheffield\nJanuary 2015 to September 2015\nPrimary Investigator: Victoria Williamson\n\u2022 Lab analyzes internet-based survey information using qualitative and quantitative techniques to determine interactions between music and sleep\n\u2022 Designed and conducted pilot study research based on survey findings\n\u2022 Found significant effects of music on subjective sleep measures using linear\nmixed effects modelling\n\u2022 Editing paper for peer reviewed journal submission.']","[u""Master's""]",[u''],"degree_1 : ""Masters"""
0,https://resumes.indeed.com/resume/2b7b1fd8dc10c3d6,"[u""Machine Learning Engineer\nTeradata - Raleigh, NC\nJuly 2017 to Present\nDescription: Teradata is the world's leading provider of business analytics solutions, data and analytics solutions, and hybrid cloud products and services.\n\nResponsibilities:\n\u2022 Implement machine learning algorithms for document recommendation in enterprise taking advantage of several data sources available in enterprise.\n\u2022 Used R, SQL to create Statistical algorithms involving Multivariate Regression, Linear Regression, Logistic Regression, PCA, Random forest models, Decision trees, Support Vector Machine for estimating the risks.\n\u2022 Created a recommendation system using k-means clustering, NLP and Flask to generate list for potential users and worked on NLP algorithm consists of TF-IDF and LSI on the user reviews.\n\u2022 Developed OLAP cubes for the branding analysis and developed OLAP and Excel Reports for SEC reporting.\n\u2022 Managed data operations team and collaborated with data warehouse developers to meet business user needs, promote data security, and maintain data integrity.\n\u2022 Data Collection, Features creation, Model Building (Linear Regression, SVM, Logistic Regression, Decision Tree, Random Forest, GBM), Evaluation Metrics, Model Serving - R, Scikit-learn, Spark SQL, Spark ML, Flask, Redshift, AWS S3\n\u2022 Experience on Cassandra node tool to manage Cassandra cluster.\n\u2022 Involved in the process of designing Cassandra Architecture.\n\u2022 Installed, Configured, Tested Datastax Enterprise Cassandra multi-node cluster which has 4 Datacenters and 5 nodes each.\n\u2022 Worked on Real Time as well as Batch Data and have build lambda architecture to process the data using Kafka, Spark Streaming, Spark Core and Spark SQL\n\u2022 Designed and provisioned the platform architecture to execute Hadoop and machine learning use cases under Cloud infrastructure, AWS, EMR, and S3.\n\u2022 Involved in creating Data Lake by extracting customer's Big Data from various data sources into Hadoop HDFS. This included data from Excel, Flat Files, Oracle, SQL Server, MongoDb, Cassandra, HBase, Teradata, Netezza and also log data from servers\n\u2022 Developed Python code for data analysis (also using NumPy and SciPy), Curve-fitting.\n\u2022 Performed extensive Data Validation, Data Verification against Data Warehouse and performed debugging of the SQL-Statements and stored procedures for business scenarios.\n\u2022 Used Spark Data frames, Spark-SQL, Spark MLLib extensively and developing and designing POC's using Scala, Spark SQL and MLlib libraries.\n\u2022 Created and reviewed Informatica mapping documents too with business and data governance rules.\n\u2022 Created a recommendation system using k-means clustering, NLP and Flask to generate vehicles list for potential users and worked on NLP algorithm consists of TF-IDF and LSI on the user reviews.\n\u2022 Worked on predictive and what-if analysis using R from HDFS and successfully loaded files to HDFS from Teradata, and loaded from HDFS to HIVE.\n\u2022 Designed the schema, configured and deployed AWS Redshift for optimal storage and fast retrieval of data.\n\u2022 Developed ETL mappings, testing, correction and enhancement and resolved data integrity issues and coordinated multiple OLAP and ETL projects for various data lineage and reconciliation.\n\u2022 Analyzed data and predicted end customer behaviors and product performance by applying machine learning algorithms using Spark MLlib.\n\u2022 Involved in moving the complete website infrastructure to Amazon Web Service\n\u2022 Prediction of Function and Industry based on Job Text Analytics using R, Scikit-learn, NLTK, TF-IDF, Bayesian Classifier and Gensim\n\u2022 Performed transformations of data using Spark and Hive according to business requirements for generating various analytical datasets.\n\u2022 Design, Develop ETL process and create UNIX shell scripts to execute Teradata SQL, BTEQ, jobs.\n\u2022 Analyzed the bug reports in BO reports by running similar SQL queries against the source system (s) to perform root-cause analysis.\n\u2022 NLTK, Stanford NLP, RAKE to preprocess the data, entity extraction and keyword extraction.\n\u2022 Used concepts of Data Modeling Star Schema/Snowflake modeling, FACT & Dimensions tables and Logical & Physical data modeling.\n\u2022 Translated cell formulas for business users in Excel into VBA code to design, analyze, and deploy programs for their ad-hoc needs.\n\u2022 Created dimension and fact tables in Redshift, ETL to get data from different sources and insert into Redshift, Tableau for reporting using Redshift as data source\n\u2022 Coding using Teradata Analytical functions, BTEQ SQL of TERADATA, write UNIX scripts to validate, format and execute the SQLs on UNIX environment.\n\u2022 Worked on analyzing the data statistically and also prepared statistical reports SAS tool.\n\u2022 Created mapreduce running over HDFS for data mining and analysis using R and Loading & Storage data to Pig Script and R for MapReduce operations.\n\u2022 Created various types of data visualizations using R, and Tableau.\n\u2022 Created numerous dashboards in tableau desktop based on the data collected from zonal and compass, while blending data from MS-excel and CSV files, with MS SQL server databases.\n\u2022 Developed SPSS Macro, which reduced time of programming syntax and increased the productivity for whole data processing steps.\n\u2022 Participated in big data architecture for both batch and real-time analytics and mapped data using scoring system over large data on HDFS\n\nEnvironment:Horton works - Hadoop Map Reduce, Pyspark, Spark, R, Spark MLLib, Tableau, Informatica, SQL, Excel, VBA, BO, CSV, Erwin, SAS, AWS Redshift, Scala Nlp, Cassandra, Oracle, MongoDB, Cognos,SQL Server 2012, Teradata, DB2, SPSS, T-SQL, PL/SQL, Flat Files, XML, and Tableau."", u""Data Scientist\nEquinox Fitness - New York, NY\nApril 2016 to June 2017\nDescription:Equinox Fitness, an American luxury fitness company, operates a separate fitness brands like Equinox, Pure Yoga, Blink fitness and SoulCycle.\n\nResponsibilities:\n\u2022 Utilized ApacheSpark with Python to develop and execute Big Data Analytics and Machine learning applications, executed machine Learning use cases under Spark ML and Mllib.\n\u2022 Solutions architect for transforming business problems into BigData and DataScience solutions and define Big Data strategy and Roap map.\n\u2022 Identified areas of improvement in existing business by unearthing insights by analyzing vast amount of data using machine learning techniques. TensorFlow, Scala, Spark, MLLib, Python and other tools and languages needed.\n\u2022 Create and validate machine learning models with AzureMachineLearning\n\u2022 Designing a machine learning pipeline using MicrosoftAzureMachineLearning to predict and prescribe and Implemented a machine learning scenario for a given data problem\n\u2022 Used Scala for coding the components in Play and Akka.\n\u2022 Worked on different Machine learning models like Logistic Regression, Multilayer perceptron classifier, K-means clustering by creating Scala-SBT packaging and run it in Spark-shell (Scala) and Auto-encoder model with using R programming.\n\u2022 Worked on setting up and configuring AWS's EMR Clusters and Used Amazon IAM to grant fine-grained access to AWS resources to users\n\u2022 Created detailed AWS Security Groups, which behaved as virtual firewalls that controlled the traffic allowed to reach one or more AWSEC2 instances\n\u2022 Wrote scripts and indexing strategy for a migration to Redshift from Postgres9.2 and MySQL databases.\n\u2022 Wrote Kinesis agents to pipe data from streaming app into S3.\n\u2022 Good Knowledge in Azure clouds ervices, Azure storage, Azure active directory, Azure Service Bus. Create and manage AzureAD tenants, and configure application integration with AzureAD. Integrate on-premises WindowsAD with AzureAD Integrating on-premises identity with Azure Active Directory.\n\u2022 Working knowledge of Azure Fabric, Microservices, IoT&Docker containers in Azure. Azure infrastructure management & PaaS Solution Architect - (Azure AD, Licenses, Office365, DR on cloud using AzureRecoveryVault, AzureWebRoles, WorkerRoles, SQLAzure, AzureStorage).\n\u2022 Utilized Spark, Scala, Hadoop, HBase, Cassandra, MongoDB, Kafka, Spark Streaming, MLLib, Python, a broad variety of machine learning methods including classifications, regressions, dimensionally reduction etc. and Utilized the engine to increase user lifetime by 45% and triple user conversations for target categories.\n\u2022 Designed and developed NLP models for sentiment analysis.\n\u2022 Led discussions with users to gather business processes requirements and data requirements to develop a variety of Conceptual, Logical and Physical Data Models. Expert in Business Intelligence and Data Visualization tools: Tableau, Microstrategy.\n\u2022 Performed Multinomial Logistic Regression, Random forest, Decision Tree, SVM to classify package is going to deliver on time for the new route and Performed data analysis by using Hive to retrieve the data from Hadoop cluster, Sql to retrieve data from Oracle database.\n\u2022 Worked on machine learning on large size data using Spark and MapReduce.\n\u2022 Let the implementation of new statistical algorithms and operators on Hadoop and SQL platforms and utilized optimizations techniques, linear regressions, K-means clustering, Native Bayes and other approaches.\n\u2022 Developed Spark/Scala,Python for regular expression (regex) project in the Hadoop/Hive environment with Linux/Windows for big data resources.\n\u2022 Developed Data Mapping, Data Governance, Transformation and Cleansing rules for the Master Data Management Architecture involving OLTP, ODS and OLAP.\n\u2022 Data sources are extracted, transformed and loaded to generate CSV data files with Python programming and SQL queries.\n\u2022 Stored and retrieved data from data-warehouses using Amazon Redshift.\n\u2022 Worked on TeradataSQL queries, Teradata Indexes, Utilities such as Mload, Tpump, Fast load and FastExport.\n\u2022 Application of various machine learning algorithms and statistical modeling like decision trees, regression models, neural networks, SVM, clustering to identify Volume using scikit-learn package in python, Matlab.\n\u2022 Used Data Warehousing Concepts like Ralph Kimball Methodology, Bill Inmon Methodology, OLAP, OLTP, Star Schema, Snow Flake Schema, Fact Table and Dimension Table.\n\u2022 Refined time-series data and validated mathematical models using analytical tools like R and SPSS to reduce forecasting errors.\n\u2022 Worked on data pre-processing and cleaning the data to perform feature engineering and performed data imputation techniques for the missing values in the dataset using Python.\n\u2022 Created Data Quality Scripts using SQL and Hive to validate successful data load and quality of the data. Created various types of data visualizations using Python and Tableau.\n\nEnvironment: Python, Azure, ER Studio, Hadoop, Map Reduce, EC2, S3, Pyspark, Spark, Spark MLLib, Tableau, Informatica, SQL, Excel, VBA, BO, CSV, Netezza, SAS, Matlab, AWS, Scala Nlp, SPSS, Cassandra, Oracle, Amazon Redshift, MongoDB, SQL Server 2012, Teradata, DB2, T-SQL, PL/SQL, Flat Files, XML, Tableau."", u""Data Scientist/Data Architect\nFarm Bureaus - Columbus, OH\nDecember 2014 to March 2016\nDescription:Over the last 85 years, Nationwide has grown from a small mutual auto insurer owned by Policyholders to one of the largest insurance and financial services companies in the world. Early growth came\nfrom working together with Farm Bureaus that sponsored the company. Nine Farm Bureaus continue to promote Nationwide.\n\nResponsibilities:\n\u2022 Used R, SQL to create Statistical algorithms involving Multivariate Regression, Linear Regression, Logistic Regression, PCA, Random forest models, Decision trees, Support Vector Machine for estimating the risks.\n\u2022 Managed data operations team and collaborated with data warehouse developers to meet business user needs, promote data security, and maintain data integrity.\n\u2022 Used R and python for Exploratory DataAnalysis, A/Btesting, Anova test and Hypothesis test to compare and identify the effectiveness of Creative Campaigns.\n\u2022 Designed and provisioned the platform architecture to execute Hadoop and machine learning use cases under Cloud infrastructure, AWS, EMR, and S3.\n\u2022 Involved in designing and deploying a multiple applications utilizing almost all of the AWS stack (Including EC2, Route53, S3, RDS, DynamoDB, SNS, SQS, IAM) focusing on high-availability, fault tolerance, and auto-scaling in AWSCloudformation.\n\u2022 Experience in Performance Tuning and Query Optimization in AWSRedshift and ConfiguredAWSIdentityAccessManagement (IAM) Group and users for improved login authentication.\n\u2022 Used ETL Tools for masking and cleaning data andmined data from various sources.\n\u2022 Involved in creating Data Lake by extracting customer's Big Data from various data sources into Hadoop HDFS. This included data from Excel, Flat Files, Oracle, SQL Server, MongoDb, Cassandra, HBase, Teradata, Netezza and also log data from servers\n\u2022 Developed Python code for data analysis (also using NumPy and SciPy), Curve-fitting.\n\u2022 Performed extensive Data Validation, Data Verification against Data Warehouse and performed debugging of the SQL-Statements and stored procedures for business scenarios.\n\u2022 Used Spark Data frames, Spark-SQL, Spark MLLib extensively and developing and designing POC's using Scala, Spark SQL and MLlib libraries.\n\u2022 Worked on predictive and what-if analysis using R from HDFS and successfully loaded files to HDFS from Teradata, and loaded from HDFS to HIVE.\n\u2022 Developed ETL mappings, testing, correction and enhancement and resolved data integrity issues and coordinated multiple OLAP and ETL projects for various data lineage and reconciliation.\n\u2022 Optimized and tuning the Redshift environment, enabling queries to perform up to 100 xs faster for Tableau and SASVisualAnalytics.\n\u2022 Design, Develop ETL process and create UNIX shell scripts to execute Teradata SQL, BTEQ, jobs.\n\u2022 Analyzed the bug reports in BO reports by running similar SQL queries against the source system (s) to perform root-cause analysis.\n\u2022 NLTK, Stanford NLP, RAKE to preprocess the data, entity extraction and keyword extraction.\n\u2022 Used concepts of Data Modeling Star Schema/Snowflake modeling, FACT & Dimensions tables and Logical & Physical data modeling.\n\u2022 Translated cell formulas for business users in Excel into VBA code to design, analyze, and deploy programs for their ad-hoc needs.\n\u2022 Coding using Teradata Analytical functions, BTEQ SQL of TERADATA, write UNIX scripts to validate, format and execute the SQLs on UNIX environment.\n\u2022 Created MapReduce running over HDFS for data mining and analysis using R and Loading & Storage data to Pig Script and R for MapReduce operations.\n\u2022 Created numerous dashboards in tableau desktop based on the data collected from zonal and compass, while blending data from MS-excel and CSV files, with MS SQL server databases.\n\nEnvironment:Horton works - Hadoop Map Reduce, Pyspark, Spark, R, Spark MLLib, Tableau, Informatica, SQL, Excel, VBA, BO, CSV, Erwin, SAS, AWS Redshift, Scala Nlp, Cassandra, Oracle, MongoDB, Cognos,SQL Server 2012, Teradata, DB2, SPSS, T-SQL, PL/SQL, Flat Files, XML, and Tableau."", u""Data Architect/ Data Modeler\nAmeresco Inc - Framingham, MA\nApril 2013 to November 2014\nDescription:Ameresco, Inc. provides energy efficiency solutions for facilities in North America. The company engages in the design, engineering, development, and installation of projects that reduce the energy, and operations and maintenance costs of its customers' facilities.\nResponsibilities:\n\u2022 Understand and analyze business data requirements and architect an accurate, extensible, flexible and logical data model and Defining and implementing conceptual, logical, and physical data modeling concepts.\n\u2022 Defining Data Sources and data models, documenting actual data flows, data exchanges, and systems interconnections and interfaces. Ensuring these is aligned with the enterprise data model.\n\u2022 Design and build world class high-volume real-time data ingestion frameworks and automate various data sources into Bigdata technologies like Hadoop etc.\n\u2022 Performed Data mapping between source systems to Target systems, logical data modeling, created class diagrams and ER diagrams and used SQL queries to filter data.\n\u2022 Developed MapReduce programs to parse the raw data, populate staging tables and store the refined data in partitioned tables in the EDW.\n\u2022 Designed different type of STAR schemas using ERWIN with various Dimensions like time, services, customers and FACT tables.\n\u2022 Analyze database infrastructure to insure compliance with customer security standards, database performance considerations, and reverse engineering of existing database environments.\n\u2022 Used Hive and created Hive tables and involved in dataloading and writing Hive UDFs.\n\u2022 Creation of BTEQ, Fast export, MultiLoad, TPump, Fast load scripts for extracting data from various production systems.\n\u2022 Creation of database objects like tables, views, Materialized views, procedures, packages using Oracle tools like PL/SQL, SQL* Plus, SQL*Loader and Handled Exceptions.\n\u2022 Worked on Hive for exposing data for further analysis and for generating transforming files from different analytical formats to text files.\n\u2022 Extensively used Erwin for developing data model using star schema methodologies.\n\u2022 Worked on importing and exporting data from Oracle and DB2 into HDFS using Sqoop\n\u2022 Created, optimized, reviewed and executed Teradata SQL test queries to validate transformation rules used in source to target mappings/source views, and to verify data in target tables.\n\u2022 Used Pig as ETL tool to do transformations, event joins and some pre-aggregations before storing the data onto HDFS.\n\u2022 Provided ad-hoc queries and data metrics to the Business Users using Hive, Pig.\n\nEnvironment: ERWIN 9.x, Informatica Power Mart (Source Analyzer, Data warehousing designer, Mapping Designer, Transformations), MS SQL Server, Oracle, SQL, Hive, Map Reduce, PIG, Sqoop, HDFS, Hadoop, Teradata, Netezza, PL/SQL, Informatica, SSIS, SSRS."", u""Data Modeler/Data Analyst\nKotak Mahindra Bank - Mumbai, Maharashtra\nNovember 2011 to March 2013\nDescription:Kotak Mahindra Bank is an Indian private sector bank headquartered in Mumbai, Maharashtra, India. In February 2003, Reserve Bank of India (RBI) gave the licence to Kotak Mahindra Finance Ltd., the group's flagship company, to carry on banking business.\n\nResponsibilities:\n\u2022 Analyzed data sources and requirements and business rules to perform logical and physicaldatamodeling.\n\u2022 Analyzed and designed best fit logical and physicaldatamodels and relational database definitions using DB2. Generated reports of data definitions.\n\u2022 Involved in Normalization/De-normalization, Normal Form and database design methodology.\n\u2022 Maintained existing ETL procedures, fixed bugs and restored software to production environment.\n\u2022 Developed the code as per the client's requirements using SQL, PL/SQL and Data Warehousing concepts.\n\u2022 Involved in Dimensional modeling (Star Schema) of the Data warehouse and used Erwin to design the business process, dimensions and measured facts.\n\u2022 Worked with Data Warehouse Extract and load developers to design mappings for Data Capture, Staging, Cleansing, Loading, and Auditing.\n\u2022 Developed enterprise data model management process to manage multiple data models developed by different groups\n\u2022 Designed and created Data Marts as part of a data warehouse.\n\u2022 Wrote complex SQL queries for validating the data against different kinds of reports generated by Business Objects XIR2.\n\u2022 Using Erwin modeling tool, publishing of a data dictionary, review of the model and dictionary with subject matter experts and generation of data definition language.\n\u2022 Coordinated with DBA in implementing the Database changes and also updating DataModels with changes implemented in development, QA and Production.Worked Extensively with DBA and Reportingteam for improving the ReportPerformance with the Use of appropriate indexes and Partitioning.\n\u2022 Developed Data Mapping, Transformation and Cleansing rules for the Master Data Management Architecture involved OLTP, ODS and OLAP.\n\u2022 Tuned and coded optimization using different techniques like dynamic SQL,dynamic cursors, and tuningSQL queries, writing generic procedures, functions and packages.\n\u2022 Experienced in GUI, Relational Database Management System (RDBMS), designing of OLAP system environment as well as Report Development.\n\u2022 Extensively used SQL, T-SQL and PL/SQL to write stored procedures,functions, packages and triggers.\n\u2022 Analyzed of data report were prepared weekly, biweekly, monthly using MS Excel, SQL & UNIX.\n\nEnvironment: ER Studio, Informatica Power Center 8.1/9.1, Power Connect/ Power exchange, Oracle 11g, Mainframes,DB2 MS SQL Server 2008, SQL,PL/SQL, XML, Windows NT 4.0, Tableau, Workday, SPSS, SAS, Business Objects, XML, Tableau, Unix Shell Scripting, Teradata, Netezza, Aginity."", u""Data Analyst\nJSW Steel Ltd - Mumbai, Maharashtra\nJanuary 2009 to October 2011\nDescription: JSW Steel Ltd. The flagship company of over $11 billion JSW Group, JSW Steel is one of India's leading integrated steel manufacturers with a capacity of 18 MTPA.\n\nResponsibilities:\n\u2022 Understood and articulated business requirements from user interviews and then convert requirements into technical specifications. Effectively communicated with the SMEs to gather the requirements.\n\u2022 Worked on Regression in performing Safety Stock and Inventory Analysis using R and performed data visualizations using Tableau and R.\n\u2022 Used SQL to retrieve data from the Oracle database for data analysis and visualization and performed Inventory Analysis with Statistical and Data Visualization Tools.\n\u2022 Followed the RUP based methods using Rational Rose to create Use Cases, Activity Diagrams / State Chart Diagrams, Sequence Diagrams.\n\u2022 Designed different type of STAR schemas for detailed data marts and plan data marts in the OLAP environment.\n\u2022 Implemented Classification using supervised algorithms like Logistic Regression, Decision trees, KNN, Naive Bayes.\n\u2022 Involved with Data Analysis primarily Identifying Data Sets, Source Data, Source Meta Data, Data Definitions and Data Formats\n\u2022 Performed Decision Tree Analysis and Random forests for strategic planning and forecasting and manipulating and cleaning data using dplyr and tidyr packages in R.\n\u2022 Involved in data analysis and creating data mapping documents to capture source to target transformation rules.\n\u2022 Creating or modifying the T-SQL queries as per the business requirements and worked on creating role playing dimensions, fact-less Fact, snowflake and star schemas.\n\u2022 Wrote, executed, performance tuned SQL Queries for Data Analysis& Profiling and wrote complex SQL queries using joins, sub queries and correlated sub queries.\n\u2022 Involved in development and implementation of SSIS, SSRS and SSAS application solutions for various business units across the organization.\n\u2022 Developed mappings to load Fact and Dimension tables, SCD Type 1 and SCD Type 2 dimensions and Incremental loading and unit tested the mappings.\n\u2022 Wrote test cases, developed Test scripts using SQL and PL/SQL for UAT.\n\u2022 Worked with the ETL team to document the Transformation Rules for Data Migration from OLTP to Warehouse Environment for reporting purposes.\n\u2022 Transferred data from various OLTP data sources, such as Oracle, MS Access, MS Excel, Flat files, CSV files into SQL Server.\n\u2022 Performed data testing, tested ETL mappings (Transformation logic), tested stored procedures, and tested the XML messages.\n\u2022 Created Use cases, activity report, logical components to extract business process flows and workflows involved in the project using Rational Rose, UML and Microsoft Visio.\n\nEnvironment: R, SQL, Tableau, SSRS, Oracle, T-SQL, UNIX Shell Scripting, DB2.""]","[u""Bachelor's""]",[u''],"degree_1 : ""Bachelors"""
0,https://resumes.indeed.com/resume/3dfbcfd1a92f4393,"[u'Data Scientist in Training\nGeneral Assembly - Boston, MA\nJanuary 2018 to Present\n\u2022 500 hours of data analysis and coding experience.\n\u2022 Projects Completed:\n\u25e6 SAT Scores Analysis - Pandas, Matplotlib\n\u25e6 Iowa Liquor Analysis - Pandas, Seaborn\n\u25e6 Reddit - Web Scraping, Pandas, Natural Language Processing\n\u25e6 Capstone - Web Scraping, Convolutional Neural Networks', u'Senior Financial Analyst\nState Street Global Advisors\nJanuary 2013 to January 2018\n\u2022 Operated as the sole financial analyst for the Charitable Asset Management division handling assets of over $3 billion; generated monthly accounting reports.\n\u2022 Generated management-oriented reports and dashboards pertaining to accounts receivable\n\u2022 Spearheaded collection processes company-wide']","[u'MBA', u""Bachelor's in Marketing""]","[u'UMASS Dartmouth North Dartmouth, MA\nSeptember 2010 to December 2011', u'UMass Dartmouth North Dartmouth, MA\nSeptember 2006 to May 2010']","degree_1 : MBA, degree_2 :  ""Bachelors in Marketing"""
0,https://resumes.indeed.com/resume/e02eddffd349df71,"[u'Data Analytics Intern\nInvestopedia - New York, NY\nFebruary 2018 to Present\n\u2022 Use MySQL, Tableau, Python, and Google cloud platform for ad-hoc analysis requests for Business Intelligence team', u'Data Science Intern\nNew York Magazine\nSeptember 2017 to December 2017\n* Use SQL to optimize and automate dashboards for analysis and eCommerce teams\n* Conduct analyses on ROI from third party marketing. Results used to determine lifetime product revenue versus costs and consumer loyalty derived from outside partners\n* Work with eCommerce team at planning stage to ensure marketing campaigns can be accurately\nmeasured', u'Data Analyst Consultant\nTumblr - New York, NY\nApril 2017 to August 2017\n* Conduct analyses on Action Tumblr campaign website by investigating site\nmetrics to determine how to improve strategy\n* Determined time of day for post execution to maximize number of followers based on past data;\nfollowers for campaign increased after adoption of improved strategy\n* Present findings and effectively communicated insights to executive staff via data visualization tools:\nTableau, Excel', u'Data Analyst\nNew York City Department of Health - New York, NY\nDecember 2016 to August 2017\n* Created spreadsheets via Excel and Visual Basic (VBA) coding incorporating Google Maps API for internal personnel and distribution amongst external clients\n* Explained methods and conclusions to many NYC DOH employees with varying technical expertise', u'SAS Programmer\nMathematica Policy Research - Washington, DC\nJune 2016 to August 2016\n* Developed data comparison programs in SAS for CMS MSIS (Medicaid Statistical Information System) for files containing summary Medicaid data from all states in the US\n* Developed systems and presented programs via code reviews to senior staff', u'Data Scientist\nIntegrated Mechanical Care - Tallahassee, FL\nSeptember 2015 to June 2016\nUtilized research methods in R and SAS for research papers such as: Kappa coefficients for agreement\nstudies, calculating prevalence and bias indexes, developing matrices to facilitate data manipulation']","[u'Master in Epidemiology', u'Bachelor of Science in Economics']","[u'Columbia University New York, NY\nJuly 2018', u'Florida State University Tallahassee, FL']","degree_1 : Master in Epidemiology, degree_2 :  Bachelor of Science in Economics"
0,https://resumes.indeed.com/resume/d1f1ef87c6e092d4,"[u""Data Scientist\nMilliman Inc\nAugust 2017 to November 2017\nImproved the minority class prediction of client's claims data by using class imbalance correction techniques like\nSynthetic Minority class Over-sampling Technique (SMOTE) combined with Ada Boost classifier\n* Increased the performance of the best classifiers by tuning the hyperparameters of the algorithm using grid search cross\nvalidation in Scikit-Learn which trains the model in parallel on a 36 VCPU AWS virtual machine."", u'Machine Learning Intern\nYobs\nJune 2017 to August 2017\n* Designed, developed and tested the data preparation pipeline comprising of data collection, feature scaling and feature\nselection mechanisms which reduced the turnaround time by 80%\n* Built and optimized machine learning models for classifying personality of a candidate from multi-modal (Audio, Video,& Text) data sources which was used as a prime indicator by recruiters to select appropriate candidates for the job', u""Senior Data Analyst\nLatentView Analytics\nJune 2014 to July 2016\nPredicted most valuable merchants for PayPal Europe using a random forest and gradient boosting ensemble in Python,\nimproving the accuracy of the existing model by 14%. Wrote a web scrapper to engineer features from the merchant's\nwebsite\n* Automated the feature engineering and prediction process in shell and saved 20 man-hours a week\n* Developed A/B Testing experimentation on webpage variants for PayPal US, aided in the tracking of different webpage\nmetrics using Google Analytics and gave recommendations for optimizing to improve conversion rates\n* Identified the topics and sentiments from people's tweets using sentiment analysis, word association and topic\nmodelling and visualized the results in a dashboard designed using D3.Js\n* Designed a web interface for an anomaly detection tool using Python-Flask for PayPal US"", u'Decision Scientist\nMu Sigma Business Solutions\nJune 2013 to June 2014\nDeveloped a market mix model for Advanced Analytics division of Sanofi US that effectively optimizes the spend of different pharmaceutical marketing channels thereby facilitating a 7% increase in the ROI']","[u'Master of Science in Data Science', u'Bachelors of Technology in Information Technology']","[u'Indiana University\nMay 2018', u'Madras Institute of Technology\nMay 2013']","degree_1 : Master of Science in Data Science, degree_2 :  Bachelors of Technology in Information Technology"
0,https://resumes.indeed.com/resume/3af3d0d237b34448,"[u'DATA SCIENTIST INTERN\nSIEMENS B\nJune 2017 to August 2017\nStatistical computing\nApplied regression analysis \u2022 Working on data cleaning, Regression and clustering of data using libraries in\nOptimisation techniques Python such as Scikit, Pandas, Numpy, Matplotlib\nInternet of things \u2022 Optimisation of various processes such as enthalpy to reduce power\nDatawarehousing consumption by developing algorithms in MATLAB such as Steepest descent,\nApplied Mathematics-1,2, 3, 4 Conjugate gradient, Exterior and interior penalty functions.\nNatural languages technologies \u2022 Visualising data in Javascript using d3 and coorelating it with the plots\nQuality management in IT generated by Matplotlib to find out if there is any anomalous activity.', u'GM\nHOME AUTOMATION SYSTEM\nAugust 2016 to December 2016\nGuide: Prof.Anthony Smith\nNETWORKING \u2022 Wrote scripts in Python for calculating the number of times the door has\nopened and closed.\nACCOUNTS \u2022 Extracted the data from google sheets and did statistical analysis on it.\nHacker-rank: pvangur \u2022 Based on the data received, suggested the location of the sensors to provide\nGithub account: Pradyuth1 accurate reading.\nLinkedin account: Pradyuth Vangur \u2022 Regression analysis was done based upon various variables and a formula was\nvalidated using SPSS software, which was used for predictive analytics.', u'ENGINEERING INTERN\nMay 2014 to July 2014\nSKILLS\nLanguages \u2022 Used SAS to generate regression models.\nSAS\u2022R\u2022MATLAB\u2022Python \u2022 Optimisation of various design processes in MATLAB.\n\u2022SQL\u2022Javascript\u2022C++\nSoftwares PROJECTS\nKnime \u2022Tableau\u2022Hadoop\u2022Excel\nAPPLIED REGRESSION ANALYSIS G']",[u'MS in EDUCATION'],[u'PURDUE UNIVERSITY W\nAugust 2012 to May 2016'],degree_1 : MS in EDUCATION
0,https://resumes.indeed.com/resume/ce8c5722820503a7,"[u""Data Scientist Co-op\nAhold Delhaize - Quincy, MA\nJanuary 2017 to June 2017\n\u2022 Visualized data gathered by Computer Vision application prototype to comprehend and analyse hotspots for footprints in the store using Tableau and Power BI.\n\u2022 Modelled a backend rule based recommendation engine using Azure studio for healthy eating choices for Stop & Shop customers.\n\u2022 Ideated and developed an alerting system for proactive sales monitoring using transaction logs on hive, hue and oozie workflow; received Auroro Monroe award (co-op Spring'17) for storming through big data."", u'Senior Software Engineer\nTata Consultancy Services - Mumbai, Maharashtra\nDecember 2013 to July 2014\n\u2022 Managed migration of 50,000 business critical documents from Documentum to SharePoint for the entire UK and APAC region individually and well within project deadline resulting in enhanced client relationships.\n\u2022 Promoted to the role of Module Lead for upstream Documentum services for entire UK Region within 3 months of joining the company.', u""Software Engineer\nWipro Infotech Ltd - Hyderabad, Telangana\nSeptember 2010 to October 2013\n\u2022 Analysed, developed and implemented reusable DFC code and Documentum jobs for identifying and migrating huge data from SAN to SATA storage; received 'Star Performer' award for resolving space crunch issue.\n\u2022 Managed Requirement Gathering (SPOC) from multiple clients from different Business Organizational Units for a new development project worth $60,000.""]","[u'Master of Science in Information Systems', u'Bachelor of Engineering in Computer Science']","[u'Northeastern University Boston, MA\nDecember 2017', u'M.P. Christian College of Engineering & Technology\nJune 2010']","degree_1 : Master of Science in Information Systems, degree_2 :  Bachelor of Engineering in Compter Science"
0,https://resumes.indeed.com/resume/bf61259a07d398e8,"[u'Manager of Information and Data Standards\nAMERICAN COLLEGE OF SURGEONS, COMMISSION ON CANCER, NATIONAL CANCER DATABASE\nNovember 2014 to Present\n\u2022 Serves as a thought leader for the NCDB and the CoC, increasing communication and collaboration across all Cancer Programs; provides guidance and leadership to managers and professional staff, and influences others internally and externally, including senior leaders.\n\u2022 Maintains national leadership presence serving as the primary CoC communicator and collaborator with the cancer registration and surveillance community, participating on over 15 national taskforces, steering committees and work groups, including high level groups such as the National Coordinating Council for Cancer Surveillance, Technical Advisory Group, Change Management Board, and the Stage Transition Workgroup.\n\u2022 Serves as the NCDB representative on the American Joint Committee on Cancer (AJCC) Executive Committee, served on the AJCC 8th Edition Data Collection Core Group and currently serves on the AJCC Informatics Committee; contributes heavily to discussions with Leadership regarding issues such as AJCC 8th edition staging manual implementation, alignment of AJCC efforts with the cancer surveillance community and various other issues.\n\u2022 Participates in Quality of Care Measure development and implementation, providing expertise related to data items used in measure algorithms.\n\u2022 Responsible for all aspects of data submission to the NCDB, including daily submission to the Rapid Quality Reporting System and annual submission to the NCDB Annual Call for Data; supervises NCDB Database/ETL Developer.\n\u2022 Works with outside vendor serving as the sole Subject Matter Expert to design and communicate specifications for an all-new, streamlined, and user-friendly NCDB Data Submission and Processing system.\n\u2022 Contributes to day-to-day efforts of the NCDB IT team, providing expertise in automated registry functions to design and enhance NCDB reporting applications; communicates system issues and participates in the resolution of identified issues; frequently works directly with the individual programmers offering guidance and trouble-shooting support.\n\u2022 Administers the NCDB Data Quality Assurance Program including maintenance of the Facility Oncology Registry Data Standards (FORDS) coding manual used nationally by ~1500 CoC-accredited hospitals; supervises NCDB User Support Specialist staff to improve and maintain overall timeliness and accuracy of all NCDB communications, including all inquiries submitted to the NCDB and RQRS email boxes; implements standard operating procedures to ensure the CoC CAnswer Forum remains a respected resource for gold standard coding.', u'Health Research Analyst 4/Subject Matter Expert\nNORTHROP GRUMMAN / CENTERS\nNovember 2005 to November 2014\nFOR DISEASE CONTROL AND PREVENTION\nNational Program of Cancer Registries, Registry Plus Development Team\n11/05-11/14 Health Research Analyst 4/Subject Matter Expert\n\nAssisted in the development of a suite of cancer registry software in support of the National Program for Cancer Registries; provided special expertise in the area of automated registry functions to design, test, and document modules of central registry software; provided technical support and training to users of the system.', u'Research Scientist III/Syndromic Surveillance Coordinator\nHEALTH RESEARCH INC\nDecember 2004 to November 2005\nBioterrorism Epidemiology Program\n12/04-11/05 Research Scientist III/Syndromic Surveillance Coordinator\n\nInvolved in development and implementation of statewide surveillance tools to collect, integrate, and analyze secondary-use, health-related data for indicators of diseases due to natural causes, terrorist agents, and chemical, radiological, or other environmental exposures.', u'Research Scientist III\nHEALTH RESEARCH INC\nSeptember 2001 to December 2004', u'Research Scientist II\nAugust 1999 to September 2001\nInvolved in research, surveillance, and data quality assurance of NYS Cancer Registry data, as well as design of data collection and processing systems.']","[u'in Epidemiology', u'in Pharmacology and Toxicology', u'Bachelor of Arts in Biopsychology']","[u'SCHOOL OF PUBLIC HEALTH AT SUNY ALBANY Albany, NY\nSeptember 2000 to December 2001', u'ALBANY MEDICAL COLLEGE Albany, NY\nAugust 1992 to July 1994', u'VASSAR COLLEGE Poughkeepsie, NY\nJanuary 1990']","degree_1 : in Epidemiology, degree_2 :  in Pharmacology and Toxicology, degree_3 :  Bachelor of Arts in Biopsychology"
0,https://resumes.indeed.com/resume/e23fc73bd962e8ac,"[u'Sr. Big Data Architect/Data Scientist\nPilot Travel Centers @TN\nJuly 2017 to Present\nLeading big data initiative for On-Premise fuel pricing strategic solution to complete local gas markets dynamically and predict price changes, adjust fuel supply with automated B2B ordering.\n\u2713 Architect real-time solution, install, configure from ground up, saved $200k+ project cost.\n\u2713 Solved fuel pricing problem using regression models, Monte Carlo simulation for <2 hrs SLA.\n\u2713 Using tools such as Hadoop, Hive, Python, R, Spark, Kafka, Spark MLlib, GraphX, Docker etc.\n\u2713 Accelerate deployment timelines using Hortonworks HDP and Continuous Delivery approaches.', u""Sr. Big Data Architect/Data Scientist\nThomson Reuters\nApril 2014 to June 2017\nThomson Reuters @ PA) April 2014 - June 2017\nKey big data architecture leader in support of Thomson Reuters IP & Science division a team of 6 architects, developers in various domains. Key stakeholder in shaping new gen research analytics metrics, led big data & AWS initiatives, POCs and led development and implementation team. Leading Big data enablement in Cloud, migration, and increase adoption in general across IP & Science applications. Based on Cloudera, Qubole, AWS platforms.\n\u2713 Saved >$1.5M over 3 years for ML based research recommendation engine with AWS-enabled Qubole solution:\n\u2022 Adopted text mining with ML, SVM, KNN, K-means, deep learning for engine design.\n\u2022 Lambda & kappa architecture for analytics implementation.\n\u2022 AWS Well-Architected Framework & DevOps approach using Jenkins, Docker.\n\u2022 Hybrid cloud capabilities such as On-premise AWS Storage Gateway, AWS Directory Service etc\n\u2713 Exceeded client's expectation with significant platform redesign & performance tuning with $11m budget, saving ~$2m support contract costs for 5 years by:\n\u2022 Creating and presenting a custom scenario to demonstrate value with POCs.\n\u2022 Collaborated with client, internal business teams & IT teams.\n\u2713 Established TR IP&S as an industry leader by new data acquisition, enrich with new data sources."", u'Big Data Architect\nCMS.gov\nJanuary 2012 to January 2014\nLed medical claims fraud detection & flags using machine learning algorithms, regression, ANOVA, clustering to save multi million $.\nArchitected new Platform-as-a-Service offering by modernizing data and application architecture for data science to accelerate productizing new data science products/models.\n\u2713 Developed Big Data Architecture to support Data Science & exploratory analysis work @CMS.\n\u2713 Unleashed Big Data and Artificial Intelligence toolsets such as R, SAS, Scala, Tableau, Accumulo, Python (NumPy and SciKit) etc.\n\u2713 Installed Cloudera CDH 4.x/5.x, integrated with SAS. Tuned code to exceed HIPPA SLAs.', u'Sr. Manager & Architect\nEBay Enterprise\nJanuary 2011 to January 2012\nKey Delivery Manager & Architect in defining the direction for eBay/GSI Commerce data integration & data analytics for 20+ online retail clients. Established high performance data analytics team, data governance processes and technology standards.\n\u2713 Flawless implementation of data integration, predictive analytics solution architecture with analytical building blocks & toolkit to save $3m+ new development costs.\n\u2713 Led a $12m+ legacy application modernization effort to SAS, DataStage, Netezza, Tableau platform. Saved +$2m per annum support costs estimated. Client contract negotiations.\n\u2713 Architected analytical & ETL solution, design of experiments. Managed a team size of 14.', u'Sr. Consultant\nBoydan Associates\nJanuary 2003 to January 2011\nChallenged in data analysis, data integration, data quality, data governance, master data domains with complex problems for timely results. Extensively used SAS, DataStage, IBM MDM as hands on Architect, Statistics programmer & ETL Developer.\nClients: FDIC, BCBS Association, Constant Contact, IBM USA, IBM Belgium, Wal-Mart, US Bank, De Lage Landen, US Bank, Budget Rental, AVIS, Verizon.']","[u'Masters in Computational Biology', u'Masters in Computer Science']","[u'New Jersey Inst. of Tech Newark, NJ\nJanuary 2001 to January 2003', u'New Jersey Inst. of Tech Newark, NJ\nJanuary 1999 to January 2001']","degree_1 : Masters in Comptational Biology, degree_2 :  Masters in Compter Science"
0,https://resumes.indeed.com/resume/bbd769ba19106907,"[u'Data Scientist\nSantander Bank - Holmdel, NJ\nJune 2017 to Present\nSantander Bank is based in Boston and its principal market is the northeastern United States. The Bank offers financial services and products including retail banking, mortgages, corporate banking, cash management, credit card, capital markets, trust and wealth management, and insurance.\n\nThe project was to build predictive models for the identification/detection of fraudulent transactions by applying machine learning methods, principle component analysis, and logistic regression on large dataset.\n\nResponsibilities:\n\u2022 Participated in all phases of data acquisition, data cleaning, developing models, validation, and visualization to deliver data science solutions.\n\u2022 Worked on fraud detection analysis on payments transactions using the history of transactions with supervised learning methods.\n\u2022 Collected data in Hadoop and retrieved the data required for building models using Hive.\n\u2022 Developed Spark Python modules for machine learning & predictive analytics in Hadoop.\n\u2022 Used Pandas, Numpy, Seaborn, Matplotlib, Scikit-learn in Python for developing various machine learning models and utilized algorithms such as Decision Trees, Logistic regression, Gradient Boosting, SVM and KNN.\n\u2022 Used cross-validation to test the models with different batches of data to optimize the models and prevent overfitting.\n\u2022 Used PCA and other feature engineering techniques for high dimensional datasets while maintaining the variance of most important features.\n\u2022 Created Transformation Pipelines for preprocessing large amount of data with methods such as imputing, scaling, selecting, etc.\n\u2022 Ensemble methods were used to increase the accuracy of the training model with different Bagging and Boosting methods.\n\nEnvironment:\nHadoop 2.x, HDFS, Hive, Pig Latin, Python 3.x (Numpy, Pandas, Scikit-learn, Matplotlib), Jupyter, GitHub, Linux', u""Data Analyst/Data Scientist\nSCIO Health Analytics - Hartford, CT\nApril 2015 to May 2017\nSCIO Health Analytics provides analytics solutions and services that turns data into actionable insights for health care providers in the United States and globally. Services also include medical and pharmacy claims auditing, inpatient data pursuits, care gaps closure, and commercial analytics.\n\nI was part of the team that worked with Subrogation claims of Healthcare Providers such as Humana. The objective was to load data, analyze, and provide monthly reports for the predictions on a claim's potential of a third-party recovery. Tableau and SSRS were used to build claim and recovery reports.\n\nResponsibilities:\n\u2022 Assembled a Predictive Modelling module by using supervised learning for Subrogation Claim Prediction to identify which claims would be classified as having Subrogation potential.\n\u2022 Implemented models such as Logistic Regression and Na\xefve Bayes, in Python using scikit-learn, to predict the claim potential outcome.\n\u2022 Dimensionality Reduction techniques applied to refine the attribute lists and feature selection applied to rank selected features to generate accurate results.\n\u2022 Gathered requirements and business rules from business users to implement Predictive Modelling.\n\u2022 Designed and developed ETL packages using SSIS to create Data Warehouses from different tables and file sources like Flat and Excel files, with different methods in SSIS such as derived columns, aggregations, Merge joins, count, conditional split and more to transform the data.\n\u2022 Designed reporting solutions for different stakeholders from mock-up till deployment in different areas such as Potential Subrogation claims, Monthly Revenue from Subrogation & Transactions.\n\u2022 Performed data visualization and designed dashboards with Tableau, and provided complex reports, including charts, summaries, and graphs to interpret the findings for Adjustors to view various claim information.\n\u2022 Optimized queries in T-SQL by removing unnecessary columns and redundant data, normalized tables, established joins and indices; developed complex SQL queries, stored procedures, views, functions and reports that meet customer requirements.\n\nEnvironment:\nPython 3.x (Scikit-learn, Matplotlib), Jupyter, SQL Server 2012, MS SQL Server Management Studio, MS BI Suite (SSIS/SSRS), T-SQL, Visual Studio BIDS, Tableau"", u'SQL BI Developer\nADP - Chennai, Tamil Nadu\nFebruary 2012 to March 2015\nADP is a leading provider of human resources management software and services worldwide.\n\nThis project was done for an internal business unit (ADP France) to comply with French statutory requirements for employee training. Goal was to develop a web-based SQL application built upon a baseline HRMS application to generate/support the Report development for training plans, budget preparation, cost tracking and 2483 reporting.\n\nResponsibilities:\n\u2022 Collected requirements from business users, and designed report models to meet business requirements.\n\u2022 Directed and managed meetings with clients, tracked document changes and ensured sign-off from clients.\n\u2022 Maintained and developed complex SQL queries, stored procedures, views, functions and reports that meet customer requirements using Microsoft SQL Server 2008 R2.\n\u2022 Created Views and Table-valued Functions, Common Table Expression (CTE), joins, complex subqueries to provide the reporting solutions.\n\u2022 Optimized the performance of queries with modification in T-SQL queries, removed the unnecessary columns and redundant data, normalized tables, established joins and created index.\n\u2022 Created, managed, and delivered interactive web-based reports to support daily operations.\n\u2022 Validated reports and resolve issues in a timely manner.\n\u2022 Developed and implemented several types of Reports (Training Reports, Schedules, Costs Summary Reports and Annual 2483 Report) by using features of SSRS such as sub-reports, drill down reports, summary reports and parameterized reports.\n\u2022 Designed and developed new reports and maintained existing reports for the Human Resource Management System Dashboards using Tableau, Qlikview and Microsoft Excel to support the business strategy and management.\n\u2022 Identified process improvements that significantly reduce workloads or improve quality.\n\nEnvironment:\nSQL Server 2008 R2, MS SQL Server Management Studio, SSRS, T-SQL, Visual Studio BIDS, Tableau, Qlikview']",[u'Master of Science in Business & Information Systems in Business & Information Systems'],"[u'New Jersey Institute of Technology Newark, NJ']",degree_1 : Master of Science in Bsiness & Information Systems in Bsiness & Information Systems
0,https://resumes.indeed.com/resume/ccca8b7b348e29db,"[u'Data Scientist\nCrixlabs - San Francisco, CA\nJuly 2016 to February 2018\n- Applied deep learning, computer vision / image processing on image datasets.\n- Performed data analysis and extracted insights from data.']","[u""Master's in Electrical Engineering""]","[u'University of Southern California Los Angeles, CA\nAugust 2014 to May 2016']","degree_1 : ""Masters in Electrical Engineering"""
0,https://resumes.indeed.com/resume/a90523aac9609797,"[u'DATA SCIENTIST\nUT Austin - Austin, TX\nMay 2017 to Present\n\u2022 Building dashboard-style website with dynamic visualizations, summary data, and record lookup, with a SQL backend database.\n\u2022 Built dataset with millions of nonprofit IRS returns (100GB+).\n\u2022 Wrote Python code to flatten unstructured xmls into structured csv/SQL data. Wrote R analytics code.\n\u2022 Wrote optimized Linux/Bash code to improve the accuracy of messy csv data processing and reduce runtimes from weeks to hours.', u'STATS & METHODS TEACHING\nUT Austin - Austin, TX\nJanuary 2012 to May 2017\n\u2022 Personally selected by Professors in several departments for expertise to assistant teach stats, programming, and methods for 6 semesters \x03(4 for Ph.D. students, and 2 for undergrads).\n\u2022 Designed stats programming labs for Stata, still in use by Sociology Department.', u'DATA/STATS RESEARCH & CONSULTING\nUT Austin - Austin, TX\nJanuary 2011 to May 2017\n\u2022 Hired for data/programming skills by 4 Professors from different departments during 13 semesters as a Research Assistant.\n\u2022 Built 6 datasets with 100s of GB of data from many data types (structured, unstructured, spatial, network, demographic, temporal sources, etc.)\n\u2022 Fitted and interpreted complex models (network analysis, multi-level/mixed-effects regressions, and survival analysis, etc.).\n\u2022 Wrote R and Stata functions to perform custom calculations (e.g. a genetic algorithm to estimate network core-periphery position of nodes).\n\u2022 Developed a new method for diagnosing and imputing missing data in time-series, network data, using multi-level and Bayesian prediction.\n\u2022 Designed/programmed replicable data visualizations (2D, time-series, network, and maps).', u'PH.D. & DISSERTATION RESEARCH\nUT Austin - Austin, TX\nAugust 2010 to May 2017\n\u2022 Built unique dataset by (1) geocoding map data, (2) reducing data of over 150gb, (3) government data in Arabic and French.\n\u2022 Predicted protest and violence during the Tunisian Revolution of 2010-11, with temporal and spatial effects, using time series and survival models.\n\u2022 Wrote up and presented methods/results for non-technical audiences, leading to 4 published, peer-reviewed journal articles.\n\u2022 Collaborated with experts in various fields.\n\u2022 Frequently consulted on data in the department.', u'PROGRAM & POLICY ADVISING\nUnited Nations Development Programme (UNDP) - New York, NY\nMay 2005 to July 2010\n\u2022 Policy & program advisor to high-level UN officials working on political/economic development in Arab countries. Based in Beirut and New York.\n\u2022 Strategic planning & program management.']","[u'Ph.D. in Sociology (Politics, Statistical Methods)', u'Fulbright Fellowship in Arab Language and Literature', u'M.A., Cum Laude in Arab Studies (Political Science)', u'B.A., Magna Cum Laude in French, Spanish, Religious Studies']","[u'The University of Texas at Austin Austin, TX\nAugust 2010 to May 2017', u""L'Institut Fran\xe7ais du Proche-Orient Damascus, MD\nJanuary 2004 to January 2005"", u'Georgetown University Washington, DC\nJanuary 2002 to January 2004', u'New York University New York, NY\nJanuary 1998 to January 2002']","degree_1 : Ph.D. in Sociology (Politics, degree_2 :  Statistical Methods), degree_3 :  Flbright Fellowship in Arab Langage and Literatre, degree_4 :  M.A., degree_5 :  Cm Lade in Arab Stdies (Political Science), degree_6 :  B.A., degree_7 :  Magna Cm Lade in French, degree_8 :  Spanish, degree_9 :  Religios Stdies"
0,https://resumes.indeed.com/resume/5e4377a0ca6ef901,"[u""Data Analytic/ Preparation/Scientist\nSupreme Airport Shuttle LLC - Gaithersburg, MD\nJune 2016 to January 2018\nDUTIES/SKILLS/ABILITIES:\n\u2022 Experienced in tableau using multiple measures like individual Axes, Blended Axes, and Dual Axes for creating worksheet and dashboards.\n\u2022 Experienced in creating and developing operational Tableau dashboard for monitoring Tableau server performance and user activities.\n\u2022 Designed dashboard using filters, parameters and action filters as well as produce standard monthly reports using tableau. Connected tableau server with share-point portal and setup auto refresh features.\n\u2022 Building and publishing customized interactive reports and dashboards, report scheduling using tableau server.\n\u2022 In-depth experienced/ knowledge in Tableau advance analysis features like Actions, Calculations, forecasting, Parameters and Trendlines.\n\n\u2022 Participated in various meetings, reviews, and user group discussion as well as communicating with stakeholders and business groups.\n\u2022 Developed tableau workbooks from multiple data sources using joins and data blending.\n\u2022 Used Pivot tables to create reports that did dicing and slicing functions on data. This is called dimensional analysis and gives business a 360 view of what is going on.\n\u2022 Proficient in creating Calculations (LOD, Cube Calculations, Hide Columns, Creating/using Parameters, Table Calculations, Totals) and Formatting (Annotations, Layout Containers, Mark Labels, Mar Card Changes, Rich Text Formatting, Shared Encoding) using Tableau Desktop.\n\u2022 Excellent to automate error handling in SSIS.\n\u2022 Used conditional split to created powerful project in SSIS.\n\n\u2022 A handy trick to find anomalies in SQL.\n\u2022 Expert on how to do when a source file is corrupted.\n\n\u2022 Excellent using NotePad ++ with two files side-by-side.\n\n\u2022 Strong ability to write SQL queries using PROC SQL.\n\u2022 Knowledge of Software Development Life Cycle, Quality Assurance, Waterfall and Agile (Scrum).\n\nSTATISTICAL MODELING DATA ANALYTICS TOOLS PROJECTS:\n\u2022 Performed data entry and analysis using SPSS, R, SAS, STATA, Excel and GNU Regression Econometrics and Time-Series Library (GRETL).\n\n\u2022 Developed data visualization techniques (graphs, charts, scatter plots, structural equation modeling). Matrix/ Linear Algebra methods, ANOVA, MANOVA, Correlation, Linear Regression, Multiple Regression, Chi-square, Chronbach 's Alpha, post hoc assessments, the Time Series method, Bayesian\nmethods, and other methodologies.\n\nDEVELOPING QUANTITATIVE DATA ANALYSIS TOOLS AND STATISTICAL SOFTWARE PACKAGE:\n\n\u2022 Analyzed findings and made recommendations on substantive operating programs.\n\u2022 Applied knowledge of qualitative and quantitative analytical techniques along with knowledge of the mission, organization, and work processes of programs and the relationships of administrative support activities.\n\u2022 Provided independent and objective analysis and evaluation of human and financial resources utilization and program performance."", u""Data Analytic/ Modeler/ Scientist\nChardonnay Dialysis Inc - Jessup, MD\nJanuary 2009 to August 2014\nDUTIES/SKILLS/ABILITIES:\n\u2022 Experience in using Tableau server commands like synchronization, and Tableau server backup activity.\n\u2022 Experienced in creating various views in Tableau (Treemaps, Heat Maps, Scatterplot).\nExperience in creating Filters, quick filters, table calculations, calculated measures, and parameters.\n\n\u2022 Experienced in enormous data cleansings (Raw, ETL, Database, Analyze, Visualize Insights).\n\u2022 Hands-on building groups, hierarchies, set to create detail level summary reports and dashboards using KPI's.\n\n\u2022 Expert in creating Animation in Tableau.\n\u2022 Strong ability to visualized an ad-hoc A-B test in the tableau.\n\n\u2022 Expert validating Tableau data mining with a Chi-Square.\n\u2022 Experienced using Quality Assurance for Table Calculations.\n\u2022 Hands-on experienced on Backward, Forward, Bidirectional, and All possible models.\n\n\u2022 Expert using Adjusted R-square in modeling.\n\u2022 Experienced interpreting Coefficients of a Multiple Linear Regression (MLS) using GNU Regression Econometrics and Time-Series Library (GRETL) Tool, and SAS.\n\u2022 An expert on the Backward step-by-step built model.\n\n\u2022 Expertise in the concepts of Data Warehousing, Data Marts, ER Modeling, Dimensional Modeling, Fact and Dimensional Tables.\n\u2022 Practical understanding of the Data modeling (Dimensional & Relational) concepts like Star-Schema Modeling, Snowflake Schema Modeling, Fact and Dimension tables.\n\u2022 Experienced in Data mapping, Data transformation between sources to target data models.\n\u2022 Extracted data from existing data stores, Developing and executing departmental reports for performance and response purposes by using Oracle SQL, SAS, procedures, packages, functions, database triggers.\n\u2022 Excellent analytical skills for understanding the business requirements, business rules, business process and detailed design of the application.\n\u2022 Extensive experience in System Development Life Cycle (SDLC) involved in all stages of development including elicitation, analysis, specification, design, implementation, testing and deployment of applications.\n\u2022 Extensive experience using Microsoft SQL Server, Microsoft Visual Studio (MSVS) Shell, SSDT-BI, SSIS, SSAS, SSRS, MS-Word, MS-Excel, MS-PowerPoint, MS-Visio and Requisite Pro.\n\u2022 Experienced in Extraction, Transformation, and Loading (ETL) of data directly from different source systems like flat files, Excel, Oracle and SQL Server.\n\nSTATISTICAL MODELING DATA ANALYTICS TOOLS PROJECTS:\n\u2022 Performed data entry and analysis using SPSS, SAS, R, STATA, Excel and GNU Regression Econometrics and Time-Series Library (GRETL).\n\n\u2022 Developed data visualization techniques (graphs, charts, scatter plots, structural equation modeling). Matrix/ Linear Algebra methods, ANOVA, MANOVA, Correlation, Linear Regression, Multiple Regression, Chi-square, Chronbach 's Alpha, post hoc assessments, the Time Series method, Bayesian methods, and other methodologies.\n\nDEVELOPING QUANTITATIVE DATA ANALYSIS TOOLS AND STATISTICAL SOFTWARE PACKAGE:\n\n\u2022 Analyzed findings and made recommendations on substantive operating programs.\n\u2022 Applied knowledge of qualitative and quantitative analytical techniques along with knowledge of the mission, organization, and work processes of programs and the relationships of administrative support activities.\n\u2022 Provided independent and objective analysis and evaluation of human and financial resources utilization and program performance."", u""Data Analytic/Preparation/ Scientist\nFresenius Medical Care (FMC)\nAugust 2006 to September 2013\nDUTIES/ SKILLS / ABILITIES:\n\n\u2022 Created dummies for categorical independent variables and to avoid the dummy variable trap.\n\u2022 Odd Ratio and interpreting coefficients of a logistic regression.\n\u2022 Why model deteriorates and how to maintained using Assess, Retrain, Rebuild (ARR) methodology.\n\n\u2022 Build a Cumulative Accuracy Profile (CAP) curve in excel and how to avoid overfitting\n\nmodel and how to verified model via test data.\n\u2022 Checked for multicollinearity using Variance Inflation Factors (VIFs) and read the correlation matrix.\n\n\u2022 Hands-on building a practical real Geodemographic segmentation model.\n\u2022 Extensive hands-on experienced tableau parameters, heat maps, tree maps, table calculations, histograms, waterfalls, trend lines, geographic maps etc.\n\u2022 Created SAS code to be used by the team members to export and import data in and out of SAS and SQL Server.\n\u2022 Experienced in data analysis and ETL techniques for loading a large amount of data and finding anomalies in data and corrected them.\n\u2022 Strong dashboard design experience and passionate practitioner of effective data visualization.\n\u2022 Correctly open files in excel via TEXT IMPORT WIZARD.\n\u2022 Experienced handling data truncation in SQL Server Integration Services (SSIS).\n\u2022 Expert using Text Qualifiers in SSIS from SQL Server Data Tool- Business Intelligence (SSDT-BI).\n\u2022 Strong ability in developing SQL queries to extract, manipulate, and/or calculate information to fulfill data and reporting requirements including identifying the tables and columns from which data is extracted.\n\n\u2022 Performed Detailed Data Analysis, Data Quality Analysis and Data Profiling on source data.\n\u2022 Interaction with Client manager/team members on weekly basis.\n\n\u2022 Participated in various meetings, reviews, and user group discussion as well as communicating with stakeholders and business groups.\n\u2022 Advocated the use of Tableau solution and help educate business users to facilitate early adoption of the tool.\n\u2022 Worked closely with Business Analyst, Data Modeler, and all team within the organization to understand business processes.\n\nSTATISTICAL MODELING DATA ANALYTICS TOOLS PROJECTS:\n\u2022 Performed data entry and analysis using SPSS, SAS, R, STATA, Excel and GNU Regression Econometrics and Time-Series Library (GRETL).\n\n\u2022 Used specialized methods such as sampling, statistics, and economic forecasting to gather data.\n\u2022 Tools of analysis included supply and demand, cost-benefit, labor market, Investment/Saving (ISLM) equilibrium, inter-temporal external balance methods, first and second order condition optimization, and Lagrangian optimization techniques. End analysis comprised game theory, economic forecasting, regression analysis, Ordinary Least Squares (OLS) methods, Mini- Tab, Matlab and SAS statistical program.\n\n\u2022 Developed data visualization techniques (graphs, charts, scatter plots, structural equation modeling). Matrix/ Linear Algebra methods, ANOVA, MANOVA, Correlation, Linear Regression, Multiple Regression, Chi-square, Chronbach 's Alpha, post hoc assessments, the Time Series method, Bayesian methods, and other methodologies.\n\nLEAD PROCESS IMPROVEMENTS DATA ANALYSIS PROJECTS:\n\u2022 Implemented data process improvement methods that significantly reduced project cost, lowered staff resource requirements, and thus improved the project management process and performance results.\n\u2022 Projects under my control included supervising the work of professional and technical personnel assigned to projects.\n\u2022 Completed project scope, schedules, and cost estimates, providing professional expertise on technical areas and changes.\n\u2022 The result of this project included identification and completion of several data analysis projects, achieving a $200k cost savings.""]","[u'Master of Science in Data Statistical Project Management', u'Bachelor of Science in Aeronautics']","[u'Embry-Riddle Aeronautical University Daytona Beach, FL\nOctober 2016', u'Embry-Riddle Aeronautical University Daytona Beach, FL\nJanuary 2014']","degree_1 : Master of Science in Data Statistical Project Management, degree_2 :  Bachelor of Science in Aeronatics"
0,https://resumes.indeed.com/resume/6474ae7658169d15,"[u""Data Scientist\nNEXTHEALTH TECHNOLOGIES\nSeptember 2016 to Present\nIdentifying key insights to impact avoidable ER visits\n\u2022 Owner of ETL framework for wrangling & feature selection of 1.2 million claims across multiple lines of payer\n\u2022 Implemented Random Forest algorithm to generate impactable clusters for high cost avoidable ER visits\n\u2022 Proved causality of outreach programs with statistical significance of P-value < 0.05 and lift of $126 PMPY\nTrack Gaps - in - Care of patients & Develop Propensity models for compliance\n\u2022 Conceptualized & implemented Python framework tracking Gaps in EPSDT services for Medicaid population\n\u2022 Identified key variables & implemented Propensity models for targeting highly compliant candidates\n\u2022 Increased compliance ratio from 69% to 74% resulting in savings of $300000 within 2 quarters of deployment\nDescriptive Analysis & Visualization of Program Performance in Tableau Reporting Dashboard\n\u2022 Owner of backend Data models in PostgreSQL for Platform Business Intelligence Dashboard\n\u2022 Automated SQL queries and Python jobs for BI extracts for all clients' Measurement Programs\n\u2022 Time taken for Results to Dashboard decreased by 16 hours resulting in increased Platform adoption"", u""Word Frequency\nFebruary 2016 to February 2016\n\u2022 Split each line of text file in space to map each word to a key value pair and reduce number by keys sum-up\n\u2022 Swap Key value with Count to find frequent words and deduce hypothesis about sentiment using tf-idf statistic\nClassification of Patients diagnosed with Breast Cancer using Support Vector Machine classifier Oct'15\n\u2022 Modelled a SVM classifier in a quadratic formulation for needle aspirate data from human breast tissue\n\u2022 Maximized hyperplane objective in feature space to obtain optimal subset of kernel parameters for SVM"", u""Data Science Intern - Predictive Analytics\nSUTTER HEALTH\nJune 2015 to August 2015\nRisk stratification and readmissions prediction of Accountable Care Organizations patients\n\u2022 Univariate statistical data exploration using R, HANA, to visualize patient, demographic & datasets\n\u2022 Utilized logistic regression, neural network, decision trees & clustering algorithms to model risk of readmission\n\u2022 Evaluated & recommended predictive models based on desired F1 score, specificity & robustness\nExploratory prediction of asthma patients' arrival and Parkinson's disease characteristics visualization\n\u2022 Combined data from EPA (AQI), FEMA (wildfires) and Epic (EHR) to conduct root cause analysis\n\u2022 Combined Twitter (asthma related tweets) and Epic (EHR) data to predict patient arrival with 85% accuracy\n\u2022 Visualizations of characteristics of Parkinson's disease patients using MicroStrategy web UI from Hadoop cluster"", u""Software Developer Analyst\nACCENTURE\nJuly 2012 to December 2013\nAccenture Insight for Enterprise Systems\n\u2022 Developed SAP metadata based intelligent analytics portal to reduce FTE resource requirement by 75%\n\u2022 Number of clients increased by 10% within 3 months of deployment of application\nACADEMIC/INDEPENDENT PROJECTS\nAnalyzing Global sales of movies and its correlation with feature set Feb' 18\n\u2022 Utilized Tableau to wrangle and visualize relationship between global sales and key feature variables\n\u2022 Established deep insights with strong correlation between target variable & critic count, user score and critic score\n\u2022 Predicted future global sales on cross validation dataset using regression models in Python with mean accuracy of 75%""]",[],[],degree_1 : 
0,https://resumes.indeed.com/resume/039c9848f43ec0df,"[u'Data Scientist\nFirst Tennessee bank - Memphis, TN\nJanuary 2017 to Present\nDescription:\nFirst Tennessee Bank is a regional bank and financial services firm based out of Memphis, Tennessee, and operates as a subsidiary of the First Horizon National Corporation. The bank offers personal, business and corporate banking, and private client services to families and businesses statewide.\n\nResponsibilities:\n\u2022 Setup storage and data analysis tools in Amazon Webservices cloud computing infrastructure.\n\u2022 Used pandas, NumPy, Seaborn, SciPy, matplotlib, sci-kit-learn, NLTK in Python for developing various machine learning algorithms.\n\u2022 Installed and used CaffeDeep Learning Framework\n\u2022 Worked on different data formats such as JSON, XML and performed machine learning algorithms in Python.\n\u2022 Worked as Data Architects and IT Architects to understand the movement of data and its storage and ER Studio 9.7\n\u2022 Participated in all phases of datamining; data collection, data cleaning, developing models, validation, visualization and performed Gap analysis.\n\u2022 Data Manipulation and Aggregation froma different source using Nexus, Toad, Business Objects, PowerBL and SmartView.\n\u2022 Implemented Agile Methodology for building an internal application.\n\u2022 Focus on integration overlap and Informatica newer commitment to MDM with the acquisition of Identity Systems.\n\u2022 Good knowledge of Hadoop Architecture and various components such as HDFS, Job Tracker, Task Tracker, Name Node, Data Node, SecondaryNameNode, and MapReduce concepts.\n\u2022 As Architect delivered various complex OLAPdatabases/cubes, scorecards, dashboards and reports.\n\u2022 Programmed a utility in Python that used multiple packages (SciPy, NumPy, pandas)\n\u2022 Implemented Classification using supervised algorithms like Logistic Regression, Decision trees, KNN, Naive Bayes.\n\u2022 Updated Python scripts to match training data with our database stored in AWS Cloud Search, so that we would be able to assign each document a response label for further classification.\n\u2022 Data transformation from various resources, data organization, features extraction from raw and stored.\n\u2022 Handled importing data from various data sources, performed transformations using Hive, MapReduce, and loaded data into HDFS.\n\u2022 Interaction with Business Analyst, SMEs, and other Data Architects to understand Business needs and functionality for various project solutions\n\u2022 Researched, evaluated, architected, and deployed new tools, frameworks, and patterns to built sustainable Big Data platforms for the clients\n\u2022 Identifying and executing process improvements, hands-on in various technologies such as Oracle, Informatica, Business Objects.\n\u2022 Designed both 3NF data models for ODS, OLTP systems and dimensional data models using Star and Snowflake Schemas.\nEnvironment:R 9.0, Informatica 9.0, ODS, OLTP, Bigdata, Oracle 10g, Hive, OLAP, DB2, Metadata, Python, MS Excel, Mainframes MS Vision, Rational Rose.', u""Data Scientist\nWalmart - Bentonville, AR\nNovember 2015 to December 2016\nDescription:\nWalmart Inc., formerly Wal-Mart Stores, Inc., incorporated on October 31, 1969, is engaged in the operation of retail, wholesale and other units in various formats around the world. The Company offers an assortment of merchandise and services at everyday low prices. The Company operates through three segments: Walmart U.S., Walmart International and Sam's Club.\nResponsibilities:\n\n\u2022 Analyzed the business requirements of the project by studying the Business Requirements Specification document.\n\u2022 Extensively worked on Data Modeling tools Erwin Data Modeler to design the data models.\n\u2022 Designeda mapping to process the incremental changes that exist in the source table. Whenever source data elements were missing in source tables, these were modified/added inconsistency with third normal form based OLTP source database.\n\u2022 Worked in Statistical Modeling and Machine Learning techniques (Linear, Logistics, Decision Trees, Random Forest, SVM, K-Nearest Neighbors, Bayesian, XGBoost) in Forecasting/ Predictive Analytics, Segmentation methodologies, Regression-basedmodels, Hypothesis testing, Factor analysis/ PCA, Ensembles.\n\u2022 Designed tables and implemented the naming conventions for Logical and PhysicalData Models in Erwin7.0.\n\u2022 Participated inthe conversion of ITS (Immigration Tracking System) Visual Basic client-server application into C#, ASP.NET3-tierIntranet application.\n\u2022 Performed Exploratory Data Analysis and Data Visualizations using R, and Tableau.\n\u2022 Perform a proper EDA, Uni-variate and bi-variate analysis to understand the intrinsic effect/combined effects.\n\u2022 Worked with Data Governance, Data quality, data lineage, Data architect to design various models and processes.\n\u2022 Independently coded new programs and designed Tables to load and test the program effectively for the given POC's using with Big Data/Hadoop.\n\u2022 Designed data models and data flow diagrams using Erwin and MS Visio.\n\u2022 Utilized ADO.Net Object Model to implement middle-tier components that interacted with MSSQL Server 2000database.\n\u2022 Participated in AMS (AlertManagementSystem) JAVA and SYBASE project. Designed SYBASE database utilizing ERWIN. Customized error messages utilizing SP_ADDMESSAGE and SP_BINDMSG. Created indexes, made query optimizations. Wrote stored procedures, triggers utilizing T-SQL.\n\u2022 Explained the data model to the other members of thedevelopment team. Wrote XML parsing module that populates alerts from theXML file into the database tables utilizing JAVA, JDBC, BEAWEBLOGICIDEand DocumentObject Model.\n\u2022 As an Architect implemented MDMhub to provide clean, consistent data for anSOA implementation.\n\u2022 Developed, Implemented & Maintained the Conceptual, Logical&PhysicalData Models using Erwin for forwarding/Reverse Engineered Databases.\n\u2022 Explored and Extracted data from source XML in HDFS, preparing data for exploratory analysis using data mining.\n\nEnvironment:SQL Server 2008R2/2005 Enterprise, SSRS, SSIS, Crystal Reports, Hadoop, Windows Enterprise Server 2000, DTS, Bigdata, Python, machine learning, SQL Profiler, and Query Analyzer."", u""Data Scientist\nFord motors - Dearborn, MI\nJanuary 2014 to October 2015\nDescription:Mortgage services tailored to you. Whether you're looking to purchase your first home, tackle that kitchen renovation project or save money on your current mortgage, the experts at Valley will guide you to a solution that fits your needs.\n\nResponsibilities:\n\u2022 Supported MapReduce Programs running on clusters.\n\u2022 Evaluated business requirements and prepared detailed specifications that follow project guidelines required to develop written programs.\n\u2022 Configured Hadoop cluster with Name node and slaves and formatted HDFS.\n\u2022 Used Oozie workflow engine to run multiple Hive and Pig jobs.\n\u2022 Performed MapReduce Programs those are running on the cluster.\n\u2022 Developed multiple MapReduce jobs for data cleaning and preprocessing.\n\u2022 Analyzed the partitioned and bucketed data and compute various metrics for reporting.\n\u2022 Involved in loading data from RDBMS and web logs into HDFS using Sqoop and Flume.\n\u2022 Developed Hive queries for Analysis across different banners.\n\u2022 Extracted data from Twitter using Java and Twitter API. Parsed JSON formatted twitter data and uploaded to database.\n\u2022 Launching AmazonEC2 Cloud Instances using Amazon Images (Linux) and Configuring launched instances with respect to specific applications.\n\u2022 Exported the result set from Hive to MySQL using Sqoop after processing the data.\n\u2022 Analyzed the data by performing Hive queries and running Pigscripts to study customer behavior.\n\u2022 Used Hive to partition and bucketdata.\n\u2022 Wrote PigScripts to perform ETL procedures on the data in HDFS.\n\u2022 Created HBase tables to store various data formats of data coming from different portfolios.\n\u2022 Worked on improving performance of existing Hive Queries.\n\nEnvironment:Linux, Hadoop, Python, MapReduce, HDFS, Python, GIT, Hive, SQL, Pig, Scoop, Flume, AWS, EC2, Twitter API, Oozie"", u""Data Analyst\nValley National Bank - Wayne, NJ\nOctober 2012 to December 2013\nDescription:Valley National Bancorp is a regional bank holding company headquartered in Wayne, New Jersey. Its principal subsidiary, Valley National Bank, operates 209 branches in 30 counties in northern and central New Jersey, Manhattan, Brooklyn, Queens, Long Island and Florida\n\nResponsibilities:\n\u2022 Developed complex SQL queries using group by, join, where clause to answer tester questions.\n\u2022 Communicated and coordinated with other departments to collect Business Requirement Analysis and developed data flow mapping to load OLAP server for analysis of policies details.\n\u2022 Worked on missing value imputation, outlier's identification using Random Forest and Box Plots.\n\u2022 Tackled highly imbalanced dataset using under sampling with ensemble methods, oversampling and cost sensitive algorithms.\n\u2022 Improved prediction performance by using random forest and gradient boosting for feature selection with the help of Scikit-learn library in Python.\n\u2022 Utilized Parametric and Non-Parametric test in SPSS to draw insights from data for making business decisions.\n\u2022 Implemented machine learning models (logistic regression, XGboost) with the help of Scikit-learn in Python.\n\u2022 Validated and selected models using k-fold cross validation, confusion matrices and worked on optimizing models for high recall rate.\n\u2022 Involved with data profiling for multiple sources and answered complex business questions by providing data to business users.\n\u2022 Participated in Agile planning process and daily scrums, providing details and discuss with team lead, Data Scientist, Data Analyst, Data Engineer and others.\n\u2022 Experience with routine DBA activities like Query Optimization, Performance Tuning and Effective SQL Server configuration for better performance and cost reduction.\n\u2022 Developed Tabular Reports, Sub Reports, Matrix Reports, drill down Reports and Charts using SQL Server Reporting Services (SSRS).\n\u2022 Designed rich data visualizations with Tableau 9.4 and dynamic dashboards for business analysis.\n\nEnvironment:SQL Server 2012, R programming, Python, MATLAB, SSRS, SSIS, SSAS, SPSS, Tableau, Minitab, Microsoft office, SQL Server Management Studio, Business Intelligence Development Studio, MS Access."", u""Data Analyst\nData wise - Hyderabad, Telangana\nJanuary 2011 to September 2012\nResponsibilities:\n\u2022 Data analysis and reporting using MySQL, MS Power Point, MS Access and SQL assistant.\n\u2022 Involved in MySQL, MS Power Point, MS Access Database design and design new database on Netezza which will have optimized outcome.\n\u2022 Involved in writing T-SQL, working on SSIS, SSRS, SSAS, Data Cleansing, Data Scrubbing and Data Migration.\n\u2022 Involved in writing scripts for loading data to target data Warehouse using Bteq, Fast Load, Multiload\n\u2022 Create ETL scripts using Regular Expressions and custom tools (Informatica, Pentaho, and Sync Sort) to ETL data.\n\u2022 Developed SQL Service Broker to flow and sync of data from MS-I to Microsoft's master database management (MDM).\n\u2022 Involved in loading data between Netezza tables using NZSQL utility.\n\u2022 Worked on Data modeling using Dimensional Data Modeling, Star Schema/Snow Flake schema, and Fact& Dimensional, Physical & Logical data modeling.\n\u2022 Generated Stats pack/AWR reports from Oracle database and analyzed the reports for Oracle wait events, time consuming SQL queries, table space growth, and database growth.\n\nEnvironment:MySQL, MS Power Point, MS Access, MY SQL, MS Power Point, MS Access, Netezza, DB2, T-SQL, DTS, SSIS, SSRS, SSAS, ETL, MDM, Teradata, Oracle, Star Schema and Snow Flake Schema."", u'Data Analyst\nKarvy Analytics - Hyderabad, Telangana\nJune 2009 to December 2010\nResponsibilities:\n\u2022 Worked with internal architects, assisting in the development of current and target state data architectures.\n\u2022 Worked with project team representatives to ensure that logical and physical ER/Studio data models were developed in line with corporate standards and guidelines.\n\u2022 Involved in defining the business/transformation rules applied for sales and service data.\n\u2022 Implementation of Metadata Repository, Transformations, Maintaining Data Quality, Data Standards, Data Governance program, Scripts, Stored Procedures, triggers and execution of test plans\n\u2022 Define the list codes and code conversions between the source systems and the data mart.\n\u2022 Involved in defining the source to business rules, target data mappings, data definitions.\n\u2022 Responsible for defining the key identifiers for each mapping/interface.\n\u2022 Remain knowledgeable in all areas of business operations in order to identify systems needs and requirements.\n\u2022 Responsible for defining the key identifiers for each mapping/interface.\n\u2022 Performed data quality in Talend Open Studio.\n\u2022 Enterprise Metadata Library with any changes or updates.\n\u2022 Document data quality and traceability documents for each source interface.\n\u2022 Establish standards of procedures.\n\u2022 Coordinated meetings with vendors to define requirements and system interaction agreement documentation between client and vendor system.\n\nEnvironment:Windows Enterprise Server 2000, SSRS, SSIS, Crystal Reports, DTS, SQL Profiler, and Query Analyze.']","[u""Bachelor's""]",[u''],"degree_1 : ""Bachelors"""
0,https://resumes.indeed.com/resume/77dbb13d4600d50b,"[u""Data Scientist\nRR Donnelley - Bolingbrook, IL\nNovember 2015 to Present\n\u2022 Working on the various projects with the Company's Logistics Group in designing and implementing mathematical and predictive models to address business problems and simplify decision making process. Utilizing the agile/scrum methodology for all the projects. Rating Engine - Built a machine learning model using Random Forest Regression Algorithm to predict the cost of future shipments using the historical truck load data.\n\u2022 Currently Involved in the development and continuous improvement of the predictive model with the goal of giving highly competitive rates to the customers with optimized margin for the company.\n\u2022 Testing various analytics solutions using statistical modeling tools like R, SQL and Python.\n\u2022 Utilizing Tableau for data visualization to ensure that data analysis is easily consumable by the business and clearly conveys insights to colleagues within the Analytics group.\n\u2022 Actively monitoring and evaluating the impact and effectiveness of predictive model in production by generating reports using IRKernel package in R via Jupyter Notebooks. Carrier Assignment Model\n\u2022 Load Planning Optimization Model - Building a Mathematical Model to create optimized load Plans both in the deterministic and stochastic networks across business units, locations and customers using R and Python, gathering business requirements by shadowing and interviewing national load planning team, Analyzing the results and efficiency of currently used optimization tool.\n\u2022 Worked closely with the Company's Logistics Group for designing and implementing processes and layouts for data sets used for modelling, data mining and research."", u'Statistician\nAmerican College of Surgeons\nSeptember 2010 to January 2015\n\u2022 Programming in SAS and SQL to extract data from Oracle tables and create comprehensive datasets\n\u2022 Merging and managing large datasets to create analytic datasets\n\u2022 Produced tables, listings and figures for safety and efficacy analysis\n\u2022 Monitored internal project timelines to ensure that deadlines for deliverables were met\n\u2022 Reviewed programming outputs to ensure compliance with the Statistical Analysis Plan and study requirements\n\u2022 Documentation of more complicated programs\n\u2022 Generate descriptive and bivariate statistical tables and reports\n\u2022 Conduct multivariate analyses including but not limited to logistic regression, survival analysis, multilevel modeling, longitudinal analysis, sensitivity and specificity analysis, ROC analysis and modeling\n\u2022 Understanding of health care data, both in general and specifically for TQIP/NTDB\n\u2022 Working with other divisions of NTDB to complete specific statistical table requests\n\u2022 Wrote and debugged SAS programs to generate tables, listings and graphs in support of Trauma Quality Improvement Program.\n\u2022 Transferred survey data from paper into computer files for analysis\n\u2022 Performed regular backups to ensure data preservation']","[u'Master of Sciences in Applied Statistics', u'MBA in Accounting and Finance']","[u'Loyola University Graduate School of Mathematics and Statistics Chicago, IL\nJanuary 2010', u'University of Benin Lome, TG\nJanuary 1996']","degree_1 : Master of Sciences in Applied Statistics, degree_2 :  MBA in Acconting and Finance"
0,https://resumes.indeed.com/resume/d1c4260c815594fb,"[u""Sr. Data Scientist\nAmerican Express - Atlanta, GA\nJanuary 2015 to Present\nAmerican Express, Atlanta, GA, USA\nSr. Data Scientist Jan 2015 - Present\nResponsibilities\n\u2022 Designed, built, and deployed Machine Learning algorithms including Classification, Regression, Clustering, and Collaborative Filtering for prediction and identifying patterns and correlations for competitive intelligence and advanced analytics using Python / 'R'/Spark MLLib.\n\u2022 Implemented a Random Forest model for predicting Financial Services' Upsell opportunities\n\u2022 Discovered digital marketing opportunities using Python-Recsys recommender\n\u2022 Developed Mortgage sales forecasting Predictive Model using a combination of Time-Series analysis and Regression\n\u2022 Prototyped a Market Basket Analysis for identifying Cross-sell and Up-sell opportunities\n\u2022 Implemented Graph Algorithms for clickstream analysis for Digital Marketing teams to generate sales-leads by inferring purchase intent from browsing behavior\n\u2022 Developed an NLP-based financial products Recommendation Engine using Latent Semantic Analysis\n\u2022 Performed exploratory data analyses and visualizations on multivariate data sets.\n\u2022 Worked with neural network libraries such as TensorFlow and Caffe.\n\u2022 Actively involved in sentiment analysis, text mining and unstructured data parsing.\n\u2022 Utilized Python libraries such as 'numpy', 'scipy', 'pandas', 'matploitlib' and 'scikit-learn', 'Keras', 'TensorFlow', 'Theano', 'NLTK' to implement the data mining and machine learning tasks.\n\u2022 Utilized R Packages such as dplyr , missForest, mice, class, gmodels, tm, e1071, c50, Rweka, stats, rpart, neuralnet, kernlab, arules, caret, forecast, xgboost, sqldf, nloptr, lpSolve, ggplot2 , CaffR\n\u2022 Imported and exported data using Sqoop from HDFS to RDBMS and vice-versa and developed ETL pipelines for structured data for different business units\n\u2022 Utilized Flume for ingesting large amounts of log data from sources\n\u2022 Architected solutions using Hive to process structured data in Hadoop for querying and analyzing data using HQL\n\u2022 Utilized Hbase for storing huge data on top of HDFS and access random real time data in HDFS\n\u2022 Utilized Spark for processing data from the local file system, HDFS, Amazon S3, Relational and NoSQL databases and using Spark SQL, Import data into RDD and Ingesting data from a range of sources using Spark Streaming\n\nStar Accomplishments/Key Contributions\n\u2022 Sentiment analysis and visualizations to look for patterns and correlations in social media data including tweets, blog posts and website logs helped steer decisions to provide long-term business value, and cost-effectiveness that would save the enterprise millions of dollars per year.\n\u2022 The data science and machine learning initiatives generated millions of dollars in revenues for the company."", u'Data Scientist/Data Architect\nCitigroup - Irving, TX\nAugust 2012 to December 2014\nCitigroup, Irving, TX, USA\nData Scientist/Data Architect Aug 2012 - Dec 2014\nResponsibilities\n\u2022 Implemented a Na\xefve Bayesian predictor for demography details of Global Consumer Group customers\n\u2022 Developed a Neural Network model to analyze huge volumes of data and predict fraud detection and alert the customer of alleged fraud case.\n\u2022 Using Random Forest, SVM and xgBoost algorithms developed a root-cause analysis prototype tool\n\u2022 Implemented an Item-based Collaborative Filtering recommender for financial products\n\u2022 Developed Predictive Analytics for Churn Detection and boosting Customer Retention and estimating Life-time Value of users\n\u2022 Provided Data Architecture support for the Corporate wide Genesis Data Warehouse and Business Intelligence framework applied to the Deposits, Cards, Investments and Mortgage Portfolio, utilizing Ab Initio, Datastage, Terradata, and CA-Erwin technologies\n\u2022 Provided Architecture Governance, Data Standards/Governance, Big Data Analytics, and Cloud Strategy recommendations.\n\u2022 Key member of the Global Consumer Group Enterprise Information architecture providing business, data, and technology consulting and leadership for Cards, Mortgage and Consumer Bank IT portfolio.\n\u2022 Responsible for creating data analytics design charters that include architecture diagrams, flow diagrams, architecture assertions, specifications, rationale on the directions, data models, and navigation maps\n\u2022 Responsible for the data analytics and business intelligence current state gap analysis, future state architecture and roadmap recommendations for Global Functions and Citi Holdings\n\u2022 Guided standards development, COB/DR strategy, and governance.\n\u2022 Provided the data architecture support for implementing regulatory initiatives like FATCA\n\u2022 Interfaced with business and operations community and provide ongoing status.\n\nStar Accomplishments/Key Contributions\n\u2022 The data science and machine learning initiatives generated millions of dollars in revenues for the company.\n\u2022 Played an important role in the data warehousing, data analytics and data management redesign initiatives that would save the enterprise millions of dollars per year by leveraging reference architectures.', u'Data Architect/Data Scientist\nBank of America - Charlotte, NC\nSeptember 2010 to July 2012', u""Data Scientist/Data Architect\nBank of America - Charlotte, NC\nSeptember 2010 to July 2012\nResponsibilities\n\u2022 Created a Fraud Detection predictive model Matrix Factorization and Kmeans clustering\n\u2022 Developed a Random Forest predictive model for matching users with financial products\n\u2022 Implemented sentiment analysis algorithms using social media and online data and text\n\u2022 Aggregate location-specific requests to detect patterns, spot repetitive issues, and predict what's causing challenges for a particular online user.\n\u2022 Implemented dynamic price optimization using machine learning to help correlating pricing trends with sales trends by using an algorithm, along with other factors such as product management goals and cost to deliver/service the product or service.\n\u2022 Key member and responsible for the architecture of Big Data and Master Data Management based Future State Application and Infrastructure that maintains product, offers management, agreements management and pricing SORs.\n\u2022 Responsible for the enterprise and platform current state gap analysis, future state architecture and roadmap recommendations for Big Data Analytics and Master Data Management.\n\u2022 Architected application and data analytics solution supporting Deposits and Card -- Customer and Small Business Banking business unit.\n\nStar Accomplishments/Key Contributions\n\u2022 Saved the enterprise, millions of dollars through the data science and machine learning initiatives.\n\u2022 Strategic member of future state roadmap and data management/analytics initiatives that would save the company millions of dollars per month."", u'Solution/Enterprise Architect\nSouthern California Edison - Los Angeles, CA\nMay 2007 to August 2010', u'Enterprise/Solution Architect\nSouthern California Edison, An Edison Intl Company - Los Angeles, CA\nMay 2007 to August 2010\nResponsibilities\n\u2022 Directed and managed the solution architecture, analysis, design, development and delivery of\n\u21e8 Corporate wide incident case management system with an estimated 30,000+ end users.\n\u21e8 Corporate Injury/Incident Root Cause Application.\n\u21e8 Secure email communication initiative between Edison and Sedgwick.\n\u21e8 eTariff Filing Management system to comply with NERC and FERC regulations.\n\u21e8 Intranet Video Streaming Project.\n\u2022 Worked with the Architecture team to frame an Architecture vision, socialize it and get buy-in from senior IT and Business leaders.\n\u2022 Developed prototypes based on J2EE and Java based web services for case management system in accordance with system requirements and Use Cases.\n\u2022 Developed, maintained, and communicated processes and standard practices related to the design, development, and implementation of application solutions and patterns.\n\u2022 Fostered and encouraged solution reuse within business units and at the enterprise level.\n\u2022 Facilitated and enabled the integration of applications and business processes using common methods.\n\u2022 Focused on purposeful migration of application components and functionality from current state to future state within an application area, business unit or enterprise context.\n\u2022 Identified shared service candidates, qualified them and built buy-in to elevate them to shared service standards.\n\u2022 Led the ""Community of Practice"" for cross functional teams to discuss the practice and adoption of cloud, big data analytics and SOA.\n\u2022 Responsible for the development and maintenance of conceptual, logical, and physical application architecture, in the research and development, planning, deployment, and training of application technologies, and the retirement of obsolete applications.\n\u2022 Focused on development of functional and/or component solution to meet current and future needs.\n\u2022 Provided leadership in the logical and physical design of application systems for general business or specific technical functions, and ensured that design is consistent and well integrated with existing conceptual, logical, and physical application architectures, balancing operational optimization with resource use factors.\n\u2022 Led the business analysis and requirements gathering sessions.\n\u2022 Advised the alignment of the architect\'s output to the overall enterprise architecture and strategy.\n\nStar Accomplishments/Key Contributions\n\u2022 Architected the solutions with a portfolio that worth $10 million in budget per year.\n\u2022 Saved the enterprise, millions of dollars through efficient and effective solution/enterprise architecture and design.', u'Sr. Data/BI Analyst\nBank of America - Pasadena, CA\nMay 2005 to April 2007', u'Sr. Data / BI Analyst\nCountrywide Financials, A Bank of America Company - Pasadena, CA\nMay 2005 to April 2007\nResponsibilities\n\u2022 Worked independently and collaboratively throughout the complete analytics project lifecycle including data extraction/preparation, design and implementation of scalable data mining and machine learning analysis and solutions, and documentation of results.\n\u2022 Applied various machine learning algorithms and statistical modeling like decision trees, regression models, clustering, SVM to identify and predict mortgage loan volume using scikit-learn package in R and Python.\n\u2022 Performed extensive data cleansing and analysis, using pivot tables, formulas (V-lookup and others), data validation, conditional formatting, and graph and chart manipulation using excel.\n\u2022 Collaborated with technical and non-technical resources across the business to leverage their support and integrate our efforts.\n\u2022 Sourced training and testing of Data under data modelling process for machine learning algorithm implementation.\n\u2022 Created pivot tables and charts using worksheet data and external resources, modified pivot tables, sorted items and group data, and refreshed and formatted pivot tables.\n\u2022 Analyzed the Root cause in a code and incorporated changes in programs as cost-effective solution.\n\u2022 Worked on different data formats such as JSON, XML and implemented machine-learning algorithms in Python.\n\u2022 Directed the Analysis, Architecture, Design and Development of\n\u21e8 Complex staffing models.\n\u21e8 Databases for branch operations and compliance.\n\u2022 Architected, designed and led the implementation of enterprise wide data warehouse and business\n\u2022 intelligence solution both from a technical and functional standpoint in order to make automated data analytics solutions more suitable for C-level management decision support tools.\n\u2022 Built strong client relationship and maintained high-level customer satisfaction by managing expectations.\nStar Accomplishments/Key Contributions\n\u2022 Received accolades for efficiently and quickly securities data analytics initiatives to support of business functions and processes, a Six Sigma Gold Project that would save $500K per year to the company.', u'Sr. Application Developer\nBank of America - Louisville, KY\nMay 2003 to April 2005\nBank of America Corporation, Louisville, KY USA\nSr. Application Developer May 2003 - Apr 2005\nResponsibilities\n\u2022 Led the design and development of .NET based ETL process for SQL Server data mart.\n\u2022 Managed the Day-to-Day Database Administration (DBA) like backup, recovery, security, clustering, replication, disaster recovery and documenting SQL standards for the SQL server data marts.\n\u2022 Defined, reviewed and approved System-Design specifications, Process Models and Data-Flow Diagrams\n\u2022 Directed the development, architecture, design specifications, documenting architectural artifacts coding standards, new technologies and products.\n\u2022 Directed and managed the development of SSIS, SSAS, SQL Server DTS packages, T-SQL Stored Procedures, Views, Triggers and UDF.\n\u2022 Directed and managed the design and development of VB.NET/ASP.NET/SQL Server based Complaint Tracking System.\n\u2022 Streamlined and instituted the process, design, development, testing and implementation of transaction programs for LSBO Finance, routines for the LSBO Data Quality cleanup efforts, Private Investor Accounting, Data Quality & Integrity initiatives.\n\u2022 Utilized R and SQL to manipulate data, and develop and validate quantitative models.\n\u2022 Involved in analyzing system failures, identifying root causes, and recommended the course of actions. Documented the systems processes and procedures for future references.\n\u2022 Involved in testing and modelling the data in order to migrate it to production environment.\n\u2022 Implemented numerous codes in R and python to retrieve the data from a .csv or .xl file.\n\u2022 Analyzed data collected in stores (JCL jobs, stored-procedures and queries) and provided reports to the Business team by storing the data in excel/CVS file.\n\u2022 Used advanced Microsoft Excel functions such as Pivot tables and VLOOKUP in order to analyze the mortgage data.\n\u2022 Involved in Brainstorming sessions to propose hypothesis, approaches and techniques.\n\u2022 Performed various statistical tests and exploratory data analysis to ensure clear understanding of the business problem.\n\nStar Accomplishments/Key Contributions\n\u2022 Swiftly architected and redesigned the ad-hoc reporting system to a much scalable and robust data mart on-the-fly and reporting system and accounting automation process implementation that saved the company $1 million /month.\n\u2022 Productively delivered the tracking system saving at least 10 analyst days per month to the company.\n\u2022 Honored with the spirit reward and appreciation for identification of common functions to leverages synergies and sharing opportunities across the group and eliminate potential duplications.\n\u2022 Distinctly recognized by the Corporate Treasury for the excellent teamwork and significantly exceeding performance standards in the Data Quality and Integrity cleanup initiatives.', u'Software Engineer/Team Lead\nHumana Inc - Louisville, KY\nJune 2001 to May 2003\nHumana Inc., Louisville, KY, USA\nSoftware Engineer /Team Lead June 2001 - May 2003\nResponsibilities\n\u2022 Led a team of 4 FTE in Data Scalability Project and directed the project from inception to close.\n\u2022 Successfully implemented Service Fund Database Redesign, Vendor Number Modification.\n\u2022 Designed, Developed and Implemented Three Tier Pharmacy Copay Project, Service Fund Data mart\n\u2022 Optimized process and system performance through successful implementation of Claim Rules Database collapse, Contract/ Fund/ Membership Purges, provider data redesign and payment option database redesign.\nStar Accomplishments/Key Contributions\n\u2022 Largely contributed in scalability project that reduced run time of monthly jobs from 3 weeks to 10 days.\n\u2022 Affected millions of dollars savings by troubleshooting several database and process related production issues.\n\nData Science Skills\n\u2022 Exploratory Data Analysis: Univariate/Multivariate Outlier detection, Missing value imputation, Histograms/Density estimation, EDA in Tableau\n\u2022 Supervised Learning: Linear/Logistic Regression, Decision Trees, Ensemble Methods, Random Forests, Support Vector Machines, Gradient Boosting, Deep Neural Networks, Bayesian Learning\n\u2022 Unsupervised Learning: Principal Component Analysis, Association Rules, Factor Analysis, K-Means, Hierarchical Clustering, Gaussian Mixture Models, Market Basket Analysis, Collaborative Filtering and Low Rank Matrix Factorization\n\u2022 Feature Selection: Stepwise, Recursive Feature Elimination, Relative Importance, Filter Methods, Wrapper Methods and Embedded Methods\n\u2022 Statistical Tests: T Test, Chi-Square tests, Stationarity tests, Auto Correlation tests, Normality tests, Residual diagnostics, Partial dependence plots and Anova\n\u2022 Sampling Methods: Bootstrap sampling methods and Stratified sampling\n\u2022 Model Tuning/Selection: Cross Validation, Walk Forward Estimation, Grid Search and Regularization\n\u2022 Time Series: ARIMA, Exponential smoothing, Bayesian structural time series']","[u'PhD in Computer Science & Engineering', u'M.B.A in Information Systems', u'M.S in Computer Science', u'B.S in Computer Science & Engineering']","[u'Speed School of Engg, Univ. of Louisville, KY USA', u'College of Business, University of Louisville, KY USA', u'Speed School of Engg, University of Louisville, KY USA', u'Govt. College of Technology Coimbatore, Tamil Nadu']","degree_1 : PhD in Compter Science & Engineering, degree_2 :  M.B.A in Information Systems, degree_3 :  M.S in Compter Science, degree_4 :  B.S in Compter Science & Engineering"
0,https://resumes.indeed.com/resume/bdb6d525654a2eb3,"[u'Data Scientist\nNew York, NY\nMarch 2015 to Present']","[u'PhD. in Neuroscience, Computer Engineering, Computer Science']","[u'McGill University Montr\xe9al, QC']","degree_1 : PhD. in Neroscience, degree_2 :  Compter Engineering, degree_3 :  Compter Science"
0,https://resumes.indeed.com/resume/dceac5438661aec2,"[u""Senior staff data scientist\nGE Transportation - Chicago, IL\nJanuary 2012 to Present\nColumbia, Maryland - Remote Location)\n\nKEY QUALIFICATIONS\n\u2022 Strong analytical skills using data mining and statistical methods in both scientific and industrial settings, Six Sigma green belt certified\n\u2022 Very solid data management skill, work proficiently from traditional databases, data warehouses, to PostgreSQL data lake, write complex SQL queries for data extracting, linking parameters from diverse data systems\n\u2022 Proficient in multiple data science platforms: SAS, JMP, R, Python for data manipulation, modeling and visual analysis (ex : Graphical and R markdown inline reporting, SAS Ods graphics, R ggplot2, etc. )\n\u2022 Extensive programming skills in multiple low-level computer languages: C, C++, Java and familiar with multiple data visualization tools: IBM Cognos, Tableau, Sisense,Qlikview\n\u2022 Excellent corporate soft skill. Have numerous GE trainings on individual contributor leadership, influential communication, adaptive change management, effective writing and executive presentation\n\n\nKey contribution includes: (from latest to earliest)\n\n\u2022 Hands-on predictive modeling /analysis. Performed various in-depth sensor data\nanalysis for locomotive reliability, some examples:\no Decision tree classification/3-sigma control limit analysis for supplier quality/risk\no Linear regression analysis on oil pressure, turbo temperature, material life. etc.\no Linear regression analysis on locomotive fuel consumption\no Logistic regression /neuro network modeling on fault codes frequency with regard to odds of certain locomotive failure modes.\no Consumable part life model\n\u2022 Data scientist for the Reliability COE Prognostic/Analytics team, scouring/linking various\ndata sources to detect key features that contribute to component failures\n\n\u2022 Designated data architect for GE Transportation Global Services analytics team.\nManaged an international data integration program to link the business's asset failure\ndata, service frequency and many other data sources. Established the enterprise data\nmanagement platform and master data libraries and automated various manual\nprocesses on un-structured valuable data"", u'Lead Computer Scientist\nGE Global Research Center - Niskayuna, NY\nJanuary 2000 to January 2011\nKey accomplishment:\n\n\u2022 Built up a broad experience in working with a wide variety of business domains, from NBC to GE Locomotives, developed multiple innovative prototypes for various GE\nBusiness segments in a research and development environment\n\u2022 Developed a strong technical career path as well-rounded data scientist, from hands on data transformation to model development. Published multiple conference papers and\ntechnical reports\n\u2022 Acquired extended experience and skills with many business intelligence and analytical\nsoftware tools\n\nMajor Global Research Center projects and GE Business involved\no Financial anomaly fraud detection \u2013 GE Capital (prototype)\no A java-based project that integrates a logistic regression model on commercial finance default risk. Role in the project: preprocessing the input data, integrating the model and producing output with a color coded heatmap\no Aircraft Engine Contractual Service Agreement Modeling \u2013 GE Aviation (prototype)\no A stochastic simulation model (using the Anylogic software) that combines an aircraft engine life limited parts (LLP) and its failure weibul distribution and simulates the service cost over the life of the engine.\no Aircraft engine contract deal pricing model \u2013 GE Aviation (prototype)\no Implemented a java based pricing optimization solver that involves solving an inequality matrix with pricing factor step functions and graphic representation of hundreds of pricing options\n\n\nLANGUAGES and OTHER INTERESTS IN LIFE\nFluent in English, Chinese, French (part of my education in Quebec, Canada)\nOil painting, member of local artist club\nCooking, Traveling']","[u'M.Sc. in Computer Science', u'MBA in Finance', u""Master's in Economics""]","[u'Concordia University Montr\xe9al, QC\nJanuary 1999', u'Laval University Quebec City, Canada', u'Nankai University Tianjin, China']","degree_1 : M.Sc. in Compter Science, degree_2 :  MBA in Finance, degree_3 :  ""Masters in Economics"""
0,https://resumes.indeed.com/resume/737c6369750a8163,"[u'Data Scientist\nSantander Bank - Holmdel, NJ\nJune 2017 to Present\nSantander Bank is based in Boston and its principal market is the northeastern United States. The Bank offers financial services and products including retail banking, mortgages, corporate banking, cash management, credit card, capital markets, trust and wealth management, and insurance.\n\nThe project was to build predictive models for the identification/detection of fraudulent transactions by applying machine learning methods, principle component analysis, and logistic regression on large dataset.\n\nResponsibilities:\n\u2022 Participated in all phases of data acquisition, data cleaning, developing models, validation, and visualization to deliver data science solutions.\n\u2022 Worked on fraud detection analysis on payments transactions using the history of transactions with supervised learning methods.\n\u2022 Collected data in Hadoop and retrieved the data required for building models using Hive.\n\u2022 Developed Spark Python modules for machine learning & predictive analytics in Hadoop.\n\u2022 Used Pandas, Numpy, Seaborn, Matplotlib, Scikit-learn in Python for developing various machine learning models and utilized algorithms such as Decision Trees, Logistic regression, Gradient Boosting, SVM and KNN.\n\u2022 Used cross-validation to test the models with different batches of data to optimize the models and prevent overfitting.\n\u2022 Used PCA and other feature engineering techniques for high dimensional datasets while maintaining the variance of most important features.\n\u2022 Created Transformation Pipelines for preprocessing large amount of data with methods such as imputing, scaling, selecting, etc.\n\u2022 Ensemble methods were used to increase the accuracy of the training model with different Bagging and Boosting methods.\n\nEnvironment:\nHadoop 2.x, HDFS, Hive, Pig Latin, Python 3.x (Numpy, Pandas, Scikit-learn, Matplotlib), Jupyter, GitHub, Linux', u""Data Analyst/Data Scientist\nSCIO Health Analytics - Hartford, CT\nApril 2015 to May 2017\nSCIO Health Analytics provides analytics solutions and services that turns data into actionable insights for health care providers in the United States and globally. Services also include medical and pharmacy claims auditing, inpatient data pursuits, care gaps closure, and commercial analytics.\n\nI was part of the team that worked with Subrogation claims of Healthcare Providers such as Humana. The objective was to load data, analyze, and provide monthly reports for the predictions on a claim's potential of a third-party recovery. Tableau and SSRS were used to build claim and recovery reports.\n\nResponsibilities:\n\u2022 Assembled a Predictive Modelling module by using supervised learning for Subrogation Claim Prediction to identify which claims would be classified as having Subrogation potential.\n\u2022 Implemented models such as Logistic Regression and Na\xefve Bayes, in Python using scikit-learn, to predict the claim potential outcome.\n\u2022 Dimensionality Reduction techniques applied to refine the attribute lists and feature selection applied to rank selected features to generate accurate results.\n\u2022 Gathered requirements and business rules from business users to implement Predictive Modelling.\n\u2022 Designed and developed ETL packages using SSIS to create Data Warehouses from different tables and file sources like Flat and Excel files, with different methods in SSIS such as derived columns, aggregations, Merge joins, count, conditional split and more to transform the data.\n\u2022 Designed reporting solutions for different stakeholders from mock-up till deployment in different areas such as Potential Subrogation claims, Monthly Revenue from Subrogation & Transactions.\n\u2022 Performed data visualization and designed dashboards with Tableau, and provided complex reports, including charts, summaries, and graphs to interpret the findings for Adjustors to view various claim information.\n\u2022 Optimized queries in T-SQL by removing unnecessary columns and redundant data, normalized tables, established joins and indices; developed complex SQL queries, stored procedures, views, functions and reports that meet customer requirements.\n\nEnvironment:\nPython 3.x (Scikit-learn, Matplotlib), Jupyter, SQL Server 2012, MS SQL Server Management Studio, MS BI Suite (SSIS/SSRS), T-SQL, Visual Studio BIDS, Tableau"", u'SQL BI Developer\nADP - Chennai, Tamil Nadu\nFebruary 2012 to March 2015\nADP is a leading provider of human resources management software and services worldwide.\n\nThis project was done for an internal business unit (ADP France) to comply with French statutory requirements for employee training. Goal was to develop a web-based SQL application built upon a baseline HRMS application to generate/support the Report development for training plans, budget preparation, cost tracking and 2483 reporting.\n\nResponsibilities:\n\u2022 Collected requirements from business users, and designed report models to meet business requirements.\n\u2022 Directed and managed meetings with clients, tracked document changes and ensured sign-off from clients.\n\u2022 Maintained and developed complex SQL queries, stored procedures, views, functions and reports that meet customer requirements using Microsoft SQL Server 2008 R2.\n\u2022 Created Views and Table-valued Functions, Common Table Expression (CTE), joins, complex subqueries to provide the reporting solutions.\n\u2022 Optimized the performance of queries with modification in T-SQL queries, removed the unnecessary columns and redundant data, normalized tables, established joins and created index.\n\u2022 Created, managed, and delivered interactive web-based reports to support daily operations.\n\u2022 Validated reports and resolve issues in a timely manner.\n\u2022 Developed and implemented several types of Reports (Training Reports, Schedules, Costs Summary Reports and Annual 2483 Report) by using features of SSRS such as sub-reports, drill down reports, summary reports and parameterized reports.\n\u2022 Designed and developed new reports and maintained existing reports for the Human Resource Management System Dashboards using Tableau, Qlikview and Microsoft Excel to support the business strategy and management.\n\u2022 Identified process improvements that significantly reduce workloads or improve quality.\n\nEnvironment:\nSQL Server 2008 R2, MS SQL Server Management Studio, SSRS, T-SQL, Visual Studio BIDS, Tableau, Qlikview']",[u'Master of Science in Business & Information Systems'],"[u'New Jersey Institute of Technology Newark, NJ']",degree_1 : Master of Science in Bsiness & Information Systems
0,https://resumes.indeed.com/resume/b3dfb5279098df46,"[u'Data Scientist Systems Engineer 2\nVerizon\nJanuary 2017 to Present\n\u2022 Launched and manage Hortonworks platform including Hive, Spark, Scala, R, and Python for advanced analytics.\n\u2022 Designed and manage multiple active learning ML models across cloud and on premise services including AWS.\n\u2022 Designed and manage continuous integration continuous delivery (CICD) pipelines for putting models in to production.\n\u2022 Automated scale out and replication of node infrastructure in the cloud using Jenkins, Ansible, and Docker.\n\u2022 Developed text classification model (Spark Scala) to improve reconciliation of circuit charges from other telecom companies and developed model to predict revenue categories over billing data.', u'Data Scientist Intern\nNetApp\nJanuary 2016 to January 2016\n\u2022 Improved quality tracking by developing a micro-batch model to visualize cluster metrics (SQLAlchemy, Elasticsearch, and Kibana).\n\u2022 Automated cluster availability metrics by facilitating integration between multiple database platforms (Salesforce, MySQL, HDFS).\n\u2022 Improved quality testing by developing an application to easily view bug reports and metrics (Python, Flask, Bootstrap, and Plotly).\n\u2022 Built model to predict when additional storage capacity or node memory is needed (AWS, Python, Tableau, MySQL).\n\u2022 Predicted and prevented software failures by calculating highest risk field nodes (Hadoop, AWS, Hive, Sqoop, MySQL).\n\u2022 Improved product quality by linking cluster downtime events within iSCSI networking faults and volume configuration (Hive, Python).', u'Analytics Engineer\nCarollo Engineers\nJanuary 2014 to January 2016\n\u2022 Established long term fiscal policy projections for multiple major west cities.\n\u2022 Estimated financial exposure according inherent and residual risk in engineering projects.\n\u2022 Adapted capital spending plans according onsite engineering assessment of municipal assets.\n\u2022 Managed data architecture for in house asset management software.\n\u2022 Increased savings by 30% on capital expenditures for city municipalities.', u""Engineering Intern\nAEI Consultants\nJanuary 2010 to January 2012\n\u2022 Programmed sensors for monitoring pumps, PLC's, and blowers.\n\u2022 Gathered and analyzed data using MATLAB, AutoCAD, and GIS centric software.""]","[u'M.A.S. in Data Science & Engineering', u'', u'B.S. in Engineering and Applied Science']","[u'School of Computer Science and Engineering\nJanuary 2017', u'University of California San Diego, CA', u'University of Colorado Boulder, CO']","degree_1 : M.A.S. in Data Science & Engineering, degree_2 :  , degree_3 :  B.S. in Engineering and Applied Science"
0,https://resumes.indeed.com/resume/a32baa20e6a60b7e,"[u'Data Scientist\nCitibank Corp - Tampa, FL\nJuly 2015 to June 2017\nR, Python, Informatica)\nProject: Trade Risk Model\nA machine learning algorithm was developed to identify rogue trading behavior using decision trees (random forest ). This analysis helps banks to identify rogue traders. Decision tree algorithm was used to identify factors to find the higher risk of illegal trading behavior.\nResponsibilities\n\u2022 Working directly with client stakeholders to understand and define analysis objectives and then translate these into actionable results.\n\u2022 Obtaining data from multiple, disparate data sources including structured, semi-structured and unstructured data.\n\u2022 Using machine learning and data mining algorithm to understand the patterns in large volumes of data, identify relationships detect data anomalies, and classify data sets.\n\u2022 Designing and building algorithms and predictive algorithm using techniques such as linear and logistic regression, support vector machines, ensemble models (random forest and/or gradient boosted trees), neural networks, and clustering techniques.\n\u2022 Optimize ML algorithm using regression techniques such as ridge, lasso and elastic regression; dimensional reduction and feature selection using SciKit learn\n\u2022 Optimize ML algorithm using boosting (GBM) and bagging approaches using SciKit-learn.\n\u2022 GridSearchCV in SciKit-learn was applied to find optimum parameter values for different regression models.\n\u2022 Design machine learning experiments in big data environment such as PySpark. Regression and classification models were implemented using the MLlib package.\n\u2022 Implement large scale deep neural netwrok (DNN) experiemnts for regression and classification task using Tensorflow. Construct estimator, DNNRegressor, DNN layers, parameter optimization to make better predictions using Tensorflow.\n\u2022 Working with data integration developers to assess data quality and define data processing business rules for cleansing, aggregation, enhancement etc. support analysis and predictive modeling activities.\n\u2022 ETL mapping to extract trading data from Ocean system to CnC (Change & Cancel) database and other layers of data mart in EDW; design SCD Type I and II.\n\u2022 Presenting complex analysis results tailored to different audiences (e.g. technical, manager, executive) in a highly consumable and actionable form including the use of data visualizations (Tableau, R).', u'Data Scientist\nBob Evans Farm - Columbus, OH\nFebruary 2014 to May 2015\n\u2022 Gather requirements for various data mining projects.\n\u2022 Involved in loading data from different database servers and imported to R for data analysis and visualization.\n\u2022 Responsible for preparing data and exploratory analysis for machine learning to develop models.\n\u2022 Developed machine learning algorithm (random-forest) to predict calorie content of the menu items.\n\u2022 Created standard data summaries, extracted subset of data and split data and created data partitions.\n\u2022 Created various types of data visualizations using R and Tableau.\n\u2022 Developed prediction algorithm using advanced data mining algorithms to classify similar properties together to develop sub-markets.\n\u2022 Conducted data preparation, and outlier detection using R and Python;\n\u2022 Developed stored procedures and views in both PL-SQL and T-SQL for data warehouse.\n\u2022 Developed several ETL/Informatica mappings to extract historical data.\n\u2022 The modification and enhancements went through a typical SDLC including code revision by peers, unit testing, source control (TFS), QA, and deployment.', u'Research Scientist\nFlorida International University - Miami, FL\nAugust 2004 to December 2013\nC++/CUDA, R, SAS, Python, MATLAB)\n\u2022 Designed and developed a simulation engine to predict behavior of multiple fluid mixtures in complex porous media (C/C++, MATLAB, R, SAS, CUDA, MPICH2, Shell, Python, and Linux).\n\u2022 Designed and developed Rainfall-Runofff forecasting models using Artificial Neural Networks (ANN) for high performance computing (C/C++/CUDA).\n\u2022 Statistical analysis of geological dataset using t-tests, ANOVA, factor analysis, linear regression, non-linear regression (MATLAB, R, Python).\n\u2022 The simulation engine was applied on high-performance parallel computing clusters. (C, CUDA, multi-threading, MATLAB, MPICH2, Shell, Linux)\n\u2022 Shell scripting to manage large dataset (several GBs) obtained from simulation engine (Shell scripting, Linux)\n\u2022 NumPy (matplotlib) python scripting was used for analysis and visualization of large data sets.\n\u2022 Developed pipelines to analyze large simulation datasets combining my own Python, and Shell scripts with established molecular modeling tools.\n\u2022 Interpreted complex simulation data using statistical methods (Monte-Carlo).\n\u2022 Worked in a cross-functional team to develop a unique multi-scale simulation technique and implemented the methodology to explain complex hydrogeological problems.\n\u2022 Trained lab students on statistical methods and High-Performance Computing (HPC).']","[u'PhD in Geosciences', u'MS in Civil Engineering', u'BS in Civil Engineering']","[u'Florida International University Miami\nJanuary 2008', u'Indian Institute of Technology Kanpur India Kanpur, Uttar Pradesh\nJanuary 2003', u'Muzaffarpur Institute of Technology India Muzaffarpur, Bihar\nJanuary 2001']","degree_1 : PhD in Geosciences, degree_2 :  MS in Civil Engineering, degree_3 :  BS in Civil Engineering"
0,https://resumes.indeed.com/resume/8c95c063a6140fd2,"[u""Data Scientist\nCapital One - Wilmington, DE\nJune 2017 to Present\nDescription: Capital One provide services to customers to open business accounts. Project is called National Small Businesses(NSB). As part of this project data of legacy customers is migrated to relational database from NoSql database. API's is developed to retrieve the information to migrate.\n\nResponsibilities:\n\u2022 Long Short-Term Memory Recurrent Neural Networks (LSTM RNNs) learnt using Deep Learning techniques applied to Problem X.\n\u2022 LSTM RNNs applied to Problem Y.\n\u2022 Improving Fraud Detection using Digital Links at Amazon, Seattle.\n\u2022 Scaled upto Machine Learning pipelines: 4600 processors, 35000 GB memory achieving 5-minute execution.\n\u2022 Deployed GUI pages by using JSP, JSTL, HTML, DHTML, XHTML, CSS, JavaScript, AJAX.\n\u2022 Configured the project on WebSphere 6.1 application servers\n\n\u2022 Designed a new Machine Learning pipeline to replace existing prod: AUC perf. increase from 83% to 90%.\n\u2022 Handled 2+ TB data with graphs upto130 GB (50M nodes, 100M edges) using single-node in-disk scaling.\n\u2022 Developed a Machine Learning test-bed with 24 different model learning and feature learning algorithms.\n\u2022 By thorough systematic search, demonstrated performance surpassing the state-of-the-art (deep learning).\n\n\u2022 Upto 10 times more accurate predictions over existing state-of-the-art algorithms.\n\u2022 Developed in-disk, huge (100GB+), highly complex Machine Learning models.\n\u2022 Used SAX and DOM parsers to parse the raw XML documents\n\u2022 Used RAD as Development IDE for web applications.\n\n\u2022 Demonstrated performances comparable to other state-of-the-art deep learning models.\n\u2022 Applied Machine Learning algorithms to diagnose blood loss from vital signs (ECG, HF, GSR, etc.)\n\u2022 Devised and implemented a Vehicle Speed Detector using low-power LEDs and field-tested for robustness.\n\u2022 National Highways Authority (Govt. of India) is evaluating the design for installations across the country.\n\u2022 IIT Madras has installed the speed detectors across the institute for permanent speed limit enforcement.\n\u2022 Developed & tested feature tracking algorithms for Intelligent Transportation Systems Computer Vision.\n\u2022 Analyzed SIFT feature descriptors and their resilience to changes in illumination.\n\u2022 Devised a novel machine learning algorithm for classification of ECG abnormalities.\n\nEnvironment: R 9.0, Informatica 9.0, ODS, OLTP, Oracle 10g, Hive, OLAP, DB2, Metadata, MS Excel, Mainframes MS Vision, Rational Rose."", u'Data Scientist\nJP Morgan Chase - Columbus, OH\nMarch 2016 to May 2017\nDescription:: Project is assessing the credit risk through Robotic process and decrease the manual labor and budget providing efficient results. Project includes sorting the customers of different levels having different credit ranges and increasing their credit limit based on their credit history and usage of credit card.\n\nResponsibilities:\n\u2022 Manipulating, cleansing & processing data using Excel, Access and SQL.\n\u2022 Responsible for loading, extracting and validation of client data.\n\u2022 Liaising with end-users and 3rd party suppliers.\n\u2022 Analyzing raw data, drawing conclusions & developing recommendations\n\u2022 Writing T-SQL scripts to manipulate data for data loads and extracts.\n\u2022 Developing data analytical databases from complex financial source data.\n\u2022 Performing daily system checks.\n\u2022 Data entry, data auditing, creating data reports & monitoring all data for accuracy.\n\u2022 Designing, developing and implementing new functionality.\n\u2022 Monitoring the automated loading processes.\n\u2022 Advising on the suitability of methodologies and suggesting improvements.\n\u2022 Carrying out specified data processing and statistical techniques.\n\u2022 Supplying qualitative and quantitative data to colleagues & clients.\n\u2022 Using Informatica & SAS to extract, transform & load source data from transaction\nsystems.\n\u2022 Createing data pipelines using big data technologies like Hadoop, spark etc.\n\u2022 Creating statistical models using distributed and standalone models to build various diagnostics, predictive and prescriptive solution.\n\u2022 Utilize a broad variety of statistical packages like SAS, R , MLIB, Graphs, Hadoop, Spark , MapReduce, Pig and others\n\u2022 Refine and train models based on domain knowledge and customer business objectives\n\u2022 Deliver or collaborate on delivering effective visualizations to support the client business objectives\n\u2022 Communicate to your peers and managers promptly as and when required.\n\u2022 Produce solid and effective strategies based on accurate and meaningful data reports and analysis and/or keen observations.\n\u2022 Establish and maintain communication with clients and/or team members; understand needs, resolve issues, and meet expectations\n\u2022 Developed web applications using .net technologies; work on bug fixes/issues that arise in the production environment and resolve them at the earliest\nEnvironment: SQL Server 2008R2/2005 Enterprise, SSRS, SSIS, Crystal Reports, Hadoop, Windows Enterprise Server 2000, DTS, SQL Profiler, and Query Analyzer.', u""Data Scientist\nAmerica's Car-Mart Inc - Bentonville, AR\nNovember 2014 to February 2016\nDescription:America's Car-Mart, Inc., through its subsidiaries, operates as an automotive retailer in the United States. It primarily sells older model used vehicles and provides financing for its customers.\nResponsibilities:\n\u2022 Data mining using state-of-the-art methods\n\u2022 Extending company's data with third party sources of information when needed\n\u2022 Enhancing data collection procedures to include information that is relevant for building analytic systems\n\u2022 Processing, cleansing, and verifying the integrity of data used for analysis\n\u2022 Doing ad-hoc analysis and presenting results in a clear manner\n\u2022 Creating automated anomaly detection systems and constant tracking of its performance\n\u2022 Strong command of data architecture and data modelling techniques.\n\u2022 Hands on experience with commercial data mining tools such as Splunk, R, Map reduced, Yarn, Pig, Hive, Floop, Oozie, Scala, HBase, Master HDFS, Sqoop, Spark, Scala (Machine learning tool) or similar software required depending on seniority level in job field.\n\u2022 Developed scalable machine learning solutions within a distributed computation framework (e.g. Hadoop, Spark, Storm etc.).\n\u2022 Utilizing NLP applications such as topic models and sentiment analysis to identify trends and patterns within massive data sets.\n\u2022 Knowledge in ML& Statistical libraries (e.g. Scikit-learn, Pandas).\n\u2022 Having knowledge to build predict models to forecast risks for product launches and operations and help predict workflow and capacity requirements for TRMS operations\n\u2022 Having experience with visualization technologies such as Tableau\n\u2022 Draw inferences and conclusions, and create dashboards and visualizations of processed data, identify trends, anomalies\n\u2022 Generation of TLFs and summary reports, etc. ensuring on-time quality delivery.\n\u2022 Participated in client meetings, teleconferences and video conferences to keep track of project requirements, commitments made and the delivery thereof.\n\u2022 Solved analytical problems, and effectively communicate methodologies and results\n\u2022 Worked closely with internal stakeholders such as business teams, product managers, engineering teams, and partner teams.\n\u2022 Created automated metrics using complex databases.\n\u2022 Foster culture of continuous engineering improvement through mentoring, feedback, and metrics.\nEnvironment: Erwin r9.0, Informatica 9.0, ODS, OLTP, Oracle 10g, Hive, OLAP, DB2, Metadata, MS Excel, Mainframes MS Visio, Rational Rose, Requisite Pro. Hadoop, PL/SQL, etc.."", u'Data Scientist\nHibbett Sports Inc - Birmingham, AL\nMarch 2013 to October 2014\nDescription:Hibbett Sports, Inc. operates sporting goods stores in small to mid-sized markets primarily in the southeast, southwest, Mid-Atlantic, and Midwest regions of the United States.\nResponsibilities:\n\n\u2022 Statistical Modelling with ML to bring Insights in Data under guidance of Principal Data Scientist\n\u2022 Data modeling with Pig, Hive, Impala.\n\u2022 Ingestion with Sqoop, Flume.\n\u2022 Used SVN to commit the Changes into the main EMM application trunk.\n\u2022 Understanding and implementation of text mining concepts, graph processing and semi structured and unstructured data processing.\n\u2022 Worked with Ajax API calls to communicate with Hadoop through Impala Connection and SQL to render the required data through it .These API calls are similar to Microsoft Cognitive API calls.\n\u2022 Good grip on Cloudera and HDP ecosystem components.\n\u2022 Used ElasticSearch (Big Data) to retrieve data into application as required.\n\u2022 Performed Map Reduce Programs those are running on the cluster.\n\u2022 Developed multiple MapReduce jobs in java for data cleaning and preprocessing.\n\u2022 Developed scalable machine learning solutions within a distributed computation framework (e.g. Hadoop, Spark, Storm etc.).\n\u2022 Analyzed the partitioned and bucketed data and compute various metrics for reporting.\n\u2022 Involved in loading data from RDBMS and web logs into HDFS using Sqoopand Flume.\n\u2022 Worked on loading the data from MySQL to HBase where necessary using Sqoop.\n\u2022 Developed Hive queries for Analysis across different banners.\n\u2022 Extracted data from Twitter using Java and Twitter API. Parsed JSON formatted twitter data and uploaded to database.\n\u2022 Launching Amazon EC2 Cloud Instances using Amazon Images (Linux/ Ubuntu) and Configuring launched instances with respect to specific applications.\n\u2022 Exported the result set from Hive to MySQL using Sqoop after processing the data.\n\u2022 Analyzed the data by performing Hive queries and running Pig scripts to study customer behavior.\n\u2022 Have hands on experience working on Sequence files, AVRO, HAR file formats and compression.\n\u2022 Used Hive to partition and bucket data.\n\u2022 Experience in writing MapReduce programs with Java API to cleanse Structured and unstructured data.\n\u2022 Wrote Pig Scripts to perform ETL procedures on the data in HDFS.\n\u2022 Created HBase tables to store various data formats of data coming from different portfolios.\n\u2022 Worked on improving performance of existing Pig and Hive Queries.\n\nEnvironment: SQL/Server, Oracle 9i, MS-Office, Teradata, Informatica, ER Studio, XML, Business Objects, HDFS, Teradata 14.1, JSON, HADOOP (HDFS), MapReduce, PIG, Spark, R Studio, MAHOUT, JAVA, HIVE, AWS..', u""Data Architect/Data Modeler\nFirst Indian Corporation Private Limited - Hyderabad, Telangana\nOctober 2011 to February 2013\nDescription:First Indian Corporation Private Limited, a member of The First American family of companies, provides specialized offshore transaction services, technology services and analytics to the mortgage industry. First Indian Corporation's primary focus is on the Title Insurance, Property Tax, Flood Certification, Default Management Services, Credit and Real Estate Information segments.\n\nResponsibilities:\n\u2022 Worked with large amounts of structured and unstructured data.\n\u2022 Knowledge in Machine Learning concepts (Generalized Linear models, Regularization, Random Forest, Time Series models, etc.)\n\u2022 Worked in Business Intelligence tools and visualization tools such as Business Objects, Tableau, ChartIO, etc.\n\u2022 Deployed GUI pages by using JSP, JSTL, HTML, DHTML, XHTML, CSS, JavaScript, AJAX.\n\u2022 Configured the project on WebSphere 6.1 application servers\n\u2022 Implemented the online application by using Core Java, Jdbc, JSP, Servlets and EJB 1.1, Web Services, SOAP, WSDL.\n\u2022 Handled end-to-end project from data discovery to model deployment.\n\u2022 Monitoring the automated loading processes.\n\u2022 Communicated with other Health Care info by using Web Services with the help of SOAP, WSDLJAX-RPC\n\u2022 Used Singleton, factory design pattern, DAO Design Patterns based on the application requirements\n\u2022 Used SAX and DOM parsers to parse the raw XML documents\n\u2022 Used RAD as Development IDE for web applications.\n\u2022 Preparing and executing Unit test cases\n\u2022 Used Log4J logging framework to write Log messages with various levels.\n\u2022 Involved in fixing bugs and minor enhancements for the front-end modules.\n\u2022 Implemented Microsoft Visio and Rational Rose for designing the Use Case Diagrams, Class model, Sequence diagrams, and Activity diagrams for SDLC process of the application\n\u2022 Doing functional and technical reviews\n\u2022 Maintenance in the testing team for System testing/Integration/UAT.\n\u2022 Guaranteeing quality in the deliverables.\n\u2022 Conducted Design reviews and Technical reviews with other project stakeholders.\n\u2022 Was a part of the complete life cycle of the project from the requirements to the production support\n\u2022 Created test plan documents for all back-end database modules\nEnvironment: R 3.0, Erwin 9.5, Tableau 8.0, MDM, QlikView, MLLib, PL/SQL, HDFS, Teradata 14.1, JSON, HADOOP (HDFS), MapReduce, PIG, Spark, R Studio, MAHOUT, JAVA, HIVE, AWS."", u'Data Analyst/Data Modeler\nAccenture - Bengaluru, Karnataka\nJanuary 2009 to September 2011\nDescription: Accenture PLC is a global management consulting and professional services company that provides strategy, consulting, digital, technology and operations.\n\nResponsibilities:\n\u2022 Developed Internet traffic scoring platform for ad networks, advertisers and publishers (rule engine, site scoring, keyword scoring, lift measurement, linkage analysis).\n\u2022 Responsible for defining the key identifiers for each mapping/interface.\n\u2022 Clients include eBay, Click Forensics, Cars.com, Turn.com, Microsoft, and Looksmart.\n\u2022 Designed the architecture for one of the first analytics 3.0. online platforms: all-purpose scoring, with on-demand, SaaS, API services. Currently under implementation.\n\u2022 Web crawling and text mining techniques to score referral domains, generate keyword taxonomies, and assess commercial value of bid keywords.\n\u2022 Developed new hybrid statistical and data mining technique known as hidden decision trees and hidden forests.\n\u2022 Reverse engineering of keyword pricing algorithms in the context of pay-per-click arbitrage.\n\u2022 Implementation of Metadata Repository, Maintaining Data Quality, Data Cleanup procedures, Transformations, Data Standards, Data Governance program, Scripts, Stored Procedures, triggers and execution of test plans\n\u2022 Performed data quality in Talend Open Studio.\n\u2022 Coordinated meetings with vendors to define requirements and system interaction agreement documentation between client and vendor system.\n\u2022 Automated bidding for advertiser campaigns based either on keyword or category (run-of-site) bidding.\n\u2022 Creation of multimillion bid keyword lists using extensive web crawling. Identification of metrics to measure the quality of each list (yield or coverage, volume, and keyword average financial value).\n\u2022 Enterprise Metadata Library with any changes or updates.\n\u2022 Document data quality and traceability documents for each source interface.\n\u2022 Establish standards of procedures.\n\u2022 Generate weekly and monthly asset inventory reports.\n\nEnvironment: Erwin r7.0, SQL Server 2000/2005, Windows XP/NT/2000, Oracle 8i/9i, MS-DTS, UML, UAT, SQL Loader, OOD, OLTP, PL/SQL, MS Visio, Informatica.']","[u""Bachelor's""]",[u''],"degree_1 : ""Bachelors"""
0,https://resumes.indeed.com/resume/1780826241756104,"[u""Data Science consultant\nFirst Tennessee Bank - Memphis, TN\nAugust 2017 to Present\nThe project utilized machine learning methods, and data mining technologies in developing innovative solutions in banking industry.\n\u2022 Improved classification of bank authentication protocols by 20% by applying clustering methods on transaction data using Python Scikit-learn locally, and Spark MLlib on production level.\n\u2022 Developed FraudNet, an xgboost-based fraud prevention machine learning model with less than 0.001%\nconfirmed false positives and over 0.985 AUC, which corrects analyst's false decisions, provides Risk\nsuggestions for Risk analysis department, and is able to reduce review rate by 50%.\n\u2022 Outlined plan of execution to leverage predictive models and other machine learning algorithms as part of company technology strategy.\n\u2022 Performed machine learning and statistical analysis methods, such as clustering and classification,\ncollaborative filtering, sentiment analysis, multivariate regression analysis, statistical inference, and validation\nmethods.\n\u2022 Docker implementation for machine learning production system deployment.\n\u2022 Increased model performance by hyperparameters optimization.\n\u2022 Used Python (NumPy, SciPy, Pandas, SciKit-Learn, Seaborn, NLTK) and Spark 1.6 / 2.0 (PySpark, MLlib) to develop variety of models and algorithms for analytical purposes.\n\u2022 Developed an algorithm that can identify fatal assessments that are expected to Fail under central review."", u'Senior Data Scientist\nHindustan Aeronautics Limited - Nashik, Maharashtra\nOctober 2013 to August 2015\nPartner with HAL Software Engagement Managers to shape Statements of Work for Data Science opportunities\nand HAL Software Solutions\n\u2022 Participate in Data Science Workouts to shape Data Science opportunities and identify opportunities to use\ndata science to create customer value\n\u2022 Guide junior Data Scientists to develop, verify, and validate analytics to address customer needs and opportunities.\n\u2022 Provide technical project and program oversight to ensure appropriate technical rigor is applied to Data Science\nengagements\n\u2022 Guide cross-functional teams to translate algorithms into commercially viable products and services.\n\u2022 Guide and otherwise contribute to technical teams in development, deployment, and application of applied\nanalytics, predictive analytics, and prescriptive analytics.\n\u2022 Partner with data engineers on data quality assessment, data cleansing and data analytics efforts\n\u2022 Generate reports, annotated code, and other projects artifacts to document, archive, and communicate your\nwork and outcomes.\n\u2022 Communicate methods, findings, and hypotheses with stakeholders.', u'Data Scientist\nHindustan Aeronautics Limited - Nashik, Maharashtra\nSeptember 2011 to August 2013\nDeveloped and applied methods to identify, collect, process, and analyze large volumes of data to build and enhance products, processes, and systems. Conducted data mining and retrieval, and apply statistical and\nmathematical analyses to identify trends, solve analytical problems, optimize performance, and gather intelligence.\n\u2022 Visualized information using a range of tools (Tableau, Excel), develop scripts and algorithms, create\nexplanatory analysis and predictive models.\n\u2022 Performed Multinomial Logistic Regression, Random Forest, Decision Tree, SVM to classify sub- component is going to fail or not.\n\u2022 Used Principal Component Analysis & Factor Analysis in feature engineering to analyze high dimensional\ndata.\n\u2022 Used Python/R machine learning library to build and evaluate different models.\n\u2022 Collected data needs and requirements by Interacting with the other departments.\n\u2022 Communicated the results with operations team for taking best decisions.\n\u2022 Led technical implementation of advanced analytics projects and defined the time-series approaches.']","[u'Master of Science in Information in systems', u'in Science & Enineering']","[u'San Diego State University\nAugust 2015 to May 2017', u'Guru Jambheshwar University\nJuly 2003 to June 2005']","degree_1 : Master of Science in Information in systems, degree_2 :  in Science & Enineering"
0,https://resumes.indeed.com/resume/1456f8fb7ba7d2f2,"[u""Data Scientist\nArgus Information & Advisory Services, LLC - White Plains, NY\nSeptember 2016 to Present\nArgus Information and Advisory Services is a Subsidiary of Verisk Analytics Company and the leading provider of analytics, information and solutions to consumer banks and their regulators. The company's clients range from financial institutions to retailers and tech companies. The project focused on detecting anti-money laundering violation using Big Data and Data Science tools and improving customer's transaction monitoring system.\nResponsibilities:\n\u2022 Collected and analyzed the business requirements, understood the particular Fraud/AML challenges that our client faces.\n\u2022 Participated in Data integration job with Data Engineer team to gather traditional transaction data and external source data together.\n\u2022 Transformed data from SQL Server database to Hadoop Clusters which is set up by using AWS EMR.\n\u2022 Conducted data cleansing and feature engineering job through python NumPy and Pandas.\n\u2022 Implemented Naive Bayes, Logistic Regression, SVM, Random Forest and Gradient boosting with weighted loss function by using Python Scikit-learn.\n\u2022 Implemented mulit-layers Neural Networks by using Google Tensorflow and Spark.\n\u2022 Performed extensive Behavioral modeling and Customer Segmentation to discover behavior patterns of customers by using K-means Clustering.\n\u2022 Managed and scheduled models by using Oozie for batch processing.\n\u2022 Updated and saved Fraud predictions to AWS S3 for application team.\n\u2022 Tested the business performance of the AML models by evaluating detection rate and false positive rate and worked on continuous improvement on model.\n\u2022 Created reports and dashboards, by using Tableau, to explain and communicated data insights, significant features, model's score and performance of new transaction monitoring system to both technical and business teams.\n\u2022 Used GitHub for version control with Data Engineer team and Data Scientists colleagues.\nEnvironment: SQL Server 2014, Hadoop 2.0, Hive 2.0, Spark (PySpark, SparkSQL), Python 3.X, Tensorflow, Oozie 4.2, Tableau 10.X, AWS S3/EC2/EMR, Github"", u'Data Scientist\nCenterLight Health System - Bronx, NY\nApril 2015 to July 2016\nCenterLight Health System, a not-for-profit organization, has evolved into a leader in serving the elderly, chronically ill and disabled. CenterLight is one of the largest long-term care providers in New York State, serving all of New York City, Westchester, Nassau, Rockland and Suffolk Counties. This project aimed to predict the billing cycles and accounting related issues to increase the efficiency of enterprise claim processing.\n\nResponsibilities:\n\u2022 Conducted reverse engineering based on demo reports to understand the data without documentation.\n\u2022 Generated new data mapping documentations and redefined the proper requirements in detail.\n\u2022 Generated different Data Marts for gathering the tables needed (Member info, Claim info, Transaction info, Appointment info, Diagnose info) from SQL Server Database.\n\u2022 Created ETL packages to transform data into the right format and join tables together to get all features required using SSIS.\n\u2022 Processed data using Python pandas to examine transaction data, identify outliers and inconsistencies.\n\u2022 Conducted exploratory data analysis using python NumPy and Seaborn to see the insights of data and validate each feature through different charts and graphs.\n\u2022 Built predictive models including Linear regression, Lasso Regression, Random Forest Regression and Support Vector Regression to predict the claim closing gap by using python scikit-learn.\n\u2022 Used GridSearchCV to evaluate each model and to find best parameters set for each model.\n\u2022 Created reports and an app demo using Tableau to show client how prediction can help the business.\n\u2022 Deployed and hosted our models by using Azure Machine Learning Studio and share an API with application development team.\n\u2022 Used Confluence to share and collaborate on projects with team members, and keep track of up to date documentations.\n\nEnvironment: SQL Server 2012, SQL Server Data Tools 2010, SQL Server Integration Services, Python 2.7/3.3, Tableau 9.4, Azure Machine Learning Studio', u""Junior Data Scientist\nAtlantic Health - Morristown, NJ\nJanuary 2014 to March 2015\nAtlantic Health System is one of the leading non-profit health care systems in New Jersey, providing a wide array of health care services to the residents of Northern and Central regions of the state as well as Pike County, PA, and southern Orange County, NY. Project was to build a predictive model to predict the readmission case. The main objective was to reduce the risk of being wrongly diagnosed and the risk of being involved in the legal disputes.\n\nResponsibilities:\n\u2022 Communicated and coordinated with other departments to gather business requirements.\n\u2022 Gathered data information from multiple sources, and performed resampling method to handle the issue of imbalanced data.\n\u2022 Worked with ETL Team and Doctors to understand the data and define the uniform standard format.\n\u2022 Conducted data cleansing by using advanced SQL queries in SQL Server Database.\n\u2022 Split the data into different smaller dataset based on different diagnoses, in charge of conducting exploratory data analysis for three of diagnoses datasets (Diabetes, cold/flu, allergy).\n\u2022 Created the whole pipeline of data preprocessing (imputing, scaling, label encoding) through python pandas to get data ready to modeling part.\n\u2022 Built predictive models, using python scikit-learn, including Support Vector Machine, Decision tree, Naive Bayes Classifier, Neural Network to predict a potential readmitted case.\n\u2022 Performed Ensemble methods, including Gradient Boosting, Random Forest, customized ensemble method to produce more accurate solutions.\n\u2022 Designed and implemented cross-validation and statistical tests including Hypothesis testing, AVOVA, Chi-square test to verify models' significance.\n\u2022 Created a API by using Flask and shared the idea with application team and help them define the requirements of new application.\n\u2022 Used Agile methodology and Scrum process for project developing.\n\nEnvironment: SQL server 2012, SQL Server Integration Services, Python 2.7, Jupyter notebook, Flask 0.10, SharePoint 2013"", u'BI Developer\nFulton Financial Corporation - Lancaster, PA\nDecember 2012 to October 2013\nFulton is a financial company based in Lancaster, Pennsylvania. They provide a wide range of financial products and personalized services in Pennsylvania, Maryland, Delaware, Virginia and New Jersey. They are comprised of several different banking subsidiaries. The main job of this project was to provide ETL solutions for data migration and provide data quality and micro strategy solutions.\n\nResponsibilities:\n\u2022 Involved in gathering user/project requirements from business users and IT managers, translated it into functional and non-functional specifications needed and created documentations for the project.\n\u2022 Assisted in design and data modeling efforts of Data Marts and Enterprise Data Warehouse.\n\u2022 Used T-SQL in SQL Server to develop complex stored procedures, triggers, clustered index & non-clustered index, Views, and User-defined Functions (UDFs).\n\u2022 Designed SSIS packages to extract, transform and load existing data into SQL Server, used lots of components of SSIS, such as Pivot Transformation, Fuzzy Lookup, Derived Columns, Condition Split, Aggregate, Execute SQL Task, Data Flow Task and Execute Package Task.\n\u2022 Created SSIS Packages that involved dealing with different source formats (Text files, XML, Database Tables)\n\u2022 Debugged and troubleshot the ETL packages by using breakpoint, analyzing process, catching error information by SQL command in SSIS.\n\u2022 Create reports with the use of SSRS to generate different types of reports such as tabular, matrix, drill down and charts reports with accordance with user requirement.\n\u2022 Maintained and updated existing reports, analyzed the SQL queries and logic behind them to improve the performance.\n\u2022 Helped deploy the report with scheduling, subscription, history snapshot configured and set up.\n\u2022 Developed in Agile environment throughout the project.\n\nEnvironment: SQL server 2008/2012, SQL Server Management Studio (SSMS), MS BI Suite (SSIS, SSRS)']",[u'Master of Science in Electrical Engineering'],[u'Stevens Institute of Technology'],degree_1 : Master of Science in Electrical Engineering
0,https://resumes.indeed.com/resume/9e1d1c323645eb59,"[u'Data Scientist\nBP - Chicago, IL\nJanuary 2017 to Present\n\u2022 Developed, tested, and validated pricing model with 85% confidence for forecasting margin based on numerous metrics using machine learning algorithms by leveraging Pandas, NumPy, SciPy.\n\u2022 Developed Decision Trees & Random Forest Machine learning models which continuously learns from the past data to make better predictions using Pandas, scikit learn, Numpy, Seaborn, and Python.\n\u2022 Performed various statistical analysis to discover co-relations & co-variance between various KPI\u2019s.\n\u2022 Performed K-fold cross validation to test, train, and validate model.\n\u2022 Developed visualizations for the model based on numerous metrics using Seaborn, Matplotlib, and Pandas.\n\u2022 Worked closely with business to gain insights on market for refining model.\n\u2022 Developed end-to-end data pipelines which include data extraction, data ingestion, data blending, publishing data on tableau server, and automation of pipelines under fast paced Agile Scrum environment.\n\u2022 Extracted data from SQL DB by writing complex SQL queries.\n\u2022 Wrote SQOOP scripts for moving data between Relational DBs, HDFS, and S3 storage.\n\u2022 Developed Hive pipelines in Data lake by implementing Partitioning, and bucketing concepts for improving performance.\n\u2022 Developed and Improved performance of Spark Pipelines by implementing repartition to manage resources efficiently.\n\u2022 Developed Tableau dashboards with Slice & Dice, Drill Down, and Drill Through capabilities by leveraging published data sources from tableau server.\n\u2022 Performed Tableau Server administrative tasks by creating AD groups, managed subscriptions for the views, adding users, managing security, and tracking performance of dashboards.\n\u2022 Worked on gathering requirements from business and designing architectural view for the use cases.', u'Graduate Research Assistant (GRA)\nUniversity of Texas At Tyler - Tyler, TX\nSeptember 2015 to December 2016\n\u2022 Worked on campus SQL Database by performing DBA activities & updating tasks.\n\u2022 Performed cleansing and transformation activities on data using Python Pandas library.\n\u2022 Developed statistical models for performing qualitative & quantitative analysis using Pandas, NumPy, SciPy.\n\u2022 Developed Tableau dashboards for analyzing user stats and break down activities.', u'Software Developer, Intern\nCloudAce Technologies - IN\nApril 2014 to August 2014\n\u2022 Worked on implementing RESTful webservices for populating and write back data from database.\n\u2022 Performed UNIT testing on production ready code.\n\u2022 Developed a master template document which is used as standard documentation template for all projects.']","[u'M.S. in Computer Science in Database Design', u'B.Tech in Computer Science Engineering in Database Management System']","[u'University of Texas Tyler, TX\nSeptember 2015 to December 2016', u'Jawaharlal Nehru Technological University Hyderabad, Telangana\nSeptember 2011 to May 2015']","degree_1 : M.S. in Compter Science in Database Design, degree_2 :  B.Tech in Compter Science Engineering in Database Management System"
0,https://resumes.indeed.com/resume/ae697c7cfa7b9a67,"[u'Data Scientist\nAffine Analytics - Bengaluru, Karnataka\nSeptember 2016 to June 2017\n\u2022 Built a threefold recommendation engine for a retail client and also assisted the member scoring model team to utilize the recommendation engine to improve their model efficiency by capturing the loyal customers.\n\u2022 Was involved in visualization of the recommendation engine using state of the art dimensionality reduction techniques like PCA and t-SNE.\n\u2022 Experimented on retail data to figure out purchasing patterns using Recurrent Neural Networks in Keras.', u'Data Scientist\nShieldsquare - Bengaluru, Karnataka\nMarch 2016 to May 2016\nPerformed data analysis on the web traffic data of a client and designed a generic algorithm by figuring out the patterns using\nhistograms to reduce the web traffic from bots.', u'Engineer\nAlcatel-Lucent - Bengaluru, Karnataka\nDecember 2013 to March 2016\n\u2022 Developed a Linear Regression Model which predicts the performance parameters of a Network Management System.\n\u2022 Automated the web page testing using Selenium with Java along with generation of test reports.\n\u2022 Was involved in the end to end Alarm Testing for the devices that were newly introduced into the network and generated test\nreports for the same along with deployment of various modules into customer environment.\n\u2022 Developed reports and dashboards which monitor the performance of various parameters in a network by pulling in data from HBASE using REST queries.\n\nPUBLICATION\n\u2022 Distributed Vector Representation Of Shopping Items, The Customer And Shopping Cart To Build A Three Fold']","[u'Master of Science in Data Science in Data Science', u'Bachelor of Engineering in Computer Science in Overall Percentage Score']","[u'Indiana University Bloomington, IN\nMay 2019', u'Bangalore Institute of Technology Bengaluru, Karnataka\nJune 2013']","degree_1 : Master of Science in Data Science in Data Science, degree_2 :  Bachelor of Engineering in Compter Science in Overall Percentage Score"
0,https://resumes.indeed.com/resume/643aec29c785bc02,"[u'Data Scientist, ProActive Web Services\nGreater Minneapolis Area\nJanuary 2018 to Present\nAs a Data Scientist, my main role is to identify data sources and automate collection process, analyze large\namounts of information to discover trends and patterns. I am working with the SEO team to best leverage Google\nAnalytics, web development team to perform website audits and do A/B testing, and the Data Science team\nimprove the customer relationship management of small businesses we work with using the clickstream data and website mining and analytics. I occasionally write articles for the website that I find interesting.\n\nPROJECTS:\n\u2022 Worked on a project with Federated Insurance in our System Analysis and Design class working to create a\npayroll system with Agile methodology. We created ERD, Database and UI design for the system.\n\u2022 Worked on personal projects of web scraping with Beautiful Soup and urllib using Python. Collected data for keywords ""McGregor"" and ""Mayweather"", analyzed the tweets based on frequency using bar chart, top 5\ncountries to support the boxers using pie chart, percentage of genders watching and tweeting about the fight etc.\n\u2022 Worked as team lead for our Technical Communications course project where we were trying to find solutions for\nUnited Health Group to decrease the number of chronic diseases. We chose Diabetes II as our chronic disease and are solved the problem by finding measures to control obesity which is the lead cause of Diabetes II.\n\nHACKATHONS/COMPETITIONS:\n\u2022 Worked on Type 2 Diabetes claims data (big data) provided by Optum to predict high-risk patients among diabetes 2 patients. The minneMUDAC hackathon was organized by MinneAnalytics.\n\u2022 On our first round, we went through three tables of judges where we got the 1st rank among other 22 participating\nteams. We got the ""Analytics Acumen Award"" on the second round where we presented our findings in an auditorium filled with judges, students, professors, employers and industry professionals.']",[u'in Data Science'],"[u'Minnesota State University Mankato, MN\nDecember 2018']",degree_1 : in Data Science
0,https://resumes.indeed.com/resume/e614a4d5d4fd5fbd,"[u'Data Scientist\nEcom Consulting Inc. - Seattle, WA\nJune 2013 to Present\nBackground\nVivek is a Data Scientist consultant leading Ecom Consulting Inc\u2019s data science practice. Based out of Seattle, he has expertise in unstructured data analytics, data science and machine learning algorithms from statistical modeling and data engineering to streaming data analytics. As an expert in predictive and statistical modeling, machine learning, and optimization, Vivek has helped numerous clients in developing better customer segmentation, failure prediction of machinery, fraud detection, marketing optimization, UX optimization.\nIn addition to extensive experience in many industries, he has degrees in Engineering, Statistics, Quality and Optimization from Case Western Reserve University and Arizona State University. He currently teaches a Data Analytics certification course at University of Washington, Seattle.\nProfessional and Industry Experience\nAs a consultant with Ecom Consulting Inc, Vivek has worked on:\nDeveloping customer segmentation tools, and product recommendation algorithms (recommendation engines) for the largest coffee retailer in the world.\nDeveloped real-time variable pricing models for a Fortune 10 retailer to encourage a customer to visit the store depending on their probability of visiting today\nApplying Deep Learning, XgBoost, RandomForest and traditional regression analyses to better explain consumer behavior, develop truly actionable KPIs, develop Customer Lifetime Value models for three Fortune 50 companies\nDeveloping machine learning based algorithm for real-time prediction of the failure of switches (signalling systems) on a leading train network in Austria. Leveraged streaming data analytics with vector similarity and xgboost to achieve 70% accuracy with 92% precision in predicting failures 7 days in advance\nLeveraging geo-spatial analytics to quantify the risk of accident associated with taking a certain route for goods transportation. Developed proprietary risk model for a startup based in midwest.\nAssisting \u201cinformation rich but insight poor\u201d companies in Healthcare, ecommerce, Retail industries by delivering deep, actionable machine and learning based insights via Tableau and Looker\nSix Sigma Process Development Projects in Financial and Manufacturing firms where he leveraged his knowledge of statistics and programming to develop fault resistant processes with inherent continuous improvement components\n\nPrior to Ecom, Vivek worked on the statistical modeling and optimization of large-scale problems, which contain hundreds of thousands of decision variables. He also teaches Intro to Data Science at local institute.\nTechnical Skills\nR, ggplot, h2o, SQL (Hive, Oracle, Presto, BigQuery), Google Analytics, Tableau, Looker, QlikView, Matlab, Excel (with PowerQuery, Data modeling, PowerBI, VBA),, BigQuery, Geo-Spatial analytics, Machine Learning - regression, XGBoost, random forest, classification, deep learning']","[u""Master's""]",[u''],"degree_1 : ""Masters"""
0,https://resumes.indeed.com/resume/996ecd91150c4ef5,"[u""Data Scientist and Research Assistant\nDiscovery Park, Purdue University\nJune 2017 to Present\n\u2022 Deliver statistical modeling and machine-learning based data analysis solutions to scientists at Bindley Bioscience Center.\n\u2022 Analyze longitudinal metabolic data to identify metabolites that show significant alterations due to food processing.\n\u2022 Increased identification recall of biomarkers used for detecting the onset of Parkinson's disease by 16%.\n\u2022 Developed a web-based application to perform unsupervised learning analysis and provide interactive 2D / 3D visualizations to aid with bioinformatics. Helped reduce annual cost by $20,000.\n\nRecent Projects""]","[u'Masters of Science in Industrial Engineering', u'']","[u'Purdue University\nMay 2018', u'Manipal Institute of Technology\nMay 2016']","degree_1 : Masters of Science in Indstrial Engineering, degree_2 :  "
0,https://resumes.indeed.com/resume/9f081663bd4858e4,"[u'Sr. Data Scientist\nElectronics Research Laboratory, Volkswagen of America Inc. - Belmont, CA\nAugust 2013 to Present\n\u2022 Automatic music/news/POI recommendation inside vehicle by using GPS location, passenger conversation, behavior and mood. Using machine learning and natural language processing(include BOW, POS, named entity tagging, LDA, and LSTM deep neural network).\n\u2022 Created and implemented Smart Navigation includes multiple category destinations, daily prediction navigation, and preferred route with a US patent granted(see patent session).\n\u2022 Created and implemented Automatically Street parking space finding and Street parking availability sharing through cloud system to reduce 1/3 traffic on Metropolitan area. The technology is based on Convolution Neural Network, TensorFlow, Caffe, Keras, OpenCV and Wireless communication.\n\u2022 Created and implemented Real Time road Lane Detection and Tracking on poor lane marker due to disrepair road or extreme weather condition for Autonomous Vehicle (Self-driving car) based on alternate feature and Deep Learning.\n\u2022 Created and implemented Smart state-of-charge monitor for electric vehicles based on Recurrent Neural Network and Seq2Seq forecast.', u'Research Scientist\n33Across Inc - Sunnyvale, CA\nJanuary 2011 to January 2013\n\u2022 Developed and implemented predictive models of user behavior data on websites, URL categorical, social network analysis, social mining and search content based on large-scale Machine Learning, Data Mining, classification and clustering for computational advertising and running on Amazon AWS.\n\u2022 Created and implemented on a new display ad target algorithms based on page view recency, stemming, frequency and ad view history to improve CTR/CVR over 30% and generates 10 million more revenues annually than previous algorithms.\n\u2022 Applied hypothesis testing to prove statistical significant improvement on new target algorithms based on A/B testing and bucket testing.', u""Senior Software Engineer\ndeCarta Inc. acquired by Uber on 2015 - San Jose, CA\nJanuary 2007 to January 2010\n\u2022 Designed/implemented GPS navigation algorithms based on Dijkstra's/A Star method using real time, predictive and historic traffic data together to improve routing and ETA(Estimate-Time-Arrival) accuracy.\n\u2022 Supported the Localization of different foreign languages for driving maneuver instruction to help expand product sale worldwide\n\u2022 Designed/implemented routing algorithms to support multiple alternate routes, hybrid route, avoiding area and exclusion zone capability before Google Map adapt the similar features.\n\u2022 Designed/implemented logistics truck routing algorithm, enhance turn-by-turn maneuver instruction. Supported LBS applications for wireless device, converted map data to vector for wireless transmission."", u'Scientist\nCarrierIQ Inc. acquired by AT&T on 2016 - Mountain View, CA\nJanuary 2005 to January 2007\n\u2022 Design/implement a Java/Eclipse/Matlab algorithm to find the cellular-based location when customer make a call on wireless communication based on TDOA (Time delay of arrival) and A-GPS instead of using GPS, applying tower geographical information and signal propagation model to increase accuracy.\n\u2022 Design/implement a Java/Eclipse/Matlab algorithm to analyze the root cause of bad call (call drop) based on decision tree.', u'Software Engineer\nNEC AMERICA INC - Cupertino, CA\nJanuary 2003 to January 2004\n\u2022 Using multiple category support vector machine to improve the accuracy of anti-spam algorithms.\n\u2022 Studying Online Social Networks of NEC\'s EAI system. Proposes using ""Normalized cut"" to find community structure in social network and using Support Vector Machine to learn and dynamically change the search profile of each user.\n\u2022 Design and implement NEC\'s multi client/server surveillance system. The responsibility includes decoder for MPEG4, motion tracking of human object based on Gaussian model algorithm, implementation of a database client with java multithread/JDBC to store and retrieve alert log from MySQL/PostgreSQL database server, and implementation of an alert email delivery based on TCP/IP/SMTP/MIME/POP3/IMAP4 protocol. A GUI based on Java Swing/JSP/ AWT/Applet/Servlet to allow users dynamically editing different security zone and stores information of security zone into MySQL database server through JDBC. Whole system is a web-based application implemented by using Microsoft .NET, Media Player, UML, XHTML, Java AWT, Applet, Servlet, NetBeans, Tomcat and J2EE/JSP. All assigned components are complete and the system has installed at SFO airport.', u'Staff Engineer\nGENOSPECTRA Inc. acquired by Affymetrix on 2008 - Fremont, CA\nJanuary 2001 to January 2002\n\u2022 Designed and implemented computer vision system for DNA microArray defect inspection. The system can inspect 10,000 spots in only one second with up to 99.9% accuracy and saves annually several millions expense for quality control. The software includes blob analysis, fast matrix transformation, and Computational Geometry.\n\u2022 Designed and implemented the spot detection and signal analysis of DNA microArray after hybridization, includes spot finding, automatic Grid alignment, background subtraction, Cy3 and Cy5 signal detection, and biclustering for gene expression.', u'Software Development- Image Analysis.\nCYTOKINETICS Inc. IPO on April 2004. - South San Francisco, CA\nJanuary 1999 to January 2000\n\u2022 Created and implemented advanced image process algorithm for cell analysis for high throughput drug screening. Whole process is scaled to run 7/24 on multiple platforms.\n\u2022 Automatic cell segmentation on medical image based on Adaptive threshold, Mathematics Morphology, Watershed and Voronoi Diagram algorithms.\n\u2022 Extracted and identified efficient features for different cell markers (Mitonic, Golgi and Microtubulin marker) for human cancer cell classification.\n\u2022 Supervised cancer cell classification using Decision Tree (CART), Bayesian, Neural Network, and Support Vector Machine(SVM) classifiers with a US patent granted(see patent).']","[u'PhD in Computer Science', u'M.S. in Electronic Engineering']","[u'Texas A&M University College Station, TX', u'Chung-Yuan Christian University Taiwan']","degree_1 : PhD in Compter Science, degree_2 :  M.S. in Electronic Engineering"
0,https://resumes.indeed.com/resume/51ac5b495a647eba,"[u""Data Review Chemist\nEnvironmental Synectics, Inc - Sacramento, CA\nJanuary 2017 to Present\nworking from personal home office)\n- Data Validation - Performed laboratory analytical data validation services using Synectics' web-based\nEnvironmental Data Management System (EDMS)."", u""Independent Environmental Consultant\nToxicological & Environmental Associates (TEA), Inc - Miramar Beach, FL\nJanuary 2013 to Present\nworking from personal home office)\n- Risk Assessment - Developed site-specific risk assessments and successfully negotiated risk-based\nclosure on sites in multiple states; developed appropriate risk-based remediation goals, with subsequent\nnegotiation for acceptance of those goals by the ruling regulatory agency.\n\n- Statistical Analysis - Degradation rate assessment of groundwater contaminants; parametric and non- parametric trend analysis; graphical presentation of data; goodness-of-fit distribution testing; derivation\nof appropriate exposure point concentrations; derivation of representative background concentrations.\n\n- Data Management - Consolidation of analytical data from various sources into a single, cohesive dataset;\ndatabase design and development; tabular and graphical presentation of analytical data.\n\nEMPLOYMENT HISTORY -- Independent Environmental Consultant ~~~~~~~)\n\n- Data Interpretation - Developed a novel methodology for groundwater data presentation, simplifying\nvisualization of the approach to target remediation goals in Natural Attenuation Monitoring. This\nmethodology was presented (Poster) at Battelle's Fourth International Symposium on Bioremediation and Sustainable Environmental Technologies (May 2017, Miami, Florida).\n- Groundwater Transport Modeling - Generated detailed contaminant transport model with future\nprojections for a site in Florida; successfully negotiated acceptance of the model by the Florida DEP.\n- Litigation Support - Provided research support and reporting for litigation cases involving potential\nexposure to various contaminants of concern.\n- Data Validation - Served as primary data validation expert; generated detailed review reports for the presentation of validation findings."", u'Senior Staff Scientist - Cardno ENTRIX\nENTRIX - Raleigh, NC\nJanuary 2007 to January 2013\nDatabase Manager - Designed and implemented a complex system of databases to document\nobservational data from multiple studies extending over a period of more than a year. These studies\nfocused on evaluating the oiling rate of coastal and pelagic birds and tracking the fate of oiled and unoiled birds following a major release of oil in the Gulf of Mexico. This database management role\ninvolved initial design of the database structure, oversight of data transfer from the field teams to a secure ftp site, extensive quality review with related documentation, and meaningful extraction of data as needed for the study team.\n- Data Validation - Performed laboratory data validation and quality assurance reviews for site\ninvestigations.\n- Mathematical Modeling - Devised a mathematical model to extend the period of record for stream flow\ndata at an existing gauging station based on historical stream flow data from multiple upstream gauging\nstations. Researched and applied an appropriate smoothing routine to remove noise in the data and determined the appropriate transport delay relative to each upstream gauging station. This work was\nused in construction of a complex hydrologic model for the Tar River Basin in North Carolina.\n- Litigation Support - Provided research support and review/comment on expert witness documents in multiple toxic tort cases. Designed and populated a large Microsoft Access database to manage toxic tort\ncase data for a law firm and provided ongoing data support to that law firm.\n- Environmental Impact - Provided Soils and Geology impact statements in a series of Environmental\nImpact Study reports.\n- Human Health Risk Assessment - Generated risk assessments for a series of gas station sites. These were\nsubmitted to various California Water Board offices, and full closure was obtained for several sites.\n- Health and Safety - Maintained OSHA HAZWOPER certification through annual 8-hour refresher training\nand annual physicals; also maintained annual training in First Aid, CPR, and operational safety. Served as the Health and Safety officer for the Cardno ENTRIX Raleigh office', u'Senior Staff Scientist\nARCADIS G&M - Raleigh, NC\nJanuary 1990 to January 2007\nHuman Health and Ecological Risk Assessment - Developed Risk Assessment reports in compliance with applicable environmental agency guidance, presenting relevant site details with appropriate exposure\nand risk assessment evaluation and conclusions for submittal to local, state, and federal environmental\nagencies. These reports often involved complex data analysis or evaluation of chemical fate-and transport scenarios to derive exposure point concentrations for the risk calculations.\n- Spreadsheet Design - Designed, created, and maintained a modular system of linked spreadsheets (in Microsoft Excel) for the calculation of human health risks or risk-based concentration goals. Also created\na detailed User Manual for guidance to co-workers in use of the spreadsheet system.\n- Statistical Analysis - Served as primary statistician on projects involving interpretation of chemical\nconcentration data from environmental media. Developed and successfully negotiated with the Illinois\nEPA an alternative statistical approach using probabilistic analysis of non-detect results in calculating\nexposure point concentrations, an approach now presented in their risk assessment guidance.\n- Database Design and Application - Designed, maintained, and applied databases of chemical\nconcentration data within Microsoft Access.\n- Quality control - Provided oversight and editorial review for quality control within a network of human\nhealth risk assessors located in multiple offices across the US and in South America and Europe.\n- Trainer/Mentor - Served as trainer and mentor to co-workers to assist and guide them in the generation\nof human health risk assessments.\n- Laboratory Data Validation - Provided data validation services in compliance with USEPA Contract\nLaboratory Program National Functional Guidelines (Organic and Inorganic Data Review). Temporarily\nrelocated (Albuquerque, New Mexico) to assist in validation of radionuclide data.\n- Health and Safety - OSHA 40-hour HAZWOPER training; maintained through annual 8-hour HAZWOPER\nrefresher training, along with annual First Aid, CPR, and operational safety training.']","[u'in Research', u'Ph.D in Environmental Consultant', u'Ph.D in Materials Science', u'Master of Science in Physical Chemistry', u'in Graduate Teaching Assistant', u'in statistics', u'Bachelor of Arts in Chemistry / Biology / Mathematics', u'Bachelor of Arts in Chemistry / Biology / Mathematics', u'in Research and application', u'Ph.D. degree in Materials Science in research', u'in Industrial Chemistry, General Biology, and Mathematics', u'in chemistry']","[u'University of North Carolina Chapel Hill, NC\nJanuary 2007 to January 2013', u'University of North Carolina Chapel Hill, NC\nJanuary 1990', u'University of North Carolina Chapel Hill, NC\nJanuary 1990', u'University of North Carolina Charlotte, NC\nJanuary 1985', u'University of North Carolina Chapel Hill, NC\nJanuary 1983 to January 1984', u'University of Pittsburgh Florence, AL\nJanuary 1980 to January 1983', u'University of North Alabama Raleigh, NC\nJanuary 1983', u'University of North Carolina Chapel Hill, NC\nJanuary 1983', u'North Carolina State University Newton, AL', u'Microelectronics Center of North Carolina', u'University of North Alabama Florence, IT', u'American Association of University Women']","degree_1 : in Research, degree_2 :  Ph.D in Environmental Consltant, degree_3 :  Ph.D in Materials Science, degree_4 :  Master of Science in Physical Chemistry, degree_5 :  in Gradate Teaching Assistant, degree_6 :  in statistics, degree_7 :  Bachelor of Arts in Chemistry / Biology / Mathematics, degree_8 :  Bachelor of Arts in Chemistry / Biology / Mathematics, degree_9 :  in Research and application, degree_10 :  Ph.D. degree in Materials Science in research, degree_11 :  in Indstrial Chemistry, degree_12 :  General Biology, degree_13 :  and Mathematics, degree_14 :  in chemistry"
0,https://resumes.indeed.com/resume/1009c3b0f818bcac,"[u'Business Assistant\nUniversity of Southern California - Los Angeles, CA\nMay 2016 to Present\nVisual Design\nConducted ASRs (Asset Summary Reports) for USC Roski and Iovine and Young\nAcademy.\nAssisted with payroll procedure Languages\nCompiled and interpreted data using Microsoft Excel\nEnglish', u'Student Relations Intern/Interpreter\nAshinaga Ikueikai - Tokyo, JP\nJune 2017 to June 2017\nDecember 2017\nPublic Speaking\nCreated a resource guide pertaining to Japanese culture.\nGuided international students throughout the Kansai area (Osaka, Kobe and Kyoto)\nTranslation\nTranslated and Interpreted from Japanese to English and vise versa for International\nstudents.\nMarketing\nEffectively presented and assisted other interns in understanding and assimilating into\nJapanese Culture Research', u'Data Scientist\nWe Are Onyx - Los Angeles, CA\nFebruary 2016 to May 2016\nFrench\nCompiled reviews and data from customers about products in our subscription boxes\nResearched extensively on product ingredients and tested effectiveness prior to giving\nit to our customers Hobbies\n\nYoga']","[u'Bachelor of Arts in International Policy and Management and East Asian', u'L.E.A.D Certificate']","[u'University of Southern California Los Angeles, CA\nAugust 2014 to May 2018', u'University of Southern California Los Angeles, CA\nDecember 2017']","degree_1 : Bachelor of Arts in International Policy and Management and East Asian, degree_2 :  L.E.A.D Certificate"
0,https://resumes.indeed.com/resume/147418ce45f90840,"[u""Data Scientist Intern\nEntropy Technology - New York, NY\nMarch 2018 to Present\n* Developed sequence labeling algorithm for Named Entities Recognition (NER), such as recognize people's names, positions, etc.\n* Conducted Regular Expression for information extraction, such as extracting email address; The accuracy reached 99.9%\n* Transformed resume from PDF, Word, and some other forms to txt file using Tika"", u'Financial Analyst Intern\nChina Huarong Asset Management Co - Beijing, CN\nJune 2017 to August 2017\n* Analyzed financial report, estimated credit risk for counterparties and performed DCF analysis to improve investment decisions\n* Conducted stress testing under different scenarios on 5 pharmaceutical companies and presented the report to the head of team\n* Built a database of stock mortgage rate for the stocks in Shanghai Stock Exchange for filtering high mortgage rate stocks', u'Business Analyst Intern\nHuarong International Trust Co - Beijing, CN\nJune 2014 to August 2014\nEstimated risk factors such as solvency ratio for potential financing and designed fund-raising strategies to proceed risk\nmanagement, presented to leadership team\n* Developed multi-factor linear model to predict the investment return in the future and run DCF analysis for better investment\ndecision']","[u'Master of Science in Financial Statistics & Risk Management', u'Bachelor of Science in Applied Mathematics & Statistics in Applied Mathematics & Statistics']","[u'Rutgers University New Brunswick, NJ\nSeptember 2016 to May 2018', u'Stony Brook University Stony Brook, NY\nAugust 2011 to May 2015']","degree_1 : Master of Science in Financial Statistics & Risk Management, degree_2 :  Bachelor of Science in Applied Mathematics & Statistics in Applied Mathematics & Statistics"
0,https://resumes.indeed.com/resume/0f511f00d462d586,"[u""Data Scientist\nAmerican Traffic Solutions\nApril 2014 to Present\nIdentified high risk locations to deploy 1,700+ safety enforcement systems by developing, testing, and implementing a multivariate log- linear regression model to predict red light running violation volume.\n\nOptimized enforcement locations for $1.4M yearly recurring revenue client using machine learning classification and regression modeling.\n\nDeveloped density heat mapping to visually identify areas with highest transportation safety risk based on the spatial distribution of numerous inputs using 2-dimensional Gaussian kernels and GIS tools in R studio.\n\nQuantified the safety benefit of red light photo-enforcement programs in a nationwide study including 70+ clients utilizing two-sample t- testing.\n\nSupported relationships with 30+ clients by developing automated analysis which quantified impact of safety enforcement on fatal crash rate trending in their jurisdiction.\n\nProvided key component of RFP response that resulted in contract awarded for 26 enforcement systems by showing that ATS systems had a 24% higher issuance rate than competing bidder's systems.\n\nEducated client stakeholders by creating and presenting client facing materials to city council members and city engineers."", u'Site Selection Technician\nAmerican Traffic Solutions\nFebruary 2012 to April 2014\nIncreased team efficiency by decreasing average time required to analyze a location from 45 minutes to 30 minutes (33%) by building standardized tools and templates within Excel.\n\nDecreased install tracking lag from 4 weeks to 1 week and forecasted future install volumes by creating a weekly update tool based on statistical analysis of historical install trending.\n\nEnhanced violation prediction modeling by integrating considerations for existing infrastructure density via GIS analysis.', u'Teaching Assistant (Physics 252)\nArizona State University\nAugust 2011 to December 2011']",[u'Bachelor of Science in Physics in Physics'],[u'Arizona State University\nJanuary 2009 to January 2011'],degree_1 : Bachelor of Science in Physics in Physics
0,https://resumes.indeed.com/resume/cccd9cf3da9f6471,"[u'LEAD ANALYTICS MODELER\nNATIONAL GENERAL INSURANCE COMPANY - Chicago, IL\nJanuary 2016 to Present\n\u2022 Company leader and subject matter expert, accountable for the creation of statistical modeling frameworks and quantitative software development applied to rating plans for personal lines homeowners\n\u2022 Created a proprietary suite of predictive modeling tools using R and SAS for sophisticated GLM development and data visualization, saving the company thousands of dollars in potential licensing fees\n\u2022 Developed and executed the strategy roadmap for personal lines homeowners predictive analytics including the prioritization of work to ensure appropriate focus on highest value projects\n\u2022 Built models for predicting property hazards using R and SAS, which reduced company costs spent on ordering home inspections by 20 percent, saving an average of $500K annually\n\u2022 Facilitated a collaborative partnership and directed a team of 3 in all phases of the development of a new GLM based rating plan for by-peril pricing, underwriting, and risk segmentation\n\u2022 Forged partnerships with external data vendors, significantly improving the quality of internal data through the intelligent purchases of additional data which had the greatest positive impact\n\u2022 Established project management frameworks, mitigating and solving for project risks', u""DATA SCIENTIST\nCHUBB INSURANCE COMPANY - Philadelphia, PA\nJanuary 2015 to January 2016\n\u2022 Repeatedly recognized for top performance through fast-track promotion and selection for high-priority initiatives\n\u2022 Managed 3 junior data scientists in accomplishing departmental goals by implementing end to end analytics solutions\n\u2022 Identified necessary staffing and skill sets to successfully execute on strategies\n\u2022 Developed talent management strategy by identifying skill gaps and devising training and hiring plans\n\u2022 Demonstrated ability to train team members and develop skill sets\n\u2022 Exercised great influence over broad organizational and technological change to facilitate easier adoption and integration of product and analytic initiatives\n\u2022 Accountable for the design and implementation of the company-wide cloud based analytics environment using Hortonworks' distribution of Hadoop across Amazon Web Services (AWS) and Microsoft Azure\n\u2022 Acted as the expert on data science solutions using Python, Hadoop, Hive, and Spark while leading training sessions, educating teams across departments, and promoting the migration to open source technologies and distributed computing across the company\n\u2022 Coordinated an engagement with an external consulting firm, mining social media for insights around drug clinical trials, which then informed underwriting decisions involving major pharmaceutical companies"", u'PREDICTIVE MODELING ANALYST\nCHUBB INSURANCE COMPANY - Philadelphia, PA\nJanuary 2013 to January 2015\n\u2022 Designed and built the Chubb Text Analytics Dashboard using Solr, Javascript, and HTML, which was a huge innovation for the company and leveraged to recover over $800K in losses by allowing real-time text search of adjuster notes from over 2M claims\n\u2022 Utilized topic modeling and logistic regression to develop proprietary approaches in Python for extensive mining of unstructured text data from insurance claim notes, supporting the claims department by identifying keywords in text data that correlated with increased claim severity\n\u2022 Accountable for the end to end development of generalized linear models (GLMs) across multiple business units (including personal and commercial auto and homeowners) directly leading to cost-savings and product optimization through improved pricing and underwriting primarily using SAS and Emblem with a global impact', u'HIGH SCHOOL MATHEMATICS TEACHER\nTHE SCHOOL DISTRICT OF PHILADELPHIA - Philadelphia, PA\nJanuary 2007 to January 2013\n\u2022 Developed data-driven analytical methods to track student growth and effectively target their strengths and weaknesses at an individual level, with a direct influence on student achievement\n\u2022 Effectively communicated and influenced skills through written and oral interpretations of highly specialized terms and data to varying audiences with different levels of expertise\n\u2022 Applied creativity and originality in a wide range of loosely defined complex situations']","[u'MASTER OF APPLIED STATISTICS in APPLIED STATISTICS', u'BS in MATHEMATICS']","[u'Penn State University\nJanuary 2013', u'Drexel University\nJanuary 2005']","degree_1 : MASTER OF APPLIED STATISTICS in APPLIED STATISTICS, degree_2 :  BS in MATHEMATICS"
0,https://resumes.indeed.com/resume/cf1a680648725716,"[u'Data Analyst\nDecision Sciences\nDecember 2017 to Present\n\u2022Perform ad- hoc quantitative analyses to answer unique and challenging business questions\n\u2022Provide analytic insights to drive business problems by implementing predictive models.\n\u2022Deploy automated ETL processes for loading multi- terabyte internal data warehouse used to store marketing data using SSIS\n\u2022Manipulate data using T-SQL and Alteryx to fix messy data for predictive model building\n\u2022Feature engineer attributes to be fed to predictive models\n\u2022Extract data from marketing ad server using python API', u'Data Scientist Intern\nSiemens Corporate Technology\nAugust 2016 to October 2016\n\u2022Analyzed loads of data generated by the sensors of the wind turbines and presented insights.\n\u2022Implemented the data preprocessing phase for predicting the power generation of the wind turbines\n\u2022Developed an algorithm for segmenting the sensor data using R', u'Data Scientist Intern\nThyssenKrupp Elevator Corporation\nMay 2016 to August 2016\n\u2022Developed predictive models using R for the prevention of customer churn\n\u2022Built a tool for continuous monitoring of customer contracts in R Shiny\n\u2022Determined statistically significant dimensions using Tableau and determined profitability by dimensions and facts', u'Software Engineering Analyst -Java Developer\nAccenture\nJuly 2014 to June 2015\n\u2022Developed the flow of business processes for ticket handling using JBoss, leading to 40% reduction in the required effort\n\u2022Designed and implemented database architecture and developed SQL scripts and procedures to maintain database\n\u2022Designed interactive dashboard and reports using Performance Analytics feature of ServiceNow for 1 million incident tickets']","[u'M.S., Information Technology in Management']","[u'The University of Texas at Dallas Dallas, TX\nMay 2017']","degree_1 : M.S., degree_2 :  Information Technology in Management"
0,https://resumes.indeed.com/resume/d0b9d483d2e3db10,"[u'DATA SCIENTIST\nIkvox Software - Hyderabad, Telangana\nJanuary 2016 to December 2016\n\u2022 Derived actionable insights by processing large sets of structured and unstructured data by performing Statistical Analysis.\n\u2022 Processed the raw data from CSV files into organized form by applying Data cleaning techniques using R and Python.\n\u2022 Built predictive models by applying supervised and unsupervised machine learning algorithms like Regression, SVM & KNN.\n\u2022 Deployed the models into production using frameworks like Flask and Django on Microsoft Azure & AWS.\n\u2022 Created visualizations using BI tools like Tableau, Qlikview and Zoho reports for communicating insights to Stakeholders.\n\u2022 Automated the process of feedback evaluation by calculating the sentiment of the users based on text and numerical data.', u'SYSTEMS ENGINEER\nInfosys Ltd - Hyderabad, Telangana\nAugust 2014 to January 2016\n\u2022 Designed web & mobile applications using HTML5, JavaScript, JSP, CSS, BootStrap and developed PL/SQL scripts by writing procedures for retrieving and scheduling data imports from relational databases.\n\u2022 Ensured the best performance of applications by identifying bottlenecks and devise a solution to solve it. Analyzing performance of applications and servers by using various monitoring tools.']","[u'Master of Science in Computer Science in Computer Science', u'in Electrical & Electronics Engineering']","[u'UNIVERSITY OF NORTH CAROLINA AT CHARLOTTE Charlotte, NC\nMarch 2017 to Present', u'JAWAHARLAL NEHRU TECHNOLOGICAL UNIVERSITY Hyderabad, Telangana\nAugust 2010 to May 2014']","degree_1 : Master of Science in Compter Science in Compter Science, degree_2 :  in Electrical & Electronics Engineering"
0,https://resumes.indeed.com/resume/8a6098b7a849c7a8,"[u'Data Scientist\nNorthwestern Mutual - Milwaukee, WI\nJanuary 2017 to Present\n\u2022 Work independently and collaboratively throughout the complete analytics project lifecycle including data extraction/preparation, design and implementation of scalable machine learning analysis and solutions, and documentation of results.\n\u2022 Performed statistical analysis to determine peak and off-peak time periods for ratemaking purposes.\n\u2022 Participated in all phases of data mining; data collection, data cleaning, developing models, validation and visualization.\n\u2022 Conducted analysis of customer data for the purposes of designing rates.\n\u2022 Identified root causes of problems, and facilitated the implementation of cost effective solutions with all levels of management.\n\u2022 Developed Regression Models based on data provided by the client.\n\u2022 Application of various machine learning algorithms and statistical modeling like decision trees, regression models, clustering, SVM to identify Volume using scikit-learn package in Python.\n\u2022 Worked on different data formats such as JSON, XML and performed machine learning algorithms in Python.\n\u2022 Hands on experience in implementing Naive Bayes and skilled in Random Forests, Decision Trees, Linear and Logistic Regression, SVM, Clustering, Principle Component Analysis.\n\u2022 Performed K-means clustering, Regression and Decision Trees in R.\n\u2022 Worked on Na\xefve Bayes algorithms for Agent Fraud Detection using R.\n\u2022 Have knowledge on A/B Testing, ANOVA, Multivariate Analysis, Association Rules and Text Analysis using R.\n\u2022 Partner with technical and non-technical resources across the business to leverage their support and integrate our efforts.\n\u2022 Partner with infrastructure and platform teams to configure, tune tools, automate tasks and guide the evolution of internal big data ecosystem; serve as a bridge between data scientists and infrastructure/platform teams.\n\u2022 Worked on Text Analytics and Naive Bayes creating word clouds and retrieving data from social networking platforms.\n\u2022 Pro-actively analyze data to uncover insights that increase business value and impact.\n\u2022 Support various business partners on a wide range of analytics projects from ad-hoc requests to large-scale cross-functional engagements.\n\u2022 Prepared Data Visualization reports for the management using R.\n\u2022 Approach analytical problems with an appropriate blend of statistical/mathematical rigor with practical business intuition.\n\u2022 Hold a point-of-view on the strengths and limitations of statistical models and analyses in various business contexts and is able to evaluate and effectively communicate the uncertainty in the results.\n\u2022 Application of various machine learning algorithms and statistical modeling like decision trees, regression models, SVM, clustering to identify Volume using scikit-learn package in python.\n\u2022 Approach analysis in multiple ways in order to evaluate approaches and compare results.\nEnvironment: R, Python, ODS, Mysql, DB2, Metadata, MS Excel, GGplot2, Matplotlib, json, Machine Learning Algorithms, Mainframes MS Vision.', u'Jr Data Scientist\nCapital One - Richmond, VA\nSeptember 2015 to December 2016\n\u2022 Used pandas, numpy, scipy, matplotlib, sci-kit-learn, NLTK in Python for developing various machine learning algorithms.\n\u2022 Participated in all phases of data mining; data collection, data cleaning, developing models, validation, visualization and performed Gap analysis.\n\u2022 Implemented Agile Methodology for building an internal application.\n\u2022 Programmed a utility in Python that used multiple packages (scipy, numpy, pandas)\n\u2022 Implemented Classification using supervised algorithms like Logistic Regression, Decision trees, KNN, Na\xefve Bayes.\n\u2022 Apply different Machine Learning algorithms/methods on data sets to predict credit risk, fraud detection.\n\u2022 Updated Python scripts to match training data with our database stored in AWS Cloud Search, so that we would be able to assign each document a response label for further classification.\n\u2022 Data transformation from various resources, data organization, features extraction from raw and stored.\n\u2022 Interaction with Business Analyst, SMEs, and other Data Architects to understand Business needs and functionality for various project solutions\n\u2022 Regularly accessing JIRA tool and other internal issue trackers for the Project development.\n\u2022 Gathering all the data that is required from multiple data sources and creating datasets that will be used in the analysis.\n\u2022 Performed data cleaning and imputation of missing values using R.\n\u2022 Created visualization on regular basis using ggplot2 and Matplotlib.\n\u2022 Interacted with the other departments to understand and identify data needs and requirements and work with other members of the IT organization to deliver data visualization and reporting solutions to address those needs.\n\nEnvironment: R, Python, Informatica, JIIRA, Teradata, ODS, Mysql, DB2, MS Excel, GGplot2, Matplotlib, json, Machine Learning Algorithms.', u'Data Analyst\nCapital One\nAugust 2014 to July 2015\nLocation: India\n\u2022 Worked on metadata clean-up and processing at regular intervals for better quality of data.\n\u2022 Involved in data load/export utilities like BTEQ, Fast Load, Multi Load, Fast Export and UNIX/Mainframes environments.\n\u2022 I am responsible to build and maintain well-managed data solutions and deliver capabilities to tackle business problems.\n\u2022 Partner with the business to provide consultancy and translate the business needs to design and develop tools, techniques, metrics, and dashboards for insights and data visualization.\n\u2022 Drive an understanding and adherence to the principles of data quality management including metadata, lineage, and business definitions.\n\u2022 Work collaboratively with appropriate Tech teams to manage security mechanisms and data access governance.\n\u2022 Build and execute tools to monitor and report on data quality.\n\u2022 Answering the technical queries, driving the product initiatives and metric collection and analysis.\n\u2022 Generate daily, weekly and monthly reports as per the specifications/requirements by business users.\n\u2022 Discuss business solutions with client business team, resolving existing problems and improving the report quality.\n\u2022 Performed Source to Target data analysis and data mapping.\n\u2022 Created SQL queries to validate Data transformation and ETL Loading.\n\u2022 Building, publishing customized interactive reports and dashboards, report scheduling using Tableau server.\nEnvironment: Teradata15.10, UNIX, Informatica 9.6, MS SQL Server 2012, SSIS & SSRS, MS Office, Microsoft Visio, Erwin Tool, Tableau.', u""Data Analyst\nAccenture\nJanuary 2013 to July 2014\nLocation: India\n\u2022 Created Daily, Weekly, monthly reports related to the collection and financial departments by using Teradata, Ms Excel, UNIX and Business Objects.\n\u2022 Wrote Bteq, SQL scripts for large data pulls and adhoc reports for analysis.\n\u2022 As per Ad-hoc request created History tables, views on the top of the Finance Data mart/ production databases by using Teradata, BTEQ, and UNIX.\n\u2022 Created tables, indexes, views, snap shots, database links to see the information extracted from SQL files.\n\u2022 Worked on tables (Like Set, Multiset, Derived, Volatile, Global Temporary), Macros, views, procedures using SQL scripts.\n\u2022 Worked on Testing Primary Indexes and SKEW ratio before populating a table, used sampling techniques, Explain Plan in Teradata before Query Large tables with billons of records and with several Joins.\n\u2022 Worked and extracted data from various database sources like Oracle, SQLServer, DB2, and Teradata.\n\u2022 Involved in Testing and creating user test cases to test the Data marts and various reports.\n\u2022 Worked closely with Database Admin's and Data Warehouse personnel.\n\u2022 Performed data analysis to look at credit exposures, risk profiles and trends of customers before making any loan granting decisions.\n\u2022 Extensively worked on Unix Shell Scripts for automating the data mart scripts, checking daily logs, sending e-mails, purging the data, extracting the data from legacy systems, archiving data in flat files to Tapes, setting user access level, automating backup procedures, Scheduling the Jobs, checking Space usage and validating the data loads.\n\u2022 Optimized SQL statements for better performance using Explain PLAN Method.\n\u2022 Creating SQL files indexes and calculate the table spaces for each table, tuning the SQL queries (based on plan tables) . Grant and revoke table access to users.\nEnvironment: Teradata SQL Assistant 7.0, BTEQ, BO XI r-2 (Designer, WebIntelligence,\nCrystal reports), UNIX Shell scripts, SQL, PL/SQL, SQL *Loader, PowerPoint, Excel.""]","[u""Bachelor's in Computer Science""]",[u'JNT University\nJanuary 2013'],"degree_1 : ""Bachelors in Compter Science"""
0,https://resumes.indeed.com/resume/72a4cba8dabd5b2b,"[u'Data Scientist\nMerck & Co - Alpharetta, GA\nJanuary 2016 to Present\nThe project utilized machine learning methods, and data mining technologies in developing innovative solutions in pharmaceutical industry. The project involved working with cross-functional teams, data engineering and stakeholders to develop ground-breaking solutions to finding new pharmaceuticals.\n* Outlined plan of execution to leverage predictive models and other machine learning algorithms as part of company technology strategy\n* Performed machine learning and statistical analysis methods, such as clustering and classification, collaborative filtering, association rules, sentiment analysis, topic modeling, multivariate regression analysis, statistical inference, and validation methods.\n* Performed complex modeling, Monte Carlos simulation, and used predictive model to search on virtual database of molecular and clinical and preclinical data.\n* Increased model performance by hyperparameters optimization.\n* Summarized data through various tabular and visual modes for preliminary discovery analysis.\n* Conducted and interpreted multivariate analyses examples, including regressions with various distributions and duration models.\n* Worked with diverse data sets, identify and develop valuable new sources of data and collaborate with product teams to ensure successful integration.\n* Created customized reports and processes in SAS and R.\n* Evaluated research for practical application by utilizing preclinical data sets and clinical trial modeling methods.\n* Pulled data from data lakes comprised of varied sources such as SQL, NoSQL structured and unstructured data.\n* Developed mixed data models using mathematical/statistical analytical methods.\n* Used Python (NumPy, SciPy, Pandas, SciKit-Learn, Seaborn, NLTK) and Spark 1.6 / 2.0 (PySpark, MLlib) to develop variety of models and algorithms for analytical purposes.\n* Developed an algorithm that can identify fatal assessments that are expected to Fail under central review.\n* Used Statistical methods to analyze the performance of cross different clinical and lab research sites.\n* Predictive analytics used to predict number of days to reach the target number of sites for a clinical study.\n* Built & tested decisions mechanism about advancing certain compounds in the larger context of optimizing drug development, based on complex data pipelines and machine learning models.\n* Created a pre-discovery system to shorten the preclinical trail processes.', u'Data Scientist\nAvalere Health - Washington, DC\nAugust 2014 to December 2015\nResponsible for applications of machine learning to IoT diagnostics on data pulled from both structured and unstructured data and created as data lakes using Hadoop big data systems. The project involved Health Economics and Advanced Analytics (HEAA) working with large healthcare datasets to provide complex data extracts, programming, and analytical modeling capabilities. The HEAA practice consists of staff with specialized experience, such as research scientists, health service researchers, health economists, clinicians, and project managers, who conceptualize, execute, and deliver qualitative, analytical, and strategy-based work products for clients.\n\n* Developed an algorithm that can identify ""bad"" assessments that are expected to fail under central review.\n* Initiated research into the development of social networking analytical system targeting the pharmaceutical industry using NLP. The automated system is intended to identify social sentiment and key opinion leaders, including their degree of connectedness and reach in social media and the blogosphere.\n* Managed BI and analytics for a multi-media-based web portal targeting patients, physicians, health care resources, and researchers associated with HIV clinical research.\n* Processed huge datasets (over billion data points, over 1 TB datasets) for data association pairing and Provided insights into meaningful data association and trends.\n* Developed pipelines for test data.\n* Enhanced statistical models (linear mixed models) for predicting the best products for commercialization using Machine Learning Linear regression models, KNN and K-means clustering algorithms.\n* Built machine learning models on independent AWS EC2 server to enhance data quality.\n* Handle Unstructured Data to derive some information from which helps in the development of the company.\n* Used Text Mining and NLP techniques find the sentiment about the organization.\n* Developed unsupervised machine learning models in the Hadoop/Hive environment on AWS EC2 instance.\n* Used clustering technique K-Means to identify outliers and to classify unlabeled data.\n* Performed Data Profiling to learn about behavior with various features such as traffic pattern, location, time, Date and Time etc.\n* Application of various machine learning algorithms and statistical modeling like decision trees, regression models, neural networks, SVM, clustering to identify Volume using SciKit-Learn package in Python.\n* Utilized Spark in Scala, Hadoop, Cassandra, MongoDB, Spark MLlib in Python, Spark SQL, along with broad variety of machine learning methods including classifications, regressions, dimensionally reduction etc.\n* Utilized the engine to increase user lifetime by 45% and triple user conversations for target categories.\n* Handled importing data from various data sources, performed transformations using Hive, Map Reduce, and loaded data into HDFS.', u'Data Scientist\nExxon Mobile Corporation - Irving, TX\nOctober 2012 to August 2014\nDeveloped and applied methods to identify, collect, process, and analyze large volumes of data to build and enhance products, processes, and systems. Conducted data mining and retrieval, and apply statistical and mathematical analyses to identify trends, solve analytical problems, optimize performance, and gather intelligence. Visualized information using a range of tools (e.g., GIS tools, RStudio), develop scripts and algorithms, create explanatory analysis and predictive models, and perform comparative analyses to address complex problems.\n* Responsible for reporting of findings that will use gathered metrics to infer and draw logical conclusions from past and future behavior.\n* Performed Multinomial Logistic Regression, Random Forest, Decision Tree, SVM to classify package is going to deliver on time for the new route.\n* Used Principal Component Analysis & Factor Analysis in feature engineering to analyze high dimensional data in MatLab.\n* Performed data analysis by using SQL to retrieve the data from Hadoop cluster.\n* Used R machine learning library to build and evaluate different models.\n* Implemented rule-based expertise system from the results of exploratory analysis and information gathered from the people from different departments.\n* Collected data needs and requirements by Interacting with the other departments.\n* Created various types of data visualizations using Python and Tableau schemas.\n* Communicated the results with operations team for taking best decisions.\n* PL/SQL, Python 2.x, HDFS, Hadoop, Impala, Linux, Tableau Desktop, MatLab, SQL Server 2012, Microsoft Excel, MS Visio, Rational Rose.\n* Utilized Python to develop and execute Big Data Analytics and Machine learning applications, prototyped machine Learning use cases under SciKit-Learn.\n* Led technical implementation of advanced analytics projects and defined the time-series approaches.\n* Develop new and effective analytics algorithms and wrote the key pieces of mission-critical source code.\n* Implemented advanced machine learning algorithms including regression trees, kernel PCA, among other methods in Scala, Hadoop 1.x, R and in other tools and languages as needed.', u""Data Scientist\nAmerisourceBergen Corp - Chesterbrook, PA\nSeptember 2010 to October 2012\nThis project focused on manufacturing strategies to make the most of distribution network ensuring products reach patients efficiently and securely.\n* Directed and provided the vision and design for a robust, flexible, scalable business intelligence (BI) solution including an enterprise data warehouse (EDW) to service all business units.\n* Revitalized and promoted the importance of data derived insights to support strategic planning and tactical decisions.\n* Created and socialized the use of key metrics/KPIs using optimized SQL query.\n* Provided analytics support, to contribute to the first profitable quarters in six years.\n* Developed strategies to access, integrate, and analyze data from disparate sources/platforms for transactions involving credit card, debit card, check, cash, and other payment methods.\n* Directed team of developers to integrate data from three platforms, formerly three different companies, into a single database as a preliminary step toward developing a dedicated EDW.\n* Created data models and identified potential savings associated with a recommended switch of select clients to a different processor prior to the fiscal year's peak processing month.\n* Segmented client population to identify data clusters in need of audit scrutiny.\n* Identified low profitability and negative net margin clients, potentially associated with previously unaddressed, and unidentified internal billing discrepancies\n* Modeled data and calculated impact of considered processes on business.\n* Modeled impact of Durbin Amendment (interchange fee reform) on revenue, to identify clients negatively affected by these legislated changes, and likely candidates for processing and/or contract restructuring.\n* Calculated customer lifetime value (CLV) to identify most valuable clients worthy of elite status, exceptional service/handling, and profiling for key driver identification.\n* Integrated marketing data with card transaction data for improved marketing lifts and better targeting strategies.\n* Calculated new product pricing schemes and modeled into the database.""]","[u""Bachelor's in Electrical Engineering""]","[u'North Dakota State University Fargo, ND\nMay 2015']","degree_1 : ""Bachelors in Electrical Engineering"""
0,https://resumes.indeed.com/resume/20219cd63dd9cb51,"[u'Data Scientist\nTP-AHA - Washington, DC\nOctober 2013 to Present\n\u2022 Designed analysis techniques to extract meaning from big data and machine learning solutions.\n\u2022 Conducted data extraction, data wrangling and analyzed complex and multi-source datasets using advanced querying, statistical analytics and visualization tools.\n\u2022 Designed and implemented data analytics and visualization for client/business exploration and prediction.\n\u2022 Developed Machine Learning solutions by testing the models for accuracy and precision on various classifiers.\n\u2022 Conducted data mining, data modeling, statistical analysis, business intelligence gathering and trend analysis.\n\u2022 Produced and presented data analytic and policy analysis reports to the management that supports on decisions for high-priority, agency initiatives for improvement and realignment.\n\u2022 Created visually interactive analytical dashboards in Tableau, Matplotlib and Seaborn for data reporting and decision-making.\n\u2022 Developed advanced customer segmentation analysis and visualization on Tableau.\n\u2022 Applied logistic regression model to client service use records to predict service use based upon various clients\u2019 features.\n\u2022 Implemented demand-forecasting models, which improved upon forecast accuracy and reduced service provision inefficiencies.', u'Senior Data Specialist\nUNFPA - International\nMay 2010 to September 2013\n\u2022 Researched, extracted and transformed information from raw data into meaningful and actionable data.\n\u2022 Used various internal and external data sources to collect, aggregate and analyze data.\n\u2022 Analyzed and interpreted complex big data using various statistical methods.\n\u2022 Identified best variables to evaluate a range of analysis and scope of information sought to answer questions.\n\u2022 Identified trends and insights for the organization\u2019s financial and programmatic activities.\n\u2022 Produced various visually impactful visualizations and deep analytical reports for decision-makers and stakeholders.\n\u2022 Worked in a cross-functional team to develop a service model and implemented the methodology to explain complex programs.\n\u2022 Trained junior data analysts on research techniques, statistical methods and analytic tools.', u'Data Analyst\nWorld Health Organization - International\nSeptember 2008 to April 2010\n\u2022 Extracted and transform raw health care or public health data into meaningful and actionable information.\n\u2022 Analyzed and interpreted health care data to identify important feature or key metrics.\n\u2022 Analyzed the factors related to prevalence and incidence rates of health problems.\n\u2022 Built statistical models using historical data to depict and predict health trends.\n\u2022 Identified, measured and recommended policy improvement strategies for better program activities.\n\u2022 Identified trends and used the information to help the organization make health program adjustments to reduce disease prevalence.\n\u2022 Interpreted information from a series of database investigations and policy analysis to make predictions and recommendations.\n\u2022 Created and discussed analytic results with various members of management in the organization, and led staff members to realize the significance of the analysis results and policy implications.', u'Analyst\nJune 2004 to August 2008\n\u2022 Analyzed quantitative data gathered to develop an understanding of customer behavior, demographics and socioeconomic situation. Presented data analytics results that helped guide decisions of the agency.\n\u2022 Ensured accurate and consistent statistical analysis by meticulously going through the data and validating results.\n\u2022 Developed agency\u2019s guidelines and best practices based on information learned through an analysis of client data.\n\u2022 Determined additional means of organization improvement with partners by using data collected by project evaluation surveys.\n\u2022 Developed company guidelines and best practices based on information learned through an analysis of consumer behavior data.\n\u2022 Created and presented executive analytic reports to show the trends in the data.']","[u""Doctorate's""]",[u''],"degree_1 : ""Doctorates"""
0,https://resumes.indeed.com/resume/643ff555608260f7,"[u""Data Scientist Co-op\nTouchkin - Bengaluru, Karnataka\nJanuary 2017 to April 2017\n\u2022 Managed data pipeline on Excel/Python to clean user data, and removed duplicates, blanks, and corrupted data\n\u2022 Collaborated with head data scientist in identifying key predictors of depression via independent t-tests\n\u2022 Developed machine learning model from key passive mobile data in predicting depression (as given from PHQ-9)\n\u2022 Achieved 85% accuracy with model based off pipelining PCA, logistic regression and SVM with Gaussian kernel\n\u2022 Improved chatbot's ability to detect 'ambiguous' messages by over 50% via a modified ELIZA file and NLTK"", u'Data Visualization & BI Co-op\nAmazon Robotics - Boston, MA\nJanuary 2016 to June 2016\n\u2022 Conducted interviews with Technical Stability managers to identify relevant KPIs within teams\n\u2022 Developed high-impact tableau data visualizations as part of Scrum team to quickly identify areas of team inefficiencies\n\u2022 Pinpointed system performance inefficiencies in team that could be resolved with BASH automation scripts\n\u2022 Deployed multiple BASH scripts that collectively saved over a hundred engineer hours per year', u'Systems Engineer Co-op\nGE Aviation - Dayton, OH\nJanuary 2015 to June 2015\n\u2022 Identified bugs in existing FPGA model of real-time simulation of aircraft electric power systems on Opal-RT\n\u2022 Collaborated with researchers in resolving bugs by piecewise black-box simulation via test signals and oscilloscopes\n\u2022 Programmed relevant MatLab, Simulink, and C++ files to streamline propagation of test signals through model', u'IT Technician\nResNet Northeastern - Boston, MA\nJanuary 2014 to December 2014\n\u2022 Provided campus technology support for students and faculty, including individual computer and in-person support\n\u2022 Resolved hundreds of tickets regarding virus removal, reformatting, network issues, file recovery, and hardware\n\u2022 Earned CCENT, CompTIA A+, and CompTIA Network+ professional certifications']","[u'Bachelor of Science in Physics and Computer Engineering', u'Diploma']","[u'Northeastern University Boston, MA\nSeptember 2013 to May 2018', u'Iolani High School Honolulu, HI\nMay 2013']","degree_1 : Bachelor of Science in Physics and Compter Engineering, degree_2 :  Diploma"
0,https://resumes.indeed.com/resume/538d6d3ba026f4cb,"[u'Data Scientist\nCBS INTERACTIVE - New York, NY\nNovember 2015 to Present\nSupport Finance, Yield, Ad Operations, and Revenue Operations from a data perspective which includes data exploration and analysis, dashboard creation, report generation and process automation\n\nBuilt a revenue forecast model for CBS All Access product using time-series models and survival analysis in order to aid Finance with budgeting, forecasting and reporting activities\n\nDeveloped machine learning models for user segmentation, subscriber churn prediction, trial conversion prediction, and virality studies for CBS All Access product\n\nCreated a streams and advertiser impressions forecast model for CBS video content using time-series models in order to help Yield manage inventory, capacity and sell-through', u'Data Scientist\nBOREDOMTHERAPY.COM - New York, NY\nFebruary 2015 to October 2015\nBuilt data and ETL pipelines using Python and 3rd party APIs for collecting daily and intraday data from Facebook, Double Click for Publishers and Google Analytics and ingesting into a MySQL database for warehousing\n\nDesigned dashboards to monitor KPIs from Facebook advertising, promote partners and website performance\n\nBuilt machine learnings models for NLP to uncover impact of keywords on engagement and CPMs\n\nResponsible for data integrity and consistency by means of tools built for daily monitoring\n\nAd hoc analysis including A/B testing, Facebook bidding and budgeting optimization, and topic modeling', u'Co-founder\nGO CAPITAL LLC - New York, NY\nMarch 2013 to December 2014\nResearched, developed, and maintained an automated, quantitative long/short U.S. equities trading strategy\n\nResponsible for full cycle algorithm development from ideation, data exploration, and simulation to production using proprietary software written in C and C++\n\nUtilized machine learning models including regression, ensemble trees, k-means clustering, Na\xefve Bayes for NLP and PCA and KNN for image classification in order to optimize current trading strategy and research new strategies\n\nDesigned a modified VWAP and smart order router to minimize trading costs, limit market impact and maximize execution\n\nSet up FIX Engine utilizing FIX Protocol that enabled direct market access, provided pre-trade risk checks, and managed portfolio size and construction\n\nBuilt tools using Perl and Python that leveraged publicly available data primarily through web scraping for corporate action data, earnings events, news flow and FDA calendars\n\nAdministered multiple Linux (Ubuntu) servers for development and production purposes', u'Proprietary Trader\nG-2 TRADING - New York, NY\nJuly 2010 to February 2013', u'Proprietary Trader\nASSENT LLC - New York, NY\nJanuary 2010 to June 2010', u'Senior Vice President\nBTIG LLC - New York, NY\nMarch 2008 to December 2009', u'Senior Analyst\nE*TRADE Capital Markets - New York, NY\nSeptember 2005 to February 2008', u""Consultant\nTOWERS PERRIN - New York, NY\nMarch 2003 to August 2005\n\u2022 Assisted in design of annual and long-term incentive plans within a value-based management framework\n\u2022 Involved in developing and implementing the firm's equity options valuation services\n\u2022 Analyzed data from company proxy statement filings for peer group construction and top-five compensation benchmarking\n\u2022 Participated in the design of a total compensation philosophy and program for a post-IPO $500 million insurance company\n\u2022 Developed a formulaic approach to determine annual bonus payouts to asset managers at a bulge bracket investment bank"", u'Supervisor, Financial Planning and Analysis\nEMI RECORDED MUSIC - New York, NY\nJune 2002 to March 2003\n\u2022 Responsible for monthly budgeting, forecasting and reporting of income statement, balance sheet, and cash flow statements\n\u2022 Prepared biweekly cash forecast used to determine funding needs for Virgin Records and Capitol Jazz & Classics\n\u2022 Participated in all facets of the annual budget process for Virgin Records and Capitol Jazz & Classics\n\u2022 Served as key liaison between Finance and other functional departments at Virgin Records and Capitol Jazz & Classics', u'Actuarial Consultant\nTOWERS PERRIN - New York, NY\nJanuary 1999 to July 2000\n\u2022 Participated in due diligence, total benefits redesign, and development of employee communications for a Fortune 100 company involved in a $4 billion acquisition\n\u2022 Assisted in new business development through the preparation of written, visual, and web-based communication\n\u2022 Developed software to perform workforce demographic projections', u'Actuarial Consultant\nPRICEWATERHOUSECOOPERS - Teaneck, NJ\nJuly 1996 to January 1999\n\u2022 Performed yearly valuations for pension, retiree medical, and disability plans in accordance with IRS and FASB regulations\n\u2022 Cleaned and validated employee data for use in valuations and asset/liability forecasting\n\u2022 Provided support to retirement plan administrators in the area of Qualified Domestic Relation Orders, benefit certifications and IRC Section 415 calculations for highly compensated employees\n\u2022 Developed and managed small staff of college hires training to become consultants']","[u'MBA in Finance', u'BA in Mathematics']","[u'THE ANDERSON SCHOOL AT UCLA Los Angeles, CA\nJune 2002', u'UNIVERSITY OF PENNSYLVANIA Philadelphia, PA\nMay 1996']","degree_1 : MBA in Finance, degree_2 :  BA in Mathematics"
0,https://resumes.indeed.com/resume/296ebf3d4b9f367f,"[u""Data Scientist\nState Street - Boston, MA\nFebruary 2017 to Present\nDescription: State Street Corporation, through its subsidiaries, provides a range of financial products and services to institutional investors worldwide. The company offers investment servicing products and services, including custody; product- and participant-level accounting; daily pricing and administration; master trust and master custody.\n\nResponsibilities:\n\u2022 Setup storage and data analysis tools in Amazon Web services cloud computing infrastructure.\n\u2022 Used pandas, numpy, Seaborn, scipy, matplotlib, sci-kit-learn, NLTK in Python for developing various machine learning algorithms.\n\u2022 Installed and used CaffeDeep Learning Framework\n\u2022 Worked on different data formats such as JSON, XML and performed machine learning algorithms in Python.\n\u2022 Worked as Data Architects and IT Architects to understand the movement of data and its storage and ER Studio 9.7\n\u2022 Worked with the UNIX team and installed TIDAL job scheduler on QA and Production Netezza environment.\n\u2022 Performed Data preparation on a High dimensional (Big data with large volume and variety) Data sample collected from the live customer data.\n\u2022 Data preparation, writing SQL queries Includes Mapping of unlined data from various formats, Identifying The missing data, Finding the correlations, scaling and removing the Junk data to further process the data for building a predictive model\n\u2022 Closely working with Network engineers and Ericsson analytics Expert Team to find the rule sets to build a predictive model.\n\u2022 Processed data using R programming and developed a Predictive model to predict KPI'S (Key performance indicators) Such as VOLTE accessibility, Connection success, Session Set Up success and Retain Ability.\n\u2022 Handled importing data from various data sources, performed transformations using Hive, Map Reduce, and loaded data into HDFS.\n\u2022 Worked with BTEQ to submit SQL statements, import and export data, and generate reports in Teradata.\n\u2022 Designed and documented Use Cases, Activity Diagrams, Sequence Diagrams, OOD (Object Oriented Design) using UML and Visio.\n\u2022 Summarized data through various tabular and visual modes for preliminary discovery analysis.\n\u2022 Conducted and interpreted multivariate analyses examples, including regressions with various distributions and duration models.\n\u2022 Created customized reports and processes in SAS and R.\n\u2022 Evaluated research for practical application by utilizing preclinical data sets and clinical trial modeling methods.\n\u2022 Pulled data from data lakes comprised of varied sources such as SQL, NoSQL structured and unstructured data.\n\u2022 Interaction with Business Analyst, SMEs, and other Data Architects to understand Business needs and functionality for various project solutions\n\u2022 Researched, evaluated, architected, and deployed new tools, frameworks, and patterns to built sustainable Big Data platforms for the clients\n\u2022 Identifying and executing process improvements, hands-on in various technologies such as Oracle, Informatica, Business Objects.\n\u2022 Designed both 3NF data models for ODS, OLTP systems and dimensional data models using Star and Snowflake Schemas.\n\nEnvironment: R 9.0, Informatica 9.0, ODS, OLTP, Bigdata, Oracle 10g, Hive, OLAP, DB2, Metadata, Python ,MS Excel, Mainframes MS Vision, Rational Rose."", u""Data Scientist\nZions Bancorporation - Salt Lake City, UT\nDecember 2015 to January 2017\nDescription: Zions Bancorporation, a financial holding company, provides a range of banking and related services primarily in Arizona, California, Colorado, Idaho, Nevada, New Mexico, Oregon, Texas, Utah, Washington, and Wyoming. The company offers community banking services, such as small and medium-sized business and corporate banking.\n\nResponsibilities:\n\u2022 Analyzed the business requirements of the project by studying the Business Requirements Specification document.\n\u2022 Extensively worked on Data Modeling tools Erwin Data Modeler to design the datamodels.\n\u2022 Handled importing data from various data sources, performed transformations using Hive and loaded data into HDFS.\n\u2022 Collaborate with data engineers to implement ETL process, write and optimized SQL queries to perform data extraction from Cloud and merging from Oracle 12c.\n\u2022 Collect unstructured data from MongoDB and completed data aggregation.\n\u2022 Worked in Statistical Modeling and Machine Learning techniques (Linear, Logistics, Decision Trees, Random Forest, SVM, K-Nearest Neighbors, Bayesian, XGBoost) in Forecasting/ Predictive Analytics, Segmentation methodologies, Regression-basedmodels, Hypothesis testing, Factor analysis/ PCA, Ensembles.\n\u2022 Designed tables and implemented the naming conventions for Logical and Physical Data Models in Erwin7.0.\n\u2022 Participated in the conversion of ITS (Immigration Tracking System) Visual Basic client-server application into C#, ASP.NET3-tierIntranet application.\n\u2022 Performed Exploratory Data Analysis and Data Visualizations using R, and Tableau.\n\u2022 Perform a proper EDA, Uni-variate and bi-variate analysis to understand the intrinsic effect/combined effects.\n\u2022 Worked with Data Governance, Data quality, data lineage, Data architect to design various models and processes.\n\u2022 Independently coded new programs and designed Tables to load and test the program effectively for the given POC's using with Big Data/Hadoop.\n\u2022 Designed data models and data flow diagrams using Erwin and MS Visio.\n\u2022 Utilized ADO.Net Object Model to implement middle-tier components that interacted with MSSQL Server 2000database.\n\u2022 Participated in AMS (Alert Management System) JAVA and SYBASE project. Designed SYBASE database utilizing ERWIN. Customized error messages utilizing SP_ADDMESSAGE and SP_BINDMSG. Created indexes, made query optimizations. Wrote stored procedures, triggers utilizing T-SQL.\n\u2022 Explained the data model to the other members of the development team. Wrote XML parsing module that populates alerts from the XML file into the database tables utilizing JAVA, JDBC, BEAWEBLOGICIDE, Document Object Model.\n\u2022 As an Architect implemented MDM hub to provide clean, consistent data for anSOA implementation.\n\u2022 Developed, Implemented & Maintained the Conceptual, Logical &Physical Data Models using Erwin for forwarding/Reverse Engineered Databases.\n\u2022 Explored and Extracted data from source XML in HDFS, preparing data for exploratory analysis using data mining.\n\nEnvironment: SQL Server 2008R2/2005 Enterprise, SSRS, SSIS, Crystal Reports, Hadoop, Windows Enterprise Server 2000, DTS, Bigdata, Python, machine learning, SQL Profiler, and Query Analyzer."", u""Data Scientist\nWright Express Corp - South Portland, ME\nMarch 2014 to November 2015\nDescription:Wright Express Corporation provides business payment processing and information management solutions in North America, the Asia Pacific, and Europe. It operates in two segments, Fleet Payment Solutions, and Other Payment Solutions.\nResponsibilities:\n\n\u2022 Coded R functions to interface with Caffe Deep Learning Framework\n\u2022 Working in Amazon Web Services cloud computing environment\n\u2022 Worked with several R packages including knitr, dplyr, Spark R, Causal Infer, space-time.\n\u2022 Implemented end-to-end systems for Data Analytics, Data Automation and integrated with custom visualization tools using R, Mahout, Hadoop, and MongoDB.\n\u2022 Extracting the source data from Oracle tables, MS SQL Server, sequential files and excel sheets.\n\u2022 Perform analysis using predictive modeling, data/text mining, and statistical tools\n\u2022 Developed MapReduce/Spark Python modules for machine learning & predictive analytics in Hadoop on AWS.\n\u2022 Collaborate cross-functionally to arrive at actionable insights\n\u2022 Developing and maintaining Data Dictionary to create metadata reports for technical and business purpose.\n\u2022 Performed Exploratory Data Analysis and Data Visualizations using R, and Tableau.\n\u2022 Perform a proper EDA, Uni-variate and bi-variate analysis to understand the intrinsic effect/combined effects.\n\u2022 Worked with Data Governance, Data quality, data lineage, Data architect to design various models and processes.\n\u2022 Independently coded new programs and designed Tables to load and test the program effectively for the given POC's using with Big Data/Hadoop.\n\u2022 Designed data models and data flow diagrams using Erwin and MS Visio.\n\u2022 As an Architect implemented MDM hub to provide clean, consistent data for anSOA implementation.\n\u2022 Developed, Implemented &Maintained the Conceptual, Logical& Physical Data Models using Erwin for forwarding/Reverse Engineered Databases.\n\u2022 Established Data architecture strategy, best practices, standards, and roadmaps.\n\u2022 Lead the development and presentation of a dataanalytics data-hub prototype with the help of the other members of the emerging solutions team\n\u2022 Performed datacleaning and imputation of missing values using R.\n\u2022 Worked with Hadoop eco system covering HDFS, HBase, YARN, andMapReduce\n\u2022 Take up ad-hoc requests based on different departments and locations\n\u2022 Used Hive to store the data and perform datacleaning steps for huge datasets.\n\u2022 Created dash boards and visualization on regular basis using ggplot2 and Tableau.\n\u2022 Creating customized business reports and sharing insights to the management.\n\u2022 Worked with BTEQ to submit SQL statements, import and export data, and generate reports in Teradata.\n\u2022 Interacted with the other departments to understand and identify data needs and requirements and work with other members of the IT organization to deliver data visualization and reporting solutions to address those needs.\n\nEnvironment: Erwin r, Informatica, ODS, OLTP, Oracle 10g, Hive, OLAP, DB2, Metadata, MS Excel, Mainframes MS Visio, Rational Rose, and Requisite Pro, Hadoop, PL/SQL, etc."", u'Data Scientist\nQuality Systems Inc - Irvine, CA\nMay 2013 to February 2014\nDescription:Quality Systems, Inc., together with its subsidiaries, develops and markets healthcare information systems in the United States. The company operates through four divisions: QSI Dental, NextGen, Inpatient Solutions, and Practice Solutions.\nResponsibilities:\n\n\u2022 Statistical Modeling with ML to bring Insights in Data under guidance of Principal Data Scientist\n\u2022 Data modeling with Pig, Hive, Impala.\n\u2022 Ingestion with Sqoop, Flume.\n\u2022 Define the list codes and code conversions between the source systems and the data mart.\n\u2022 Implementation of Metadata Repository, Maintaining Data Quality, Data Cleanup procedures, Transformations, Data Standards, Data Governance program, Scripts, Stored Procedures, triggers and execution of test plans\n\u2022 Used SVN to commit the Changes into the main EMM application trunk.\n\u2022 Understanding and implementation of text mining concepts, graph processing and semi-structured and unstructured data processing.\n\u2022 Worked with Ajax API calls to communicate with Hadoop through Impala Connection and SQL to render the required data through it. These API calls are similar to Microsoft Cognitive API calls.\n\u2022 Good grip on Cloudera and HDP ecosystem components.\n\u2022 Used Elasticsearch (Big Data) to retrieve data intothe application as required.\n\u2022 Performed Map Reduce Programs those are running on the cluster.\n\u2022 Developed multiple MapReduce jobs in java for data cleaning and preprocessing.\n\u2022 Analyzed the partitioned and bucketed data and compute various metrics for reporting.\n\u2022 Involved in loading data from RDBMS and weblogs into HDFS using Sqoop and Flume.\n\u2022 Worked on loading the data from MySQL to HBase where necessary using Sqoop.\n\u2022 Developed Hive queries for Analysis across different banners.\n\u2022 Extracted data from Twitter using Java and Twitter API. Parsed JSON formatted twitter data and uploaded to the database.\n\u2022 Launching Amazon EC2 Cloud Instances using Amazon Images (Linux/ Ubuntu) and Configuring launched instances with respect to specific applications.\n\u2022 Exported the result set from Hive to MySQL using Sqoop after processing the data.\n\u2022 Analyzed the data by performing Hive queries and running Pig scripts to study customer behavior.\n\u2022 Have hands-on experience working withSequence files, AVRO, HAR file formats and compression.\n\u2022 Used Hive to partition and bucket data.\n\u2022 Experience in writing MapReduce programs with Java API to cleanse Structured and unstructured data.\n\u2022 Wrote Pig Scripts to perform ETL procedures on the data in HDFS.\n\u2022 Created HBase tables to store various data formats of data coming from different portfolios.\n\u2022 Worked on improving the performance of existing Pig and Hive Queries.', u'Data Architect/Data Modeler\nVirtela - Mumbai, Maharashtra\nMarch 2011 to April 2013\nEnvironment: SQL/Server, Oracle, MS-Office, Teradata, Informatica, ER Studio, XML, Business Objects, HDFS, Teradata 14.1, JSON, HADOOP (HDFS),Python, MapReduce, PIG, Spark, R Studio, MAHOUT, JAVA, HIVE, AWS.\nClient: Virtela, Mumbai, India Mar 2011 -Apr 2013\nRole: Data Architect/Data Modeler\n\nDescription: Virtela, an NTT Communications Company, is the smart alternative for enterprise networking and virtualized IT services.\n\nResponsibilities:\n\u2022 Worked with large amounts of structured and unstructured data.\n\u2022 Knowledge of Machine Learning concepts (Generalized Linear models, Regularization, Random Forest, Time Series models, etc.)\n\u2022 Worked in Business Intelligence tools and visualization tools such as Business Objects, ChartIO, etc.\n\u2022 Deployed GUI pages by using JSP, JSTL, HTML, DHTML, XHTML, CSS, JavaScript, and AJAX.\n\u2022 Configured the project on WebSphere 6.1 application servers\n\u2022 Implemented the online application by using Core Java, JDBC, JSP, Servlets and EJB 1.1, Web Services, SOAP, WSDL.\n\u2022 Handled end-to-end project from data discovery to model deployment.\n\u2022 Monitoring the automated loading processes.\n\u2022 Communicated with other Health Care info by using Web Services with the help of SOAP, WSDL JAX-RPC\n\u2022 Used Singleton, factory design pattern, DAO Design Patterns based on the application requirements\n\u2022 Used SAX and DOM parsers to parse the raw XML documents\n\u2022 Used RAD as Development IDE for web applications.\n\u2022 Preparing and executing Unit test cases\n\u2022 Used Log4J logging framework to write Log messages with various levels.\n\u2022 Involved in fixing bugs and minor enhancements to the front-end modules.\n\u2022 Implemented Microsoft Visio and Rational Rose for designing the Use Case Diagrams, Class model, Sequence diagrams, and Activity diagrams for SDLC process of the application\n\u2022 Doing functional and technical reviews\n\u2022 Maintenance in the testing team for System testing/Integration/UAT.\n\u2022 Guaranteeing quality in the deliverables.\n\u2022 Conducted Design reviews and Technical reviews with other project stakeholders.\n\u2022 Used pandas, NumPy, seaborn, SciPy, matplotlib, Scikit-Learn, NLTK in Python for utilizing various machine learning algorithms.\n\u2022 Worked on different data formats such as JSON, XML and performed machine learning algorithms in Python.\n\u2022 Was a part of the complete life cycle of the project from the requirements to the production support\n\u2022 Created test plan documents for all back-end database modules\n\u2022 Implemented the project in Linux environment.\n\nEnvironment: R, Erwin, MDM, QlikView, Machine learning, MLlib, PL/SQL, HDFS, Teradata, Python, JSON, HADOOP (HDFS), MapReduce, PIG, R Studio, MAHOUT, JAVA, HIVE, AWS.', u'Data Analyst/Data Modeler\nPeople Tech Group - Hyderabad, Telangana\nAugust 2009 to February 2011\nDescription: Founded in 2006, People Tech is an emerging leader in the Enterprise Applications and IT Services marketplace. People Tech draws its expertise from strategic partnerships with technology leaders like Microsoft, Oracle and SAP and combines that with the deep understanding of its employees.\n\nResponsibilities:\n\u2022 Developed Internet traffic scoring platform for ad networks, advertisers, and publishers (rule engine, site scoring, keyword scoring, lift measurement, linkage analysis).\n\u2022 Responsible for defining the key identifiers for each mapping/interface.\n\u2022 Clients include eBay, Click Forensics, Cars.com, Turn.com, Microsoft, and Look smart.\n\u2022 Implementation of Metadata Repository, Maintaining Data Quality, Data Cleanup procedures, Transformations, Data Standards, Data Governance program, Scripts, Stored Procedures, triggers and execution of test plans.\n\u2022 Designed the architecture for one of the first analytics 3.0. Online platforms: all-purpose scoring, with on-demand, SaaS, API services. Currently under implementation.\n\u2022 Experience in Creating visually impactful dashboards in Excel and Tableau for data reporting by using pivot tables and VLOOKUP.\n\u2022 Carried out data manipulation, data cleaning, dealt with missing records, purged and consolidated data for analysis and selected the appropriate visualization techniques; developing reports and communicating insights to stakeholders.\n\u2022 Developed Enterprise applications in the Client/Server, Web application using PHP, Word Press, JavaScript, Jquery, AJAX, HTML5, CSS, MySQL & SQL Server database.\n\u2022 Designing and developing dynamic web pages using PHP and WordPress. Effectively used Tableau, Excel, jQuery, HTML, CSS and AJAX interactions.\n\u2022 Creation of multimillion bid keyword lists using extensive web crawling. Identification of metrics to measure the quality of each list (yield or coverage, volume, and keyword average financial value).\n\u2022 Enterprise Metadata Library with any changes or updates.\n\u2022 Document data quality and traceability documents for each source interface.\n\u2022 Establish standards of procedures.\n\u2022 Generate weekly and monthly asset inventory reports.\n\nEnvironment: Erwin r, SQL Server 2000/2005, Windows XP/NT/2000, Oracle, MS-DTS, UML, UAT, SQL Loader, OOD, OLTP, PL/SQL, MS Visio, Informatica.']",[u'Bachelors in Computer Science'],[u'Informatica Power Centre'],degree_1 : Bachelors in Compter Science
0,https://resumes.indeed.com/resume/85e125a9e8672ab9,"[u""Data Scientist\nKEYBANK - Cleveland, OH\nJanuary 2017 to Present\n\u2022 Part of the KeyBank Analytics Team providing the insight into banking & payment products through metrics, visualizations, and models.\n\u2022 Developed a Tableau deposit dashboard to help product and sales team to manage $32B commercial deposit portfolio through KPIs. Replaced manual process of ETL with new data pipeline to speed up the decision-making process from 7 days to 2 hours. Logged over 3k view since launch and maintained the user's retention rate around 70%.\n\u2022 Discovered $150M cross-selling opportunity through text mining of clients ACH statements (>1.5B records) and product usage history. Designed a 360-degree product recommendation dashboard for the sales team in relationship review. The algorithm behind recommendation contains both predefined business rules and association model result.\n\u2022 Organized weekly meeting with CEO and CFO to review monthly performance in financials, sales, and product. Collaborated with 5 developers to complete 50+ visualization projects and monitored the progress of each project.\n\u2022 Coached 20 CMU graduate students to finish their capstone projects related to online web portal design, customer social network analysis, and card fraud analysis & prevention."", u""Data Scientist Intern\nKEYBANK - Cleveland, OH\nJune 2016 to December 2016\n\u2022 Analyzed Commercial Card Platform in different aspects(size, industry, interchange rate, ramp-up period, risk grading, decline reasons)\n\u2022 Utilized Mckinsey GCI benchmark data and created a Tableau dashboard for Sales to recommend card spending opportunities through merchant categories.\n\u2022 Utilized current KeyBank Clients' ACH and check transaction pattern to create a real-time cross-selling opportunities tool through tuning parameters like % of vendor matching and rebate %.\n\u2022 Lead 4 CMU student consultants in designing the prototype of the commercial card online portal including data pipeline, front-end web design, benchmark and spend prediction(ARIMA) modeling."", u'Software Engineer\nFocusFeed - Pittsburgh, PA\nOctober 2015 to March 2016\n\u2022Contributed to the web application development that allows users to customize their favorite cloud solutions (Slack, Google, Dropbox\u2026) into one single view. Demo video of website: https://youtu.be/7F68s3byVPQ\n\u2022Implemented Google Calendar RESTful API into the Java Server and enabled users to view calendars side-by-side to avoid conflicts. Enabled users to create, update and delete events with GMT time, attendees and location proximity info\n\u2022Tested Google and Twitter APIs through Postman and different browsers\n\u2022Worked closely with other severs developers through JIRA and Git to ensure that the product was efficient and bug-free']",[u'Master of Information Systems Management in Data Mining'],"[u'CARNEGIE MELLON UNIVERSITY Pittsburgh, PA\nAugust 2015 to December 2016']",degree_1 : Master of Information Systems Management in Data Mining
0,https://resumes.indeed.com/resume/b8e7ca9b1276654a,"[u'Data Scientist / Quality Engineer\nJohn Deere - Dubuque, IA\nMay 2016 to Present\nLed root cause analysis for software and hardware related issues by incor- porating Machine Learning Techniques using Python.\n+ Led the design and implementation of an in-house Neural Network for use in SDLC planning using Python.\n+ Developed web platform tool to utilize Machine Learning methods.\n+ Utilized SOM principles for pattern recognition techniques to identify is- sue origin using Python.\n+ Developed a SQL database to monitor issues.\n+ Developed software solutions to meet needs of Quality Engineering tasks\nand requirements.\n+ Developed a RNN for aid in data quality issue origin.', u""Mobile Developer\nRoboticize LLC - Tampa, FL\nMay 2014 to May 2016\nDeveloped front end solutions for customer's mobile applications on An- droid and iOS natively.\n+ Performed Agile methodologies throughout the entire development life\ncycle.\n+ Managed back end server databases and integrated servers with existing\nclient services.\n+ Led a diverse team of 7 developers to complete projects to client satisfac- tion.\n+ Developed and maintained iOS and Android applications utilizing Loca- tion Services, Hardware functionalities, and other Core Frameworks.\n+ Developed and deployed custom APIs using JavaScript and Node.js and integrated third party APIs.\n+ Maintained codebases and led update efforts from Objective-C to Swift."", u'Software Engineer Intern\nHemingway Technologies - Riverview, FL\nJune 2011 to August 2013\nResearched patches and implemented them in the sandbox to maintain\nstable performance of client database.\nConfiguration management of patches which resulted in decreased trou- bleshooting by 20%.']","[u'Master of Science in Mechanical Engineering in Mechanical Engineering', u'Bachelor of Science in Mechanical Engineering in Mechanical Engineering']","[u'University of Louisville Louisville\nAugust 2014 to December 2015', u'Tuskegee University Tuskegee\nAugust 2010 to May 2014']","degree_1 : Master of Science in Mechanical Engineering in Mechanical Engineering, degree_2 :  Bachelor of Science in Mechanical Engineering in Mechanical Engineering"
0,https://resumes.indeed.com/resume/37cbd384e6083cf0,"[u'Owner, Chief Data Scientist\nDallas Digital Data - DeSoto, TX\nFebruary 2018 to Present\n\u2022 Created python web scrapping application for Real Estate Foreclosure specializing company. Gathered data from 3 different sources and consolidated into a sqlite db. Client has requested a desktop application. Currently generating application using JavaScript Electron using visual studio code as ide. Currently generating a recommendation to client for integration of machine learning into application.\n\n\u2022 Pro Bono work for Larson Arlington Elementary School (Arlington, Texas) yearbook committee. Use python to extract photos and data structure from photographer supplied data CD and integrate into Lifetouch yearbook generator. Saved days of repetitive work for client.\n\n\u2022 In the process of generating a machine learning notebook series that I am sharing on LinkedIn. Each notebook of the series will focus on a different aspect of Data Science. (Data Mining, Statistics and Visualization and Machine Learning) First of three published and has over 240 views in 1 business week. (IPython Notebooks)', u""Senior Data Scientist/ Bi Engineer\nPrimexx - Dallas, TX\nDecember 2014 to February 2018\n\u2022 Created field development optimization program on Primexx's largest asset. Used three different programming languages based on the best use. Custom generated java program to visualize the field development, ArcGIS Pro with python for infrastructure design and modelling, and finally production modelling using Spotfire with R. I combined data from Operations, Geology, Land and Engineering departments into the application. CI/CD Standards were observed to speed up implementation. ROI: ETL software development with modeling all in house. Saved millions.\n\n\u2022 Business Intelligence (BI) visualization specialist using data from each department (land, legal, accounting, engineering and financial using) Spotfire, Tableau and Matplotlib depending on the requirements and funding of the department and the capabilities of each package. ROI: No outsourcing visualizations.\n\n\u2022 Generate and maintain databases that include data form state regulatory agencies, field meters, and private vendors (3rd party API). The database was designed to support unique software packages of each department. The database was automatically updated from multiple vendors sources. The purpose was to monitor our areas interests utilizing using multivariable logistic regression and to better analyze trends in the Permian Basin. ROI: Basin wide analysis with small team. Saved millions.\n\n\u2022 Create entire Data Science workflow including data mining and dashboard generation for VP of Petrophysics to calibrate masks of Nuclear Magnetic Resonance with core data in our primary asset. Incorporated principle component analysis and support vector machine to predict rock properties with limited core data. Saved over 6 months of time and several million dollars. ROI: Exceeded VP project estimates by six months two weeks into project."", u'Data Scientist\nPrimexx - Dallas, TX\nDecember 1999 to December 2014\n\u2022 Demonstrate the ability to learn new and create new processes, workflows, policies, and procedures. Supervise, train and lead geologists, petrophysicists, geophysicists, accountants, and others in python, data exploration, visualization, storage, and machine learning. ROI: No outsourcing training.\n\n\u2022 Provide support within busy office in all aspects of the exploration and production business. Prepare reports, spreadsheets and presentations for investor meetings; manage records; and administer Petra and SMT (Seismic Micro Tech) database. Review prospects and new projects and report to Exploration Manager ROI: Create reproducible workflows reducing the required number of employees.\n.\n\u2022 Generate a company wide database used to keep track of production and inventory. The database was used extensively by Primexx for many years for investor and regulatory reporting. Generated VBA scripts use to populate the database from excel datafiles. ROI: Company wide data base.']","[u'MS in DataScience', u'B.S. in Geoscience']","[u'Southern Methodist University Dallas, TX\nJanuary 2016 to August 2017', u'University of Texas at Dallas Richardson, TX\nJanuary 2014']","degree_1 : MS in DataScience, degree_2 :  B.S. in Geoscience"
0,https://resumes.indeed.com/resume/583e32ed3d6172e1,"[u""Machine Learning intern\neBay Inc\nApril 2016 to December 2016\nProduct Recommender system: (Machine learning, Python(Tensor flow, MLlib, Apache spark, SQL)\n\u2023 Performed customer segmentation by clustering the data using similarity metrics to send personalised recommendations of products to customers depending on their activ- ity, products they buy, demographics etc.\n\u2023 Acted upon Consumer segmentation and characterization to predict behavior. Applied K-Means clustering to the customer data profiles to do customer segmentation and analysis and used PCA for Dimensionality reduction and achieved a return response rate of 92%\nCustomer sentiment analysis: (Natural language processing(NLP) NLTK, Document classification, Sentiment analysis word2vec, SQL, deep learning)\n\u2023 Worked on analyzing the sentiment expressed in the emails sent by customers by classifying the emails to the degree of displeasure.\n\u2023 Determined the most common features that can be obtained from the customer's emails using Naive Bayes classifier, logistic regression and other reliable classification\nmodels."", u'Data Scientist intern\nDeloitte\nMay 2015 to January 2016\nRestaurant Visitor forecasting: (Machine learning(ML), python, data analysis, data preprocessing, SQL, time series forecasting)\n\u2023 Worked on predicting the number of visitors to a Restaurant based on the data of previous visitors Created intuitive visualisations from data using python to provide in sights, determine problems, wins and trends that are paramount to get accurate predictions.\n\u2023 Performed Predictive analysis to create machine learning models that are correlated positively with historical data and use these models to forecast future results.\n\u2023 Responsible for application of various machine learning algorithms and statistical modeling techniques like decision trees, regression models, neural networks, SVM to ensure better results and performance.\nFraud Detection: (Classification, Bayesian, Statistical Modeling, Deep learning, confidence intervals, random forest, Boosting)\n\u2023 Analyzed transaction data of 140k for a fraud detection model, optimized the production code and improved speed by 4 times, and put the model into production.\n\u2023 Used probabilistic and bayesian models assuming gaussian noise to model the data and predicting the fraudulent transactions.', u'Data science intern\nAP Police\nMay 2014 to December 2014\nCrime data analysis: (Machine learning(ML), Scikit-learn, Lasso, Ridge regularization)\n\u2023 Attempted to mine the Crime data for useful and relevant patterns, to substantiate our suggestions to the AP Police in order to control crime rate.\n\u2023 Geospatial nature of the data allowed me to visualise the clusters after plotting the data onto a map revealing some interesting findings which played a crucial role in uder- standing the crime patterns.\n\u2023 Implemented different machine learning algorithms including Generalised Linear Model, Random Forest, and Neural Network and improved the accuracy from 58% to 81%']","[u'Master of Science in Computer Science in Data Science', u'Bachelor of Technology in Computer science']","[u'THE UNIVERSITY OF TEXAS AT DALLAS Dallas, TX\nAugust 2017 to May 2019', u'INDIAN INSTITUTE OF INFORMATION TECHNOLOGY\nAugust 2013 to June 2017']","degree_1 : Master of Science in Compter Science in Data Science, degree_2 :  Bachelor of Technology in Compter science"
0,https://resumes.indeed.com/resume/696e46e580fa421c,"[u'Data Scientist\nFidelity Investments - Westlake, TX\nFebruary 2017 to Present\nDescription: Fidelity Investments is a multinational financial services corporation. Fidelity Investments is among the most diversified financial services companies in the world. Fidelity Investments operates a brokerage firm, manages a large family of mutual funds, provides fund distribution and investment advice, retirement services, wealth management, securities execution and clearance, and life insurance.\n\nResponsibilities:\n\u2022 Transformed Logical Data Model to Erwin, Physical Data Model Foreign Key relationships in PDM and ensuring the Primary Key, Consistency of definitions of Data Attributes and Primary Index Considerations.\n\u2022 Setup storage and data analysis tools in Amazon Web Services cloud computing infrastructure.\n\u2022 Installed and used Caffe Deep Learning Framework.\n\u2022 Updated Python scripts to match training data with our database stored in AWS Cloud Search, so that we would be able to assign each document a response label for further classification.\n\u2022 Used pandas, numpy, seaborn, scipy, matplotlib, scikit-learn, NLTK in Python for developing various machine learning algorithms.\n\u2022 Lead data discovery, handling structured and unstructured data, cleaning and performing descriptive analysis, and storing as normalized tables for dashboards.\n\u2022 Unearthed the raw data by doing the Exploratory Data Analysis (Classification, splitting, cross-validation).\n\u2022 Implemented Classification using supervised algorithms like Logistic Regression, Decision trees, KNN, Naive Bayes.\n\u2022 Worked on Hadoop Architecture and various components using HDFS, Job Tracker, Task Tracker, Name Node, Data Node, Secondary Name Node, and Map Reduce concepts.\n\u2022 Used common data science toolkits, such as R, Python, NumPy, Keras, Theano, Tensor flow, etc.\n\u2022 Data transformation from various resources, data organization, features extraction from raw and stored.\n\u2022 Converted raw data to processed data by merging, finding out liers, errors, trends, missing values and distributions in the data.\n\u2022 Worked as Data Architects and IT Architects to understand the movement of data and its storage and ER Studio9.7\n\u2022 Detected the near-duplicated news by applying NLP methods and developing machine learning models like label spreading, clustering\n\u2022 Worked on different data formats such as JSON, XML and performed machine learning algorithms in Python.\n\u2022 As Architect delivered various complex OLAP databases/cubes, scorecards, dashboards and reports.\n\u2022 Programmed a utility in Python that used multiple packages (scipy, numpy, pandas).\n\u2022 A highly immersive Data Science program involving Data Manipulation & Visualization, Web Scraping, Machine Learning, Python programming, SQL, GIT, Unix Commands, NoSQL, MongoDB, Hadoop.\n\u2022 Participated in all phases of data mining; data collection, data cleaning, developing models, validation, visualization and performed Gap analysis.\n\u2022 Data Manipulation and Aggregation from different source using Nexus, Toad, Business Objects, Power BI and Smart View.\n\u2022 Implemented Agile Methodology for building an internal application.\n\u2022 Focus on integration overlap and Informatica newer commitment to MDM with the acquisition of Identity Systems.\n\u2022 Surveyed deep learning tools on NLP tasks, and extracted information from documents based on part-of-speech tagging, syntactic parsing, and named entity recognition\n\u2022 Utilized various techniques like Histogram, bar plot, Pie-Chart, Scatter plot; Box plots to determine the condition of the data.\n\u2022 Validated the machine learning classifiers using ROC Curves and Lift Charts.\n\u2022 Extracted data from HDFS and prepared data for exploratory analysis using data munging.\n\u2022 Used Teradata 15 utilities such as Fast Export, MLOAD for handling various tasks data migration/ETL from OLTP Source Systems to OLAP Target Systems\n\u2022 Used Hadoop ecosystem components like Hadoop Map Reduce, HDFS, HBase, Oozie, Hive, Sqoop, Pig, Flume including their installation and configuration.\n\u2022 Developed hybrid model to improve the accuracy rate.\n\u2022 Communicate with team members, leadership and stakeholders on findings to ensure models are well understood and incorporated into business processes.\n\nEnvironment: ER Studio 9.7, Tableau 9.03, AWS, Teradata 15, MDM, GIT, Unix, Python 3.5.2, MLLib, SAS, regression, logistic regression, Hardtop, NoSQL, Teradata, OLTP, random forest, OLAP, HDFS, ODS, NLTK, SVM, JSON, XML, MapReduce, R, Microsoft Excel, Hive, Pig, Flume, Swoop and Oozie, PySpark, MongoDB, SQL Server, SDLC, ETL, SSIS, recommendation systems, Machine Learning Algorithms.', u""Data Scientist\nNielsen - Oldsmar, FL\nDecember 2015 to January 2017\nDescription: The Nielsen Company (US), LLC provides performance management solutions for clients around the globe. It provides consumer packaged goods manufacturers and retailers with comprehensive view of the consumer through information and insights, and retail measurement services helping clients understand current performance and provide advanced analytical capabilities and solutions that aid in managing and improving future performance.\n\nResponsibilities:\n\u2022 Used Tableau to automatically generate reports. Worked with partially adjudicated insurance flat files, internal records, 3rd party data sources, JSON, XML and more.\n\u2022 Experienced in building models by using Spark (PySpark, SparkSQL, Spark MLLib, Spark ML).\n\u2022 \u2022 Experienced in Cloud Services such as AWS EC2, EMR, RDS, S3 to assist with big data tools, solve the data storage issue and work on deployment solution.\n\u2022 Worked with several R packages including knitr, dplyr, SparkR, Causal Infer, spacetime.\n\u2022 Performed Exploratory Data Analysis and Data Visualizations using R, and Tableau.\n\u2022 Implemented end-to-end systems for Data Analytics, Data Automation and integrated with custom visualization tools using R, Mahout, Hadoop and MongoDB.\n\u2022 Gathering all the data that is required from multiple data sources and creating datasets that will be used in analysis.\n\u2022 Knowledge extraction from Notes using NLP (Python, NLTK, MLLIB, PySpark,)\n\u2022 Independently coded new programs and designed Tables to load and test the program effectively for the given POC's using with Big Data/Hadoop.\n\u2022 Worked with BTEQ to submit SQL statements, import and export data, and generate reports in Teradata.\n\u2022 Built and optimized data mining pipelines of NLP, and text analytic to extract information.\n\u2022 Coded R functions to interface with Caffe Deep Learning Framework\n\u2022 Working in Amazon Web Services cloud computing environment\n\u2022 Interacted with the other departments to understand and identify data needs and requirements and work with other members of the IT organization to deliver data visualization and reporting solutions to address those needs.\n\u2022 Perform a proper EDA, Univariate and bi-variate analysis to understand the intrinsic effect/combined effects.\n\u2022 Designed data models and data flow diagrams using Erwin and MS Visio.\n\u2022 Established Data architecture strategy, best practices, standards, and roadmaps.\n\u2022 Performed datacleaning and imputation of missing values using R.\n\u2022 As an Architect implemented MDM hub to provide clean, consistent data for a SOA implementation.\n\u2022 Developed, Implemented & Maintained the Conceptual, Logical & Physical Data Models using Erwin for Forward/Reverse Engineered Databases.\n\u2022 Built and optimized data mining pipelines of NLP, and text analytic to extract information.\n\u2022 Worked with Hadoop eco system covering HDFS, HBase, YARN and Map Reduce.\n\u2022 Creating customized business reports and sharing insights to the management.\n\u2022 Take up ad-hoc requests based on different departments and locations.\n\u2022 Used Hive to store the data and perform data cleaning steps for huge datasets.\n\u2022 Created dash boards and visualization on regular basis using ggplot2 and Tableau.\n\u2022 Lead the development and presentation of a data analytics data-hub prototype with the help of the other members of the emerging solutions team.\n\nEnvironment: R 3.0, Erwin 9.5, Tableau 8.0, MDM, QlikView, MLLib, PL/SQL, HDFS, Teradata 14.1, JSON, HADOOP (HDFS), Map Reduce, PIG, Spark, R Studio, MAHOUT, JAVA, HIVE, AWS."", u""Data Scientist/Data Analyst\nOscar Health Insurance - New York, NY\nOctober 2014 to November 2015\nDescription: Oscar Health Insurance is a technology-focused health insurance company founded in 2012 and headquartered in New York City. This project was to support auditing team and claim department to improve accounting accuracy and reduce risk of fraudulent activities via providing machine learning and modeling solutions to identify suspicious insurance claims.\n\nResponsibilities:\n\u2022 Collaborated with database engineers to implement ETL process, wrote and optimized SQL queries to perform data extraction and merging from SQL server database.\n\u2022 Conducted analysis in assessing customer behaviors and discover value of customers, applied customer segmentation with clustering algorithm.\n\u2022 Research NLP techniques to construct coherent and clinically comprehensive sentences\n\u2022 Performed data integrity checks, data cleansing, exploratory analysis and feature engineer using python and data visualization packages such as Matplotlib, Seaborn.\n\u2022 Used Python to develop a variety of models and algorithms for analytic purposes.\n\u2022 Developed logistic regression models to predict subscription response rate based on customer's variables like past transactions, promotions, response to prior mailings, demographics, interests and hobbies, etc.\n\u2022 Used prediction model to rank the importance of features and deliver feature engineering.\n\u2022 Used Python to implement different machine learning algorithms, including Generalized Linear Model, Random Forest, SVM and Gradient Boosting.\n\u2022 Evaluated parameters with K-Fold Cross Validation and optimized performance of models.\n\u2022 Recommended and evaluated marketing approaches based on quality analytics on customer consuming behavior.\n\u2022 Collected and analyzed the customer feedback by using the streaming data from social networks stored in Hadoop system with Hive.\n\u2022 Performed data visualization and Designed dashboards with Tableau, and provided complex reports, including charts, summaries, and graphs to interpret the findings to the team and stakeholders.\n\u2022 Identified process improvements that significantly reduce workloads or improve quality.\n\nEnvironment: Python (Scikit-Learn/Scipy/Numpy/Pandas), Linux, Tableau, MS SQL Server 2012, Hardtop, Hive, MS SSIS, Windows 8/XP, JIRA"", u'Data Analyst/Data Modeller\nOffice Depot - Boca Raton, FL\nNovember 2012 to September 2014\nDescription: Office Depot, Inc. is an American office supply retailing company headquartered in Boca Raton, Florida, United States. The company has combined annual sales of approximately $14 billion and employs about 66,000 associates with businesses in 59 countries.\n\nResponsibilities:\n\u2022 Involved in migration projects to migrate data from data warehouses on Oracle/DB2 and migrated those to Teradata.\n\u2022 Designed, implemented and automated modeling and analysis procedures on existing and experimentally created data.\n\u2022 Implemented a job which leads an electronic medical record, extract data into Oracle Database and generate an output.\n\u2022 Used Model Manager Option in Erwin to synchronize the data models in Model Mart approach.\n\u2022 Parsed data, producing concise conclusions from raw data in a clean, well-structured and easily maintainable format. Developed clustering models for customer segmentation using Python.\n\u2022 Developed data mapping documents between Legacy, Production, and User Interface Systems.\n\u2022 Reverse Engineering the reports and identified Data Elements (in the source system) . Dimensions, Facts and Measures required for reports.\n\u2022 Developed logical and Physicaldata models using ER winto design OLTP system for different applications.\n\u2022 Worked with DBA group to create Best-Fit Physical Data Model with DDLfrom the Logical Data Model using Forward engineering.\n\u2022 Analyzed the data and provide the insights about the customers using Tableau.\n\u2022 Developed Data Migration and Cleansing rules for the Integration Architecture (OLTP, ODS, DW)\n\u2022 Used Teradata utilities such as Fast Export, MultiLOAD for handling various tasks.\n\u2022 Created dynamic linear models to perform trend analysis on customer transactional data in Python.\n\u2022 Developed anObject modeling in UML for Conceptual Data Model using Enterprise Architect.\n\u2022 Created entity process association matrices using Zachman Framework, functional decomposition diagrams and data flow diagrams from business requirements documents.\n\u2022 Worked with the ETL team to document the transformation rules for data migration from OLTP to Warehouse environment for reporting purposes.\n\u2022 Generated comprehensive analytical reports by running SQL queries against current databases to conduct data analysis.\n\u2022 Generated ad-hoc repots using Crystal Reports 9 and SQL Server Reporting Services (SSRS).\n\nEnvironment: Teradata SQL Assistant, Teradata Loading utilities (Bteq, FastLoad, MultiLoad), Python, UNIX, Tableau, MS Excel, MS Power Point, Business Objects, Oracle', u'Data Analyst\nNGRI - Hyderabad, Telangana\nFebruary 2011 to October 2012\nDescription: National Geophysical Research Institute (NGRI), a constituent Laboratory of CSIR, was established in 1961 with the mission to carry out research in multi-disciplinary areas of Earth Sciences.\n\nResponsibilities:\n\u2022 Document the complete process flow to describe program development, logic, testing, and implementation, application integration, coding.\n\u2022 Analyze business information requirements and model class diagrams and/or conceptual domain models.\n\u2022 Gather & Review Customer Information Requirements for OLAP and building the data mart.\n\u2022 Involved in defining the business/transformation rules applied for sales and service data.\n\u2022 Analyzed business process workflows and assisted in the development of ETL procedures for mapping data from source to target systems.\n\u2022 Work with users to identify the most appropriate source of record and profile the data required for sales and service.\n\u2022 Worked with BTEQ to submit SQL statements, import and export data, and generate reports in Teradata.\n\u2022 Calculated and analyzed claims data for provider incentive and supplemental benefit analysis using Microsoft Access and Oracle SQL.\n\u2022 Managed Development of Enterprise Information Security Performance Measurement Reporting\n\u2022 Developed Regulatory Monthly/Quarterly Reports\n\u2022 Document, clarify, and communicate requests for change requests with the requestor and coordinate with the development and testing team.\n\u2022 Performed document analysis involving creation of Use Cases and Use Case narrations using Microsoft Visio, in order to present the efficiency of the gathered requirements.\n\u2022 Responsible for defining the key identifiers for each mapping/interface.\n\u2022 Performed data quality in Talend Open Studio.\nEnvironment: SQL/Server, Oracle 9i, MS-Office, Teradata, Informatica, ER Studio, XML, Business Objects.', u""Data Analyst(Sql/Python)\nSoftech - Hyderabad, Telangana\nAugust 2009 to January 2011\nDescription: Softech Solutions is the training and Development Company with major business interests in Software Services, Web Development and Training.\n\nResponsibilities:\n\u2022 Analyzed User Requirement Document, Business Requirement Document (BRD), Technical Requirement Specification and Functional Requirement Specification (FRS).\n\u2022 Individually developed, implemented and managed a data operation platform system to ensure company's routine work, reduce unnecessary repetitive operations, and highly improve all departments working efficiency.\n\u2022 Used the technical document to design tables.\n\u2022 Improved data collection and distribution processes by using pandas and numpy packages in Python while enhancing reporting capabilities to provide clear line of sight into key performance trends and metrics.\n\u2022 Performed data analysis, data profiling, data scrubbing, data cleansing, generated data frequency reports.\n\u2022 Analyzed historical demand, filter out outliers/exceptions, identify the most appropriate statistical forecasting algorithm, develop base plan, understand variance, propose improvement opportunities, and incorporate demand signal into forecast and executed data visualization by using plotly package in Python. Documented the technical specification for the reports and tested the generated reports.\n\u2022 Generated SQL and PL/SQL scripts to install, create, and drop database objects including tables, views, primary keys, indexes, constraints, packages, sequences, grants and synonyms.\n\u2022 Created and managed Databases and worked on AWS redshift.\n\u2022 Created Database triggers to maintain the audit data in the tables.\n\u2022 Optimized the SQL queries for improved performance.\n\u2022 Prepared test plans for various modules.\n\nEnvironment: Oracle 9i, SQL Loader, PL/SQL, SQL, Toad, XML, Eclipse, Rational Clear Case, Rational Clear Quest, Python, MySQL""]","[u""Bachelor's""]",[u''],"degree_1 : ""Bachelors"""
0,https://resumes.indeed.com/resume/aad55c5633999012,"[u'Data Scientist\nState Street - Boston, MA\nFebruary 2017 to Present\nDescription: State Street Corporation, through its subsidiaries, provides a range of financial products and services to institutional investors worldwide. The company offers investment servicing products and services, including custody; product- and participant-level accounting; daily pricing and administration; master trust and master custody.\n\nResponsibilities:\n\u2022 Setup storage and data analysis tools in Amazon Webservices cloud computing infrastructure.\n\u2022 Used pandas, numpy, Seaborn, scipy, matplotlib, sci-kit-learn, NLTK in Python for developing various machine learning algorithms.\n\u2022 Installed and used CaffeDeep Learning Framework\n\u2022 Worked on different data formats such as JSON, XML and performed machine learning algorithms in Python.\n\u2022 Worked as Data Architects and IT Architects to understand the movement of data and its storage and ER Studio 9.7\n\u2022 Participated in all phases of datamining; data collection, data cleaning, developing models, validation, visualization and performed Gap analysis.\n\u2022 Data Manipulation and Aggregation froma different source using Nexus, Toad, Business Objects, PowerBI and SmartView.\n\u2022 Implemented Agile Methodology for building an internal application.\n\u2022 Focus on integration overlap and Informatica newer commitment to MDM with the acquisition of Identity Systems.\n\u2022 Good knowledge of Hadoop Architecture and various components such as HDFS, JobTracker, Task Tracker, Name Node, Data Node, SecondaryNameNode, and MapReduce concepts.\n\u2022 As Architect delivered various complex OLAPdatabases/cubes, scorecards, dashboards and reports.\n\u2022 Programmed a utility in Python that used multiple packages (scipy, numpy, pandas)\n\u2022 Implemented Classification using supervised algorithms like Logistic Regression, Decision trees, KNN, Naive Bayes.\n\u2022 Updated Python scripts to match training data with our database stored in AWS Cloud Search, so that we would be able to assign each document a response label for further classification.\n\u2022 Data transformation from various resources, data organization, features extraction from raw and stored.\n\u2022 Handled importing data from various data sources, performed transformations using Hive, MapReduce, and loaded data into HDFS.\n\u2022 Interaction with Business Analyst, SMEs, and other Data Architects to understand Business needs and functionality for various project solutions\n\u2022 Researched, evaluated, architected, and deployed new tools, frameworks, and patterns to built sustainable Big Data platforms for the clients\n\u2022 Identifying and executing process improvements, hands-on in various technologies such as Oracle, Informatica, Business Objects.\n\u2022 Designed both 3NF data models for ODS, OLTP systems and dimensional data models using Star and Snowflake Schemas.\n\nEnvironment: R 9.0, Informatica 9.0, ODS, OLTP, Oracle 10g, Hive, OLAP, DB2, Metadata, MS Excel, Mainframes MS Vision, Rational Rose.', u""Data Scientist\nZions Bancorporation - Salt Lake City, UT\nDecember 2015 to January 2017\nDescription: Zions Bancorporation, a financial holding company, provides a range of banking and related services primarily in Arizona, California, Colorado, Idaho, Nevada, New Mexico, Oregon, Texas, Utah, Washington, and Wyoming. The company offers community banking services, such as small and medium-sized business and corporate banking.\n\nResponsibilities:\n\u2022 Analyzed the business requirements of the project by studying the Business Requirements Specification document.\n\u2022 Extensively worked on Data Modeling tools Erwin Data Modeler to design the datamodels.\n\u2022 Designeda mapping to process the incremental changes that exist in the source table. Whenever source data elements were missing in source tables, these were modified/added inconsistency with third normal form based OLTP source database.\n\u2022 Designed tables and implemented the naming conventions for Logical and PhysicalDataModels in Erwin7.0.\n\u2022 Participated inthe conversion of ITS (Immigration Tracking System) Visual Basic client-server application into C#, ASP.NET3-tierIntranet application.\n\u2022 Performed Exploratory Data Analysis and Data Visualizations using R, and Tableau.\n\u2022 Perform a proper EDA, Uni-variate and bi-variate analysis to understand the intrinsic effect/combined effects.\n\u2022 Worked with Data Governance, Data quality, data lineage, Data architect to design various models and processes.\n\u2022 Independently coded new programs and designed Tables to load and test the program effectively for the given POC's using with Big Data/Hadoop.\n\u2022 Designed data models and data flow diagrams using Erwin and MS Visio.\n\u2022 Utilized ADO.Net Object Model to implement middle-tier components that interacted with MSSQL Server 2000database.\n\u2022 Participated in AMS (AlertManagementSystem) JAVA and SYBASE project. Designed SYBASE database utilizing ERWIN. Customized error messages utilizing SP_ADDMESSAGE and SP_BINDMSG. Created indexes, made query optimizations. Wrote stored procedures, triggers utilizing T-SQL.\n\u2022 Explained the data model to the other members of thedevelopment team. Wrote XML parsing module that populates alerts from theXML file into the database tables utilizing JAVA, JDBC, BEAWEBLOGICIDE, Document Object Model.\n\u2022 As an Architect implemented MDMhub to provide clean, consistent data for anSOA implementation.\n\u2022 Developed, Implemented & Maintained the Conceptual, Logical&PhysicalDataModels using Erwin for forwarding/Reverse Engineered Databases.\n\u2022 Explored and Extracted data from source XML in HDFS, preparing data for exploratory analysis using data mining.\n\nEnvironment: SQL Server 2008R2/2005 Enterprise, SSRS, SSIS, Crystal Reports, Hadoop, Windows Enterprise Server 2000, DTS, SQL Profiler, and Query Analyzer."", u""Data Scientist\nWright Express Corp - South Portland, ME\nMarch 2014 to November 2015\nDescription: Wright Express Corporation provides business payment processing and information management solutions in North America, the Asia Pacific, and Europe. It operates in two segments, Fleet Payment Solutions, and Other Payment Solutions.\nResponsibilities:\n\u2022 Coded R functions to interface with CaffeDeepLearningFramework\n\u2022 Working in Amazon Web Services cloud computing environment\n\u2022 Worked with several R packages including knitr, dplyr, SparkR, Causal Infer, space-time.\n\u2022 Implemented end-to-end systems for Data Analytics, Data Automation and integrated with custom visualization tools using R, Mahout, Hadoop, andMongoDB.\n\u2022 Gathering all the data that is required from multiple data sources and creating datasets that will be used in theanalysis.\n\u2022 Performed Exploratory Data Analysis and Data Visualizations using R, and Tableau.\n\u2022 Perform a proper EDA, Uni-variate and bi-variate analysis to understand the intrinsic effect/combined effects.\n\u2022 Worked with Data Governance, Data quality, data lineage, Data architect to design various models and processes.\n\u2022 Independently coded new programs and designed Tables to load and test the program effectively for the given POC's using with Big Data/Hadoop.\n\u2022 Designed data models and data flow diagrams using Erwin and MS Visio.\n\u2022 As an Architect implemented MDM hub to provide clean, consistent data for anSOA implementation.\n\u2022 Developed, Implemented &Maintained the Conceptual, Logical&PhysicalDataModels using Erwin for forwarding/Reverse Engineered Databases.\n\u2022 Established Data architecture strategy, best practices, standards, and roadmaps.\n\u2022 Lead the development and presentation of a dataanalytics data-hub prototype with the help of the other members of the emerging solutions team\n\u2022 Performed datacleaning and imputation of missing values using R.\n\u2022 Worked with Hadoop eco system covering HDFS, HBase, YARN, andMapReduce\n\u2022 Take up ad-hoc requests based on different departments and locations\n\u2022 Used Hive to store the data and perform datacleaning steps for huge datasets.\n\u2022 Created dash boards and visualization on regular basis using ggplot2 and Tableau.\n\u2022 Creating customized business reports and sharing insights to the management.\n\u2022 Worked with BTEQ to submit SQL statements, import and export data, and generate reports in Teradata.\n\u2022 Interacted with the other departments to understand and identify data needs and requirements and work with other members of the IT organization to deliver data visualization and reporting solutions to address those needs.\n\nEnvironment: Erwin r, Informatica, ODS, OLTP, Oracle 10g, Hive, OLAP, DB2, Metadata, MS Excel, Mainframes MS Visio, Rational Rose, and Requisite Pro, Hadoop, PL/SQL, etc."", u'Data Scientist\nQuality Systems Inc - Irvine, CA\nMay 2013 to February 2014\nDescription: Quality Systems, Inc., together with its subsidiaries, develops and markets healthcare information systems in the United States. The company operates through four divisions: QSI Dental, NextGen, Inpatient Solutions, and Practice Solutions.\nResponsibilities:\n\u2022 Statistical Modeling with ML to bring Insights in Data under guidance of Principal Data Scientist\n\u2022 Data modeling with Pig, Hive, Impala.\n\u2022 Ingestion with Sqoop, Flume.\n\u2022 Used SVN to commit the Changes into the main EMM application trunk.\n\u2022 Understanding and implementation of text mining concepts, graph processing and semi-structured and unstructured data processing.\n\u2022 Worked with Ajax API calls to communicate with Hadoop through Impala Connection and SQL to render the required data through it. TheseAPI calls are similar to Microsoft Cognitive API calls.\n\u2022 Good grip on Cloudera and HDP ecosystem components.\n\u2022 Used Elasticsearch (Big Data) to retrieve data intothe application as required.\n\u2022 Performed Map Reduce Programs those are running on the cluster.\n\u2022 Developed multiple MapReduce jobs in java for data cleaning and preprocessing.\n\u2022 Analyzed the partitioned and bucketed data and compute various metrics for reporting.\n\u2022 Involved in loading data from RDBMS and weblogs into HDFS using Sqoop and Flume.\n\u2022 Worked on loading the data from MySQL to HBase where necessary using Sqoop.\n\u2022 Developed Hive queries for Analysis across different banners.\n\u2022 Extracted data from Twitter using Java and Twitter API. Parsed JSON formatted twitter data and uploaded tothe database.\n\u2022 Launching Amazon EC2 Cloud Instances using Amazon Images (Linux/ Ubuntu) and Configuring launched instances with respect to specific applications.\n\u2022 Exported the result set from Hive to MySQL using Sqoop after processing the data.\n\u2022 Analyzed the data by performing Hive queries and running Pig scripts to study customer behavior.\n\u2022 Have hands-on experience working withSequence files, AVRO, HAR file formats and compression.\n\u2022 Used Hive to partition and bucket data.\n\u2022 Experience in writing MapReduce programs with Java API to cleanse Structured and unstructured data.\n\u2022 Wrote Pig Scripts to perform ETL procedures on the data in HDFS.\n\u2022 Created HBase tables to store various data formats of data coming from different portfolios.\n\u2022 Worked on improvingthe performance of existing Pig and Hive Queries.', u'Data Architect/Data Modeler\nVirtela - Mumbai, Maharashtra\nMarch 2011 to April 2013\nEnvironment: SQL/Server, Oracle, MS-Office, Teradata, Informatica, ER Studio, XML, Business Objects, HDFS, Teradata 14.1, JSON, HADOOP (HDFS), MapReduce, PIG, Spark, R Studio, MAHOUT, JAVA, HIVE, AWS.\nClient: Virtela, Mumbai, India Mar 2011 - Apr 2013\nRole: Data Architect/Data Modeler\n\nDescription: Virtela, an NTT Communications Company, is the smart alternative for enterprise networking and virtualized IT services.\n\nResponsibilities:\n\u2022 Worked with large amounts of structured and unstructured data.\n\u2022 Knowledge of Machine Learning concepts (Generalized Linear models, Regularization, Random Forest, Time Series models, etc.)\n\u2022 Worked in Business Intelligence tools and visualization tools such as Business Objects, Tableau, ChartIO, etc.\n\u2022 Deployed GUI pages by using JSP, JSTL, HTML, DHTML, XHTML, CSS, JavaScript, and AJAX.\n\u2022 Configured the project on WebSphere 6.1 application servers\n\u2022 Implemented the online application by using Core Java, JDBC, JSP, Servlets and EJB 1.1, Web Services, SOAP, WSDL.\n\u2022 Handled end-to-end project from data discovery to model deployment.\n\u2022 Monitoring the automated loading processes.\n\u2022 Communicated with other Health Care info by using Web Services with the help of SOAP, WSDL JAX-RPC\n\u2022 Used Singleton, factory design pattern, DAO Design Patterns based on the application requirements\n\u2022 Used SAX and DOM parsers to parse the raw XML documents\n\u2022 Used RAD as Development IDE for web applications.\n\u2022 Preparing and executing Unit test cases\n\u2022 Used Log4J logging framework to write Log messages with various levels.\n\u2022 Involved in fixing bugs and minor enhancements to the front-end modules.\n\u2022 Implemented Microsoft Visio and Rational Rose for designing the Use Case Diagrams, Class model, Sequence diagrams, and Activity diagrams for SDLC process of the application\n\u2022 Doing functional and technical reviews\n\u2022 Maintenance in the testing team for System testing/Integration/UAT.\n\u2022 Guaranteeing quality in the deliverables.\n\u2022 Conducted Design reviews and Technical reviews with other project stakeholders.\n\u2022 Was a part of the complete life cycle of the project from the requirements to the production support\n\u2022 Created test plan documents for all back-end database modules\n\u2022 Implemented the project in Linux environment.\n\nEnvironment: R, Erwin, Tableau, MDM, QlikView, MLlib, PL/SQL, HDFS, Teradata, JSON, HADOOP (HDFS), MapReduce, PIG, Spark, R Studio, MAHOUT, JAVA, HIVE, AWS.', u'Data Analyst/Data Modeler\nPeople Tech Group - Hyderabad, Telangana\nAugust 2009 to February 2011\nDescription: Founded in 2006, People Tech is an emerging leader in the Enterprise Applications and IT Services marketplace. People Tech draws its expertise from strategic partnerships with technology leaders like Microsoft, Oracle and SAP and combines that with the deep understanding of its employees.\n\nResponsibilities:\n\u2022 Developed Internet traffic scoring platform for ad networks, advertisers, and publishers (rule engine, site scoring, keyword scoring, lift measurement, linkage analysis).\n\u2022 Responsible for defining the key identifiers for each mapping/interface.\n\u2022 Clients include eBay, Click Forensics, Cars.com, Turn.com, Microsoft, and Look smart.\n\u2022 Implementation of Metadata Repository, Maintaining Data Quality, Data Cleanup procedures, Transformations, Data Standards, Data Governance program, Scripts, Stored Procedures, triggers and execution of test plans.\n\u2022 Designed the architecture for one of the first analytics 3.0. Online platforms: all-purpose scoring, with on-demand, SaaS, API services. Currently under implementation.\n\u2022 Web crawling and text mining techniques to score referral domains, generate keyword taxonomies, and assess commercial value of bid keywords.\n\u2022 Developed new hybrid statistical and data mining technique known as hidden decision trees and hidden forests.\n\u2022 Reverse engineering of keyword pricing algorithms in the context of pay-per-click arbitrage.\n\u2022 Coordinated meetings with vendors to define requirements and system interaction agreement documentation between client and vendor system.\n\u2022 Automated bidding for advertiser campaigns based either on keyword or category (run-of-site) bidding.\n\u2022 Creation of multimillion bid keyword lists using extensive web crawling. Identification of metrics to measure the quality of each list (yield or coverage, volume, and keyword average financial value).\n\u2022 Enterprise Metadata Library with any changes or updates.\n\u2022 Document data quality and traceability documents for each source interface.\n\u2022 Establish standards of procedures.\n\u2022 Generate weekly and monthly asset inventory reports.\n\nEnvironment: Erwin r, SQL Server 2000/2005, Windows XP/NT/2000, Oracle, MS-DTS, UML, UAT, SQL Loader, OOD, OLTP, PL/SQL, MS Visio, Informatica.']","[u""Bachelor's""]",[u''],"degree_1 : ""Bachelors"""
0,https://resumes.indeed.com/resume/f740ff7bd4108b57,"[u""Data Scientist/Machine Learning\nMinneapolis, MN\nMarch 2017 to Present\nDescription:\nTarget Corporation is the second-largest discount store retailer in the United States. Target is global sourcing organization locates merchandise from around the world for Target and helps import the merchandise to the United States.\n\nResponsibilities:\nInteracting with other data scientists and architects, custom solutions for data visualization using tools like a tableau, R-Shiny and Packages in R, performing statistical data analysis and data visualization using R and Python, performed analysis of implementing Spark uses Scala and wrote spark sample programs using PySpark, Designed the Data Marts in dimensional data modelling using star and snowflake schemas.\n\nDeveloped and implemented SSIS, SSRS and SSAS application solutions for various business units across the organization analyzed Data Set with SAS programming, R, and Excel, maintenance of large data sets, combining data from various sources by Excel, Enterprise, and SAS Grid, Access and SQL queries.\n\nInvolved in extensive hoc reporting, routine operational reporting and data manipulation to produce routine metrics and dashboards for management, created parameters, action filters and calculated sets for preparing dashboards and worksheets in Tableau, building, publishing customized interactive reports, report scheduling and dashboards using Tableau server, worked on creating filters and calculated sets for preparing dashboards and worksheets in Tableau, implemented data refreshes on Tableau Server for biweekly and monthly increments based on business change to ensure that the views and dashboards were displaying the changed data accurately, performed Tableau administering by using tableau admin commands.\n\nCreated Hive queries that helped market analysts spot emerging trends by comparing incremental data with Teradata reference tables and historical metrics, created UDFs to calculate the pending payment for the given residential or small business customer's quotation data and used in Pig and Hive Scripts, worked on moving data from Hive tables into HBase for real time analytics on Hive tables, handled importing of data from various data sources, performed transformations using Hive. (External tables, partitioning), responsible for creating Hive tables, loading the structured data resulted from Map Reduce jobs into the tables and writing hive queries to further analyze the logs to identify issues and behavioral patterns.\n\nDevelop and automate solutions for a new billing and membership Enterprise data Warehouse including ETL routines, tables, maps, materialized views, and stored procedures incorporating Informatica and Oracle PL/SQL toolsets, developed Teradata SQL scripts using OLAP functions like rank and rank over to improve the query performance while pulling the data from large tables, design and development of ETL processes using Informatica ETL tools for dimension and fact file creation, developed normalized Logical and Physical database models for designing an OLTP application.\n\nInvolved in running Map Reduce jobs for processing millions of records, written complex SQL queries using joins and OLAP functions like Count, CSUM, and Rank etc., developed in Python programs for manipulating the data reading from various Teradata and convert them as one CSV Files, wrote several Teradata SQL Queries using Teradata SQL Assistant for Ad Hoc Data Pull request.\n\nWorked in AWS Environment for loading data files from on prim to Redshift cluster, performed SQL Testing on AWS Redshift databases, and developed MapReduce Python modules for machine learning & predictive analytics in Hadoop on AWS.\n\nCreated data models in Splunk using pivot tables by analyzing the vast amount of data and extracting key information to suit various business requirements, created new scripts for Splunk scripted input for the system, collecting CPU and OS data.\n\nEnvironment: SQL/Server, Oracle 10g/11g, MS-Office, Teradata, Informatica, ER Studio, XML, Hive, HDFS, Flume, Sqooq, R connector, Python, R, Tableau 9.2"", u""Data Scientist/Data Architect\nAmerican Express, NY\nOctober 2015 to February 2017\nDescription:\nThe American Express Company, also known as Amex, is an American multinational financial services corporation headquartered in Three World Financial Center in New York City. The company was founded in 1850, and is one of the 30 components of the Dow Jones Industrial Average.4 The company is best known for its credit card, charge card, and traveler's cheque businesses.\n\nResponsibilities:\nDeveloped personalized product recommendation with Machine learning algorithms, including Gradient Boosting Tree and Collaborative filtering to better meet the needs of existing customers and acquire new customers, conducted analysis in assessing customer consuming behaviors and discover value of customers with RMF analysis, performed data integrity checks, data cleaning, exploratory analysis and feature engineer using R and Python, applied customer segmentation with clustering algorithms such as K-Means Clustering and Hierarchical Clustering.\n\nWorked on data cleaning, data preparation and feature engineering with Python, including Numpy, Scipy, Matplotlib, Seaborn, Pandas, and Scikit-learn, collaborated with data engineers to implement ETL process, wrote and optimized SQL queries to perform data extraction and merging from Oracle, Involved in managing backup and restoring data in the live Cassandra Cluster, worked on benchmarking Cassandra Cluster using the Cassandra stress tool, developed logistic regression models to predict subscription response rate based on customer's variables like past transactions, promotions, response to prior mailings, demographics, interests and hobbies, etc.\n\nUsed R, Python, and Spark to develop a variety of models and algorithms for analytic purposes, Identified risk level and eligibility of new insurance applicants with Machine Learning algorithms, Used Python and Spark to implement different machine learning algorithms, including Generalized Linear Model, random Forest, SVM, Boosting and Neural Network.\n\nA highly immersive Data Science program involving Data Manipulation and Visualization, Web Scraping, Machine Learning, GIT, SQL, UNIX Commands, Python programming, NoSQL, MongoDB, Hadoop, Collected unstructured data from MongoDB and completed data aggregation, Evaluated parameters with K-Fold Cross Validation and optimized performance of models, determined customer satisfaction and helped enhance customer using NLP, utilized SQL and HiveQL to query, manipulate data from variety data sources including Oracle and HDFS, while maintaining data integrity.\n\nEnvironment: R Studio, Python, Tableau, SQL Server 2012, 2014 and Oracle 10g, 11g"", u'Data Scientist/Data Architect\nSyntel Inc - Troy, MI\nDecember 2014 to September 2015\nDescription: Syntel, Inc. provides information technology (IT) and knowledge process outsourcing (KPO) services worldwide. It operates in four segments: Applications Outsourcing, KPO, e-Business, and Team Sourcing.\nResponsibilities:\n\nWork with users to identify the most appropriate source of record required to define the asset data for financing, implemented Agile Methodologies, Scrum stories and sprints in a Python based environment, along with data analytics and Excel data extracts.\n\nCreated Hive queries that helped market analysts spot emerging trends by comparing fresh data with EDW reference tables and historical metrics, performed Dataanalysis using Python Pandas, and performed Dataanalysis and Dataprofiling using complex SQL on various sources systems including Oracle and Teradata, involved in defining the source to target data mappings, business rules, business, and data definitions, imported the customer data into Python using Pandas libraries and performed various data analysis - found patterns in data which helped in key decisions for the company.\nDeveloped normalized Logical and Physical database models for designing an OLTP application, utilize a broad variety of OLAP function like Count, SUM, CSUM and worked on MS Excel using Pivot tables, Graphs, created Teradata SQL scripts using OLAP functions like RANK to improve the query performance while pulling the data from large tables.\n\nDeveloped new scripts for gathering network and storage inventory data and make Splunk ingest data, created tables in Hive and loaded the structured (resulted from MapReduce jobs) data, using HiveQL developed many queries and extracted the required information.\n\nDesign and deploy rich Graphic visualizations with Drill Down and Drop down menu option and Parameterized using Tableau, designed and developed weekly, monthly reports by using MS Excel Techniques (Graphs, Charts, Pivot tables) and Power point presentations, involved in creation of Excel skills, including Vlookup, pivots, conditional formatting, large record sets, data manipulation and cleaning.\n\nWorked on MongoDB database concepts such as locking, transactions, indexes, Sharding, replication, schema design, etc., exported the data required information to RDBMS using Sqoop to make the data available for the claims processing team to assist in processing a claim based on the data.\n\nEnvironment: SQL/Server, Oracle 11g, MS-Office, Teradata, Informatica, ER Studio, XML, Hive, HDFS, Flume, Sqoop, R connector, Python, R, Tableau.', u'Java Developer\nSterlite Technologies Ltd - Pune, Maharashtra\nJuly 2009 to January 2011\nDescription: Sterlite Technologies Limited designs, builds and manages smarter networks. Sterlite Tech develops & delivers optical communication products, network & system integration services and software solutions for telecoms globally.\n\nResponsibilities:\nDeveloped Admission and Census module, which monitored a wide range of detailed information for each resident upon pre-admission or admission to their facility.\n\nDeveloped UI using HTML, JavaScript, and JSP, and developed business logic and interfacing components using Business Objects, XML, and JDBC, Designed JSPs and Servlets for navigation among the modules, designed cascading style sheets and XML part of Order Entry Module and Product Search Module and performed client-side validations with JavaScript, designed user-interface and checked validations using JavaScript, responsible for Development and maintenance of Online & Batch code and batch JCLs, managed connectivity using JDBC for\nQuerying/inserting and data management including triggers and stored procedures, developed various EJBs for handling business logic and data manipulations from database.\n\nInvolved in development of General Ledger module, which streamlined analysis, reporting and recording of accounting information, Attending Teleconferences with onsite team, preparation of Weekly, Monthly status reports, and preparation of Requirement documents for enhancement activities.\n\nPreparation of Unit Test Plans and Unit Test Reports, conducting Unit Testing and document the test case with test results.\n\nEnvironment: Java, JSP, HTML, JDBC, XML, JavaScript, EJB, spring, Windows.']","[u""Bachelor's""]",[u''],"degree_1 : ""Bachelors"""
0,https://resumes.indeed.com/resume/02c31a8fe3907bd0,"[u""Data scientist\nCADREON - San Francisco, CA\nJanuary 2016 to Present\nDESCRIPTION OF THE PROJECT: Market Segmentation to find targeted customers.\nThe project was about Segmenting customers into groups with similar demographics or buying patterns for targeted marketing campaigns and/or detailed analysis of purchasing behavior by subgroup to provide recommendations about who were the engaging customers for the brand.\n\nROLES AND RESPONSIBILITIES IN THE PROJECT:\n\u2022 Utilized domain knowledge and application portfolio knowledge to play a key role in defining the future state of large, business technology programs.\n\u2022 Worked on cleaning the data using EDA and python libraries (Numpy, Pandas) by replacing the missing values using imputation techniques, outlier's identification using statistical methodologies.\n\u2022 Lead efforts to structure the company's customer data and manage integration of multiple customer data sources, in SAS environment.\n\u2022 Involved in creating data frames in spark using Pyspark and then applying Hive/SQL queries into Spark transformations using Spark RDDs, Python libraries and worked on resource manager in spark in increasing the executor memory size to support all the data processing steps.\n\u2022 Participated in all phases of data mining, data collection, data cleaning, developing models, validation, and visualization and performed Gap analysis.\n\u2022 Applied different feature reduction and Supervised Learning techniques to build the models using Apache Spark and Scikit Machine Learning Libraries.\n\u2022 Used Pandas, NumPy, SciPy, Matplotlib, Scikit-learn, Tensor Flow in Python for developing various machine learning algorithms and utilized machine learning algorithms such as linear regression, multivariate regression, naive Bayes, Random Forests, K-means, & KNN for data analysis.\n\u2022 Demonstrated experience in design and implementation of Statistical models, Predictive models, enterprise data model, metadata solution and data life cycle management in both RDBMS, Big Data environments.\n\u2022 Analyzed large data sets apply machine learning techniques and develop predictive models, statistical models and developing and enhancing statistical models by leveraging best-in-class modeling techniques.\n\u2022 Created new or modify SAS programs to load data from the source and create study specific datasets, which are used as source datasets for report generating programs. Used SQL pass through facility in SAS in writing the most efficient queries to extract the data from data warehouses.\n\u2022 Developed MapReduce/Spark Python modules for machine learning & predictive analytics in Hadoop on AWS.\n\u2022 Worked on customer segmentation using an unsupervised learning technique - clustering.\n\u2022 Utilized Spark, Scala, Hadoop, HBase, Kafka, Spark Streaming, MLlib, Python, a broad variety of machine learning methods including classifications, regressions, dimensionally reduction etc.\n\u2022 Worked on text analytics and used NLP to see the ad placements by using keyword matching and sense disambiguation.\n\nEnvironment: Python, SQL, Oracle 12c, SQL Server, SSRS, PL/SQL, T-SQL, Tableau, MLlib, Regression, Cluster analysis, Scala NLP, Spark, Kafka, MongoDB, Logistic regression, Hadoop, Hive, Teradata, Random forest, Azure, HDFS, NLTK, SVM, Json, Tableau, Xml, Cassandra, MapReduce, AWS."", u""Jr. Data Scientist\nHONEYWELL\nJune 2015 to December 2015\nDESCRIPTION OF THE PROJECT: Worked on Honeywell Sentience platform designed to handle industrial data. Analyzed data generated from wide range of enterprise applications and worked on reducing unplanned shutdowns in the plant and tried to reduce risk and improve safety and security of the platform.\n\nROLES AND RESPONSIBILITIES IN THE PROJECT:\n\u2022 Participated in all phases of data mining, data collection, data cleaning, developing models, validation, and visualization.\n\u2022 Used Pandas, Numpy, Seaborn, SciPy, Matplotlib, Scikit-learn Python libraries for developing various machine learning algorithms.\n\u2022 Provided data driven solutions for many customers and helped them to manage their plants by giving instructions virtually on the Honeywell platform using different machine learning algorithms.\n\u2022 Designed and implemented Statistical models, Predictive models, metadata solutions and data life cycle management in both RDBMS, Big Data environments.\n\u2022 Provided insights that impact data design and improve the quality of software.\n\u2022 Worked on data cleaning and ensured data quality, consistency, integrity using Pandas, Numpy\n\u2022 Worked on outlier's identification with box-plot, K-means clustering using Pandas, Numpy\n\u2022 Participated in features engineering such as feature intersection generating, feature normalize and Label encoding with Scikit-learn preprocessing\n\u2022 Performed univariate and multivariate analysis on the data to identify any underlying pattern in the data and associations between the variable\nEnvironment: Python 2.7(Numpy. Pandas, Scikit-learn, Tensor Flow), Jupyter Notebook 4.0, SQL Server 2008, Spark ML, Tableau 9.2."", u'Data Analyst\nSONATA SOFTWARE - Hyderabad, Telangana\nSeptember 2013 to December 2014\nROLES AND RESPONSIBILITIES OF THE PROJECT:\n\u2022 Involved in gathering business requirement for the SAS Process and guiding technical team on the approach.\n\u2022 Collaborated with multiple business teams to create dashboards used for individual team needs.\n\u2022 Write complex queries to access, extract, transform and load data.\n\u2022 Gathered user requirements and created the business requirements documents(BRD).\n\u2022 Documented the technical specification for the reports and tested the generated reports.\n\u2022 Used the technical document to design tables.\n\u2022 Performed data analysis, data profiling, data scrubbing, data cleansing, generated data frequency reports.\n\u2022 Generated SQL and PL/SQL scripts to install, create, and drop database objects including tables, views, primary keys, indexes, constraints, packages, sequences, grants and synonyms.\n\u2022 Created Database triggers to maintain the audit data in the tables.\n\u2022 Created stored procedures and functions to be called by the application.\n\u2022 Worked along with senior data scientists and managers to learn and help grow a team.\n\u2022 Familiarized with data visualization packages like Tableau and Apache Graphs.\n\nEnvironment: SQL* Loader, PL/SQL, SQL, Toad, XML, Eclipse, UNIX, Python (3.4/2.7), SQL, R, Tableau, Statistical Modelling, predictive algorithms.', u'Data Analyst\nAMAZON - Hyderabad, Telangana\nJuly 2011 to August 2013\nROLES AND RESPONSIBILITIES IN THE PROJECT:\n\u2022 Reviewed, evaluated, designed, implemented, and maintained reporting to support current business initiatives.\n\u2022 Prepared various statistical data analysis reports for corporate clients, provided ad-hoc updates and modified report templates as required.\n\u2022 Successfully completed training on in-house databases, extracted data from various data sources, applied Excel functions to transform raw data in business information required for reporting and data analysis purposes.\n\u2022 Created complex formulas and conditions to clean raw data, applied V-lookups, and Index-match functions to merge and filter information from various data sets for reporting and statistical gathering purposes.\n\u2022 Analyzed historical demand, filter out outliers/exceptions, identify the most appropriate statistical forecasting algorithms.\n\u2022 Improved data collection and distribution processes by using pandas and numpy packages in Python while enhancing reporting capabilities to provide clear line of sight into key performance trends and metrics.\n\nEnvironment: Python, MySQL, Hadoop (HDFS), Machine Learning Algorithms (Regression, Classification).']","[u""Master's""]",[u''],"degree_1 : ""Masters"""
0,https://resumes.indeed.com/resume/c8c70f8db25aa89b,"[u""Data Scientist\nAT&T - Phoenix, AZ\nFebruary 2017 to Present\nDescription: AT&T Inc. is an American multinational conglomerate holding company headquartered at Whit acre Tower in downtown Dallas, Texas.8 AT&T is the world's largest telecommunications company. AT&T is also the second largest provider of mobile telephone services and the largest provider of fixed telephone services in the United States.\n\nResponsibilities:\n\u2022 Developed, Implemented & Maintained the Conceptual, Logical & Physical Data Models using Erwin for Forward/Reverse Engineered Databases.\n\u2022 Designed algorithms to identify and extract incident alerts from a daily pool of incidents.\n\u2022 Reduced redundancy among incoming incidents by proposing rules to recognize patterns.\n\u2022 Scheduled searches Using Splunk tool.\n\u2022 Worked with Machine learning algorithms like Regressions (linear, logistic etc..), SVMs, Decision trees.\n\u2022 Solution architecting BIG Data solution for Projects & Proposal using Hadoop, Spark, ELK Stack, Kafka, Tensor flow.\n\u2022 Worked on Clustering and classification of data using machine learning algorithms.\n\u2022 Good applied statistics skills, such as statistical sampling, testing, regression, etc.\n\u2022 Build analytic models using a variety of techniques such as logistic regression, risk scorecards and pattern recognition technologies.\n\u2022 Work with technical and development teams to deploy models. Build Model Performance Reports and Modeling Technical Documentation to support each of the models for the product line.\n\u2022 Performed Exploratory Data Analysis and Data Visualizations using R, and Tableau.\n\u2022 Analyzed data from Primary and secondary sources using statistical techniques to provide daily reports.\n\u2022 Created Custom Dashboards and reports using Splunktool.\n\u2022 Estimation and Requirement Analysis of project timelines.\n\u2022 Analyzed data and recommended new strategies for root cause and finding quickest way to solve big data sets.\n\u2022 Used packages like dplyr, tidyr&ggplot2 in R Studio for data visualization.\n\u2022 Extending company's data with third party sources of information when needed.\n\u2022 Enhancing data collection procedures to include information that is relevant for building analytic systems.\n\u2022 Experienced in Delivery, Portfolio, Team / Career, Vendor and Program management Competency in Solution Architecture, implementation & delivery of Big Data, data science analytics &DWH projects on GreenPlum, SPARK, Keras, Python and TensorFlow.\n\u2022 Processing, cleansing, and verifying the integrity of data used for analysis\n\u2022 Doing ad-hoc analysis and presenting results in a clear manner\n\u2022 Constant tracking of model performance\n\u2022 Excellent understanding of machine learning techniques and algorithms , such as Logistic Regression, SVM, Random Forests, Deep Learning etc.\n\u2022 Worked with Data governance, Data quality, data lineage, Data architect to design various models.\n\u2022 Independently coded new programs and designed Tables to load and test the program effectively for the given POC's using with Big Data/Hadoop.\n\u2022 Designed data models and data flow diagrams using Erwin and MS Visio.\n\u2022 As an Architect implemented MDM hub to provide clean, consistent data for a SOA implementation.\n\u2022 Experience with common data science toolkits, such as R, Python, Spark, etc.\n\u2022 Developed and designed SQL procedures and Linux shell scripts for data export/import and for converting data.\n\u2022 Used Test Driven Development (TDD) for the project.\n\u2022 Written SQL Queries, Store Procedures, Triggers and functions for MySQL Databases.\n\u2022 Coordinate with data scientists and senior technical staff to identify client's needs and document assumptions.\n\u2022 Perform a proper EDA, Univariate and bi-variate analysis to understand the intrinsic effect/combined.\n\u2022 Established Data architecture strategy, best practices, standards, and roadmaps.\n\u2022 Lead the development and presentation of a data analytics data-hub prototype with the help of the other members of the emerging solutions team.\n\u2022 Worked with several R packages including knitr, dplyr, SparkR, CausalInfer, spacetime.\n\u2022 Interacted with the other departments to understand and identify data needs and requirements.\n\u2022 Involved in analysis of Business requirement, Design and Development of High level and Low level designs, Unit and Integration testing.\n\nEnvironment: R, R Studio, Splunk, SQL, MYSQL and Windows, UNIX, Python 3.5, MLLib, SAS, regression, logistic regression, Hadoop, NoSQL, Teradata, TensorFlow, OLTP, random forest, OLAP, HDFS, ODS"", u'Data Scientist\nDeloitte - Orlando, FL\nDecember 2015 to January 2017\nDescription: ""Deloitte"" is the brand under which tens of thousands of dedicated professionals in independent firms throughout the world collaborate to provide audit, consulting, financial advisory, risk management, tax, and related services to select clients.\n\nResponsibilities:\n\u2022 Implementation of MetadataRepository, MaintainingDataQuality, DataCleanup procedures, Transformations, Data Standards, Data Governance program, Scripts, Stored Procedures, triggers and execution of test plans.\n\u2022 Developed multiple MapReduce jobs in java for data cleaning and preprocessing.\n\u2022 Applied linear regression on data and predicted the sales. Without much advertisement how the sales is happening is predicted and measures are taken from insights how to advertise in a better manner.\n\u2022 Analyzed the partitioned and bucketed data and compute various metrics for reporting.\n\u2022 Involved in loading data from RDBMS and web logs into HDFS using Sqoop and Flume.\n\u2022 Worked on loading the data from MySQL to HBase where necessary using Sqoop.\n\u2022 Understanding the project Objectives and requirements from a domain prospective and understanding the problem definition.\n\u2022 Extracting the desired data from multiple sources integrating and preprocessing, cleaning and filling in missing data.\n\u2022 Evaluation of models for Bias and variance problem. Evaluating models for best parameters using K folds.\n\u2022 Update and maintained existing universes based on changes in user requirements& in data source.\n\u2022 Visually analyses chasm traps and Fan traps and resolve loops by creating aliases and contexts.\n\u2022 Design, develop, test, and deploy reports and dashboards that feed into mobile applications for ABM\'s, BRM\'s and Marketing users.\n\u2022 Manage JAD Sessions with Business Users in Design, Development& maintenance of analysis needs.\n\u2022 Provide input to project management role in terms of development/test activities and sequencing.\n\u2022 Utilize Business Objects reporting/ dashboard tools to import KPI\'s from data warehouse and ERP systems and present business insights to the business leaders and stakeholders.\n\u2022 Troubleshooting of universe schema with loops, chasm and fan traps, and cardinality problems.\n\nEnvironment: Erwin r9.0, Informatica 9.0, ODS, OLTP, Oracle 10g, Hive, OLAP, DB2, Metadata, MS Excel, Mainframes MS Visio, Rational Rose and Requisite Pro. Hadoop, PL/SQL, etc..', u""Data Scientist\nInfor - Dallas, TX\nFebruary 2014 to November 2015\nDescription: Worked as a part of Analytical Group and worked on development and designing of statistical models for the client. Coordinated with end users on designing and implementing data solutions as per project Requirements\nResponsibilities:\n\u2022 Designed algorithms to identify and extract incident alerts from a daily pool of incidents.\n\u2022 Reduced redundancy among incoming incidents by proposing rules to recognize patterns.\n\u2022 Scheduled searches Using Splunk tool.\n\u2022 Worked with Machinelearning algorithms like Regressions (linear, logistic etc..), SVMs, Decision trees.\n\u2022 Worked on Clustering and classification of data using machine learning algorithms.\n\u2022 Created Custom Dashboards and reports using Splunk tool.\n\u2022 Estimation and Requirement Analysis of project timelines.\n\u2022 Analyzed data and recommended new strategies for root cause and finding quickest way to solve big data sets.\n\u2022 Used packages like dplyr, tidyr&ggplot2in R Studio for data visualization.\n\u2022 Analyzed data from Primary and secondary sources using statistical techniques to provide daily reports.\n\u2022 Developed and designed SQL procedures and Linuxshellscripts for data export/import and for converting data.\n\u2022 Used Test Driven Development (TDD) for the project.\n\u2022 Written SQL Queries, Store Procedures, Triggers and functions for MySQL Databases.\n\u2022 Coordinate with data scientists and senior technical staff to identify client's needs and document assumptions.\n\nEnvironment: R, R Studio, Splunk, SQL, MYSQL and Windows."", u""R Programmer / Data Scientist\nActelion Pharmaceuticals - San Francisco, CA\nNovember 2012 to January 2014\nDescription: Actelion is a global biopharmaceutical company developing and marketing innovative drugs for high unmet medical needs.\n\nResponsibilities:\n\u2022 Conducted research on development and designing of sample methodologies, and analyzed data for pricing of client's products.\n\u2022 Use Correlation analysis to identify relation between variables, patterns, outliers and causal factors.\n\u2022 Identified statistically significant variables.\n\u2022 Investigated market sizing, competitive analysis and positioning for product feasibility.\n\u2022 Worked on Business forecasting, segmentation analysis and Data mining.\n\u2022 Used Support vector machines for classification of data in groups.\n\u2022 Generated graphs and reports using ggplot package in R-Studio for analytical models.\n\u2022 Developed and implemented R and Shiny application which showcases machine learning for business forecasting.\n\u2022 Developed predictive models using Decision Tree, Random Forest and Na\xefve Bayes.\n\u2022 Performed time series analysis using Spotfire.\n\u2022 Collaborating with dev-ops teams for production deployment.\n\u2022 Worked in AmazonWebServices cloud computing environment.\n\u2022 Worked with Caffe Deep LearningFramework.\n\u2022 Developed various workbooks in Spot fire from multiple data sources.\n\u2022 Created dashboards and visualizations using Spot fire desktop.\n\u2022 Worked on R packages to interface with CaffeDeep Learning Framework.\n\u2022 Performed analysis using JMP.\n\u2022 Perform validation on machine learning output from R.\n\u2022 Written connectors to extract data from databases.\n\nEnvironment: R, R Studio, Excel 2013, Amazon Web Services, Machine Learning, Spotfire , JMP, Segmentation analysis."", u'Data Analyst\nCyient Ltd - Hyderabad, Telangana\nFebruary 2011 to October 2012\nDescription: Cyient is a global solutions provider focused on engineering, manufacturing, data analytics, and networks & operations. Infotech Enterprises Ltd.\n\nResponsibilities:\n\u2022 Analyze business information requirements and model class diagrams and/or conceptual domain models.\n\u2022 Gather & Review Customer Information Requirements for OLAP and building the data mart.\n\u2022 Performed document analysis involving creation of Use Cases and Use Case narrations using Microsoft Visio, in order to present the efficiency of the gathered requirements.\n\u2022 Calculated and analyzed claims data for provider incentive and supplemental benefit analysis using Microsoft Access and Oracle SQL.\n\u2022 Analyzed business process workflows and assisted in the development of ETL procedures for mapping data from source to target systems.\n\u2022 Worked with BTEQ to submit SQL statements, import and export data, and generate reports in Terra-data.\n\u2022 Responsible for defining the key identifiers for each mapping/interface\n\u2022 Responsible for defining the functional requirement documents for each source to target interface.\n\u2022 Coordinated meetings with vendors to define requirements and system interaction agreement documentation between client and vendor system.\n\u2022 Enterprise Metadata Library with any changes or updates.\n\u2022 Document data quality and traceability documents for each source interface.\n\u2022 Establish standards of procedures.\n\u2022 Generate weekly and monthly asset inventory reports.\n\u2022 Managed the project requirements, documents and use cases by IBM Rational RequisitePro.\n\u2022 Assisted in building an Integrated LogicalDataDesign, propose physical database design for building the data mart.\n\u2022 Document all data mapping and transformation processes in the Functional Design documents based on the business requirements.\nEnvironment: SQL Server 2008R2/2005 Enterprise, SSRS, SSIS, Crystal Reports, Windows Enterprise Server 2000, DTS, SQL Profiler, and Query Analyzer.', u'Java Developer\nBirla Soft - Hyderabad, Telangana\nAugust 2009 to January 2011\nDescription: Birlasoft provides IT Consulting, offshore technological software solutions, Business Automation, Custom Business Applications and empower digitization across industries.\n\nResponsibilities:\n\u2022 Participate in requirements & design discussions\n\u2022 Responsible and active in the analysis, design, implementation and deployment of full Software Development Lifecycle (SDLC) of the project.\n\u2022 Designed and developed user interface using JSP, HTML and JavaScript.\n\u2022 Developed Struts action classes, action forms and performed action mapping using Struts framework and performed data validation in form beans and action classes.\n\u2022 Extensively used Struts framework as the controller to handle subsequent client requests and invoke the model based upon user requests.\n\u2022 Defined the search criteria and pulled out the record of the customer from the database. Make the required changes and save the updated record back to the database.\n\u2022 Validated the fields of user registration screen and login screen by writing JavaScript validations.\n\u2022 Developed build and deployment scripts using Apache ANT to customize WAR and EAR files.\n\u2022 Used DAO and JDBC for database access.\n\u2022 Developed application using AngularJS and Node.JS connecting to Oracle on the backend.\n\u2022 Designed and developed a Restful APIs using Spring REST API.\n\u2022 Consumed RESTful web services using AngularJSHTTP service & rendered the JSON data on the screen.\n\u2022 Used AngularJSframework for building web-apps and is highly efficient in integrating with Restful services.\n\u2022 Design and develop XML processing components for dynamic menus on the application.\n\u2022 Involved in postproduction support and maintenance of the application.\n\nEnvironment: Oracle, Java, Struts, Servlets, HTML, XML, SQL, J2EE, Angular JS, JUnit, RESTful, SOA, Tomcat']","[u""Bachelor's""]",[u''],"degree_1 : ""Bachelors"""
0,https://resumes.indeed.com/resume/cac1ef621de044ec,"[u""Data Scientist\nDaimler - Portland, OR\nJanuary 2017 to Present\nDescription: Daimler AG is a German multinational automotive corporation. Daimler AG is headquartered in Stuttgart, Baden-W\xfcrttemberg, Germany. As of 2014, Daimler owns or has shares in a number of car, bus, truck and motorcycle brands including Mercedes-Benz,\nResponsibilities:\n\n\u2022 Built data pipelines for reporting, alerting, and data mining. Experienced with table design and data management using HDFS, Hive, Impala, Sqoop, MySQL, Mem SQL, Grafana/Influx DB, and Kafka.\n\u2022 Worked with statistical models for data analysis, predictive modelling, machine learning approaches, recommendation and optimization algorithms.\n\u2022 Working in Business and Data Analysis, Data Profiling, Data Migration, Data Integration and Metadata Management Services.\n\u2022 Worked extensively on Databases preferably Oracle 11g/12c and writing PL/SQL scripts for multiple purposes.\n\u2022 Built models using Statistical techniques like Bayesian HMM and Machine Learning classification models like XG Boost, SVM, and Random Forest using R and Python packages.\n\u2022 Worked with data compliance teams, data governance team to maintain data models, Metadata, data Dictionaries, define source fields and its definitions.\n\u2022 Worked with Big Data Technologies such Hadoop, Hive, Map Reduce.\n\u2022 Built an NLP email Classifier Using Python, Scikit-Learn, with an aim of helping them to classify the most important emails that needs to be taken care immediately with an accuracy of 93.78 percent with Logistic, XGBoost and Ensemble models.\n\u2022 Used MLLib, Spark's Machine learning library to build and evaluate different models.\n\u2022 Implemented rule-based expertise system from the results of exploratory analysis and information gathered from the people from different departments.\n\u2022 Performed Data Cleaning, features scaling, features engineering using pandas and numpy packages in python.\n\u2022 Complex quality rule development and implementation patterns with cleanse, parse, standardization, validation, exception, notification and reporting with ETL and Real-Time consideration.\n\u2022 Built models using Statistical techniques like Bayesian HMM and Machine Learning classification models like XGBoost, SVM, and Random Forest.\n\u2022 Designed, Developed and Supported ETL Process for data migration with Informatica.\n\u2022 Developed MapReduce pipeline for feature extraction using Hive.\n\u2022 Created Data QualityScripts using SQL and Hive to validate successful data load and quality of the data. Created various types of data visualizations using Python and Tableau.\n\u2022 Handled importing data from various data sources, performed transformations using Hive, Map Reduce, and loaded data into HDFS.\n\u2022 Used R and python for Exploratory Data Analysis, A/B testing, Anova test and Hypothesis test to compare and identify the effectiveness of Creative Campaigns.\n\u2022 Created clusters to classify Control and test groups and conducted group campaigns.\n\u2022 Analyzed and calculated the lifetime cost of everyone in the welfare system using 20 years of historical data.\n\u2022 Developed LINUX Shell scripts by using NZSQL/NZLOAD utilities to load data from flat files to Netezza database.\n\u2022 Developed triggers, stored procedures, functions and packages using cursors and ref cursor concepts associated with the project using Pl/SQL\n\u2022 Created various types of data visualizations using R, python and Tableau.\n\u2022 Used Python, R, SQL to create Statistical algorithms involving Multivariate Regression, Linear Regression, Logistic Regression, PCA, Random forest models, Decision trees, Support Vector Machine for estimating the risks of welfare dependency.\n\u2022 Identified and targeted welfare high-risk groups with Machine learning algorithms.\n\u2022 Conducted campaigns and run real-time trials to determine what works fast and track the impact of different initiatives.\n\u2022 Used Graphical Entity-Relationship Diagramming to create new database design via easy to use, graphical interface.\n\u2022 Created multiple custom SQLqueries in TeradataSQL Workbench to prepare the right data sets for Tableau dashboards\n\u2022 Perform analyses such as regression analysis, logistic regression, discriminant analysis, cluster analysis using SAS programming.\n\u2022 Used TensorFlow to design a seven-layer convolutional neural network combined with density detection, refining the results with small and occluded objects.\n\nEnvironment: Erwin 8, Teradata 13, SQL Server 2008, Oracle 9i, SQL Loader, PL/SQL, ODS, OLAP, OLTP, SSAS, Informatica Power Center 8.1, NLP, TensorFlow."", u""Data Scientist\nBBVA Compass - Birmingham, AL\nNovember 2015 to December 2016\nDescription: BBVA Compass Bancshares, Inc. (formerly Compass Bancshares) is a United States-based financial holding company headquartered in Birmingham, Alabama. It has been a subsidiary of the Spanish multinational Banco Bilbao Vizcaya Argentaria since 2007 and operates chiefly in Alabama, Arizona, California, Colorado,\n\nResponsibilities:\n\u2022 Designed an Industry standard data Model specific to the company with group insurance offerings, Translated the business requirements into detailed production level using Workflow Diagrams, Sequence Diagrams, Activity Diagrams and Use Case Modeling\n\u2022 Involved in design and development of data warehouse environment, liaison to business users and technical teams gathering requirement specification documents and presenting and identifying data sources, targets and report generation.\n\u2022 Worked with DataGovernance, Dataquality, datalineage, Dataarchitect to design various models and processes.\n\u2022 Independently coded new programs and designed Tables to load and test the program effectively for the given POC's using with BigData/Hadoop.\n\u2022 Designed data models and data flow diagrams using Erwin and MSVisio.\n\u2022 Utilized ADO.Net Object Model to implement middle-tier components that interacted with MSSQL Server 2000database.\n\u2022 Participated in AMS (AlertManagementSystem) JAVA and SYBASE project. Designed SYBASE database utilizing ERWIN. Customized error messages utilizing SP_ADDMESSAGE and SP_BINDMSG. Created indexes, made query optimizations. Wrote stored procedures, triggers utilizing T-SQL.\n\u2022 Built and optimized data mining pipelines of NLP, and text analytic to extract information.\n\u2022 Successfully optimized codes in Python to solve a variety of purposes in data mining and machine learning in Python.\n\u2022 A highly immersive Data Science program involving Data Manipulation Visualization, Web Scraping, Machine Learning, Python programming, SQL, GIT, Unix Commands, NoSQL, MongoDB, Hadoop.\n\u2022 Designing the ETL process flows to load the data into Oracle Database from Heterogeneous sources.\n\u2022 Designed and developed UNIX shell scripts as part of the ETL process to compare control totals, automate the process of loading, pulling and pushing data from and to different servers.\n\u2022 Building programing logics for developing analysis datasets by integrating with various data marts in the sandbox environment\n\u2022 Facilitated stakeholder meetings and sprint reviews to drive project completion.\n\u2022 Successfully managed projects using Agile development methodology.\n\u2022 Interaction with Business Analyst, SMEs and other Data Architects to understand Business needs and functionality for various project solutions\n\u2022 Researched, evaluated, architected, and deployed new tools, frameworks, and patterns to build sustainable Big Data platforms for the clients\n\u2022 Project experience in Data mining, segmentation analysis, business forecasting and association rule mining using Large Data Sets with Machine Learning.\n\u2022 Automated Diagnosis of Blood Loss during Accidents and Applied Machine Learning algorithms to diagnose blood loss from vital signs (ECG, HF, GSR, etc.). Demonstrated performances of 94.6% on par with state-of-the-art models used in industry\n\nEnvironment: Erwin r9.6, DB2, Teradata, SQL-Server2008, Informatica 8.1, Enterprise Architect, Power Designer, MS SSAS, Crystal Reports, SSRS, ER Studio, Lotus Notes, Windows XP, MS Excel, word and Access, NLP."", u""Data Analyst\nVerizon wireless - Township of Warren, NJ\nJanuary 2014 to October 2015\nDescription: Verizon Wireless is an American telecommunications company, a wholly owned subsidiary of Verizon Communications, which offers wireless products and services.\n\nResponsibilities:\n\u2022 Responsible for the Study/Creation of SAS Code, SQL Queries, Analysis enhancements and documentation of the system.\n\u2022 Used R, SAS, and SQL to manipulate data, and develop and validate quantitative models.\n\u2022 Brainstorming sessions and propose hypothesis, approaches, and techniques.\n\u2022 Created and optimized processes in the Data Warehouse to import, retrieve and analyze data from the Cyber Life database.\n\u2022 Analyzed data collected in stores (JCL jobs, stored-procedures, and queries) and provided reports to the Business team by storing the data in excel/SPSS/SAS file.\n\u2022 Performed Analysis and Interpretation of the reports on various findings.\n\u2022 Prepared Test documents for zap before and after changes in Model, Test, and Production regions.\n\u2022 Responsible for production support Abend Resolution and other production support activities and comparing the seasonal trends based on the data by Excel.\n\u2022 Used advanced Microsoft Excel functions such as pivot tables and VLOOKU Pin order to analyze the data and prepare programs.\n\u2022 Successfully implemented migration of client's requirement application from Test/DSS/Model regions to Production.\n\u2022 Prepared SQL scripts for ODBC and Teradata servers for analysis and modeling.\n\u2022 Provided complete assistance of the trends of the financial time series data.\n\u2022 Various statistical tests performed for clear understanding to the client.\n\u2022 Implemented procedures for extracting Excel sheet data into the mainframe environment by connecting to the database using SQL.\n\u2022 Provided training to Beginners regarding the Cyber Life system and other basics.\n\u2022 Complete support to all regions. (Test/Model/System/Regression/Production).\n\nEnvironment: Python, R, SQL, Tableau, Spark, Machine Learning Software Package, recommendation systems."", u'Data Analyst\nEQT Corporation\nOctober 2012 to December 2013\nDescription: EQT Corporation is a petroleum and natural gas exploration and pipeline company headquartered in EQT Plaza in Pittsburgh, Pennsylvania.\n\nResponsibilities:\n\u2022 Worked with BI team in gathering the report requirements and Sqoop to export data into HDFS and Hive.\n\u2022 Worked on Map Reduce jobs in Java for data cleaning and pre-processing.\n\u2022 Assisted with data capacity planning and node forecasting.\n\u2022 Collaborated with the infrastructure, network, database, application and BIteams to ensure data quality and availability.\n\u2022 Developed Map Reduce programs to extract and transform the data sets and results were exported back to RDBMS using Sqoop.\n\u2022 Was responsible for importing the data (mostly log files) from various sources into HDFS using Flume.\n\u2022 Created tables in Hive and loaded the structured (resulted from MapReduce jobs) data.\n\u2022 Using HiveQL developed many queries and extracted the required information.\n\u2022 Exported the data required information to RDBMS using Sqoop to make the data available for the claims processing team to assist in processing a claim based on the data.\n\u2022 Developed MapReduce programs to parse the raw data, populate staging tables and store the refined data in partitioned tables in the EDW.\n\u2022 Worked on MongoDB database concepts such as locking, transactions, indexes, Sharing, replication, schema design, etc.\n\u2022 Created Hive queries that helped market analysts spot emerging trends by comparing fresh data with EDW reference tables and historical metrics.\n\u2022 Enabled speedy reviews and first mover advantages by using Oozie to automate data loading into the Hadoop Distributed File System and PIG to pre-process the data.\n\u2022 Involved in defining the source to target data mappings, business rules, business and data definitions.\n\u2022 Responsible for defining the key identifiers for each mapping/interface.\n\nEnvironment: SQL/Server, Oracle 9i, MS-Office, Teradata, Informatica, ER Studio, XML, Business Objects, Help-Point Claims Services', u'Internship Data Analyst\nAccenture - Bengaluru, Karnataka\nJanuary 2011 to September 2012\nResponsibilities:\n\u2022 Implemented Microsoft Visio and Rational Rose for designing the Use Case Diagrams, Class model, Sequence diagrams, and Activity diagrams for SDLC process of the application\n\u2022 Worked with other teams to analyze customers to analyze parameters of marketing.\n\u2022 Conducted Design reviews and Technical reviews with other project stakeholders.\n\u2022 Was a part of the complete life cycle of the project from the requirements to the production support.\n\u2022 Created test plan documents for all back-end database modules\n\u2022 Used MS Excel, MS Access and SQL to write and run various queries.\n\u2022 Used traceability matrix to trace the requirements of the organization.\n\u2022 Recommended structural changes and enhancements to systems and databases.\n\u2022 Conducted Design reviews and Technical reviews with other project stakeholders.\n\u2022 Maintenance in the testing team for System testing/Integration/UAT\n\u2022 Guaranteeing quality in the deliverables.\n\nEnvironment: UNIX, C++, SQL, Oracle 10g, MS Office, MS Visio.', u'Internship Data Analyst\nFirst Indian Corporation Private Limited - Hyderabad, Telangana\nJanuary 2009 to December 2010\nResponsibilities:\n\u2022 Developed Internet traffic scoring platform for ad networks, advertisers, and publishers (rule engine, site scoring, keyword scoring, lift measurement, linkage analysis).\n\u2022 Responsible for defining the key identifiers for each mapping/interface.\n\u2022 Clients include eBay, Click Forensics, Cars.com, Turn.com, Microsoft, and Looksmart.\n\u2022 Implementation of Metadata Repository, Maintaining Data Quality, Data Cleanup procedures, Transformations, Data Standards, Data Governance program, Scripts, Stored Procedures, triggers and execution of test plans.\n\u2022 Designed the architecture for one of the first analytics 3.0. Online platforms: all-purpose scoring, with on-demand, SaaS, API services. Currently under implementation.\n\u2022 Web crawling and text mining techniques to score referral domains, generate keyword taxonomies, and assess commercial value of bid keywords.\n\u2022 Developed new hybrid statistical and data mining technique known as hidden decision trees and hidden forests.\n\u2022 Reverse engineering of keyword pricing algorithms in the context of pay-per-click arbitrage.\n\u2022 Coordinated meetings with vendors to define requirements and system interaction agreement documentation between client and vendor system.\n\u2022 Automated bidding for advertiser campaigns based either on keyword or category (run-of-site) bidding.\n\u2022 Creation of multimillion bid keyword lists using extensive web crawling. Identification of metrics to measure the quality of each list (yield or coverage, volume, and keyword average financial value).\n\u2022 Enterprise Metadata Library with any changes or updates.\n\u2022 Document data quality and traceability documents for each source interface.\n\u2022 Establish standards of procedures.\n\u2022 Generate weekly and monthly asset inventory reports.\n\nEnvironment: Erwin r, SQL Server 2000/2005, Windows XP/NT/2000, Oracle, MS-DTS, UML, UAT, SQL Loader, OOD, OLTP, PL/SQL, MS Visio, Informatica.']","[u""Bachelor's""]",[u''],"degree_1 : ""Bachelors"""
0,https://resumes.indeed.com/resume/a63f9fb54f727603,"[u""Data Scientist\nCGI Group Inc - New York, NY\nFebruary 2017 to Present\nCompany Description:CGI is turning data into diamonds. Big data is becoming a reality for many businesses and government organizations. Those that are successful in leveraging data through analytics are outperforming their peers. Getting more value from data is a key priority, both to gain insights about customers, citizens, employees and operations, and to reduce the costs and complexity of managing ever-growing volumes of data.CGI can help with expertise, solutions and partnerships using our Data2Diamonds approach to be simplifying data management and realizing value from analytics. This framework provides a blueprint for success in putting information to work.\nResponsibilities:\n\u2022 Interact with finance team to build a prediction model for Q1, Q2, Q3, Q4 (Government FY)\n\u2022 Extending company's data with third-party sources of information like GOVWIN, FedBizOpps, etc.\n\u2022 Responsible for quantitative analysis of structured, semi-structured, and unstructured data working in small teams to develop, test, and harden advanced analytical models as required.\n\u2022 Responsible for performing Machine-learning techniques regression/classification to predict the outcomes.\n\u2022 Responsible for design and development of Python programs to prepare transform and harmonize data sets in preparation for modeling.\n\u2022 Handled importing data from various data sources(GOVWIN), performed transformations using Spark, and loaded data into HDFS.\n\u2022 Interaction with Business Analyst, SMEs and other Data Architects to understand Business needs and functionality for various project solutions.\n\u2022 Created custom visualizations for ZOHO Dashboards shared amongst team of data scientists and analysts.\n\u2022 Created visualization and trends for Government spending on Defense DoD, Navy DoN, FDA, DOA, NIH, etc.\n\u2022 Opposition research to build data-sets to perform Triage and BNB analysis.\n\u2022 Developed and maintained data dictionary to create metadata reports for technical and business purpose.\n\u2022 Involved with Data Analysis Primarily Identifying Data Sets, Source Data, Source Meta Data, Data Definitions and Data Formats\n\u2022 Wrote simple and advanced scripts to create standard and adhoc reports for senior managers.\n\u2022 Collaborate the data mapping document from source to target and the data quality assessments for the source data.\n\u2022 Participated in Business meetings to understand the business needs & requirements.\n\u2022 Create workflow using Apache Oozie\n\u2022 Prepare ETLarchitect& design document which covers ETLarchitect, SSISdesign, extraction, transformation and loading of data into dimensional model.\n\u2022 Provide technical & requirement guidance to the team members for ETL -SSISdesign.\n\u2022 Participated in Architect solution meetings & guidance in Dimensional Data Modeling design.\n\u2022 Provided detailed customized representation of data, which helped and improved decision-making process.\n\nEnvironment:Linux, Hadoop, Python, Machine learning, AWS, Spark, HDFS, Python Libraries (Scikit-Learn/SciPy/NumPy/Pandas), SSIS, ETL, Tableau, ZOHO, Apache Oozie, GIT, PyCharm, GOVWIN, FBO"", u'Data Scientist\nWalt Disney - Glendale, CA\nOctober 2015 to January 2017\nCompany Description:The Walt Disney Co. is a diversified international family entertainment and media enterprise. It operates through four business segments: Media Networks, Parks & Resorts, Studio Entertainment and Consumer Products & Interactive Media.\nResponsibilities:\n\u2022 Performed Data Profiling to learn about behavior with various features such as caller ID, traffic pattern, location, number validity.\n\u2022 Application of various machine learning algorithms like decision trees, regression models, neural networks, SVM, clustering to identify fraudulent profiles using scikit-learn package in python.\n\u2022 Used clustering technique K-Means to identify outliers and to classify unlabeled data.\n\u2022 Evaluated models using Cross Validation, Log loss function, ROC curves and used AUC for feature selection.\n\u2022 Analyze traffic patterns by calculating autocorrelation with different time lags.\n\u2022 Ensured that the model has low False Positive Rate.\n\u2022 Used Principal Component Analysis in feature engineering to analyze high dimensional data.\n\u2022 Created and designed reports that will use gathered metrics to infer and draw logical conclusions of past and future behavior.\n\u2022 Performed Multinomial Logistic Regression, Random forest, Decision Tree, SVM to classify Scammer, Telemarketer.\n\u2022 Performed data analysis by using Hive to retrieve the data from Hadoop cluster, SQL to retrieve data from Oracle database.\n\u2022 Implemented rule-based expertise system from the results of exploratory analysis and information gathered from the people from different departments.\n\u2022 Performed Data Cleaning, features scaling, features engineering using pandas and NumPy packages in Python.\n\u2022 Developed MapReduce pipeline for feature extraction using Hive.\n\u2022 Created DataQualityScripts using SQL and Hive to validate successful data load and quality of the data. Created several types of data visualizations using Python and Tableau.\n\u2022 Communicated the results with operations team for taking best decisions.\n\u2022 Collected data needs and requirements by Interacting with the other departments within Walt Disney\n\nEnvironment:Linux, Hadoop, Python, Machine learning, MapReduce, HDFS, Python Libraries (NumPy/Pandas), Tableau, GIT, NLP, Neural Network, Hive, SQL', u'Data Scientist\nValley National Bank - Wayne, NJ\nDecember 2014 to September 2015\nCompany Description:Valley National Bancorp is a regional bank holding company headquartered in Wayne, New Jersey. Its principal subsidiary, Valley National Bank, operates 209 branches in 30 counties in northern and central New Jersey, Manhattan, Brooklyn, Queens, Long Island and Florida\nResponsibilities:\n\u2022 Supported MapReduce Programs running on clusters.\n\u2022 Evaluated business requirements and prepared detailed specifications that follow project guidelines required to develop written programs.\n\u2022 Configured Hadoop cluster with Name node and slaves and formatted HDFS.\n\u2022 Used Oozie workflow engine to run multiple Hive and Pig jobs.\n\u2022 Performed MapReduce Programs those are running on the cluster.\n\u2022 Developed multiple MapReduce jobs for data cleaning and preprocessing.\n\u2022 Analyzed the partitioned and bucketed data and compute various metrics for reporting.\n\u2022 Involved in loading data from RDBMS and web logs into HDFS using Sqoop and Flume.\n\u2022 Developed Hive queries for Analysis across different banners.\n\u2022 Extracted data from Twitter using Java and Twitter API. Parsed JSON formatted twitter data and uploaded to database.\n\u2022 Launching AmazonEC2 Cloud Instances using Amazon Images (Linux) and Configuring launched instances with respect to specific applications.\n\u2022 Exported the result set from Hive to MySQL using Sqoop after processing the data.\n\u2022 Analyzed the data by performing Hive queries and running Pig scripts to study customer behavior.\n\u2022 Used Hive to partition and bucket data.\n\u2022 Wrote Pig Scripts to perform ETL procedures on the data in HDFS.\n\u2022 Created HBase tables to store various data formats of data coming from different portfolios.\n\u2022 Worked on improving performance of existing Hive Queries.\n\nEnvironment:Linux, Hadoop, Python, MapReduce, HDFS, Python,GIT, Hive, SQL, Pig, Scoop, Flume, AWS, EC2, Twitter API, Oozie', u""Data Analyst / Scientist\nNetApp - Wichita, KS\nSeptember 2013 to November 2014\nCompany Description:NetApp knows storage backwards and forwards and on premise and in the cloud. The company makes data storage systems used by businesses for archiving and backup. It offers products for hybrid cloud storage, extending customers' IT infrastructure to the cloud environments of Amazon, Google, IBM, and Microsoft. NetApp enables customers' use of flash storage, another relatively new market for the company\nResponsibilities:\n\u2022 Worked on data cleaning and reshaping, generated segmented subsets using NumPyand Pandas in Python.\n\u2022 Generated cost-benefit analysis to quantify the model implementation comparing with the former situation.\n\u2022 Conducted model optimization and comparison using stepwise function.\n\u2022 Wrote and optimized SQL queries involving multiple joins and advanced analytical functions to perform data extraction and merging from large volumes of historical data stored in Oracle, validating the ETL processed data in target database.\n\u2022 Worked on model selection based on confusion matrices, minimized the Type II error\n\u2022 Developed Python scripts to automate data sampling process. Ensured the data integrity by checking for completeness, duplication, accuracy, and consistency.\n\u2022 Applied various machine learning algorithms and statistical modeling like decision tree, logistic regression, Gradient Boosting Machine to build predictive model using scikit-learn package in Python\n\u2022 Identified the variables that significantly affect the target\n\u2022 Continuously collected business requirements during the whole project life cycle.\n\u2022 Generated data analysis reports using Matplotlib, Tableau, successfully delivered and presented the results for C-level decision makers.\n\nEnvironment:Windows, Hadoop, Python, MapReduce, Oracle, NumPy, Pandas, Matplotlib, Tableau, ETL, ML algorithms.\nProject 5"", u""Data Analyst/Data Modeler\nInfotech Enterprises IT Services Pvt. Ltd - Hyderabad, Telangana\nJuly 2011 to August 2013\nCompany Description:Infotech Enterprises IT Services Pvt. Ltd. (Infotech IT), part of the $ 260 million Infotech Enterprises group, leverages its business process knowledge, technological competence, strategic alliances and strong global presence to offer innovative IT solutions to the Retail Industry.\nResponsibilities:\n\u2022 Data analysis and reporting using MySQL, MS Power Point, MS Access and SQL assistant.\n\u2022 Involved in MySQL, MS Power Point, MS Access Database design and design new database on Netezza which will have optimized outcome.\n\u2022 Involved in writing T-SQL, working on SSIS, SSRS, SSAS, Data Cleansing, Data Scrubbing and Data Migration.\n\u2022 Involved in writing scripts for loading data to target data Warehouse using Bteq, Fast Load, Multiload\n\u2022 Create ETL scripts using Regular Expressions and custom tools (Informatica, Pentaho, and Sync Sort) to ETL data.\n\u2022 Developed SQL Service Broker to flow and sync of data from MS-I to Microsoft's master database management (MDM).\n\u2022 Involved in loading data between Netezza tables using NZSQL utility.\n\u2022 Worked on Data modeling using Dimensional Data Modeling, Star Schema/Snow Flake schema, and Fact& Dimensional, Physical&Logicaldatamodeling.\n\u2022 Generated Stats pack/AWR reports from Oracle database and analyzed the reports for Oracle wait events, time consuming SQL queries, table space growth, and database growth.\n\nEnvironment:MySQL, MS Power Point, MS Access, MY SQL, MS Power Point, MS Access, Netezza, DB2, T-SQL, DTS, SSIS, SSRS, SSAS, ETL, MDM, Teradata, Oracle, Star Schema and Snow Flake Schema."", u'Data Analyst\nNetBlade Solutions - Mumbai, Maharashtra\nMay 2009 to June 2011\nCompany Description:Netblade Solution - Service Provider of web design solutions, website development & mobile development in Thane, Maharashtra. Company Factsheet. Nature of Business Service Provider; Legal Status of Firm Private Limited Company. Know More \u2022 Sure Quote; Hashtag Planet. Our Services. Web Design Solutions.\nResponsibilities:\n\u2022 Collaborating with business and technology teams.\n\u2022 Data Analysis-Data collection, data transformation and data loading the data using different ETL systems like SSIS and Informatica.\n\u2022 Performed source to target mapping as part of data migration from JD Edwards system to Agile PDM system.\n\u2022 Data Migration testing and implementation activities using SSIS and SSRS tools of Microsoft SQL Server 2008.\n\u2022 Responsible for accuracy of the data collected and stored in the corporate support system.\n\u2022 Performed data review, evaluate, design, implement and maintain company database.\n\u2022 Involved in construction of data flow diagrams and documentation of the processes.\n\u2022 Interacted with end users for requirements study and analysis by JAD (Joint Application Development).\n\u2022 Performed gap analysis between the present data warehouse to the future data warehouse being developed and identified data gaps and data quality issues and suggested potential solutions.\n\u2022 Participated in system and use case modeling like activity and use case diagrams.\n\u2022 Analyzed user requirements & worked with data modelers to identify entities and relationship for data modeling.\n\u2022 Actively participated in the design of data model like conceptual, logical models using Erwin. Used Exception handling application block for checking errors/exceptions across the website.\n\u2022 Developed Report Component, so that it retrieves the data by executing Stored Procedures throw Data Access component.\n\nEnvironment:Windows, Oracle, MS Excel, SSIS, Informatica, GAP Analysis, ERWIN']","[u""Bachelor's""]",[u''],"degree_1 : ""Bachelors"""
0,https://resumes.indeed.com/resume/3b6df293aa84598e,"[u""Data Scientist\nActive Health Management - Alpharetta, GA\nSeptember 2016 to Present\nAlpharetta, GA September 2016- Present\nActive Health Management is a global leader in the creation of innovative prescription medicines for all health, mental health, and anesthesia - products that contribute to the health of people and their quality of life. Its objective is to be a reliable partner of health care professionals and patients by providing products and services for the worldwide improvement of human health and quality of life.\nThe project mostly consists of data processing activities and analyzing data to build patient and treatment models, capture patient and treatment behaviors and aggregate rules for customer treatment classification there by creating customer treatment recommendations.\nDesignation: Data Scientist\n\u2022 Experience working in Data Requirement analysis for transforming data according to business requirements.\n\u2022 Worked thoroughly with data compliance teams such as Data Analysts and Data Engineers to gathered require raw data and define source fields in Hadoop.\n\u2022 Applied Forward Elimination and Backward Elimination for data sets to identify most statically significant variables for Data analysis such PBDIT, PBIT and PBT ratios as statically significant in Profitability ratios.\n\u2022 Utilized Label Encoders in Python to create dummy variables for geographic locations to identify their impact on pre-acquisition and post acquisitions by using 2 sample paired t test.\n\u2022 Worked with ETL SQL Server Integration Services (SSIS) for data investigation and mapping to extract data and applied fast parsing and enhanced efficiency by 17%.\n\u2022 Developed Data Science content involving Data Manipulation and Visualization, Web Scraping, Machine Learning, Python programming, SQL, GIT and ETL for Data Extraction.\n\u2022 Developed Analytical systems, data structures, gather and manipulate data, using statistical techniques.\n\u2022 Designing suite of Interactive dashboards, which provided an opportunity to scale and measure the statistics of the HR dept. which was not possible earlier and schedule and publish reports.\n\u2022 Provided and created data presentation to reduce biases and telling true story of people by pulling millions of rows of data using SQL and performed Exploratory Data Analysis.\n\u2022 Applied breadth of knowledge in programming (Python, R), Descriptive, Inferential, and Experimental Design statistics, advanced mathematics, and database functionality (SQL, Hadoop).\n\u2022 Migrated data from Heterogeneous Data Sources and legacy system (DB2, Access, Excel) to centralized SQL Server databases using SQL Server Integration Services (SSIS).\n\u2022 Applied Descriptive statistics and Inferential Statistics on varies data attributes using SPSS to draw insights of data regarding providing products and services for patients.\n\u2022 Utilized data reduction techniques such as Factor analysis to identify most correlated values to underlying factors of the data and categorized the variable according to factors.\n\u2022 Applied Wilcoxon sign test to patient and treatment data for pre-acquisition and post-acquisition for different sectors to find the statistical significance in R programming.\n\u2022 Performance Tuning: Analyze the requirements and fine tune the stored procedures/queries to improve the performance of the application.\n\u2022 Designed and built scalable infrastructure to support real- time analytics with the help of Scala.\n\u2022 Used PySpark Streaming to divide streaming data into batches for batch processing and Spark-SQL/Streaming for faster testing.\n\u2022 Used Kafta to perform real time processing on patient's lab data, patient's record and testing reports to get immediate results.\n\u2022 Developed various Tableau9.4 Data Models by extracting and using the data from various sources files, DB2, Excel, Flat Files and Bigdata.\n\u2022 Handled importing data from various data sources, performed transformations using Hive, Map Reduce, and loaded data into HDFS.\n\u2022 Utilized Amazon Web Services S3, EC2, EMR and RDS, Redshift to setup storage and data analysis tools like Python (Scikit Learn, Pandas, Numpy, and Scipy).\n\u2022 Interaction with Business Analyst, SMEs and other Data Architects to understand Business needs and functionality for various project solutions.\n\u2022 Identifying and executing process improvements, hands-on in various technologies such as Oracle, Informatica, and Business Objects.\n\u2022 Designed both 3NF data models for ODS, OLTP systems and dimensional data models using Star and Snowflake Schemas.\n\nEnvironment: Python, Jupyter, Tensor Flow, Keras, R Programming, SPSS, SQL Server 2014, SSRS, SSIS, SSAS, PySpark, Spark, Scala, AWS, EMR, RDS, RedShift, Microsoft office, SQL Server Management Studio, Business Intelligence Development Studio, MS Access, Informatica, SAP Business Objects and Business Intelligence."", u""Data Scientist\nMISO - Carmel, IN\nJune 2015 to August 2016\nCarmel, IN June 2015- August 2016\nMISO is a cost-effective delivery of electric power consumption in Carmel, IN that specializes in power service. The company to find new ways to bringing new customers.\nThe project involves data extraction and applying data integrity and analytical techniques for story telling from the data. The major key performance indicators such as power reliability and improved interconnected transmission data etc. using supervised and unsupervised machine learning techniques.\n\nDesignation: Data Scientist\n\u2022 Involved in gathering, analyzing and translating business requirements into analytic approaches.\n\u2022 Developed scripts in Python (Pandas and Numpy) for data ingestion, analyzing and data cleaning.\n\u2022 And utilized SAS for developing Pareto Chart for identifying highly impacting categories in modules to find the work force distribution and created various data visualization charts.\n\u2022 Performed univariate, bivariate and multiple analysis of approx. 4890 tuples using bar charts, box plots and histograms.\n\u2022 Participated in features engineering such as feature creating, feature scaling and One-Hot encoding with Scikit-learn.\n\u2022 Converted raw data to processed data by merging, finding outliers, errors, trends, missing values and distributions in the data.\n\u2022 Generated detailed report after validating the graphs using R, and adjusting the variables to fit the model.\n\u2022 Worked on Clustering and factor analysis for classification of data using machine learning algorithms.\n\u2022 Developed Descriptive statistics and inferential statistics for Logistics optimization, Value throughput data to at 95% confidence interval.\n\u2022 Imported data by using Power Query in MS Excel from API's and Web API's then created relationship between data tables by using Power Pivot.\n\u2022 Used Power Map and Power View to represent data very effectively to explain and understand technical and non-technical users.\n\u2022 Written MapReduce code to process and parsing the data from various sources and storing parsed data into HBase and Hive using HBase - Hive Integration.\n\u2022 Created SQL tables with referential integrity and developed advanced queries using stored procedures and functions using SQL server management studio.\n\u2022 Used Pandas, NumPy, seaborn, SciPy, Matplotlib, Scikit-learn, NLTK in Python for developing various machine learning algorithms and utilized machine learning algorithms such as linear regression, multivariate regression, naive Bayes, Random Forests, K-means, and KNN for data analysis.\n\u2022 Used packages like dplyr, tidyr and ggplot2 in R Studio for data visualization and generated scatter plot and high low graph to identify relation between different variables.\n\u2022 Worked on Business forecasting, segmentation analysis and Data mining and prepared management reports defining the problem; documenting the analysis and recommending courses of action to determine the best outcomes.\n\u2022 Experience with risk analysis, root cause analysis, cluster analysis, correlation and optimization and K-means algorithm for clustering data into groups.\n\u2022 Coordinated with data scientists and senior technical staff to identify client's needs.\n\nEnvironment: SQL Server 2012, Python, Jupyter, R 3.1.2, MATLAB, SSRS, SSIS, SSAS, MongoDB, HBase, HDFS, Hive, Pig, SAS, Power Query, Power Pivot, Power Map, Power View, Microsoft office, SQL Server Management Studio, Business Intelligence Development Studio, MS Access."", u""Data Analyst\nEducational Testing Service - Princeton, NJ\nJanuary 2014 to May 2015\nPrinceton, NJ January 2014- May 2015\nEducational Testing Service is a leading testing service organization. It provided and maintained all testing information of test takers.\nThe project consisted of data preprocessing activities from OLTP server using ETL and analyzing data to build testing report and test models, capture test taker behaviors and aggregate rules for test takers by their recommendations.\nDesignation: Data Analyst\n\u2022 Developed complex SQL queries using group by, join, where clause to answer tester questions.\n\u2022 Communicated and coordinated with other departments to collect Business Requirement Analysis and developed data flow mapping to load OLAP server for analysis of policies details.\n\u2022 Worked on missing value imputation, outlier's identification using Random Forest and Box Plots.\n\u2022 Tackled highly imbalanced dataset using under sampling with ensemble methods, oversampling and cost sensitive algorithms.\n\u2022 Improved prediction performance by using random forest and gradient boosting for feature selection with Python Scikit-learn.\n\u2022 Utilized Parametric and Non-Parametric test in SPSS to draw insight from data for making business decisions.\n\u2022 Implemented machine learning models (logistic regression, XGboost) with Python.\n\u2022 Validated and selected models using k-fold cross validation, confusion matrices and worked on optimizing models for high recall rate.\n\u2022 Involved with data profiling for multiple sources and answered complex business questions by providing data to business users.\n\u2022 Participated in Agile planning process and daily scrums, providing details to create stories.\n\u2022 Experience with routine DBA activities like Query Optimization, Performance Tuning and Effective SQL Server configuration for better performance and cost reduction.\n\u2022 Developed Tabular Reports, Sub Reports, Matrix Reports, Drill down Reports and Charts using SQL Server Reporting Services (SSRS).\n\u2022 Designed rich data visualizations with Tableau 9.4 and dynamic dashboards for business analysis.\n\nEnvironment: SQL Server 2012, R programming, Python, MATLAB, SSRS, SSIS, SSAS, SPSS, Minitab, Microsoft office, SQL Server Management Studio, Business Intelligence Development Studio, MS Access."", u""Data Analyst\nDecision Craft - Bengaluru, Karnataka\nSeptember 2012 to December 2013\nBangalore, India September 2012- December 2013\nDecision Craft is a provider of information technology (IT) services. The Company delivers a range of IT services through globally integrated onsite and offshore delivery locations. It offers its services to customers through industry-focused practices, including insurance, manufacturing, financial services, healthcare and telecommunications, and through technology-focused practices.\nThe project consisted of extracting data from various sources and transforming the data according to the business requirements and loading data to target destination. The SQL Server Integration Services (SSIS) is used to create mapping for automating data pipelines from OLTP to OLAP. The Data cleansing and Data integrity checks are performed on data and customized business report are developed and deployed on to SQL Server Reporting Services (SSRS) server for making Business Decisions.\nDesignation: Data Analyst\n\u2022 Gathered requirements and documented requirements with Use cases in Requisite Pro and created different traceability views by MS Visio.\n\u2022 Worked with Data Compliance teams to identify the most suitable source of record and outlined the data.\n\u2022 Functioned with project team representatives to ensure that logical and physical ER/Studio data models were established in line with business standards and guidelines.\n\u2022 Deeply analyzed the clients' data by using SQL Server Analytic Services (SSAS).\n\u2022 Completed data cleaning process to discover like taking care of missing values by utilizing strategies, like, supplanting by mean, forward/backward fill, evacuating whole rows/columns/values, expelling outliers and errors, normalizing, and scaling information in data set.\n\u2022 Implemented metadata repository, maintained data quality, data cleanup procedures, transformations, data standards, data governance program, scripts, stored procedures, triggers and executed test plans.\n\u2022 Worked with team by Extracting Mainframe Flat Files (Fixed or CSV) onto UNIX Server and then converting them into Teradata Tables using SQL Server Integration Services (SSIS).\n\u2022 Developed visualizations and dashboards using SQL Server Reporting Services (SSRS) to present analysis outcomes in terms of patterns, anomalies, and predictions use of bar charts, Line Plots, Scatter Plots, 3D plots, and histograms and connect with Hive for generating daily reports.\n\u2022 Documented the complete process flow to describe program development, logic, testing, implementation, application integration, and coding by using SQL Server Reporting Services (SSRS).\n\u2022 Created customized SQL Queries using SQL Server 2008/2008R2 Enterprise to pull specified data for analysis and report building in conjunction with Crystal Reports.\n\u2022 Designed & developed various Ad hoc reports for different teams in Business (Teradata and MS ACCESS, MS EXCEL).\n\nEnvironment: SSRS, SSIS, SSAS, SQL Server 2008/2008 R2 Enterprise, MS Visio, MS Excel, MS Project, Teradata, Crystal Reports, ER Studio, Crystal reports, and Business Objects.""]",[u'Bachelors of Technology in Technology'],[u'Jawaharlal Nehru technological university'],degree_1 : Bachelors of Technology in Technology
0,https://resumes.indeed.com/resume/1d8dc6ce014b4611,"[u'Data Scientist Intern\nCisco Systems - San Francisco Bay Area, CA\nJanuary 2017 to December 2017\n\u2022 Work in a Data Science team. Acquire, clean and structure data from multiple databases and data sources.\n\n\u2022 Work on advanced machine learning algorithms and their applications in root cause analysis and time series modeling.\n\n\u2022 Work on advanced machine learning algorithms and their applications in root cause analysis and time series modeling.\n\n\u2022 Analyze large time series APM datasets to develop models and perform machine learning algorithms to predict application\nperformance, trend, and cost. Improve model accuracy through back-testing.\n\n\u2022 Interpret data, visualize data, analyze results using statistical techniques and provide analytics reports to Product Managers.', u'Data Scientist Intern\nCisco - AppDynamics - San Francisco Bay Area, CA\nJune 2017 to September 2017\n\u2022 Work on real time petabyte scale Application Performance Monitoring data mining and develop root cause analysis\nalgorithms which are significant new functionality in recent product release.\n\n\u2022 Anomaly Detection. Apply advanced machine learning algorithms to recognize application performance anomalies based on some Key Performance Indicators (KPI) for a node.\n\n\u2022 Root Cause Analysis. Deploy correlation analytics, clustering and other machine learning algorithms to discover the metrics\nthat are the most likely contributors to poor performance, and identify the likely degree of impact on the KPI for each metric.\n\n\u2022 Program in R and Python to carry out time series ranking, modeling and prediction, and transaction analytics and implement these advanced algorithms via Docker container.', u'NASA Project Team Member\nNASA, SJSU\nJanuary 2017 to August 2017\n\u2022 Developed Generalized Bayesian Block algorithm handling differently shaped blocks to model the sorted real-time data.\n\n\u2022 Improved accuracy for fitting intensity functions and finding optimal partitions by using Iterated Least Squares methods.', u""Risk Management Intern\nIndustrial and Commercial Bank of China\nNovember 2014 to June 2015\n\u2022 Served in Risk Management department; Evaluate financial performance and potential risks of various enterprises.\n\n\u2022 Engaged in loan programs; Involved in the whole process, especially in analyzing enterprises' financial and operation\nconditions; Implemented methods of evaluating risks and returns."", u'Financial Analyst Intern\nGreentown China\nJuly 2014 to September 2014\n\u2022 Forecasting, financial modeling, variance analysis. Supported budgeting and planning goals and cost reduction targets.', u""Credit Risk Analyst Intern\nBank of Beijing\nJuly 2013 to September 2013\n\u2022 Focused on post-loan and credit risk analysis. Evaluated companies' financial performance before renewing loans.""]","[u'Master of Science in Statistics', u'Bachelor of Science in Finance']","[u'San Jose State University\nAugust 2015 to December 2017', u'Zhejiang University of Technology\nSeptember 2011 to July 2015']","degree_1 : Master of Science in Statistics, degree_2 :  Bachelor of Science in Finance"
0,https://resumes.indeed.com/resume/c3ccff85b8bfc78d,"[u""Senior Data Scientist\nThomson Reuters - Boston, MA\nJanuary 2015 to Present\nDesigned and implemented the Bayesian inference model used to produce the rankings for\nThomson Reuters Top 100 Global Energy Leaders and Thomson Reuters Top 100 Global\nTech Leaders.\n- Elevator pitch: https://www.youtube.com/watch?v=rTKTRtLbn4s\n- Technology stack: Python, Stan, Thomson Reuters Elektron Data Platform.\n+ Designed and implemented a latent parameter political opinion model, which powered the scoring algorithm in White House Run, a mobile game released by Reuters during the 2016\npresidential campaign, aimed at increasing political awareness, and driving traffic to Reuters'\npolling site. This work was presented at Strata 2016.\n- Brief description: https://goo.gl/hqhouz\n- Conference presentation: https://goo.gl/mAH5Ra\n- Technology stack: Oracle, Python, Scala, Spark.\n+ Designed and implemented a synthetic counterfactual model, which was used to analyze\nUN Comtrade data, and measure the impact of NAFTA on the trade patterns between the member countries. The results were incorporated into a data journalism piece authored by\none of our data visualization experts.\n- Article: https://goo.gl/8MtJGH\n- Technology stack: Postgres, Python, R, Stan.\n+ Prototyped an information fusion engine, which uses heterogeneous sources of information to compute the background probability of non-compliance with regulations, such as the US\nForeign Corrupt Practices Act (FCPA), Conflict Minerals Rule (US Dodd Frank Section 1502),\nUK Bribery Act (UKBA), and the UK Modern Day Slavery Act, in various countries and 193A School St. - Acton, MA 01720 - USA\n\u02d8 +1 (585) 752 9743 \u2022 * +1 (424) 253 6797\nQ alexander.the.average@gmail.com\nfl https://www.linkedin.com/in/alinconstandache/ 1/4\n\nindustrial sectors.\n- Description: https://goo.gl/94WBAL\n- Technology stack: Postgres, Python, Stan, Thomson Reuters Open Calais.\n+ Participated in the Thomson Reuters team that entered the 2016 FEIII Challenge. The task of the challenge was to identify matching entities, across multiple files, from different financial\nregulators, each using a different identifier schema. The results were presented at the Second\nInternational Workshop on Data Science for Macro-Modeling (DSMM16)\n- Challenge description: https://ir.nist.gov/dsfin/2016-challenge.html""]","[u'PhD in Physics', u'BS in Physics']","[u'University of Rochester\nJanuary 1998 to January 2004', u'University of Bucharest\nJanuary 1993 to January 1997']","degree_1 : PhD in Physics, degree_2 :  BS in Physics"
0,https://resumes.indeed.com/resume/f00390ec292ea26f,"[u""Data Scientist\nCitibank - Irving, TX\nOctober 2014 to Present\nResponsibilities\n\u2022 Worked with data source team and client systems to obtain data, identify data issues, data gaps, identified and recommended solutions.\n\u2022 Worked on data manipulation and accessed raw marketing data in varied formats from multiple sources for analyzing and processing.\n\u2022 Involved in performing extensive Testing by writing T-SQL queries and stored procedures to extract the data from Database.\n\u2022 Involved in data pattern recognition and data cleaning. Identified missing, invalid values and outliers, checked data for distribution, classification, correlation and VIF etc, analyzed and categorized variables of datasets.\n\u2022 Worked with BI team in data investigation, responsible for interpreting data variables, making instructions and data dictionaries.\n\u2022 Worked on PPNR and loss estimation analysis for housing and credit historical data.\n\u2022 Worked with Business team on variable selection, principal analysis and segmentation on datasets. Responsible for model development, comparing, validation and optimization. Established our linear model for forecasting.\n\u2022 Carried out Linear/Logistic Regression Analysis with R, investigated on the model for fitting/overfitting, significance, Multicollinearity, Heteroscedasticity and Autocorrelation etc.\n\u2022 Involved in trend analysis and segmentation analysis based on 12 years of market data. Provided graphic report to illustrate portfolio performance under various market trend.\n\u2022 Finished stress testing on different portfolios under different market scenarios to further establish our forecasting models. Compared and Created standards for choosing models.\n\u2022 Utilized SQL server's reporting services, SSRS, to support reporting requirements.\nEnvironment: R, T-SQL, SQL Server 2012, Teredata, SSRS, Linux Server, VBA."", u'Research Assistant, Ph.D. Candidate\nUniversity of Texas at Arlington - Arlington, TX\nJanuary 2009 to December 2014\nRoles & Responsibilities:\n\u2022 Experience in simulation code development, data generation, processing and analysis of 3D supersonic flow with MVG. Carried out following stability analysis for understanding complex problem.\n\u2022 Worked on clustering analysis on 30,000 data sets and profiling of turbulent flow data. Extended research to topology and physics analysis based on analysis results.\n\u2022 Built predictive model with linear regression analysis. Validated models with model comparison and visualization methods. Compared results against wind tunnel test data and achieved good fitting results.\n\u2022 Utilized mode recognition and FFT methods to process and filter signal and noise produced in computation results. Carried out time series analysis on time step datasets.\n\u2022 Used ANOVA package in R to keep track of variables of turbulent flow and analyze influence of dimension and position on flow parameters.\n\u2022 Employed Unix Shell Scripting and Parallel Computing for R and Fortran on Texas Advanced Computing Center (TACC) to guarantee efficient and successful computation of our case.\n\u2022 Familiar with computational numerical algorithms. Experienced in advanced high speed computing and efficiency improvement.\n\u2022 Responsible for data visualization in both graphical form and motion pictures to analyze topology and support future research.\n\u2022 Employed methods of interpolation, extrapolation and Fast Fourier Transformation to achieve clear working results which achieved quality similarity with experiment data.\n\u2022 Teaching Assistant: College Algebra, Trigonometry, Calculus I, II, III, ODE and Statistics\n\nEnvironment: Unix/Linux, TACC ranger, R, Parallel Computing, Binary Datasets, Fortran, Cygwin, AutoCAD, Latex, Tecplot.', u'Sr. Data Analyst/Jr. Data Scientist\nUS Bank - Richfield, MN\nAugust 2013 to May 2014\nResponsibilities\n\u2022 Used and supported database applications and tools for extraction, transformation and analysis of raw data.\n\u2022 Worked with end users to gain an understanding of information and core data concepts behind their business.\n\u2022 Analyzed business requirements, system requirements, data mapping requirement specifications, and responsible for documenting functional requirements and supplementary requirements.\n\u2022 Worked on Data Quality Controls and Business Requirements Document. Attended project release meetings.\n\u2022 Created data masking mappings to mask the sensitive data between production and test environment.\n\u2022 Identify source systems, their connectivity, related tables and fields and ensure data suitably for analysis.\n\u2022 Involved in extensive data validation with several complex SQL queries and involved in Back-End Testing and worked with data quality issues.\n\u2022 Worked on logistic regression analysis on credit datasets. Built logistic models to support Business department in determining credit issue.\n\u2022 Developed, managed and validated existing data models including logical and physical models of the data warehouse and source systems utilizing SAS.\n\u2022 Wrote simple and advanced SQL queries and scripts to create standard reports for senior managers.\nEnvironment: SQL Server 2008, Informatica 8.1, Quality Center 8.2, R, SAS, PL/SQL, Windows Server 2008', u""Leader of 5 independent projects\nWind Engineering Center of Hunan Univ\nAugust 2005 to May 2008\nAug 2005 - May 2008\nMaster's Candidate\n\n\u2022 Experience with sensor data collection and conversion from experimental and real infrastructures. Responsible for data analysis based on sensor data such as tension, displacement, vibration and load.\n\u2022 Carried out failure model analysis, fissure development prediction and maximum load analysis.\n\u2022 Responsible for linear and nonlinear analysis for spatial cable figuration analysis. Employed Fortran and Matlab for the calculation procedure and 3D data processing.\n\u2022 Leader of 5 independent projects. Responsible for project planning, data collection and adjustment, FFT for filtering noises, data collection and model building.\nEnvironment: Windows XP, Fortran, Matlab, ANSYS."", u""Bank of Communication - CN\nJune 2003 to May 2005\nConstruction Project Investor\n\nResponsibilities\n\u2022 Worked with Tai'an City Construction Survey & Design Institute on the project survey and feasibility study.\n\u2022 Responsible for communication and negotiating with project related aspects on project loan, including construction fee, design alterations, and unexpected events from the project, etc.\n\u2022 Responsible for data collecting and organizing through the bank system for project loan analysis. Helped establishing data system for mortgage on city constructions.\n\u2022 Worked on project cost along with bank. Investigated on construction cost models from the bank and from Construction Company, compared the difference and made relational datasets.\n\u2022 Completed program in Fortran to retrieve data from different construction models and make comparison. Assisted bank and simplified auditing process for similar construction projects.\nEnvironment: Windows XP, Fortran, EXCEL""]","[u'Ph.D. in Mathematics', u""Master's in Engineering""]","[u'Direct Numerical Simulation Center of the UT Arlington Arlington, TX\nJanuary 2014', u'Wind Engineering Center of Hunan Univ. Changsha Changsha, CN\nJanuary 2008']","degree_1 : Ph.D. in Mathematics, degree_2 :  ""Masters in Engineering"""
0,https://resumes.indeed.com/resume/1b42619396d942b4,"[u'Data Scientist\nFidelity National Financial - Orange, CA\nApril 2017 to February 2018\nPerformed tasks that enable learning the semantics of text and image data on scanned legal documents using natural language processing and computer vision techniques.\n\nSolved difficult, non-routine analysis problems, applying advanced analytical methods as needed using artificial intelligence, deep learning, and machine learning.\n\u2022 Barcode detection, document zone identification, and optical character recognition using Tesseract on each identified zone on document-style images.\n\u2022 Document similarity indexing using scanned document images, text recognized from scanned images, intersection-over-union statistics of identified zones on each document, and locality-sensitive hashing.\n\u2022 Dictionary-based bad OCR document rejection.\n\u2022 Document classification using term frequency-inverse document frequency statistics, dictionary count-vectorization, truncated singular value decomposition, and neural networks.\n\u2022 Variable extraction using labeled strided word-gram data preparation, word vector embeddings, and neural networks.\n\u2022 Federal Tax Lien identification using convolutional neural networks.\n\nEvaluated enterprise hardware, managed cloud assets, and administered servers in a multi-user environment.\n\u2022 HP DL380 G9 and Asus ESC8000 G3 server hardware evaluation and setup. Ubuntu Server 16.04 installation and administration through SSH and HP iLO.\n\u2022 VPC network configuration on Google Cloud Platform in a multi-user environment for Jupyter and Tensorboard.\n\u2022 Installation and administration of Ubuntu Server 16.04 virtual instances on Google Cloud Platform.\n\u2022 Secure mount of remote file system shares through FUSE protocol.', u'Junior Data Analyst\nMyriad Consulting - Redmond, WA\nJuly 2016 to April 2017\nWrote stored procedures, views, functions, ad-hoc queries to support analytical dashboards and reports, managed data quality to ensure consistency and accuracy, data manipulation and extraction. Identified, analyzed, and interpreted trends and patterns, and backed up and restored databases as needed.', u'Intern, Pactera\nVanceInfo - Seattle, WA\nMay 2013 to July 2013\nCisco network access server configuration. Configured Cisco networking equipment to set up network devices such as routers, switches, firewalls, terminals, AAA servers, and file servers with their respective MAC and IP addresses.']","[u'BA in Computational Mathematics', u'AS in Mathematics']","[u'University of California Santa Cruz, CA\nJune 2016', u'Orange Coast College Costa Mesa, CA\nAugust 2014']","degree_1 : BA in Comptational Mathematics, degree_2 :  AS in Mathematics"
0,https://resumes.indeed.com/resume/814b2cdf60bff649,"[u'Associate Data Scientist\nPfizer Inc - New York, NY\nFebruary 2016 to Present\n* Machine Learning / AI-\n\u25e6 Developing and deploying machine learning models within Dataiku\'s DSS and SparkBeyond.\n\u25e6 Solving complex prediction problems in the areas of digital communications churn, field force best practices, post-LoE (Loss of Exclusivity) budget allocation.\n* Big Data Processing and Analytics-\n\u25e6 Building an all-encompassing data-product named the ""Customer 360"" that currently enables other analytical colleagues to quickly view HCP interactions, prescriptions, and demographics all in the same location.\n\u25e6 Managing a team of 2 contracted resources responsible for the big data processing in Teradata, Hadoop HDFS, and Redshift.\n\u25e6 Integrating and connecting to new datasources, applying business rules in SQL (Spark and Impala), and automating build schedules.\n\u25e6 Marketing, partnering, and scaling data-products, communicating with key business leaders throughout the organization.\n* Presentation and Visualization Capabilities\n\u25e6 Leading live interactive analytics sessions with key business stakeholders, connecting to and processing the data, slicing the data using SQL queries and Tableau visualizations, and running predictive models to generating business actionable insights by the end of the meeting.\n\u25e6 Transforming PowerPoint into an interactive experience for the audience, leading to high levels of participation and action.\n\u25e6 Creating powerful and beautiful dashboards in Tableau using efficiently organized connections.']","[u'Bachelor of Science in Biomedical Engineering', u'in Neuroscience']","[u'Georgia Institute of Technology Atlanta, GA\nDecember 2015', u'Georgia Tech']","degree_1 : Bachelor of Science in Biomedical Engineering, degree_2 :  in Neroscience"
0,https://resumes.indeed.com/resume/4842a1bbb5dc7a8f,"[u'Data Scientist\nCisco Systems\nJanuary 2016 to September 2017\n\u2022 Research, use case development, prototyping and POC in network security, performance and user experience domain in collaboration with network engineers and data scientists.\n\n\u2022 Used published academic and technical papers and use the insight to create new approach\n\u2022 Developed ensemble based classification using Naive Bayes, Random Forests, XGBoost,\nNeural Network & Deep Learning algorithms to improve DDoS attack classification accuracy by 7 with potential impact of millions of dollar.\n\n\u2022 Testing, validating and incubating data analytic products and analyzed standard and coherence for Merger and Acquisition.\n\n\u2022 Research and development in microservice machine learning and edge computing.\n\n\u2022 Network Telemetry: Research and development in SNMP/IOS-XR Telemetry data for pipelining,\nautomation and orchestration using Pipeline, Grafana, PNDA etc. SNMP and BGP data analytic\ndifferent use cases using different methods including association rule mining method.\n\n\u2022 Laboratory Management: Manage laboratory with several rack servers and routers.\nInstalled and used virtualization tools, hypervisors, softwares such as docker and other\nopen-sourced and commercial software to study network flow.', u'Graduate Teaching/Research Assistant\nDepartment of Physics - Austin, TX\nJanuary 2008 to January 2016\nThe University of Texas at Austin and University of Minnesota Duluth\n\n\u2022 Researched data from STAR collaboration at Brookhaven National Lab and MINOS collaboration at Fermi National Lab and contributed experiment control room with different roles.\n\n\u2022 Software Development and Data Engineering for massive data analysis (5TB).\n\n\u2022 Developed Monte Carlo Simulation packages for collision of atomic nuclei and compared results with experimental data to validate physical theory.\n\n\u2022 Prepared Curriculum and Taught undergraduate physics courses']",[u'MS in Statistics and Data Sciences'],"[u'The University of Texas at Austin Austin, TX\nJanuary 2015']",degree_1 : MS in Statistics and Data Sciences
0,https://resumes.indeed.com/resume/fd8138804473211d,"[u'Data Scientist\nFord Motor Corporation - Dearborn, MI\nAugust 2016 to January 2018\n\u2022 Built a classification prediction model in R / Python (Scikit-learn) to classify the Customs Data.\n\u2022 Clustering Stocks using KMeans.\n\u2022 Solved Classification and Regression problem using Keras.\n\u2022 Performed Text preprocessing and Topic Identification using Python packages (NLTK and Genism). Built a classifier model with TfidfVectorizer (NLP)\n\u2022 Conducted a Training session on how to analyze and process the data using Pig Latin and HiveQL (Hadoop Ecosystem).\n\u2022 Python pandas package method (Pivot table, Melt and Merge) is used for data wrangling/ Data cleansing.\n\u2022 Developed a pig script to blend data and show it in a QlikView Dashboard.\n\u2022 Migrated Relational database table to Hadoop (Hive table) Using SQOOP\n\u2022 Schedule/Automated Sqoop job in Oozie.\n\u2022 Performed Hive Query to transform the data.\n\u2022 Built a Data modelling (Dimensional and Relational modelling).\n\u2022 Developed a Macro in Alteryx using R (HTTR package). Macro will upload a file from Local Disk or Network Drive to SharePoint Libraries. Macro eliminates latency problem in Alteryx (Windows Authentication) to SharePoint.\n\n\u2022 Developed Alteryx security tool (xml parse and R). Tool provides summary report about the tools used in workflows (lists, data/table/query and connection) and act as a firewall (check memory utilization, Load to the server, Personal data and access issue) to stop workflow before publishing to server.\n\u2022 Built a custom object in Qlikview using JavaScript to get user input from the Dashboard and send it to Alteryx server to process the data and store it in a Database.\n\u2022 Automated the Alteryx software license File installation by developing a VBScript and T-SQL.\n\u2022 Conducted a Training session on how to build a custom object in QlikView.\n\u2022 Proficient in R, JavaScript, VBScript, HDFS, Hive QL, Pig Latin, Sqoop and SQL.', u'Solutions Consultant\nVitria Technology Inc - San Francisco Bay Area, CA\nSeptember 2015 to August 2016\n\u2022 Worked on Flight prediction model, joining different datasets (flight data and weather data) using Spark SQL and Hive QL, processed a weather data in Scala.\n\u2022 Built a whole machine learning pipeline to predict if flights will be late (Pyspark)\n\u2022 Built a predictive model using Random forest algorithm in R to find Driver alertness in probability based on various factors Human, Vehicle and Environmental factors. Model is used with the streaming data and published in Dashboard to visually see how each driver is performing at timestamp and alert them through Notifications.\n\u2022 Parsed a Json dataset and converted to csv file format using spark SQL and R\n\u2022 Performed network word count using Spark streaming\n\u2022 Bringing streaming data using Apache Kafka and Spark Streaming\n\u2022 Spark streaming context is used on top of Spark Core Engine to read a line from the SocketTextStream. Split the lines into multiple words based on space. Mapped each word with 1, reduce them based on key and count each occurrence of word.\n\u2022 Performed a server log analysis using Spark and wrote the result to MySQL Database.\n\u2022 Build an IOT Model in Vitria software that will fetch data from SFTP server and push the data to HDFS target. Apache spark service will fetch the data from HDFS, will process the data using predictive model (Random forest and linear regression) based on key factor indicators and published the result to the dashboard. Dashboard will display the result based on population level, device level and group level.\n\u2022 Wrote a Scala code that fetches a current and Future weather data from Rest URL. Format of data from the REST URL is Json. Parsed the desired Json field and stored the field in the database.\n\u2022 Written a JavaScript function that parse Xml data from a REST URL.\n\u2022 Written a JavaScript function to show the sample sales data in bipartite widget.\n\u2022 Created a form using HTML 5 and validate the form using JavaScript function.\n\u2022 Created SVG diagram like paths, point of interest and show details when Hover on Point of Interest in Geo-map overlay using Dojo JavaScript.\n\u2022 Worked in MYSQL and Oracle database for pulling the data.', u'Academic Internship\nPetrofac Information Service - Chennai, Tamil Nadu\nJune 2014 to July 2014\n\u2022 Performed Oracle SQL query to provide insight about change management process in the departments.\n\u2022 Created dashboard in spreadsheet to provide insight about change management process', u'Student Placement Coordinator\nK.S. Rangasamy College of Technology - Namakkal, TAMIL NADU, IN\nJune 2012 to April 2013\n\u2022 Maintained spreadsheet and database regarding student details.\n\u2022 Wrote SQL query and stored procedure to generate report every week about individual student performance.\n\u2022 Organized team presentation at the end of each week to review the group activities and to clarify their concerns.']","[u""Master's Degree in Business Analytics in Business Intelligence"", u""Master's Degree in Business Administration in Business Administration"", u""Bachelor's Degree in Engineering in Engineering""]","[u'The University of Michigan Dearborn, MI\nSeptember 2014 to August 2015', u'VIT University Vellore, Tamil Nadu\nJuly 2013 to August 2014', u'KS Rangasamy College of Technology Namakkal, TAMIL NADU, IN\nAugust 2009 to April 2013']","degree_1 : ""Masters Degree in Bsiness Analytics in Bsiness Intelligence"", degree_2 :  ""Masters Degree in Bsiness Administration in Bsiness Administration"", degree_3 :  ""Bachelors Degree in Engineering in Engineering"""
0,https://resumes.indeed.com/resume/67c2e5616a703672,"[u""Data Scientist\nAggeles\nMay 2015 to Present\nSoftware Developer\npresent 'Tips' - Java based finance software (see github repo)\nIntegrated open source .jar files for data management""]","[u'in AI', u'A.B. in Cognitive Science, minor in Computer Science', u'in Electronics Engineering']","[u'University of Georgia - Institute for Artificial Intelligence Athens, GA\nAugust 2015 to March 2016', u'University of Georgia Athens, GA\nJune 2012 to May 2015', u'Athens Technical College\nJune 2008 to May 2012']","degree_1 : in AI, degree_2 :  A.B. in Cognitive Science, degree_3 :  minor in Compter Science, degree_4 :  in Electronics Engineering"
0,https://resumes.indeed.com/resume/9c9d2952092cee05,"[u'Data Scientist (contractor)\nBig Data Solutions\nSeptember 2017 to November 2017\nSinglehandedly led and developed predictive models using complex data to recuperate medical bills.\n\u25cf Implemented advanced machine learning techniques in R such as Regression, Random forest, Bagging etc\n\u25cf Furnished executive leadership team with insights, analytics, reports and recommendations enabling effective strategic planning for the business.', u'Data analytics and Model strategy intern\nNorthern Trust\nMarch 2017 to August 2017\nAnalyzed and processed complex data sets using SQL advanced querying & analytical tool SAS E-miner.\n\u25cf Led monitoring & stress testing across credit risk reporting of two major client portfolios.', u'Research Assistant\nDePaul University\nNovember 2016 to February 2017\n\u2022 Developed and presented new ways for students to learn machine learning with tools like Python & R\n\u2022 Worked with Dean to produce well prepared, articulate and reliable course material for data science.', u'Data Analytics Intern\nDesireList\nJune 2016 to September 2016\nLed the analytics reporting of the company website using google analytics to understand the customers, consumer trends and user behavior models.\n\u25cf Identified, measured and recommended improvement strategies for KPIs across all business areas.\nQuantifiable results (For Big data solutions)\n\u2212 Propelled 1-year revenue growth from $22.1K to $32.5K for patient portfolio\n\u2212 Propelled 1-year revenue growth from $81K to $2.6M for patient portfolio with no history.\n\u2212 Achieved an improvement in model performance by 20%.']","[u'Masters of Science in Predictive Analytics in Predictive Analytics', u'Bachelors of Science in Computer Engineering in Computer Engineering']","[u'DePaul University\nJanuary 2015 to January 2017', u'Pun e University\nJanuary 2011 to January 2015']","degree_1 : Masters of Science in Predictive Analytics in Predictive Analytics, degree_2 :  Bachelors of Science in Compter Engineering in Compter Engineering"
0,https://resumes.indeed.com/resume/f6b5281e9ba3e8a9,"[u'Data Scientist\nQianbei in China\nJanuary 2018 to February 2018\n\u2022 Data cleaning, and data preparation;\n\u2022 Data visualization, data mining and predictive models using machine learning;\n\u2022 Evaluate data models/architecture.', u'Data Scientist\nKarna, LLC\nAugust 2015 to November 2017\nCDC Contractor - Division of Global Migration and Quarantine, National Center for Emerging and Zoonotic Infectious Diseases at CDC Aug. 2015 - Nov. 2017\n\u2022 Data cleaning, data quality assurance, and data management;\n\u2022 Conduct data analysis of unplanned school closure data, and create reports, outline findings;\n\u2022 Data mining, and qualitative analysis for unusual data patterns, clusters and trends;\n\u2022 Geo-mapping of unplanned school closure data and conduct geospatial analysis;\n\u2022 Machine learning, data visualization, and statistical modeling of unplanned school closures.', u'Statistician\nEagle Medical Services, LLC\nOctober 2013 to August 2015\nCDC Independent Contractor - Division of Global Migration and Quarantine, National Center for Emerging\nand Zoonotic Infectious Diseases at CDC Oct. 2013 - Aug. 2015\n\n\u2022 Modeling, simulating and data mining of unplanned school closure data;\n\u2022 Data cleaning, data management, and data analysis of unplanned school closure data;\n\u2022 Data wrangling and web data scraping;\n\u2022 Reviewed, evaluated BioMosaic web-based application, and provided school layer data for BioMosaic project;\n\u2022 Manipulated, aggregated and derived useful information from data stored across many sources and different databases;\n\u2022 Data mining of unplanned school closure data with CART and Random Forest;\n\u2022 Reviewed peer-reviewed articles. ;', u'Statistician\nChenega Government Consulting, LLC\nNovember 2010 to September 2013\nCDC Contractor - Division of Global Migration and Quarantine, National Center for Emerging and Zoonotic Infectious Diseases at CDC Nov. 2010 - Sep. 2013\n\u2022 Pulled and cleaned data from several survey databases for analysis;\n\u2022 Translated data from MS ACCESS to SAS/EXCEL data format;\n\u2022 Performed data quality assurance and quality control;\n\u2022 Updated and modified statistical programs for technical solutions;\n\u2022 Provided statistical support in the areas of survey design and analysis;\n\u2022 Applied data mining, machine learning techniques to school closure data;\n\u2022 Modeling, simulating and data mining with Machine Learning algorithms.\n\u2022 Conducted statistical analysis for 2009 Spring school closure data and Michigan Principal Survey data;\n\u2022 Worked with Epidemiologists, research fellows and EIS officers to convert specific requirements to technical solutions.', u'Research Statistician\nSRA International, Inc\nSeptember 2010 to October 2010\nCDC Contractor - Division of Global Migration and Quarantine, National Center\nfor Emerging and Zoonotic Infectious Disease at CDC Sep. 2010 - Oct. 2010\n\u2022 Conducted statistical analysis for 2009 Spring School Closure data due to H1N1;\n\u2022 Provided support in data management and data quality assurance for 2009 Spring & Fall School closure data;\n\u2022 Cleaned data from Michigan Principal Survey database for analysis.', u'Statistician\nNorthrop Grumman Information System\nApril 2008 to August 2010\nCDC Contractor - Division of Foodborne, Waterborne and Environmental Diseases within the Office for Infectious Disease at CDC Apr. 2008 - Aug. 2010\n\n\u2022 Provided statistical support in the areas of survey design and analysis, surveillance data, case-control study design and analysis;\n\u2022 Modeling of binary, count, and continuous data for the FoodNet unit and NARMS unit within the Division of Foodborne, Waterborne and Environmental Diseases;\n\u2022 Developed statistical and mathematical models to describe FoodNet Epidemiological and Laboratory findings;\n\u2022 Described current trends and changes in trends in FoodNet surveillance and NARMS data;\n\u2022 Conducted data cleaning and data management for Foodborne disease outbreak data;\n\u2022 Data mining of foodborne disease data with MCMC and Bayesian statistics.', u'Computer System Analyst\nPediatrics Infectious Disease, Vanderbilt University Medical Center\nJanuary 2001 to January 2008\n\u2022 Data analysis, data cleaning and data management on pediatric HIV infections and pediatric respiratory diseases including RSV, Rhinovirus, pertussis (whooping cough) and invasive pneumococcal infections in children;\n\u2022 Developed and maintained programs and data processing systems including Vanderbilt Pediatric Vaccine Practice and Dengue study;\n\u2022 Assisted in data collection and data management on Dengue and Shingle studies;\n\u2022 Designed, tested and modified computer-based information systems in response to user requests and problems;\n\u2022 Provided high level server support and application development.']","[u'MS in Computer Science in Artificial Intelligence', u""Bachelor's of Medicine in Biostatistics""]","[u'University of South Carolina Columbia, SC\nJanuary 1995 to December 1997', u'Fudan University Shanghai, CN']","degree_1 : MS in Compter Science in Artificial Intelligence, degree_2 :  ""Bachelors of Medicine in Biostatistics"""
0,https://resumes.indeed.com/resume/4cb7c1096d17c672,"[u'Big Data Engineer\nVintech Solutions\nApril 2017 to Present\nInvolved in Stream processing using spark and flink on the data ingesting using kafka and build analytical models out of that for downstream applications. Used hive for archival reporting on the historical data.', u'Big Data Engineer\nGCN Media Publishing\nMarch 2016 to March 2017\nMigrate the ONEcount platform from its current MySQL architecture to an appropriate, big data engine(s) so that it can scale to meet the growing demands of expanding customer base. Change existing data pipeline based on LAMP architecture to big data based toolkit.', u'Big Data Engineer\nMicron Technologies\nDecember 2014 to December 2015\nData warehousing\nBuild data warehouse to ingest data from multiple sources and analyze data to reflect internal systems. Implemented processing using spark and migrated existing workflows from MapReduce for more efficiency.', u'Data Scientist (Intern)\nVintech Solutions\nJune 2015 to August 2015\nProduct Demand Forecasting Data Scientist Intern\nEvaluate different machine learning techniques for the demand forecast of different products and adjust the production. This includes data ingestion cleaning using Hadoop environment and implementation of different classification techniques.', u""Big Data analyst\nMorgan Stanley\nJanuary 2012 to November 2014\nBuild Data Warehouse for archiving data, develop data access layer to provide unified data access platform for downstream applications and support various data analytics. HDFS and MongoDB are used as storage layer for warehouse depending on the required response times from the warehouse. The data ingestion flow is built using Flume, Sqoop and Kafka.\n\nClient: Western Union\nSocial Media Influencer Data Analyst\nPerform Sentiment analysis of different users from the data extracted from various social networking sites like Facebook, Twitter, Blogs and review websites. The data is ingested and cleansed using Map-Reduce. Polarity of users is evaluated using Open NLP package and Bag-of-Words approach.\n\nClient: Barclays\nMarket Trend Analyzer Big Data analyst\nAggregate and analyze log files generated by the web servers and stock transactional information and derive useful information to identify patterns of market trends and trader's interests. The future predictions are made about the scrip's from the past market trends and integrating that information from the news and implementing different machine learning techniques on them.\n\nClient: Nielsen Holdings\nAssort Man Hadoop Developer.\nStudy the effects of different variables in market on a particular product by analyzing transactional and operational Logs. Regression is implemented for SKU Rationalization, filling distribution gaps and to identify regional effects, effects of competitors, study and minimize effects of cannibalization.\n\nPricing Insights Hadoop Developer.\nEfficient pricing strategy is determined by analyzing prices in different areas and the prices of competitors. The pricing information is extracted from various databases using Sqoop and combined with log information ingested using Flume and cleaned and pricing strategy is developed by analysis on cleaned dataset.""]","[u'Master of Science in Computer and Information Sciences', u'Bachelor of Technology in Electronics and Communications Engineering']","[u'Purdue School of science, IUPUI\nDecember 2015', u'Jawaharlal Nehru Technological University\nMay 2011']","degree_1 : Master of Science in Compter and Information Sciences, degree_2 :  Bachelor of Technology in Electronics and Commnications Engineering"
0,https://resumes.indeed.com/resume/eedaf7ba18fdd353,"[u'Data Scientist\nVisuMenu - Berkeley, CA\nJuly 2017 to Present\nI am a Data Scientist at VisuMenu, working on digitizing interactive voice systems.\n\nI am helping make voice options visual and searchable, and creating a search engine that crawls\naudible conversations.\n\nOur goal is to make the phone become a frictionless point for user interactions, with all features of customer service platforms.\n\nThe company is part of the prestigious Berkeley SkyDeck Accelerator Program.\n\nI created and queried hierarchical trees using MySQL and expanded nested sets in MySQL.\n\nI am creating a free text search implementation using MySQL and phpMyadmin.\n\nI created hierarchical trees using Python, and used Python to find the descendants of a node when given a full hierarchical tree. I created various JSON structures to store information.', u'Associate\nDevices, and Technologies - Albany, CA\nJanuary 2011 to January 2016\nI helped develop a scientific instruments business with my grandfather who is a renowned\nscientist in the area of semi-conductors.\n\nI helped determine product specifications and requirements for researchers, academic institutions, and corporations.\n\nEmployee Status: U.S. Citizen']","[u'in Data Administration and Management', u'in Data Analysis', u'B.S. in Biochemistry and Molecular Biology']","[u'University of California, Berkeley Berkeley, CA\nDecember 2016', u'University of California, Berkeley Berkeley, CA\nMay 2016', u'University of California Davis, CA\nJune 2011']","degree_1 : in Data Administration and Management, degree_2 :  in Data Analysis, degree_3 :  B.S. in Biochemistry and Moleclar Biology"
0,https://resumes.indeed.com/resume/2bfa2aab9909efcb,"[u'DATA SCIENTIST\nCLUSTERED SEARCH OF PUBMED CORPUS - Bloomington, IN\nFebruary 2017 to July 2017\nOver 1000 lines: \u2022 Lead a project to cluster a corpus of 28 million medical and pharmaceutical\nR \u2022 LaTeX documents using unsupervised machine learning algorithms such Minibatch\nFamiliar: K-means.\nMongo \u2022 Neo4j \u2022 MySQL \u2022 Analyzed different vectorization techniques such as Tf-Idf, Hashing vectorizer\nPackages: and word embeddings.\nScikit-Learn \u2022 Pandas \u2022 Ggplot \u2022 Numpy \u2022 Performed cluster analysis by looking at inter and intra cluster distances, as well topical coherence of random clusters.\n\u2022 Built a demo search UI with Python Django, which queries clustering output\nindexed using Apache Solr.']","[u'in Sciences', u'B.ENG IN COMPUTER SCIENCE in business', u'M.S in DATA SCIENCE']","[u'Indiana University Bloomington, IN\nSeptember 2017 to May 2018', u'ANNA UNIVERSITY Bloomington, IN\nSeptember 2016 to May 2018', u'INDIANA UNIVERSITY\nMay 2018']","degree_1 : in Sciences, degree_2 :  B.ENG IN COMPUTER SCIENCE in bsiness, degree_3 :  M.S in DATA SCIENCE"
0,https://resumes.indeed.com/resume/3d1a1a721aca76ec,"[u'Data Science Consultant\nSan Francisco, CA\nOctober 2017 to Present\n\u2022 Part-time consultant in data science, analytics, artificial intelligence, and web development\n\u2022 Some personal projects:\no Developed an algorithm that recognizes sign language from video feed data using hidden markov models (python, scikit- learn)\no Built a chess game and AI using minimax tree search with alpha beta pruning (python)\no Implemented a feedforward neural network with backwards propagation using first principles (numpy, tensorflow)\no Implemented a recommendation system for music with ALS matrix factorization in Spark.\no Implemented multiple object image detection using convolutional neural network with convolutionally based bounding\nboxes (Tensorflow)\no Built a data pipeline to detect the street lanes using camera feed from a self-driving car.\no Taught a self driving car to drive in a 3d simulator by training a CNN to predict the driving angle given data from external\nsensors (Keras)', u'Data Scientist / Engineer -- Data Applications\nLinkedIn - San Francisco, CA\nOctober 2014 to October 2017\nBuilt ""Churnguard, "" a churn prediction model/data product which enables relationship managers to intelligently identify high risk\naccounts. My contribution included feature engineering, data pipelining, modeling (gradient boosting machines, random forest),\nmodel evaluation, and web development. Projected to save company millions of dollars per year.\n\u2022 Worked on a redesign of ""Crystal Ball"", a system for notifying account executives of key account activities. Designed and implemented an algorithm for personalizing the alerts feed based on users preference and past behavior. Implemented data\npipeline in production. Used Pig / Hadoop, build deploy tools (gradle, azkaban), and built custom DB connectors and microservices with Java.\n\u2022 Worked on ""Prospect Finder"", a webapp that allows sales reps to search for key contacts within LinkedIn\'s 400MM members (jquery,\npython). Created frontend and backend features\n\u2022 Implemented frontend and backend features for new and existing tools (html, javascript, python)\n\u2022 Optimized a templatable custom SQL generation engine in Python. Implemented features such as error handling, logging, retry\nhandling, queue management, and unit testing.\n\u2022 Created a tool that gathers feedback on relevance of job recommendations. This included implementing security features such as ldap login, session management, whitelist, and logging with encryption. (html, jquery, dust.js, python flask).\n\u2022 Created high performance dashboards utilizing Pinot, a real-time columnar data store. (html, javascript, highcharts, java play).\n\u2022 Created a web crawler using node.js/phantom.js to reliably scrape millions of Google jobs for SEO model.', u'Senior Data Analyst\nSan Francisco, CA\nMay 2013 to August 2014\n\u2022 Owned all payment fraud mitigation at Tagged\n\u2022 Developed rules and predictive models to proactively detect chargeback fraud. Reduced fraud levels from 10% to less than 1%.\n(logistic regression, random forest)\n\u2022 Created interactive web dashboards using SQL and R (shiny).\n\u2022 Designed tools and data models for payments and fraud team. Worked closely with engineers in a product management role to ensure successful implementation.\n\u2022 Created graph based tool in R to detect the laundering of fraudulent in-game currency across accounts. Responsible for hundreds of fraudulent accounts being shut down per month.\n\u2022 Created a tool that slices AB test results by user specified metrics for company Hackathon event. Won ""technological achievement""\nprize in company Hackathon competition (2013).\n\u2022 Created a website that interactively visualized geolocation of all users. Used Mapbox API. Won ""overall awesomeness"" prize in company Hackathon competition (2014).\nMidland Group - Phoenix, AZ\nReal Estate Development June 2012 - September 2012\n\u2022 Part of a team that developed banking centers and shopping centers across Arizona and Oklahoma.\n\u2022 Directed planning, budgeting, and reporting activities of real estate development group.', u'Enova Financial - Chicago, IL\nJune 2011 to June 2012\n\u2022 Designed profitability projection models to predict the lifetime value of new customers.\n\u2022 Designed price optimization models to determine the optimal marketing spend allocation.\n\u2022 Designed loan amount optimization models to determine the optimal loan amount to give to customers.\n\u2022 Automated the monitoring of critical business metrics using VBA, SQL, and Python.\n\u2022 Presented recommendations to senior management during weekly meetings.']","[u'Bachelors in Economics', u'in National Merit Finalist']","[u'University of Chicago', u'Barry Goldwater High School Phoenix, AZ']","degree_1 : Bachelors in Economics, degree_2 :  in National Merit Finalist"
0,https://resumes.indeed.com/resume/a2e0265862c3e626,"[u'Data Scientist\nAffine Analytics - Bengaluru, Karnataka\nNovember 2016 to July 2017\nBangalore, India\n\u2022 Worked for client, Sears Holdings\n- Implemented gradient boosting method (LightGBM) to build a logistic classifier (Dataset size: 1 Million) to output\npurchase propensity of the member to visit store next month for a business unit\n- Engineered new features by building embeddings thereby encrypting the buying pattern of a member, improving\nmodel accuracy by 4%\n- Built a gradient boosting model to capture the member engagement over mailers sent for Shop-Your-Way, Sears and K-Mart separately', u'Senior Analytics Engineer\nRobert Bosch - Bengaluru, Karnataka\nJuly 2015 to November 2016\nBangalore, India\n\u2022 Implementing supervised and unsupervised Machine Learning algorithms to solve different business problems\n- Developed a two stage SVM model (0.92 precision, 0.96 recall) to identify anaemia severity by using data from non-invasive sensors; Used agglomerative clustering to tackle class imbalance in the data; Part of research project at John Hopkins University\n- Identified Anomalies in vehicle test data files using Kullback-Leibler divergence; Implemented the solution in\nApache Spark\n- Engineered a regression model to classify the dispensed beverage through signal taken from the vibrations of coffee vending machine and predicted raw material requirements; Used Independent Component Analysis (ICA)\ntechnique to remove noise from the data\n\u2022 Building tools useful for the department as a part of Centre of Excellence team\n- Worked on a text mining platform to build modules to clean text (lemmatization, punctuation & numbers removal) and extract features (Sentence body detection, POS-tagging, key words, Chunk parser); Used nltk & gensim\nlibraries in python (English language); Hosted the tool on the cluster which could be accessed using API call\n- Built a tool for internal use in R to accommodate for forecasting techniques like exponential smoothing, ARIMA and Holt-Winters along with data preprocessing modules\nPROJECTS\nHypothesis testing Data Analysis\n\u2022 Analyzed credit card data for different hypothesis testing; Used ANOVA, Multivariate regression to identify factors for credit limit, default rate and analyze buying patterns over six months period.\nMovie Recommender Fundamentals of Computing\n\u2022 Implemented machine learning models from scratch on movie-lens data set. Compared performance of three different\nmodels: Collaborative filtering, K-means, hybrid model.\nDissertations, IIT Kharagpur Guide Prof. P. K. Ray\n\u2022 Used Holt-Winters model to improve throughput of the inventory management system by 3%; Optimized the inventory\nreplenishment process of spares and consumables at TATA Bearings, Kharagpur\n\u2022 Modeled product scheduling with machine breakdown and normally distributed processing time; Employed evolutionary\nsearch algorithm, to arrive at schedules with reduced variability of 8% compared against genetic algorithm']","[u'Master of Science in Applied Mathematics and Statistics', u'B.Tech']","[u'Stony Brook University\nJanuary 2017 to January 2019', u'Indian Institute of Technology Kharagpur, West Bengal\nJanuary 2010 to January 2015']","degree_1 : Master of Science in Applied Mathematics and Statistics, degree_2 :  B.Tech"
0,https://resumes.indeed.com/resume/e46343c3a7794934,"[u'Data Analyst\nHyve Solutions - Fremont, CA\nNovember 2017 to Present\n\u2022 Sole and first data analyst for company of 2,000+ employees\n\u2022 Analyze weekly and monthly reporting on staffing needs such as headcount, turnover, cost analysis, prepare presentation slides for weekly meetings with executives\n\u2022 Build reporting on inventory of goods for the products the Warehouses build, communicate with management and supervisors across different campuses and locations', u'People Analytics\nFederal Reserve Bank of San Francisco - Los Angeles, CA\nMay 2017 to September 2017\n\u2022 Cleaned and prepared large HR data sets using SQL and Excel from multiple sources and partnered cross-functionally with HR, Finance, Police teams to determine the optimal staffing level while minimizing overtime for Police Services (140+ employees who protect Bank assets and personnel)\n\u2022 Build interactive and dynamic Tableau dashboards using customizations and visualizations, identifying new trends and unlocking impactful insights into overtime to make workforce more cost effective\n\u2022 Communicate complex analysis and insights to broad audiences through poster and PowerPoint presentations to management and executives\n\u2022 Various ad-hoc analysis and visualizations to support various HR initiatives; translate business problems and requirements into data questions', u'Data Scientist\nCalifornia Department Of Transportation - Sacramento, CA\nJune 2015 to September 2015\n\u2022 Applied expertise in statistical methods such as multiple log-linear regression to understand and analyze how Caltrans past projects can predict future workflows with a combination of Excel and R\n\u2022 Partnered with cross-functional business and technical teams to deliver data insights, identify trends and opportunities, and answer business questions\n\u2022 Acquire, transform, manipulate and analyze large sets of data from multiple sources to understand the impact of project changes and delivery towards goals']","[u'BS in Statistics', u""Bachelor's""]","[u'UC Davis Davis, CA\nSeptember 2013 to March 2017', u'']","degree_1 : BS in Statistics, degree_2 :  ""Bachelors"""
0,https://resumes.indeed.com/resume/4a566ae0f56880ef,"[u'Chief Executive Officer\nEmbrace HealthWear\nFebruary 2018 to Present\nEmbrace Healthwear is a team of driven and devoted engineers. We are building groundwork for advances in predictive motion analytics, beginning with Libra - our wearable device that predicts\nfall risk in seniors and intervenes to prevent falls before they occur.\n\n- Business strategy, market research and evaluation\n- Operations oversight\n- Accounting and Finance\n- Investor relations, outreach and sales\n- Vision\n- Resource allocation and management', u'Machine Learning Engineer\nEmbrace HealthWear\nOctober 2017 to February 2018\nData analysis and machine learning for biometric data, with emphasis on seamless pipelining and mapping research literature to procedural findings.\n- Data preprocessing, vectorization and feature engineering.\n- Signal processing and transformation.\n- Supervised and unsupervised classification modeling, with high frequency time-series data.', u'Data Scientist\nMarch 2017 to September 2017\n-Advanced statistal methods including: Multiple imputation, structural equation modeling, missing\ndata analysis, power analysis, bayesian statistics, multivariate regression, and validity testing.\n\n-Familiar with Unix, Elixir, Python, TensorFlow, Node.js, R, LISREL, MPlus, SAS, SPSS, Matlab\nExperience with machine learning, neural nets, supervised learning, learning algorithms, and training\nmethods.\n-Developed a neural net constructor from the ground up in Elixir, with flexibility concerning training techniques, neural density, supervised learning, and unsupervised learning.\n\n-Deployed various TensorFlow machine learning programs on Google Cloud Computing platform\nto evaluate performance across various parameters.', u'Property Manager\nMay 2009 to May 2016\nManagement of rental properties, ranging from four units to 100.\nLeasing, tenant relations, contractor oversight, accounting.\n\nCarpenter', u'Program Evaluator\nAugust 2010 to June 2011\nAugust 2010 - June 2011 (11 months)\nReconstructed, tested, and validated of ARC Self-Determination Scale, using advanced data\nanaltyics.\nActed as liaison between Program Evaluation Team and clientele to explain technical results and communicate project goals.\nDeveloped, deployed, and administered surveys for grant evaluation and renewal. Presented\nfindings to Program Evaluation Team, stakeholders, and research symposium.\nData entry, cleaning, and database manipulation']","[u""Bachelor's degree in Statistics"", u'']","[u'The University of Kansas\nJanuary 2006 to January 2011', u'Caltech']","degree_1 : ""Bachelors degree in Statistics"", degree_2 :  "
0,https://resumes.indeed.com/resume/651540d55c05cedf,"[u'Data Scientist\nCBT Nuggets - Eugene, OR\nJanuary 2015 to Present\nBuilt multiple self-service reports within Tableau for all departments\n\u25cf Accessed Redshift, MongoDB, and Postgresql data with Python and advanced SQL\n\u25cf Built a rule-based Algorithm around fraud-based customers with Python\n\u25cf Forecasted Revenue based on Arima model with R\n\u25cf Collaborated on large-churn analysis project that deployed a lasso generalized model within production environment\n\u25cf Regularly provided inferential based analysis on pricing, customer information, and other\ninsights to the executive team', u'Data Analyst\nHawes Financial Group - Springfield, OR\nJanuary 2014 to January 2015\nMoved from ad hoc to dynamic-based SQL reporting for the entire company\n\u25cf Developed Advanced SQL skills\n\u25cf Forecasted Revenue\n\u25cf Built a price liquidity risk model in R', u'English Teacher\nSena ""Senaprasit"" School\nJanuary 2013 to January 2014\nManaged 17 classes with 50 students in each class\n\u25cf Presented in front of the entire school\n\u25cf Worked continually to create more engaging lesson plans\n\u25cf Planned holiday events and collaborated with other teachers in English club', u'Graduate Assistant\nWKU Hill House - Bowling Green, KY\nJanuary 2012 to January 2013\nPresented in two research conferences\n\u25cf Collaborated with other graduate students from varying disciplines\n\u25cf Conducted community-based research in a particular neighborhood to find out what resources\nthe community needed the most', u'Client Services Analyst\nSkywriter Marketing - Bowling Green, KY\nJanuary 2011 to January 2013\nCategorized projects with organization and monitoring to ensure that we could meet customer\nsatisfaction\n\u25cf Communicated clearly and precisely with both customers and vendors\n\u25cf Maintained strong profit margins, while also keeping our products at or below retail\n\u25cf Forecast potential costs involved within projects']","[u'M.A. in Applied Economics', u'', u'MA in banking']","[u'Western Kentucky University\nDecember 2013', u'Georgetown College Anthropological Huaraz, PE\nDecember 2010 to June 2013', u'Western Kentucky University']","degree_1 : M.A. in Applied Economics, degree_2 :  , degree_3 :  MA in banking"
0,https://resumes.indeed.com/resume/96458dc9ed9b8366,"[u'Research Assistant\nUniversity of Colorado at Boulder - Boulder, CO\nJanuary 2011 to Present\nExploring diffusion, virus propagation and community search in networks using graph theory and epidemics\n\xa7 Forecasted how ecological relationships (predator-pray and hosts-parasite) evolve using non-linear\ntechniques. Collaborated with the Department of Ecology and Evolutionary Biology', u""Data Scientist\nInsuranceQuotes.com - Denver, CO\nJanuary 2014 to June 2014\nMay 2016\n\xa7 Optimized lead acquisition bidding strategy using support vector machine, predictive analysis and linear\nprograming optimization\n\xa7 Created a lead scoring model that also estimated the parameters for the optimal mix of volume and quality using an ensemble of hierarchical machine-learning models\n\xa7 Developed a dynamic lead distribution model to best spread high quality leads amongst our customer\nbase to optimize satisfaction, pre-existing volume guarantees, and company profitability\n\xa7 Built a machine-learning 'Learning Lab' to discover and then automate the construction of basic/repeated\nalgorithmic steps. Reduced start-up times for new machine-learning projects from 4 weeks to 3 days"", u'Teacher Assistant\nUniversity of Colorado at Boulder - Boulder, CO\nJanuary 2013 to August 2013\nTaught C and Matlab to a class of 60 freshmen', u'Research Assistant\nIllinois Institute of Technology - Chicago, IL\nJanuary 2010 to February 2010\nctober 2010\n\xa7 Uncovered crime patterns of the city of Chicago combining unsupervised fuzzy clustering, HMM,\ngeospatial modeling, and image processing. Collaborated with the Chicago Police Department']","[u'PhD. in Applied Maths', u'MSc in Electrical Engineering', u""BS + Master's in Electrical Engineering""]","[u'University of Colorado Boulder Boulder, CO\nJanuary 2011 to January 2016', u'University of Colorado Boulder Boulder, CO\nJanuary 2011 to January 2012', u'Polytechnic University of Catalonia Barcelona, Barcelona\nJanuary 2004 to January 2011']","degree_1 : PhD. in Applied Maths, degree_2 :  MSc in Electrical Engineering, degree_3 :  ""BS + Masters in Electrical Engineering"""
0,https://resumes.indeed.com/resume/d9879843cc8f0875,"[u'Graduate Teaching Assistant\nAuburn University - Auburn, AL\nAugust 2017 to Present\n\u2022Lab assistant for undergraduate/graduate courses. Experimental Statistics-R , SAS programming. Teaching assistant for Statistics for Biological and Health Sciences, Calculus II.\n\u2022Responsible for making sure students clearly understand how to do homework/projects in R and SAS. Instructed all levels of undergraduate and graduate students with their R and SAS programming problems.\n\u2022Responsible for helping undergraduate students to understand the core concepts in a natural and relatable manner, making sure they can finish their homework and no problem for taking the tests for Stat. for Biological and Health Sciences and Calculus II.', u'Data Scientist\nAuburn University Statistical Consulting Center - Auburn, AL\nJanuary 2017 to Present\nHelped clients realize the importance of Data Analysis. Worked with university researchers and industry peers to build Statistical Models based on their needs. Conducted several statistical analyses, including Neural Network Predictive Modeling, clustering analytics, regression analysis, time-series forecasting, MALDI-TOF analysis.', u'Master Researcher\nAuburn University - Auburn, AL\nAugust 2016 to Present\n\u2022Developed neural networks prediction method for biomedical and aviation data. The method performance was evaluated using Cross-Validation and Monte-Carlo simulations. Lead the improvement of more than 30% compared to previous method.\n\u2022Performed Data Cleaning, Transforming, Mining, Analysis of big data sets. Strengthened the ability of using methodology.', u'Marketing Manager\nShanghai Chengdui Finance Information Co. ltd., Shanghai, China - Shanghai\nMay 2014 to September 2015\nUse of data-driven techniques to classify all-level-risk individuals in the online financial population, analyzing customer preference and behavioral patterns based on investment risk tolerance levels. Developed the analytical models to optimize marketing strategies and provide doable sale promotion plans.', u'Intern\nCHINA CENTRAL TELEVISION - Shanghai\nJune 2013 to March 2014\nUse of MS Excel and statistical and analytical tools to perform strategic analysis to support and enable the continued growth critical to CCTV\u2019s media organization.']","[u""Master's in Statistics""]","[u'Auburn University Auburn, AL\nAugust 2016 to Present']","degree_1 : ""Masters in Statistics"""
0,https://resumes.indeed.com/resume/ac55e77ea6a05aba,"[u'Data Scientist\nPepsiCo - Denver, CO\nJune 2016 to Present\nResponsibilities:\n\u2022 As an Architect design conceptual, logical and physical models using Erwin and build data marts using hybrid Inmon and Kimball DW methodologies.\n\u2022 Worked closely with business, data governance, SMEs and vendors to define data requirements.\n\u2022 Worked with data investigation, discovery and mapping tools to scan every single data record from many sources.\n\u2022 Designed the prototype of the Data mart and documented possible outcome from it for end-user.\n\u2022 Involved in business process modeling using UML\n\u2022 Developed and maintained data dictionary to create metadata reports for technical and business purpose.\n\u2022 Created SQL tables with referential integrity and developed queries using SQL, SQL*PLUS and PL/SQL\n\u2022 Experience in maintaining database architecture and metadata that support the Enterprise Data-warehouse.\n\u2022 Design, coding, unit testing of ETL package source marts and subject marts using Informatica ETL processes for Oracle database.\n\u2022 Developed various Qlik-View Data Models by extracting and using the data from various sources files, DB2, Excel, Flat Files and Big data.\n\u2022 Handled importing data from various data sources, performed transformations using Hive, Map Reduce, and loaded data into HDFS.\n\u2022 Interaction with Business Analyst, SMEs and other Data Architects to understand Business needs and functionality for various project solutions\n\u2022 Researched, evaluated, architected, and deployed new tools, frameworks, and patterns to build sustainable Big Data platforms for the clients\n\u2022 Identifying and executing process improvements, hands-on in various technologies such as Oracle, Informatica, and Business Objects.\n\u2022 Designed both 3NF data models for ODS, OLTP systems and dimensional data models using Star and Snow flake Schemas.', u'Data Scientist\nCharter Communications - Stamford, CT\nMarch 2013 to May 2016\nResponsibilities:\n\u2022 Worked as a Data Scientist to generate Data Models using Erwin and developed relational database system.\n\u2022 Analyzed the business requirements of the project by studying the Business Requirement Specification document.\n\u2022 Designed mapping to process the incremental changes that exists in the source table. Whenever source data elements were missing in source tables, these were modified/added in consistency with third normal form based OLTP source database.\n\u2022 Designed tables and implemented the naming conventions for Logical and Physical Data Models in Erwin 7.0.\n\u2022 Provide expertise and recommendations for physical database design, architecture, testing, performance tuning and implementation.\n\u2022 Designed logical and physical data models for multiple OLTP and Analytic applications.\n\u2022 Extensively used the Erwin design tool & Erwin model manager to create and maintain the Data Mart.\n\u2022 Designed the physical model for implementing the model into oracle 9i physical data base.\n\u2022 Involved with Data Analysis Primarily Identifying Data Sets, Source Data, Source Meta Data, Data Definitions and Data Formats\n\u2022 Performance tuning of the database, which includes indexes, and optimizing SQL statements, monitoring the server.\n\u2022 Wrote simple and advanced SQL queries and scripts to create standard and ad hoc reports for senior managers.\n\u2022 Collaborated the data mapping document from source to target and the data quality assessments for the source data.\n\u2022 Used Expert level understanding of different databases in combinations for Data extraction and loading, joining data extracted from different databases and loading to a specific database.\n\u2022 Co-ordinate with various business users, stakeholders and SME to get Functional expertise, design and business test scenarios review, UAT participation and validation of financial data.\n\u2022 Worked very close with Data Architects and DBA team to implement data model changes in database in all environments.\n\u2022 Created PL/SQL packages and Database Triggers and developed user procedures and prepared user manuals for the new programs.\n\u2022 Performed performance improvement of the existing Data warehouse applications to increase efficiency of the existing system.\n\u2022 Designed and developed Use Case, Activity Diagrams, Sequence Diagrams, OOD (Object oriented Design) using UML and Visio.', u'Data Architect/Data Modeler\nPeople Tech Group - IN\nOctober 2011 to February 2013\nDescription: Founded in 2006, People Tech is an emerging leader in the Enterprise Applications and IT Services marketplace. People Tech draws its expertise from strategic partnerships with technology leaders like Microsoft, Oracle and SAP and combines that with the deep understanding of its employees.\n\nResponsibilities:\n\u2022 Deployed GUI pages by using JSP, JSTL, HTML, DHTML, XHTML, CSS, JavaScript, and AJAX.\n\u2022 Configured the project on Web-Sphere 6.1 application servers\n\u2022 Implemented the online application by using Core Java, Jdbc, JSP, Servlets and EJB 1.1, Web-Services, SOAP, WSDL\n\u2022 Communicated with other Health Care info by using Web-Services with the help of SOAP, WSDL JAX-RPC.\n\u2022 Used Singleton, Factory Design Pattern (FDP), DAO Design Patterns based on the application requirements\n\u2022 Used SAX and DOM parsers to parse the raw XML documents.\n\u2022 Used RAD as Development IDE for web applications.\n\u2022 Preparing and executing Unit test cases\n\u2022 Used Log4J logging framework to write Log messages with various levels.\n\u2022 Involved in fixing bugs and minor enhancements for the front-end modules.\n\u2022 Implemented Microsoft Visio and Rational Rose for designing the Use Case Diagrams, Class model, Sequence diagrams, and Activity diagrams for SDLC process of the application\n\u2022 Doing functional and technical reviews\n\u2022 Maintenance in the testing team for System testing/Integration/UAT\n\u2022 Guaranteeing quality in the deliverables.\n\u2022 Conducted Design reviews and Technical reviews with other project stakeholders.\n\u2022 Was a part of the complete life cycle of the project from the requirements to the production support\n\u2022 Created test plan documents for all back-end database modules\n\u2022 Implemented the project in Linux environment.', u'Data Analyst/Data Modeler\nInnova Infotech - Bengaluru, Karnataka\nMarch 2009 to September 2011\nDescription: SYSINNOVA Infotech is an offshore software services and IT consulting company based in Bangalore, India. As a committed outsourcing partner and an IT vendor, our goal is to ensure cost effective, technical excellence and on-time deliveries.\n\nResponsibilities:\n\u2022 Worked with project team representatives to ensure that logical and physical ER/Studio data models were developed in line with corporate standards and guidelines.\n\u2022 Involved in defining the source to target data mappings, business rules, data definitions.\n\u2022 Worked with BTEQ to submit SQL statements, import and export data, and generate reports in Teradata.\n\u2022 Responsible for defining the key identifiers for each mapping/interface.\n\u2022 Responsible for defining the functional requirement documents for each source to target interface.\n\u2022 Document, clarify, and communicate requests for change requests with the requestor and coordinate with the development and testing team.\n\u2022 Work with users to identify the most appropriate source of record and profile the data required for sales and service.\n\u2022 Document the complete process flow to describe program development, logic, testing, and implementation, application integration, coding.\n\u2022 Involved in defining the business/transformation rules applied for sales and service data.\n\u2022 Define the list codes and code conversions between the source systems and the data mart.\n\u2022 Worked with internal architects and, assisting in the development of current and target state data architectures.\n\u2022 Coordinate with the business users in providing appropriate, effective and efficient way to design the new reporting needs based on the user with the existing functionality.\n\u2022 Remain knowledgeable in all areas of business operations in order to identify systems needs and requirements.\n\u2022 Responsible for defining the key identifiers for each mapping/interface.\n\u2022 Implementation of Metadata Repository, Maintaining Data Quality, Data Cleanup Procedures, Transformations, Data Standards, Data Governance Program, Scripts, Stored Procedures, Triggers and execution of test plans\n\u2022 Performed data quality in Talend Open Studio.\n\u2022 Coordinated meetings with vendors to define requirements and system interaction agreement documentation between client and vendor system.\n\u2022 Enterprise Metadata Library with any changes or updates.\n\u2022 Document data quality and traceability documents for each source interface.\n\u2022 Establish standards of procedures.\n\u2022 Generate weekly and monthly asset inventory reports.']","[u""Bachelor's""]",[u''],"degree_1 : ""Bachelors"""
0,https://resumes.indeed.com/resume/2a6d4cfefb77a9f9,"[u'Data Scientist\nCallen-Lorde Community Health Center - New York, NY\nSeptember 2015 to Present\n\u2022 Developed algorithms based on deep-dive statistical analysis and predictive data modeling that were used to deepen relationships, strengthen longevity and personalize interactions with customers.\n\u2022 Generated various Monthly, Quarterly, Bi-Yearly, Yearly reports by writing query.\n\u2022 Developed drill down, drill through, cascading parameters table and matrix report and deploy to report server. Set up cache, snapshot, subscriptions and manage report server permissions.\n\u2022 Apply machine learning models and methodologies via Python/R (Neural Nets, Bayesian, decision tree, regression etc.) to improve business outcomes\n\u2022 Developing and maintaining OLAP data warehouse to perform multidimensional analysis of health information data and provides the capability for complex calculations, trend analysis, and sophisticated data modeling\n\u2022 Improve the SSRS report performance and adjust report to better adapt with report rendering according to end-user requested format.\n\u2022 Import data from SQL Server to Excel workbook is used to establish Power Pivot table, chart and Power View dashboard.\n\u2022 Utilized SSIS to insert and update rows in a table based on contents and doing the Data cleansing caused by user entry errors or corrupted data to guarantee it is unambiguous, correct, and complete.\n\u2022 Maintain and develop the SSAS cube, build the Excel report based on cube. Quickly response to users repot request via cube.\n\u2022 Designed PowerPivot report and Power View Dashboard via various Data source. Such as SSAS cube, SQL Server Database and Excel spreadsheet.', u'Data Scientist\nEducationDynamics - Hoboken, NJ\nMay 2015 to September 2015\n\u2022 Using data mining regression technique via R/Python programming to predict revenue of our new product\n\u2022 Building Classification model (k-nearest, Decision tree, logistic Regression)to classify and filter the customers and building models to predict the effect of product and business decisions\n\u2022 Create and maintain database tables, stored procedures, views, functions, complex query and set up SQL agent jobs to execute stored procedure daily morning.\n\u2022 Use Excel Workbook to get data from SQL database, organize columns and design diagram relationships.\n\u2022 Using Power Pivot to create the Pivot table and Pivot chart based on SSAS cube to provide deep data analysis.\n\u2022 Manage the SharePoint 2013, edit pages, add web part, grant permissions to users and configure power pivot, SQL Report Server and cube for further development.\n\u2022 Establish Daily/Weekly/Monthly Power view dashboard based on the PowerPivot and SSAS cube and schedule time to refresh data automatically.\n\u2022 Create analytic chart, analytic grid, scorecard and dashboard in dashboard designer share point 2013.\n\u2022 Import and Export data from and to Excel and achieve requirement via adding conditional split, derived column, union all, sort, loop up and multiple cast transforms and deploy to SQL Server.\n\u2022 Deploy the SSIS package to load/update new data and mapping data from one server to another one.\n\u2022 Schedule jobs for executing stored SSIS packages\n\u2022 Explore the PowerBI report.', u""Database Administrator\nDepartment of Sanitation New York City - New York, NY\nDecember 2014 to April 2015\n\u2022 Database Server Migration Services from Win Server 2003 to 2008R2.\n\u2022 Create table, stored procedure, views, functions, indexes and operate agent jobs to execute stored procedure daily.\n\u2022 Include execution plan and SQL profiler to do query performance tuning. And trace running objects by third party program via SQL profiler.\n\u2022 Making and planning migration checklist to make sure every step is done.\n\u2022 Migrate the SSIS packages, SSAS database cubes and SSRS reports.\n\u2022 Set up maintenance plan, full back up of all databases, log back up of databases, weekly back up databases and optimize databases.\n\u2022 Create stored procedure, function and views and optimize existing stored procedure and function in order to improve database performance.\n\u2022 Schedule agent jobs to update and insert new records to database every day.\n\u2022 Create SSIS package to load every day's data to database.\n\u2022 Design SSRS report and dashboard to show flexible time period data, deploy report to report server.\n\u2022 Manage report server and subscribe, snapshot and cache report\n\u2022 Troubleshooting the SSIS package and SSRS report when cannot work and try to optimize process time.""]","[u'Master of Science in Statistics', u'Master of Science in mining']","[u'Fordham University\nJanuary 2017 to Present', u'Columbia University\nJanuary 2012 to January 2014']","degree_1 : Master of Science in Statistics, degree_2 :  Master of Science in mining"
0,https://resumes.indeed.com/resume/3c144554604823eb,"[u'Data Scientist\nEarthnetworks R&D Group - Germantown, MD\nOctober 2014 to April 2018\n- Develop an energy consumption prediction model for demand response, Reduce 5% total energy consumption in peak time.\n- Create a HVAC schedule ranking algorithm for WeatherBug monthly score card, to help user archive better energy efficiency.\n- Creatively crawled real estate data to build a statistic model to solve any house size estimation problem for Connective Saving Program.\n- Improve and automate the weekly report email generation and delivery process, integrate database, restful API with AWS Lambda.\n- Design a web crawler framework to greatly simplify the work and codebase by 50% for energy price, census, historical thermo data crawler.', u'Data Scientist\nThe EagleForce Inc - Herndon, VA\nFebruary 2014 to October 2014\n- Full Stack developer, develop entire ETL process and software that performs data acquisition, wrangling and storage.\n- Analyze Electronic Health Record (EHR), disease and drug data, build monitor/alert system to help day care facilities prevent accident and give them real-time alerts.\n- Integrate server code (Ruby) with scientific code (Python), bridge gaps between software engineering and scientist.', u'Data analyst/Test engineer\nCybioms Corporation - Rockville, MD\nSeptember 2013 to February 2014\n- Perform signal process analysis and data visualization on the measurement data to demonstrate business story.\n- Analyze measurement and instrument log data to quickly identify internal problem.\n- Develop automated testing tools in python to manage hardware stack.']","[u'M.S. in Electrical and Computer Engineering', u'M.S. Electrical Engineering and Information System in Machine Learning', u'B.S. Observation Control Techniques and Instruments in Electro-Mechanical Engineering']","[u'The George Washington University Washington, DC\nSeptember 2011 to May 2013', u'Xidian University XiAn\nSeptember 2008 to July 2011', u'Xidian University XiAn\nSeptember 2003 to July 2007']","degree_1 : M.S. in Electrical and Compter Engineering, degree_2 :  M.S. Electrical Engineering and Information System in Machine Learning, degree_3 :  B.S. Observation Control Techniqes and Instrments in Electro-Mechanical Engineering"
0,https://resumes.indeed.com/resume/6e73894a8cce5dac,"[u'Data Scientist\nSanford - Sioux Falls, SD\nJanuary 2017 to Present\n-Present\n\u2022 Query financial, clinical, health plan, and external data with SQL\n\u2022 Develop machine learning models using R with structured and unstructured data\n\u2022 Maintain quick turnaround on high level confidential projects for corporate executives and leadership teams\n\u2022 Created a python users group to help other colleagues in EDA understand python\n\u2022 Automate manual excel work using the pandas library in python saving co-workers around 100 hours of work time each month', u'Data Scientist\nSanford - Sioux Falls, SD\nJune 2016 to August 2016\nGain organizational skills\n\u2022 Learn how to visualize data geographically', u'Graduate Teaching Assistant\nSouth Dakota State University - Brookings, SD\nJanuary 2016 to January 2016\n-January 2017\n\u2022 Experienced firsthand how to be an effective leader in the classroom\n\u2022 Sought feedback from both students and teachers on how to become a better teacher\n\u2022 Learned different teaching styles by working with students of all levels of understanding', u'Student\nEDA\nJanuary 2013 to August 2013\nand South Dakota SAS Users\nGroup on machine learning with an emphasis on clustering and KNN models\nBrookings High School, Student Teaching, Brookings, SD August 2013-May 2014']","[u'Masters in Data science', u'Bachelor in Mathematics']","[u'South Dakota State University Brookings, SD\nJanuary 2016 to May 2017', u'South Dakota State University Brookings, SD\nAugust 2012 to December 2015']","degree_1 : Masters in Data science, degree_2 :  Bachelor in Mathematics"
0,https://resumes.indeed.com/resume/8867eb57046946a4,"[u'Bioengineer and Data Analyst\nParatus Diagnostics, LLC - Austin, TX\nSeptember 2015 to April 2017\n\u2022Analyzed data to optimize biodevices for reducing product cost from $10 to $0.1\n\u2022Invented 1 patent and designed microfluidic system based on engineering data\n\u2022Resulting in assisting to expand the size from 4 to 30 employees and getting the investments from investors millions of dollars', u'Product R&D Scientist\nShanghai Jahwa United Co, Ltd - Shanghai, CN\nMay 2012 to December 2013\n(The Largest Cosmetics Company in China as P&G)\n\u2022Supervised 1 professional engineer\n\u2022Developed over 10 products based on the data analysis of marketing and customers\n\u2022Resulting in inventing 2 patents and all the following products produced based on my patents (product line value over 10 million)', u'Researcher\nShanghai Academy of Agricultural Sciences - Shanghai, CN\nJuly 2010 to January 2012\n\u2022Supervised 2 professionals to generate the gene vaccine project\n\u2022Resulting in reducing the cost (2 dollars/chicken, each farmer owns over 10000 chickens) for chicken farmers']","[u'M.S. in Data Analytics Engineering', u'M.S. in Biology', u'B.E. in Bioengineering']","[u'Northeastern University Boston, MA\nJanuary 2019', u'Texas State University San Marcos, TX\nJanuary 2017', u'Southwest University of Science and Technology\nJanuary 2010']","degree_1 : M.S. in Data Analytics Engineering, degree_2 :  M.S. in Biology, degree_3 :  B.E. in Bioengineering"
0,https://resumes.indeed.com/resume/32a96fa71c7072e4,"[u'Data Engineer\nGlobal Algorithmic Institute - New York, NY\nFebruary 2017 to Present\n\u2022 Cloud service: Constructed & maintained data pipeline on Google & IBM Cloud (Spark, Hadoop, GPU, EC2, Database)\n\u2022 Data engineering: Built API to scrape & process data from multiple resources (Quandl, Bloomberg, Intrinio, Yahoo) with Python & PySpark (HDFS); Deployed MongoDB & SQL database on Cloud & performed data ETL.\n\u2022 ML strategy: Developed trading strategy with machine learning (SVM, RandomForest, Logistic, Boosting & ECT)', u'Data Scientist\nRebellion Research - New York, NY\nJune 2017 to December 2017\n\u2022 Data engineering: Designed data pipeline to perform data processing & feature engineering including missing data processing, abnormal & outlier detection, feature generation, normalization & encoding with SQL, Python & PySpark\n\u2022 NLP: Built API to extract News Data of target topics from Gdelt database (53 TB); Processed News data with NLP to extract features including TF-IDF, Sentiment, Topic Analysis, LDA & Text Summarization.\n\u2022 Strategy: Sought alphas in US Equity with machine learning & deep learning (Earnings, Fund Flow, VIX, HMM)\n\u2022 Risk models: Built risk models to optimize existing strategies (Skewed-t, OGARCH)', u'Founder, Trader\nDalian Ruijin Capital - Dalian, CN\nAugust 2014 to September 2016\n\u2022 Fund raising: Raised $10 million through marketing on social media (Wechat, Blog & Website)\n\u2022 Strategy: Developed strategies based on fundamental research, technical analysis & statistic models\n\u2022 Trading: Managed $ 18.5 million (peak value), end up with 52.31% cumulative return ($ 5.23 M)']","[u'Master of Arts in Statistics in Statistics', u'Bachelor of Science in Applied Mathematics in Applied Mathematics']","[u'Columbia University New York, NY\nSeptember 2016 to December 2017', u'Dongbei University of Finance & Economics Dalian, CN\nSeptember 2012 to July 2016']","degree_1 : Master of Arts in Statistics in Statistics, degree_2 :  Bachelor of Science in Applied Mathematics in Applied Mathematics"
0,https://resumes.indeed.com/resume/f6b6eb9c52105cdf,"[u'Data Scientist\nMY DR NOW - Chandler, AZ\nJune 2017 to Present\n\u2022 Reduced appointment scheduling time by automating provider search and augmenting insurance credentialing information.\n\u2022 Reduced time in referral process by automating and migrating tasks out of emr into custom editable reports.\n\u2022 Performed data forecasting for call center department to improve staffing efficiency\n\u2022 Employed machine learning to identify missing data in patient information to use it towards improving customer satisfaction.\n\u2022 Formulated SQL queries for ad-hoc reports including custom gamification of multiple ancillary teams.\n\u2022 Generated and automated detailed reports using visualization software SiSense and Power BI\n\u2022 Performed data wrangling to automate data migration using Python, VBA from multiple platforms into reporting tools.\n\u2022 Generated automated alerts for managers and providers to update on individual and department level statistics\n\u2022 Formulated CLV for different groups of patients to identify business revenue profitability and improve marketing strategies.', u'Data Scientist, Internship\nGreen Living Magazine - Scottsdale, AZ\nJuly 2016 to May 2017\n\u2022 Diagnosed demographics of magazine readers using SQL queries to strengthen marketing strategy and boost sponsorship.\n\u2022 Gathered and processed unstructured data in MS Excel, implementing advanced pivot tables to analyze the data.\n\u2022 Developed new analytical projects as a research programmer by identifying problems and writing improved procedures.\n\u2022 Communicated the analyzed results with top management, and collaborated in decision making for magazine articles.\n\u2022 Managed multiple ongoing projects to ensure objectives and timelines are met.', u'Operations Assistant\nASU SkySong - Tempe, AZ\nAugust 2015 to May 2017\n\u2022 Collaborated in Data Analytics project to develop a database of clients for Economic Development Staff.\n\u2022 Administrated data on client invoices by providing company metrics, and translated ad-hoc tasks into software solutions.\n\u2022 Developed and maintained relation with clients, advising them towards organizing events with ASU SkySong.\n\u2022 Supervised all client conference issues, and coordinated with team and team lead on facility operations.\nPROJECTS:', u'Data Analyst, Industrial Engineer\nTime Series Analysis in Healthcare - Tempe, AZ\nSeptember 2016 to November 2016\n\u2022 Implemented Time Series Analysis on real world data for total population in waiting list at National Health Services, London.\n\u2022 Gathered potential models from JMP, performed cross-validation identifying best forecast capability model.\n\u2022 Examined Exponential Smoothing methods, ARIMA models with seasonal effect and Vector Auto Regression models.', u'Data Analyst, Industrial Engineer\nData Mining Project to Build Classification Model\nSeptember 2016 to November 2016\nPerformed preprocessing in Weka to achieve equal class population and analyzed various classifiers to select best fitting\nmodel that predicted class for new observations. Analyzed Bayes Net, Na\xefve Bayes, J48, and Random forest classifiers\n\u2022 Implemented SMOTE (Synthetic Minority Over sampling Technique) and Ada-Boosting M1 with Random Forest.', u'Data Analyst, Industrial Engineer\nDecision Support System for Taiwan Federal Aviation Administration\nMarch 2016 to May 2016\n\u2022 Generated decision support system in MS Access to maintain database about commercial airlines serving in Taiwan.\n\u2022 Shaped queries, forms and reports in visual studio to increase functionality of database in SQL workbench and MS Access.\n\u2022 Conducted data migration using Python, designed database, data models, ETL processes, and BI reports through SQL.', u'Data Analyst, Industrial Engineer\nRegression Model\nSeptember 2015 to November 2015\n\u2022 Engineered a model that will provide rating for a soccer team based on its performance, and past statistics.\n\u2022 Synthesized Regression Analysis with R on ongoing league data for 90 soccer teams across 16 variable regressors.\n\u2022 Executed model adequacy, multicollinearity and model validation tests in R and Minitab generating reliable model.\nImplementing Uber Rush in Tempe Spring-2017\n\u2022 Instituted vehicle routing with multiple pick up and drop off in a route with environment friendly vehicle constraint.\n\u2022 Built a mathematical model in AMPL that provides optimized route to a rider to process multiple customers simultaneously.\n\u2022 Developed in-depth understanding of online network transportation, incorporated it for bike riders in Tempe.']","[u'Master of Science in Industrial Engineering', u'Bachelor of Technology in Mechanical Engineering']","[u'Arizona State University Tempe, AZ\nMay 2017', u'Nirma University\nJune 2015']","degree_1 : Master of Science in Indstrial Engineering, degree_2 :  Bachelor of Technology in Mechanical Engineering"
0,https://resumes.indeed.com/resume/b84e7c4770157c44,"[u'Data Scientist Engineer (Contractor)\nDecision Resources Group - Boston, MA\nJuly 2017 to December 2017\n\u2022 Designed and developed a scalable data pipeline to automate the workflow of the projects using Airflow, SnakeMake, Python, and AWS\n\u2022 Designed and developed a data profiling tool to help scientists find the right features in multi-terabyte datasets using SQL, Python, and Snowflake\n\u2022 Analyzed social network of physicians using SNA methods', u'Data Scientist Engineer (Intern)\nDecision Resources Group - Boston, MA\nJanuary 2017 to June 2017', u'System Analyst\nKhorasan News and Publication Institute\nJanuary 2009 to May 2014\nAnalyzed and Designed 25 subsystems such as Computer Aided Publication System, and Management Information System using Unified Process, UML, BPMN', u'Lead Developer\nNational Manuscript Project\nJune 2001 to May 2014\nDeveloped and maintained six generations of an award-winning multi-user application to record and publish information of millions of manuscript books. Designed and developed tools that saved roughly 85,000 man-hours, using MS SQL Server, Borland Delphi, and Sphinx', u'Programmer Analyst\nPaya System Marv, LTD\nJanuary 2001 to January 2006\nServed as a programmer analyst, developing several Two/Three-Tier software applications using Borland Delphi and MS SQL Server.']","[u'Master of Science in Information Systems', u'Bachelor of Science in Computer Science']","[u'Northeastern University Boston, MA\nMay 2018', u'Azad University of Mashhad\nMay 2004']","degree_1 : Master of Science in Information Systems, degree_2 :  Bachelor of Science in Compter Science"
0,https://resumes.indeed.com/resume/b39a06c6af92a37b,"[u'Data Scientist\nDATA SCIENCE - Houston, TX\nJanuary 2017 to Present\nRES0318AData.doc']","[u'BS in Chemical Engineering and Microbiology', u'Master of Engineering in Chemical Engineering in Chemical Engineering']","[u'University of Louisiana Lafayette, LA', u'Tulane University New Orleans, LA']","degree_1 : BS in Chemical Engineering and Microbiology, degree_2 :  Master of Engineering in Chemical Engineering in Chemical Engineering"
0,https://resumes.indeed.com/resume/c628bccd48f726b2,"[u'Data Scientist\nLumiata - San Mateo, CA\nApril 2016 to January 2018\nContributed to implementing, and maintaining medical knowledge graph database of current biomedical literature with automated update pipeline. Helped develop clinical rationale engine that provided medical reasoning for every clinical diagnosis prediction from Lumiata Matrix Suite. Built deep learning models trained on clinical EHR data sources (HL7 FHIR) for use in Risk and Care Matrix core products. Researched usage of novel graph-based techniques and algorithms for feature engineering. Used Python stack to write production code and Apache Spark big data processing for all experiments and simulations.', u'Fellow\nFellowship.AI - San Francisco, CA\nNovember 2015 to March 2016\nWorked on financial industry projects to build and implement data science and machine learning solutions. Performed data preparation, modeling and feature engineering techniques.', u'Postdoctoral Researcher\nUniversity of Pennsylvania Health System - Philadelphia, PA\nOctober 2012 to October 2015\nDeveloped and tested applications that implement texture feature extraction from large sets of clinical imaging data. Explored feature analysis in breast cancer risk estimation using correlation and regression analysis of parenchymal texture features with breast percent density.', u'Graduate Student Researcher\nUC Davis Medical Center, Department of Radiology - Sacramento, CA\nDecember 2005 to October 2012\nExamined feasibility of implementing computer-aided segmentation of hepatic lesions from clinical CT abdominal image sets and implemented segmentation performance and validation studies. Performed quantitative imaging analysis on dedicated breast x-ray cone-beam CT patient scans with clinically-diagnosed lesions. Designed and implemented prototype computer-aided diagnosis (CADx) system.']","[u'Ph.D. in Biomedical Engineering', u'Master in Biomedical Engineering', u'B.S. in Electrical Engineering']","[u'University of California-Davis Davis, CA\nJanuary 2008 to October 2012', u'University of California Davis, CA\nSeptember 2004 to January 2008', u'University of California-Davis Davis, CA\nSeptember 1999 to June 2003']","degree_1 : Ph.D. in Biomedical Engineering, degree_2 :  Master in Biomedical Engineering, degree_3 :  B.S. in Electrical Engineering"
0,https://resumes.indeed.com/resume/26ee6478ee613c64,"[u""Data scientist\nSamhsa - Rockville, MD\nJanuary 2017 to Present\nDescription: The Substance Abuse and Mental Health Services Administration is a branch of the U.S. Department of Health and Human Services.\n\nResponsibilities:\n\u2022 Acquire, clean using Talend and structure data from multiple source including external and internal databases\n\u2022 Perform data extraction, Manipulation, analysis and data mining using SQL\n\u2022 Develop and execute processes for accurate data capture across all clients to obtain key insights and relationships to overall business objectives using Statistical Hypotheses Modeling\n\u2022 Analyzed data and predicted end customer behaviors and product performance by applying machine learning algorithms using Spark MLlib.\n\u2022 Worked on enormous amounts of data to enhance customer value or reduce non-credit losses.\n\u2022 Identify and extract entity, and discover knowledge from structured and unstructured content.\n\u2022 Converted chunks of text into more formal representations using Natural language processing.\n\u2022 Generation of semantic graph based on invoice analysis to mine and identify executed action using the open source SADLlanguage\n\u2022 Perform data extraction, manipulation, cleaning, analysis, modeling and data mining using R programming in R Studio\n\u2022 Utilized Spark, Scala, Hadoop, HBase, Kafka, Spark Streaming, MLLib, R, a broad variety of machine learning methods including classifications, regressions, dimensionally reduction etc.\n\u2022 Exploratory analysis and model building to develop predictive insights and visualize, interpret, report findings and develop strategic uses of data.\n\u2022 Analyst and developer an expert system used to mine invoice data and identify executed actions using a mix of VBA and SADL Language\n\u2022 NLTK, Stanford NLP, RAKE to preprocess the data, entity extraction and keyword extraction.\n\u2022 Utilized Booted Decision Tree, Linear and Bayesian Linear Regression Machine Learning models in Microsoft Azure to develop and implement interactive Webservice predictive models\n\u2022 Executed ad-hoc data analysis for customer insights using SQL using AWS Hadoop Cluster.\n\u2022 Used External Loaders like Multi Load, T Pump and Fast Load to load data into Teradata Database, Involved in analysis, development, testing, implementation and deployment\n\u2022 Exploratory data analysis using R and PYTHON to deep dive into internal and external data to diagnose areas of improvement to increase efficiency\n\u2022 Created different charts such as Heat maps, Bar charts, Line charts, etc.,\n\u2022 Building, publishing and scheduling customized interactive reports and dashboards using Tableau Server.\n\u2022 Visualize patterns, anomalies, and future trends by applying predictive analytic techniques\n\u2022 Designed, built and deployed a set of R modeling APIs for customer analytics, which integrate multiple machine learning techniques for various user behavior prediction (CLTV, marketing funnel propensity models etc.) and support multiple marketing segmentation programs.\n\u2022 Lead the company's machine learning and statistical modeling effort including building predictive models and generate data products to support customer segmentation, product recommendation and allocation planning; prototyping and experimenting ML/DL algorithms and integrating into production system for different business needs.\n\u2022 Built models using decision trees, segmentation, regression and clustering intelligent decision models to analyze customer response behaviors, interaction patterns and propensity\n\nEnvironment:MLLib, SAS, regression, logistic regression, Hadoop 2.7.4, NoSQL, Teradata, Python, MySQL, Linux, R, Numpy, Pandas, Tableau, Excel, HTML, CSS, Bootstrap, OLTP, random forest, OLAP, HDFS, ODS, NLTK, SVM, JSON, XML and MapReduce"", u'Data scientist\nSociete Generale Americas Securities\nOctober 2015 to December 2016\nDescription: SG Americas Securities, LLC provides investment banking services. It focuses on capital markets, securities, underwriting, mergers and acquisitions, derivatives, and trading services. The firm also provides clearing, settlement, and custodial services.\n\nResponsibilities:\n\u2022 Utilized Apache Spark with Python to develop and execute Big Data Analytics and Machinelearning applications, executed machine learning use cases under Spark ML and Mllib.\n\u2022 Analyzed large data sets using R and used regression models to predict future data using R forecasting and visualized them in tableau\n\u2022 Maintained SQL scripts to create and populate tables in data warehouse for daily reporting across departments\n\u2022 Created effective visualizations using tableau and investigated a dataset to create data visualizations to tell trends and patterns in data\n\u2022 Worked in MySQL database on simple queries and writing Stored Procedures for normalization and renormalization.\n\u2022 Worked on machine learning on large size data using Spark and MapReduce.\n\u2022 Used SQL language to write queries inside the SQL server database.\n\u2022 Maintained SQL scripts to create and populate tables in data warehouse for daily reporting across departments.\n\u2022 Performed data analysis by using Hive to retrieve the data from Hadoop cluster, SQL to retrieve data from Oracle database.\n\u2022 Used SQL language to write queries inside the SQL server database\n\u2022 Performed preliminary data analysis using descriptive statistics and handled anomalies such as removing duplicates and imputing missing values.\n\u2022 Analyzed large sales data of Cisco devices and used R for predicting the future sales using regression models\n\u2022 Created alerts and notifications for system errors, insufficient resources, and fatal database errors.\n\u2022 Used the Django Framework to develop the application.\n\u2022 Collaborated with data engineers, wrote and optimized SQL queries to perform data extraction from SQL tables\n\u2022 Responsible for preparing the existing SQL platform for upgrades that were installed as soon as they were released. Assisted in creating and presenting informational reports for management based on SQL data\n\u2022 Created histograms, bar charts, frequency plots, computed 3 measures of distributions: the mean, median and mode\n\u2022 Provided dedicated supports for development, testing and production SQL Server environment.\n\u2022 Performed daily tasks including backup and restore by using SQL Server 2014 tools like SQL Server Management Studio and SQL Server Agent\n\u2022 Used tableau to visualize data from a given dataset. Used R to do statistical modeling and did data transformation before using the data in tableau and visualizing it. Wrote R scripts and connected with tableau using an external ODBC in tableau.\n\u2022 Build SQL queries for performing various CRUD operations like create, update, read and delete\n\u2022 Worked on NOSQL databases like MongoDB, HBase.\n\u2022 Extracted data from HDFS and prepared data for exploratory analysis using data munging.\n\nEnvironment:CDH5, HDFS, Hadoop 2.3, Hive, Impala, AWS, Linux, Spark, python, Tableau Desktop, SQL Server 2014, Microsoft Excel, Matlab, Spark SQL, Pyspark.', u'Data Analyst/Data Modeler\nGE Capital - Norwalk, CT\nDecember 2013 to September 2015\nDescription:At GE Capital worked at the project named ALLL. The objective of this project was to create a central repository for data storage, historical analytics, and federal requests. As a Business/Data analyst I worked closely with business and technical team to implement central repository for users so they can store data securely and get the proper report at the end of the quarter.\n\nResponsibilities:\n\u2022 Designed and Developed Oracle11g, PL/SQL Procedures and UNIX Shell Scripts for Data Import/Export and Data Conversions.\n\u2022 Worked on Data modeling using Dimensional Data Modeling, Star Schema/Snow Flake schema, and Fact & Dimensional, Physical & Logical data modeling\n\u2022 Data exploration, Data Profiling, Data Quality and ETL to load and transform huge data sets.\n\u2022 Data Profiling, Data Analysis; identify and implement business rules to uniquely identify Securities.\n\u2022 Worked on logical and physical modeling of various data marts as well as data warehouse using Taradata14.\n\u2022 Gathered and analyzed existing physical data models for in scope applications and proposed the changes to the data models according to the requirements.\n\u2022 Custom Asset Classification using Python\n\u2022 Performed Data Validation and Data Cleaning using PROC SORT, PROC FREQ and through various SAS formats.\n\u2022 Developed Tableau Dashboard for Enterprise Security Reporting and Analytics\n\u2022 Assess the data quality using python scripts and provide the insights.\n\u2022 Implemented R packages for data manipulation\n\u2022 Worked in using Teradata14.1 tools like Fast Load, Multi Load, T Pump, Fast Export, Teradata Parallel Transporter (TPT) and BTEQ.\n\u2022 Prepared Data Visualization reports for the management using R.\n\u2022 Create MDM base objects, Landing and Staging tables to follow the comprehensive data model in MDM.\n\u2022 Created jobs, alerts to run SSIS, SSRS packages periodically. Created the automated processes for the activities such as database backup processes and SSIS, SSRS Packages run sequentially using SQL Server Agent job and windows Scheduler.\n\u2022 Utilized SDLC and Agile methodologies such as SCRUM\nEnvironment:ERwin9.1, Teradata14, Oracle11g, PL/SQL, UNIX, Agile, Azure, TIDAL, MDM, ETL, BTEQ, SQL Server2008, Netezza, DB2, SAS, Tableau, UNIX, SSRS, SSIS, T-SQL, MDM, Informatica, SQL.', u'Business Analyst/Data Analyst\nBDIPlus Company - New York, NY\nSeptember 2012 to November 2013\nDescription:BDIPlus is a Strategy Consulting and Software company focused on providing capabilities and solutions that result in the creation of forward-looking and long lasting competitive advantage. Our solutions reflect our unparalleled technological expertise and our deep domain knowledge of the Financial Services and Insurance industries.\nResponsibilities:\n\n\u2022 Collaborated with business partners from multiple disciplines to elicit, document, prepare and manage business requirements package for stakeholder sign-off and delivery to technical teams.\n\u2022 Broke down Intense analysis into multiple sessions for all the team members to get a clear understanding on the requirements and made sure there are no show stoppers.\n\u2022 Documented requirement artifacts utilizing industry standard diagram techniques to enhance the clarity of definition, including: process flows, context diagrams, use cases, Wireframes, etc.\n\u2022 Established meaningful traceability between related requirements. Assisted the Project Manager &Business Analyst in communicating to project stakeholders on project progress and risks and collected feedback.\n\u2022 Created Use-Cases and Business Use-Case Model after accessing the status and scope of the project and understanding the business processes.\n\u2022 Provided project support to the Project Manager& reviewed the efficiency and effectiveness of service delivery.\n\u2022 Worked with third-party vendors when documenting Statement of Work (SoW).\n\u2022 Acted as a liaison between the business and IT design and delivery teams.\n\u2022 Interfaced with Business Partners, Technical resources (i.e. Solution Engineers, Systems Analysts, Developers, Architecture, and Quality Assurance) to translate and simplify requirements, ensure requirements are met and verify that the implemented solution meets the requirements.\n\u2022 Collaborated with QA team and SMEs to ensure adequate test coverage.\n\u2022 Interacted with various end user teams during beta testing to gather feedback on application usability.\n\u2022 Extensively worked on IBM RTC to track aspects of work such as work items, source control, reporting, and build management.\n\u2022 Conducted User Acceptance Testing (UAT) and collaborated with the QA team to develop the test plans, test scenarios, test cases, test data to be used in testing based on business requirements, technical specifications and/or product knowledge.\n\u2022 Performed due diligence and identified possible production failure scenarios, created incident tickets in Service Now and communicated effectively with the development team and the business units.\n\u2022 Expertise in Design and development of CMDB (Configuration Management) in Service Now.\n\u2022 Worked extensively on SoapUI for testing SOAP Web Services functional testing, REST APIfunctional testing.\n\u2022 Worked on Jira for Average age report, Pie chart report, resolution time report, user workload report, version workload report, and workload pie chart report & HPQC environment to help the Quality Assurance team in defect tracking.\n\u2022 Extensively worked on tableau for creating Pie chart, Line chart, and Grid chart, created interactive dashboards, created interactive maps.\n\u2022 Used Axure RP to create Workflow, Sitemaps, Templates, widgets.\n\u2022 Extensively worked on Requirement Management Plan\n\u2022 Worked on SQL for testcase execution.\n\nEnvironment:Windows, MS Office (MS Word, MS Excel, MS PowerPoint), Quality Center, Axure RP Pro, Jira, JAVA, IOS, .Net, Rally, MS Visio, SharePoint.', u'Business Analyst\nHSBC Bank - Bengaluru, Karnataka\nDecember 2010 to August 2012\nDescription:HSBC Bank India, is an Indian subsidiary of UK-based HSBC Holdings plc, is a bank with its operational head office in Mumbai. It is a foreign bank under the Banking Regulation Act, 1949 and thus is regulated by the Reserve Bank of India (RBI).\n\nResponsibilities:\n\u2022 Assisted Project Manager and Senior Management with Project Plans, Timeline and Workforce.\n\u2022 Coordinated in Sprint Planning to identify and monitor the activities performed in each iterative.\n\u2022 Conducted GAP Analysis and Impact Analysis to define the changes of the project.\n\u2022 Interacted with user groups to derive the Business and Functional Requirement Documents.\n\u2022 Participated in Front Office Analytics and Pricing, including Asset Classes valuation against Bloomberg (future/forward, options, FX options, Eurodollar/interest rate future, CFD, warrants, etc.) using Black Sholes Model and cash flow discount model.\n\u2022 Worked with accountants to write specification on NAV, local/base MV, P/L, Cr/Dr, etc. to prepare performance reports, trial balances and financial statements.\n\u2022 Calculated intrinsic value and time value, and Greeks (delta, gamma, vega, theta).\n\u2022 Set up securities (T-Bonds, IR Futures, Eurodollar and Futures, IRS, CDS, Commodity Futures and Options, etc.) in Front Arena for trading.\n\u2022 Constructed yield curves (Libor, Swap Rates) and volatility surfaces to serve as basis for valuation.\n\u2022 Assisted in defining business process flows for Street-to-Fund Reconciliation and Hedge Fund-to-PB Reconciliation.\n\u2022 Analyzed Bloomberg data and streamline of loading Security Master, time series data and prices from Bloomberg and Reuters.\n\u2022 Worked with FAs to retrieve and set up benchmarks like BEY, EAR, Expected Return, Market Risk, Sharpe Ratio, NPV and IRR, etc.\n\u2022 Created Test Plans and Test Cases and coordinated team effort on Functional Testing, Migration Testing, Performance Testing, Regression Testing.\n\u2022 Coordinated with Identifying defects and gaps in the core products, managing bugs and assisted development of enhancements to meet business requirements in JIRA Platform.\n\u2022 Defined testing methods and created/executed test, assisted with end-to-end UAT.\n\nEnvironment:Agile, UML, Rational Requisite Pro, Enterprise Architect, MS Word, MS Excel, MS Outlook, MS PowerPoint, MS Project 2010, Seapine, MS Visio, Rational Rose, .Net, SAP, SQL Server, JIRA.', u'Software Engineer\nRanbaxy Pharmaceuticals - Hyderabad, Telangana\nMay 2009 to November 2010\nDescription:Ranbaxy Laboratories Limited is an Indian multinational pharmaceutical company that was incorporated in India in 1961. The company went public in 1973 and Japanese pharmaceutical company Daiichi Sankyo acquired a controlling share in 2008.\n\nResponsibilities:\n\u2022 Worked in development of applications especially in UNIX environment and familiar with all its commands.\n\u2022 Design, develop, test, deploy and maintain the website.\n\u2022 Designed and developed the UI of the website using HTML, CSS and JavaScript.\n\u2022 Build SQL queries implementing functions, packages, views, triggers, and tables.\n\u2022 Using Subversion version control tool to coordinate team-development.\n\u2022 Designed and developed data management system using MySQL.\n\u2022 Handled all the client-side validation using JavaScript.\n\u2022 Responsible for debugging and troubleshooting the web application.\n\u2022 Handling the day to day issues and fine tuning the applications for enhanced performance.\n\nEnvironment: MySQL, Linux, HTML, XHTML, CSS, AJAX, JavaScript, SQL, MySQL, Apache Web Server, UNIX.']","[u""Bachelor's""]",[u''],"degree_1 : ""Bachelors"""
0,https://resumes.indeed.com/resume/1425b67fee53fda6,"[u'Data scientist and statistical analyst (Supervisor\nCenter for Renewable Carbon, the University of Tennessee - Knoxville, TN\nJune 2017 to Present\nTimothy M. Young)\n\u25e6 Developed multiple models (Multiple linear regression, Partial least squares, Decision tree, Boosted tree, Bootstrap forest, Neural network, and Bayesian additive regression tree) using a manufacture data\n\u25e6 Utilized regression tree and MLR model to identify the interactive effects using the same manufacture data\n\u25e6 Identified opportunities to apply quantitative methods (e.g., DOE) to improve business performance', u'Research &Teaching assistant (TA)\nCenter for Renewable Carbon, the University of Tennessee - Knoxville, TN\nAugust 2014 to May 2017\nDeveloped survey for project of certification and finished survey collection using face-to-face method\n\u25e6 TA for Multivariate and Data Mining Techniques (STAT 576; Graduate level)\n\u25e6 TA for Human Dimension/Natural Resources (FWF 320; Undergraduate level)\n\u25e6 TA for Conservation (FWF 250; Undergraduate level)']","[u'M.S. in Statistics', u'Ph.D. in Natural Resources', u'M.S. in Forestry']","[u'The University of Tennessee Knoxville, TN\nMay 2017', u'The Mississippi State University Starkville, MS\nMay 2017', u'The Shandong Agricultural University Taian, CN\nMay 2014']","degree_1 : M.S. in Statistics, degree_2 :  Ph.D. in Natral Resorces, degree_3 :  M.S. in Forestry"
0,https://resumes.indeed.com/resume/de2cd31ea5a2e206,"[u""Marketing Principal Data Scientist\nTractor Supply Company - Nashville, TN\nDecember 2017 to Present\n\u2022 Support marketing with advanced analytics, machine learning by SQL, R and Python, and Hadoop.\n\u2022 Building models with machine learning by Python (numpy, scipy, matplotlib, scikit-learn, keras, and pandas) to predict loyalty membership in segmented markets for campaign purposes.\n\u2022 Cohort analysis with different years by sql in python to build customer insight for the best customer groups.\n\u2022 Analysis of variance (ANOVA) by building linear regression in Python and R to measure the multiple factors related with sales' transactions impacting the total margin."", u""Marketing Analytics Manager\nSunTrust Bank - Richmond, VA\nApril 2013 to December 2017\n\u2022 Build and validate a mortgage refinance cash out model by Python and SAS for marketing purposes.\n\u2022 Helped marketing people to analyze/monitor/generate executable programs for business purposes with Adobe Omniture digital marketing data.\n\u2022 Used Hadoop to facilitate insightful periodical analyses of website data collected by external sources, in order to get useful information for potential clients. Also created a fuzzy logic to find matching with clients' other information.\n\u2022 Responsible for the design and development of the new programs for supporting Mortgage campaign, consumer lending, and cross sell with the Marketing database. Also responsible for repairing and improving the existing programs for digital marketing with SQL and Python.\n\u2022 Identify solutions and develop technology plans to improve efficiency and services to our clients. Responsible for building the validation mechanisms for the team targeting routine and ad-hoc projects using different databases.\n\u2022 Responsible for performing digital marketing data quality analysis. Responsible for taking requests for ad-hoc analysis and making them production ready including exploratory data analysis, code preparation and review, summary findings and conclusions, and delivery to the customer.\n\u2022 Statistical analysis including paired and un-paired t-test and multi factor analysis by R on the Mortgage marketing campaign projects to find out the better marketing strategy such as direct mail or email or paid search or other social media."", u""Senior Statistician\nHome Depot Inc - Atlanta, GA\nJanuary 2011 to February 2013\n\u2022 Support Customer Insight Strategic Planning by SAS programming to generate many different sale-transaction/demographic reports. \u2022 Created various SAS Macros for different projects such as trigger-target and additional income variables for demographic data.\n\u2022 Support CRM with data analysis and programming for campaign purposes such as the new mover program and the military discounts' program. Customer marketing basket analysis including on-line shopping behavior.\n\u2022 Support Credit Card Department to help generating the Home Depot credit card users' shipping free programs. Using ETL to support Home Depot external business partners such as Fico, IDM, Epsilon, Yahoo, Paypal for marketing or digital marketing purposes.\n\u2022 Built and validated segmentation models for splitting professional and consumers with data sampling/survey and advanced statistical techniques including decision tree, and logistic regression.\n\u2022 Built a CLV (customer lifetime value) model with the non-linear regression targeting HD professional customers."", u'Senior Analyst\nTransamerica Insurance Inc - Charlotte, NC\nApril 2009 to October 2010\n\u2022 Performed demographic data analysis including sex, age, occupation, education, region, health conditions using claim data.\n\u2022 Created various SAS datasets with SQL and performed statistical analysis and validation.\n\u2022 Prepared datasets by sampling claim and non-claim data and built a logistic model to predict claim counts.\n\u2022 Performed ad-hoc analysis to support new and existing program development and customer inquiry. Developed various metrics for measuring, monitoring and controlling products risk related claims.\n\u2022 Developed SAS Macros and performed statistical analysis working with large volume of data for supporting Market Operation Department in order for customer segmentation.', u""Senior Scientist\nAnalyst Philip Morris Inc - Richmond, VA\nJanuary 2008 to February 2009\n\u2022 Developed SAS time series models to forecast new products' sales volume with different regions, promotions, flavor combinations, and product price in order for quality improvement, and strategic planning.\n\u2022 Provided statistical assessments associated with product specifications. Created various SAS datasets with SQL and performed statistical analysis and validation.\n\u2022 Developed SAS demand forecasting models for estimation of premium products' sales volumes using historic data in order for strategic planning."", u'Senior Scientist/Analyst\nPfizer Pharmaceutical Inc - Pearl River, NY\nOctober 2006 to January 2008\n\u2022 Designed experiments for formulation development, analytical method development, process capability, and stability studies.\n\u2022 Developed SAS macros to facilitate automate data management, tabular and graphical data summaries for statistical designs and analyses.\n\u2022 Responsible for data management, statistical programming, and statistical analysis of product development data (e.g. formulation development, analytical development, stability trending, specification setting, process capability).', u""Senior Scientist\nAnalyst Philip Morris Inc - Nashville, TN\nApril 1996 to September 2006\n\u2022 Generated statistical reports of market analysis on new products launching and provided trend analysis of expenses that were used as basis of products pricing process.\n\u2022 Worked with R&D scientists in the problem definition, study design, data collection, statistical analysis, and communication of results.\n\u2022 Developed, modified and validated VBA programs to produce customized weekly and monthly, quarterly, and yearly statistical analysis reports on production operations for risk management and quality improvements by the plant management team.\n\u2022 Developed, optimized and validated mathematical models and macros to arrive at the best forecasting formula for raw materials' chemical and physical properties in manufacturing operations, based on past usage data that helped reduce forecasting errors from \xb132% to within \xb112%."", u'Modeler\nPurdue University - W Lafayette, IN\nNovember 1994 to April 1996\nModeling and optimization for an ethanol bioconversion process.']","[u'', u'in Statistics', u'Ph.D. in Chemical Engineering in Chemical Engineering', u'B.S. in Chemical Engineering in Chemical Engineering']","[u'Nashville Tech Institute Nashville, TN\nJanuary 1999 to January 2001', u'Ohio University Athens, OH\nJanuary 1992 to January 1994', u'Ohio University Athens, OH\nJanuary 1994', u'Zhejiang University Hangzhou, CN\nJanuary 1990']","degree_1 : , degree_2 :  in Statistics, degree_3 :  Ph.D. in Chemical Engineering in Chemical Engineering, degree_4 :  B.S. in Chemical Engineering in Chemical Engineering"
0,https://resumes.indeed.com/resume/8d48e76dc4e45b0a,"[u'Data Analyst Intern\nLegacy Research Institute - Portland, OR\nJanuary 2017 to Present\nUSA\n\u2022 Provided data management and data handling in support of new clinical outcomes\nresearch program.\n\u2022 Collaboratively worked with Research Scientists, Managers, and Regulatory\nCompliance Coordinator to optimize study design and data collection.\n\u2022 Contributed to the development of research proposals, progress reports, and manuscripts.', u'Graduate Research Assistant\nOregon State University - Corvallis, OR\nJanuary 2012 to January 2017\nUSA\n\u2022 Provided data analysis to the Gramene database resulting in 3 publications and annual renewal of a $10 million grant awarded by the NSF to provide the research\ncommunity with genetic resources for multiple plant species\n\u2022 Developed and adapted pipelines for processing large sets of genomic data to reduce computational time\n\u2022 Managed multiple research projects resulting in co-author publications', u'Junior Scientist\nPacific Northwest National Laboratory - Richland, WA\nJanuary 2011 to January 2012\nUSA\n\u2022 Engaged with Lead Scientists and Project Managers to communicate program\nstatus and meet project deliverables deadlines.\n\u2022 Provided data analysis to the Low Dose Radiation Research Program for risk\nestimates of skin radiation during manned space travel.\n\n\u2022 Managed information and time to contribute data analysis and scientific writing to 2\nco-author publications within an 8-month period.']",[u'B.S. in Biology'],"[u'Oregon State University Corvallis, OR\nJanuary 2007 to January 2012']",degree_1 : B.S. in Biology
0,https://resumes.indeed.com/resume/3c358cee0fc8d5b8,"[u'Data Scientist Intern\nData Society - Washington, DC\nJune 2017 to October 2017\n\u2022 Outlining the course flow for Data Science courses and guiding a team of two interns\n\u2022 Creation of course content and exercises for advanced classification, clustering and Principal Component Analysis\n\u2022 Building a shiny dashboard using plotly, highcharter, leaflet and deploying on shiny server', u'Data Analyst Intern\nIndian Institute of Tropical Meteorology\nMay 2014 to March 2016\n\u2022 Analyzing 9 years of Clouds, Rainfall and Sea Surface Temperature data to study time series correlations, spatial correlations and finding dominant patterns using PCA\n\u2022 Comparing 45 years of rainfall and Sea Surface Temperature data based on bias and seasonal cycles\n\u2022 Presenting and discussing results with a group of scientists']","[u'Master of Science in Computational Science', u'Master of Technology in Modeling and Simulation', u'Bachelor of Engineering in Computer Engineering']","[u'George Mason University Fairfax, VA\nMay 2018', u'University of Pune Pune, Maharashtra\nJune 2015', u'University of Pune Pune, Maharashtra\nJune 2013']","degree_1 : Master of Science in Comptational Science, degree_2 :  Master of Technology in Modeling and Simlation, degree_3 :  Bachelor of Engineering in Compter Engineering"
0,https://resumes.indeed.com/resume/ff6ebdde25279a3c,"[u'Data Scientist\nData Science for the Public Good (DSPG)\nMay 2017 to July 2017\n\u2022 Worked on projects funded by the NSF and Army Research institute\n\u2022 Used R and SQL to collect, cleanse, and provide modeling and analyses of structured and unstructured data\n\u2022 Collaborated and coordinated directly with sponsors and team members to determine projects to determine project scope and specifications.', u'Sales associate\nFamily Christian book store\nJune 2016 to August 2016\n\u2022 Managed inventory, sales work, and maintenance in the store\n\u2022 Learned skills such as using cash register and promoting products that were featured']",[u'B.S. in Business Information Technology in Decision Support Systems'],"[u'Virginia Polytechnic Institute and State University Blacksburg, VA\nMay 2019']",degree_1 : B.S. in Bsiness Information Technology in Decision Spport Systems
0,https://resumes.indeed.com/resume/adb1e4cc2d420503,"[u'Associate Data Scientist\nDatacubes Inc - Chicago, IL\nAugust 2017 to Present', u'Inside Sales/Data Analyst Intern\nSilicon Labs - Austin, TX\nMay 2016 to August 2016\n1. B2B Integration: Tasked with data synchronization of historical sales data for a distributor to facilitate shift to B2B system of recording sales. Wrote a script using Python for the synchronization of 50,000 records and automated many manual tasks.\n2. Kit Improvement: Tasked with making a tool and kit reference database so that it can be accessed company wide and that\nit can be used to update the website kit list. Wrote a Python script to combine data from different excel databases obtained from various departments and removed any mismatched and deprecated items.', u'Analyst\nEclipse International - New Delhi, Delhi\nJune 2014 to June 2015\nWorked with mattress production company in the sales and marketing department and briefly in the supply chain department.\n\u2022 Helped formulate strategic plan by benchmarking current mattress manufacturers and analyzing current industry landscape\n\u2022 Optimized and expanded the supply chain of mattresses by identifying new suppliers and distributors\n\u2022 Worked on a project in tandem with the sales department and design team, which met weekly to discuss product penetration in various states and changes if required to the product', u""Analyst Intern\nJ.P. Morgan - Mumbai, Maharashtra\nJanuary 2014 to June 2014\nWorked in the Investment banking research arm for Technology, Media and Telecommunications (TMT) sector in the Europe, Middle East, Africa (EMEA) region.\n\u2022 Worked in tandem with London bankers to prepare pitch books including market and industry landscapes, financial analysis and valuations, equity research, target companies\u2019 profiles\n\u2022 Worked on some notable deals: $990mn Yahoo's acquisition of Tumblr, $18bn Liberty Global acquisition of Virgin Media\n\u2022 Analyzed financial statements and updated trading and transaction comparable on a quarterly basis, used for calculating deal multiples and financial ratios"", u'Assistant Manager\nTata Motors Ltd - Rudrapur, Uttarakhand\nSeptember 2013 to May 2014\nWorked in production in the vehicle assembly line having a roll-out of 300 vehicles/day and supervised a workforce of 400 people.\n\u2022 Led new product introduction team in my shop and worked in cross-functional teams to roll out new products entailing: production planning, design modifications, process management and demand forecast\n\u2022 Conducted MOST (Maynard Operation Sequence Technique) study for line balancing and reduction of men on roll in the shop by 50 manpower/day, part of Process management activity\n\u2022 Updated and improved Work Instruction Sheets (WIS) in my shop to make them according to each process rather than the previous standard of making them station wise, part of Process management activity\n\u2022 Trained workers in the practices of waste reduction, lean principles and work instruction sheets in shop\n\u2022 Analyzed defects and performed root cause analysis to improve First Shot Okay(FSO) percentage from 22% to 98% and thus increasing efficiency\n\u2022 Coolant filling automation idea implemented on assembly line achieving a saving of 26 man-days and Rs.10,000/month: presented in a case study competition at INSSAN (Indian National Suggestion Scheme Association) - 2014, competing against 300 teams from over 100 companies in India']","[u'Master of Science in Operations Research and Industrial Engineering', u'Bachelor of Engineering in Mechanical Engineering', u'Master of Science in Economics']","[u'The University of Texas at Austin Austin, TX\nAugust 2015 to August 2017', u'Birla Institute of Technology and Science, Pilani Pilani, Rajasthan\nAugust 2008 to July 2013', u'Birla Institute of Technology and Science, Pilani Pilani, Rajasthan\nAugust 2008 to July 2013']","degree_1 : Master of Science in Operations Research and Indstrial Engineering, degree_2 :  Bachelor of Engineering in Mechanical Engineering, degree_3 :  Master of Science in Economics"
0,https://resumes.indeed.com/resume/b55552475b5e4309,"[u'Data Scientist\nAutomattic - Atlanta, GA\nJanuary 2016 to Present\nText analysis and general anlytics', u'Data Scientist\nInform - Atlanta, GA\nMarch 2014 to January 2016\nNews recommendation']","[u'PhD in Computer Science', u'MA in Education', u'MS in Electrical Engineering', u'BS in Mathematics']","[u'University of Chicago\nDecember 1998', u'University of California at Berkeley Berkeley, CA\nMay 1990', u'Georgia Institute of Technology\nDecember 1985', u'Morehouse College\nMay 1985']","degree_1 : PhD in Compter Science, degree_2 :  MA in Edcation, degree_3 :  MS in Electrical Engineering, degree_4 :  BS in Mathematics"
0,https://resumes.indeed.com/resume/2e0b0e7ce8309142,"[u'Data scientist\nSamsson - Tehran\nSeptember 2016 to Present\nCreating an open source recommender system called Rexy, a gamification engine and a django platform for managing the product documentations.', u'Python Developer\nParspooyesh - Tehran\nAugust 2015 to June 2016\nWorking on core of an Internet Billing System application by rewriting and optimizing the codes, migrating to Python-3.x, working with Django framework and mongodb']","[u""Bachelor's Sofware engineer""]",[u''],"degree_1 : ""Bachelors Sofware engineer"""
0,https://resumes.indeed.com/resume/ab4eeebaa49e1e6c,"[u""Data Scientist\nBRP - Evinrude - Quality Department - Sturtevant, WI\nJuly 2012 to Present\nAt BRP my main focus is to set, maintain and analyze the warranty budget using statistical analyses and modeling. Maintaining the budget consists of monitoring product performance metrics and identifying emerging trends as risks and opportunities.\n\nOutside of my main function, I strive to create an efficient technical atmosphere for myself as well as others within the organization. I take data from SQL and funnel it down to Access databases in which I've modeled. From there I design user interfaces for the user to easily be able to gather the information they are looking for. For example, I've created an interface for engineers to be able to statistically compare the field failure rate (on a relative time basis) on something as generic as an entire line of engines, down to each specific part.\n\nPrimary roles and responsibilities...\n\nSet and maintain yearly warranty budget ($10M).\nDatabase Administration.\nAnalyze emerging failure modes.\nStreamline cross-functional processes.\nCommunicate and present data to all levels of the company as well as product dealers.\nMentor co-ops."", u'Franchise Owner and Manager\nCollege Pro Painters\nJanuary 2011 to December 2011\nCollege Pro Painters is a well-known painting service that specializes in giving college students the opportunity to run their own business by providing painting services. Owning and managing a franchise allowed me to put many of my personal skills to the test on the fly. The opportunity taught me the true importance of acceptance and adaptability.\n\nPrimary Roles and Responsibilities\nHad to swiftly learn how to run a small business and immediately transfer the knowledge to the field.\nAttended a training seminar in Minneapolis to learn effective methods to run a successful franchise.\nWas in charge of hiring, training, marketing, sales, accounting and customer service.']",[u'BBA in Finance'],"[u'University of Wisconsin Milwaukee, WI']",degree_1 : BBA in Finance
0,https://resumes.indeed.com/resume/a4b70be6a994ac36,"[u'Senior Data Scientist\nOracle - Las Vegas, NV\nJune 2016 to Present\n\u2022 Developed and implemented data pipeline process for Oracle Cancer Research Cloud\n\n\u2022 Developed and implemented machine learning algorithms to enable data curation for Oracle Cancer Research Cloud\n\n\u2022 Developed and implemented machine learning algorithms for anomaly detection in Oracle Managed Cloud customer instances\n\nTools, techniques, and skills used: R, Spark, Python, Oracle dB, Oracle Exadata Platform, Machine Learning, Neural Networks, Statistical Analysis, Data Quality Management', u'DATA SCIENTIST\nNIKE - Beaverton, OR\nNovember 2015 to June 2017\n\u2022 Developed a neural network model to evaluate customer preferences among groups of Nike products, which will be used to optimize targeted communications and aggregate demand forecasting\n\n\u2022 Created an intermediate data table structure for reduced clickstream data for a new Nike app and led the team that implemented the ETL to create the intermediate data tables, empowering the business to quickly prioritize new features and in-app content\n\n\u2022 Developed and delivered into production a predictive model to evaluate a customer\u2019s probability to purchase a specific Nike product based on their previous purchasing, browsing, and app usage history, resulting in over a 2x increase in the conversion rate of in-app offers\n\n\u2022 Led the effort to successfully divest Nike from an outside vendor used to resolve a customer\u2019s identify from his or her multiple touch-points within the Nike app ecosystem by developing the capability in-house, producing an annual savings of over $4M\n\nTools, techniques, and skills used: R, Hive, Impala, Hadoop, Spark, Python, TensorFlow, SQL, Elastic Map Reduce (EMR), Machine Learning, Neural Networks, Statistical Analysis, Data Quality Management', u'DATA SCIENTIST\nINTERNATIONAL GAME TECHNOLOGY - Las Vegas, NV\nJanuary 2012 to October 2015\n\u2022 Developed a recommendation engine for slot machine game content that analyzed gaming patterns to recommend changes to casino operators, producing an 8% increase in slot machine performance in field trial\n\n\u2022 Led intra-departmental big data and data science effort resulting in improved understanding of IGT product portfolio and competitive positioning based on a dataset containing data from over 200,000 gaming machines, giving the game development team new insights into play preferences around mathematically important game features\n\n\u2022 Designed, developed, and delivered a suite of interactive visualizations of casino slot floor performance, allowing casino operators to gain new insight into the performance of the different machines on their floor and better understand important time dependencies such as time of day and day of week\n\nTools, techniques, and skills used: R, Shiny, Tableau, Hive, Impala, Hadoop, SQL, Statistical Analysis, Monte Carlo Simulation\ndataset containing data from over 200,000 gaming machines', u'VICE PRESIDENT OF QUANTITATIVE RESEARCH\nHYDE PARK GLOBAL INVESTMENTS - Atlanta, GA\nAugust 2005 to December 2011\n\u2022 Developed, tested, and implemented new proprietary models to predict medium term individual equity movements based on historical price patterns, resulting in a successful run of ~1 year showing a return of 8%\n\n\u2022 Managed vendor selection and product implementation for Hyde Park Global\u2019s automated trading systems\n\n\u2022 Developed and implemented analyses for transaction cost analysis for Hyde Park Global\u2019s automated trading system\n\nTools, techniques, and skills used: Visual c++, Java, SQL, Machine Learning, Time Series Analysis, Statistical Analysis, Excel, MINITAB, Reuters Stock Market Live Quote Feed, Reuters News Sentiment Analysis Feed', u'CONSULTANT\nTHE BOSTON CONSULTING GROUP - Atlanta, GA\nAugust 2000 to April 2003\n\u2022 Developed and supervised tire testing plan for a major tire manufacturer, which served as the basis for that manufacturer\u2019s tire comparison website\n\n\u2022 Analyzed land holding portfolio for a major forest products company, modeling the expected future value of the land against the costs of holding the land and procuring timber from other sources, allowing the client to sell off over 2 million acres of timber land\n\n\u2022 Optimized product delivery system for an Atlanta area charity, producing a 50% reduction in client wait time for the 2001 Christmas season\n\nTools, techniques, and skills used: Consulting, Excel, Statistical Analysis']","[u'Ph. D. in Physics', u'B.S. in Physics']","[u'University of Chicago Chicago, IL\nJanuary 2000', u'Georgia Institute of Technology Atlanta, GA\nJanuary 1994']","degree_1 : Ph. D. in Physics, degree_2 :  B.S. in Physics"
0,https://resumes.indeed.com/resume/05cead52f7d533ad,"[u'Data Scientist\nArine - San Francisco, CA\nJanuary 2018 to Present\nStartup in California, Dec 2017 - Current\n\u2022 Work with AWS technology stack to process healthcare data. This involves writing code in Python, Spark, ML, SQL, Postgres.\n\u2022 Develop algorithms to calculate medication adherence, and to generate insights that impact patient health outcomes using machine learning techniques.\n\u2022 Integrate third party healthcare data (e.g. drug information, medication diagnoses, national provider identifier) into a single repository.\n\u2022 Write production quality code and unit tests.\n\u2022 Work closely with members of Data Science, Cloud Services, Clinical teams.', u'Associate Instructor\nIndiana University Bloomington\nAugust 2016 to Present\n\u2022 Tutoring graduate students in topics on machine learning. For topics covered, please refer to skills on my web page (http://pages.iu.edu/~vmarni/)\n\u2022 Assisting with the grading of programming assignments, exams, and projects.', u""Data Scientist Intern\nProteus Digital Health - Redwood City, CA\nMay 2017 to August 2017\nEffects of medication adherence on Heart Failure Hospital Re-admissions in Inovalon Administrative Claims Dataset.\n\u2022 Statistical modeling to evaluate the time-dependent association between medication adherence and re-admission events in heart failure patients with comorbidities.\n\u2022 Built a unique NDC Drug database to map drugs to pharmacy claims to classify drugs into heart failure medication classes.\n\u2022 Worked on one of the nation's largest healthcare data sets (>50 GB) in which patients are joined longitudinally across insurance plans.\nTechnologies: AWS EMR Cluster with Hadoop-2.7.3, Hive-2.3, Spark-2.1 (Pyspark, SparkR, Spark-SQL), AWS-S3."", u'Project Engineer\nWipro Technologies\nJuly 2014 to December 2015\n\u2022 Worked on preprocessing raw data and generate useful features specific to tasks. Improved my statistical skills.\n\u2022 Worked with RDBMS databases to gather the required data required for backend support operations.']","[u'Master of Science in Data Science in Data Science', u'in Electronics & Communication']","[u'Indiana University', u'SRM University']","degree_1 : Master of Science in Data Science in Data Science, degree_2 :  in Electronics & Commnication"
0,https://resumes.indeed.com/resume/717e8b0df3a1721c,"[u'Graduate Teaching Associate\nUniversity of Tennessee - Knoxville, TN\nAugust 2017 to Present\nTaught Geography 131: Weather and Climate in a 170+ class as the core instructor.', u'Graduate Teaching Assistant\nUniversity of Tennessee - Knoxville, TN\nAugust 2014 to Present\nTaught labs in Remote Sensing, Geo-visualization.', u'Data scientist\nAll China Marketing Research - Beijing, CN\nJuly 2013 to June 2014\nImproved data cleaning efficiency by 150% using an anomaly detection system for online customer reviews. Achieved a 40% reduction in data processing time using regularization to extract critical features for housing price prediction.']","[u'Ph.D. in Geographical Information Sciences', u'Minor in Computational Sciences', u'B.S. in Geographical information sciences']","[u'University of Tennessee Knoxville Knoxville, TN\nAugust 2014 to Present', u'University of Tennessee-Knoxville Knoxville, TN\nAugust 2014 to May 2018', u'Beijing Forestry University Beijing\nJuly 2006 to July 2010']","degree_1 : Ph.D. in Geographical Information Sciences, degree_2 :  Minor in Comptational Sciences, degree_3 :  B.S. in Geographical information sciences"
0,https://resumes.indeed.com/resume/df29feeedd8af8de,"[u'Data Scientist\nAndPlus - Southborough, MA\nJanuary 2018 to January 2018\n- May 2018\n\u2022 Predicted picture\'s transformation result in ""Banner.com"" which is designed to transfer multiple layers\' picture to \u2022 PDF version by Python and SQLite.\n\u2022 Read data by developing the API and designed new JSON parser method to parse the JSON files of picture.\n\u2022 Deployed a pipeline by Python to predict the pictures\' transformation result by analyzing bugs from C#.\n\nPROJECTS', u'ERP Development Co-op, Source International Corp - Sutton, MA\nJuly 2017 to July 2017\n- May 2018\n\u2022 Performed data migration from old IBM database into new MySQL database and applied updates.\n\u2022 Designed and created User Interface for the whole types of products with Excel VBA and MySQL, and implemented\nall different functions and modules in VBA and SQL queries, including BOM, fabric, labor, purchasing order,\ninventory, and billings.\n\u2022 Implemented all the functions of purchasing order, inventory by MySQL.\n\u2022 Kept documents for all interfaces, functions, processes, and corresponding database designs and entities.', u""Alibaba Repeat Buyers Recommendation\nJuly 2017 to July 2017\nApplied KNN, Kmeans, PCA, Lasso regression, Ridge regression, to reduce the over 2000 variables into 40\ncomponents by Python.\n\u2022 Deployed exhaustive feature engineering work and stored the 20GB data in MongoDB..\n\u2022 Unbiased the dataset with over sampling and down sampling method and constructed a pipeline with different machine\nlearning models including Logistitic Regression(accuracy rate 85%), , Random Forest (95%) and XGBoost (99%) to predict repeating buyers by Python, Scala, Spark and applied Jar package by Java on AWS.\nDeveloped a multi-user transactional database server (Java) May 2017\n\u2022 Fulfilled JDBC(Java Database Connectivity) and enabled users to do the query, update and delete work.\n\u2022 Realized the buffer manager function by Naive, FIFO and LRU algorithms and compared their performance.\n\u2022 Detected deadlock by Time out algorithm Wait-die algorithm.\nFlight Reservation Software Development (Java) Apr 2017\n\u2022 Developed the flight reservation software for full stack in Test Driven Development.\n\u2022 Documented the customers' requirements analysis and applied JUnits, Integration testing and etc to test every feature, and completed the testing plan, developed the software by Java."", u'Brad Pitt and Angelina Jolie\nOctober 2016 to October 2016\nObtained the data through Twitter API, parsed XML files and used semantic analysis to analyze tweets of divorce by\nPython.\n\u2022 Applied NLTK corpus library along with regular expression to filter the stop words, some common emotions and punctuation mark\n\u2022 Designed real-time website by JavaScript to show the top words occurred in the Tweet related to the divorce between\nBrad and Angelina.']","[u'MS in Software Design', u'Bachelor of Economics in National Economic Management']","[u'Worcester Polytechnic Institute Worcester, MA\nMay 2018', u'Sichuan University Chengdu, CN\nJune 2016']","degree_1 : MS in Software Design, degree_2 :  Bachelor of Economics in National Economic Management"
0,https://resumes.indeed.com/resume/f9f1d8acb6130d3c,"[u""Data Scientist\nDanske Bank - Tampa, FL\nJanuary 2017 to Present\nResponsibilities:\n\u2022 Participated in all phases of data mining, data collection, data cleaning, developing models, validation, and visualization and performed Gapanalysis.\n\u2022 Used pandas, numpy, seaborn, scipy, matplotlib, scikit-learn, NLTK in Python for developing various machine learning algorithms.\n\u2022 Provide dataarchitecture support to enterprisedatamanagement efforts, such as the development of the enterprise data model and master and reference data, as well as support to projects, such as the development of physicaldatamodels, data warehouses and datamarts.\n\u2022 Worked on ApacheSpark with Python to develop and execute BigDataAnalytics and Machine learning applications, executed machine learning use cases under SparkML and MLLib.\n\u2022 Handled importing data from various data sources, performed transformations using Hive, MapReduce, and loaded data into HDFS\n\u2022 Developed MapReduce/Spark Python modules for machine learning & predictive analytics in Hadoop.\n\u2022 Utilized machine learning algorithms such as linear regression, multivariate regression, Naive Bayes, Random Forests, K-means, & KNN for dataanalysis\n\u2022 Designed both 3NF data models for ODS, OLTP systems and dimensional data models using Star and SnowflakeSchemas.\n\u2022 Implemented a Python-based distributed random forest via Pythonstreaming.\n\u2022 Worked with Datagovernance, Dataquality, datalineage, Dataarchitect to design various models and processes.\n\u2022 Developed Analytical systems, data structures, gather and manipulate data, using statistical techniques.\n\u2022 Used Python and R for programming for improvement of model. Upgrade the entire models for improvement of the product\n\u2022 Independently coded new programs and designed Tables to load and test the program effectively for the given POC's using with Big Data/Hadoop.\n\u2022 Designed data models and data flow diagrams using Erwin and MSVisio.\n\u2022 Assists business with casual inferences & observations with finding patterns, relationships in data\n\u2022 Used the AgileScrum methodology to build the different phases of Softwaredevelopmentlifecycle.\n\u2022 Application of various machine learning algorithms and statistical modeling like Decision Trees, Random Forest, Regression Models, neural networks, SVM, clustering to identify Volume using scikit-learn package\n\u2022 Contribute to the detailed statistical analysis of our Inclusion and Collaboration data, to identify actionable insights, and building (predictive) models.\n\u2022 Evaluate the performance of various algorithms/models/strategies based on the real world data sets.\n\u2022 Used PySpark Streaming to divide streaming data into batches for batch processing and Spark-SQL/Streaming for faster testing.\n\u2022 Worked with statistical methodologies such as: logit models, generalized linear models, categorical dataanalyses, ANOVA and regression\n\u2022 Used Tableau and designed various charts and tables for data analysis and creating various analytical Dashboards to showcase the data to managers.\n\u2022 Create and statistically analyze large data sets of internal and external data\n\u2022 Applied statistical modeling techniques on the queried datasets such as regression analysis and curve fitting techniques\nEnvironment: Python3.3.2, Jupyter Notebook, SQL, Oracle, MapReduce, R studio, SPSS, Hadoop (HDFS), Hive, Tableau, MLLib, regression, Spark, regression, logistic regression, random forest, neural networks, SVM (Support Vector Machine)"", u""Data Scientist\nStefanini IT Solution - Dearborn, MI\nSeptember 2015 to November 2016\n\u2022 Created a high-level industry standard, generalized data model to convert it into logical and physical model at later stages of the project using Erwin and Visio\n\u2022 Worked on designing, Constructing, Testing and Deploying the Cognos Framework Manager Models, Packages, Reports and Power Play Models.\n\u2022 Implemented the Dashboard reports with the various Cognos Dimensional Functions.\n\u2022 Enforced referential integrity in the OLTP data model for consistent relationship between tables and efficient database design.\n\u2022 Created logical data model from the conceptual model and it's conversion into the physical database design using Erwin.\n\u2022 Tuned and optimization of Teradata Queries Created the ETL data mapping documents between source systems and the target data warehouse.\n\u2022 Performed Data mapping using Business Rules and data transformation logic for ETL purposes.\n\u2022 Developed and maintained data dictionary to create metadata reports for technical and business purpose.\n\u2022 Designed the ER diagrams, logical model (relationship, cardinality, attributes, and, candidate keys) and physical database (capacity planning, object creation and aggregation strategies) as per business requirements.\n\u2022 Worked on building up a SQL query for extraction of data from the warehouse for a reporting schedule\n\u2022 Facilitated JAR/JAD sessions to determine data definitions and business rules governing the data and facilitated review sessions with subject matter experts (SMEs) on the logical and physical data models.\n\u2022 Worked with statistical methodologies such as: logit models, generalized linear models, categorical data analyses, ANOVA and regression\n\u2022 Applied data naming standards, created the data dictionary and documented data model translation decisions and maintained DW metadata.\n\u2022 Worked on enterprise logical data modeling project (in third normal form) to gather data requirements for OLTP enhancements.\n\u2022 Involved in extensive Data validation by writing several complex SQL queries and Involved in back-end testing and worked with data quality issues.\n\u2022 Used Erwin and Visio to create 3NF and dimensional data models and published to the business users and ETL / BI teams.\n\u2022 Used R studio to apply Machine Learning to find correct model for business using Linear regression, Logistic regression, SVM, KNN, K -Mean algorithms for decision making.\n\u2022 Data Visualization using TABLEAU, ggplot2 package in Python.\n\u2022 Text Mining Packages under Python Programming to understand the frequent words for each categorical rating.\n\u2022 Worked in importing and cleansing of data from various sources like Teradata, Oracle, flat files, SQL Server with high volume data.\n\nEnvironment: Erwin 9.5, Tableau 9.3, R Studio, Python 3.0, MS Visio, Oracle 11g, Oracle Designer, SQL Server 2008 R2, SQL, PL/SQL, Flat Files, Teradata, MS Excel 2007, PL/SQL, Business Objects, SSIS"", u'Data Modeler\nLevel one Bank - Farmington Hills, MI\nJanuary 2014 to August 2015\nResponsibilities:\n\u2022 Data modeling process was created and developed on the samples of data to test for the new data base system\n\u2022 Constructed the conceptual model & the logical model for the data modelling using Access\n\u2022 SQL quires are conducted for the verification of the conceptual model that built using the Access tool\n\u2022 Involved in Data mapping specifications to create and execute detailed system testplans.\n\u2022 Built a system model-based design for the big book business with the help of a Simulink for the work flow operations.\n\u2022 Used Erwin and Visio to create 3NF and dimensional data models\n\u2022 Developed mathematical/statistical modeling techniques to synthesize multiple scientific sources ofinformation and applied data management\n\u2022 Work independently to develop models that address specific business problems related to enrollmentmanagement, retention, marketing and class scheduling\n\u2022 Perform complex modeling, simulation and analysis of data and processes\n\u2022 Communicated reports and presentations for explaining the data, methodology, trends and analysis, to abroad group of clients with varying backgrounds\n\u2022 Skilled in System Analysis, E-R/Dimensional Data Modeling, Database Design and implementing RDBMS specific features.\nEnvironment: SQL, Simio, Erwin 9.3, Tableau 8., MS Access', u'Data Analyst/Data Modeler\nData care LLC, HYD, IND\nFebruary 2012 to December 2013\nResponsibilities:\n\u2022 Performed data analysis and profiling of source data to better understand the sources.\n\u2022 Analyzed the source data coming from different sources (SQL Server, Oracle, Sales force and from flat files like Access and Excel) and working with business users and developers to develop the Model\n\u2022 Designed and Developed Oracle PL/SQL Procedures and UNIX Shell Scripts for Data Import/Export and Data Conversions.\n\u2022 Skilled in developing Entity-Relationship diagrams, modeling Transactional Databases and Data Warehouse.\n\u2022 Created 3NF business area data modeling with de-normalized physical implementation data and information requirements analysis using Erwin tool.\n\u2022 Expertise in data base programming SQL, MS Access, Oracle, DB2, Database tuning and Query optimization.\n\u2022 Extracted data from the source database using Proc SQL. Created SAS/Views from tables in Oracle database.\n\u2022 Used ODS to generate results in Excel, PDF, and HTML formats. Involved in handling the ad-hoc reporting requirements. Extensively used Data _NULL_ to create XML files from SAS datasets.\n\u2022 Involved in extensive data analysis on the Teradata and Oracle systems querying and writing in SQL.\n\u2022 Created tables, views, sequences, indexes, constraints and generated SQL scripts for implementing physical data model.\n\u2022 Developed SAS datasets from Excel files, XML files, Oracle Server, TD SQL and DB2 database.\n\u2022 Conducted several Physical Data Model training sessions with the ETL Developers.\n\u2022 Conducted complex ad-hoc programming and analysis including statistical programming and analysis.\n\u2022 Perform Data validation and data quality work.\n\u2022 Strong data collection, analysis and interpretation skills\n\u2022 Strong SQL querying skills\n\u2022 Strong written and verbal communication skills\n\u2022 Proven ability to work independently and within a team structure\n\u2022 Proficiency in Microsoft Office Suite\nEnvironment: SAS, Base SAS (MACROS, Proc SQL, ODS), SAS/ACCESS, SAS/STAT, SAS/GRAPH, SAS Enterprise Guide , SAS Enterprise Miner, SQL, PL/SQL, UNIX, MS-Windows XP, MS-Office.']",[],[],degree_1 : 
0,https://resumes.indeed.com/resume/a18200276c452a56,"[u""Data Analyst\nJanus of Santa Cruz - Santa Cruz, CA\nJanuary 2017 to Present\nAs the Lead Data Analyst, I collaborate with the Janus team on all levels to design a HIPPA compliant PaaS Business Intelligence solution to manage our program data and present dashboards on demand. MS Azure, C#, Visual Studio, MS SQL\n\n\u2022 Innovates KPI's and visualizes dashboards to manage financial budgets and project revenues.\n\u2022 Reports on client census/ programs outcomes to drive decision-making, improve our programs, and apply for grants.\n\u2022 Optimizes strategy and workflow for acquiring and reporting program data on monthly basis."", u'Staff Data Scientist\nSigma3 Inc\nJanuary 2015 to January 2017\n\u2022 Investigates and conducts industry wide studies to find patterns in project design and performance.\n\u2022 Collaborates with managers on Engineering and Sales teams to gather the best insight from analysis.\n\u2022 Investigates greatest marketable business targets for our sales teams.\n\u2022 Optimizes strategy and workflow for acquiring and reporting on industry data.\n\u2022 Writes documentation on analysis and algorithm development.', u'Data Warehouse Analyst\nSigma3 Inc\nMay 2014 to January 2015\n\u2022 Automates gathering of industry data from government websites into centralized SQL databases using R and Python.\n\u2022 Develops operating policies, database layout, and standards to align information into a practical and reproducible state.\n\u2022 Tracts department statistics and reports on work completed and analysis to Engineering and Sales teams.\n\u2022 Uses advanced map-building skill with Geographic Information Systems to convey geospatial data and build infographics for reports.\n\u2022 Researches and warehouses new public data sources.', u'Teaching Assistant\nCU Science Discovery\nMay 2011 to August 2011\n\u2022 Teaches classrooms of students practical scientific and artistic skills.\n\u2022 Organizes and documentes photos, information, and stories for each course in catalog.']","[u'Data Science Certificate in Data Science', u'Python Programming Certificate in Python', u""Bachelor's Degree in Geological Sciences"", u'', u'Certificate in Spanish']","[u'The Johns Hopkins University\nJanuary 2015 to January 2016', u'Rice University\nJanuary 2015 to January 2016', u'University of Colorado Boulder, CO\nJanuary 2009 to January 2014', u'Universitat de Barcelona\nJanuary 2013', u'Estudios Hispanicos']","degree_1 : Data Science Certificate in Data Science, degree_2 :  Python Programming Certificate in Python, degree_3 :  ""Bachelors Degree in Geological Sciences"", degree_4 :  , degree_5 :  Certificate in Spanish"
0,https://resumes.indeed.com/resume/0e3c5bb36955f6bc,"[u'Data Scientist\nSouthern Company Gas - Atlanta, GA\nApril 2017 to Present\nGas Operations Capacity Planning - Utilizing neural networks and ensemble modeling for error reduction to forecast natural gas demand daily and hourly for 21 markets in 7 highly populated states. Full-cycle data analytics work flow. Project management, systems engineering, end-user-engagement, database administration, real-time data ingestion, data collection, data governance & data validation, data manipulation, data cleaning, feature extraction, feature engineering, neural net, gradient boosting models, regression & tree-based modeling, application design and implementation, automation, statistical modeling, time series analysis, data visualization and KPI dashboards.', u'Sourcer\nDedicated Resources www.dedicatedresources.co\nMay 2016 to Present\nRecruitment Assistance\nWorking with companies and recruiters from REI, Microsoft, Lowes.\nSpecializing in Big Data, IoT, and Open Source Cloud', u'Data Scientist Intern\nThe Big Data Company\nDecember 2016 to March 2017\nAccomplished predictive analytics models forecasting gross revenue for limited and wide release opening weekend box office movies prior to release with a mean variance of 8 %. Focused on experimental design, feature engineering, feature selection, data wrangling, data cleaning, data visualization, model selection and hyper tuning parameters in AWS and Azure. Remote - contract role.', u'Field Consultant\nASI AG - Denver, CO\nJanuary 2015 to January 2015\nCertified Air Seeder Mechanic\nExecuted 13 dealership, 4 state distribution contracts with John Deere franchise\nSold $365,000 of products and services in 75 days.\nAsk me about my first day in Kentucky if you are hiring me to sell solutions.', u'Inbound Technical Support for Asia Pacific markets\nApple - Boulder, CO\nJanuary 2014 to January 2015\n100 percent satisfaction rating during tenure', u'Owner / Founder\nG. D. P. C\nJanuary 2010 to January 2014\nOperations Manager\nGrew business from 0 to 500K inside 36 months - avg. transaction 14 dollars\nNurtured business to 75 percent average annual growth\nManaged schedule for 300 plus customers and 10 employees. Exited and sold']","[u'B.S. in Data Management', u'', u'']","[u'Western Governors University\nJanuary 2018', u'University of California San Diego, CA', u'John Hopkins University']","degree_1 : B.S. in Data Management, degree_2 :  , degree_3 :  "
0,https://resumes.indeed.com/resume/ce9d743b1e203cee,"[u'Data Scientist\nElectronic Arts WWCE Insights\nOctober 2014 to Present\n\u2022 Conceived, developed and deployed topic modeling algorithms in R/Shiny to help EA game experts identify emerging issues in near real-time during product launches\n\u2022 Applied document clustering, topic modeling and sentiment analysis in R/Python to help tickets, social posts, and product reviews to ascertain which aspects of EA games were least aligned to customer expectations\n\u2022 Extracted high-dimensional game telemetry data with Apache Hive and combined it with player text to identify patterns of game play associated with the most frequent player-reported game issues\n\u2022 Currently engineering telemetry-based features to predict player contact likelihood with Python/Apache Spark', u'Postdoctoral Fellow\nCenter for Learning and Memory, UT Austin\nMay 2012 to October 2014\n\u2022 Devised novel experimental protocol for quantification and statistical analysis of new synaptic substructure\n\u2022 Contributed to development of cellular auto-segmentation algorithms in Python', u'Graduate Research Assistant\nInstitute for Neuroscience, UT Austin\nAugust 2005 to May 2012\n\u2022 Designed visual stimuli in Matlab to elicit contrasting responses in vision cortex during decision-making\n\u2022 Applied PCA to determine key components of patterns of brain activity during decision-making in fMRI data']","[u'PhD in Neuroscience', u'MA in Theoretical Physics', u'BA in Physics']","[u'The University of Texas at Austin Austin, TX\nAugust 2005 to May 2012', u'The University of Texas at Austin Austin, TX\nAugust 1999 to December 2002', u'Hendrix College\nAugust 1995 to May 1999']","degree_1 : PhD in Neroscience, degree_2 :  MA in Theoretical Physics, degree_3 :  BA in Physics"
0,https://resumes.indeed.com/resume/fe13b4bac133d0cd,"[u'Data Scientist\nCreighton University - Omaha, NE\nFebruary 2017 to Present\n\u2022 Leading a team of Big Data engineers.\n\u2022 Architecting data flows with Spark, Hive, Cassandra, and Python.\n\u2022 Developing complex PL/SQL queries for Oracle databases.\n\u2022 Implementing data integrations with SQL Server Integration Services (SSIS).\n\u2022 Administering Linux (CentOS 7) and Windows Server 2012 servers.', u'Data Programmer/Analyst\nMWI Direct - Lincoln, NE\nJuly 2015 to February 2017\n\u2022 Designing and maintaining SQL Server databases.\n\u2022 Producing machine learning and business intelligence solutions in Python, R, and RedPoint Data Management.\n\u2022 Designing print and web layouts in Photoshop, PReS, and PReS Connect.\n\u2022 Maintaining legacy Visual FoxPro applications and porting them to RedPoint Data Management, SQL Server Integration Services, C#, Python, and R.', u'Field Engineer\nFive Nines Technology Group - Lincoln, NE\nFebruary 2015 to July 2015\n\u2022 Provided first-class technical support for Windows 7 and Windows 8.1 users.\n\u2022 Administered servers and networks for a broad range of small- and medium-sized businesses.\n\u2022 Automated systems administration with PowerShell.\n\u2022 Resolved problems as quickly and thoroughly as possible.\n\u2022 Recognized twice for outstanding \u201ccustomer savvy.\u201d', u'Support Engineer\nFive Nines Technology Group - Lincoln, NE\nSeptember 2014 to February 2015\n\u2022 Provided first-class technical support for Windows 7 and Windows 8.1 users.\n\u2022 Administered servers and networks for a broad range of small- and medium-sized businesses.\n\u2022 Resolved problems as quickly and thoroughly as possible.\n\u2022 Recognized twice for outstanding \u201ccustomer savvy.\u201d', u'Data Curation Intern\nLogos Research Systems - Bellingham, WA\nMay 2014 to December 2014\n\u2022 Curated and analyzed cultural data from ancient literature using MySQL and SQLite databases, Python, and proprietary software.', u'Teaching Assistant\nUniversity of Iowa - Iowa City, IA\nAugust 2012 to May 2014\n\u2022 Communicated complex material in understandable ways.\n\u2022 Named a \u201cFirst-Year Student Champion\u201d in 2012.\n\u2022 Received the Heidel Award for Teaching Assistants in 2014.\n\u2022 Managed the production of high-quality web products.', u'Graduate Researcher\nDigital Studio for Public Arts and Humanities - Iowa City, IA\nJanuary 2012 to May 2014\n\u2022 Partnered closely with faculty and staff to produce excellent public, digital scholarship.\n\u2022 Provided education and technical support to students, faculty, and staff.\n\u2022 Summarized technical research for non-specialist audiences.']","[u'MA', u'BA']","[u'University of Iowa Iowa City, IA\nJanuary 2015', u'Union University Jackson, TN\nJanuary 2010']","degree_1 : MA, degree_2 :  BA"
0,https://resumes.indeed.com/resume/00de47212090e2a4,"[u'Data Scientist\nState Street Bank - Boston, MA\nFebruary 2017 to Present\nDescription:State Street\'s past can be dated back to the founding years of Boston\'s banking industry. In 1792 the Union Bank became the third bank to be chartered in Boston and was located at the corner of State and Exchange Streets. State Street was known as the ""Great Street to the Sea"" as Boston became a flourishing maritime capital. The clipper in State Street\'s logo today reflects this period.\n\nResponsibilities:\n\u2022 Analyze and Prepare data, identify the patterns on thedataset by applying historical models. Collaborating with Senior Data Scientists for understanding of data\n\u2022 Perform data manipulation, data preparation, normalization, and predictive modeling. Improve efficiency and accuracy by evaluating model in R\n\u2022 This project was focused on customer segmentation based on machine learning and statistical modeling effort including building predictive models and generate data products to support customer segmentation\n\u2022 Used R and Python for programming for improvement of themodel. Upgrade the entire models for improvement of the product\n\u2022 Develop a pricing model for various product and services bundled offering to optimize and predict the gross margin\n\u2022 Built price elasticity model for various product and services bundled offering\n\u2022 Developed predictive causal model using annual failure rate and standard cost basis for the new bundled service offering\n\u2022 Design and develop analytics, machine learning models, and visualizations that drive performance and provide insights, from prototyping to production deployment and product recommendation and allocation planning\n\u2022 Utilized Spark, Scala, Hadoop, HBase, Kafka, Spark Streaming, MLlib, R, a broad variety of machine learning methods including classifications, regressions, dimensionality reduction etc.\n\u2022 Worked with sales and Marketing team for Partner and collaborate with a cross-functional team to frame and answer important data questions\n\u2022 prototyping and experimentation ML/DL algorithms and integrating into production system for different business needs\n\u2022 Worked on Multiple datasets containing two billion values which are structured and unstructured data about web applications usage and online customer surveys\n\u2022 Good hands on experience on Amazon Redshift platform\n\u2022 Performed Data cleaning process applied Backward - Forwardfillingmethods on dataset for handling missing values\n\u2022 Design, built and deployed a set of python modeling APIs for customer analytics, which integrates multiple machine learning techniques for various user behavior predictionand supports multiple marketing segmentation programs\n\u2022 Segmented the customers based on demographics using K-means Clustering.\n\u2022 Explored different regression and ensemble models in machine learning to perform forecasting\n\u2022 Presented Dashboards to Higher Management for more Insights using Power BI\n\u2022 Used classification techniques including Random Forest and Logistic Regression to quantify the likelihood of each user referring.\n\u2022 Worked with using a different kind of compression techniques to save data and optimize data transfer over the network using LZO, Snappy,and GZip etc.\n\u2022 Analyze large and critical datasets using Cloudera, HDFS, HBase, MapReduce, Hive, HiveUDF, Pig, Sqoop, Zookeeper, & Spark.\n\u2022 Developed custom aggregate functions using SparkSQL and performed interactive querying.\n\u2022 Connected Tableau from client end with AWS IP addresses and view the end results\n\u2022 Performed Boosting method on predicted model for the improve efficiency of the model\n\u2022 Designed and implemented end-to-end systems for Data Analytics and Automation, integrating custom, visualization tools using R, Tableau, and Power BI.\n\u2022 Collaborating with the project managers and business owners to understand their organizational processes and help design the necessary reports\n\nEnvironment: MS SQL Server, R/R studio, SQL Enterprise Manager, Python, Redshift, MS Excel, Power BI, Tableau, T-SQL, ETL, MS Access, XML, MS Office, Outlook, AS E-Miner, Power BI, K-means.', u""Data Scientist\nSynchrony Financial - Chicago, IL\nDecember 2015 to January 2017\nDescription:\nSynchrony Financial is one of the nation's premier consumer financial services companies. They are the largest provider of private label credit cards in the United States based on volume and receivables.\n\nResponsibilities:\n\u2022 Used various approaches to collect the business requirements and worked with the business users for ETL application enhancements by conducting various JRD sessions to meet the job requirements\n\u2022 Performed exploratory data analysis like calculation of descriptive statistics, detection of outliers, assumptions testing, factor analysis, etc., in R\n\u2022 Worked exclusively on making applications more scalable and highly available system in AWS (load balancing) with full automation.\n\u2022 Extracted data from the database using SAS/Access, SAS/SQL procedures and created SAS datasets for statistical analysis, validation, and documentation.\n\u2022 Extensively understanding BI, analytics focusing on consumer and customer space\n\u2022 Innovate and leverage machine learning, data mining and statistical techniques to create new, scalable solutions for business problems.\n\u2022 Extensive experience in working with TableauDesktop, Tableau Server, and TableauReader in various versions of Tableau9.2 and 10 as a Developer and Analyst.\n\u2022 Analyzed different types of data to derive insights about relationships between locations, statistical measurements and qualitatively assess the data using R/R Studio\n\u2022 Performed Data Profiling to assess data quality using SQL through complex internal database\n\u2022 Improved sales and logistic data quality by data cleaning using NumPy, SciPy, Pandas in Python\n\u2022 Designed data profiles for processing, including running SQL, PL/SQL queries and using R for Data Acquisition and Data Integrity which consists of Datasets Comparing and Dataset schema checks\n\u2022 Used R to generate regression models to provide statistical forecasting\n\u2022 Used drill downs, filter actions and highlight actions in Tableau for developing dashboards in Tableau.\n\u2022 Applied Clustering Algorithms such as K-Means to categorize customers into certain groups\n\u2022 Performed data management, including creating SQL Server Report Services to develop reusable code and an automatic reporting system and designed user acceptance test to provide end with an opportunity to give constructive feedback\n\u2022 Used Tableau and designed various charts and tables for data analysis and creating various analytical Dashboards to showcase the data to managers.\n\u2022 Applied association rule mining & chain model to identify hidden patterns and rules in remedy ticket analysis which aid in decision making.\n\u2022 Created AMI images of critical ec2 instances as backup using AWS CLI and GUI.\n\u2022 Created AWS Cloud formation templates on creating IAM Roles & total architecture deployment end to end (Creation of EC2 instances & its infrastructure).\n\u2022 Isolating customer behavioral patterns by analyzing millions of customer data records over a period and correlating multiple customers' attributes.\n\u2022 Empowered decision makers with data analysis dashboards using Tableau and Power BI\n\nEnvironment: R/R Studio, SAS, SSRS, SSIS, Oracle Database 11g, Oracle BI tools, Tableau, MS-Excel, Python, Naive Bayes, SVM, K- means, ANN, Regression, MS Access, SQL Server Management Studio, SAS E-Miner."", u""Big Data/Hadoop Architect\nMolina Health Care - Herndon, VA\nFebruary 2014 to November 2015\nDescription: Molina Healthcare is a managed care company headquartered in Long Beach, California, United States. In 2016, Molina Healthcare was ranked 201 in Fortune 500. In 2015, the company's health plans served about 3.5 million people through government-based healthcare programs.\n\nResponsibilities:\n\u2022 Designed and developed multiple MapReduce jobs in Java for complex analysis. Importing and exporting the data using Sqoop from HDFS to Relational Database systems and vice-versa.\n\u2022 Integrated Apache Storm with Kafka to perform web analytics. Uploaded clickstream data from Kafka to HDFS, HBase, and Hive by integrating with Storm.\n\u2022 Configured Flume to transport web server logs into HDFS. Also used Kite logging module to upload webserver logs into HDFS.\n\u2022 Developed UDF functions for Hive and wrote complex queries in Hive for data analysis\n\u2022 Performed Installation of Hadoop in fully and Pseudo Distributed Mode for POC in early stages of the project.\n\u2022 Analyze, develop, integrate, and then direct the operationalization of new data sources.\n\u2022 Regression (linear, multivariate) analysis using R language and plotting graphs of regression results using Shiny R framework.\n\u2022 Generating ScalaandJava classes from the respective APIs so that they can be incorporated into the overall application.\n\u2022 Responsible for working with different teams in building Hadoop Infrastructure\n\u2022 Gathered business requirements in meetings for successful implementation and POC and moving it to Production and implemented POC to migrate map reduce jobs into Spark RDD transformations using Scala.\n\u2022 Implemented different machine learning techniques in Scala using Scala machine learning library.\n\u2022 Developed Spark applications using Scala for easy Hadoop transitions.\n\u2022 Successfully loaded files to Hive and HDFS from Oracle, Netezza and SQL Server using SQOOP.\n\u2022 Uses Talend Open Studio to load files into Hadoop HIVE tables and performed ETL aggregations in Hadoop Hive.\n\u2022 Developed Simple to Quebec and Python MapReduce streaming jobs using Python language that is implemented using Hive and Pig.\n\u2022 Designing & Creating ETL Jobs through Talend to load huge volumes of data into Cassandra, Hadoop Ecosystem, and relational databases.\n\u2022 Worked on analyzing, writing HadoopMapReducejobs using Java API, Pig, and Hive.\n\u2022 Developed some machine learning algorithms using Mahout for data mining for the data stored in HDFS\n\u2022 Used Flume extensively in gathering and moving log data files from Application Servers to a central location in Hadoop Distributed File System (HDFS)\n\u2022 Worked with Oozie Workflow manager to schedule Hadoop jobs and high intensive jobs\n\u2022 Responsible for cluster maintenance, adding and removing cluster nodes, cluster monitoring, and troubleshooting, manage and review data backups, manage and review Hadoop log files.\n\u2022 Extensively used Hive/HQL or Hive queries to query data in Hive Tables and loaded data into HIVE tables.\n\u2022 Creating UDF functions in Pig &Hive and applying partitioning and bucketing techniques in Hive for performance improvement\n\u2022 Creating indexes and tuning the SQL queries in Hive and Involved in database connection by using Sqoop\n\u2022 Involved in Hadoop Name node metadata backups and load balancing as a part of Cluster Maintenance and Monitoring\n\u2022 Used File System Check (FSCK) to check the health of files in HDFS and used Sqoop to import data from SQL server to Cassandra\n\u2022 Monitored Nightly jobs to export data out of HDFS to be stored offsite as part of HDFS backup\n\u2022 Used Pig for analysis of large datasets and brought data back to HBase by Pig\n\u2022 Worked with various Hadoop Ecosystem tools like Sqoop, Hive, Pig, Flume, Oozie, Kafka\n\u2022 Developed Python Mapper and Reducer scripts and implemented them using Hadoop streaming.\n\u2022 Created schema and database objects in HIVE and developed Unix Scripts for data loading and automation\n\u2022 Involved in thetraining of big data ecosystem to end-users.\n\nEnvironment: Hadoop, MapReduce, Sqoop, AWS, Hive, Flume , Oozie , Pig , HBase, Scala, Zookeeper 3.4.3, Talend Open Studio, Talend, Oracle 12c, Apache Cassandra, SQL Server 2012, MySQL, Java, SQL, PL/SQL, UNIX shell script, Eclipse Kepler IDE, Microsoft Office 2010."", u'Bigdata/Hadoop Developer\nTransamerica Corporation - Cedar Rapids, IA\nNovember 2012 to January 2014\nDescription:Transamerica Corporation, a financial services company, provides insurance small to large organizations.\n\nResponsibilities:\n\u2022 Responsible for building scalable distributed data solutions using Hadoop.\n\u2022 Written multiple MapReduce2.0programs to power data for extraction, transformation, and aggregation from multiple file formats including XML, JSON, CSV& other compressed file formats.\n\u2022 Reviewed the HDFS usage and system design for future scalability and fault-tolerance.\n\u2022 Worked on python 3.5.1 scripts to analyze the data of the customer.\n\u2022 Loaded and transformed large sets of structured, semi-structured and unstructured data in various formats like text, zip, XML, and JSON.\n\u2022 Defined job flows and developed simplytoo complex MapReduce2.0 jobs as per the requirement.\n\u2022 Developed MapReduce jobs in Python for data cleaning and data processing.\n\u2022 Developed PIGUDFs for manipulating the data according to Business Requirements and worked on developing custom PIG Loaders.\n\u2022 Migration of Informatics jobs to Hadoop Sqoop jobs and load into oracle database.\n\u2022 Used Spark API over Hadoop YARN to perform analytics on data in Hive.\n\u2022 Efficiently put and fetched data to/from HBase by writing MapReduce job.\n\u2022 Responsible for creating Hive tables based on business requirements.\n\u2022 Involved in design Cassandra data model, used CQL (Cassandra Query Language) to perform CRUD operations on Cassandra file system\n\u2022 Imported data from Cassandra into HDFS using Mongo export utility.\n\u2022 Worked with NiFi for managing the flow of data from source to HDFS.\n\u2022 Experience in using NIFI processor groups, processors and concepts on process flow management.\n\u2022 Implemented Partitioning, Dynamic Partitions, and Buckets 5.5 in HIVE for efficient data access.\n\u2022 Collaborated with the infrastructure, network, database, application, and BI teams to ensure data quality and availability.\n\u2022 Purveyor of competitive intelligence and holistic, timely analyses of Bigdata made possible by the successful.\n\u2022 Implementing various modules in Spark using language Scala and Map Reduce processing layer. Able to assess\n\u2022 Uses Talend Open Studio to load files into Hadoop HIVE tables and performed ETL aggregations in Hadoop Hive.\n\u2022 Developed Simple to Quebec and Python MapReduce streaming jobs using Python language that is implemented using Hive and Pig.\n\u2022 Designing & Creating ETL Jobs through Talend to load huge volumes of data into Cassandra, Hadoop Ecosystem, and relational databases.\n\u2022 Worked on analyzing, writing Hadoop MapReducejobs using Java API, Pig and Hive.\n\u2022 Developed map Reduce jobs to analyze data and provide heuristics reports\n\u2022 Good experience in writing data ingested and complex MapReduce jobs in java for data cleaning and preprocessing and fine-tuning them as per data sets.\n\u2022 Created HBase tables to load large sets of structured, semi-structured and unstructured data coming from UNIX, NoSQL and a variety of portfolios.\n\u2022 Implemented Hive tables and HQL Queries for the reports.\n\u2022 Involved in data modeling and sharing and replication strategies in MongoDB.\n\u2022 Worked on hive data warehouse modeling to interface with BI tools such as Tableau.\n\u2022 Knowledge of handling Hive queries using SparkSQL that integrate Spark environment.\n\u2022 Exported the analyzed data into relational databases using Sqoop for visualization and to generate reports for the BI team.\n\u2022 Utilized Agile Scrum Methodology to help manage and organize a team of 4 developers with regular code review sessions.\nEnvironment: Apache Hadoop 2x, HDFS, MapReduce 2.0, Hortonworks, Hive1.1.1, Pig0.15.0, HBase1.1.2, Spark0.15.0, Scala, Kafka, Python3.5.1, Sqoop 1.99.6-rc2, Tableaus, Cassandra2.0, Informatica, Oracle 11g/10g, Linux.', u'Java Developer\nSociete Generale Global Solution PVT Ltd - Bengaluru, Karnataka\nFebruary 2011 to October 2012\nDescription: Societe Generale Global Solution Center (SG Global Solution Centre), a 100% owned subsidiary of European banking major Societe Generale (SG), embarked on its Indian journey in 2000 and has displayed remarkable growth since. In 2002, the IT division expanded to include Investment Banking, and the back-office operations were launched in 2004.\n\nResponsibilities:\n\u2022 Involved in various phases of Software Development Lifecycle (SDLC) of the application like requirement gathering, Design, Analysis and code development.\n\u2022 Followed Agile Scrum methodology, involved in sprint planning, retros and code reviews.\n\u2022 Effectively involved in developing the logic to implement the requirements\n\u2022 Implemented MVC architecture using Spring MVC.\n\u2022 Involved in developing the class diagrams and sequence diagrams.\n\u2022 Involved in designing and developing the rich internet application using JSP, JavaScript, CSS,and HTML.\n\u2022 Actively involved in developing Servlet classes and unit testing.\n\u2022 Involved in writing Spring Configuration XML file that contains declarations and another dependent object declaration\n\u2022 Involved in the JMS- queue configurations which are used to connect to the back-end systems.\n\u2022 Developed complete Business tier with Stateless, Stateful Session beans and Session Beans with JPA with EJB standards.\n\u2022 Used JMS API for asynchronous communication by putting the messages in the Message queue.\n\u2022 Applied the Session Fa\xe7ade, Data Access Object, Data Transfer Object design patterns.\n\u2022 Developed and maintained User authentication and authorization by employing EJB2.0.\n\u2022 Created the new web services (soap over HTTP) and exposed to front-end layer to consume it.\n\u2022 Tested the web services which are created using SOAP UI.\n\u2022 Analyzed and fixed the defects raised in all testing phase (SIT, UAT and performance testing).\n\u2022 Responsible for setup, installation of WebSphere Application Server on UNIX and Linux platforms in Test, DEV and PROD environments.\n\u2022 Configured WebSphere admin console.\n\u2022 Configured IBM HTTPWeb server to work with WAS.\n\u2022 Created SQL views, queries, functions and triggers to be used to fetch data from the system.\n\u2022 Used Hibernate as the ORM tool to develop the persistence layer\n\u2022 Implemented JDBC specification to connect to the database.\n\u2022 Utilized Java debugging and error handling classes and techniques to troubleshoot and debug issues.\n\u2022 Used log4J for error reporting and debugging.\n\u2022 Worked with JUNIT extensively to define various Test Suites and Test Cases.\n\nEnvironment:Java/J2EE, Core Java, Jdk1.6, Spring Boot, Hibernate, Webservices, JAX-RS, Mockito, WADL, SOAPUI, JSP, JDBC, jQuery, AJAX, Html, CSS, Maven, log4j, Oracle, MS SQL, PL/SQL, SQL Developer, JIRA, JMS, APACHE AXIS, Source Tree, IntelliJ, GIT, UNIX, AGILE-SCRUM.', u'Java Developer\nAspect Software - Bengaluru, Karnataka\nAugust 2009 to January 2011\nDescription: Aspect Software, Inc. is an American multinational call center technology and customer experience company headquartered in Phoenix, Arizona.\n\nResponsibilities:\n\u2022 Implemented the application in SpringFramework and it is Model View Controller design pattern based framework.\n\u2022 Configured web.xml, faces-config.xml for navigations and managed beans. Spring and Hibernate Frameworks.\n\u2022 Involved in various Software Development Life Cycle (SDLC) phases of the project which was modeled using Rational Unified Process (RUP).\n\u2022 Inorder to load the data to Oracle using Java and JExcelAPI, we developed dump and Load Utility to extract the data.\n\u2022 Implemented workflow system in an SOA environment, through web services built using Axis2 for SOAP over HTTP and SMTP.\n\u2022 In the project, we used PL/SQL commands to work on Oracle database.\n\u2022 The configuration of Jenkins along with Maven and Python Scripts for Automated build and Deployment Process.\n\u2022 Configured Managed and controlled the source code repository, housed in Subversion, Git.\n\u2022 Used Junit to simplify the client-side scripting of HTML.\n\u2022 Used AngularJS directives to specify custom and reusable HTML-like elements.\n\u2022 Worked in retail and merchandising website to update the goods and services.\n\u2022 Developed Servlets and back-end Java classes using Web Sphere application server.\n\u2022 Developed additional UI Components and implemented an asynchronous, AJAX (JQuery) based rich client to improve customer experience.\n\u2022 Built ANT scripts for automated deployment and for the build operation of the entire application.\n\u2022 Developed web pages using HTML5/CSS and JavaScript, Angular JS\n\u2022 Developed many JSP pages, used Dojo in JavaScript Library, jQuery UI for client-side validation.\n\nEnvironment: JDK1.5, JBoss, JSP, Angular JSP, WEB API, Hibernate, Spring XML, Servlets, CVS, SQL, HTML, JSP, Servlets, JavaScript, CSS, Apache Server, SVN, Oracle 10g.']","[u""Bachelor's""]",[u''],"degree_1 : ""Bachelors"""
0,https://resumes.indeed.com/resume/7d7aed8d4f9dc858,"[u'Data Scientist\nSystrac Solutions - Hyderabad, Telangana\nDecember 2016 to January 2018\nResponsibilities:\n\u2022 Mine and analyze data from company database to drive optimization and improvement of product development and business strategies\n\n\u2022 Using algorithms and programming to efficiently go through large datasets and apply treatments, filters and conditions to the data\n\u2022 Extending companies data with third party sources of information when needed\n\u2022 Application of bi-variate and multi variate analysis to understand the relationship between the variables, missing value treatment and imputation of missing values\n\u2022 Doing Adhoc analysis and presenting results in a clear manner\n\u2022 Used multi-collinearity analysis to understand the highly correlating variables\n\u2022 Involved in coding Logistic Regression, Decision Trees etc using R\n\u2022 Creation of confusion matrix to interpret the output\n\u2022 Developed Tableau visualizations and dashboards using Tableau Desktop.']",[u'Bachelor of Technology in Electronics and Communication Engineering'],"[u'VIF College, Jawaharlal Nehru Technological University Hyderabad, Telangana']",degree_1 : Bachelor of Technology in Electronics and Commnication Engineering
0,https://resumes.indeed.com/resume/037934992ce5e365,"[u""Data Scientist\nAT&T - Phoenix, AZ\nFebruary 2017 to Present\nDescription: AT&T Inc. is an American multinational conglomerate holding company headquartered at Whit acre Tower in downtown Dallas, Texas.8 AT&T is the world's largest telecommunications company. AT&T is also the second largest provider of mobile telephone services and the largest provider of fixed telephone services in the United States.\n\nResponsibilities:\n\u2022 Developed, Implemented & Maintained the Conceptual, Logical & Physical Data Models using Erwin for Forward/Reverse Engineered Databases.\n\u2022 Designed algorithms to identify and extract incident alerts from a daily pool of incidents.\n\u2022 Reduced redundancy among incoming incidents by proposing rules to recognize patterns.\n\u2022 Scheduled searches Using Splunk tool.\n\u2022 Worked with Machine learning algorithms like Regressions (linear, logistic etc..), SVMs, Decision trees.\n\u2022 Solution architecting BIG Data solution for Projects & Proposal using Hadoop, Spark, ELK Stack, Kafka, Tensor flow.\n\u2022 Worked on Clustering and classification of data using machine learning algorithms.\n\u2022 Good applied statistics skills, such as statistical sampling, testing, regression, etc.\n\u2022 Build analytic models using a variety of techniques such as logistic regression, risk scorecards and pattern recognition technologies.\n\u2022 Work with technical and development teams to deploy models. Build Model Performance Reports and Modeling Technical Documentation to support each of the models for the product line.\n\u2022 Performed Exploratory Data Analysis and Data Visualizations using R, and Tableau.\n\u2022 Analyzed data from Primary and secondary sources using statistical techniques to provide daily reports.\n\u2022 Created Custom Dashboards and reports using Splunktool.\n\u2022 Estimation and Requirement Analysis of project timelines.\n\u2022 Analyzed data and recommended new strategies for root cause and finding quickest way to solve big data sets.\n\u2022 Used packages like dplyr, tidyr&ggplot2 in R Studio for data visualization.\n\u2022 Extending company's data with third party sources of information when needed.\n\u2022 Enhancing data collection procedures to include information that is relevant for building analytic systems.\n\u2022 Experienced in Delivery, Portfolio, Team / Career, Vendor and Program management Competency in Solution Architecture, implementation & delivery of Big Data, data science analytics &DWH projects on GreenPlum, SPARK, Keras, Python and TensorFlow.\n\u2022 Processing, cleansing, and verifying the integrity of data used for analysis\n\u2022 Doing ad-hoc analysis and presenting results in a clear manner\n\u2022 Constant tracking of model performance\n\u2022 Excellent understanding of machine learning techniques and algorithms , such as Logistic Regression, SVM, Random Forests, Deep Learning etc.\n\u2022 Worked with Data governance, Data quality, data lineage, Data architect to design various models.\n\u2022 Independently coded new programs and designed Tables to load and test the program effectively for the given POC's using with Big Data/Hadoop.\n\u2022 Designed data models and data flow diagrams using Erwin and MS Visio.\n\u2022 As an Architect implemented MDM hub to provide clean, consistent data for a SOA implementation.\n\u2022 Experience with common data science toolkits, such as R, Python, Spark, etc.\n\u2022 Developed and designed SQL procedures and Linux shell scripts for data export/import and for converting data.\n\u2022 Used Test Driven Development (TDD) for the project.\n\u2022 Written SQL Queries, Store Procedures, Triggers and functions for MySQL Databases.\n\u2022 Coordinate with data scientists and senior technical staff to identify client's needs and document assumptions.\n\u2022 Perform a proper EDA, Univariate and bi-variate analysis to understand the intrinsic effect/combined.\n\u2022 Established Data architecture strategy, best practices, standards, and roadmaps.\n\u2022 Lead the development and presentation of a data analytics data-hub prototype with the help of the other members of the emerging solutions team.\n\u2022 Worked with several R packages including knitr, dplyr, SparkR, CausalInfer, spacetime.\n\u2022 Interacted with the other departments to understand and identify data needs and requirements.\n\u2022 Involved in analysis of Business requirement, Design and Development of High level and Low level designs, Unit and Integration testing.\n\nEnvironment: R, R Studio, Splunk, SQL, MYSQL and Windows, UNIX, Python 3.5, MLLib, SAS, regression, logistic regression, Hadoop, NoSQL, Teradata, TensorFlow, OLTP, random forest, OLAP, HDFS, ODS"", u'Data Scientist\nDeloitte - Orlando, FL\nDecember 2015 to January 2017\nDescription: ""Deloitte"" is the brand under which tens of thousands of dedicated professionals in independent firms throughout the world collaborate to provide audit, consulting, financial advisory, risk management, tax, and related services to select clients.\n\nResponsibilities:\n\u2022 Implementation of MetadataRepository, MaintainingDataQuality, DataCleanup procedures, Transformations, Data Standards, Data Governance program, Scripts, Stored Procedures, triggers and execution of test plans.\n\u2022 Developed multiple MapReduce jobs in java for data cleaning and preprocessing.\n\u2022 Applied linear regression on data and predicted the sales. Without much advertisement how the sales is happening is predicted and measures are taken from insights how to advertise in a better manner.\n\u2022 Analyzed the partitioned and bucketed data and compute various metrics for reporting.\n\u2022 Involved in loading data from RDBMS and web logs into HDFS using Sqoop and Flume.\n\u2022 Worked on loading the data from MySQL to HBase where necessary using Sqoop.\n\u2022 Understanding the project Objectives and requirements from a domain prospective and understanding the problem definition.\n\u2022 Extracting the desired data from multiple sources integrating and preprocessing, cleaning and filling in missing data.\n\u2022 Evaluation of models for Bias and variance problem. Evaluating models for best parameters using K folds.\n\u2022 Update and maintained existing universes based on changes in user requirements& in data source.\n\u2022 Visually analyses chasm traps and Fan traps and resolve loops by creating aliases and contexts.\n\u2022 Design, develop, test, and deploy reports and dashboards that feed into mobile applications for ABM\'s, BRM\'s and Marketing users.\n\u2022 Manage JAD Sessions with Business Users in Design, Development& maintenance of analysis needs.\n\u2022 Provide input to project management role in terms of development/test activities and sequencing.\n\u2022 Utilize Business Objects reporting/ dashboard tools to import KPI\'s from data warehouse and ERP systems and present business insights to the business leaders and stakeholders.\n\u2022 Troubleshooting of universe schema with loops, chasm and fan traps, and cardinality problems.\n\nEnvironment: Erwin r9.0, Informatica 9.0, ODS, OLTP, Oracle 10g, Hive, OLAP, DB2, Metadata, MS Excel, Mainframes MS Visio, Rational Rose and Requisite Pro. Hadoop, PL/SQL, etc..', u""Data Scientist\nInfor - Dallas, TX\nFebruary 2014 to November 2015\nDescription: Worked as a part of Analytical Group and worked on development and designing of statistical models for the client. Coordinated with end users on designing and implementing data solutions as per project Requirements\nResponsibilities:\n\u2022 Designed algorithms to identify and extract incident alerts from a daily pool of incidents.\n\u2022 Reduced redundancy among incoming incidents by proposing rules to recognize patterns.\n\u2022 Scheduled searches Using Splunk tool.\n\u2022 Worked with Machinelearning algorithms like Regressions (linear,logistic etc..),SVMs, Decision trees.\n\u2022 Worked on Clustering and classification of data using machine learning algorithms.\n\u2022 Created Custom Dashboards and reports using Splunk tool.\n\u2022 Estimation and Requirement Analysis of project timelines.\n\u2022 Analyzed data and recommended new strategies for root cause and finding quickest way to solve big data sets.\n\u2022 Used packages like dplyr, tidyr&ggplot2in R Studio for data visualization.\n\u2022 Analyzed data from Primary and secondary sources using statistical techniques to provide daily reports.\n\u2022 Developed and designed SQL procedures and Linuxshellscripts for data export/import and for converting data.\n\u2022 Used Test Driven Development (TDD) for the project.\n\u2022 Written SQL Queries, Store Procedures, Triggers and functions for MySQL Databases.\n\u2022 Coordinate with data scientists and senior technical staff to identify client's needs and document assumptions.\n\nEnvironment: R, R Studio, Splunk, SQL, MYSQL and Windows."", u""R Programmer / Data Scientist\nActelion Pharmaceuticals - San Francisco, CA\nNovember 2012 to January 2014\nDescription: Actelion is a global biopharmaceutical company developing and marketing innovative drugs for high unmet medical needs.\n\nResponsibilities:\n\u2022 Conducted research on development and designing of sample methodologies, and analyzed data for pricing of client's products.\n\u2022 Use Correlation analysis to identify relation between variables, patterns, outliers and causal factors.\n\u2022 Identified statistically significant variables.\n\u2022 Investigated market sizing, competitive analysis and positioning for product feasibility.\n\u2022 Worked on Business forecasting, segmentation analysis and Data mining.\n\u2022 Used Support vector machines for classification of data in groups.\n\u2022 Generated graphs and reports using ggplot package in R-Studio for analytical models.\n\u2022 Developed and implemented R and Shiny application which showcases machine learning for business forecasting.\n\u2022 Developed predictive models using Decision Tree, Random Forest and Na\xefve Bayes.\n\u2022 Performed time series analysis using Spotfire.\n\u2022 Collaborating with dev-ops teams for production deployment.\n\u2022 Worked in AmazonWebServices cloud computing environment.\n\u2022 Worked with Caffe Deep Learning Framework.\n\u2022 Developed various workbooks in Spot fire from multiple data sources.\n\u2022 Created dashboards and visualizations using Spot fire desktop.\n\u2022 Worked on R packages to interface with CaffeDeep Learning Framework.\n\u2022 Performed analysis using JMP.\n\u2022 Perform validation on machine learning output from R.\n\u2022 Written connectors to extract data from databases.\n\nEnvironment: R, R Studio, Excel 2013, Amazon Web Services, Machine Learning,Spotfire , JMP, Segmentation analysis."", u'Data Analyst\nCyient Ltd - Hyderabad, Telangana\nFebruary 2011 to October 2012\nDescription: Cyient is a global solutions provider focused on engineering, manufacturing, data analytics, and networks & operations. Infotech Enterprises Ltd.\n\nResponsibilities:\n\u2022 Analyze business information requirements and model class diagrams and/or conceptual domain models.\n\u2022 Gather & Review Customer Information Requirements for OLAP and building the data mart.\n\u2022 Performed document analysis involving creation of Use Cases and Use Case narrations using Microsoft Visio, in order to present the efficiency of the gathered requirements.\n\u2022 Calculated and analyzed claims data for provider incentive and supplemental benefit analysis using Microsoft Access and Oracle SQL.\n\u2022 Analyzed business process workflows and assisted in the development of ETL procedures for mapping data from source to target systems.\n\u2022 Worked with BTEQ to submit SQL statements, import and export data, and generate reports in Terra-data.\n\u2022 Responsible for defining the key identifiers for each mapping/interface\n\u2022 Responsible for defining the functional requirement documents for each source to target interface.\n\u2022 Coordinated meetings with vendors to define requirements and system interaction agreement documentation between client and vendor system.\n\u2022 Enterprise Metadata Library with any changes or updates.\n\u2022 Document data quality and traceability documents for each source interface.\n\u2022 Establish standards of procedures.\n\u2022 Generate weekly and monthly asset inventory reports.\n\u2022 Managed the project requirements, documents and use cases by IBM Rational RequisitePro.\n\u2022 Assisted in building an Integrated LogicalDataDesign, propose physical database design for building the data mart.\n\u2022 Document all data mapping and transformation processes in the Functional Design documents based on the business requirements.\nEnvironment: SQL Server 2008R2/2005 Enterprise, SSRS, SSIS, Crystal Reports, Windows Enterprise Server 2000, DTS, SQL Profiler, and Query Analyzer.', u'Java Developer\nBirla Soft - Hyderabad, Telangana\nAugust 2009 to January 2011\nDescription: Birlasoft provides IT Consulting, offshore technological software solutions, Business Automation, Custom Business Applications and empower digitization across industries.\n\nResponsibilities:\n\u2022 Participate in requirements & design discussions\n\u2022 Responsible and active in the analysis, design, implementation and deployment of full Software Development Lifecycle (SDLC) of the project.\n\u2022 Designed and developed user interface using JSP, HTML and JavaScript.\n\u2022 Developed Struts action classes, action forms and performed action mapping using Struts framework and performed data validation in form beans and action classes.\n\u2022 Extensively used Struts framework as the controller to handle subsequent client requests and invoke the model based upon user requests.\n\u2022 Defined the search criteria and pulled out the record of the customer from the database. Make the required changes and save the updated record back to the database.\n\u2022 Validated the fields of user registration screen and login screen by writing JavaScript validations.\n\u2022 Developed build and deployment scripts using Apache ANT to customize WAR and EAR files.\n\u2022 Used DAO and JDBC for database access.\n\u2022 Developed application using AngularJS and Node.JS connecting to Oracle on the backend.\n\u2022 Designed and developed a Restful APIs using Spring REST API.\n\u2022 Consumed RESTful web services using AngularJS HTTP service & rendered the JSON data on the screen.\n\u2022 Used AngularJS framework for building web-apps and is highly efficient in integrating with Restful services.\n\u2022 Design and develop XML processing components for dynamic menus on the application.\n\u2022 Involved in postproduction support and maintenance of the application.\n\nEnvironment: Oracle, Java, Struts, Servlets, HTML, XML, SQL, J2EE, Angular JS, JUnit, RESTful, SOA, Tomcat']","[u""Bachelor's""]",[u''],"degree_1 : ""Bachelors"""
0,https://resumes.indeed.com/resume/f246f35bfdb60458,"[u'Data Scientist\nBXB Digital - Mountain View, CA\nJanuary 2017 to Present\n\u2022 Responsible for modeling complex problems, discovering insights and identifying opportunities through the use of statistical, algorithmic, mining and visualization techniques.\n\u2022 Work closely with key stakeholders to turn data into critical information and knowledge that can be used to make sound organizational decisions.\n\u2022 Present findings to the business by exposing their assumptions and validation work in a way that can be easily understood by business counterparts.\n\u2022 Drive analytical rigor over the entire asset management lifecycle. Lead the design, testing, and development of operational models for all of our markets. Partner with executives, local teams, customers and other internal business units to unlock opportunities for growth.\n\u2022 Document performance risks, results, analysis and recommendations; Effectively communicate information and insights to stakeholders.', u'Data Science Associate Instructor\nGeneral Assembly - San Francisco, CA\nSeptember 2016 to January 2017\nDraw on technical knowledge to teach business-centric data science, assess student work & provide feedback, and contribute to curriculum development.', u'Fellow\nThe Data Incubator - San Francisco, CA\nJanuary 2016 to March 2016\nDesigned, executed and web-deployed a series of exploratory & explanatory data science projects.\nCompleted coursework covering data mining, feature extraction, graph analysis, time series analysis, natural language processing (NLP), web scraping, A/B testing, mapreduce and big data.\nCompleted projects in machine learning regression and classification (linear regression with regularization, logistic regression, decision trees, ensemble methods, support vector machines).\nSKILLS\nOrganization: Python (pandas, lxml, beautifulsoup), PostgreSQL, SQLite, Git, GitHub\nAnalysis: Python (numpy, scipy, scikit-learn, scikit-image), MATLAB, Spark, Scala, R\nVisualization: Python (matplotlib, bokeh, jupyter, flask), HTML, CSS, MATLAB, Illustrator\nOperating systems, scripting and file systems: UNIX, OS X, Linux, Windows, Hadoop, Bash, AWS', u'Postdoctoral Researcher\nUC Davis / Novartis - Sacramento, CA\nAugust 2013 to December 2015\nDesigned, executed and interpreted experimental work on a confidential, multi-year project to develop a biocompatible chemical sensor for the pharmaceutical industry.\nApplied both MATLAB & Python to automate organization & processing of experimental data and statistical analysis to identify trends in spectral data.\nConstructed a classification model with dimensionality reduction to identify bacteria samples.']","[u'in Data Science', u'Ph.D. in Applied Science Engineering', u'M.S. in Physics', u'B.S. in Physics']","[u'The Data Incubator San Francisco, CA\nJanuary 2016 to January 2016', u'University of California Davis, CA\nJanuary 2009', u'San Francisco State University\nJanuary 2005', u'University of California San Diego, CA\nJanuary 2002']","degree_1 : in Data Science, degree_2 :  Ph.D. in Applied Science Engineering, degree_3 :  M.S. in Physics, degree_4 :  B.S. in Physics"
0,https://resumes.indeed.com/resume/266d3fa26c0887db,"[u'Sr. Data Architect /Data Scientist/Modeler\nCognos Impromptu - Chicago, IL\nMarch 2017 to Present\n7.0/6.0/5.0, Informatica Analytics Delivery Platform, Micro Strategy, SSRS, Tableau\n\u2022 Tools: MS-Office suite (Word, Excel, MS Project and Outlook), VSS\n\u2022 Programming Languages: SQL, T-SQL, Base SAS and SAS/SQL, HTML, XML\nIRI, Chicago, IL Mar 17 to till date\nSr. Data Architect /Data Scientist/Modeler\nResponsibilities:\n\u2022 Design, Develop and implement Comprehensive Data Warehouse Solution to extract, clean, transfer, load and manage quality/accuracy of data from various sources to EDW Enterprise Data Warehouse.\n\u2022 Architect framework for data warehouse solutions to bring data from source system to EDW and provide data mart solutions for Order/Sales operation, Salesforce activity, Inventory tracking, in depth data mining and analysis for market projection etc.\n\u2022 Utilized Apache Spark with Python to develop and execute Big Data Analytics and Machine learning applications, executed machine Learning use cases under Spark ML and Mllib.\n\u2022 Handled importing data from various data sources, performed transformations using Hive, Map Reduce, and loaded data into HDFS.\n\u2022 Utilized Spark, Scala, Hadoop, HBase, Cassandra, MongoDB, Kafka, Spark Streaming, MLLib, Python, a broad variety of machine learning methods including classifications, regressions, dimensionally reduction etc. and Utilized the engine to increase user lifetime by 45% and triple user conversations for target categories.\n\u2022 Designed and developed NLP models for sentiment analysis.\n\u2022 Designed and provisioned the platform architecture to execute Hadoop and machine learning use cases under Cloud infrastruture, AWS, EMR, S3.\n\u2022 Developed and configured on Informatica MDM hub supports the Master Data Management (MDM), Business Intelligence (BI) and Data Warehousing platforms to meet business needs.\n\u2022 Transforming staging area data into a STAR schema (hosted on Amazon Redshift) which was then used for developing embedded Tableau dashboards\n\u2022 Worked on machine learning on large size data using Spark and MapReduce.\n\u2022 Let the implementation of new statistical algorithms and operators on Hadoop and SQL platforms and utilized optimizations techniques, linear regressions, K-means clustering, Native Bayes and other approaches.\n\u2022 Developed Spark/Scala, Python for regular expression (regex) project in the Hadoop/Hive environment with Linux/Windows for big data resources.\n\u2022 Proficiency in SQL across a number of dialects (we commonly write MySQL, PostgreSQL, Redshift, Teradata, and Oracle)\n\u2022 Responsible for full data loads from production to AWS Redshift staging environment and Worked on migrating of EDW to AWS using EMR and various other technologies.\n\u2022 Deep understand of deep learning algorithms and workflows, in particular working with large scale visual data and agile integration of deep learning, data collection and failure analysis.\n\u2022 Worked on TeradataSQL queries, Teradata Indexes, Utilities such as Mload, Tpump, Fast load and FastExport.\n\u2022 Application of various machine learning algorithms and statistical modeling like decision trees, regression models, neural networks, SVM, clustering to identify Volume using scikit-learn package in python, Matlab.\n\u2022 Worked on data pre-processing and cleaning the data to perform feature engineering and performed data imputation techniques for the missing values in the dataset using Python.\n\u2022 Developing new algorithms and prototyping them for deep learning training and inference and visualization and deeper understanding of deep learning networks.\n\u2022 Created Data Quality Scripts using SQL and Hive to validate successful data load and quality of the data. Created various types of data visualizations using Python and Tableau.\n\u2022 Worked with BTEQ to submit SQL statements, import and export data, and generate reports in Teradata.\n\u2022 Build and maintain scalable data pipelines using the Hadoop ecosystem and other open source components like Hive and HBase.\n\u2022 Build advanced Deep Learning (DL) based systems and drive their adoption into next-generation of MultiOmyx image analytics applications and Worked with TensorFlow, Keras, CNTK, Caffe or other Deep Learning frameworks.\n\u2022 Created Hive architecture used for real time monitoring and HBase used for reporting and worked for map reduce and query optimization for Hadoop hive and HBase architecture.\n\u2022 Involved in Teradata utilities (BTEQ, Fast Load, Fast Export, Multiload, and Tpump) in both Windows and Mainframe platforms.\n\u2022 Built analytical data pipelines to port data in and out of Hadoop/HDFS from structured and unstructured sources and designed and implemented system architecture for Amazon EC2 based cloud-hosted solution for client.\nEnvironment: Erwin9.6.4, Oracle 12c, Python, Pyspark, Spark, Spark MLLib, Tableau, ODS, PL/SQL, OLAP, OLTP, AWS, Hadoop, MapReduce, HDFS, Python, MDM, Teradata 15, Hadoop, Spark, Cassandra, SAP, MS Excel, Flat files, Tableau, Informatica, SSIS, SSRS, AWS EC2, AWS EMR, Elastic Search.', u""Sr. Data Architect /Data Scientist/Modeler\nSEI Investments - Malvern, PA\nJuly 2016 to February 2017\nResponsibilities:\n\u2022 Provide data architecture support to enterprise data management efforts, such as the development of the enterprise data model and master and reference data, as well as support to projects, such as the development of physical data models, data warehouses and data marts.\n\u2022 Create new data designs and make sure they fall within the realm of the overall Enterprise BI Architecture and Building relationships and trust with key stakeholders to support program delivery and adoption of enterprise architecture.\n\u2022 Implemented HQL Scripts in creating Hive tables, loading, analyzing, merging, binning, backfilling, cleansing using hive.\n\u2022 Used R, SQL to create Statistical algorithms involving Multivariate Regression, Linear Regression, Logistic Regression, PCA, Random forest models, Decision trees, Support Vector Machine for estimating the risks.\n\u2022 Used R for Exploratory Data Analysis, A/B testing, Anova test and Hypothesis test to compare and identify the effectiveness of Creative Campaigns.\n\u2022 Providing technical leadership, mentoring throughout the project life-cycle, developing vision, strategy, architecture and overall design for assigned domain and for solutions and responsible for the development of target data architecture, design principles, quality control, and data standards for the organization\n\u2022 Developed and maintains data models and data dictionaries, data maps and other artifacts across the organization, including the conceptual and physical models, as well as metadata repository\n\u2022 Performed extensive Data Validation, Data Verification against Data Warehouse and performed debugging of the SQL-Statements and stored procedures for business scenarios.\n\u2022 Used Spark Data frames, Spark-SQL, Spark MLLib extensively and developing and designing POC's using Scala, Spark SQL and MLlib libraries.\n\u2022 Working on a Map RHadoop platform to implement Bigdata solutions using Hive, Map reduce, shell scripting and Pig.\n\u2022 Worked with cloud based technology like Redshift, S3, AWS, EC2 Machine, etc. and extracting the data from the Oracle financials and the Redshift database.\n\u2022 Used Teradata OLAP functions like RANK, ROW_NUMBER, QUALIFY, CSUM and SAMPLE.\n\u2022 Involved in designing and developing Data Models and Data Marts that support the Business Intelligence Data Warehouse.\n\u2022 Extensively used Aginity Netezza work bench to perform various DML, DDLetc operations on Netezza database.\n\u2022 Developed multiple MapReduce jobs in java for Data Cleaning and pre-processing analyzing data in PIG.\n\u2022 Worked on predictive and what-if analysis using R from HDFS and successfully loaded files to HDFS from Teradata, and loaded from HDFS to HIVE.\n\u2022 Designed the schema, configured and deployed AWS Redshift for optimal storage and fast retrieval of data.\n\u2022 NLTK, Stanford NLP, RAKE to preprocess the data, entity extraction and keyword extraction.\n\u2022 Transforming staging area data into a STAR schema (hosted on Amazon RedShift) which was then used for developing embedded Tableau dashboards\n\u2022 Developed SQL scripts for loading data from staging area to Target tables and worked on SQL and SAS script mapping.\n\u2022 Performed transformations of data using Spark and Hive according to business requirements for generating various analytical datasets.\n\u2022 Performed Multinomial Logistic Regression, Random forest, Decision Tree, SVM to classify package is going to deliver on time for the new route and Performed data analysis by using Hive to retrieve the data from Hadoop cluster, Sql to retrieve data from Oracle database.\n\u2022 Proposed the EDW data design to centralize the data scattered across multiple datasets and Worked on migrating of EDW to AWS using EMR and various other technologies.\n\u2022 Worked on the development of Data Warehouse, Business Intelligence architecture that involves data integration and the conversion of data from multiple sources and platforms.\n\u2022 Let the implementation of new statistical algorithms and operators on Hadoop and SQL platforms and utilized optimizations techniques, linear regressions, K-means clustering, Native Bayes and other approaches.\n\u2022 Created mapreduce running over HDFS for data mining and analysis using R and Loading & Storage data to Pig Script and R for MapReduce operations.\n\u2022 Worked on Teradata SQL queries, Teradata Indexes, Utilities such as Mload, Tpump, Fast load and Fast Export.\n\u2022 Used Metadata tool for importing metadata from repository, new job categories and creating new data elements and proficiency in SQL across a number of dialects (we commonly write MySQL, PostgreSQL, Redshift, SQL Server, and Oracle)\nEnvironment: Oracle 12c, SQL Plus, Erwin 9.6, MS Visio, SAS, Source Offsite (SOS), Hive, PIG, Windows XP, AWS, QC Explorer, Share point workspace, Teradata, Oracle, Agile, PostgreSQL, Data Stage, MDM , Netezza, IBM Infosphere, SQL, PL/SQL, IBM DB2, SSIS, Power BI, AWS Redshift, Business Objects XI3.5, COBOL, SSRS, QuickData, Hadoop, MongoDB, HBase, Hive, Cassandra, JavaScript."", u""Sr. Data Architect/Data Modeler\nTSYS - Atlanta, GA\nJanuary 2014 to June 2016\nResponsibilities:\n\u2022 Architect and design, solutions for complex business requirements, including data processing, analytics and ETL and reporting processes to improve performance of data loads and processes.\n\u2022 Develop a high performance, scalable data architecture solution that incorporates a matrix of technology to relate architectural decision to business needs.\n\u2022 Participated in the design, development, and support of the corporate operation data store and enterprise data warehouse database environment.\n\u2022 Conducting strategy and architecture sessions and deliver artifacts such as MDM strategy (Current state, Interim State and Target state) and MDM Architecture (Conceptual, Logical and Physical) at detail level.\n\u2022 Owned and managed all changes to the data models, Created data models, solution designs and data architecture documentation for complex information systems.\n\u2022 Design and development of dimensional data model on Redshift to provide advanced selection analytics platform and developed Simple to complex Map Reduce Jobs using Hive and Pig.\n\u2022 Analyze change requests for mapping of multiple source systems for understanding of Enterprise wide information architecture to devise Technical Solutions.\n\u2022 Worked on AWS Redshift and RDS for implementing models and data on RDS and Redshift.\n\u2022 Worked with SME's and other stakeholders to determine the requirements to identify Entities and Attributes to build Conceptual, Logical and Physical Data Models.\n\u2022 Provided data sourcing methodology, resource management and performance monitoring for data acquisition.\n\u2022 Designed and implemented Near Real Time ETL and Analytics using Redshift database\n\u2022 Supported and followed information governance and data standardization procedures established by the organization. Documents reports library as well as external data imports and exports.\n\u2022 Prepared Tableau reports and dashboards with calculated fields, parameters, sets, groups or bins and publish on the server.\n\u2022 Developed mappings to load Fact and Dimension tables, SCD Type 1 and SCD Type 2 dimensions and Incremental loading and unit tested the mappings.\n\u2022 Extensively used Netezza utilities like NZLOAD and NZSQL and loaded data directly from Oracle to Netezza without any intermediate files.\n\u2022 Performed analysis of data sources and processes to ensure data integrity, completeness and accuracy.\n\u2022 Created a logical design and physical design in Erwin.\n\u2022 Enforced referential integrity in the OLTP data model for consistent relationship between tables and efficient database design.\n\u2022 Implemented Hive Generic UDF's to incorporate business logic into Hive Queries and Creating Hive tables and working on them using Hive QL.\n\u2022 Developed DataMapping, DataGovernance, and transformation and cleansing rules for the Master Data Management Architecture involving OLTP, ODS and generated ad-hoc reports using OBIEE.\n\u2022 Responsible for migrating the data and data models from SQL server environment to Oracle environment.\n\u2022 Analysis and designing the ETL architecture, creating templates, training, consulting, development, deployment, maintenance and support.\n\u2022 Created SSIS Packages which loads the data from the CMS to the EMS library database and Involved in data modeling and providing technical solutions related to Teradata to the team.\n\u2022 Build a real time event analytic systems using dynamic Amazon redshift schema.\n\u2022 Designed the physical model for implementing the model into Oracle 11g physical data base and Developed SQL Queries to get complex data from different tables in Hemisphere using joins, database links.\n\u2022 Wrote SQL queries, PL/SQL procedures/packages, triggers and cursors to extract and process data from various source tables of database.\n\u2022 Created Hive Tables, loaded transactional data from Teradata using Sqoop and created and worked Sqoop jobs with incremental load to populate Hive External tables.\n\u2022 Developed LINUX Shell scripts by using NZSQL/NZLOAD utilities to load data from flat files to Netezza database.\n\u2022 Worked with cloud based technology like Redshift, S3, AWS, EC2 Machine, etc. and extracting the data from the Oracle financials and the Redshift database.\n\u2022 Used Erwin to create logical and physical data models for enterprise wide OLAP system and Involved in mapping the data elements from the User Interface to the Database and help identify the gaps.\n\u2022 Designing and customizing data models for Data warehouse supporting data from multiple sources on real time. Requirements elicitation and Data analysis. Implementation of ETL Best Practices.\n\u2022 Generated comprehensive analytical reports by running SQLqueries against current databases to conduct data analysis.\n\u2022 Created data models for AWS Redshift and Hive from dimensional data models.\n\u2022 Developed complex SQL scripts for Teradata database for creating BI layer on DW for Tableau reporting.\n\u2022 Extensively used ETL methodology for supporting data extraction, transformations and loading processing, in a complex EDW using Informatica.\n\u2022 Created Active Batch jobs to load data from distribution servers to PostgreSQL DB using *.bat files and worked on CDC schema to keep track of all transactions.\nEnvironment: Erwin 9.5, MS Visio, Oracle 11g, Oracle Designer, MDM , Power BI, SAS, SSIS, Tableau, Tivoli Job Scheduler, SQL Server 2012, DATAFLUX 6.1, JavaScript, AWS Redshift, PL/SQL, SQL/PL SQl, SSRS, PostgreSQL, Data Stage, SQL Navigator Crystal Reports 9, Hive, Netezza, Teradata, T-SQL, Informatica."", u'Sr. Data Architect/Data Analyst/Data Modeler\nMolina Healthcare - Long Beach, CA\nMarch 2012 to December 2013\nResponsibilities:\n\u2022 Design and develop data warehouse architecture, data modeling/conversion solutions, and ETL mapping solutions within structured data warehouse environments\n\u2022 Reconcile data and ensure data integrity and consistency across various organizational operating platforms for business impact.\n\u2022 Define best practices for data loading and extraction and ensure architectural alignment of the designs and development.\n\u2022 Used Erwin for effective model management of sharing, dividing and reusing model information and design for productivity improvement.\n\u2022 Involved in preparing Logical Data Models/Physical Data Models.\n\u2022 Worked extensively in both Forward Engineering as well as Reverse Engineering using data modeling tools.\n\u2022 Involved in the creation, maintenance of Data Warehouse and repositories containing Metadata.\n\u2022 Involved using ETL tool Informatica to populate the database, data transformation from the old database to the new database using Oracle and SQL Server.\n\u2022 Identifying inconsistencies or issues from incoming HL7 messages, documenting the inconsistencies, and working with clients to resolve the data inconsistencies\n\u2022 Resolved the data type inconsistencies between the source systems and the target system using the Mapping Documents and analyzing the database using SQL queries.\n\u2022 Extensively used both Star Schema and Snow flake schema methodologies in building and designing the logical data model in both Type1 and Type2Dimensional Models.\n\u2022 Worked with DBA group to create Best-Fit Physical Data Model from the Logical Data Model using Forward Engineering.\n\u2022 Worked with Data Steward Team for designing, documenting and configuring Informatica DataDirector for supporting management of MDM data.\n\u2022 Conducting HL7 integration testing with clients systems that is testing of business scenarios to ensure that information is able to flow correctly between applications.\n\u2022 Extensively worked with MySQL and Redshift performance tuning and reduced the ETL job load time by 31% and DW space usage by 50%\n\u2022 Developed Data Migration and Cleansing rules for the Integration Architecture (OLTP, ODS, DW).\n\u2022 Used Teradata SQL Assistant, Teradata Administrator, PMON and data load/export utilities like BTEQ, Fast Load, Multi Load, Fast Export, Tpump on UNIX/Windows environments and running the batch process for Teradata.\n\u2022 Created Dashboards on Tableau from different sources using data blending from Oracle, SQL Server, MS Access and CSV at single instance.\n\u2022 Used the Agile Scrum methodology to build the different phases of Software development life cycle.\n\u2022 Documented logical, physical, relational and dimensional data models. Designed the data marts in dimensional data modeling using star and snowflake schemas.\n\u2022 Created dimensional model based on star schemas and designed them using ERwin.\n\u2022 Performed match/merge and ran match rules to check the effectiveness of MDM process on data.\n\u2022 Carrying out HL7 interface unit testing aiming to confirm that HL7 messages sent or received from each application conform to the HL7 interface specification.\n\u2022 Used tools such as SAS/Access and SAS/SQL to create and extract oracle tables.\n\u2022 Data modeling and design of data warehouse and data marts in star schema methodology with confirmed and granular dimensions and FACT tables.\n\u2022 Developed SQL Queries to fetch complex data from different tables in remote databases using joins, database links and Bulk collects.\n\u2022 Enabled the SSIS package configuration to make the flexibility to pass the connection strings to connection managers and values to package variables explicitly based on environments.\n\u2022 Responsible for Implementation of HL7 to build Orders, Results, ADT, DFT interfaces for client hospitals\n\u2022 Connected to Amazon RedShift through Tableau to extract live data for real time analysis.\n\u2022 Developed SQL Queries to fetch complex data from different tables in remote databases using joins, database links and Bulk collects.\n\u2022 Developed Slowly Changing Dimensions Mapping for Type 1 SCD and Type 2 SCD and Used OBIEE to create reports.\n\u2022 Worked on data modeling and produced data mapping and data definition specification documentation.\nEnvironment: Erwin, Oracle, SQL server 2008, Power BI, MS Excel, Netezza, Agile, MS Visio, Rational Rose, Requisite Pro, SAS, SSIS, SSRS, Windows 7, PL/SQL, , SQl Server, MDM, Teradata, MS Office, MS Access, SQL, SSIS, MS Visio, Tableau, Informatica, Amazon Redshift.', u'Sr. Data Modeler/Data Analyst\nCapital Bank, FL\nJanuary 2010 to February 2012\nResponsibilities:\n\u2022 Designed logical and physical data models for multiple OLTP and Analytic applications.\n\u2022 Involved in analysis of business requirements and keeping track of data available from various data sources, transform and load the data into Target Tables using Informatica Power Center.\n\u2022 Extensively used the Erwin design tool &Erwin model manager to create and maintain the Data Mart.\n\u2022 Extensively used Star Schema methodologies in building and designing the logical data model into Dimensional Models\n\u2022 Created stored procedures using PL/SQL and tuned the databases and backend process.\n\u2022 Involved with Data Analysis primarily Identifying Data Sets, Source Data, Source Meta Data, Data Definitions and Data Formats\n\u2022 Performance tuning of the database, which includes indexes, and optimizing SQL statements, monitoring the server.\n\u2022 Developed Informatica mappings, sessions, workflows and have written Pl SQL codes for effective and optimized data flow coding.\n\u2022 Wrote SQL Queries, Dynamic-queries, sub-queries and complex joins for generating Complex Stored Procedures, Triggers, User-defined Functions, Views and Cursors.\n\u2022 Created new HL7 interface based on the requirement using XML, XSLT technology.\n\u2022 Experienced in creating UNIX scripts for file transfer and file manipulation and utilized SDLC and Agile methodologies such as SCRUM.\n\u2022 DataStage jobs were scheduled, monitored, performance of individual stages was analyzed and multiple instances of a job were run using DataStage Director.\n\u2022 Led successful integration of HL7 Lab Interfaces and used expertise of SQL to integrate HL7 Interfaces and carried out detailed and various test cases on newly built HL7 interface.\n\u2022 Wrote simple and advanced SQL queries and scripts to create standard and ad hoc reports for senior managers.\n\u2022 Involved in collaborating with ETL/Informatica teams to source data, perform data analysis to identify gaps\n\u2022 Used Expert level understanding of different databases in combinations for Data extraction and loading, joining data extracted from different databases and loading to a specific database.\n\u2022 Designed and Developed PL/SQL procedures, functions and packages to create Summary tables.\nEnvironment: SQL Server, UML, Business Objects 5, Teradata, Windows XP, SSIS, SSRS, Embarcadero, ER studio, Erwin, DB2, Informatica, HL7, Oracle, Query Management Facility (QMF), SSRS, Data Stage, Clear Case forms, SAS, Agile, Unix and Shell Scripting.', u'Data Analyst/Data Modeler\nGrowelSoftech\nAugust 2007 to December 2009\nResponsibilities:\n\u2022 Developed Data Mapping, Data Governance and transformation and cleansing rules for the Master Data Management Architecture involving OLTP, ODS.\n\u2022 Created new conceptual, logical and physical data models using ERWin and reviewed these models with application team and modeling team.\n\u2022 Performed numerous data pulling requests using SQL for analysis and created databases for OLAP Metadata catalog tables using forward engineering of models in Erwin.\n\u2022 Enforced referential integrity in the OLTP data model for consistent relationship between tables and efficient database design.\n\u2022 Proficient in importing/exporting large amounts of data from files to Teradata and vice versa.\n\u2022 Developed Data Mapping, Data Governance, and Transformation and cleansing rules for the Master Data Management Architecture involving OLTP, ODS.\n\u2022 Identified and tracked the slowly changing dimensions, heterogeneous sources and determined the hierarchies in dimensions.\n\u2022 Utilized ODBC for connectivity to Teradata & MSExcel for automating reports and graphical representation of data to the Business and Operational Analysts.\n\u2022 Extracted data from existing data source, Developing and executing departmental reports for performance and response purposes by using oracle SQL, MS Excel.\n\u2022 Extracted data from existing data source and performed ad-hoc queries and used BETQ to run and Teradata SQL scripts to create physical data model.\nEnvironment: UNIX scripting, Oracle SQL Developer, SSRS, SSIS, Teradata, Windows XP, SAS data sets.']","[u""Bachelor's""]",[u''],"degree_1 : ""Bachelors"""
0,https://resumes.indeed.com/resume/fc44172915acc502,[u'Data Scientist'],"[u""MASTER'S in INFORMATION ASSURANCE & MANAGEMENT"", u""BACHELOR'S in ELECTRICAL & ELECTRONICS ENGINEERING""]","[u'Wilmington University\nDecember 2017', u'Osmania University\nMay 2015']","degree_1 : ""MASTERS in INFORMATION ASSURANCE & MANAGEMENT"", degree_2 :  ""BACHELORS in ELECTRICAL & ELECTRONICS ENGINEERING"""
0,https://resumes.indeed.com/resume/461a75a0818473ca,"[u'Data Scientist - Tech Lead, Decision Sciences\nCASHe - Fintech Startup, AERIES TECHNOLOGY GROUP LTD - Mumbai, Maharashtra\nOctober 2016 to July 2017\nMumbai, India Oct 2016 - July 2017\nData Scientist - Tech Lead, Decision Sciences\nProjects - Risk Analytics Artificial Intelligence Framework, Behavioral Customer Segmentation\n\u2022 Developed algorithms and supporting codes in R, Python and Java to establish scalable, efficient and fully\nautomated Risk Analytics Framework having 92% AUC to assess risk profile of customers for CASHe.\n\u2022 Lead 5-member team to build machine learning system using ensemble of models to perform regression\n\u2022 Operationally deployed ML models to analyze 100GB data of over 1M customers & USD 35M loan disbursals\n\u2022 Leveraged Kmeans clustering for segmentation of customer based on their repayment behavior and social footprint.\n\u2022 Played a major role to help company in securing $3.8 million-dollar Series-A Venture Capital Funding from investor\ngroup. Reduced turnaround time to process application by 30% and default rate from 11.3% to below 6%.\n\u2022 Technology/Tools Used - Python, R, Java, MapReduce, HBase, Statistical Analysis, Regression, Boosting, Clustering', u'Data Scientist - Tech Lead, Analytics\nLIBREOSOFT - Mumbai, Maharashtra\nJune 2014 to September 2016\nProjects - Credit Risk Evaluation Model, Churn Analysis Model\n\u2022 Led a team of data scientists responsible for the development, implementation of front-line credit decision models.\n\u2022 Developed analytical framework for statistical models in partnership with the Compliance Team.\n\u2022 Improved model performance (AUC from 84% to 91%) by using statistical techniques like k-fold cross-validation.\n\u2022 Reduced the customer churn rate for telecom company from 20% to under 11%.\n\u2022 Technology/Tools Used - Python, R, Hadoop, Data Mining, Machine Learning', u'Assistant Systems Engineer - Client & Account Data\nTATA CONSULTANCY SERVICES - Mumbai, Maharashtra\nMarch 2013 to May 2014\nMumbai, India Mar 2013 - May 2014\nAssistant Systems Engineer - Client & Account Data\n\u2022 Performed end-to-end manual & automation testing for Bank of America Wealth Management Applications.\n\u2022 Improved business process and efficiency by 21% using excel analysis. These measures helped the client in $1.2\nmillion-dollar of annual cost-savings.']","[u'Masters in Data Science', u'in Data & Analytics', u'Bachelors in Technology']","[u'THE GEORGE WASHINGTON UNIVERSITY Washington, DC\nMay 2019', u'S P JAIN SCHOOL OF GLOBAL MANAGEMENT Mumbai, Maharashtra\nAugust 2016', u'GURUKUL KANGRI UNIVERSITY Haridwar, Uttarakhand\nJuly 2012']","degree_1 : Masters in Data Science, degree_2 :  in Data & Analytics, degree_3 :  Bachelors in Technology"
0,https://resumes.indeed.com/resume/2a4219659f750901,"[u'Cost Analyst/Data Scientist\nBooz Allen Hamilton\nSeptember 2017 to Present\no Use mathematical and data science techniques to analyze patterns and trends of data\nrelated to the contract. Analyze data and compile feedback to help the team make smart\ndecisions for the project.', u'Bartender/Server\nThe Loma Club - San Diego, CA\nMay 2016 to Present\nBartender/Barback/server for an establishment in Liberty Station. Make drinks, cook food, serve food, clean glasses, and more for job duties. Experience with working events/parties.', u'Tutor\nSan Diego, CA\nJanuary 2011 to Present\nTeach all levels of Math/Science/SAT courses to students in the San Diego area.', u""Information Assurance Internship\nU.S. Air Force Research Laboratory\nMay 2013 to August 2013\no Internship included formal education on the science of Information Assurance and training on the art of cyber warfare that applied to solving cyber security research\nproblems of interest to the Air Force Senior Scientist for Information Assurance, crisis\ndecision-making, teamwork and on-time performance under the mentorship of government scientists, extensive oral and written communication, and weekly 8-mile\nruns. Interns are hand-picked globally from thousands of applicants ranging from the\nUnited States to Europe.\n\nRESEARCH PROJECTS:\nInvestigation of the Method of Images in Electromagnetism\no The project starts out discussing the classic image problem and introduces the idea of the method of images. The project also discusses three alternative cases where conductors and charge distributions are introduced to develop a more demanding problem. The\nultimate goal of the project is to study the methods used to solve Poisson's partial\ndifferential equation for electrostatics. Through some research, a new mathematical\nformula was found to determine the total charge of the image.\nQuantum States in a Deformed Spherical Well\no The sphere is a special case to study in quantum mechanics due to close adaptation to particles and atoms. This project assesses the case of the deformed spherical well with a\nknown potential. The particle's behavior can be described through the wave function and its set of corresponding differential equations in this special well at certain energy states.\nDetermining Rotational Equations for Asymmetric Rigid Bodies\no This project determines the special equations used to describe the rotation of three types of asymmetric rigid bodies. The project gives insight into how newer technologies have\nbecome developed, such as vehicle stability control and robotics.\nPeople, Time, and Stereograms\no The intended purpose of this project was to test men and women on the amount of time it\ntook them to react to a stereogram. Data was collected from 50 different men and women to construct probability results that would help prove theories concluded by other\nresearchers.""]",[u'Bachelor of Science in Applied Mathematics'],"[u'State University of New York Institute of Technology New York, NY\nAugust 2010 to May 2014']",degree_1 : Bachelor of Science in Applied Mathematics
0,https://resumes.indeed.com/resume/d1cf3260f9c3fafa,"[u'Senior Data Scientist\nVMware Inc\nApril 2016 to Present', u'Data Scientist\nCapita PLC\nNovember 2011 to April 2016', u'Senior Data Analyst\nITC Infotech\nNovember 2010 to October 2011', u'Data Analyst\nTCS\nApril 2010 to November 2010', u'Technical Lead - Analytics\nInfosys Technologies Limited\nMay 2006 to March 2010\nMachine Learning & Data Sciences\n* Insourced partner propensity to sell models from vendor to in-house saving 4.5M$ annually.\n* Built the Partner survey questionnaire to understand the sentiments of Partners and their overall satisfaction.\n* Led List price optimization end to end resulting in 75M$ incremental revenue annually for VMware, Inc.\n* Provided recommendation to increase the threshold of the minimum order size to avail special discounts (SPF) by minimizing the loss in order conversion.\n* Built recommendation engine for Discount driver analysis based on different attributes of discount.\n* Led FACE of life product to predict sum assured which helped in accurate prediction compared to actuaries.\n* Built Sales forecast for home product which resulted in saving of 4.3M GBP.\n\nBusiness Consulting & Stakeholder Management\n* Spearheaded the implementation of Partner Survey analytics project and drivers of Net Promoter Score.\n* Instituted a system of enterprise level customer segmentation based on customer life-time value.\n* Recommended action points to business to target top Partners across products on Quarterly basis.\n* Coordinated with MDM team, Business Intelligence team and the business stakeholders to standardize the price book data and the set of analyses related to general pricing action.\n* Developed a pricing strategy for introduction of new virtualization SKUs at VMware.\n* Worked with various stakeholders across verticals to standardize the Partner Propensity to Sell models which reduced the overall time for end to end modelling effort.\n\nThought Leadership & People Management\n* Managed multiple project delivery with a team including scope definition, analysis structuring, resource planning, work accuracy and efficiency, client communication.\n* Led team of data scientists to develop Conjoint Models and Factor analysis to understand the attributes which drive Partner satisfaction across product categories.\n* Instrumental in implementation of recommendations coming out of the Discount Guidance tool which was built using custom decision trees.\n* Introduced and implemented the practice of using Discount as a proxy to overall Price perception in list price optimization.\n* Introduced the concept of identifying fraudulent claims which was implemented for Zurich Insurance UK which resulted in overall cost savings of ~3 MM GBP.']","[u""Bachelor's""]",[u''],"degree_1 : ""Bachelors"""
0,https://resumes.indeed.com/resume/48444f8c084e310e,"[u""Data Analyst/Data Scientist\nNational Student Clearing House - Ashburn, VA\nJanuary 2018 to Present\n\u2022 Worked on data pre-processing and cleaning the data to perform feature engineering and performed data imputation techniques for the missing values in the dataset using Python.\n\u2022 Perform detailed data analysis (i.e. determine the structure, content, and quality of the data through examination of source systems and data samples)\n\u2022 Identify, retrieve, manipulate, relate and/or exploit multiple structured and unstructured data sets from various sources, including building or generating new data sets as appropriate.\n\u2022 Interpret and evaluate the results of data science algorithms, understanding the meaning, limitations and scope of the results, translating into insightful output for data science and/or mission and identifying other applications for use.\n\u2022 Initiate the efficient implementation of methods, tools, algorithms, including preliminary data exploration, data visualization and preparation, model calibration, algorithm validation and verification.\n\u2022 Define, execute and interpret complex/nested SQL queries.\n\u2022 Created Data Quality Scripts using SQL to validate successful data load and quality of the data.\n\u2022 Used Python's multiple data science packages like Pandas, NumPy, matplotlib, Seaborn, SciPy, Scikit-learn and NLTK.\n\u2022 Worked on different data formats such as JSON, XML and performed machine learning algorithms in Python.\n\u2022 Design and develop data visualizations, complex reports and dashboards as required to support user stories."", u""Data Analyst Consultant\nPerficient Inc - Boston, MA\nJuly 2017 to October 2017\n\u2022 Investigation of potential misconduct cases. The process consists of risk ranking and validation of 24 test items.\n\u2022 Ensure data completeness and quality check, extract and analyze data from various sources using MS Access/ SQL, identification of defects/issues/gaps in the system, analysis of data and remediation of defect pertaining to the company's system processes.\n\u2022 Identifying the root cause for misconduct cases and provide recommendations for strengthening the internal processes and procedures.\n\u2022 Based on analysis, developed dashboards using Tableau to present our findings to management.\n\u2022 Design, Develop and execute test scenarios of large structured and unstructured data using SQL.\n\u2022 Developing the overall program, determining specific methodology to carry out fraud analysis, reporting requirements and documenting the program."", u'IT Analyst (Graduate Student and Academic Services)\nBentley University - Waltham, MA\nSeptember 2016 to May 2017\n\u2022 Carried out analysis using SQL to better meet the requirement of students using existing resources.\n\u2022 Assisted Graduate Student Affairs office in smooth functioning of its activities, which involves deciding content for website, carrying out surveys, benchmarking and maintain MIS reports.\n\u2022 Interprets data, analyzes results using statistical techniques and provide ongoing reports.\n\u2022 Develops and implements data collection systems and other strategies that optimize statistical efficiency and data quality.\n\u2022 Identify, analyze, and interpret trends or patterns in complex data sets.', u'Data Analyst/Data Scientist\nIDBI Bank Limited - Mumbai, Maharashtra\nJune 2013 to August 2016\n\u2022 Integrate data from multiple data sources or functional areas, ensures data accuracy and integrity, and updates data as need.\n\u2022 Develops and/or uses algorithms and statistical predictive models and determines analytical approaches and modeling techniques to evaluate scenarios and potential future outcomes\n\u2022 Performs analyses of structured and unstructured data to solve multiple and/or complex business problems utilizing advanced statistical techniques and mathematical analyses.\n\u2022 Collaborates with business partners to understand their problems and goals, develop predictive modeling, statistical analysis, data reports and performance metrics\n\u2022 Worked on data that was a combination of unstructured and structured data from multiple sources and automated the cleaning using Python scripts.\n\u2022 Developed advanced models using multivariate regression, Logistic regression, Random forests, decision trees and clustering.\n\u2022 Used Pandas, Numpy, Seaborn, Scipy, Matplotlib, Scikit-learn, NLTK in Python for developing various machine learning algorithms.\n\u2022 Designed, developed, and implemented data quality validation rules to inspect and monitor the health of the data.\n\u2022 Worked extensively with data governance team to maintain data models, Metadata and dictionaries.\n\u2022 Participate in the on-going design and development of a consolidated data warehouse supporting key business metrics across the organization.\n\u2022 Applied predictive analysis and statistical modeling techniques to analyze customer behavior and offer customized products, reduce delinquency rate and default rate. Lead to fall in default rates from 5% to 2%.\n\u2022 Applied machine learning techniques to tap into new markets, new customers and put forth my recommendations to the top management which resulted in increase in customer base by 5% and customer portfolio by 9%.\n\u2022 Analyzed customer master data for the identification of prospective business, to understand their business needs, built client relationships and explored opportunities for cross-selling of financial products. 60% (Increased from 40%) of customers availed more than 6 products.\n\u2022 Designed, developed, and implemented data quality validation rules to inspect and monitor the health of the data.\n\u2022 Dashboard and report development experience using Tableau and QlikView.', u'ETL Developer\nJuly 2009 to May 2013\n\u2022 Involved in full SDLC of BI Project including Data Analysis, Designing, Development of Data Warehouse environment.\n\u2022 Used the Oracle Data Integrator Designer to develop processes for extracting, cleansing, transforming, integrating, and loading data into data warehouse database\n\u2022 Experience in Developing and customizing PL/SQL packages, procedures, functions, triggers and reports using Oracle SQL Developer.\n\u2022 Responsible for designing, developing and testing of the ETL strategy to populate the data from various source systems (Flat files, Oracle).\n\u2022 Worked with the Business units to identify data quality rule requirements against identified anomalies.\n\u2022 Develop Data Mapping, Join and queries - Validation, and addressing/fixing data queries raised by project team in a timely manner.\n\u2022 Worked closely with Business analyst and interacted with the Business users to gather new business requirements and to understand the accurate business and current requirements.\n\u2022 Created Repositories, Agent, Contexts and both of Physical & Logical Schema in Topology Manager for all the source and target schemas.\n\u2022 Data mapping, logical data modeling, created class diagrams and ER diagrams and used SQL queries to filter data within the Oracle database.\n\u2022 Installed and Setup ODI Master Repository, Work Repository, Execution Repository.\n\u2022 Used Topology Manager to manage the data describing the information systems physical and logical architecture.\n\u2022 Extensively worked and utilized ODI Knowledge Modules (Reverse Engineering, Loading, Integration, Check, Journalizing and service)\n\u2022 Created various procedures and variables.\n\u2022 Created ODI Packages, Jobs of various complexities and automated process data flow.\n\u2022 Configured the Change data capture (CDC).\n\u2022 Configured and setup ODI, Master repository, Work repository, Project, Models, sources, targets, packages, Knowledge Modules, Interfaces, Scenarios, filters, condition, metadata.', u'Systems Analyst\nCoramandel Infrastructure Pvt. Ltd - Hyderabad, Telangana\nMarch 2006 to May 2009\n\u2022 Participate in full-life cycle of SQL database development; Create conceptual, logical and physical database models to support project requirements\n\u2022 Design, implement, and maintain databases for Corporate Data, Finance, and Operations business units.\n\u2022 Responsible for maintaining the integrity of the SQL database and reporting any issues to the database architect.\n\u2022 Assisted in creating and presenting informational reports to Management based on SQL data.\n\u2022 Responsible for designing, developing and testing of the ETL strategy to populate the data from various source systems (Flat files, Oracle).\n\u2022 Created Repositories, Agent, Contexts and both of Physical & Logical Schema in Topology Manager for all the source and target schemas.\n\u2022 Designed, developed, and implemented data quality validation rules to inspect and monitor the health of the data.\n\u2022 Worked on ODI Designer for designing the mapping between source and target with applied transformations as business needs by defining the data stores, interfaces and packages.', u'Associate Software Engineer\nTorry Harris Business Solutions - Bengaluru, Karnataka\nAugust 2005 to February 2006\n\u2022 Part of a team that was involved in building an automated online travel agency that catered to booking flight tickets and hotel rooms online.\n\u2022 Experience in implementing software best practices, including Use Cases, Object Oriented analysis and design, and Agile methodology.']","[u'M.S. in Business in Analytics', u'Bachelor of Engineering in C.S. in Engineering']","[u'Bentley University Waltham, MA\nDecember 2017', u'Dr. Ambedkar Institute of Technology\nMay 2005']","degree_1 : M.S. in Bsiness in Analytics, degree_2 :  Bachelor of Engineering in C.S. in Engineering"
0,https://resumes.indeed.com/resume/406782f1e438ba21,"[u'BI Developer\n(SSRS) and Analysis Services (SSAS) - Cambridge, MA\nMarch 2015 to Present\nMASSACHUSETTS\nDeveloped Business Intelligence solutions using MS SQL Server 2012 tools, including Integration Services\n(SSIS), Reporting Services(SSRS) and Analysis Services (SSAS)\nBuilt and developed many Tableau workbooks/stories using Tableau desktop visualization tool and published\nthem to Tableau server for business users with various requirements\nWrote complex stored procedures for reporting and ETL\nPerformance tuning for both SSRS datasets and SSIS packages using various MS and third party tools\nGenerated daily/monthly reports, dashboard, ad-hoc reports with drill-through features, and exported them from BI Portal into excel/csv files by data-driven subscription', u'Data Scientist\nMerck & Co - Boston, MA\nNovember 2011 to March 2015\nDeveloped projects using SQL Server Analysis Service (SSAS) with Microsoft Visual Studio, created data\nsource/data source view/cube with measure and dimensions, configured attributes of dimension table with hierarchy relationships; modified cube and dimensions, added new measure groups and dimensions;\ndeployed cube package to SSAS instance\nLoaded data generated from company and external source into data warehouse and generated SSRS reports\nbased on preclinical/clinical trial data in data warehouse\n\nComputer Skills\nDatabase and Programming Language: SQL Server 2008 R2/2012, T-SQL, Visual Basic, Java\nHeavily-used Third Party Tools: Task Factory (SSIS), CozRoc (SSIS), RedGate (source control and SQL), etc\nMicrosoft Office Tools: Word, Excel, Power Point, Access\nSelected Achievements on SQL Server Database\nConfigured SQL Server 2008 R2/2012 and Oracle SQL 11g database parameter files.\nCreated database tables with various constraints including primary key, foreign key, etc.\nLoaded data from flat data files into SQL Server 2008/2012 database tables using BCP, bulk insert, and table\nexport/import; evaluated, cleaned and reformatted raw data; wrote T-SQL scripts to manipulate data loads and extracts.\nExtracted data from SQL Server 2008/2012 database tables and Oracle database tables into flat data files and\nExcel sheet.\nPower user of advanced SQL queries with multi-table joins, group functions, sub-queries, set operations and T-\nSQL or PL/SQL functions.\nGenerated a variety of business reports using SQL Server 2008/2012 SQL query, SSRS, Crystal Report 2008 and Excel 2007 Pivot table.\n\nSelected Achievements on Business Intelligence\nCreated ETL packages using SSIS Designer to extract data from flat file, reformat the data, and insert the reformatted data into fact table, loaded different types of dimension and fact tables into data warehouse,\ndeployed SSIS packages and solutions using BIDS and SSDT.\nDeveloped projects using SQL Server 2008 R2/2012 SSAS, created data source/data source view/cube with measure and dimensions, configured attributes of dimension table with hierarchy relationships; modified cube and dimensions, added new measure groups and dimensions; created data-mining solutions in BIDS, deployed\nthem to SSAS instance, trained the model and queried the model.\nGenerated a variety of reports using SQL Server 2008/2012 SSRS including matrix report, parameter report,\ndashboard with charts and drill-down function; built reports from Analysis Services cube using MDX query\ndesigner; configured report server and deployed reports on web server.']","[u'Master of Science in Chemistry', u'Master of Engineering in Polymer Material', u'Bachelor of Science in Chemistry']","[u'New Mexico State University Las Cruces, NM\nJanuary 2006', u'Beijing Institute of Technology Beijing, CN\nJanuary 1998', u'Xiangtan University Xiangtan, CN\nJanuary 1995']","degree_1 : Master of Science in Chemistry, degree_2 :  Master of Engineering in Polymer Material, degree_3 :  Bachelor of Science in Chemistry"
0,https://resumes.indeed.com/resume/c6226ed96504623d,"[u""Data Scientist\nTriNet Group, Inc - Pleasanton, CA\nFebruary 2017 to Present\nDescription:TriNet Group, Inc. provides human resources solutions for small and midsize businesses in the United States and Canada. The company offers multi-state payroll processing and tax administration; employee benefits programs, including health insurance and retirement plans.\n\nResponsibilities:\n\u2022 Perform Data Profiling to learn about user behavior and merge data from multiple data sources.\n\u2022 Implemented big data processing applications to collect, clean and normalization large volumes of open data using Hadoop eco system such as PIG, HIVE, and HBase.\n\u2022 Designing and developing various machine learning frameworks using Python, R, and Matlab.\n\u2022 Integrate R into Micro Strategy to expose metrics determined by more sophisticated and detailed models than natively available in the tool.\n\u2022 Independently coded new programs and designed Tables to load and test the program effectively for the given POC's using with Big Data/Hadoop.\n\u2022 Solution architecting BIG Data solution for Projects & Proposal using Hadoop, Spark, ELK Stack, Kafka, Tensor flow.\n\u2022 Correct minor data errors that prevent loading of EDI files\n\u2022 Worked on Clustering and classification of data using machine learning algorithms. Used Tensor Flow machine learning to create sentimental and time series analysis.\n\u2022 Develop documents and dashboards of predictions in Microstrategy and present it to the BusinessIntelligence team.\n\u2022 Used CloudVision API integrate vision to detection features within applications, including image labeling, face and landmark detection, optical character recognition (OCR), and tagging of explicit content.\n\u2022 Implemented Text mining to transposing words and phrases in unstructured data into numerical values\n\u2022 Developed various QlikViewDataModels by extracting and using the data from various sources files, DB2, Excel, Flat Files and Bigdata.\n\u2022 Good knowledge of HadoopArchitecture and various components such as HDFS, JobTracker, TaskTracker, NameNode, DataNode, SecondaryNameNode, and MapReduce concepts.\n\u2022 As Architect delivered various complex OLAPdatabases/cubes, scorecards, dashboards and reports.\n\u2022 Track and enable communication across multiple departments to make sure all parties are as educated about potential issues as they can be.\n\u2022 Utilized human face recognition OpenCV and tackled the challenge of long running time on personal computer for face\n\u2022 Programmed a utility in Python that used multiple packages (scipy, numpy, pandas)\n\u2022 Implemented Classification using supervised algorithms like LogisticRegression, Decisiontrees, KNN, NaiveBayes.\n\u2022 Gained knowledge about OpenCV and learned to apply it to achieve the red color object identifying with the drone's camera.\n\u2022 Used Teradata15 utilities such as FastExport, MLOAD for handling various tasks data migration/ETL from OLTP Source Systems to OLAP Target Systems\n\u2022 Handled importing data from various data sources, performed transformations using Hive, MapReduce, and loadeddata into HDFS.\n\u2022 Collaborate with data engineers to implement ETL process, write and optimized SQL queries to perform data extraction from Cloud and merging from Oracle 12c.\n\u2022 Collect unstructured data from MongoDB 3.3 and completed data aggregation.\n\u2022 Perform data integrity checks, data cleaning, exploratory analysis and feature engineer using R 3.4.0.\n\u2022 Work with freight carriers to correct EDI issues as they arise\n\u2022 Conducted analysis on assessing customer consuming behaviors and discover value of customers with RMF analysis; applied customer segmentation with clustering algorithms such as K-MeansClustering and Hierarchical Clustering.\n\u2022 Work on outliers identification with box-plot, K-means clustering using Pandas, Numpy.\n\u2022 Participate in features engineering such as feature intersection generating, feature normalize and Label encoding with Scikit-learn preprocessing.\n\u2022 Use Python 3.0 (numpy, scipy, pandas, scikit-learn, seaborn, NLTK) and Spark 1.6 / 2.0 (PySpark, MLlib) to develop variety of models and algorithms for analytic purposes.\n\u2022 Analyze Data and Performed Data Preparation by applying historical model on the data set in AZUREML.\n\u2022 Experienced in Delivery, Portfolio, Team / Career, Vendor and Program management Competency in Solution Architecture, implementation & delivery of Big Data, data science analytics & DWH projects on GreenPlum, SPARK, Keras, Python and TensorFlow.\n\u2022 Coordinate the execution of A/B tests to measure the effectiveness of personalized recommendation system.\n\u2022 Perform data visualization with Tableau 10 and generate dashboards to present the findings.\n\u2022 Recommend and evaluate marketing approaches based on quality analytics of customer consuming behavior.\n\u2022 Determine customer satisfaction and help enhance customer experience using NLP.\n\u2022 Work on Text Analytics, NaiveBayes, Sentiment analysis, creating word clouds and retrieving data from Twitter and other social networking platforms.\n\u2022 Use Git 2.6 to apply version control. Tracked changes in files and coordinated work on the files among multiple team members.\n\nEnvironment:R, Matlab, MongoDB, exploratory analysis, feature engineering, K-Means Clustering, Hierarchical Clustering, Machine Learning), Python, Spark (MLlib, PySpark), Tableau, MicroStrategy, Git,Unix, , MLLib, SAS, Tensor Flow, regression, logistic regression, Hadoop 2.7, OLTP, random forest, OLAP, HDFS, ODS, NLTK, SVM, JSON, XML and MapReduce.OpenCV."", u'Data Scientist\nF5 Networks Inc - San Jose, CA\nDecember 2015 to January 2017\nDescription:\nF5 Networks, Inc. develops, markets, and sells application delivery networking products that optimize the security, performance, and availability of network applications, servers, and storage systems. It offers Local Traffic Manager, which provides intelligent load-balancing, traffic management, and application health checking; BIG-IP DNS that automatically directs users to the closest or best-performing physical, virtual, or cloud environment.\n\nResponsibilities:\n\u2022 Used Pandas, NumPy, seaborn, SciPy, Matplotlib, Scikit-learn, NLTK in Python for developing various machine learning algorithms and utilized machine learning algorithms such as linear regression, multivariate regression, naive Bayes, Random Forests, K-means, &KNN for data analysis.\n\u2022 Demonstrated experience in design and implementation of Statistical models, Predictive models, enterprise data model, metadata solution and data life cycle management in both RDBMS, Big Data environments.\n\u2022 Utilized domain knowledge and application portfolio knowledge to play a key role in defining the future state of large, business technology programs.\n\u2022 Developed MapReduce/SparkPython modules for machine learning & predictive analytics in Hadoop on AWS. Implemented a Python-based distributed random forest via Python streaming.\n\u2022 Performed Source System Analysis, database design, data modeling for the warehouse layer using MLDM concepts and package layer using Dimensional modeling.\n\u2022 Created ecosystem models (e.g. conceptual, logical, physical, canonical) that are required for supporting services within the enterprise data architecture (conceptual data model for defining the major subject areas used, ecosystem logical model for defining standard business meaning for entities and fields, and an ecosystem canonical model for defining the standard messages and formats to be used in data integration services throughout the ecosystem)\n\u2022 Developed LINUX Shell scripts by using NZSQL/NZLOAD utilities to load data from flat files to Netezza database.\n\u2022 Designed and implemented system architecture for Amazon EC2 based cloud-hosted solution for client.\n\u2022 Tested Complex ETL Mappings and Sessions based on business user requirements and business rules to load data from source flat files and RDBMS tables to target tables.\n\u2022 Hands on database design, relational integrity constraints, OLAP, OLTP, Cubes and Normalization (3NF) and De-normalization of database.\n\u2022 Developed MapReduce/Spark Python modules for machine learning & predictive analytics in Hadoop on AWS.\n\u2022 Worked on customer segmentation using an unsupervised learning technique - clustering.\n\u2022 Worked with various Teradata15 tools and utilities like Teradata Viewpoint, Multi Load, ARC, Teradata Administrator, BTEQ and other Teradata Utilities.\n\u2022 Utilized Spark, Scala, Hadoop, HBase, Kafka, Spark Streaming, MLlib, Python, a broad variety of machine learning methods including classifications, regressions, dimensionally reduction.\n\u2022 Analyzed large data sets apply machine learning techniques and develop predictive models, statistical models and developing and enhancing statistical models by leveraging best-in-class modeling techniques.\n\nEnvironment: Erwin r9.6, Python, SQL, Oracle 12c, Netezza, SQL Server, Informatica, Java, SSRS, PL/SQL, T-SQL, Tableau, MLlib, regression, Cluster analysis, Scala NLP, Cassandra, MapReduce, Spark, Kafka, MongoDB, logistic regression, Hadoop, Hive, Teradata0, random forest, OLAP, Azure, MariaDB, SAP CRM, HDFS, ODS, NLTK, SVM, JSON, Tableau, XML, AWS.', u""Data Scientist\nAlbertsons Companies - Boise, ID\nFebruary 2014 to November 2015\nDescription:Albertsons Companies LLC is an American grocery company founded and based in Boise, Idaho. It is privately owned and operated by investors, including Cerberus Capital Management.\n\nResponsibilities:\n\u2022 Developed applications of Machine Learning, Statistical Analysis and Data Visualizations with challenging data Processing problems in sustainability and biomedical domain.\n\u2022 Compiled data from various sources public and private databases to perform complex analysis and data manipulation for actionable results.\n\u2022 Designed and developed Natural Language Processing models for sentiment analysis.\n\u2022 Worked on Natural Language Processing with NLTK module of python for application development for automated customer response.\n\u2022 Used predictive modeling with tools in SAS, SPSS, R, Python.\n\u2022 Applied concepts of probability, distribution and statistical inference on given dataset to unearth interesting findings through use of comparison, T-test, F-test, R-squared, P-value etc.\n\u2022 Applied linear regression, multiple regression, ordinary least square method, mean-variance, theory of large numbers, logistic regression, dummy variable, residuals, Poisson distribution, Bayes, Naive Bayes, fitting function etc to data with help of Scikit, Scipy, Numpy and Pandas module of Python.\n\u2022 Applied clustering algorithms i.e.Hierarchical, K-means with help of Scikit and Scipy.\n\u2022 Developed visualizations and dashboards using ggplot, Tableau\n\u2022 Worked on development of data warehouse, DataLake and ETL systems using relational and non relational tools like SQL, No SQL.\n\u2022 Built and analyzed datasets using R, SAS, Matlab and Python (in decreasing order of usage).\n\u2022 Applied linear regression in Python and SAS to understand the relationship between different attributes of dataset and causal relationship between them\n\u2022 Performs complex pattern recognition of financial time series data and forecast of returns through the ARMA and ARIMA models and exponential smoothening for multivariate time series data\n\u2022 Pipelined (ingest/clean/munge/transform) data for feature extraction toward downstream classification.\n\u2022 Used ClouderaHadoop YARN to perform analytics on data in Hive.\n\u2022 Wrote Hive queries for data analysis to meet the business requirements.\n\u2022 Expertise in Business Intelligence and data visualization using R and Tableau.\n\u2022 Expert in Agile and Scrum Process.\n\u2022 Validated the Macro-Economic data (e.g. BlackRock, Moody's etc.) and predictive analysis of world markets using key indicators in Python and machine learning concepts like regression, Boot strap Aggregation and Random Forest.\n\u2022 Worked in large scale database environment like Hadoop and MapReduce, with working mechanism of Hadoop clusters, nodes and Hadoop Distributed File System (HDFS).\n\u2022 Interfaced with large scale database system through an ETL server for data extraction and preparation.\n\u2022 Identified patterns, data quality issues, and opportunities and leveraged insights by communicating opportunities with business partners.\n\nEnvironment: Machine learning, AWS, MS Azure, Cassandra, Spark, HDFS, Hive, Pig, Linux, Python (Scikit-Learn/Scipy/Numpy/Pandas), R, SAS, SPSS, MySQL, Eclipse, PL/SQL, SQL connector, Tableau."", u""Data Scientist/Data Modeller\nMolina Healthcare - Long Beach, CA\nNovember 2012 to January 2014\nDescription: Molina Healthcare is a managed care company headquartered in Long Beach, California, United States. In 2016, Molina Healthcare was ranked 201 in Fortune 500.In 2015, the company's health plans served about 3.5 million people through government-based healthcare programs.\n\nResponsibilities:\n\u2022 Developing propensity models for Retail liability products to drive proactive campaigns.\n\u2022 Extraction and tabulation of data from multiple data sources using R, SAS.\n\u2022 Data cleansing, transformation and creating new variables using R.\n\u2022 Built predictive scorecards for Cross-selling Car loan, Life Insurance, TDand RD.\n\u2022 Scoring predictive models as per regulatory requirements & ensuring deliverables with PSI.\n\u2022 Data modeling and formulation of statistical equations using advanced statistical forecasting techniques.\n\u2022 Provide guidance and mentoring to team members.\n\u2022 Arrange and chair Data Workshops with SME's and related stake holders for requirement data catalogue understanding.\n\u2022 Responsible for defining the key identifiers for each mapping/interface.\n\u2022 Responsible for defining the functional requirement documents for each source to target interface.\n\u2022 Document, clarify, and communicate requests for change requests with the requestor and coordinate with the development and testing team.\n\u2022 Work with users to identify the most appropriate source of record and profile the data required for sales and service.\n\u2022 Document the complete process flow to describe program development, logic, testing, and implementation, application integration, coding.\n\u2022 Design Logical Data Model which will fit and adopt the Teradata Financial Logical Data Model (FSLDM11) using Erwin data modeler tool.\n\u2022 Present and approve designed LogicalData Model in DataModel Governance Committee (DMGC).\n\u2022 Identifying the Customer and account attributes required for MDM implementation from disparate sourcesand preparing detailed documentation.\n\u2022 Validated the machine learning classifiers using ROC Curves and Lift Charts.\n\u2022 Extracted data from HDFS and prepared data for exploratory analysis using datamunging.\n\nEnvironment:Erwin 8, Teradata 13, SQL Server 2008, Oracle 9i, SQL*Loader, PL/SQL, ODS, OLAP, OLTP, SSAS, Informatica Power Center 8.1."", u'Data Analyst\nCyient Ltd - Hyderabad, Telangana\nFebruary 2011 to October 2012\nDescription: Worked on structured data and created reports using Excel.\n\nResponsibilities:\n\u2022 Implemented Microsoft Visio and Rational Rose for designing the Use Case Diagrams, Class model, Sequence diagrams, and Activity diagrams for SDLC process of the application\n\u2022 Worked with other teams to analyze customers to analyze parameters of marketing.\n\u2022 Conducted Design reviews and Technical reviews with other project stakeholders.\n\u2022 Was a part of the complete life cycle of the project from the requirements to the production support\n\u2022 Created test plan documents for all back-end database modules\n\u2022 Used MS Excel, MS Access and SQL to write and run various queries.\n\u2022 Used traceabilitymatrix to trace the requirements of the organization.\n\u2022 Recommended structural changes and enhancements to systems and databases.\n\u2022 Conducted Design reviews and Technical reviews with other project stakeholders.\n\u2022 Maintenance in the testing team for System testing/Integration/UAT\n\u2022 Guaranteeing quality in the deliverables.\n\nEnvironment:UNIX, SQL, Oracle 10g, MS Office, MS Visio.', u'Data Analyst\nBirla Soft - Hyderabad, Telangana\nAugust 2009 to January 2011\nDescription:Participated in designing system and coordinate with business groups in order to define the requirements and develop plans and schedules.\n\nResponsibilities:\n\u2022 Developed ETL processes for data conversions and construction of data warehouse using IBM InfoSphere DataStage.\n\n\u2022 Used Star Schema and designed Mappings between sources to operational staging targets.\n\n\u2022 Involved in defining the business/transformation rules applied for sales and service data.\n\n\u2022 Define the list codes and code conversions between the source systems and the data mart.\n\n\u2022 Provided On-call Support for the project and gave a knowledge transfer for the clients.\n\n\u2022 Used Rational Application Developer (RAD) for version control.\n\n\u2022 Developed transformations using jobs like Filter, Join, Lookup, Merge, Hashed file, Aggregator, Transformer and Dataset.\n\n\u2022 Worked with internal architects and, assisting in the development of current and target state data architectures.\n\n\u2022 Coordinate with the business users in providing appropriate, effective and efficient way to design the new reporting needs based on the user with the existing functionality.\n\n\u2022 Remain knowledgeable in all areas of business operations to identify systems needs and requirements.\n\n\u2022 Document the complete process flow to describe program development, logic, testing, and implementation, application integration, coding.\n\nEnvironment:IBM Rational Clear Case & Clear Quest and IBM InfoSphere Metadata Workbench 8.7,IBMInfoSphereDataStage and Quality Stage, IBM InfoSphere CDC version 6.5.1, XML files.']",[u'Bachelor of Computer Science in TOOLS AND TECHNOLOGIES'],[u'Informatica Power Centre'],degree_1 : Bachelor of Compter Science in TOOLS AND TECHNOLOGIES
0,https://resumes.indeed.com/resume/e916eed1ff594ce3,"[u""Founder\nTBD - Sacramento, CA\nSeptember 2017 to Present\nI'm currently founding a non-profit that helps researchers use resources more efficiently, so they can focus on research, teaching, mentorship, and other priorities."", u'CTO\nERA Economics - Sacramento, CA\nNovember 2016 to November 2017\n- Guiding business decisions, choosing appropriate technical and analytic solutions\n- Creating internal systems for developing, managing, and analyzing proprietary data\n- Developing analytics products for clients, including commodity and risk reports', u'Software Engineer and Data Scientist\nSourceress - San Francisco, CA\nApril 2016 to November 2016\n- Created the foundational back- and front-end infrastructure for internal analytics\n- Developed innovative statistical and machine learning models, especially with NLP\n- Informed critical business decisions with internal dashboards and reports', u'Data Scientist and Software Engineer\nFarmers Business Network - San Francisco, CA\nApril 2014 to April 2016\n- Developed weather analytics services, with data pipelines, analytic models, and internal API\n- Developed mapping analytics services to clean, visualize, and communicate spatial insights\n- Led full-stack engineering teams to create high-value products and features', u'Research Scientist\nUC Davis and UW Madison - Davis, CA\nApril 2008 to December 2014\n- Collaborated with ~50 researchers around the world on ~15 papers and data products in < 5 years\n- Quantified ecosystem structure, dynamics, stability with mathematical models and network theory\n- Assessed ecosystem health under climate scenarios with numerical simulations and machine learning']","[u'PhD in Quantitative Ecology', u'BS in Economics', u'BS in Mathematics', u'Doctorate']","[u'UC Davis Davis, CA\nSeptember 2009 to December 2014', u'UW Madison Madison, WI\nAugust 2004 to May 2009', u'UW Madison Madison, WI\nAugust 2004 to May 2009', u'']","degree_1 : PhD in Qantitative Ecology, degree_2 :  BS in Economics, degree_3 :  BS in Mathematics, degree_4 :  Doctorate"
0,https://resumes.indeed.com/resume/5704e63ac2e161b8,"[u'Lecturer/Instructor\nUniversity of Massachusetts - Dartmouth, MA\nSeptember 2016 to Present\nTaught face-to-face and online courses in Quantitative Business Analysis, Business Analytics and Business Statistics using POM-QM for Windows and MS Excel.', u'Graduate Assistant\nRochester Institute of Technology - Rochester, NY\nSeptember 2015 to May 2016\n\u2022Supported research by applying statistical methodologies such as Sure Independence Screening, Regularization Paths for Penalized Regression, and Elastic-Net Regularized Generalized Linear Models using R.\n\u2022Judged Master\u2019s Level 3\u2013Minute Thesis competition at Rochester Institute of Technology, Rochester, NY.\napplication R as Graduate Assistant.', u'Data Analyst/Data Scientist\nGovernment of India - New Delhi, Delhi\nAugust 2010 to August 2015\nData Scientist / Member of Establishment Committee (Ministry of Defence) New Delhi, India\nEstablishment Committee under Ministry of Defence, Government of India is responsible for rationalizing the resource requirement of various integral units working under them.\n\u2022 Attended presentations and interacted with the team leaders of the various field units dealing with operations, manufacturing, maintenance, medical, finance, and supply chain.\n\u2022 Wrote algorithm in SAS, R and Python to import datasets from organization\u2019s data warehouse.\n\u2022 Used WHERE and GROUP BY Clause with Comparison and Logical Operators in SQL to Classify all the task into broad categories and Cluster integral units based on these task categories.\n\u2022 Validated, and analyzed data of the historical workload and current assignments of the various integral units of the organization by utilizing algorithm in SAS/Macros, SQL, Python and R.\n\u2022 Used Predictive Analytics and Factor Analysis to predict and modify the resource requirement of over 300 integral units based on algorithm in SAS, SQL, Python, and R.\n\u2022 Created Efficient Reports with Data Visualization utilizing SAS, R, Python, Minitab, and MS Excel to present complex analyses to diverse audiences.\n\uf0a7 This improved the process of assessment and recommendation of optimum resources to various integral units.\n\uf0a7 Achieved twice, the average result and saved about 5 million USD by reduction in resource requirement.\n\nStatistician/In charge, Pune, India\nResponsible for the training and administrative needs of 480 trainees, in a training Academy, India.\n\u2022 Created and altered table in SQL with complete details of all the trainees and using another table.\n\u2022 Populated, Updated, Inserted, Sorted and Joined tables in SQL using WHERE and ORDER BY Clause for various administrative needs and managed vendors for supply chain with required financial prudence and active market survey.\n\u2022 Used V-lookups and created Pivot Tables in MS Excel to identify and plan additional scientific training to meet the objective of the Academy.\n\u2022 Executed several projects including procurement and construction of prefabricated shelter and associated facilities for 120 trainees worth about 2 million USD.\n\u2022 Projected fund requirement with complete details for the financial year and procured of resources as per Defence Procurement Procedure of India for the successful training of the cadets.\n\uf0a7 Saved over 1 million USD by successfully completing all-round development of the cadets within stipulated timeframe.\nTeam Leader, Chandigarh, India\nResponsible for operational, maintenance, supply chain, finance and administrative needs of 200 staff.\n\u2022 Worked on IMMOLS for procurement and handled funds for maintenance, administrative, supply chain, and welfare of staff.']","[u'MS in Applied Statistics', u'Master of Aviation Management in Tripura', u'MS in Mathematics']","[u'Rochester Institute of Technology Rochester, NY\nJanuary 2014 to May 2017', u'ICFAI University\nDecember 2010 to December 2012', u'ICFAI University\nJanuary 2005 to January 2007']","degree_1 : MS in Applied Statistics, degree_2 :  Master of Aviation Management in Tripra, degree_3 :  MS in Mathematics"
0,https://resumes.indeed.com/resume/c4d00455d47ab57e,"[u'Data Scientist\nTHE BOEING COMPANY - Huntington Beach, CA\nJanuary 2014 to Present\nHelped establish and set up a new analytics team within the enterprise by creating a project intake process and providing visibility of analytics projects. Created a new portal for key supply chain metrics using ERP systems, which helped drive inventory reduction efforts by 18%. Developed recommendation systems using apriori algorithms for airplane features marketed to customers and engineering courses offered at Boeing. Collaborated in the analysis for airplane maintenance records by developing metrics that increased first time quality. Saved on engineering man hours by creating text classification algorithms which identified non-conformance issues. Led and co-hosted an enterprise R user\u2019s forum.', u'Engineering Selection Analyst\nKENWORTH TRUCK COMPANY - Kirkland, WA\nJanuary 2011 to January 2014\nCreated production files for existing and new engineering designs. Analyzed and interpreted engineering design drawings. Efficiently communicated project updates with leadership. Implemented VBA macros for data extraction and automated reporting. Analyzed truck chassis for appropriate selection of engineering data using boolean programming. Managed inter-divisional database for new product development. Generated system notifications to update engineering changes.', u'Wind Data Analyst\nDNV RENEWABLES, USA INC - Seattle, WA\nJanuary 2010 to January 2010\nPrepared periodic summary reports for the firm and clients. Used custom built software tools and statistical methods for data analysis. Provided project updates to clients and team members in a timely manner. Performed quality control testing and analysis for large-scale projects. Managed and updated databases with SQL queries. Contributed in the development of application tools for data analysis. Participated in power performance testing for potential wind turbine site locations.']","[u'Master of Science in Electrical Engineering in Communications Systems and Machine Learning', u'Graduate Certificate in Electrical Engineering', u'Bachelor of Science in Applied & Computational Mathematical Sciences']","[u'LOYOLA MARYMOUNT UNIVERSITY Los Angeles, CA\nJanuary 2019', u'UNIVERSITY OF WASHINGTON Bothell, WA\nJanuary 2016', u'UNIVERSITY OF WASHINGTON Seattle, WA\nJanuary 2011']","degree_1 : Master of Science in Electrical Engineering in Commnications Systems and Machine Learning, degree_2 :  Gradate Certificate in Electrical Engineering, degree_3 :  Bachelor of Science in Applied & Comptational Mathematical Sciences"
0,https://resumes.indeed.com/resume/caa1778bf34cf9d4,"[u'Senior Data Management Analyst/Data Scientist\nQuintilesIMS Inc - Arcadia, CA\nSeptember 2006 to Present']","[u'Master of Science in Statistics in Statistics', u'Bachelor of Science in Engineering in Engineering']","[u'Washington University St. Louis, MO\nAugust 2006', u'Wuhan University of Science & Technology Wuhan, CN']","degree_1 : Master of Science in Statistics in Statistics, degree_2 :  Bachelor of Science in Engineering in Engineering"
0,https://resumes.indeed.com/resume/03b218422965c8a9,"[u""Data Scientist\nCivilian - San Antonio, TX\nSeptember 2016 to Present\nTexas\nPresented analysis findings in written reports and visualized in charts and dashboards. Reports vary from Squadron level to Congressional.\nEfficiently designed, developed and maintained web-based applications written in different JavaScript frameworks such as Node, Vue, D3, Crossfilter and DC while\nusing SAS as back-end.\nRecognized for implementing new services without supervision, such as the framework to keep applications and dashboards synchronized while minimizing\ndata flow.\nPROJECTS\nActive on Kaggle, building and optimizing Machine Learning models built with\nScikit-learn, Keras, XGBoost and testing various ensembling/stacking methods.\nBuilding CNN/RNN image processing models.\nRen-Page.com uses various API such as Google's Firebase and Amazon EC2\ninstance as back-end in the News, while displays an example of data being filtered in an interactive app in the Data Page.\nMaintained and developed SAS Stored Processes to deliver web reports and visualize data in 8 different custom tailored Dashboards using agile cycles.\nImplemented interface that allows Analysts to analyze any combination of data\npoints from a DataSet and visualize the results if requested, allowing few clicks to access to a database instead of needing coding experience.""]",[u'Bachelor of Science in Computer Science'],"[u'The University of Texas at San Antonio San Antonio, TX\nJanuary 2016']",degree_1 : Bachelor of Science in Compter Science
0,https://resumes.indeed.com/resume/2c097b8c1d4ca5f3,"[u'Data Scientist-Mid\nBooz - Washington, DC\nJanuary 2017 to Present\nSoftware Developer (Python)\u2014Big Data, Data Science, and API development with a back-end development emphasis in Python 3.5, Django, and PostgreSQL development; utilizing command line and Postman application for testing and debugging. Iterative deliverable focus in agile software development utilizing GitHub for branches, merges, and pull requests.\n\nSoftware Developer (Java)\u2014Big Data and Micro-Services project with a back-end development emphasis in Java utilizing Apache Ecosystem Products (Kafka, Maven, Tomcat, Ant, Zookeeper, Spark, Swarm), and DevOps functionality (bash scripting, Docker, and Continuous Integration with Jenkins).', u'Owner\nCryptAthlos, LLC - Washington, DC\nOctober 2016 to Present\nOwner\u2014Applied my passions of software development, hardware configuration, networking, finance, and sports to build on current evolving technologies.\nCryptocurrency Mining\u2014Designing, building, deploying, operating, and maintaining AMD based graphics card mining rigs locally (Windows 10 & Ubuntu) and remotely (LAMP Stack).\nBlockchain & Smart Contracts\u2014Software development emphasis in python, solidity, truffle, Webpack, web3.js, Go, C++, and/or compiling from source in a Linux (Ubuntu) based environment.\nMachine Learning & Predictive Analytics\u2014Software development emphasis in python and related packages (SciKit Learn, NLTK, NumPy, Pandas, Matplotlib) to perform statistical analysis on various sports and financial related topics.', u'Data Scientist\nDeepBD, INC - Fairfax, VA\nMay 2016 to August 2016\n\u2022 Implement machine learning algorithms in Python, building natural language processing systems.\n\u2022 Collect, track, and integrate multiple sources of big data.\n\u2022 Design, develop, test and deploy in AWS architecture.\n\u2022 Query databases and perform statistical analysis.\n\u2022 Develop and program databases in MSSQL and Elasticsearch.\n\u2022 Develop working prototypes of algorithms.\n\u2022 Evaluate the performance of various algorithms/models/strategies based on the real world data sets.\n\u2022 Oversee the process of implementing algorithms and models in production.', u'Financial Business Analyst\nFEDERAL AVIATION ADMINISTRATION - Washington, DC\nMay 2014 to May 2016\n\u2022 As the lead Cost Control Analyst, I\u2019ve overseen and monitored multiple business units to ensure fiscal year estimates of savings and cost avoidance are met\u2014with an average annual cost savings of over $90 Million.\n\u2022 Lead developer of user fees and their subsequent rates structures for the Pilot Record Database.\n\u2022 Apply data analysis techniques through Excel, Python, and R programming analyzing large data sets and visualization for presentations.', u'Accounting Analyst\nNATIONAL OCEANIC & ATMOSPHERIC ADMINISTRATION - Seattle, WA\nJanuary 2010 to February 2013\n\u2022 Provided budgetary, auditing, and financial management support by maintaining a database of: Purchase Orders, BPAs, MOA(s), MOU(s), Leases, and Utilities; and processing transactions through a proprietary government software program and an online certification system.', u'Forex, Futures, Options Trader\nINVESTABULL CAPITAL - Chicago, IL\nAugust 2008 to December 2009\n\u2022 Started my own investment and proprietary trading shop with a futures and options market focus on grains, energies, and softs; as well as, foreign currency (FOREX) trading.\n\u2022 Used computer applications and trading software to relate concepts to practice and execution.', u'Financial Analyst\nSTARCOM MEDIAVEST GROUP - Bannockburn, IL\nMarch 2007 to August 2008\n\u2022 Lead account receivables financial analyst responsible for presenting analysis and reports to CEO, VP of Finance, and Account Directors on statistical data relevant to vendor and publisher advertising.', u'Defined Contributions Analyst\nING - Des Moines, IA\nAugust 2006 to March 2007\n\u2022 Series 6 registered rep working with mutual funds and variable annuity contracts--assisting clients with defined contribution benefit plans (i.e., 401(k), 403(b), 457, IRA); while working with Investor Services in the retention of over $3,000,000 of benefit funds and assets.']","[u'Certificate in Data Science', u'Bachelor of Science in Finance, Mathematics']","[u'GEORGETOWN UNIVERSITY\nJanuary 2016 to January 2016', u'IOWA STATE UNIVERSITY - COLLEGE OF BUSINESS Ames, IA\nJanuary 2002 to January 2006']","degree_1 : Certificate in Data Science, degree_2 :  Bachelor of Science in Finance, degree_3 :  Mathematics"
0,https://resumes.indeed.com/resume/9b4d2599216dfa72,"[u'Data Scientist Co-op\nCharles River Laboratories Inc - Wilmington, MA\nJanuary 2017 to August 2017\nConducted exploratory statistical analysis and designed time-series; ARIMA, Holt Winters, ETS as well\nas regression algorithms to predict revenue/sales 12 months into the future with accuracy up to 91.07%.\n\u2212 Designed Long Short Term Memory (LSTM) Deep Learning models using Recurrent Neural Networks\n(RNN) to increase prediction accuracy for sales and revenue by 4-5%.\n\u2212 Reduced 70% training time by coding optimized R, Python and SQL scripts to automate the process of fetching data in R environment, building Artificial Intelligence models, picking the best one, and pushing\npredictions back to SQL Server database for over 900 scenarios.\n\u2212 Generated descriptive and predictive Power Bi reports based on the analysis.\n\u2212 Mined HR data to predict attrition using Decision Trees, Bayesian techniques and Logistic Regression.\nAPPLIED PROJECTS\nMachine Learning, Northeastern University\n\u2212 Image Classification: Built a multi-class Deep ANN image classifier to learn the Signs dataset using the Tensorflow framework and achieved 84% accuracy. Implemented a ResNet (CNN) model using Keras and\nincreased its accuracy to 86.8%.\n\u2212 Object Detection using YOLO: Developed algorithms to detect cars, buses, pedestrians, etc. using You\nOnly Look Once model.\n\u2212 Anomaly Detection (Unsupervised Learning): Identified credit card frauds using Self Organizing Maps\nand used Artificial Neural Network to predict the probability of a customer being fraudulent.\n\u2212 Binary Classification: Built Logistic Regression, Na\xefve Bayes, Kernel SVM, Random Forest and XGBoost models to predict safe/risky loans using LendingClub dataset.\n\u2212 CTR Optimization: Performed advertisement CTR optimization using Thompson Sampling and Upper\nConfidence bound.\n\u2212 Recommendation Systems: Built a movie recommendation Boltzman Machine that predicts binary\nratings and AutoEncoder that predicts numerical ratings (1-5) using Pytorch using Movielens dataset.\n\u2212 Operations Research: Mathematically programmed several problems in scheduling, transportation and assignment, network flows, and supply chain optimization using Lingo, Matlab.\n\u2212 Visualizations: Matplotlib, Plotly, ggplots, Tableau, QlikView.\nBig Data Analysis, Northeastern University\n\u2212 AWS and Azure: Analyzed data using AWS tools like Redshift, Aurora, Athena, and Azure ML.\n\u2212 PySpark: Built a music recommendation engine using MLlib in PySpark.\n\u2212 Hive: Explored Twitter feed stored in JSON for Presidential elections using.\nDatabase Design and Data Integration, Northeastern University\n\u2212 Captured sales and purchase data of an enterprise from MS Sql Server, MySql, Oracle, PostgreSQL, and flat files to a target database using Talend and SSIS to improve analytical performance statistics.']","[u'Master of Science in Engineering Management in Engineering Management', u'Bachelor of Mechanical Engineering in Mechanical Engineering']","[u'Northeastern University Boston, MA\nDecember 2017', u'University of Mumbai Mumbai, Maharashtra\nMay 2014']","degree_1 : Master of Science in Engineering Management in Engineering Management, degree_2 :  Bachelor of Mechanical Engineering in Mechanical Engineering"
0,https://resumes.indeed.com/resume/d930285b1f5e1520,"[u'Data Scientist Intern\nIntegrated Musculoskeletal Care - Tallahassee, FL\nNovember 2017 to Present\n\u2022 Program extensively with R to write duplicable scripts, run statistical regressions, & wrangle data\n\u2022 Use supervised and unsupervised learning such as clustering & decision tree-based estimations\n\u2022 Innovate new ways to express relationships with large healthcare & physical-therapy data', u'Private Tutor\nFlorida State University - Tallahassee, FL\nAugust 2017 to Present\n-Tutor econometrics, Analysis of Economic Data, and Math courses to university and high school students', u'Associate Financial Representative\nNorthwestern Mutual - Provo, UT\nMay 2016 to July 2017\n\u2022 Prospected for & financially advised new clients in a fiduciary capacity\n\u2022 Continued education in the field of finance to increase licenses and qualifications\n\u2022 Aggregated & presented insurance & investment packages to clients\n\u2022 Learned extensively about the healthcare industry becoming familiar with common issues', u""Research Assistant\nUVU Finance & Economics Department - Orem, UT\nSeptember 2016 to May 2017\n\u2022 Ran regressions/tests & interpreted results to assist in academic research of faculty\n\u2022 Assisted in operations of department's scholarly journal including research and source-verification\n\u2022 Worked as a Private Tutor in the Finance & Economic department, with focus on math""]","[u'MS in Applied Economics', u""Bachelor's in Business Management""]","[u'Florida State University Tallahassee, FL\nAugust 2017 to July 2018', u'Utah Valley University Orem, UT\nDecember 2013 to April 2016']","degree_1 : MS in Applied Economics, degree_2 :  ""Bachelors in Bsiness Management"""
0,https://resumes.indeed.com/resume/775a0bbb259b01d6,"[u'Data Scientist Co-op\nAhold Delhaize - Quincy, MA\nJune 2017 to December 2017\n\u2022 Modelled a backend rule-based recommendation engine using Azure studio for healthy eating choices for Stop & Shop.\n\u2022 Implemented a graphical database in Neo4j for suggesting recipes based on the market basket of customers.\n\u2022 Created and developed a benchmarking framework to track the accuracy and error of a system that includes hundreds\nof individual predictive models.\n\u2022 Created a script in Node.js to automate image scraping for Unique Product Code(UPC) from Item master API.\n\u2022 Visualized data gathered by Computer Vision application prototype to comprehend and analyze hot spots in the store\nusing Tableau and Power BI.\n\u2022 Ideated and developed an alerting system for proactive sales monitoring using transaction logs on hive, hue and oozie workflow.\n\u2022 Predictive modeling for content demand using a Python-based machine learning framework.']","[u'Master of Science in Information Systems', u'Graduate Certificate in Data Mining Engineering', u'Bachelor of Engineering in Electronics & Communications']","[u'Northeastern University Boston, MA\nJanuary 2016 to May 2018', u'Northeastern University Boston, MA\nJanuary 2016 to December 2017', u'College of Engineering Guindy Anna University Chennai, Tamil Nadu\nAugust 2010 to April 2014']","degree_1 : Master of Science in Information Systems, degree_2 :  Gradate Certificate in Data Mining Engineering, degree_3 :  Bachelor of Engineering in Electronics & Commnications"
0,https://resumes.indeed.com/resume/572f51a2261ba440,"[u'Lead Data Scientist\nVerisk Analytics\nDecember 2016 to Present\nLead efforts to create new products surrounding Micro-Territories, Commercial Auto Symbols, Liability Frequency Trend projection as well as providing actionable recommendations for updating key assets by using ML algorithms to identify which data assets will produce the most revenue. My role has involved management of, and participation on, a small team (3 - 5). I have been heavily involved in every phase from data preparation (cleaning, merging, feature engineering, etc) to creating the final predictive models.\nMost recently, I have conceived of and spearheaded the creation of a Property Vacancy Detection product with a projected revenue of $5M-8M. I was responsible for ideation, data acquisition, data base creation, feature engineering, proof of concept using out of time hold out data, and pitching the product to senior management. Expected product release date: June, 2018.\nTools used include Machine Learning, Spark, Hadoop, H2O, R, SAS, SQL, Python, Gradient Boosted Trees, Random Forest, Decision Trees, GLMs (Tweedie, Elastic Net, LASSO, Ridge)', u'Data Scientist\nVerisk Analytics\nAugust 2015 to December 2016', u""Assistant Manager\nLeah's Outdoor Catering\nMay 2014 to August 2015\nSupervised and worked with a team of chefs and caterers. Oversaw employee/customer interactions as well as food production"", u'Scientist\nMcGill University - Montr\xe9al, QC\nJanuary 2008 to June 2015\nLead researcher on three major projects where I designed, conducted, and analyzed output of experiments while earning my PhD. Major accomplishments include: 1) using large scale numerical simulations on a high performance computing cluster to discover and understand how different types of variation and assumptions on the nature of variation affect model predictions. 2) Using data extraction techniques to Scrape and Structure Unstructured empirical data sets that relate resource density to consumption rate. Then using model selection methodologies (AIC) I compared non-linear regression scores of the previously accepted model to novel models that I created and showed that the novel models outperform the previously accepted model. Additionally, the novel models are able to resolve two long standing controversies in theoretical biology. Tools used include Numerical Simulation, Distributed (Cloud) Computing, Python (PyCont, NumPy, SciPy), MatLab, R, Unix, SAS, Statistical Analyses (ANOVA, ANCOVA, Linear Regression, Non-Linear Regression, Akaike Information Criteria, Bayesian Information Criteria)', u'Teaching Assistant\nMcGill University\nJanuary 2008 to January 2013\nCommunicated complex ideas to small and large groups of undergraduate students. Taught undergraduates coding languages such as MATLAB.']","[u'Ph.D. in Computational/Mathematical Biology', u'Certificate in Environmental Geomatics', u'']","[u'McGill University\nMarch 2015', u'Rutgers University (New Brunswick New Jersey, NB\nMay 2008', u'Rutgers University New Brunswick, NJ\nApril 2000']","degree_1 : Ph.D. in Comptational/Mathematical Biology, degree_2 :  Certificate in Environmental Geomatics, degree_3 :  "
0,https://resumes.indeed.com/resume/c51c872cde57387c,"[u'Data Scientist\nPfizer, Inc - Cambridge, MA\nJuly 2017 to Present\n\u2022 Supporting production operations and ""First-line"" technical support for the Prevnar vaccine manufacturing process.\n\u2022 Process step owner for seed/production fermenters and UF/DF unit operations.\n\u2022 Supporting and leading investigations, change controls (have led 1), PHP improvements, and CAPAs', u'Technical Scientist/Engineer\nPfizer, Inc - Andover, MA\nAugust 2014 to July 2017\n\u2022 Leveraging manufacturing data to support change controls, work processes, and continuous improvement efforts.\n\u2022 Process step owner for seed/production fermenters and UF/DF unit operations.\n\u2022 Providing technical support for the Prevnar vaccine manufacturing process.', u'Associate Data Analyst Contractor\nPfizer, Inc - Andover, MA\nMay 2012 to May 2014\nParticipated on cross-functional teams for multivariate analysis to support investigations, site Mission Award\nRecipient.\n\u2022 Built VBA-based platform to automate performance monitoring of the site chromatography operations by linking data from company historian.\n\u2022 Managed and supported multiple projects to introduce new analytical technologies (PAT) to manufacturing processes to provide cost reductions and maintain quality assurance.', u'Undergraduate Researcher\nUMass Lowell Biomanufacturing Center - Lowell, MA\nNovember 2011 to March 2012\n\u2022 Developed metabolic flux analysis platform for DG44 CHO cell growth modeling in Matlab.\n\u2022 Monitored and accessed cell growth using Cedex Hi-Res cytometer and other analytical lab equipment.', u'Undergraduate Researcher\nNortheastern University - Boston, MA\nMay 2010 to August 2010\n\u2022 Designed of experiments to optimize carbon nanotube deposition across electrodes.\n\u2022 Collaborated closely with Ph.D. student on development of a nanobiosensor for pathogen detection.', u'Undergraduate Tutor\nUMass Lowell Center for Learning - Boston, MA\nJanuary 2009 to March 2010\nTutored undergraduate students in calculus, chemistry, physics, and biology', u""Engineering Intern\nEndodynamix - Salem, MA\nApril 2009 to August 2009\nOperated fabrication machines to assist in manufacturing including Bridgeport, band saws, high pressure presses,\ngrinders, Instron, CNC, and various other pieces of equipment.\n\u2022 Tested and validated final products to ensure they meet specifications and tolerances.\n\nAcademic Honors and Activities\n\u2713 Full Merit Scholarship as Commonwealth Scholar \u2713 Continuous Dean's List Recipient\n\u2713 Recipient of the Chemical Engineering Department \u2713 Who's Who at American Universities (2012)\nScholarship Award (only recipient, merit-based) \u2713 AIChe Club Member""]","[u'Masters of Sciences in Bioinformatics', u'Bachelor of Science in Chemical Engineering']","[u'Tufts University Medford, MA\nMay 2018', u'University of Massachusetts Lowell, MA\nMay 2013']","degree_1 : Masters of Sciences in Bioinformatics, degree_2 :  Bachelor of Science in Chemical Engineering"
0,https://resumes.indeed.com/resume/dd13a17a23545bc5,"[u'Data Scientist\nBloomberg - New York, NY\nJanuary 2016 to Present\n\u2022 Generating statistical features, training data, and random forest model for predicting performance of FX time series products with software engineers.\n\u2022 Developed ETL processes using python, luigi, pyspark, multiprocessing, and cron jobs in a dev environment.\n\u2022 Developed time series algorithms for products that are currently in production based on Kalman filtering.\n\u2022 Partnering with product and engineering to change environment from reactive to data-driven predictive.\n\u2022 Performed analysis on Bloomberg and its main competitor, results of which I directly reported to an executive.', u'Fellow\nNew York City Data Science Academy - New York, NY\nApril 2016 to July 2016\n\u2022 Proficient in Python, R, and SQL for analytics, data manipulation, predictive models, and data visualization.\n\u2022 Supervised and unsupervised learning techniques such as regression, tree based, SVM, and PCA models.\n\u2022 Projects:\n\u2022 AirBnB new user behavior predicted using demographics and web browsing history.\n\u2022 Allrecipes.com web scraping to discover food trends.\n\u2022 R Shiny app examining the correlation between changing demographics and US obesity rates.\n\u2022 Higgs Boson Kaggle competition.', u'Process Technology Development Engineer\nMicrochip Technology Inc - Gresham, OR\nMarch 2015 to March 2016\n\u2022 Developed new plasma-etch processes for technologies that will generate over $1MM in cost savings.\n\u2022 Performed design of experiments and determined the statistical significance of parameters on new processes.\n\u2022 Conducted research on a novel method that resulted in a patent.', u'Graduate Research Assistant\nColumbia University and University of Delaware - New York, NY\nNovember 2009 to February 2015\n\u2022 Contributed research that resulted in three national conference presentations and 10 peer-reviewed publications (first author on four) that have been cited over 800 times by other publications.\n\u2022 Collected, cleaned, and fitted over 1 GB of data to linear regression and Gaussian models to identify trends in the activity and stability of metals for electrochemical reactions.\n\u2022 Mentored undergraduate and graduate students\u2019 projects.']",[u'Doctorate'],[u''],degree_1 : Doctorate
0,https://resumes.indeed.com/resume/f090aabcdfa6e1ce,"[u""Data Scientist\nVerizon - Irving, TX\nJanuary 2017 to Present\nDescription: Verizon Communications Inc. ( About this sound listen (help\u2022info)) (/v\u0259\u02c8ra\u026az\u0259n/ v\u0259-RY-z\u0259n), commonly known as Verizon, is an American multinational telecommunications conglomerate and a corporate component of the Dow Jones Industrial Average\n\nResponsibilities:\n\u2022 Worked as Data Architects and IT Architects to understand the movement of data and its storage and ER Studio 9.7\n\u2022 Participated in all phases of datamining; data collection, data cleaning, developing models, validation, visualization and performed Gapanalysis.\n\u2022 Data Manipulation and Aggregation from a different source using Nexus, Toad, Business Objects, PowerBI and Smart View.\n\u2022 Implemented Agile Methodology for building an internal application.\n\u2022 Focus on integration overlap and Informatica newer commitment to MDM with the acquisition of Identity Systems.\n\u2022 Good knowledge of Hadoop Architecture and various components such as HDFS, JobTracker, TaskTracker, Name Node, Data Node, Secondary Name Node, and MapReduce concepts.\n\u2022 As Architect delivered various complex OLAP databases/cubes, scorecards, dashboards and reports.\n\u2022 Setup storage and data analysis tools in Amazon Web Services cloud computing infrastructure.\n\u2022 Used pandas, numpy, Seaborn, scipy, matplotlib, sci-kit-learn, NLTK in Python for developing various machine learning algorithms.\n\u2022 Installed and used Caffe Deep Learning Framework\n\u2022 Worked on different data formats such as JSON, XML and performed machine learning algorithms in Python.\n\u2022 Programmed a utility in Python that used multiple packages (scipy, numpy, pandas)\n\u2022 Implemented Classification using supervised algorithms like Logistic Regression, Decision trees, KNN, Na\xefve Bayes.\n\u2022 Independently coded new programs and designed Tables to load and test the program effectively for the given POC's using with BigData/Hadoop.\n\u2022 Designed data models and data flow diagrams using Erwin and MSVisio.\n\u2022 Utilized ADO.Net Object Model to implement middle-tier components that interacted with MSSQL Server 2000database.\n\u2022 Participated in AMS (Alert Management System) JAVA and SYBASE project. Designed SYBASE database utilizing ERWIN. Customized error messages utilizing SP_ADDMESSAGE and SP_BINDMSG. Created indexes, made query optimizations. Wrote stored procedures, triggers utilizing T-SQL.\n\u2022 Launching Amazon EC2 Cloud Instances using Amazon Images (Linux/ Ubuntu) and Configuring launched instances with respect to specific applications.\n\u2022 Exported the result set from Hive to MySQL using Sqoop after processing the data.\n\nEnvironment: R 9.0, Informatica 9.0, ODS, OLTP, Oracle 10g, Hive, OLAP, DB2, Metadata, MS Excel, Mainframes MS Vision, Rational Rose."", u""Data Scientist\nExpeed Software, Colombus, OH\nNovember 2015 to December 2016\nDescription: Our Software Engineering division includes UX/UI designers, solution architects and application developer. Additionally, we have an R&D team that spends a significant amount of time exploring new technologies and processes such as DevOps, Continuous Integration and Continuous Deployment/Delivery.\n\nResponsibilities:\n\u2022 Applied Lean Six Sigma process improvement in plant and developed Capacity Calculation systems using purchase order tracking system and improvement inbound efficiency by 23.56%.\n\u2022 Worked with Machine learning algorithms like Linear Regressions (linear, logistic etc.) SVMs, Decision trees for classification of groups and analyzing most significant variables such as FTE, Waiting times of purchase orders and Capacities available and applied process improvement techniques.\n\u2022 And calculated Process Cycle efficiency of 33.2% and identified value added and non-value added activities\n\u2022 And utilized SAS for developing Pareto Chart for identifying highly impacting categories in modules to find the work force distribution and created various data visualization charts.\n\u2022 Performed univariate, bivariate and multivariate analysis of approx. 4890 tuples using bar charts, box plots and histograms.\n\u2022 Participated in features engineering such as feature creating, feature scaling and One-Hot encoding with Scikit-learn.\n\u2022 Converted raw data to processed data by merging, finding outliers, errors, trends, missing values and distributions in the data.\n\u2022 Generated detailed report after validating the graphs using R, and adjusting the variables to fit the model.\n\u2022 Worked on Clustering and factor analysis for classification of data using machine learning algorithms.\n\u2022 Developed Descriptive statistics and inferential statistics for Logistics optimization, Average hours per job, Value throughput data to at 95% confidence interval.\n\u2022 Written Map Reduce code to process and parsing the data from various sources and storing parsed data into HBase and Hive using HBase - Hive Integration.\n\u2022 Created SQL tables with referential integrity and developed advanced queries using stored procedures and functions using SQL server management studio.\n\u2022 Used Pandas, NumPy, seaborn, SciPy, Matplotlib, Scikit-learn, NLTK in Python for developing various machine learning algorithms and utilized machine learning algorithms such as linear regression, multivariate regression, naive Bayes, Random Forests, K-means, & KNN for data analysis.\n\u2022 Used packages like dplyr, tidyr & ggplot2 in R Studio for data visualization and generated scatter plot and high low graph to identify relation between different variables.\n\u2022 Worked on Business forecasting, segmentation analysis and Data mining and prepared management reports defining the problem; documenting the analysis and recommending courses of action to determine the best outcomes.\n\u2022 Worked on various Statistical models like DOE, hypothesis testing, Survey testing and queuing theory.\n\u2022 Experience with risk analysis, root cause analysis, cluster analysis, correlation and optimization and K-means algorithm for clustering data into groups.\n\u2022 Coordinate with data scientists and senior technical staff to identify client's needs and document assumptions.\n\nEnvironment: SQL Server 2012, Jupyter, R 3.1.2, Python, MATLAB, SSRS, SSIS, SSAS, MongoDB, HBase, HDFS, Hive, Pig, Microsoft office, SQL Server Management Studio, Business Intelligence Development Studio, MS Access."", u'Data Science\nCity of New Orleans - New Orleans, LA\nJanuary 2014 to October 2015\nDescription: New Orleans is a Louisiana city on the Mississippi River, near the Gulf of Mexico. Nicknamed the ""Big Easy,"" it\'s known for its round-the-clock nightlife, vibrant live-music scene and spicy, singular cuisine reflecting its history as a melting pot of French, African and American cultures. Embodying its festive spirit is Mardi Gras, the late-winter carnival famed for raucous costumed parades and street parties.\n\nResponsibilities:\n\u2022 Gathered, analyzed, documented and translated application requirements into data models and Supports standardization of documentation and the adoption of standards and practices related to data and applications.\n\u2022 Participated in Data Acquisition with Data Engineer team to extract historical and real-time data by using Sqoop, Pig, Flume, Hive, Map Reduce and HDFS.\n\u2022 Wrote user defined functions (UDFs) in Hive to manipulate strings, dates and other data.\n\u2022 Performed Data Cleaning, features scaling, features engineering using pandas and numpy packages in python.\n\u2022 Applied clustering algorithms i.e. Hierarchical, K-means using Scikit and Scipy.\n\u2022 Performs complex pattern recognition of automotive time series data and forecast demand through the ARMA and ARIMA models and exponential smoothening for multivariate time series data.\n\u2022 Delivered and communicated research results, recommendations, opportunities to the managerial and executive teams, and implemented the techniques for priority projects.\n\u2022 Designed, developed and maintained daily and monthly summary, trending and benchmark reports repository in Tableau Desktop.\n\u2022 Generated complex calculated fields and parameters, toggled and global filters, dynamic sets, groups, actions, custom color palettes, statistical analysis to meet business requirements.\n\u2022 Implemented visualizations and views like combo charts, stacked bar charts, pareto charts, donut charts, geographic maps, spark lines, crosstabs etc.\n\u2022 Published workbooks and extract data sources to Tableau Server, implemented row-level security and scheduled automatic extract refresh.\n\nEnvironment: Machine learning(KNN, Clustering, Regressions, Random Forest, SVM,Ensemble), Linux, Python 2.x (Scikit-Learn/Scipy/Numpy/Pandas), R, Tableau (Desktop 8.x/Server 8.x), Hadoop, Map Reduce,HDFS, Hive, Pig, HBase,Sqoop, Flume,Oracle 11g, SQL Server 2012', u'Data Analytics\nQuality Systems Inc - Irvine, CA\nOctober 2012 to December 2013\nDescription: Quality Systems Inc. develops industry-leading health IT software for medical and dental clients. Get to know our brands including NextGen Healthcare, QSIDental, and Mirth.\n\nResponsibilities:\n\u2022 Good grip on Cloudera and HDP ecosystem components.\n\u2022 Used Elastic Search (Big Data) to retrieve data into the application as required.\n\u2022 Performed Map Reduce Programs those are running on the cluster.\n\u2022 Developed multiple MapReduce jobs in java for data cleaning and preprocessing.\n\u2022 Analyzed the partitioned and bucketed data and compute various metrics for reporting.\n\u2022 Involved in loading data from RDBMS and weblogs into HDFS using Sqoop and Flume.\n\u2022 Worked with large amounts of structured and unstructured data.\n\u2022 Knowledge of Machine Learning concepts (Generalized Linear models, Regularization, Random Forest, Time Series models, etc.)\n\u2022 Worked in Business Intelligence tools and visualization tools such as Business Objects, Tableau, ChartIO, etc.\n\u2022 Deployed GUI pages by using JSP, JSTL, HTML, DHTML, XHTML, CSS, JavaScript, and AJAX.\n\u2022 Configured the project on WebSphere 6.1 application servers\n\u2022 Implemented the online application by using Core Java, JDBC, JSP, Servlets and EJB 1.1, Web Services, SOAP, WSDL.\n\u2022 Worked on loading the data from MySQL to HBase where necessary using Sqoop.\n\u2022 Developed Hive queries for Analysis across different banners.\n\u2022 Extracted data from Twitter using Java and Twitter API. Parsed JSON formatted twitter data and uploaded to the database.\n\u2022 Handled end-to-end project from data discovery to model deployment.\n\u2022 Monitoring the automated loading processes.\n\u2022 Communicated with other Health Care info by using Web Services with the help of SOAP, WSDL JAX-RPC\n\u2022 Used Singleton, factory design pattern, DAO Design Patterns based on the application requirements\n\u2022 Used SAX and DOM parsers to parse the raw XML documents\n\u2022 Used RAD as Development IDE for web applications.\n\u2022 Preparing and executing Unit test cases\n\nEnvironment: SQL/Server, Oracle, MS-Office, Teradata, Informatica, ER Studio, XML, Business Objects, HDFS, Teradata 14.1, JSON, HADOOP (HDFS), MapReduce, PIG, Spark, R Studio, MAHOUT, JAVA, HIVE,AWS.', u'Data Analyst\nCigna - Hyderabad, Telangana\nJanuary 2011 to September 2012\nDescription: Cigna is an American worldwide health services organization based in suburban Bloomfield, Connecticut. Its insurance subsidiaries are major providers of medical, dental, disability, life and accident insurance and related products and services.\nResponsibilities:\n\n\u2022 Developed and implemented predictive models using Natural Language Processing Techniques and machine learning algorithms such as linear regression, classification, multivariate regression, Naive Bayes, Random Forests, K-means clustering, KNN, PCA and regularization for data analysis.\n\u2022 Designed and developed Natural Language Processing models for sentiment analysis.\n\u2022 Developed visualizations and dashboards using ggplot, Tableau.\n\u2022 Worked on development of data warehouse, Data Lake and ETL systems using relational and non relational tools like SQL, No SQL.\n\u2022 Built and analyzed datasets using R, SAS, Matlab and Python (in decreasing order of usage).\n\u2022 Participated in all phases of data mining; data collection, data cleaning, developingmodels, validation, visualization and performed Gapanalysis.\n\u2022 Data Manipulation and Aggregation from different source using Nexus, Toad, Business Objects, Power BI and SmartView.\n\u2022 Implemented Agile Methodology for building an internal application.\n\u2022 Good knowledge of Hadoop Architecture and various components such as HDFS, Job Tracker, Task Tracker, Name Node, Data Node, Secondary Name Node, and MapReduce concepts.\n\u2022 Implemented Classification using supervised algorithms like Logistic Regression, Decisiontrees, KNN, Na\xefve Bayes.\n\u2022 Used Teradata15 utilities such as Fast Export, MLOAD for handling various tasks data migration/ETL from OLTP Source Systems to OLAP Target Systems\n\u2022 Maintenance in the testing team for System testing/Integration/UAT.\n\u2022 Involved in preparation & design of technical documents like Bus Matrix Document, PPDM Model, and LDM & PDM.\n\u2022 Understanding the client business problems and analyzing the data by using appropriate Statistical models to generate insights.\n\nEnvironment: R 3.0, Erwin 9.5, Tableau 8.0, MDM, QlikView, ML Lib, PL/SQL, HDFS, Teradata 14.1, JSON, HADOOP (HDFS), MapReduce, PIG, Spark, R Studio, MAHOUT, JAVA, HIVE, AWS.', u'Data Analyst\nHexaware Technologies - Hyderabad, Telangana\nJune 2009 to December 2010\nDescrption: Hexaware Technologies Limited (HTL) (BSE: 532129, NSE: HEXT) is an information technology and business process outsourcing service provider company based in Navi Mumbai, India. Founded in 1990. The company provides software services in Banking and Financial services.\nResponsibilities:\n\n\u2022 Identifying what factors could influence the overall satisfaction of consumers. Range (1-5).\n\u2022 Considered the SMG (Service Management Group) database survey results in analyzing the impact on overall customer satisfaction.\n\u2022 Analyzed business process workflows and assisted in the development of ETL procedures for mapping data from source to target systems\n\u2022 We used Ordinal logistic regression methodology in explaining the importance of features.\n\u2022 The analysis involved predicting the overall satisfaction - ordinal rating, by analyzing the impact of each independent factors in explaining the output.\n\u2022 Packages used: MASS package, plot function for Ordinal logistics regression model.\n\nEnvironment: R Studio, SQL Server, Dplyr, Tidyr, ggplot2, Tableau, MASS package - plot function']","[u""Bachelor's""]",[u''],"degree_1 : ""Bachelors"""
0,https://resumes.indeed.com/resume/a834f22bf29502a2,"[u""Machine Learning/Data Scientist\nSAMSUNG - MOUNTAINVIEW, CA, US\nFebruary 2017 to Present\nDescription: Samsung Group is a South Korean multinational conglomerate headquartered in Samsung Town, Seoul. It comprises numerous affiliated businesses; most of them united under the Samsung brand, and is the largest South Korean chaebol (business conglomerate).\n\nResponsibilities:\n\u2022 Design and develop state-of-the-art deep-learning / machine-learning algorithms for analysing the image and video data among others.\n\u2022 Develop and implement innovative AI and machine learning tools that will be used in the Risk.\n\u2022 Counter intuitive predictors were identified using machine-learning methods.\n\u2022 Liaise with functional lead to understand and clarify meaning and impact of key data variables.\n\u2022 Effective software development processes to customize and extend the computer vision and image processing techniques to solve new problems for Automation Anywhere.\n\u2022 Develop and implement innovative data quality improvement tools.\n\u2022 Descriptive statistics, predictive models like ARIMA for forecasting the prices and GARCH for forecasting variance to know the price volatility.\n\u2022 Open Source Frameworks: Apache Hadoop, Hive, Spark, TensorFlow, Spark Mllib.\n\u2022 Experimented and applied various Data science algorithms like regression, classification, KNN and clustering to create decide and create models for solving various business requirements.\n\u2022 Will demonstrate cross-functional resource interaction to accomplish your goals.\n\u2022 Experience with deep learning LSTM and RNN based speech recognition using TensorFlow.\n\u2022 Analyse a large amount of data classify data.\n\u2022 Visualize data using D3.js.\n\u2022 Create a model for forecast revenue.\n\u2022 Used Teradata utilities such as Fast Export, MLOAD for handling various tasks data migration/ETL from OLTP Source Systems to OLAP Target Systems.\n\u2022 Performed data analysis by using Hive to retrieve the data from Hadoop cluster, Sql to retrieve datafrom Oracle database and used ETL for data transformation.\n\u2022 ML performance a deep analysis of the HTPD/RTPD/LTPD test data to define a model of FBC growth rate across the temperature.\n\u2022 MLmodels for projection pre-production SLC, MLC, TLC single and multi-die packages ICC memory.\n\u2022 Used TensorFlow library in dual GPU environment for training and testing of the Neural Networks\n\u2022 Co-authored two manuscripts; they are under review for two high impact journals.\n\u2022 Develop project requirements and deliverable timelines; execute efficiently to meet the plan timelines.\n\u2022 Creating and supporting a data management workflow from data collection, storage, and analysis to training and validation.\n\u2022 Predict future sales value and volume of a brand/item using ARIMA model in SAS/R.\n\u2022 Develop necessary connectors to plug ML software into wider data pipeline architectures.\n\u2022 Creating andsupporting a data management workflow from data collection, storage, and analysis to training and validation.\n\u2022 Identify and assess available machine learning and statistical analysis libraries (including regressors, classifiers, statistical tests, and clustering algorithms).\n\u2022 Worked Hands on with ETL process.\n\u2022 Design and build scalable software architecture to enable real-time / big-data processing.\n\u2022 Acquire business knowledge in the Firm's risk management processes.\n\u2022 Be very passionate about quality and have a strong sense of ownership of the work accomplished.\n\u2022 Be quick to learn new technologies as well as deliver on them in short order.\n\u2022 Taking responsibility for technical problem solving, creatively meeting product objectives and developing best practices.\n\u2022 Have a high sense of urgency to deliver projects as well as troubleshoot and fix data queries/ issues.\n\u2022 Work independently with R&D partners to understand requirements.\n\nEnvironment: R 9.0, R Studio, Machine learning, Informatic a 9.0, Scala, Spark, Cassandra, DL4J, ND4J, Ml, DL,Scikit-learn, Shogun, Accord Framework/AForge.net, Mahout, Data Warehouse,MLlib, H2O ,Cloudera Oryx, GoLearn, Apache."", u""Machine Learning/Data Scientist\nBBVA COMPASS, BIRMINGHAM, AI - AI\nDecember 2015 to January 2017\nDescription: BBVA Compass Bancshares, Inc. (formerly Compass Bancshares) is a United States-based financial holding company headquartered in Birmingham, Alabama. It has been a subsidiary of the Spanish multinational Banco Bilbao Vizcaya Argentaria since 2007 and operates chiefly in Alabama, Arizona, California, Colorado.\nResponsibilities:\n\n\u2022 Built an IEbot for automating KYC extraction for Institutional entities and Risk& Compliance\ndomain.\n\u2022 Implementation and design of the patented algorithm for signature less intrusion detection and prevention for SIPtraffic in Siberia's first IDS/IPS product line.\n\u2022 Ana moly detection using MultivariateGaussian model for traffic optimization\n\u2022 ACL implementation using TACACS Authentication adapter, OpenSSHcustomization, and PAM\nmodule implementation.\n\u2022 Implementation of Character Recognition using Support vector machine for performance optimization.\n\u2022 Image compression and reconstruction using Principle component analysis.\n\u2022 Market Mix Modelling: Helped a leading Sanitary napkin brand in India to identify\ndrivers of sales and distribution across key states through MMM.\n\u2022 Recommender system using low-rank matrix factorization to increase CTR for add network.\n\u2022 Collaborate with distributed cross-functional teams on common goals.\n\u2022 Innovate and leverage machine learning, data mining and statistical techniques to create new, scalable solutions for business problems.\n\u2022 Optimisation of statistical analysis and forecasting using Lasso, Cross Validation, Random forest, Log\nregression and prediction Tree basedML models with in R for diabetes incidences on patient cohorts.\n\u2022 Experience with deep learning LSTM and RNN based speech recognition using TensorFlow.\n\u2022 Analysea large amount of data classify data.\n\u2022 Visualize data using D3.js.\n\u2022 Createa model for forecast revenue.\n\u2022 Used TensorFlow library in dual GPU environment for training and testing of the Neural Networks\n\u2022 Co-authored two manuscripts; they are under review for two high impact journals.\n\u2022 Addressed spend optimization complexities using coefficients of Market Mix exercise.\n\u2022 Machine learning automatically scores user assignment based on few manually scored assignments.\n\u2022 Developed Online Slot booking system for Assessment test.\n\u2022 Developing new products in the site with client's specification and solving the issues in the Freshersworld.com site.\n\u2022 Adding Google analytics code in site.\n\u2022 Testing the compatibility and functionality of websites in different browsers and mobile.\n\u2022 DB design and Maintenance.\n\u2022 Extracted Statistics for performance metrics analysis for various components on load testing.\n\u2022 Applied association rule mining & chain model to identify hidden patterns and rules in remedy ticket analysis which aid in decision making.\n\u2022 Segmenting ABO population and developing demographic profile against each fragment.\n\u2022 Isolating customer behavioural patterns by analysing millions of customer data records overa period of time and correlating multiple customers' attributes.\n\u2022 Worked on various strategic projects (Customer engagement, SAclosures, Locker surrender etc.) &AdhocproactiveAnalysis with product teams to provide insights and trends using SAS&Excel.\n\nEnvironment: Apache, Spark MLlib, TensorFlow, Oryx 2, Accord.NET, Amazon Machine Learning (AML)Python, Django, Flask, Tensor Flow, ORM, Jinja 2, Mako, Naive Bayes, SVM,K- means, ANN, Regression."", u""Machine Learning/Data Scientist\nSOUTHWEST AIRLINES - Dallas, TX\nFebruary 2014 to November 2015\nDescription:The mission of Southwest Airlines is dedication to the highest quality of customer service delivered with a sense of warmth, friendliness, individual pride, and company spirit.\nResponsibilities:\n\n\u2022 Developing and scaling REST APIs and Good understanding and working experience in JSON, JSON Schema, and REST API.\n\u2022 Object-Oriented Design using common design patterns / Software Debugging, Test-driven development, UML, SVN.\n\u2022 Created Machine Learning and statistical methods, (SVM, CRF, HMM, sequential tagging) or willingness to intensely learn.\n\u2022 Used data mining algorithms and approach.\n\u2022 Developed using Python unit test framework, or any other unit test framework.\n\u2022 Using Python and Productionizing end-to-end systems.\n\u2022 Worked on file systems, server architectures, databases, SQL, and data movement (ETL).\n\u2022 Exposure to python and python packages.\n\u2022 Collaborate with Risk Analytics teams; Stress Testing team, Middle Office, IT and other departments.\n\u2022 Be responsible for creation and execution of test plans, protocols, and documentation.\n\u2022 Develop and implement innovative AI and machine learning tools that will be used in the Risk.\n\u2022 Be very passionate about quality and have a keen sense of ownership of the work accomplished.\n\u2022 Acquire business knowledge in the Firm's risk management processes.\n\u2022 Be responsible for creation and execution of test plans, protocols, and documentation.\n\u2022 Formulate and test hypotheses, extract signals from petabyte scale, unstructured data sets, and ensure that our display advertising business delivers the highest standards of performance.\n\u2022 Understanding the trade-offs between competing approaches and identifying the ones that are likely to havea real impact on theproduct.\n\u2022 Lead a project team of systems engineers (HW, FW & SW) and internal and outsourced development partners to develop reliable, cost effective and high-quality solutions.\n\u2022 Identify and assess available machine learning and statistical analysis libraries (including regressors, classifiers, statistical tests, and clustering algorithms).\n\u2022 NLP engineer witha profound interest in research and development for cutting edge machine learning techniques.\n\u2022 Architecture and Design of reusable server components for the web as well as Mobile applications.\nEnvironment: Erwin r9.0, Informatic a 9.0, ODS, OLTP, Oracle 10g, Hive, OLAP, DB2, Metadata, MS Excel, Mainframes MS Visio, Rational Rose, Requisite Pro, Hadoop, PL/SQL, etc."", u'Machine Learning\nCOVENTRY HEALTH CARE - Minneapolis, MN\nSeptember 2012 to November 2013\nDescription:Coventry Health Care Management utilizes multiple software systems to support the intake and processing of authorization requests, the exchange of data between the payer and vendors contracted to perform services on our behalf, manage Case and Disease programs, provide robust reporting and decision support, and generally automate and facilitate their business Processes.\nResponsibilities:\n\n\u2022 Developing propensity models for Retail liability products to drive proactive campaigns.\n\u2022 Extraction and tabulation of data from multiple data sources using R, SAS.\n\u2022 Data cleansing, transformation and creating new variables using R.\n\u2022 Built predictive scorecards for Cross-sellingCarolan, Life Insurance, TD, and RD.\n\u2022 Scoring predictive models as per regulatory requirements & ensuring deliverables with PSI.\n\u2022 Using Statistical techniques on a need basis ranging from simple significance test to Regression Segmentation techniques for delivering analyses and building strategies.\n\u2022 Data modelling and formulation of statistical equations using advanced statistical forecasting techniques.\n\u2022 Provide guidance and mentoring to team members.\n\u2022 Establish scalable, efficient, automated processes for large scale data analyses, model development, model validation and model implementation.\n\u2022 Formulate and test hypotheses, extract signals from petabyte scale, unstructured data sets, and ensure that our display advertising business delivers the highest standards of performance.\n\u2022 Lead a multi-functional project team.\n\u2022 Develop necessary connectors to plug ML software into wider data pipeline architectures.\n\u2022 Applied association rule mining &chain model to identify hidden patterns and rules in remedy ticket analysis which aid in decision making.\n\u2022 Understanding the client business problems and analysing the data by using appropriate Statistical models to generate insights.\n\u2022 Integrated Teradata with R for BI platform and also implemented corporate business rules.\n\nEnvironment: R Studio, Machine learning, Informatic a 9.0, Scala, Amazon Machine Learning (AML)Python, Django, SAAS.', u'Data Architect/Data Modeler\nPEOPLE TECH GROUP - IN\nFebruary 2010 to October 2012\nDescription:Founded in 2006, People Tech is an emerging leader in the Enterprise Applications and IT Services marketplace. People Tech draws its expertise from strategic partnerships with technology leaders like Microsoft, Oracle and SAP and combines that with the deep understanding of its employees.\nResponsibilities:\n\u2022 Worked as DataExpert on a data mining ETL development project using SAS Enterprise Guide.\n\u2022 Created test plan documents for all back-end database modules.\n\u2022 Worked with large amounts of structured and unstructured data.\n\u2022 Responsible for data collection, cleansing, andANOVA. Designed technical solution roadmap to deal with noise in sales data.\n\u2022 Worked on loading the data from MySQL to HBase where necessary using Scoop.\n\u2022 Handled end-to-end project from data discovery to model deployment\n\u2022 Knowledge in Business Intelligence tools and visualization tools such as BusinessObjects, Tableau, ChartIO, etc.\n\u2022 Knowledge in Machine Learning concepts (GeneralizedLinear models, Regularization, RandomForest, TimeSeries models, etc.).\n\u2022 Deployed GUI pages by using JSP, JSTL, HTML, DHTML, XHTML, CSS, JavaScript, and AJAX.\n\u2022 Configured the project on WebSphere 6.1 application servers.\n\u2022 Implemented the online application by using Core Java, JDBC, JSP, Servlets and EJB 1.1, Web Services, SOAP, WSDL.\n\u2022 Monitoring the automated loading processes.\n\u2022 Communicated with other Health Care info by using Web Services with the help of SOAP, WSDLJAX-RPC.\n\u2022 Used Singleton, factory design pattern, DAO Design Patterns based on the application requirements\n\u2022 Used SAX and DOM parsers to parse the raw XML documents\n\u2022 Used RAD as Development IDE for web applications.\n\u2022 Used Log4J logging framework to write Log messages with various levels.\n\u2022 Involved in fixing bugs and minor enhancements for the front-end modules.\n\u2022 Implemented Microsoft Visio and Rational Rose for designing the Use Case Diagrams, Class model, Sequence diagrams, and Activity diagrams for SDLC process of the application.\n\u2022 Maintenance in the testing team for System testing/Integration/UAT.\n\u2022 Guaranteeing quality in the deliverables.\n\u2022 Conducted Design reviews and Technical reviews with other project stakeholders.\n\u2022 It was a part of the complete life cycle of the project from the requirements to the production support.\n\u2022 Implemented the project in Linux environment.\n\nEnvironment: R 3.0, Erwin 9.5, Tableau 8.0, MDM, QlikView, MLLib, PL/SQL, HDFS, Teradata 14.1, JSON, HADOOP (HDFS), MapReduce, PIG, Spark, R Studio, MAHOUT, JAVA, HIVE, AWS.', u'Data Analyst\nINNOVA INFOTECH - Bengaluru, Karnataka\nJanuary 2009 to January 2010\nDescription: SYS INNOVA Infotech is an offshore software services and IT consulting company based in Bangalore, India. As a committed outsourcing partner and an IT vendor, our goal is to ensure cost effective, technical excellence and on-time deliveries. While we take care of their end-to-end programming and consulting needs, our clients focus on core business activities which correlate directly to their revenues and profitability. Strategic partnership with us gives our clients the access to latest technology, skilled manpower and scalable team which ultimately results in lower risk and higher ROI.\nResponsibilities:\n\n\u2022 Implementation of Metadata Repository, Maintaining Data Quality, Data Clean-up procedures, Transformations, Data Standards, DataGovernanceprogram, Scripts, Stored Procedures, triggers and execution of test plans.\n\u2022 Developed Internet traffic scoring platform for ad networks, advertisers, and publishers (rule engine, site scoring, keyword scoring, lift measurement, linkage analysis).\n\u2022 Responsible for communication and negotiation with project related aspects of project loading, construction budget, design alterations, and unexpected events on the project.\n\u2022 Responsible for defining the key identifiers for each mapping/interface.\n\u2022 Clients include eBay, Click Forensics, Cars.com, Turn.com, Microsoft, and Look smart.\n\u2022 Designed the architecture for one of the first analytics 3.0. Online platforms: all-purpose scoring, with on-demand, SaaS, API services.\n\u2022 Web crawling and text mining techniques to score referral domains, generate keyword taxonomies and assess commercial value of bid keywords.\n\u2022 Used RAD as Development IDE for web applications.\n\u2022 Developed new hybrid statistical and data mining technique known as hidden decision trees and hidden forests.\n\u2022 Reverse engineering of keyword pricing algorithms in the context of pay-per-click arbitrage.\n\u2022 Performed data quality in TalentOpenStudio.\n\u2022 Coordinated meetings with vendors to define requirements and system interaction agreement documentation between client and vendor system.\n\u2022 Automated bidding for advertiser campaigns based either on keyword or category (run-of-site) bidding.\n\u2022 Creation of multimillion bid keyword lists using extensive web crawling. Identification of metrics to measure the quality of each list (yield or coverage, volume, and keyword average financial value).\n\u2022 Enterprise Metadata Library with any changes or updates.\n\u2022 Document data quality and traceability documents for each source interface.\n\u2022 Establish standards of procedures.\n\u2022 Generate weekly and monthly asset inventory reports.\n\nEnvironment: Erwin r7.0, SQL Server 2000/2005, Windows XP/NT/2000, Oracle 8i/9i, MS-DTS, UML, UAT, SQL Loader, OOD, OLTP, PL/SQL, MS Visio, Informatic a.']","[u""Bachelor's""]",[u''],"degree_1 : ""Bachelors"""
0,https://resumes.indeed.com/resume/ac7fdb07c9e658b1,"[u""Lead Data Scientist\nNokia Solutions and Networks\nApril 2015 to January 2017\nManaged a team of 5 developers with responsibility over their performance reviews, training, goal setting and performance\nlevel with focus on quality and on-time delivery for all team projects\n* Developed predictive Customer Experience Index(CEI) by using machine learning techniques to calculate customer experience which increased accuracy by 55% way to measure Nokia service; used 10000 sample to predict the rest of subscriber's CEI\n* Automated anomalous pattern Detection used by network operations and customer experience team resulting in 50% savings in product development time by removing the need for manual setting of business rules to identify anomalies\n* Created a framework for intelligently selecting the best classification algorithm to give highest predictive accuracy among logistic regression, decision tree, random forests, gradient boosted machines and deep learning, using Bayesian optimization"", u""Product Manager/Senior Data Scientist\nFlutura Business Solutions Private Limited\nJuly 2012 to March 2015\n* Ideated, conceptualized and led a 12-member team for the development of Flutura's innovative flagship product- Signal Studio, an advanced toolkit for data scientists aiding in quicker development and deployment of machine learning and statistical models\n* Curated data science and machine learning use cases and user stories based on customer requirements and issues, which helped acquire the first customer in the Utility domain\n* Streamlined the process using agile-lean methodologies resulting in a 40% savings in platform development and delivery time.\n* Maintained 100% on-time project delivery reputation with 20% of the projects being delivered ahead of schedule"", u'Junior Specialist\nArbitron India Technology Services Private Limited\nAugust 2010 to June 2012\nDesigned operational data-ware house and formulated automated data workflow which cut in half the time needed to generate\ndaily reports/dashboards and it completely replaced the error-prone resource intensive manual process.\nLeadership\n* Presented optimal solutions and new use cases in front of the business stakeholders and senior management teams at Nokia\n* Conducted orientation for the new joiners at Nokia to educate them about the data science practices and projects\n* Conducted multiple marketing workshops to engage with potential customers resulting in 2 confirmed clients at Flutura\n* Recruited, mentored and trained upcoming data scientists for realization of different projects at Nokia and Flutura\nData Science Projects\nImage Processing Using Neural Networks - Artificial Intelligence Project to predict the gender of the celebrity using celebrity images.\nBuilt deep neural networks from scratch in Python, including mini-batch gradient descent, Adam optimization, Dropout and L2\nregularization, and convolutional neural networks(CNN) architectures -VGG16, ResNet, Inception Networks in Keras and Tensorflow\nBNP Paribas Claims Management(Rank-156/2940) - Kaggle Classification competition to identify claims which can be approved or not. Developed R scripts to perform K-means clustering, PCA, T-SNE, oversampled the imbalanced data set using SMOTE, XGBoost,\nGBM, Random Forest models. Used bagging and stacking to ensemble advanced models to get optimum log-loss to finish in top 6%\nNew York Taxi Trip prediction -(Rank-140/1257) - Kaggle Regression competition to predict the total ride duration of taxi trips in\nNew York City. Performed geospatial exploratory data analysis to interpolate distance between two coordinates, feature engineering\n& selection in Python. Algorithms used: LightGBM, xgboost, Ridge Regression, Lasso Regression, Neural Networks; Result- top 11%']","[u'M.S in Business Analytics and Project Management', u'Bachelor of Technology in Electronics and Communications']","[u'University of Connecticut Hartford, CT\nMay 2018', u'Cochin University of Science and Technology Kochi, Kerala\nMay 2010']","degree_1 : M.S in Bsiness Analytics and Project Management, degree_2 :  Bachelor of Technology in Electronics and Commnications"
0,https://resumes.indeed.com/resume/d05fc80ab4c22405,"[u'Data Scientist\nData Masked\nSeptember 2016 to Present', u""Scientist\nOrthobond, Inc - North Brunswick, NJ\nJanuary 2011 to January 2013\nCasual Inference and Mechanism study for Drug Efficacy and Toxicity Evaluation\n\u2022 Reduced the production cost by 80% by implementing experimental design and data analysis in Excel\n\u2022 Managed multiple projects with Excel for various clients and successfully achieved milestones\n\u2022 Figured out causality of four antibiotics' efficacy and toxicity by executing hundreds of experiments\n\u2022 Technology Included: Causal Inference, Experimental Design, Randomization, Excel, PowerPoint""]","[u'Master of Science in Statistics in Statistics', u'Master of Arts in Chemistry in Chemistry', u'Bachelor of Science in Chemistry in Chemistry']","[u'Baruch College New York, NY\nJanuary 2012 to February 2014', u'State University of New York at Buffalo Buffalo, NY\nAugust 2006 to February 2010', u'Sichuan University Chengdu, CN\nSeptember 2001 to July 2005']","degree_1 : Master of Science in Statistics in Statistics, degree_2 :  Master of Arts in Chemistry in Chemistry, degree_3 :  Bachelor of Science in Chemistry in Chemistry"
0,https://resumes.indeed.com/resume/a305c81aca492ae8,"[u'Data Scientist\nTineo Cardenas LLC\nSeptember 2017 to Present\nNatural Language Processing project Personalized Medicine: Redefining Cancer Treatment. Predicting gene mutation by creating random forest, linear regression models.\n\u2022 Exploratory data analysis, Classification and Regression Trees (CART), Clustering analysis, Density-based Spatial Clustering of Applications with Noise for Major League Baseball data set.\n\u2022 Predicting Income using Machine Learning using CARTs', u'Data Scientist\nGeneral Assembly - Ames, IA\nJanuary 2017 to June 2017\nData Science Immersive program focused on Statistical Analysis, Machine Learning and other modeling techniques exclusively in Python.\n\u2022 Projects include: Predicting Management Liability Insurance Losses for a Securities Class Action Lawsuit; Predicting Real Estate Predicting in Ames, Iowa using Linear Regression models; Predicting Data Science Salaries for an indeed.com Webscrape; Disaster Management by Predicting Survival Rate using Titanic data; Predicting Movie Ratings Using Decision Trees, Random Forest, Extra Trees from Application Programming Interface (API).\n\u2022 Topics covered: Logistic/Linear Regression, Hypothesis Testing, Web Scraping using Selenium and Beautiful Soup, Decision Trees, Time-Series, ARIMA, Hierarchical Clustering, Principal Component Analysis, Support Vector Machine, Regular Expression\nKey Achievements / Capstone Project:\n\u2022 Performed a training data/test data split in order to better manage variance / bias tradeoff in model building process. Performed Lasso and Ridge Regularization on Logistic, Linear Regression Model.\n\u2022 Produced Decision Tree, Bagging Decision Tree, Extra Trees and Random Forest to guide the building of a logistic regression model that predicts whether a Securities Class Action lawsuit will settle or be dismissed. Trees and ensembles determined feature importance to benefit feature reduction.\n\u2022 Fit Logistic Regression, Linear Regression Models on training data, using SciKit Learn.\n\u2022 Produced Confusion Matrix and Classification Reporting to visualize the performance of the logistic regression model according to accuracy, precision, recall scores.\n\u2022 Using plot.ly produced ROC Curve to visually represent True Positive Rate versus False Positive Rate. Equally produced visualization of Precision Recall Curve for Area under the Curve.\n\u2022 Used non-parametric K-nearest neighbors technique by fitting model on test data and minimizing the sum of squares for the distances between data and finds the corresponding cluster centroids.\n\u2022 To address non-linear data, performed DBSCAN (Density-based Spatial Clustering of Applications with Noise) unsupervised learning clustering technique. Visualized the identified clusters from DBSCAN using plot.ly.', u""Senior Consultant/Data Scientist\nNationwide Management Liability & Specialty Insurance Co\nJanuary 2010 to January 2016\nAnalyzed the Securities Class Action SCA lawsuit data landscape to ensure the accuracy of the Director and Officer (D&O) predictive pricing and relevant to the underwriting staff.\n\u2022 Ran logistic regression models using PROC LOGISTIC to test likelihood of U.S. Corporation would have a SCA given corporate financial data, external risk factors and proprietary and external loss data. Goal was to strengthen prediction power of risk score.\n\u2022 Ran PROC GLM to create linear regression, risk score as target variable. Created linear regression models testing dimensionality reduction. Also feature engineered based on existing data: dummy variables, interaction variables.\n\u2022 Performed various T-test analyses to determine whether there is a statistically significant difference among SCA loss data across segments (e.g. subprime / merger and acquisition).\n\u2022 Monitored and analyzed Securities Class Action (SCA) related trending economic events to build an accurate modeling dataset used for policy pricing, risk assessment, and actuarial loss triangle development processing.\n\u2022 Utilized SAS Visual Analytics to create tile charts, heat maps, graphs, maps and other representations to analyze the SCA landscape. Uploaded data via VA Laser server.\n\u2022 Scheduled SAS jobs via Platform Computing scheduler software.\n\u25e6 Used SAS Stored Process (SAS Output Delivery tasks) to call SAS Data Integration jobs to compare data and for data transformation. Executed monthly SAS jobs via UNIX Shell scripting.\nKey Achievements\n\u2022 Performed text analytics of legal SCA complaints to categorize and segment relevant data points into the predictive model.\n\u25e6 Performed credit rating analysis to identify the likelihood of public company SCA based on downgrade/upgrade of rating. Analysis focused on predicting a SCA given changes in Moody's/S&P credit rating systems.\n\u25e6 Gathered derivative class action data and default/bankruptcy data for modeling data preparation.\n\u25e6 Created an underwriter reference guide that detailed data sources/processing/warehousing/output used for the Predictive Model. Trained the new underwriting staff in the account submission system that utilizes Predictive Model for Risk Assessment.\n\u2022 Developed a litigation management front-end system that allows underwriting and claims staff to research the SCA universe. Incorporated Extract Transform and Load (ETL) components using SAS Data Integration in addition to querying/reporting functionalities.\n\u2022 Provided analytics and price mechanism that allowed underwriting team to avoid adverse risks at adverse levels and avoided 1%-3% toxic accounts and toxic risks.\n\u2022 Streamlined processes that saved the Company over $100,000 in costs, ~10% of a $1.1M budget.\n\u2022 Presented at 2015 Insurance Accounting & Systems Association's Educational Conference and Business Show. The topic discussed 'Transforming underwriting with Data and Analytics.'""]","[u'Certification in General Assembly', u'B.A.']","[u'M.S. Intellectual Property Albany Law School', u'Hobart & William Smith Colleges']","degree_1 : Certification in General Assembly, degree_2 :  B.A."
0,https://resumes.indeed.com/resume/4ee2ee862c727097,"[u'Data Scientist\nGoogle - Sunnyvale, CA\nSeptember 2017 to Present\nAnalyse de donnees pour Google Platform.\nExtraction des donnees a partir de Google Cloud, creation de requetes et tableaux parametrables.\nMachine learning pour analyse des textes libres.', u""Data Scientist\nPaypal - San Jose, CA\nMarch 2017 to September 2017\nModelisation de donnees financieres. Creation d'un modele de prediction (time series) des couts et revenus. Extraction des donnees (Haddop, Hive) et controle des donnees."", u""Data Scientist\nGooglex - Mountain View, CA\nMarch 2015 to January 2017\nDans le cadre du projet d'intelligence artificielle pour la Self Driving Car: indicateurs de qualite des donnees, affinement des nets, modelisation de donnees pour determiner les facteurs influencant la qualite des images acquises par les vehicules.\nAnalyse approfondie des incidents.\nAnalyse des donnees geographiques."", u'Data Analyst\nLegal Medicine Laboratory - Grenoble (38)\nFebruary 2009 to August 2014\nCollaborated with study teams, lawyers, and physicians to develop solutions and evaluate costs.\n\u25cf In charge of methodologies, data entry and cleaning, statistical analysis plans, bibliography.\n\u25cf Designed databases and coded (SQL, SAS, STATA, R, SQL), testing.\n\u25cf Completed Statistical data analysis (tests, regressions, time series and correlation), interpretation, randomization.\n\u25cf Created reports, summaries, presentation to study teams and customers (Microsoft Office).\n\u25cf Took care of quality standards compliance (GCP, CDISC), related legislation (government regulated),', u'Clinical Data Analyst\nBiostatistics Unit - University Hospital - Grenoble (38)\nJanuary 2001 to January 2008\nManaged projects, completed statistical studies including audits, case controls, surveys, cohorts and clusters.\n\u25cf Designed and managed data collections associated (SAS, STATA, SQL).\n\u25cf Deployed incident reporting software and conducted user training (JAVA).', u'Quantitative Analyst (Internship)\nAvalanche Laboratory - University of Grenoble (France) - Grenoble (38)\nFebruary 2004 to September 2004\nCleaned and reorganized data collections (SQL, JAVA, C++), provided documentation.\n\u25cf Modeled geographical data to create new top quality maps about avalanche risk.\n\u25cf Presented conclusions and solutions to accurate risk management.', u""Business Analyst\nDecathlon, Toulon (France)\nJanuary 1992 to January 1997\nManaged projects, completed marketing studies: audits, clients' workflows, surveys, campaign impact.\n\u25cf Developed innovative solutions and guidance for customers.""]","[u'M.S. in Statistics and Information Systems', u'in Statistics and Data Management']","[u'Grenoble University\nJanuary 2004', u'Grenoble University\nJanuary 1992']","degree_1 : M.S. in Statistics and Information Systems, degree_2 :  in Statistics and Data Management"
0,https://resumes.indeed.com/resume/da5f61874c411095,"[u'Data Scientist Intern\nCamino Financial - Los Angeles, CA\nFebruary 2018 to Present\nMine, clean and process data from company databases.\nPerform analysis and create data visualizations.\nCommunicate and collaborate with product management and engineering departments .\nDevelop statistical models for data analysis.\nPresent analysis results to management team.', u'Data Analyst\nFinancial Accounting Department of Dongxihu Education Bureau - Wuhan, Hubei\nNovember 2011 to August 2014\nManage educational data.\nDraft annual statistical reports.\nCoordinate with all departments and ensure implementation of work plans.\nInspect schools on statistical work.\nHelp formulate planning of education development.', u'Secretary\nJiyukou Town Government - Qianjiang, Hubei\nSeptember 2009 to October 2011\nPerform routine clerical and administrative functions such as scheduling appointments, arrange conferences.\nDraft report on the work of the town government.', u'Training Assistant\nZaishuiyifang Hotels and Resorts Management Department - Wuhan, Hubei\nJanuary 2007 to September 2009\nPlan, organize training programs.\nManage employee records.\nConduct performance reviews.', u'Investment Assistant\nOrient Securities Company Limited, - Wuhan, Hubei\nMay 2006 to July 2007\nSolve client service requests.\nCultivate client relationships.\nOrganize client seminars.']","[u""Master's in Mathematics""]","[u'Claremont Graduate University Claremont, CA\nJanuary 2015 to January 2016']","degree_1 : ""Masters in Mathematics"""
0,https://resumes.indeed.com/resume/0953e08b6540f9b2,"[u'Data Scientist\nCARLSON ANALYTICS LAB - Minneapolis, MN\nJune 2017 to Present\n** Client: Mall of America, Minneapolis, MN **\n\n\u2022 Improved ridership forecasting at Nickelodeon Universe by reducing mean percentage error from 20% to 16% using regularized linear model. Improved predictions reduced the overstaffing and understaffing days count by 35% (70 days), leading to reduced staffing expense and improved\ncustomer experience.\n\u2022 Automated data preparation and machine learning workflow in python using pandas, numpy, and scikit- learn, reducing total prediction time from 2 days to under 0.5 hours.\n\n** Client: United HealthCare Group **\n\u2022 Identified correctly 60% of employees having a high probability of being a costly patient in next 12\nmonths using linear regression model, leading to better selection of medical coverage for employees.\n\n** Client: PricewaterhouseCoopers **\n\u2022 Created opportunity to increase $1.5M in revenue for an auto-insurance client by identifying loss-making customer segments using K-Means clustering model, and recommending marketing strategies.', u'Assistant Manager\nLARSEN & TOUBRO LTD - Delhi, Delhi\nOctober 2014 to May 2017\n\u2022 Reduced engineering cycle time from 120 days to 85 days by revamping existing document control\nsystem and bringing 6 functions across 4 business units on system to streamline the project delivery.\n\u2022 Secured $650M worth of projects through bidding by leading a team of 12 engineers and collaborating with cross-functional teams to create engineering proposals.', u'Executive Engineer\nAugust 2011 to June 2014\n\u2022 Developed an interactive dashboard for communicating a high-level overview of project progress to senior management, reducing status update meetings count by 50%.\n\u2022 Automated the manual process of data manipulation in MS-Excel using VBA, reducing 20 man- hours/month.']","[u'Master of Science in Business Analytics', u'Bachelor of Technology in Mechanical Engineering']","[u'UNIVERSITY OF MINNESOTA Minneapolis, MN\nMay 2018', u'NATIONAL INSTITUTE OF TECHNOLOGY Jaipur, Rajasthan\nJune 2011']","degree_1 : Master of Science in Bsiness Analytics, degree_2 :  Bachelor of Technology in Mechanical Engineering"
0,https://resumes.indeed.com/resume/9e37e6d7d7ebc0a7,"[u'Data Scientist\nAfiniti Inc - Washington, DC\nNovember 2014 to Present\n\u2022 Account lead (AI production) for all AT&T queues.\n\u2022 Built and deployed a C++ based tool to perform Bayesian Regression and implements ANOVA based\nClustering Algorithms\n\u2022 Built models to accurately predict conversion propensity of calls in the retention queues of DIRECTV (AT&T)\n(with ROC .83) using a RandomForestClassifier in Python.\n\u2022 Conducted t-tests to test significance of outcomes obtained from Afiniti paired calls\n\u2022 Performed ETL tasks (using SqlServer, Mysql) to convert raw data into a desirable format for further analysis.\n\u2022 Wrote Python scripts that interact with a web based modeling tool to automate model refresh and deployment\n\u2022 Managed a team of 5 to ensure successful expansion to multiple routing queues while providing a consistent lift of 0.5-1% in performance.\n\u2022 Interact with Client on a regular basis to convey Data requirements and explain Afiniti modeling strategies', u'Research Assistant\nINLAB - Arizona State University\nJuly 2012 to August 2014\n\u2022 Worked on development of Congestion control algorithms for multi user wireless networks\n\u2022 Wrote a Python package to establish radio communications between Software defined radio-controlled UAVs']","[u'Masters of Science in Electrical Engineering', u'Bachelor of Technology in Electronics Engineering', u'in bank']","[u'Arizona State University Tempe, AZ\nAugust 2012 to June 2014', u'West Bengal University of Technology Kolkata, West Bengal\nAugust 2007 to August 2011', u'Institute of Engineering and Management']","degree_1 : Masters of Science in Electrical Engineering, degree_2 :  Bachelor of Technology in Electronics Engineering, degree_3 :  in bank"
0,https://resumes.indeed.com/resume/7b2c149c0b444c78,"[u""Data Scientist\nCognizant Tech Solutions US Corp - Fort Worth, TX\nSeptember 2017 to Present\nPricing analytics:\nBuilt an analytics tool at its core which help to making better pricing and contracting decisions and that enabled\ncommercial pricing strategy teams to properly assess the profitability of proposed deals.\nResponsibilities:\n\u2022 Performed data cleansing and data mapping using Python programming language for generating price simulations to enable pricing decisions for profit analysis of contracts.\n\u2022 Created customer segments using unsupervised clustering techniques based on sales, contracts and NBO. Compared the customer attrition and player market share with the cluster segments created.\n\u2022 Integrated disparate data from sources like SalesForce, SAP BI extracts, Siebel and third-party data like AnalySource,\nIMS NSP, Predictive Acquisition Cost\xae (PAC) and internal sources.\n\u2022 Leveraged big data technology tools HIVE, PySpark and developed functions to interact with Hadoop data lake on\nAWS for data transformation(ETL).\nwww.linkedin.com/in/davidsnayakanti https://github.com/nayaksu7 nsundeepdavid@gmail.com 313-505-5479\n\u2022 Developed comprehensive data visualizations in QlikSense to illustrate complex ideas to various stakeholder levels on KPI's like product supply & capacity, cost for moving out of recommended tiers, price increase trend analysis by competitor etc.\n\u2022 Helped client to accelerate responses o competitive situations and provide insights into revenue leaks for stronger\nnegotiating position."", u""Graduate Assistant\nBGSU - Bowling Green, OH\nJanuary 2017 to May 2017\nNegative Outcomes Risk Prediction Model:\nAnalyzed Medicare resource utilization groups (RUG's) and Managed Care insurance claims data from healthcare\nprovider and predicted negative outcomes risk using regression analysis. Built a logistic regression model that\nhelped providers and members keeping patient population healthy, assist healthcare providers, improve outcomes and reduce costs.\nResponsibilities:\n\u2022 Developed predictive model to identify negative outcome margins.\n\u2022 Handled class imbalance using re-sampling techniques.\n\u2022 Utilized Logistic regression in R to identify the factors affecting margin and predict residents with negative margins.\n\u2022 Build Gradient Boost Model utilizing H20.ai in R to analyze variable importance and evaluate model performance.\n\u2022 Performed clustering analysis on historical patient level data to classify them into payment (total expense per stay)\ngroups and identified parameters impacting expenditures and provided recommendations to drive reimbursements.\nTools/Techniques: R /H2O-R/Tableau/H2O-FLOW/HIVE/HDFS"", u'Data Scientist\nNovartis\nFebruary 2014 to August 2015\nText analytics:\n\u2022 Implemented a text mining based algorithmic approach in R language to find nearest-neighbour NCIs (Non-\nConformance Incidents) for each NCI based on distance metric computed between vectors of keywords.\n\u2022 Identified recurring NCIs through similarity between the product/process related and issue related information\navailable in NCI records.\n\u2022 Distance match between products and issue key words was computed using cosine similarity.\n\u2022 Generated percentile confidence scores for product distance, issue distance and overall distance between complaints and CAPAs to help user define the cut-off confidence.\n\u2022 Developed CAPA/Complaints effectiveness metrics dashboard in Tableau to provide visual insights to business\nusers.\nTools/Techniques: R /SQL server/ Tableau /Microsoft Excel\n\nMarketing analytics:\nSegmented physicians and provided meaningful insights to marketing & salesforce team for physician targeting\nleveraging APLD patient Level data from Symphony, IMS Xponent, IMS Sales and Distribution data(DDD) and various internal datasets.\nResponsibilities:\n\u2022 Created a robust physician segmentation framework based on physician prescribing potential and adoption rate.\n\u2022 Performed AS-IS analysis on physician level data and assess initial current target list provided by sales force team.\n\u2022 Leveraged k-means algorithm to profile physicians and provided recommendations to salesforce team to set brand\nstrategy, direct promotional activity, allocate resources and to ultimately increase brand profitability through efficient and effective marketing.\n\u2022 Provided distinct segments with key physician characteristics which helped marketing team to prioritize market\nsegments and devise promotional messages.\nwww.linkedin.com/in/davidsnayakanti https://github.com/nayaksu7 nsundeepdavid@gmail.com 313-505-5479', u'Data Analytics Specialist\nNovartis\nJanuary 2013 to January 2014\nPatient Satisfaction analytics:\nAnalyzed CAHPS survey data which included clinical and demographic data and came up with factors leading to dis-satisfaction by patients.\nResponsibilities:\n\u2022 To Identify key variable for which hypothesis testing was done using Chi-Squared, Kruskal Wallis and Wilcoxon tests.\n\u2022 Classification of patients based on Clinical and demographic traits (like age, race, financial characteristics) to enable\ncustomized servicing.\n\u2022 Identification and prioritization of the pain areas that are leading to lower satisfaction like Discharge codes, discharge\nhour, age, length of stay, deprivation Index etc.\n\u2022 Quantify the impact under interactions for the key variables. Leveraged Decision tree and Logistic to build the models and quantify the impact.\n\u2022 Inferences were drawn from the models and variables were prioritized.\nTools/Techniques: R /SQL/ Microsoft Excel/Tableau', u'Data Analyst\nNovartis\nJune 2012 to January 2013\nResponsibilities:\n\u2022 Performed data pre-processing and cleaning to prepare data sets for further statistical analysis; including outlier\ndetection and treatment, missing value treatment, variable transformation and various other data manipulation\ntechniques using SAS programming language.\n\u2022 Developed codes utilizing SAS Base/SAS SQL and prepared datasets of adverse events generated from clinical trials.\n& Post-Market Surveillance for further analysis by HEOR (Health Economics & Outcomes Research) team.\n\u2022 Modified existing SAS/SQL programs and created new programs using SAS macro variables to improve ease and speed of modification as well as consistency of results.\n\u2022 Data Extraction, Data analysis and Data compilation using SQL and Microsoft Excel.', u""Data Analyst\nCR BIO Sciences\nOctober 2011 to May 2012\nResponsibilities:\n\u2022 Performed data pre-processing and cleaning to prepare data sets for further statistical analysis; including outlier\ndetection and treatment, missing value treatment, variable transformation and various other data manipulation\ntechniques using R programming language.\n\u2022 Data pulling, Data analysis and Data compilation using SQL and Microsoft Excel.\n\u2022 Ensured reconciliation between Clinical Database (OCRDC-Oracle Clinical Remote Data Capture) and Safety\ndatabase.\nOTHER ANALYTICS PROJECTS\n\u2022 Parkinson's Disease Prediction model Toolkit: Python, SAS Base & SAS E-Miner & Advanced Excel\nFocus: Developed a prediction model that help healthcare practitioners for early detection of Parkinson's disease\nbased on speech signals. The model reduces the effort and cost of the patient visiting the clinic multiple times.\n\u2022 Transportation Safety Board of Nebraska Toolkit: R, SAS Base & SAS E-Miner, Tableau, Advanced Excel\nFocus: Implemented CRISP-DM to build a predictive model. Identified the factors which determined whether a\ncrash will occur using techniques such as Logistic Regression, correlation Analysis, Random Forest and extracted\nkey factors using association rule mining.\n\u2022 Big Data- Analysis of Climatic and Temperature Data from NCDC Toolkit: Cloudera Impala, Hive (Hadoop\nEnvironment), Spark, Pyhton & Tableau.\nFocus: Finding descriptive statistics such as the min/max/mean/median temperatures of 40 years of data and clustered weather stations with similar trends, build a time series model to predict future temperatures.""]","[u'Masters of Science in Analytics', u'Certificate in Big Data Analytics & Optimization', u'Master of Pharmacy in Pharmacy']","[u'Bowling Green State University\nMay 2017', u'Carnegie Mellon University\nAugust 2015', u'Jawaharlal Nehru Technological University\nMay 2012']","degree_1 : Masters of Science in Analytics, degree_2 :  Certificate in Big Data Analytics & Optimization, degree_3 :  Master of Pharmacy in Pharmacy"
0,https://resumes.indeed.com/resume/6b5e5cb16af56210,"[u'Data Scientist Intern\nKeck School of Medicine\nJune 2017 to Present\n\u2022 Prototyping and developing Data models to analyze data for obtaining necessary information for several projects.\n\u2022 Collaborating with business leaders to construct quantitative analysis model to discover likely donors.\n\u2022 Recommending new prospects and likelihood with Tableau for better comprehension of data.', u'Trainee Analyst\nWissen Infotech Pvt. Ltd - Bengaluru, Karnataka\nJune 2016 to October 2016\n\u2022 Coordinated with business leaders and engineers to discuss new features and investigated issues associated with client on-boarding.\n\u2022 Developed client on-boarding application using Java Spring framework to manage prospective clients of Morgan Stanley.', u'Intern\n9 Series Solution Ltd\nJune 2015 to July 2015\n\u2022 Worked with team for designing and developing ""Mentors Around"" application to connect mentors and students.\n\u2022 Developed web application by structuring front-end with AngularJS and configuring back-end using PHP cake framework and MySQL.']","[u'MS Computer Science in Artificial Intelligence', u'B. Tech in Computer Science']","[u'University of Southern California\nDecember 2018', u'Vellore Institute of Technology Vellore, Tamil Nadu\nMay 2016']","degree_1 : MS Compter Science in Artificial Intelligence, degree_2 :  B. Tech in Compter Science"
0,https://resumes.indeed.com/resume/eab2f3ac2a288b53,"[u""Data Scientist and Algorithm Developer\nWayne State University - Detroit, MI\nJanuary 2016 to Present\nIn this position, I am serving as Data Analytics Expert and Predictive Modeling Researcher to design, develop, and implement data analytics algorithms for Virtually Guided Resistance Spot Welding Project (VRWP). The goal of VRWP is generating data-driven solutions for weldment design in the automotive industry. This project is a joint project (Wayne State University, Ford Motor Company and Digital Manufacturing and Design Innovation Institute (DMDII))\nMajor Responsibility: Design, development, and/or implementation of data mining and machine learning algorithms\n\nSample works developed for VRWP\n\u2022 Constructed and tuned the parameters of predictive models.\n. Artificial Neural Net (ANN)\n. Deep Neural Net (DNN)\n. Decision Tree (RF, M5, CART)\n. Lazy models K Nearest Neighbor (KNN) and KStar\n. Statistical regression models\n. Support Vector Machines (epsilon, nu, SMO)\n\u2022 Designed and implemented Progressive Sample Size Planning (PSSP) algorithm to reduce the cost of process\n\u2022 Implemented feature engineering algorithms\n\u2022 Designed and implemented Robustness Assessment Algorithm (RAA) based on Bootstrapping techniques\n\u2022 Devised accuracy and precision assessment methodology for predictive algorithms based on statistical hypothesis testing techniques (t and Levene's tests)\n\u2022 Designed and implemented Simulated Weld Lobe Generation Algorithm (SWLGA) based on Monte Carlo simulation technique\n\u2022 Implemented Signal to Noise (S/N) data quality analysis methodology based on Taguchi experiments\n\u2022 Integrated Ford Motor Company engineering and tolerance specifications to measure the quality of data\n\u2022 Conceptualized and devised a novel algorithm performance assessment measure called Mean Tolerance-base Error\n\u2022 Standardizing coding system for the predictive algorithms"", u'Predictive Maintenance Engineer & Data Analyst\nTabriz Oil Refining Company\nJanuary 2009 to January 2013\nPrimary role: predictive maintenance of capital-intensive physical assets, risk and failure assessment and prevention, intelligent asset maintenance management, Computerized Maintenance Management Systems\n(CMMS) expert, planning and scheduling preventive and predictive maintenance programs, organizational\nrealignment and process reengineering, deploying data-driven smart maintenance management systems.\nSecondary role: energy management, energy data analysis and consumption analysis and prediction, design and deployment of energy management soft systems.']","[u'PhD in Industrial and Systems Engineering', u'MS in Computer Science in Computer Science', u'MS in Mechanical Engineering']","[u'Wayne State University Detroit, MI\nMay 2018', u'Wayne State University Detroit, MI\nMarch 2018', u'Amirkabir University of Technology Tehran, IR\nJanuary 2006']","degree_1 : PhD in Indstrial and Systems Engineering, degree_2 :  MS in Compter Science in Compter Science, degree_3 :  MS in Mechanical Engineering"
0,https://resumes.indeed.com/resume/28d7f6b4f458afdf,"[u'IOS Developer\nEntradasoft Pvt Ltd\nAugust 2017 to Present\nBarter - Performed in a Lead developer role on an enterprise iOS app. App\nallows users to get details of food items of their nearby Restaurants, Rate and\nReview the food items. I used Xcode (9.2) for development, Objective-C for\nProgramming, cocoa pods, Also it consume RESTful web services to interact with Database.', u'Data scientist\nEntradasoft Pvt Ltd\nSeptember 2018 to November 2018\nCharging station - A Data science project for the identifying best suitable\nplace for charging station in Delhi. I Developed this project in python in\nAnaconda and used some python(3.6.4) library like Pandas, Numpy,\nMatplotlib, Bokeh, Scikit-Learn. My responsibility is clean the structured\ndata like handling null data, blank column etc, store data in pandas\ndataframe, Analytics and showing suitable location in green and unsuitable\nlocation in red on Google Map. Also i used Tableau for some Analytics.', u'Software Developer Intern\nEntradasoft Pvt Ltd\nFebruary 2017 to August 2017\nBarter - A web Application for Restaurants, for Online Ordering, Inventory\nManagement, Employee Management and Basics Analytics on sales Data. It is\nDeveloped in Struts Framework, Html, CSS, Bootstrap, Javascript and Ajax.\nWork - Development, Design, Bug Fixing.']","[u'B.tech', u'']","[u'Delhi Institute of Technology & Management\nJanuary 2016', u'Senior Secondary\nJanuary 2011']","degree_1 : B.tech, degree_2 :  "
0,https://resumes.indeed.com/resume/55b97bfc2ecb274f,"[u'Data Scientist\nSouth Dakota State University - Brookings, SD\nMarch 2017 to January 2018\n\u2022 Analyzed and processed complex data sets using advanced querying, visualization and analytics tools.\n\u2022 Create complex SQL stored procedures, Triggers, Functions, Views, Indexes in Microsoft SQLServer\n\u2022 Scheduled jobs to automate different database related activities such as backups, maintaining index, and monitoring disk space and backup verification\n\u2022 Developed subject segmentation algorithm using R.\n\u2022 Involved in the process of load, transform, and analyze data from various sources into HDFS (Hadoop\nDistributed File System) using Hive, Pig and Sqoop.\n\u2022 Used Python 3.0 (numpy, scipy, pandas, scikit-learn, seaborn, NLTK) and Spark 1.6 / 2.0 (PySpark, ML) to develop variety of models and algorithms for analytic purposes.\n\u2022 Developed an algorithm that can identify ""bad"" assessments that are expected to Fail under central review.\n\u2022 Used Statistical methods to analyze the performance of each clinical site across 27 countries on 30 studies.\nPredicting number of days to reach the target number of sites for a clinicalstudy.\n\u2022 Processed huge datasets (over billion data points, over 1 TB of datasets) for data association pairing and provided insights into meaningful data association and trends.\n\u2022 Developed pipelines for test data.\n\u2022 Enhanced statistical models(linear mixed models) for predicting the best products for commercialization\nusing Machine Learning Linear regression models, KNN and K-means clustering algorithms.\n\u2022 Builds machine learning models on independent AWS EC2 server to enhance data quality.\n\u2022 Handle Unstructured Data to derive some information from which helps in development of the company.\n\u2022 Finding the sentiment about the organization using Text Mining and NLP techniques.', u""Data Analyst\nATOM SOLUTIONS\nJanuary 2015 to June 2016\nProject:\n\u2022 Creating a web application to narrow job search for job seekers.\n\u2022 Scheduled jobs to automate different database related activities such as backups, maintaining index, and monitoring disk space and backup verification\n\u2022 Using text mining techniques to infer information from unstructured data.\n\u2022 Used unsupervised learning algorithms on job description to classify them into different clusters.\n\u2022 Applied Clustering Algorithms such as K-Means to categorize customers into certain groups.\n\u2022 Data cleansing, transformation and creating new variables using R\n\u2022 Coded JSP pages, Java Servlets in struts framework, prepared test cases for reviewing and understanding\nflow of application.\nProduct Sales:\n\u2022 Performed exploratory data analysis like calculation of descriptive statistics, detection of outliers,\nassumptions testing, factor analysis, etc., in R\n\u2022 Analyze and Prepare data, identify the patterns on dataset by applying historical models\n\u2022 Performed data transformation method for rescaling and normalizing variables\n\u2022 Exploratory data analysis and Feature engineering to best fit the regression model.\n\u2022 Applied predictive models to car maintenance records to predict car sales based upon it's usage and fuel\nefficiency.\n\u2022 Implemented forecasting models to predict car sales for to coming seasons so that we can meet the demand.""]","[u'Master of Science in Data Science', u'Bachelor of Technology in Computer Science and Engineering']","[u'SOUTH DAKOTA STATE UNIVERSITY\nAugust 2016 to December 2018', u'GITAM UNIVERSITY\nJune 2012 to April 2016']","degree_1 : Master of Science in Data Science, degree_2 :  Bachelor of Technology in Compter Science and Engineering"
0,https://resumes.indeed.com/resume/0e72527492b2dacb,"[u'Data Scientist\nFresh Direct - Long Island, NY\nAugust 2017 to Present\nDescription: Fresh Direct is an online grocer that delivers to residences and offices in the New York City metropolitan area. It also offers next-day delivery to much of New York City and parts of Nassau and Westchester Counties, New York; Fairfield County, Connecticut; Hoboken, Newark, and Jersey City; Philadelphia, Pennsylvania; and Washington, DC. Fresh Direct custom-prepares groceries and meals for its customers, a manufacturing practice called Just In Time that reduces waste and improves quality and freshness. The service is popular for its distribution of organic food and locally grown items, as well as items that consumers see in supermarkets daily. It also delivers numerous kosher foods and is recognized by the Marine Stewardship Council as a certified sustainable seafood vendor.\n\nResponsibilities:\n\u2022 Improving Fraud Detection using Digital Links at Amazon, Seattle.\n\u2022 Scaled up to Machine Learning Pipelines: 4600 processors, 35000 GB memory achieving 5-minute execution.\n\u2022 Configured the project on Web-Sphere 6.1 application servers.\n\u2022 Handled 2+ TB data with graphs up to 130 GB (50M nodes, 100M edges) using single-node in-disk scaling.\n\u2022 Developed a Machine Learning test-bed with 24 different model learning and feature learning algorithms.\n\u2022 By thorough systematic search, demonstrated performance surpassing the state-of-the-art (deep learning).\n\u2022 Up to 10 times more accurate predictions over existing state-of-the-art algorithms.\n\u2022 Developed in-disk, huge (100GB+), highly complex Machine Learning models.\n\u2022 Used SAX and DOM parsers to parse the raw XML documents.\n\u2022 Used RAD as Development IDE for web applications.\n\u2022 Demonstrated performances comparable to other state-of-the-art deep learning models.\n\u2022 Applied Machine Learning algorithms to diagnose blood loss from vital signs (ECG, HF, GSR, etc.)\n\u2022 Devised and implemented a Vehicle Speed Detector using low-power LEDs and field-tested for robustness.\n\u2022 National Highways Authority (Govt. of India) is evaluating the design for installations across the country.\n\u2022 IIT Madras has installed the speed detectors across the institute for permanent speed limit enforcement.\n\u2022 Developed & tested feature tracking algorithms for Intelligent Transportation Systems Computer Vision.\n\u2022 Analyzed SIFT feature descriptors and their resilience to changes in illumination.\n\u2022 Devised a novel machine learning algorithm for classification of ECG abnormalities.\n\u2022 Designed a new Machine Learning pipeline to replace existing prod: AUC performance Increase from 83% to 90%.\n\u2022 Deployed GUI pages by using JSP, JSTL, HTML, DHTML, XHTML, CSS, JavaScript, and AJAX.\n\u2022 Long Short-Term Memory Recurrent Neural Networks (LSTM RNNs) learnt using Deep Learning techniques applied to Problem X.\n\u2022 LSTM RNNs applied to Problem Y.\n\nEnvironment: R 9.0, Informatica 9.0, ODS, OLTP, Oracle 10g, Hive, OLAP, DB2, Metadata, MS Excel, Mainframes MS Vision, Rational Rose.', u""Data Scientist\nBlack Rock Inc - Wilmington, DE\nMay 2016 to July 2017\nDescription: Black Rock, Inc. is an American global investment management corporation based in New York City. Founded in 1988, initially as a risk management and fixed income institutional asset manager, Black Rock is the world's largest asset manager with $6.3 trillion in assets under management as of December 2017. Black Rock operates globally with 70 offices in 30 countries and clients in 100 countries. Due to its power and the sheer size and scope of its financial assets and activities, Black Rock has been called the world's largest shadow bank.\n\nResponsibilities:\n\u2022 Manipulating, cleansing & processing data using Excel, Access and SQL.\n\u2022 Responsible for loading, extracting and validation of client data.\n\u2022 Liaising with end-users and 3rd party suppliers.\n\u2022 Analyzing raw data, drawing conclusions & developing recommendations\n\u2022 Writing T-SQL scripts to manipulate data for data loads and extracts.\n\u2022 Developing data analytical databases from complex financial source data.\n\u2022 Performing daily system checks.\n\u2022 Data entry, data auditing, creating data reports & monitoring all data for accuracy.\n\u2022 Designing, developing and implementing new functionality.\n\u2022 Monitoring the automated loading processes.\n\u2022 Advising on the suitability of methodologies and suggesting improvements.\n\u2022 Carrying out specified data processing and statistical techniques.\n\u2022 Supplying qualitative and quantitative data to colleagues & clients.\n\u2022 Using Informatica & SAS to extract transform & load source data from transaction\nsystems.\n\u2022 Creating data pipelines using big data technologies like Hadoop, spark etc.\n\u2022 Creating statistical models using distributed and standalone models to build various diagnostics, predictive and prescriptive solution.\n\u2022 Utilize a broad variety of statistical packages like SAS, R , MLIB, Graphs, Hadoop, Spark , Map-Reduce, Pig and others\n\u2022 Refine and train models based on domain knowledge and customer business objectives\n\u2022 Deliver or collaborate on delivering effective visualizations to support the client business objectives\n\u2022 Communicate to your peers and managers promptly as and when required.\n\u2022 Produce solid and effective strategies based on accurate and meaningful data reports and analysis and/or keen observations.\n\u2022 Establish and maintain communication with clients and/or team members; understand needs, resolve issues, and meet expectations\n\u2022 Developed web applications using .net technologies; work on bug fixes/issues that arise in the production environment and resolve them at the earliest.\n\nEnvironment: SQL Server 2008R2/2005 Enterprise, SSRS, SSIS, Crystal Reports, Hadoop, Windows Enterprise Server 2000, DTS, SQL Profiler, and Query Analyzer."", u""Data Scientist\nIntrepid Potash Inc - Denver, CO\nJanuary 2015 to April 2016\nDescription: Intrepid Potash, Inc. produces and markets muriate of potash and langbeinite under the Trio brand name primarily in the United States.\nResponsibilities:\n\n\u2022 Data mining using state-of-the-art methods.\n\u2022 Extending company's data with third party sources of information when needed.\n\u2022 Enhancing data collection procedures to include information that is relevant for building analytic systems.\n\u2022 Processing, cleansing, and verifying the integrity of data used for analysis.\n\u2022 Doing ad-hoc analysis and presenting results in a clear manner.\n\u2022 Creating automated anomaly detection systems and constant tracking of its performance.\n\u2022 Strong command of data architecture and data modelling techniques.\n\u2022 Hands on experience with commercial data mining tools such as Splunk, R, Map reduced, Yarn, Pig, Hive, Floop, Oozie, Scala, HBase, Master HDFS, Sqoop, Spark, Scala (Machine learning tool) or similar software required depending on seniority level in job field.\n\u2022 Developed scalable machine learning solutions within a distributed computation framework (e.g. Hadoop, Spark, Storm etc.).\n\u2022 Utilizing NLP applications such as topic models and sentiment analysis to identify trends and patterns within massive data sets.\n\u2022 Knowledge in ML & Statistical libraries (e.g. Scikit-learn, Pandas).\n\u2022 Having knowledge to build predict models to forecast risks for product launches and operations and help predict workflow and capacity requirements for TRMS operations\n\u2022 Having experience with visualization technologies such as Tableau\n\u2022 Draw inferences and conclusions, and create dashboards and visualizations of processed data, identify trends, anomalies\n\u2022 Generation of TLFs and summary reports, etc. ensuring on-time quality delivery.\n\u2022 Participated in client meetings, teleconferences and video conferences to keep track of project requirements, commitments made and the delivery thereof.\n\u2022 Solved analytical problems, and effectively communicate methodologies and results\n\u2022 Worked closely with internal stakeholders such as business teams, product managers, engineering teams, and partner teams.\n\u2022 Created automated metrics using complex databases.\n\u2022 Foster culture of continuous engineering improvement through mentoring, feedback, and metrics.\n\nEnvironment: Erwin r9.0, Informatica 9.0, ODS, OLTP, Oracle 10g, Hive, OLAP, DB2, Metadata, MS Excel, Mainframes MS Visio, Rational Rose, Requisite Pro, Hadoop, PL/SQL, etc."", u'Data Scientist\nTyler Technologies Inc - Dallas, TX\nMay 2013 to December 2014\nDescription: Tyler Technologies, Inc. provides integrated information management solutions and services for the public sector with a focus on local governments in the United States and internationally.\n\nResponsibilities:\n\u2022 Statistical Modeling with ML to bring Insights in Data under guidance of Principal Data Scientist\n\u2022 Data modeling with Pig, Hive, Impala.\n\u2022 Ingestion with Sqoop, Flume.\n\u2022 Used SVN to commit the changes into the main EMM application trunk.\n\u2022 Understanding and implementation of text mining concepts, graph processing and semi structured and unstructured data processing.\n\u2022 Worked with Ajax API calls to communicate with Hadoop through Impala Connection and SQL to render the required data through it .These API calls are similar to Microsoft Cognitive API calls.\n\u2022 Good grip on Cloudera and HDP ecosystem components.\n\u2022 Used Elastic Search (Big Data) to retrieve data into application as required.\n\u2022 Performed Map Reduce Programs those are running on the cluster.\n\u2022 Developed multiple Map-Reduce jobs in java for data cleaning and preprocessing.\n\u2022 Analyzed the partitioned and bucketed data and compute various metrics for reporting.\n\u2022 Involved in loading data from RDBMS and web logs into HDFS using Sqoop and Flume.\n\u2022 Worked on loading the data from MySQL to HBase where necessary using Sqoop.\n\u2022 Developed Hive queries for Analysis across different banners.\n\u2022 Extracted data from Twitter using Java and Twitter API. Parsed JSON formatted twitter data and uploaded to database.\n\u2022 Launching Amazon EC2 Cloud Instances using Amazon Images (Linux/ Ubuntu) and Configuring launched instances with respect to specific applications.\n\u2022 Exported the result set from Hive to MySQL using Sqoop after processing the data.\n\u2022 Analyzed the data by performing Hive queries and running Pig scripts to study customer behavior.\n\u2022 Have hands on experience working on Sequence files, AVRO, HAR file formats and compression.\n\u2022 Used Hive to partition and bucket data.\n\u2022 Experience in writing Map-Reduce programs with Java API to cleanse Structured and unstructured data.\n\u2022 Wrote Pig Scripts to perform ETL procedures on the data in HDFS.\n\u2022 Created HBase tables to store various data formats of data coming from different portfolios.\n\u2022 Worked on improving performance of existing Pig and Hive Queries.\n\nEnvironment: SQL/Server, Oracle 9i, MS-Office, Teradata, Informatica, ER Studio, XML, Business Objects, HDFS, Teradata 14.1, JSON, HADOOP (HDFS), Map-Reduce, PIG, Spark, R Studio, MAHOUT, JAVA, HIVE, AWS..', u'Data Architect/Data Modeler\nCorporation Bank - Mangalore, Karnataka\nDecember 2011 to April 2013\nDescription: Corporation Bank is a public-sector banking company headquartered in Mangalore, India. The bank has a pan-Indian presence. Presently, the bank has a network of 2,440 fully automated CBS branches, 3,040 ATMs, and 4,724 branchless banking units across the country.\n\nResponsibilities:\n\u2022 Worked with large amounts of structured and unstructured data.\n\u2022 Knowledge in Machine Learning concepts (Generalized Linear models, Regularization, Random Forest, Time Series models, etc.)\n\u2022 Worked in Business Intelligence tools and visualization tools such as Business Objects, Tableau, ChartIO, etc.\n\u2022 Deployed GUI pages by using JSP, JSTL, HTML, DHTML, XHTML, CSS, JavaScript, and AJAX.\n\u2022 Configured the project on Web-Sphere 6.1 application servers\n\u2022 Implemented the online application by using Core Java, Jdbc, JSP, Servlets and EJB 1.1, Web Services, SOAP, WSDL.\n\u2022 Handled end-to-end project from data discovery to model deployment.\n\u2022 Monitoring the automated loading processes.\n\u2022 Communicated with other Health Care info by using Web Services with the help of SOAP, WSDL JAX-RPC\n\u2022 Used Singleton, factory design pattern, DAO Design Patterns based on the application requirements\n\u2022 Used SAX and DOM parsers to parse the raw XML documents\n\u2022 Used RAD as Development IDE for web applications.\n\u2022 Preparing and executing Unit test cases\n\u2022 Used Log4J logging framework to write Log messages with various levels.\n\u2022 Involved in fixing bugs and minor enhancements for the front-end modules.\n\u2022 Implemented Microsoft Visio and Rational Rose for designing the Use Case Diagrams, Class model, Sequence diagrams, and Activity diagrams for SDLC process of the application\n\u2022 Doing functional and technical reviews\n\u2022 Maintenance in the testing team for System testing/Integration/UAT.\n\u2022 Guaranteeing quality in the deliverables.\n\u2022 Conducted Design reviews and Technical reviews with other project stakeholders.\n\u2022 Was a part of the complete life cycle of the project from the requirements to the production support\n\u2022 Created test plan documents for all back-end database modules\n\u2022 Implemented the project in Linux environment.\n\nEnvironment: R 3.0, Erwin 9.5, Tableau 8.0, MDM, QlikView, MLLib, PL/SQL, HDFS, Teradata 14.1, JSON, HADOOP (HDFS), Map-Reduce, PIG, Spark, R Studio, MAHOUT, JAVA, HIVE, AWS.', u'Data Analyst/Data Modeler\nGMR Group - Bengaluru, Karnataka\nMay 2009 to November 2011\nDescription: GMR Group is an infrastructural company headquartered in Bengaluru. The company was founded in 1978 by Grandhi Mallikarjuna Rao.\n\nResponsibilities:\n\u2022 Developed Internet traffic scoring platform for ad networks, advertisers and publishers (rule engine, site scoring, keyword scoring, lift measurement, linkage analysis).\n\u2022 Responsible for defining the key identifiers for each mapping/interface.\n\u2022 Clients include eBay, Click Forensics, Cars.com, Turn.com, Microsoft, and Looksmart.\n\u2022 Designed the architecture for one of the first analytics 3.0. Online platforms: all-purpose scoring, with on-demand, SaaS, API services. Currently under implementation.\n\u2022 Web crawling and text mining techniques to score referral domains, generate keyword taxonomies, and assess commercial value of bid keywords.\n\u2022 Developed new hybrid statistical and data mining technique known as hidden decision trees and hidden forests.\n\u2022 Reverse engineering of keyword pricing algorithms in the context of pay-per-click arbitrage.\n\u2022 Implementation of Metadata Repository, Maintaining Data Quality, Data Cleanup procedures, Transformations, Data Standards, Data Governance program, Scripts, Stored Procedures, triggers and execution of test plans\n\u2022 Performed data quality in Talend Open Studio.\n\u2022 Coordinated meetings with vendors to define requirements and system interaction agreement documentation between client and vendor system.\n\u2022 Automated bidding for advertiser campaigns based either on keyword or category (run-of-site) bidding.\n\u2022 Creation of multimillion bid keyword lists using extensive web crawling. Identification of metrics to measure the quality of each list (yield or coverage, volume, and keyword average financial value).\n\u2022 Enterprise Metadata Library with any changes or updates.\n\u2022 Document data quality and traceability documents for each source interface.\n\u2022 Establish standards of procedures.\n\u2022 Generate weekly and monthly asset inventory reports.\n\nEnvironment: Erwin r7.0, SQL Server 2000/2005, Windows XP/NT/2000, Oracle 8i/9i, MS-DTS, UML, UAT, SQL Loader, OOD, OLTP, PL/SQL, MS Visio, Informatica.']",[u'Bachelor of Computer Science & Technology in TOOLS AND TECHNOLOGIES'],[u'Quality Center\nOctober 2011'],degree_1 : Bachelor of Compter Science & Technology in TOOLS AND TECHNOLOGIES
0,https://resumes.indeed.com/resume/8033f5571704646a,"[u'Data Scientist\nBlue Cross and Blue Shield of North Carolina - Durham, NC\nMay 2016 to Present\n\u25cf Created Tableau dashboards to help operational managers track current claims inventory across multiple lines of business and to guide corrective actions to minimize undesirable operational expenses.\n\u25cf Trained a binary classification predictive model to identify claims with processing errors, and provided\nactionable information on the type of error, leading to potential cost avoidance of over $1 million.\n\u25cf Forecasted volumes of received claims and created a tool to gauge future staffing needs.\n\u25cf Conducted a social network analysis to visualize potentially fraudulent payment arrangements.', u""Research Chemical Engineer\nRTI International - Research Triangle Park, NC\nSeptember 2005 to June 2015\nProject Manager and Technical Lead, Protective Garment Testing Program\n2005 to 2015\n\u25cf Developed a data acquisition strategy and programmed a dashboard for real-time monitoring and display of test chamber conditions to provide improved feedback and control.\n\u25cf Extracted quantitative data from digital images and conducted analysis to measure the amount of chemical-biological aerosol simulant deposited on skin.\n\u25cf Sharpened project management skills by leading multiple six-figure Department of Defense projects.\n\u25cf Strengthened communications skills by preparing dozens of technical reports and briefings for clients.\n\nDirector, Air Pollution Control Technology Center\n2010 to 2013\n\u25cf Managed RTI International's involvement in the (EPA's) Environmental Technology Verification\nProgram, completing the project within budget after inheriting it in a financially difficult state.\n\u25cf Guided individual projects through a complicated sequence of product evaluation, test plan\ndevelopment, facility oversight, data analysis, and reporting, resulting in thirteen EPA-verified products.\n\u25cf Developed client relationships by interfacing with manufacturers seeking testing of their technology."", u""Senior Site Operator\nAir Quality Laboratory, Washington University - St. Louis, MO\nSeptember 2002 to August 2005\n\u25cf Applied a temporal moving average filtering technique to determine spatial zones of representation for air pollutant data by separating short-duration, local events from regional patterns.\n\u25cf Coordinated and oversaw a team of graduate students conducting research on a full suite of commercial and prototype ambient particulate matter air quality samplers at an EPA-funded field research site.\n\nOther Data Science Projects\nInstitute for Advanced Analytics Practicum Project: Lowe's Home Improvement\nDesigned and developed an end user analytical interface, using SAS Enterprise Guide, which utilized advanced\nanalytical techniques on over 1 billion rows of transactional data to determine cross-product effects, forecast\nproduct demand, identify product drivers, and conduct market basket analysis with the end goal of improving\nprofitability and pricing for Lowe's Home Improvement.\n\nKaggle Competition: Animal Shelter Outcomes\nWith intake and outcome information including breed, color, sex, and age from the Austin Animal Center, our\nteam used machine learning techniques to predict the outcome for each animal. Beyond the Kaggle competition,\nwe developed and presented a web app to help predict outcomes for new intakes at the Austin Animal Shelter.\n\nCoursera Data Science Specialization Capstone Project: Text Prediction App\nCreated a lightweight, text prediction web app (R/shiny) with the goal of predicting the next word when given a\nstring of words as input. Used text analytics and natural language processing techniques to build and train a\npredictive model from compilations of English language news stories, blog posts, and tweets.""]","[u'Master of Science in Analytics', u'Graduate Certificate in Applied Statistics', u'Master of Science in Chemical Engineering', u'Bachelor of Science in Chemical Engineering']","[u'North Carolina State University Raleigh, NC\nMay 2016', u'Pennsylvania State University\nAugust 2014', u'Stanford University Stanford, CA\nJune 2000', u'Washington University St. Louis, MO\nMay 1998']","degree_1 : Master of Science in Analytics, degree_2 :  Gradate Certificate in Applied Statistics, degree_3 :  Master of Science in Chemical Engineering, degree_4 :  Bachelor of Science in Chemical Engineering"
0,https://resumes.indeed.com/resume/60cf4a62770cc9c1,"[u""Data Scientist\nWalgreens - Deerfield, IL\nJanuary 2017 to Present\nDescription: The Walgreens Company is an American company that operates as the second-largest pharmacy store chain in the United States. It specializes in filling prescriptions, health and wellness products, health information, and photo services.\n\nResponsibilities:\n\u2022 Extending company's data to third-party sources of information when needed.\n\u2022 Enhancing data collection procedures to include information that is relevant for building analytic systems\n\u2022 Processing, cleansing, and verifying the integrity of data used for analysis\n\u2022 Doing ad-hoc analysis and presenting results in a clear manner\n\u2022 Constant tracking of model performance\n\u2022 Excellent understanding of machine learning techniques and algorithms, such as Logistic Regression, SVM, Random Forests, Deep Learning etc.\n\u2022 Worked with Data Governance, Data quality, data lineage, Data architect to design various models.\n\u2022 Independently coded new programs and designed Tables to load and test the program effectively for the given POC's using with Big Data/Hadoop.\n\u2022 Designed data models and data flow diagrams using Erwin and MS Visio.\n\u2022 As an Architect implemented MDM hub to provide clean, consistent data for an SOA implementation.\n\u2022 Developed, Implemented & Maintained the Conceptual, Logical & Physical Data Models using Erwin for forwarding/Reverse Engineered Databases.\n\u2022 Experience with common data science toolkits, such as R, Python, Spark, etc.\n\u2022 Good applied statistics skills, such as statistical sampling, testing, regression, etc.\n\u2022 Build analytic models using a variety of techniques such as logistic regression, risk scorecards, and pattern recognition technologies.\n\u2022 Analyze and understand large amounts of data to determine suitability for use in models and then work to segment the data, create variables, build models and test those models.\n\u2022 Work with technical and development teams to deploy models. Build Model Performance Reports and Modeling Technical Documentation to support each of the models for the product line.\n\u2022 Performed Exploratory Data Analysis and Data Visualizations using R, and Tableau.\n\u2022 Perform a proper EDA, Univariate and bivariate analysis to understand the intrinsic effect/combined.\n\u2022 Established Data architecture strategy, best practices, standards, and roadmaps, Ruby. - Primary research expertise in Probabilistic Planning - Markov Decision Processes (MDPs)\nand Partially Observable MDPs (POMDPs).\n\u2022 Extensive research experience in Reinforcement Learning, Utility Theory, Distributed Con- straint Optimization, Classification, and Probabilistic Graphical Models.\n\u2022 General interest in topics related to Artificial Intelligence, including Machine Learning, Au- automated Planning, Multi-agent System, and Computational Biology.\n\u2022 Responsible for all embroidered products from start to finish. Monitored Machine operations to detect problems such as defective stitching, breaks in thread, or Machine malfunctions. Met quality control standards daily.\n\u2022 Lead the development and presentation of a data analytics data-hub prototype with the help of the other members of the emerging solutions team.\n\u2022 Involved in the analysis of Business requirement, Design, and Development of the High level and Low-level designs, Unit, and Integration testing.\n\u2022 Worked with several R packages including knitr, dplyr, SparkR, CausalInfer, spacetime.\n\u2022 Interacted with the other departments to understand and identify data needs and requirements\n\u2022 Researched and built Neural Network (NN) algorithms from the ground up using C++ and Python,\nhosted on AWS\n\u2022 Experimented and implemented multiple performance improvements for the deep Learning algorithm\ne.g. parallel processing, multi-threading, dropout, distributed computing framework\n\u2022 Used Swarm algorithms and reinforcement Learning to model outer space battle tactics in zero-gravity deep space.\n\u2022 Developed and implemented all-new deep learning architecture for enhanced predictive modeling, security and data\n\nEnvironment: Deep Learning, Machine Learning, UNIX, Python 3.5.2, MLlib, SAS, regression, logistic regression, Hadoop, NoSQL, Teradata, OLTP, random forest, OLAP, HDFS, ODS, Ruby."", u'Data Scientist\nLa-Z-Boy - Monroe, MI\nNovember 2015 to December 2016\nDescription: La-Z-Boy is an American furniture manufacturer based in Monroe, Michigan, USA, that makes home furniture, including upholstered recliners, sofas, stationary chairs, lift chairs and sleeper sofas.\n\nResponsibilities:\n\u2022 Involved in defining the source to target data mappings, business rules, and data definitions.\n\u2022 Performing data profiling on various source systems that are required for transferring data to ECH using\nInformatica Analyst tool 10.1/9.6.1.\n\u2022 Defining the list codes and code conversions between the source systems and the data mart using ReferenceData Management (RDM).\n\u2022 Utilizing Informatica toolset (Informatica Data Explorer, and Informatica Data Quality) to analyze legacy data for Data Profiling.\n\u2022 Worked on DTS Packages, DTS Import/Export for transferring data between SQL Server 2000 to 2005\n\u2022 Involved in upgrading DTS packages to SSIS packages (ETL).\n\u2022 Performing an end to end Informatica ETL Testing for these custom tables by writing complex SQL Queries on the source database and comparing the results against the target database.\n\u2022 Using HP Quality Center v 11 for defect tracking of issues.\n\u2022 Extracting the source data from Oracle tables, MS SQL Server, sequential files, and excel sheets.\n\u2022 Developing and maintaining Data Dictionary to create metadata reports for technical and business purpose.\n\u2022 Predictive modeling using state-of-the-art methods\n\u2022 Build and maintain dashboard and reporting based on the statistical models to identify and track key metrics and risk indicators.\n\u2022 Parse and manipulate raw, complex data streams to prepare for loading into an analytical tool.\n\u2022 Migrating Informatica mappings from SQL Server to Netezza Foster culture of continuous engineering improvement through mentoring, feedback, and metrics\n\u2022 Broad knowledge of programming, and scripting (especially in R / Java / Python)\n\u2022 Implemented Event Task for executing Application Automatically.\n\u2022 Creating data pipeline with Amazon Redshift to Tableau for faster load and real-time data potential in analytics reports and machine Learning projects.\n\u2022 Suggested and implemented SEO code changes to several client websites.\n\u2022 Worked on a team to develop/style an Android application that output/stored resulting medical MR Machine after they selected answers from qualifiers.\n\u2022 Troubleshoot and resolved bugs in .NET applications to ensure optimal development environment.\n\u2022 Involved in developing Patches & Updates Module.\n\u2022 Proven experience building sustainable and trustful relationships with senior leaders. Used Python to assist in various backpropagation based neural network projects, such as sound signal processing and data mining programs, that were used in paper publications such as the EMNLP.\n\u2022 Built test environments to assess the quality of the projects and tweaked the programs accordingly.\n\u2022 Studied research papers on Deep Learning and linear algebraic concepts and applied them to programs.\n\nEnvironment: Erwin 8, Teradata 13, SQL Server 2008, Oracle 9i, SQL*Loader, PL/SQL, ODS, OLAP, OLTP, SSAS, Informatica Power Center 8.1.', u""Data Scientist\nAcuity Insurance, Wisconsin\nJanuary 2014 to October 2015\nDescription: Acuity Insurance is an insurance company with headquarters in Sheboygan, Wisconsin. The company is the 57th largest insurer in the United States.\n\nResponsibilities:\n\u2022 Responsible for design and development of advanced R/Python programs to prepare to transform and harmonize data sets in preparation for modeling.\n\u2022 Identifying and executing process improvements, hands-on in various technologies such as Oracle, Informatica, and Business Objects.\n\u2022 Designed the prototype of the Data mart and documented possible outcome from it for end-user.\n\u2022 Involved in business process modeling using UML\n\u2022 Developed and maintained data dictionary to create metadata reports for technical and business purpose.\n\u2022 Handled importing data from various data sources, performed transformations using Hive, Map Reduce, and loaded data into HDFS.\n\u2022 Interaction with Business Analyst, SMEs, and other Data Architects to understand Business needs and functionality for various project solutions.\n\u2022 Created SQL tables with referential integrity and developed queries using SQL, SQL*PLUS, and PL/SQL.\n\u2022 Worked closely with business, data governance, SMEs, and vendors to define data requirements.\n\u2022 Design, coding, unit testing of ETL package source marts and subject marts using Informatica ETL processes for Oracle database.\n\u2022 Learned principles and considerations of designing a Machine Learning recommendation\nthe, including the concepts behind the company's proprietary clustering algorithm\n\u2022 Handle conversation with Country DMteam, Finance team, BI team and regional team to get sign off for the products logic.\n\u2022 Actively tracking and updates project status to country DM heads, regional heads and project coordinators.\n\u2022 Analyzed, Designed and Processed large datasets (over 200GB) of 24 - 36 base paired nucleotides\n\u2022 Analyzed large sequencing datasets for identification of key candidates that can be prioritized for further experiments.\n\u2022 Sequenced, assembled and annotated the transcriptome (51 bases paired reads, millions of data points).\n\u2022 Created Perl programs for combining and analyzing transcriptome and small RNA datasets to identify novel and conserved microRNAs\n\u2022 Established the core bioinformatics infrastructure for the university department\n\u2022 Leveraged working with cloud computation (AWS, UC Farm cluster), thus reducing the establishment costs\n\u2022 Provided leadership and support to colleagues and research teams in data analysis and techniques at UC Davis and with collaborative teams\n\nEnvironment: SSMS 17.x, SSIS 2016, Power BI 2.37, Oracle 12c/11g, MS Office 2013, Microsoft reporting tools, Big Data, Hadoop 2.8.1."", u'Data Scientist\nConair Corporation\nOctober 2012 to December 2013\nDescription: Conair Corporation is an American company which sells small appliances, personal care products, and health and beauty products for both professionals and consumers. It was founded in 1959 and has since expanded to include ten product divisions.\n\nResponsibilities:\n\u2022 Involved with Data Analysis primarily Identifying Data Sets, Source Data, Source Meta Data, Data Definitions and Data Formats\n\u2022 Performance tuning of the database, which includes indexes, and optimizing SQL statements, monitoring the server.\n\u2022 Wrote simple and advanced SQL queries and scripts to create standard and ad hoc reports for senior managers.\n\u2022 Collaborated the data mapping document from a source to target and the data quality assessments for the source data.\n\u2022 Used Expert level understanding of different databases in combinations for Data extraction and loading, joining data extracted from different databases and loading to a specific database.\n\u2022 Co-ordinate with various business users, stakeholders, and SME to get Functional expertise, design, and business test scenarios review, UAT participation, and validation of financial data.\n\u2022 Manipulating/mining data from database tables (Redshift, Oracle, Data Warehouse)\n\u2022 Create automated metrics using complex databases.\n\u2022 Providing analytical network support to improve quality and standard work result.\n\u2022 Root cause research to identify process breakdowns within departments and providing data through use of various skill sets to find solutions to break down\n\u2022 Foster culture of continuous engineering improvement through mentoring, feedback, and metrics\n\u2022 Broad knowledge of programming, and scripting (especially in R / Java / Python)\n\u2022 Implemented Event Task for executing Application Automatically.\n\u2022 Troubleshoot and resolved bugs in .NET applications to ensure optimal development environment.\n\u2022 Created views, queries, and data warehouse reports using SSRS providing management with financial information from SQL Server production databases.\n\u2022 Involved in developing Patches & Updates Module.\n\nEnvironment: Machine learning, AWS, MS Azure, Cassandra, Spark, HDFS, Hive, Pig, Linux, Python (Scikit-Learn/SciPy/NumPy/Pandas), R, SAS, SPSS, MySQL, Eclipse, PL/SQL, SQL connector, Tableau.', u'Data Architect/Data Modeler\nPidilite Industries - IN\nJanuary 2011 to September 2012\nDescription: Pidilite Industries Limited is an Indian-based adhesive manufacturing company. It also sells art material, construction chemicals, and other industrial chemicals. Pidilite markets the Fevicol range of adhesives.\n\nResponsibilities:\n\u2022 Participated in all phases of data mining; data collection, data cleaning, developing models, validation, visualization and performed Gap analysis.\n\u2022 data manipulation and Aggregation from a different source using Nexus, Toad, BusinessObjects, PowerBI, and SmartView.\n\u2022 Implemented Agile Methodology for building an internal application.\n\u2022 Focus on integration overlap and Informatica newer commitment to MDM with the acquisition of Identity Systems.\n\u2022 Good knowledge of Hadoop architecture and various components such as HDFS, JobTracker, TaskTracker, NameNode, DataNode, SecondaryNameNode, and MapReduce concepts.\n\u2022 As Architect delivered various complex OLAP databases/cubes, scorecards, dashboards, and reports.\n\u2022 Programmed a utility in Python that used multiple packages (scipy, numpy, pandas)\n\u2022 Involved in fixing bugs and minor enhancements to the front-end modules.\n\u2022 Implemented Microsoft Visio and Rational Rose for designing the Use Case Diagrams, Class model, Sequence diagrams, and Activity diagrams for SDLC process of the application\n\u2022 Maintenance in the testing team for System testing/Integration/UAT\n\u2022 Guaranteeing quality in the deliverables.\n\u2022 Conducted Design reviews and Technical reviews with other project stakeholders.\n\u2022 Was a part of the complete life cycle of the project from the requirements to the production support\n\u2022 Created test plan documents for all back-end database modules\n\u2022 Performs complex pattern recognition of financial time series data and forecast.\n\u2022 Maintaining relationship with clients to achieve quality service norms by resolving their service-related critical issues.\n\u2022 Implemented the Slowly changing dimension scheme (Type III) for most of the dimensions.\n\u2022 Implemented the standard naming conventions for the fact and dimension entities and attributes of the Logical and physical model.\n\u2022 Involved in defining the Source To Target data mappings, business rules, data definitions.\n\u2022 Programmed a utility in Python that used multiple packages (Scipy, Numpy, pandas).\n\u2022 Conduct complex ad-hoc analytics, summaries, and recommendations, as required.\n\nEnvironment: R 3.0, Erwin 9.5, Tableau 8.0, MDM, QlikView, MLlib, PL/SQL, HDFS, Teradata 14.1, JSON, HADOOP (HDFS), MapReduce, Pig, Spark, R Studio, MAHOUT, JAVA, HIVE, AWS.', u'Data Analyst\nSun Pharmaceutical - Mumbai, Maharashtra\nJune 2009 to December 2010\nDescription: Sun Pharmaceutical Industries Limited is an Indian multinational pharmaceutical company headquartered in Mumbai, Maharashtra that manufactures and sells pharmaceutical formulations and active pharmaceutical.\n\nResponsibilities:\n\u2022 Involved in analysis of Business requirement, Design, and Development of High level and Low-level designs, Unit, and Integration testing\n\u2022 Performed data analysis and data profiling using complex SQL on various sources systems including Teradata, SQL Server.\n\u2022 Developed the logical data models and physical data models that confine existing condition/potential status data fundamentals and data flows using ER Studio\n\u2022 Created the conceptual model for the data warehouse using Erwin data modeling tool.\n\u2022 Reviewed and implemented the naming standards for the entities, attributes, alternate keys, and primary keys for the logical model.\n\u2022 Performed second and third normalizations for ERdatamodel of OLTP system\n\u2022 Worked with data compliance teams, Data governance team to maintain data models, Metadata, Data Dictionaries; define source fields and its definitions.\n\u2022 Involved in the daily maintenance of the database that involved monitoring the daily run of the scripts as well as troubleshooting in the event of any errors in the entire process.\n\u2022 Investigate data from multiple systems, at different timescales and in complex formats to discover hidden relationships and useful information.\n\u2022 Data analysis may encompass a range of analytical capabilities including aggregations, statistical analysis, regression, time-series analysis, clustering, network analysis and other statistical modeling techniques.\n\u2022 Document implementation and design details for model choices, rationale, and reasoning.\n\u2022 Demonstrate and deliver rapid prototypes for the models and work with QA to ensure quality.\n\u2022 The primary focus will be in applying data mining techniques, doing statistical analysis, and building a high-quality prediction model to support various Business/service functions.\n\u2022 Enhancing data collection procedures to include information that is relevant for building analytic systems\n\u2022 Processing, cleansing, and verifying the integrity of data used for analysis\n\u2022 Doing ad-hoc analysis and presenting results in a clear manner\n\u2022 Building forecast model and build predictive models with the relevant information\n\u2022 Support different functions like Sales, Finance, Supply chain, competitive intelligence etc. on their analytics\n\u2022 Implementing and developing data visualization models for the business presentations\n\u2022 Research, design, implement and validate cutting-edge algorithms to analyze diverse sources of data to achieve targeted outcomes.\nEnvironment: ER Studio 9.7, Tableau 9.03, AWS, Teradata 15, MDM, GIT, Unix, Python 3.5.2, MLlib, SAS, regression, logistic regression.']","[u""Bachelor's""]",[u''],"degree_1 : ""Bachelors"""
0,https://resumes.indeed.com/resume/15bc9c68d421b5a9,"[u'Population Health Data Scientist\nArizona Care Network - Phoenix, AZ\nSeptember 2016 to November 2017\nUsing advanced statistical method to turn complex healthcare data into non-technical recommendations and meaningful actions.\n\nExperienced in population health management, payer and membership utilization, total medical cost management, quality metrics and management, government programs (Medicare and Medicaid), providers analysis (minimum adequacy etc.), and maintaining the integrity of Enterprise Data Warehouse\n\u2022 Specific projects include analyzing the claim and membership eligibility data of Medicare (using Claim and Claim Line Feed files) and multiple commercial payers (UnitedHealth, Aetna, Intel etc.) with SAS\n\n\u2022 Building attribution logic according to CMS\u2019s standard for MSSP and NextGen population with SAS\n\u2022 Analysis and profit-maximizing plans for the network provider data in Salesforce\n\u2022 Utilization reports for ambulatory surgery center etc.\n\u2022 Comprehensive understanding and evaluation of the risk adjustment models of CMS\u2019s hierarchical conditional categories (HCC), HHS-HCC for commercial health plans, and Milliman Advanced Risk Adjusters (MARA) along with their application in patients\u2019 cost management', u'MRI Physicist\nCarnegie Mellon University - Pittsburgh, PA\nAugust 2007 to January 2012\nResearch responsibilities including experimental design, imaging acquisition and processing, data analysis, and manuscript preparation to study cardiac diseases in small-animal models\n\u2022 Responsible for supervising undergraduate research students', u'Research Associate\nMallinckrodt Institute of Radiology, Washington University School of Medicine - St. Louis, MO\nAugust 2003 to July 2007\nResponsibilities including experimental and pulse sequence design, imaging acquisition, imaging processing and data interpretation\n\u2022 Responsibilities also including supervising undergraduate/graduate research students', u'Research Assistant\nDept. of Chemical and Biomedical Engineering - Cleveland, OH\nAugust 1999 to July 2003\nResponsibilities including experimental design, imaging acquisition, and data analysis\n\nAwards']","[u'M.S. in Statistics', u'M.S. in Chemical Engineering', u'B.S. in Chemical Engineering']","[u'Arizona State University Tempe, AZ\nMay 2016', u'Tianjin University \u5929\u6d25\u5e02\nMarch 1999', u'Tianjin University \u5929\u6d25\u5e02\nJuly 1996']","degree_1 : M.S. in Statistics, degree_2 :  M.S. in Chemical Engineering, degree_3 :  B.S. in Chemical Engineering"
0,https://resumes.indeed.com/resume/135d258c79c685f1,"[u""Machine Learning/Data Scientist\nLarsen & Toubro Infotech - Houston, TX\nFebruary 2017 to Present\nDescription: Larsen & Toubro Infotech (LTI), a subsidiary of Larsen & Toubro is a global IT solutions & services company based in Mumbai, India. LTI is ranked number in India IT companies in 2013-2014. The company has 39 registered offices in 27 countries. It employs standards of the Software Engineering Institute's (SEI) Capability Maturity Model Integration (CMMI) and is a Maturity Level 5 assessed organization. Sanjay Jalona is the new MD & CEO of LTI.\n\nResponsibilities:\n\u2022 Design and develop state-of-the-art deep-learning / machine-learning algorithms for analyzing image and video data among others.\n\u2022 Develop and implement innovative AI and machine learning tools that will be used in the Risk\n\u2022 Experience with TensorFlow, Caffe and other Deep Learning frameworks.\n\u2022 Effective software development processes to customize and extend the computer vision and image processing techniques to solve new problems for Automation Anywhere.\n\u2022 Develop and implement innovative data quality improvement tools.\n\u2022 Will demonstrate cross-functional resource interaction to accomplish your goals.\n\u2022 Involved in Peer Reviews, Functional and Requirement Reviews.\n\u2022 Develop project requirements and deliverable timelines; execute efficiently to meet the plan timelines.\n\u2022 Creating and support a data management workflow from data collection, storage, analysis to training and validation.\n\u2022 Understanding requirements, significance of weld point data, energy efficiency using large datasets\n\u2022 Develop necessary connectors to plug ML software into wider data pipeline architectures.\n\u2022 Creating and support a data management workflow from data collection, storage, analysis to training and validation.\n\u2022 Identify and assess available machine learning and statistical analysis libraries (including regressors, classifiers, statistical tests, and clustering algorithms).\n\u2022 Design and build scalable software architecture to enable real-time / big-data processing.\n\u2022 Acquire business knowledge in the Firm's risk management processes.\n\u2022 Be very passionate about quality and have a strong sense of ownership on the work accomplished.\n\u2022 Be quick to learn new technologies as well as deliver on them in short order.\n\u2022 Taking responsibility for technical problem solving, creatively meeting product objectives and developing best practices.\n\u2022 Worked on requirements gathering for multiple functionality enhancements by engaging with business users and ascertaining their demands.\n\u2022 Involved in maintaining and uploading the Test Scripts.\n\u2022 Have a high sense of urgency to deliver projects as well as troubleshoot and fix data queries/ issues.\n\u2022 Work independently with R&D partners to understand requirements.\n\u2022 Understanding business process and Business problems thoroughly and forecasting the business using data science techniques.\n\u2022 Gathering required data from business users to achieve accurate training data for analysis.\n\u2022 Coordinate and communicate with technical teams for any data requirements.\n\u2022 Providing status of project to project manager and business team up to date.\n\u2022 Understanding the Business requirements based on Functional specification to design the ETL methodology in technical specifications.\n\u2022 Consolidation, standardization, matching Trillium for the unstructured flat file data.\n\u2022 Responsible for developing, support and maintenance for the ETL (Extract, Transform and Load) processes using Informatica Power Center 8.5.\n\nEnvironment:Microsoft Azure HDInsight -Hadoop, Hive, HBase, Azure SQL Data Warehouse, SQL Server 2012, Integration Service (SSIS), Analysis Service (SSAS), Reporting Service (SSRS), Power BI 2.3, Share Point 2010, Naveego, Telerik, Mongo DB, Spot Fire, Tableau 10."", u""Machine Learning/Data Scientist\nApple Inc\nDecember 2015 to January 2017\nDescription:Apple Inc. is an American multinational technology company headquartered in Cupertino, California that designs, develops, and sells consumer electronics, computer software, and online services. The company's hardware products include the iPhone smartphone, the iPad tablet computer, the Mac personal computer, the iPod portable media player, the Apple Watch smartwatch, the Apple TV digital media player, and the HomePod smart speaker.\n\nResponsibilities:\n\u2022 Participated in stake holders meetings to understand the business needs & requirements.\n\u2022 Involved in preparation & design of technical documents like Bus Matrix Document, PPDM Model, and LDM & PDM.\n\u2022 Designed framework for sales requirements & Lead team of 5.\n\u2022 Provided technical solutions on MS Azure HDInsight, Hive, HBase, Mongo DB, Telerik, Power BI, Spot Fire, Tableau, Azure SQL Data Warehouse Data Migration Techniques using BCP, Azure Data Factory, and Fraud prediction using Azure Machine Learning.\n\u2022 Participated in Business meetings to understand the business needs & requirements.\n\u2022 Prepared & designed technical documents like OLAPDesignDocument, ConceptualModel, and LDM&PDM.\n\u2022 Up skilled / Trained team on SQL Server 2012 for incoming new requirements.\n\u2022 Provided technical solutions for OLAP design and reporting requirements.\n\u2022 Participated in Business meetings to understand the business needs & requirements.\n\u2022 Prepare ETLarchitect& design document which covers ETLarchitect, SSISdesign, extraction, transformation and loading of Duck Creek data into dimensional model.\n\u2022 Provide technical & requirement guidance to the team members for ETL -SSISdesign.\n\u2022 Participated in Business meetings to understand the business needs & requirements.\n\u2022 Design ETL framework and development.\n\u2022 Design Logical & Physical Data Model using MS Visio 2003 data modeler tool.\n\u2022 Prepare High Level Design (HLD) & Low Level Design (LLD) documents.\n\u2022 Provide technical & requirement guidance to the team members.\n\u2022 Manage development & enhancement of Project Change Request from client.\n\u2022 Participated in Architect solution meetings & guidance in Dimensional Data Modeling design.\nEnvironment: Apache, Spark MLlib, TensorFlow, Oryx 2, Accord.NET, Amazon Machine Learning (AML)Python, Django, Flask, ORM, Jinja 2, Mako, Naive Bayes, SVM,K- means, ANN, Regression."", u'Machine Learning/Data Scientist\nCabot Corporation\nFebruary 2014 to November 2015\nDescription:Cabot Corporation is an American specialty chemicals and performance materials company headquartered in Boston, Massachusetts.\n\nResponsibilities:\n\u2022 Design Dimensional Data Modeling using Erwintool.\n\u2022 Design ETLArchitect for SSIS&CubeArchitect for SSAS.\n\u2022 Leading 10 people team.\n\u2022 Estimation of work/task using MS Project Plan and allocation of task/work among team members.\n\u2022 Onsite team coordination - Daily / Weekly Status report / meeting.\n\u2022 Responsible for Business Analysis and Requirements Collection and Understanding Business Problems\n\u2022 Interact regularly with Business leaders to set and manage expectations aligned to group capabilities.\n\u2022 Understanding business process and Business problems thoroughly and forecasting the business using data science techniques.\n\u2022 Gathering required data from business users to achieve accurate training data for analysis.\n\u2022 Coordinate and communicate with technical teams for any data requirements.\n\u2022 Guided team to implement EDA part for given sales data and analyze the results. Involved in Gathering, exploring and cleaning the data.\n\u2022 Use of cutting edge data mining, machine learning techniques for building advanced customer solutions.\n\u2022 Assigning tasks to analytics team and reporting team and gathering inputs from them on regular basis.\n\u2022 Implemented techniques from artificial intelligence/machine learning to solve supervised and unsupervised learning problems.\n\u2022 Present results of analysis and prediction model evaluation to business executives.\n\u2022 Design, develop, maintain and communicate visual dashboards / reports and analysis based on business requirement needs\n\u2022 Providing status of project to project manager and business team up to date.\n\u2022 Understanding the Business requirements based on Functional specification to design the ETLmethodology in technical specifications.\nEnvironment: Erwin r9.0, Informatica 9.0, ODS, OLTP, Oracle 10g, Hive, OLAP, DB2, Metadata, MS Excel, Mainframes MS Visio, Rational Rose, Requisite Pro, Hadoop, PL/SQL, etc.', u""Machine Learning/Data Scientist\nPlacingIT - Palo Alto, CA\nNovember 2012 to January 2014\nDescription:PlacingIT is a Dallas-based IT staffing and recruiting company. We take tremendous pride in building responsive, lasting partnerships so we can present to our clients the highest-quality, perfect-fit candidates by focusing on efficiency, precision, and outstanding customer service.\nResponsibilities:\n\n\u2022 Developing propensity models for Retail liability products to drive proactive campaigns.\n\u2022 Extraction and tabulation of data from multiple data sources using R, SAS.\n\u2022 Data cleansing, transformation and creating new variables using R.\n\u2022 Built predictive scorecards for Cross-selling Car loan, Life Insurance, TD and RD.\n\u2022 Scoring predictive models as per regulatory requirements & ensuring deliverables with PSI.\n\u2022 Data modeling and formulation of statistical equations using advanced statistical forecasting techniques.\n\u2022 Provide guidance and mentoring to team members.\n\u2022 Establish scalable, efficient, automated processes for large scale data analyses, model development, model validation and model implementation.\n\u2022 Formulate and test hypotheses, extract signals from peta-byte scale, unstructured data sets, and ensure that our display advertising business delivers the highest standards of performance.\n\u2022 Lead a multi-functional project team.\n\u2022 Develop necessary connectors to plug ML software into wider data pipeline architectures.\n\u2022 Applied association rule mining & chaid model to identify hidden patterns and rules in remedy ticket analysis which aid in decision making.\n\u2022 Understanding the client business problems and analyzing the data by using appropriate Statistical models to generate insights.\n\u2022 Integrated Teradata with R for BI platform and also implemented corporate business rules\n\u2022 Participated in Business meetings to understand the business needs & requirements.\n\u2022 Arrange and chair Data Workshops with SME's and related stake holders for requirement data catalogue understanding.\n\u2022 Design Logical Data Model which will fit and adopt the Teradata Financial Logical Data Model (FSLDM11) using Erwin data modeler tool.\n\u2022 Present and approve designed Logical Data Model in Data Model Governance Committee (DMGC).\n\nEnvironment: R Studio, Machine learning, Informatica 9.0, Scala, Amazon Machine Learning (AML)Python, Django, SAAS."", u'Machine Learning/Data Scientist\nEssar Groups - Mumbai, Maharashtra\nFebruary 2011 to October 2012\nDescription:Essar Global Fund Limited is an Indian conglomerate group based in Mumbai, India. The Fund is a global investor, controlling a number of world-class assets diversified across the core sectors of Energy, Metals & Mining, Infrastructure and Services\nResponsibilities:\n\u2022 Worked as Data Expert on a data mining ETL development project using SAS Enterprise Guide.\n\u2022 Created test plan documents for all back-end database modules.\n\u2022 Worked with large amounts of structured and unstructured data.\n\u2022 Responsible for data collection, cleansing andANOVA. Designed technical solution roadmap to deal with noise in sales data.\n\u2022 Worked on loading the data from MySQL to HBase where necessary using Sqoop.\n\u2022 Handled end-to-end project from data discovery to model deployment\n\u2022 Knowledge in Business Intelligence tools and visualization tools such as Business Objects, Tableau, ChartIO, etc.\n\u2022 Knowledge in Machine Learning concepts (Generalized Linear models, Regularization, Random Forest, Time Series models, etc.).\n\u2022 Deployed GUI pages by using JSP, JSTL, HTML, DHTML, XHTML, CSS, JavaScript, AJAX.\n\u2022 Configured the project on WebSphere 6.1 application servers.\n\u2022 Implemented the online application by using Core Java, Jdbc, JSP, Servlets and EJB 1.1, Web Services, SOAP, WSDL.\n\u2022 Communicated with other Health Care info by using Web Services with the help of SOAP, WSDLJAX-RPC.\n\u2022 Used Singleton, factory design pattern, DAO Design Patterns based on the application requirements\n\u2022 Used SAX and DOM parsers to parse the raw XML documents\n\u2022 Used RAD as Development IDE for web applications.\n\u2022 Used Log4J logging framework to write Log messages with various levels.\n\u2022 Involved in fixing bugs and minor enhancements for the front-end modules.\n\u2022 Implemented Microsoft Visio and Rational Rose for designing the Use Case Diagrams, Class model, Sequence diagrams, and Activity diagrams for SDLC process of the application.\n\u2022 Maintenance in the testing team for System testing/Integration/UAT.\n\u2022 Conducted Design reviews and Technical reviews with other project stakeholders.\n\u2022 Planned, coordinated, and monitored project levels of performance and activities to ensure project completion in time.\n\u2022 Involved in defining the source to target data mappings, business rules, and business and data definitions.', u'Machine Learning/Data Scientist\nCognizant Technology Solutions - Pune, Maharashtra\nJanuary 2009 to January 2011\nEnvironment: R 3.0, Erwin 9.5, Tableau 8.0, MDM, QlikView, MLLib, PL/SQL, HDFS, Teradata 14.1, JSON, HADOOP (HDFS), MapReduce, PIG, Spark, R Studio, MAHOUT, JAVA, HIVE, AWS.\nClient: Cognizant Technology Solutions, Pune, IndiaAug 2009 - Jan 2011\nRole: Machine Learning/Data Scientist\n\nDescription: Cognizant is an American multinational corporation that provides IT services, including digital, technology, consulting, and operations services. It is headquartered in Teaneck, New Jersey, United States. Cognizant is listed in the NASDAQ-100 and the S&P 500 indices.\n\nResponsibilities:\n\u2022 Provide technical review, solution for Galaxy data warehouse project.\n\u2022 Predictive Analysis using Big Data & Machine Learning.\n\u2022 Importing data using sqoop (RDBMS) &flume (files) in HDFS.\n\u2022 Pig scripts used to apply the business logic and the results were stored in a flat file.\n\u2022 Hive QL to load, query and analyze data and store data in partition tables.\n\u2022 Jupyter notebook used to write Python script to train / test data sets & prediction.\n\u2022 NLP using NLTK library to analyze consumer feedback classification by Na\xefveByes.\n\u2022 Customer churns prediction using Scikit Learn using Logistic Regression and SVM.\n\u2022 As a scrum master providing facilitation for scrum meetings like Planning, Review, Retrofit =\n\u2022 Incident Management and Support BIOPS - Level 3.\n\u2022 Estimation of work/task using MS Project Plan and allocation of task/work with team leads.\n\u2022 Daily / Weekly Status report / meeting with client.\n\u2022 Client coordination for requirement proposal, planning, issues & escalations.\n\u2022 Participation in DW architecture meetings & designing the BI architect including capacity planning &HWconfiguration.\n\u2022 Developed data conversion/quality/cleansing rules and executed data cleansing activities such as data Consolidation, standardization, matching Trillium for the unstructured flat file data.\n\u2022 Responsible for developing, support and maintenance for the ETL (Extract, Transform and Load) processes using Informatica Power Center 8.5.\n\u2022 Wrote SQL-Overrides and used filter conditions in source qualifier thereby improving the performance of the mapping.\n\u2022 Designed and developed mappings using Source Qualifier, Expression, Lookup, Router, Aggregator, Filter, Sequence Generator, Stored Procedure, Update Strategy, joiner and Rank transformations.\n\u2022 Managed the Metadata associated with the ETLprocesses used to populate the Data Warehouse.\n\u2022 Implemented complex business rules in Informatica Power Center by creating re-usable transformations, and robust Mapplets.\n\u2022 Analyze business information requirements and model class diagrams and/or conceptual domain models.\n\u2022 Gather & Review Customer Information Requirements for OLAP and building the data mart.\n\u2022 Performed document analysis involving creation of Use Cases and Use Case narrations using Microsoft Visio, in order to present the efficiency of the gathered requirements.\nEnvironment: R, Random Forest Regression, Multiple Linear Regression, Predictive Analysis, Descriptive Analysis, Data Visualization, GGPlot, GGplotly, Dplyr, Data Extraction, Data Preprocessing.']",[],[],degree_1 : 
0,https://resumes.indeed.com/resume/dff8edd01c68e219,"[u'Research Associate, econometric & statistical modelling on diverse topics\nInstitute of Agricultural Economics and Development, Chinese Academy of Agricultural Sciences - Beijing, CN\nJune 2017 to Present\nBeijing, China\n\nResearch Associate, econometric & statistical modelling on diverse topics\n\u25cf dig into data, suggest and implement quantitative methods for research papers (with R, Python, and Matlab/Octave)\n\u25cf identified one key issue based on an exhaustively researched dataset, and produced a promising paper upon the topic.\n\u25cf 1 paper accomplished within 2 months and published in Contemporary Finance & Economics (a Chinese SSCI journal, ranked top 30%), 2 other conference papers (in English) accepted on International Conference of Agricultural Economists 2018, 2 papers under revision, 3 papers submitted', u""Intern Data Scientist, Data and Quantitative Analysis Department,\nCIB (Industrial Bank Co., Ltd) Research, (Largest commercial bank in China) - Shanghai, China\nJanuary 2017 to May 2017\nProject 1: Decompose China Bond Yield Spread\n\u25cf Assisted data scientist with data warehousing on Oracle 10g, PL/SQL programming and query optimization\n\u25cf With 240,000+ historical data entries retrieved, I automated a task to update, clean data and transform data into specific excel structures; The original task takes around 3.5 hours for an average analyst to complete; It now runs in 20 minutes by using Python.\nProject 2: Text mining on China Monetary Policy Report\n\u25cf I implemented a method to capture subtle wording changes in reports; I put the results into summary statistics and graphics. I automated this task which saved analysts' work in reading through the reports.""]","[u'PhD. in Economics', u'M.Sc. in Empirical & Theoretical Economics', u'M.Sc. (1-year) in Applied Mathematics', u'M.Sc. in Management', u'Bachelor in Engineering in Engineering']","[u'Paris School of Economics, University of Bielefeld Paris, FR\nOctober 2010 to December 2015', u'Universit\xe9 Paris 1, Universitat Aut\xf2noma de Barcelona Paris\nSeptember 2008 to September 2010', u'Universit\xe9 Paris 1 Paris\nSeptember 2007 to September 2008', u'NEOMA Business School Rouen, FR\nSeptember 2006 to September 2007', u'Fujian Normal University Fujian\nSeptember 2001 to September 2005']","degree_1 : PhD. in Economics, degree_2 :  M.Sc. in Empirical & Theoretical Economics, degree_3 :  M.Sc. (1-year) in Applied Mathematics, degree_4 :  M.Sc. in Management, degree_5 :  Bachelor in Engineering in Engineering"
0,https://resumes.indeed.com/resume/55e82d1485933170,"[u""Team Leader - Data Science\nACXIOM\nSeptember 2017 to Present\n\u2022 Manage a team of three people responsible for client facing custom analytics and custom/standard reporting that highlight the overall value of Acxiom data.\n\u2022 Demonstrate thought leadership to identify optimal analytic approaches that deliver actionable results and solve clients' existing business problem(s).\n\u2022 Coordinate the build-out of global reporting and modeling capabilities for multiple countries (including Canada, Mexico, Brazil, Australia, India, and Japan).\n\u2022 Engage with leadership across Acxiom on multiple key initiatives, contributing to the process flow and actively resourcing to support the outcome/deliverable(s).\n\u2022 Scope and manage custom analytic and reporting projects for clients in the following industries: movie, life insurance, healthcare, automotive, and financial.\n\u2022 Track team revenue (direct and indirect) and project workload, supplying upper management with frequent updates around both revenue growth and current/future client opportunities.\n\u2022 Provide real time and formal performance feedback to team members, highlighting strengths and identifying growth opportunities."", u""Expert Analyst\nACXIOM\nFebruary 2016 to September 2017\n\u2022 Developed and implemented a process of engagement for both internal and external acxiom business partners and clients. Determine ongoing enhancements necessary to improve efficiency.\n\u2022 Collaborate with immediate supervisor to interview and hire additional analysts for team expansion.\n\u2022 Served as lead analyst on statistical modeling projects identifying best potential prospects to drive client growth and acquisition, providing start to finish project scoping and delivery of model performance and results with corresponding recommendations.\n\u2022 Served as lead analyst on a client custom segmentation that combined Acxiom and client data to provide a holistic view of the client's customers at all stages of the product life-cycle.\n\u2022 Provide team analysts guidance and direction on engagements involving financial, retail, food, healthcare, and telecommunications clients.\n\u2022 Supervised the build of a standard modeling process that is flexible enough to ingest both Acxiom and client data as well as produce necessary quality checks and audit points throughout.\n\u2022 Communicate directly with upper management to highlight team activity, metrics and performance.\n\u2022 Participated in an 'Emerging Leaders Program' at the request of direct supervisor and VP based on previously demonstrated leadership potential and qualities."", u""Data Scientist II\nSPRINT\nJuly 2012 to February 2016\n\u2022 Served as acting manager upon immediate departure of existing manager. Managed offshore resources and acted as a communication funnel to upper management helping determine priorities.\n\u2022 Collaborated with collections to identify characteristics of customers likely to 'never pay' Sprint through a decision tree analysis.\n\u2022 Assisted with overseeing the build of a multinomial handset upgrade model(s) suite. Responsibilities included facilitating daily meetings to assess progress and troubleshoot as well as packaging the results in a digestible format.\n\u2022 Explored and evaluated the feasibility of building and implementing an uplift model to identify customers most likely to provide incremental lift when targeted in a DM or EM marketing campaign.\n\u25cf Served as lead project manager during the build of several logistic regression models supporting Gross Adds: Tablet Propensity, Handset Propensity, Hotspot on Device Activation, EM Response, DM Response, SMS Response.\n\u25cf Built a model predicting the likelihood of an account to add a data device and provided supporting targeting recommendation upon completion.\n\u25cf Provide ongoing analyses detailing the incremental benefit of predictive model segments for a variety of Gross Add direct mail, email and SMS tactics.\n\u25cf Manage and update internal Gross Add campaign calendar to ensure timely delivery of model scores with appropriate reference dates for corresponding campaigns/tactics.\n\u25cf Investigate new data and methodologies that may improve current marketing performance when used in combination with internal models."", u'Consumer Understanding & Insights Analyst\nHALLMARK CARDS, INC\nJuly 2006 to July 2012\nPrimary analyst of marketing retail tests requiring experimental design expertise to ensure informative and actionable insights upon completion.\n\u25cf Active member of a collaborative modeling team tasked with leveraging statistical analyses and data driven solutions to help business partners successfully answer key questions.\n\u25cf Experienced tactician responsible for designing, executing and providing a holistic story on performance of seasonal direct mail marketing campaigns throughout the year.\n\u25cf Devised testing and executed targeting for direct mail pieces during the January/February window. Tests helped quantify the risk of planned contact reduction strategies for 2012. Led team dedicated to preparing an executive summary conveying the full year implications ($ at risk) of budget reductions.\n\u25cf Advised marketing business partners on strategy and analysis of testing coupons at retail as a method of driving immediate category sales. Provided summarized, holistic summary of findings across several chains and methods of distribution to guide future planning in 2012.\n\u25cf Served as lead analyst on a multi-chain promotional marketing test intended to drive sales of new product. Responsible for ensuring proper test design in each chain and providing a holistic story of results with both chain specific and corporate implications.\n\u25cf Used historical data and statistical modeling to help business partners estimate the amount of missing product to place in retail accounts.\n\u25cf Presented a SAS paper on the benefits of Proc GLMSELECT, including variable selection flexibility, at the 2011 Midwest SAS Users Group Conference.\n\u25cf Designed, executed and analyzed a longitudinal market test to determine whether or not a percentage of reduction in TV advertising would adversely impact sales in a significant way.\n\u25cf Led analysis of a longitudinal direct mail test to identify pockets of consumers based on age and lifestage that might benefit from receiving additional contacts.\n\u25cf Using SAS, built participation models using logistic regression to identify consumers likely to purchase innovative product for use in direct mail targeting.\n\u25cf Collaborated with an outside vendor to apply survey results to our database of loyalty consumers using linear discriminant analysis.\n\u25cf Implemented a case-control matching macro in SAS that streamlined retail test analyses and allowed for workforce efficiencies within the division.', u'Biostatistician/Database Analyst\nMID AMERICA HEART INSTITUTE\nFebruary 2005 to July 2006\nCreated a unified and organized SAS analytic repository to house vital patient information on admissions, surgery and catheterization procedures.\n\u25cf Designed and implemented multiple Access databases for consistent data entry.\n\u25cf Served as lead biostatistician on numerous studies providing sound statistical analysis and a thorough description of methodology in abstracts and manuscripts.\n\nTechnology\nSoftware:\nSAS PC, SAS Enterprise Miner, SAS BI, Teradata, Microstrategy, MS Office (Word, Access, Excel, PowerPoint), Utraedit, Teradata SQL']",[u'Master of Statistics in Statistics'],[u'UNIVERSITY OF MISSOURI\nJanuary 1999 to January 2005'],degree_1 : Master of Statistics in Statistics
0,https://resumes.indeed.com/resume/f750e9a9208003f1,"[u""Data Scientist\nTamr - Cambridge, MA\nJanuary 2017 to Present\nData science consultant in the Field Engineering group using machine learning and other computational approaches to unify large, disparate and unruly data sets for enterprise clients.\n\nProjects include:\n- Hierarchical classification of $25B in spend from over 9M separate purchase orders into a 3-tiered taxonomy using semi-supervised machine learning for a global manufacturer\n\n- Large-scale \u201dschema mapping\u201d project to unify data from 1500 clinical trials. Developed parallel Python pipeline to process data from more than 10k separate tables distributed across multiple Postgres instances into a single, unified and SDTM-formatted data set for a large biopharmaceutical company\n\n- Clustering of 100K supplier records into groups that reflect the supplier's real-world business structure using supervised clustering algorithm. Clustering was robust to record-level noise (e.g., misspellings, multiple addresses), and leveraged custom organizational data (e.g., parent-subsidiary info) derived from web scraping to link suppliers with distinct brands but common corporate ownership."", u'Data Scientist, Target Identification Group\nYumanity Therapeutics\nJanuary 2016 to January 2017\nDeveloped novel statistical methods using high-throughput sequencing and phenotyping data, and compound SAR to elucidate cellular target of novel molecules derived from phenotypic screens of neurodegenerative disease\n\u2022 Lead experimental design, data analysis and visualization across multiple platforms\n\u2022 Defined the molecular target of two novel chemical series\n\u2022 Communicated results of statistical analyses to CSO, CEO and Board of Directors\n\u2022 Inventor on two preliminary patents for novel compounds and methods to treat neurodegenerative diseases (to be published in March 2018)\n\nResearch Experience', u'Kirschstein National Research Fellow\nNational Institute on Aging - Cambridge, MA\nJanuary 2013 to January 2016\nKirschstein National Research Fellowship to Promote Diversity for Mechanistic Analysis of Age-Induced Changes in Protein Hemostasis (Total award: $103,000)', u'Scientist\nFAS Center for Systems Biology\nJanuary 2011 to January 2016\n\u2022 Identification of the conserved Hsf1 regulon in yeast and mammalian cells using high-throughput transcriptomics and novel statistical analysis.\n\u2022 Advisors: Vladamir Denic (Harvard MCB) and David Pincus (Whitehead Institute).', u'Advisor\nHarvard Department of Statistics\nJanuary 2010 to January 2016\n2010 - 2016\n\u2022 Developed Bayesian missing data model to enable absolute quantification of protein abundance by mass spectrometry.\n\u2022 Multiplexed flow cytometry using fluorescent cell barcoding and a hierarchical, skew-normal model.\n\u2022 Advisor: Edoardo Airoldi (Harvard Statistics)', u'Junior Statistics Fellow\nNational Science Foundation\nJanuary 2008 to January 2009\nvia UMD Joint Program in Survey Methodology', u'Department of Statistics Undergrad Teaching Fellow\nThe George Washington University\nJanuary 2007 to January 2008\nHonors\n\u2022 Exemplary Contribution Award from the Chairman & CEO of Yumanity Therapeutics, 2016\n\u2022 Harvard University Applied Statistics Symposium Best Presentation Honorable Mention, 2012\n\u2022 Joint Statistics Meeting Minority Travel Award, 2009\n\u2022 The George Washington University, graduated cum laude, 2009']","[u'Ph.D. in Systems Biology', u'in A.M', u'B.S. in Statistics']","[u'Harvard University\nJanuary 2011 to January 2016', u'Harvard School of Public Health\nJanuary 2009 to January 2011', u'The George Washington University\nJanuary 2005 to January 2009']","degree_1 : Ph.D. in Systems Biology, degree_2 :  in A.M, degree_3 :  B.S. in Statistics"
0,https://resumes.indeed.com/resume/a9b0eb217e2a44e0,"[u""USA, Data Scientist Intern\nDomtar - Raleigh, NC\nAugust 2017 to Present\n\u2022 Worked on survey data of personal care product usage to develop the marketing strategy for boosting sales.\n\u2022 Converted the high dimensionality with 7000 odd features to low dimensionality by grouping into buckets.\n\u2022 Clustered 5 customer segmentations using K-Means based on the purchase behavior, income levels and age having around 88% homogeneity.\n\u2022 Performed Multiple Corresponding Analysis with the help of FactoMineR package on the survey data to find the top 5 important features towards the brand usage.\n\u2022 Designed business intuitive dashboard's for monthly Sales Tracking and for projection survey results which helped marketing team to target the required segment to the improve sales.\n\u2022 Automated a Business process to reduce the 20 man-hours a week by leveraging the capabilities of R and SQL."", u""SAP BI Developer\nPhilips Electronics - Bengaluru, Karnataka\nAugust 2015 to July 2016\n\u2022 Designed dashboards using SAP Design studio to access all reports in one go and reduced the execution time.\n\u2022 Proactively worked on the identifying defects in the solution model and Improved project delivery through Change/Problem management by providing efficient & effective structural solutions on recurring Incidents.\n\u2022 Architected an existing data model into SAP HANA as a Proof of Concept (POC) to demonstrate & leverage the advantages/optimizations of the HANA platform to the client.\n\u2022 Analyzed system database performance and implemented performance tuning for 10+ DataMart's."", u'SAP BI Developer\nCognizant Technology Solutions - Bengaluru, Karnataka\nJanuary 2014 to August 2015\n\u2022 Created 2 proposal catalogs to help the management to bid for future projects by presenting in SAP Sapphire.\n\u2022 Developed models for predicting the future revenue growth using SAP HANA Prediction algorithms and Implemented sentiment analysis using SAP BODS.\n\u2022 Developed stored procedure for the implementation of RFM algorithm using SQL in SAP HANA.\n\u2022 Optimized the data model by tuning the SQL queries to increase the performance by 25%.\n\u2022 Implemented the market basket analysis for a retail organization to find the product purchase association.\n\u2022 Trained 40+ people in the organization and mentored 20 new college recruits on SAP HANA & BW.\nACADEMIC PROJECT\n\u2022 Examined the UNdata includes various factors of UN growth using plots in Tableau, R-ggplot, Python and D3\n\u2022 Analyzed ERP system\'s major process like procurement, fulfillment, production, Inventory management and material planning process, demand planning, supply network planning and global available to promise with the help of GBI Inc data to understand the business process.\n\u2022 Designed a strategy for NFL teams which could help characterize players in the offence or defense layups so that a team contains players who are at the best of their abilities by using statistical techniques in SAS EM.\n\u2022 Designed and created a data warehouse for a retail business using MS SQL server and SSIS packages.\n\u2022 Designed ER, Dimensional model for Airbnb and deployed it in SQL Server Express to develop a data hub.\nCOMPETITIONS, LEADERSHIP, ORGANIZATIONS & AWARDS\nSAP Users\' Group, UT Dallas - Events Officer\nStudent Activity Centre, JB Institute of Engineering and Technology - President\nAwarded by Cognizant Technology Solutions ""Best campus associate"" and ""People and practice partner""']","[u'M.S in Information Technology, and Management']","[u'The University of Texas at Dallas Dallas, TX']","degree_1 : M.S in Information Technology, degree_2 :  and Management"
0,https://resumes.indeed.com/resume/6d1df9b6d9dfcc82,"[u'Data Scientist\nAnheuser-Busch InBev Bud Analytics Lab - Champaign, IL\nMay 2017 to Present\n\u2022 Developed and built Omni-channel consumer engagement management database\n\u2022 Performed exploratory data analysis of customer data sets on the e-commerce platform\n\u2022 Clustered customers by Kmodes, RObust Clustering using linKs (ROCK) and Correlation Explanation (CorEx)\n\u2022 Analyzed different promotions and how they affected customer behavior to provide business insights\n\u2022 Trained various machine learning algorithms to predict consumer future behavior, especially purchasing behavior\n\u2022 Identified important features through regression models, unbiased cforests and Permutation Importance algorithms\n\u2022 Working on optimizing the company employee turnover model, especially using NLP to incorporate HR feedback', u'Data Science Fellow\nInstitute of Mathematics and Its Applications (IMA) - Minneapolis, MN\nJanuary 2017 to April 2017\n\u2022 Extracted data of municipal bond trading activities and performed exploratory data analysis in R\n\u2022 Implemented random forest regressor in PySpark to predict price change of bonds and provided trading suggestions\n\u2022 Worked on data of Tesla Model S battery capacity loss and read papers of electric cars battery aging process\n\u2022 Determined the overall loss trend line and identified major factors responsible for its variation\n\u2022 Applied various machine learning techniques to predict capacity loss and interpret results for warranty plan strategies', u'Data Scientist Intern\nDow AgroSciences - Champaign, IL\nMay 2015 to August 2015\n\u2022 Focused on methods of how to incorporate and propagate uncertainty in inputs in agricultural data sets\n\u2022 Reviewed relevant papers and summarized applicable algorithms and methods (Deterministic Equivalent Modeling)\n\u2022 Implemented and modified new algorithms into a Maple library (arbitrary polynomial chaos, Nataf transformation)\n\u2022 Automated simulations in Python with significantly more efficient results than traditional Monte Carlo techniques']","[u'M.S. in Applied Mathematics', u'B.S. in Mathematics and Applied Mathematics']","[u'University of Illinois Urbana-Champaign Urbana, IL\nDecember 2016', u'Harbin Institute of Technology Harbin, CN\nJune 2011']","degree_1 : M.S. in Applied Mathematics, degree_2 :  B.S. in Mathematics and Applied Mathematics"
0,https://resumes.indeed.com/resume/f441fc1fcd4eca7f,"[u'Data Scientist\nAircraft Fasteners International LLC - Dallas, TX\nJune 2017 to December 2017\n\u27a2 Achieved revenue increment using sales-inventory data by targeting top 30% core customers (segmentation)\n\u27a2 Developed a machine learning model with 82% accuracy to check the conversion rate from quotes to sales\n\u27a2 Identified next best selling products by using customer/item based collaborative filtering algorithms', u'Senior Data Analyst\nZS Associates Pvt. Ltd - Pune, Maharashtra\nJanuary 2016 to July 2016\n\u27a2 Predicted budget allocation model for sales managers with 87% accuracy (Time Series forecasting)\n\u27a2 Targeted physicians optimally for each sales rep by developing a Call Planning recommendation solution\n\u27a2 Managed a team of 5 members and mentored 2 colleagues on machine learning models and A/B testing', u'Data Analyst\nZS Associates Pvt. Ltd - Pune, Maharashtra\nAugust 2014 to December 2015\n\u27a2 Executed end-to-end ETL process and designed sales dashboards for pharmaceutical clients\n\u27a2 Reduced data cleansing time by 25% by conducting exploratory data profiling and SQL query optimization\n\u27a2 Communicated business insights with data visualization tools by building KPI metric dashboards (Tableau)']",[u'M.S in Business Analytics'],"[u'The University of Texas at Dallas Richardson, TX\nApril 2018']",degree_1 : M.S in Bsiness Analytics
0,https://resumes.indeed.com/resume/2201384aafe0033e,"[u'Data Analyst (Contractor)\nIndeed - Austin, TX\nSeptember 2017 to Present\nWork with Indeed Prime under Cody Hinson and Ernesto Kufoy to define and clean candidate data from LinkedIn.', u'Analytical Scientist II\nSonic Reference Laboratory - Austin, TX\nDecember 2015 to July 2017\n- edit methods of Analyst Software to tell the program to integrate graphical area of a chemical compound efficiently (similar to Java coding)\n- quantitate chemical concentrations using regression modeling (akin to SAS statistical modeling)', u'Analytical Scientist I\nBalcones Pain Consultants - Austin, TX\nDecember 2014 to December 2015\n-performed statistical analysis of shifts and trends data to determine that\nreagents used were degrading, to purchase new material\n- export messy patient data into a concise medical PDF report for physicians to read to properly diagnose patients']","[u'M.S. in Data Science', u'B.S. in Biology, Chemistry, Violin, Mandarin Chinese', u'Summer Research in Biogeochemistry']","[u'Southern Methodist University\nMay 2017 to December 2018', u'Valparaiso University Valparaiso, IN\nAugust 2010 to June 2014', u'Zhejiang University Hangzhou\nMay 2013 to August 2013']","degree_1 : M.S. in Data Science, degree_2 :  B.S. in Biology, degree_3 :  Chemistry, degree_4 :  Violin, degree_5 :  Mandarin Chinese, degree_6 :  Smmer Research in Biogeochemistry"
0,https://resumes.indeed.com/resume/623dfc0ed49c2771,"[u'Member\nGraduate Colloquium Committee\nOctober 2016 to Present', u'Graduate Teaching Assistant\nNorthern Illinois University - DeKalb, IL\nMay 2016 to Present\nI teach Undergraduate courses in Statistics.', u'Data Scientist Summer Intern\nGraduate Colloquium Committee - Northbrook, IL\nMay 2017 to July 2017']","[u'Doctor of Philosophy in Statistics', u'Master of Science in Applied Probability and Statistics', u'Master of Arts in Economics', u'Advanced Certificate in Course Emphasized on Econometrics', u'Master of Science in Economics', u'Bachelor of Science in Economics']","[u'Northern Illinois University DeKalb, IL\nAugust 2014 to June 2019', u'Northern Illinois University DeKalb, IL\nAugust 2014 to August 2016', u'Northern Illinois University DeKalb, IL\nAugust 2012 to August 2014', u'Centre for Studies in Social Sciences Kolkata, West Bengal\nApril 2010', u'The University of Calcutta Kolkata, West Bengal\nAugust 2007 to August 2009', u'Ramakrishna Mission Residential College\nJuly 2007']","degree_1 : Doctor of Philosophy in Statistics, degree_2 :  Master of Science in Applied Probability and Statistics, degree_3 :  Master of Arts in Economics, degree_4 :  Advanced Certificate in Corse Emphasized on Econometrics, degree_5 :  Master of Science in Economics, degree_6 :  Bachelor of Science in Economics"
0,https://resumes.indeed.com/resume/82a6a3dcf8d0e9e6,"[u'Data Analyst\nIntegra Technologies\nOctober 2017 to Present\nDesigned SQL, R & Python scripts to directly query and analyze internal and external data sources to package\ntogether clear, actionable findings.\n\u2022 Provided timely, valuable solutions for ad hoc analyses using SQL with a focus to detail on crafting scripts that\ncan be easily manipulated to efficiently address similar future analyses.', u'Student Consultant\nScherzinger Termite and Pest Control - Cincinnati, OH\nJanuary 2017 to April 2017\nMarketing Analytics Project:\n\u2022 Built and evaluated multiple classification algorithms (Neural Net, Random Forest, Logistic Regression, Decision Tree)\nin R to predict the individual lead conversions based on customer details and geographical features. Performed\nClustering to segment different geographical areas based on user attributes, lead conversions to differentiate regions\nthat require more marketing and regions that are almost saturated\n\u2022 Designed visualizations in Tableau representing lead conversions and revenue generation over customer attributes\nand locations. Submitted a comprehensive report including all the conclusions and recommendations based on the model results and visualizations', u'Data Scientist\nIcomm Tele Limited - Hyderabad, Telangana\nJuly 2015 to June 2016\nHuman resource analytics project: Developed a regression model to predict the attrition time for candidates attending\ninterviews to bolster the decision-making process of choosing the right candidate\n\u2022 Data transformation was done using SQL and excel followed by developing multiple models in R and Python\n\u2022 Overall reduction of 27 percent in recruiting budget is observed in one year by using this model\nOptical character recognition project: Designed a system which uses machine learning and computer vision to the extract\nspecific amount of information into structured data that can be used for further analysis.\nChurn Analysis: Reduced the churn to 50 percent by conducting detailed analysis on existing customers', u'Data Analyst\nK Venkateshwarlu & Co - Warangal, Telangana\nMarch 2013 to March 2015\n\u2022 Performed statistical tests (sign-test, correlation test, ANOVA, t-test) to identify the relation between variables to answer crucial questions\n\u2022 Designed and maintained the supporting database objects including SQL Tables, Stored Procedures\n\u2022 Created and implemented SQL Queries, Stored procedures, Functions, Packages and Triggers in SQL Server.\n\u2022 Utilized Advanced Excel techniques like Vlookups and Pivot Tables to create over 40 weekly reports as per\nbusiness requirements\n\u2022 Performed data manipulation, transformation, and cleansing using Microsoft SQL']","[u'Master of Science in Business Analytics in Course work', u'Bachelor of Technology in Technology']","[u'University of Cincinnati, Carl H. Lindner College of Business Cincinnati, OH\nAugust 2016 to December 2017', u'National Institute of Technology Warangal Warangal, Telangana\nJuly 2011 to April 2015']","degree_1 : Master of Science in Bsiness Analytics in Corse work, degree_2 :  Bachelor of Technology in Technology"
0,https://resumes.indeed.com/resume/d0c1828fd2ccb3ca,"[u'Student Consultant\nScherzinger Termite and Pest Control - Cincinnati, OH\nJanuary 2017 to April 2017\nMarketing Analytics Project:\n\u2022 Built and evaluated multiple algorithms (Neural Net, Random Forest, Logistic Regression, Decision Tree) in R to predict the individual lead conversions based on customer details and geographical features. Arrived at the best model (Logistic Regression) having ROC of 0.82 and misclassification-rate of 0.17\n\u2022 Performed Clustering to segment different geographical areas based on user attributes, lead conversions to differentiate regions that require more marketing and regions that are almost saturated\n\u2022 Designed visualizations in Tableau representing lead conversions and revenue generation over customer attributes and locations. Submitted a comprehensive report including all the conclusions and recommendations based on the model results and visualizations', u'Data Scientist\nCliquesf Solutions - Hyderabad, ANDHRA PRADESH, IN\nJuly 2015 to June 2016\nHuman resource analytics project: Developed a model to predict the attrition time for candidates attending interviews to bolster the decision-making process of choosing the right candidate\n\u2022 Data transformation was done using SQL and excel followed by developing multiple models in R and Python\n\u2022 The best model is arrived by evaluating all the models based on misclassification-rate. The final model was a gradient boosting model with the misclassification rate of 0.15\n\u2022 Overall reduction of 27 percent in recruiting budget is observed after using this model\nOptical character recognition project: Designed a system which uses machine learning and computer vision to the extract specific amount of information into structured data that can be used for further analysis. Google Tesseract was used for image processing', u'Data Analyst\nK Venkateshwarlu & Co - Warangal, IN\nMarch 2013 to March 2015\n\u2022 Performed statistical tests (sign-test, correlation test, ANOVA, t-test) to identify the relation between variables to answer crucial questions\n\u2022 Designed and maintained the supporting database objects including SQL Tables, Stored Procedures\n\u2022 Utilised Advanced Excel techniques like Vlookups and Pivot Tables to create over 40 weekly reports as per business requirements\n\u2022 Performed data manipulation, transformation, and cleansing using Microsoft SQL\nOTHER ANALYTICS PROJECTS: (https://www.linkedin.com/in/vijay-katta-85187986/)\nHousing Prices (achieved top 10 percent in this Kaggle competition), Summer Olympic Medals, Boston Housing Prices, German Credit Score, Image Classification (Imagnet dataset), Optimisation Techniques in Deep Learning, Time Series Project, Factor Analysis, A Study of Convolutional Neural Networks']","[u'Master of Science in Business Analytics', u'Bachelor of Technology in Civil Engineering']","[u'University of Cincinnati, Carl H. Lindner College of Business Cincinnati, OH\nAugust 2016 to Present', u'National Institute of Technology Warangal Warangal, IN\nJuly 2011 to April 2015']","degree_1 : Master of Science in Bsiness Analytics, degree_2 :  Bachelor of Technology in Civil Engineering"
0,https://resumes.indeed.com/resume/94633820642402ba,"[u'Head of Data Science\nAlly Financial - Charlotte, NC\nJanuary 2017 to Present', u'Director of Data Science\nRed Ventures - Charlotte, NC\nNovember 2015 to December 2016\n\u2022 Responsible for data science efforts, growing and scaling the data science team.\n\u2022 Identified significant business opportunities where data science can help.\n\u2022 Optimized existing businesses, guide decisions, and set a data science strategy that\nsecured partnerships with clients.\n\u2022 Mentored and guided less experienced data science analysts.\n\u2022 Served as a business interface for the data science team, working with business\nteams to identify potential opportunities and to report progress on initiatives.\n\u2022 Dove into the entire business model, creating and expanding expertise in digital\nmarketing and customer acquisition to maximize the impact of Data Science at Red\nVentures.', u'Lead Data Scientist\nAutoloop, LLC - Clearwater, FL\nJanuary 2015 to December 2015\n\u2022 Worked with product teams to identify and answer pertinent business questions.\n\u2022 Used data to inform decisions regarding monetization and marketing.\n\u2022 Defined and tracked success metrics for products.\n\u2022 Designed, implemented, and analyzed the results of experiments.\n\u2022 Communicated findings with both technical and non-technical audiences.\n\u2022 Understood user behavior and engagement through segmentation, machine learning and model building.\n\u2022 Broad use and understanding of:\nLanguages: SQL, Python, Spark/pySpark.\nData analysis using tools such as:\n- Expertise in Python + NumPy, SciPy, Pandas for data mining.\n- Shell script in bash/tcsh, code repository workflow using Git.\n- Scikit-learn for machine learning (regression, neural networks, decision tree,\nrandom forest, deep learning).\n- Bayesian modeling and optimizations for hypothesis testing.\n- Matplotlib for data visualization.\n- Statistics packages used: R and SPSS.\nLinux/UNIX, MacOS, Windows and virtual machine environments.\n- Experience with SQL and T-SQL/Hadoop database.\n- Multithreaded parallel computing optimized for Big Data analytics on Spark.', u'Senior Fellow\nUniversity of Washington - Seattle, WA\nMay 2014 to March 2015\nDr. Houra Merrikh\n\u2022 Determine the mechanisms responsible for replication conflicts, their structural and functional consequences, and the mechanism of cellular response using next\ngeneration sequencing and bioinformatics using PacBio and Illumina in Bacillus and related species.\n\u2022 Develop experimental and computational models in C to determine the rate of gene\ninversions, molecular, genetic, biochemical, and cell biological approaches in the model system Bacillus subtilis.', u'Teaching Assistant\nUniversity of New Hampshire - Durham, NH\nJanuary 2014 to May 2014\nLaboratory techniques to determine morphological, cultural, biochemical, serological,\nepidemiological, and pathogenic characteristics of microorganisms causing human and animal diseases were taught. Clinical presentation in host and laboratory diagnosis and\ntreatment measures were also discussed.', u'PhD Research Assistant\nUniversity of New Hampshire - Durham, NH\nJuly 2009 to May 2014\nDr. Louis Tisa\n\u2022 Bioinformatics: Perl, DNAstar SeqMan Pro, CLC genomics Workbench, AllPaths-LG,\nMAUVE, Velvet, etc.\n\u2022 Next Generation Sequencing: Library prep for RNAseq, Genome sequencing, and broad knowledge of all other library prep for Illumina platform. Expert in Roche 454,\nIllumina and PacBio']","[u'Ph.D. in Molecular Microbiology', u'B.S. in General Biology']","[u'University of New Hampshire\nMay 2014', u'Valdosta State University Valdosta, GA\nAugust 2006 to May 2009']","degree_1 : Ph.D. in Moleclar Microbiology, degree_2 :  B.S. in General Biology"
0,https://resumes.indeed.com/resume/73e446cb6cb966f4,"[u'Data Scientist\nzData Inc\nJune 2017 to Present\nDeveloped a Natural Language Processing text analysis proof-of-concept using deep\nlearning methods using Keras package in TensorFlow.\n\n\u2022 Developed comprehensive dashboard solutions using Tableau for the models that have\nbeen created to identify the status of the sensors in Splash Mountain project by managing connections to data sources via live database connections, the creation of\ndata models to drive dashboards, and deployment of final solution to end users.\n\u2022 Perform image analytics using machine learning algorithms in PySpark.\n\n\u2022 Developed chatbot for Michaels Store Inc. in IBM Watson Bluemix using text mining and text classification methods of historical data to train the chatbot in order to have a\nbetter performance.', u'Data Scientist\nHewlett Packard Enterprise\nJune 2015 to June 2017\nBuilt fraud detection model using R and machine learning approaches by using\ndifferent statistical models such as Multinomial Logistic Regression, Support Vector\nMachine, and Neural Network.\n\n\u2022 Predictive modeling and churn analysis for insurance company to massive amounts of data using cutting edge software technologies including SQL, R, Python and machine\nlearning approaches.\n\n\u2022 Quantitative research in manipulating and analyzing complex, high-volume, high- dimensionality data from varying data warehouse sources.\n850 Barton Brook Place, Apt # 105, Winter Park Florida, 32792\nCell Phone: (936) 2159706 E-mail: asmiexr99@gmail.com\n\n\u2022 Used R to develop k-means, random forest and decision tree models to classify the members.\n\n\u2022 End-to-end responsibility of project execution including: understanding the plan or vendor problem, gathering data, managing the allocation of resources, and designing\nand implementation of the solution.', u'Research Associate and Teaching Assistant\nStephen F Austin State University\nAugust 2013 to May 2015\n\u2022 Teaching undergraduate mathematics courses\n\n\u2022 Worked on a research problem titled by ""Estimation of a proportion using inverse\nsampling when the data is subject to false-positive misclassification"" using statistical\nsoftware packages SAS and JMP.', u'Marketing Manager and Data Analyst\nRed Sea Waves LLC - Dubai, AE\nJanuary 2012 to August 2013\nPlanned and perform end-to-end data analytics projects of marketing campaigns using\nExcel, SQL, SAS and to evaluate marketing outcomes and draw accurate inferences\n\u2022 Generated detailed ad-hoc and recurring reports\n\u2022 Gathered data required from several sources and provided data driven solution to business problems', u'Instructor/ Research Analyst\nUniversity of Asmara - Asmara, ER\nJune 2003 to July 2011\n\u2022 Taught undergraduate mathematics courses\n\u2022 Advised undergraduate students\n\u2022 Prepared end semester reports of students data; cleaned, managed and analyzed and presented data using SAS and R.']","[u'Master of Science in Statistics in Statistics', u'Bachelors of Science in mathematics in mathematics']","[u'Stephen F Austin State University Nacogdoches, TX\nMay 2015', u'University of Asmara Winter Park, FL\nJuly 2004']","degree_1 : Master of Science in Statistics in Statistics, degree_2 :  Bachelors of Science in mathematics in mathematics"
0,https://resumes.indeed.com/resume/edf0c151e73baebf,"[u'Data Scientist\nA former web developer pivoted to data science with 1.5+ years of experience in Machine and Deep Learning. Able to leverage a heavy dose of machine learning models on large sets of structured, semi-structured and unstructured data for classifications, regressions, predictive analysis, visualizations and a healthy sense of data exploration with storytelling.']","[u'M.S.', u'B.E. in Information Technology']","[u'Computer Science Pace University New York, NY\nAugust 2016 to May 2018', u'Mumbai University Mumbai, Maharashtra\nAugust 2013 to July 2016']","degree_1 : M.S., degree_2 :  B.E. in Information Technology"
0,https://resumes.indeed.com/resume/f0295d64e95ebeba,"[u""OWNER\nTHEO'S FOUNTAIN WORKS - San Antonio, TX\nJanuary 2017 to Present\nOwn and operate small business selling custom pet products. Responsible for all aspects of operation, including part design, creation, and sourcing. Utilize VBA in Excel to produce automatically updated profit and loss sheets based on sales and cost datasheets. Create cost tracking sheets pulling from multiple locations which are converted into customized, automatically updating, visualization tools for quick and easy trend tracking. Responsible for marketing, accounting, cost analysis, and production. Designed custom spreadsheets calculating costs and profits down to the penny, with construction and lead times automatically calculating for easy analysis. (https://www.etsy.com/shop/TheosFountainWorks)"", u'DATA SCIENTIST\nDANNEMILLER HEALTHCARE EDUCATION\nAugust 2016 to Present\nIn-Depth knowledge of continuing medical education data collection techniques and reporting. Build reports detailing program specifics and success. Daily use of SQL and VBA programing languages. Code macros using VBA to process data inputs automatically. Construct pivot tables and pivot charts for easy and assessable data streams. Use predictive analysis to identify trends in sales by correlating input variables such as demographics, location, profession, specialties, course or credit type, ect. Work with SQL to design and build database functions, including creating and maintaining database structures. Build reports and presentations regarding financial efficiency, marketing effectiveness and educational course success. Determine through statistical analysis the effectiveness of faculty and content through feedback gathered from students and faculty. Compile and submit ACCME annual accreditation reporting data regarding all activities and participants.', u'STRATEGIC ACCOUNT MANAGER\nUPS - San Antonio, TX\nSeptember 2012 to August 2016\n\xb7 Utilize SQL and VBA programing languages to retrieve and analyze logistics data from live databases. SQL used to pull relevant data from live databases to structure into applicable and manipulatable format. Created and maintained custom VBA programs to manipulate data and output results automatically. Create tracking tables and charts to present to region manager and other decision makers. Analyze sales and operational data to understand impacts and trends regarding enterprise customers. Worked individually and in teams in a fast paced and productive environment focused on company success. Subject matter expert in internal procedures and programs, able to make and recommend changes across departments. Research and solve complex problems using internal and external resources and partners. Create schedules and agendas for field operations and sales resources.']",[u'BBA in MANAGEMENT SCIENCE'],"[u'UNIVERSITY OF TEXAS AT SAN ANTONIO San Antonio, TX\nJanuary 2010']",degree_1 : BBA in MANAGEMENT SCIENCE
0,https://resumes.indeed.com/resume/cdb97bb332631bb7,"[u'Research Assistant\nUniversity of Pennsylvania - Philadelphia, PA\nSeptember 2017 to Present\n--Multi-threaded scrap of 5 million reviews from Goodreads website using BeautifulSoup\n--Airflow Data pipeline: Scrape + Cleaning of data + dumping into MySQL; modelled Goodreads database, performed EDA\n--Successfully framed and tested hypotheses; applied PCA to extract important groups of books preferred by users', u'Data Scientist Intern\nSprint, Kansas\nMay 2017 to August 2017\nWorked on review classification in Customer Analytics department ; developed SQL queries to fetch text from Teradata\n\u25cf Cleaned, analyzed and merged data from various surveys; applied topic modelling using Latent Dirichlet Allocation in R\n\u25cf Successfully developed Naive Bayes model for 2-level classification (16 categories) of text reviews in Python\n\u25cf Impact: Successfully extended influential keywords list for existing rule based classification from 400 to 2000', u'Data Science Intern\nSPRINT CORPORTATION - Overland Park, KS\nMay 2017 to August 2017\n--Worked on review classification in Customer Analytics department; developed SQL queries to fetch text from Teradata\n--Cleaned, analyzed and merged data from various surveys; applied topic modelling using Latent Dirichlet Allocation in R\n--Successfully developed Naive Bayes model for 2-level classification (16 categories) of text reviews in Python\n--Impact: Successfully extended influential keywords list for existing rule based classification from 400 to 2000']","[u'MS in Computer & Information Science(Data Science)', u'in Technology']","[u'University of Pennsylvania Philadelphia, PA\nAugust 2016 to May 2018', u'Sardar Vallabhbhai National Institute of Technology\nJanuary 2015']","degree_1 : MS in Compter & Information Science(Data Science), degree_2 :  in Technology"
0,https://resumes.indeed.com/resume/a665b038ffb9e9ad,"[u'Research Scientist III\nUniversity of Florida, Child Institute of Research Health - Gainesville, FL\nSeptember 2016 to Present\n\u2022 Lead scientist - designs, organizes and executes all experiments.\n\u2022 Broad financial and lab management - purchasing, accounting, and disbursement of grant funds.\n\u2022 Educate undergrad and grad students - experiment design, lab safety, lab techniques, etc.\n\u2022 Insure lab compliancy: Biosafety, IACUC, and ACS guidelines/requirements.\n\u2022 Experience with NIH grant writing and submission.\n\u2022 Manages all lab orders and inventory of supplies, etc.\n\u2022 Stereotactic brain surgery of rodents: craniotomies for visual tracking.\n\u2022 Experienced in RNA/DNA isolation, RT- qPCR, IHC, Western Blots, etc.\n\u2022 Skilled in brain imaging - confocal and two-photon microscopy.\n\u2022 Improve/revise existing protocols for enhanced data collection.', u""Biological Scientist II\nUniversity of Florida - Gainesville, FL\nFebruary 2010 to September 2016\n\u2022 Vast experience and understanding of clinical research projects and processes.\n\u2022 Supervision and oversight of BMT Clinical Trials including, sample processing, protocol writing,\nmanagement of data, etc.\n\u2022 Comprehensive knowledge of blood diseases - leukemia, lymphoma, and myelodysplastic syndrome.\n\u2022 Proficient in cell culture of primary leukemia cells, leukemic cell lines, and primary bone marrow cells.\n\u2022 In depth knowledge of flow cytometry using BD FACSDiva.\n\u2022 Lead scientist in establishing functional cell studies for NIH NHLBI CCTRN Biorepository Core Lab.\n\u2022 Writes/edits SOP's and IACUC protocols to ensure compliance for in vivo and in vitro studies.\n\u2022 Oversee experiments of undergraduate and graduate students to ensure success of their projects.\n\u2022 Creates, organizes, and manages lab experiments and protocols for lab personnel."", u""Clinical Research Coordinator/Data Manager\nUF Shands Cancer Center - Gainesville, FL\nAugust 2009 to February 2010\n\u2022 Management of research protocol development, informed consents and other clinical trial materials.\n\u2022 Understanding of UF IRB's/WIRB regulations and compliance.\n\u2022 Comprehensive knowledge of IND/IDE applications and submissions.\n\u2022 Supported Principal Investigators and research staff with abstract/manuscript layouts.\n\u2022 Worked closely with Principal Investigators, oncology clinic, and study staff to support the clinical\ntrials.\n\u2022 Highly attentive to detail, ability to meet deadlines and work independently.\n\u2022 Able to problem solve and communicate effectively and efficiently.\n\u2022 Skilled in using PeopleSoft, OnCore, MS Office Suite, Prism and Adobe."", u'Juice Plus+ Representative\nJuice Plus+ Virtual Franchise - Gainesville, FL\nJuly 2006 to August 2009\n\u2022 Informed and consulted individuals on the health benefits of Juice Plus+.\n\u2022 Attended conferences and business meetings to learn how to build my business.\n\u2022 Helped build a team of people to form a franchise and educate people on Juice Plus+\n\u2022 Created a website to promote my personal business.\n\u2022 Learned the art of cold calling and to not be discouraged by a ""no"".', u""Pharmacy Technician/Patient Educator\nWise's Pharmacy - Gainesville, FL\nFebruary 1991 to June 1998\n\u2022 Assisted pharmacist in filling scripts - data entry, drug identification & analysis, patient counseling.\n\u2022 Provided excellent customer service - maintain a friendly and outgoing demeanor, listen to a patient's\nproblems/concerns, and offer advice where needed according to the pharmacist's instruction.\n\u2022 Maintained inventory, placed orders and stocked pharmacy supplies/drugs as needed.\n\u2022 Provided proper bookkeeping, accounts payable and receivable, and record keeping.""]","[u'Bachelor of Science in Biochemistry', u'Associate of Arts in Chemistry']","[u'University of Florida Gainesville, FL\nMay 2008', u'Santa Fe Community College Gainesville, FL\nMay 2006']","degree_1 : Bachelor of Science in Biochemistry, degree_2 :  Associate of Arts in Chemistry"
0,https://resumes.indeed.com/resume/1411f158bf3b6ade,"[u""Data Scientist\nAmadeus - Waltham, MA\nApril 2017 to Present\nDescription: Amadeus powers travel. Amadeus' solutions connect travelers to the journeys they want, linking them via travel agents, search engines and tour operators to airlines, airports, hotels, cars and railways. Technology has always been critical to developing global travel, increasing scale, choice and access. We have developed our technology in partnership with the travel industry. We combine a deep understanding of how people travel with the ability to design and deliver the most complex, trusted, and critical systems our customers need.\n\nResponsibilities:\n\u2022 Identifying the Customer and account attributes required for MDM implementation from disparate sources and preparing detailed documentation.\n\u2022 Performing data profiling and analysis on different source systems that are required for Customer Master.\n\u2022 Worked closely with the Data Governance Office team in assessing the source systems for project deliverables.\n\u2022 Used T-SQL queries to pull the data from disparate systems and Data warehouse in different environments.\n\u2022 Used Data Quality validation techniques to validate CriticalDataelements (CDE) and identified various anomalies.\n\u2022 Extensively used open source tools - RStudio (R) and Spyder (Python) for statistical analysis and building the machine learning.\n\u2022 Assessed existing EDW technologies and methods to ensure our EDW/BI architecture meet the needs of the business, enterprise and allows for business growth.\n\u2022 Worked extensively with Sqoop for importing and exporting the data from HDFS to Relational Database systems like Oracle and vice-versa.\n\u2022 Developed MapReduce programs to parse the raw data, populate staging tables and store the refined data in partitioned tables in the EDW.\n\u2022 Design, prototype, implement the NLP models using MSFT Cognitive based NLGPerforming Data Validation / Data Reconciliation between disparate source and target systems (Salesforce, Cisco-UIC, Cognos, DataWarehouse) for various projects.\n\u2022 Writing complexSQL queries for validating the data against different kinds of reports generated by Cognos.\n\u2022 Interaction with Business Analyst, SMEs, and other Data Architects to understand Business needs and functionality for various project solutions\n\u2022 Researched, evaluated, architected, and deployed new tools, frameworks, and patterns to built sustainable Big Data platforms for the clients\n\u2022 Used Amazon Cloud EC2 along with Amazon SQS to upload and retrieve project history.\n\u2022 Used Python and Django to interface with the jQuery UI and manage the storage and deletion of content.\n\u2022 Rewrite existing Python/Django modules to deliver certain format of data.\n\u2022 Extracting data from different databases as per the business requirements using Sql Server Management Studio.\n\u2022 Built and tested a ML/ NLP component for Apple Business Chat.\n\u2022 Interacting with the ETL, BIteams to understand / support on various ongoing projects.\n\u2022 Extensively using MS Excel for data validation.\n\u2022 Generating weekly, monthly reports for various business users according to the business requirements. Manipulating/mining data from database tables (Redshift, Oracle, Data Warehouse)\n\u2022 Providing analytical network support to improve quality and standard work results.\n\u2022 Create statistical models using distributed and standalone models to build various diagnostics, predictive and prescriptive solution.\n\u2022 Interface with other technology teams to load (ETL), extract and transform data from a wide variety of data sources\n\u2022 Utilize a broad variety of statistical packages like SAS, R, MLIB, Graphs, Hadoop, Spark, Map Reduce and others\n\u2022 Provides input and recommendations on technical issues to Business&DataAnalysts, BIEngineers and DataScientists.\n\nEnvironment: Data Governance, SQL Server, ETL, MS Office Suite - Excel (Pivot, VLOOKUP), DB2, R, Python, NLP, Visio, HP ALM, Agile, Sypder, Word, Azure, MDM, SharePoint, Data Quality, Tableau and Reference Data Management."", u""Data Scientist\nLogMeIn - Boston, MA\nJanuary 2016 to March 2017\nDescription: LogMeIn, Inc, is a provider of software as a service and cloud-based remote connectivity services for collaboration, IT management and customer engagement. The company's products give users and administrators access to remote computers.\n\nResponsibilities:\n\u2022 Developing Data Mapping, Data Governance, Transformation and Cleansing rules for the Master Data Management (MDM) Architecture involving OLTP, ODS and OLAP.\n\u2022 Providing source to target mappings to the ETL team to perform initial, full, and incremental loads into the target data mart.\n\u2022 Conducting JAD sessions, writing meeting minutes, collecting requirements from business users and analyze based on the requirements.\n\u2022 Involved in defining the source to target data mappings, business rules, and data definitions.\n\u2022 Transformation on the files received from clients and consumed by Sql Server.\n\u2022 Working closely with the ETL, SSIS, SSRSDevelopers to explain the complex Data Transformation using Logic.\n\u2022 Worked on DTS Packages, DTS Import/Export for transferring data between SQLServer2000 to 2005.\n\u2022 TensorFlow models trained with GPU instances in AWS using batch of training samples from S3.\n\u2022 Serving of TensorFlow models using TensorFlow Serving\n\u2022 Worked on moving data from Hive tables into HBase for real time analytics on Hive tables.\n\u2022 Handled importing of data from various data sources, performed transformations using Hive. (External tables, partitioning)\n\u2022 Responsible for creating Hive tables, loading the structured data resulted from Map Reduce jobs into the tables and writing hive queries to further analyze the logs to identify issues and behavioral patterns.\n\u2022 Exploring with Spark improving the performance and optimization of the existing algorithms in Hadoop using Spark context, Spark-SQL, Data Frame, pair RDD's, Spark YARN.\n\u2022 Developed Spark code and Spark-SQL/Streaming for faster testing and processing of data.\n\u2022 Experience in deploying data from various sources into HDFS and building reports using Tableau.\n\u2022 Developed a data pipeline using Kafka and Strom to store data into HDFS.\n\u2022 Performing Data Profiling, Cleansing, Integration and extraction tools (e.g. Informatica)\n\u2022 Utilizing Informatica toolset (Informatica Data Explorer, and Informatica Data Quality) to analyze legacy data for Data Profiling.\n\u2022 A highly immersive Data Science program involving Data Manipulation and Visualization, Web Scraping, Machine Learning, GIT, SQL, Unix Commands, Python programming, NoSQL, MongoDB, Hadoop.\n\u2022 Defining the list codes and code conversions between the source systems and the data mart using Reference Data Management (RDM)\n\u2022 Applying data cleansing/data scrubbing techniques to ensure consistency amongst data sets.\n\u2022 Extensively using ETL methodology for supporting data extraction, transformations and loading processing, in a complex EDW using Informatica.\n\nEnvironment: Power Center 9.x/8.1, Informatica Analyst, MDM, MS Excel, Agile, IDD, TensorFlow,\nData Governance, Oracle 11g, Meta Data, Sql Server, SOA, SSIS, SSRS, IDQ, Data Lineage, ETL, UNIX, T-SQL, HP Quality Center 11, RDM (Reference Data Management)"", u""Data Analyst\nNetApp - Wichita, KS\nJune 2014 to December 2015\nDescription:NetApp knows storage backwards and forwards and on premise and in the cloud. The company makes data storage systems used by businesses for archiving and backup. It offers products for hybrid cloud storage, extending customers' IT infrastructure to the cloud environments of Amazon, Google, IBM, and Microsoft. NetApp enables customers' use of flash storage, another relatively new market for the company\n\nResponsibilities:\n\u2022 Worked on data cleaning and reshaping, generated segmented subsets using Numpy and Pandas in Python.\n\u2022 Generated cost-benefit analysis to quantify the model implementation comparing with the former situation.\n\u2022 Designed both 3NF data models for ODS, OLTP systems and dimensional data models using Star and Snowflake Schemas.\n\u2022 Developed Teradata SQL scripts using OLAP functions like rank and rank () Over to improve the query performance while pulling the data from large tables.\n\u2022 Conducted model optimization and comparison using stepwise function based on AIC value\n\u2022 Wrote and optimized complex SQL queries involving multiple joins and advanced analytical functions to perform data extraction and merging from large volumes of historical data stored in Oracle 11g, validating the ETL processed data in target database.\n\u2022 Worked on model selection based on confusion matrices, minimized the Type II error\n\u2022 Developed Pythonscripts to automate data sampling process. Ensured the data integrity by checking for completeness, duplication, accuracy, and consistency.\n\u2022 Applied various machine learning algorithms and statistical modeling like decision tree, logistic regression, Gradient Boosting Machine to build predictive model using scikit-learn package in Python.\n\u2022 Developed MapReduce Python modules for machine learning & predictive analytics in Hadoop on AWS.\n\u2022 Performed Smoke test to do the primary checks like record counts, Column matching for database and Dashboard testing.\n\u2022 Performed testing at SIT (System Integration Testing) level and UAT (User Acceptance testing) level.\n\u2022 Gathered requirements from the development team and database developers to analyze the tables and entity relationships for understanding the database.\n\u2022 Maintenance of large data sets, combining data from various sources by Excel, Enterprise, and SAS Grid, Access and SQL queries.\n\u2022 Developed and implemented SSIS, SSRS and SSAS application solutions for various business units across the organization.\n\u2022 Identified the variables that significantly affect the target\n\u2022 Continuously collected business requirements during the whole project life cycle.\n\u2022 Generated data analysis reports using Matplotlib, Tableau, successfully delivered and presented the results for C-level decision makers.\n\nEnvironment: SQL Server 2008, Teradata 13, Erwin 8, Oracle 9i, PL/SQL, SQL*Loader, ODS, OLAP, SSAS, Informatica Power Center, OLTP."", u""Data Modeler\nMedStar Franklin Square Hospital - Baltimore, MD\nMarch 2013 to May 2014\nRole: Data Modeler\n\nDescription: The HI-Exchange Project dealt with development of an online HIE (Health information exchange) and a secure web portal to enable authorized Franklin Square Hospital providers to have fast and easy access to patient's electronic health record. The HI-Exchange web portal features EMR functions and decision Support tools for better care management.\n\nResponsibilities:\n\u2022 Installation of Talend Studio.\n\u2022 Architecting and design of data warehouse ETL processes.\n\u2022 Design and implement the ETL Data model and create staging, source and Target tables in SQLserverdatabase.\n\u2022 Demo of POC built for the prospective customer and provide guidance and gather the feedback to backend ETL testing on SQL Server 2008 using SSIS.\n\u2022 Gathering and analysis requirements definition meetings with business users and document meeting outcomes.\n\u2022 Create the Operational manual Document.\n\u2022 Develop Integrations jobs to transfer data from source system to Hadoop.\n\u2022 Task allocation for the ETL and Reporting team.\n\u2022 Create Integration Jobs to backup a copy of data in network file system.\n\u2022 Application of business rules on the data being transferred.\n\u2022 Technical design documents for Transformation processes.\n\u2022 Communicate effectively with client and their internal development team to deliver product functionality requirements.\n\nEnvironment: MS Office, Hadoop, ETL, ODS, OLAP, SQL Server 2008 and Talend Studio."", u'Java & J2EE developer\nJDA Software Group, Inc - Hyderabad, Telangana\nApril 2011 to January 2013\nDescription: JDA Software Group, Inc. is an American software and consultancy company (owned by New Mountain Capital), providing supply chain management, manufacturing planning, retail planning, store operations and collaborative category management solutions headquartered in Scottsdale, Arizona\n\nResponsibilities:\n\u2022 Design patterns of BusinessDelegates, ServiceLocator and DTO are used for designing the web module of the application.\n\u2022 Implemented various J2EE design patterns for designing this application.\n\u2022 Developed the Web Interface using Struts, JavaScript, HTML and CSS.\n\u2022 Used Factory, Singleton design patterns for implementing enterprise modules/DTO\'s.\n\u2022 Extensively used the struts application resources properties file for error codes, views labels and for Product Internationalization.\n\u2022 Extensively used the Struts controller component classes for developing the applications.\n\u2022 Struts 1.2 has provided its own Controller component and integrates with other technologies to provide the Model and the View for the Model, used Struts to interact with standard data access technologies, like JDBC and EJB.\n\u2022 Used RAD (Rational Application Developer 7.0) as a Development platform\n\u2022 Struts Framework provided the functionality to validate the form data. It\'s used to validate the data on the users browser as well as on the server side. Struts Framework emits the java scripts and it\'s used to validate the form data on the client browser.\n\u2022 JavaBeans were used to store in a number of different collections of ""attributes"". The JavaServer Pages (JSP) Specification defines scope choices.\n\u2022 Consumed webservices using AxisWebservices.\n\u2022 Used JavaScript for the web page validation and Struts Validator for server side validation of data.\n\u2022 Developed SQLstoredprocedures and prepared statements for updating and accessing data from database.\n\u2022 Used JDBC and Hibernate to connect to the database using Oracle.\n\nEnvironment: HTML, CSS, JavaScript, Struts, J2EE, JDBC, oracle 10g.', u'Java Developer\nTejas Networks - Bengaluru, Karnataka\nJune 2009 to March 2011\nDescription: Tejas Networks is an India-based optical and data networking products company. Tejas designs, develops, and sells high-performance and cost-competitive products to telecommunications service providers, internet service providers, utilities, defence and government entities in over 60 countries. Tejas products utilize a programmable software-defined hardware architecture with a common software code-base that delivers an app-like ease of development and upgrades of new features and technology standards. Tejas is ranked amongst top 10 suppliers in the global optical aggregation segment.\n\nResponsibilities:\n\u2022 Involved in implementingJ2EEdesignpatterns for designing this application.\n\u2022 Used Factory, Singleton design patterns for implementing enterprise modules/DTO\'s.\n\u2022 Extensively used the struts application resources properties file for error codes, views labels and for Product Internationalization.\n\u2022 Developed the Web Interface using Struts, HibernatesJavaScript, HTML and CSS.\n\u2022 Extensively used the Struts controller component classes for developing the applications.\n\u2022 Struts 1.2 has provided its own Controller component and integrates with other technologies to provide the Model and the View for the Model, used Struts to interact with standard data access technologies, like JDBC and EJB.\n\u2022 Used RAD (Rational Application Developer 7.0) as a Development platform\n\u2022 Used SVN for source code control and JUNIT for unit testing.\n\u2022 JavaBeans were used to store in a number of different collections of ""attributes"". The JavaServer Pages (JSP) Specification defines scope choices.\n\nEnvironment: HTML, CSS, JavaScript, Struts, J2EE, JDBC, Rational Application Developer, JSP, SVN, Oracle 10g.']","[u""Bachelor's""]",[u''],"degree_1 : ""Bachelors"""
0,https://resumes.indeed.com/resume/0d4258c3389adc25,"[u'Data Scientist Stansberry Research\nDynamoDB and PostgreSQL - Baltimore, MD\nJanuary 2017 to Present\nManagement of databases in DynamoDB and PostgreSQL. Development of machine learning models (NLP) for sentiment analysis.\n\n\u2713 Management of databases in DynamoDB and PostgreSQL.\n\u2713 Development of machine learning models for sentiment analysis using Natural Language Processing.\n\u2713 Development and analysis of financial strategies based on historical data (backtesting).', u'Data Scientist (Consultant)\nUniversities Space Research Association - Columbia, MD\nJanuary 2015 to January 2017\nLead forward-thinking management and comprehensive visualization of 3D astronomical dataset using Python / TkInter.\n\n\u2713 Spearheaded data processing, assembling, and analysis of astronomical data cubes.\n\u2713 Obtained cubes with cutting-edge instruments installed on ground-based telescopes.\n\u2713 Developed ad-hoc programs in Python for efficient data analysis and visualization purposes.', u""Science Collaborator NASA's Goddard Space Flight Center - Greenbelt, MD\nJanuary 2012 to January 2017\nCapitalize on the opportunity to direct a top-performing global team of professionals and amateur astronomers in effectively monitoring one of the most massive stars in the Galaxy, \u2022 Carinae, which led to approximately 400 spectra. Direct time series and statistical analysis on a spectroscopic dataset using Python and R to understand the physical nature of periodic variations observed in \u2022 Carinae, and achieved publication in a peer-reviewed scientific paper.\n\n\u2713 Expertly processed and analyzed 78 astronomical data cubes obtained with the Hubble Space Telescope.\n\u2713 Collaborated on one of the most comprehensive, detailed spectroscopic datasets in the history of \u2022 Carinae.\n\u2713 Improved knowledge of supermassive star \u2022 Carinae by developing ad-hoc tools in Python and IDL to perform quantitative measurements of physical properties in astronomical data obtained with Hubble Space Telescope."", u""Science Collaborator NASA's Goddard Space Flight Center - Kalamazoo, MI\nJanuary 2014 to January 2015\nStrategically steered implementation of a large-scale database for atomic spectroscopy known as AtomPy to allow users to select and download high-volume spreadsheets containing up-to-date and detailed atomic data for various ionic species.\n\n\u2713 Successfully developed a graphical user interface for atomic data visualization using Python / TkInter."", u'Post-Doctoral Researcher University - S\xe3o Paulo, BR\nJanuary 2009 to January 2012\nPlayed a vital role in developing ad-hoc tools in Python and Shell Script for management, processing, and data visualization of spectroscopic datasets developed from both present and / or historical spectra of \u2022 Carinae star.']","[u'Ph.D. in Astronomy', u'M.S. in Astronomy', u'B.S. in Physics']","[u'University of S\xe3o Paulo S\xe3o Paulo, BR\nJanuary 2009', u'University of S\xe3o Paulo S\xe3o Paulo, BR\nJanuary 2005', u'S\xe3o Paulo State University S\xe3o Paulo, BR\nJanuary 2002']","degree_1 : Ph.D. in Astronomy, degree_2 :  M.S. in Astronomy, degree_3 :  B.S. in Physics"
0,https://resumes.indeed.com/resume/cdbd3899691be23a,"[u""Data Scientist III\nEARLY WARNING SERVICES - Scottsdale, AZ\nJanuary 2012 to January 2018\nDeveloped and implemented various machine learning models with banking and credit cards data.\n\u2022 Performed Text Mining and analyzed P2P transactions' memo containing a brief description to gain information for marketing team to set marketing strategy.\n\u25e6 Developed normalization methods to word, sentence, multiple sentences, then transformed memos to single idea.\n\u25e6 Executed Sentiment Analysis and classified a person's sentiment status from his / her memos after stemming, removing stop words / neutral words, cumulated word sentiment scores to memo scores, then cumulated memo scores to classifying if account holder is positive, negative, or neutral.\n\u25e6 Provided Marketing Analysis (Association Rule) by analyzing if certain combinations of payment categories that occur together frequently among P2P transactions.\n\u2022 Created Mistype Model as part of the Company's ID related project.\n\u25e6 Compared names in application against the mode (most frequent found) name in all historical data per SSN, Phone, Address, and Driver License # for those that don't match, but close enough, were classified their mistype events (skipped / extra) letter, fat finger caused mistypes, etc.\n\u25e6 Applied various Machine Learning models to fit the data and determined which combinations were likely done by fraudsters (intentional mistype).\n\u2022 Created Money Mules' Credit Card Account Spending Profile - Money mule is a person who transfers $ acquired illegally and criminal uses a mule to hide the criminal's true identity and location.\n\u25e6 Created (card present) credit card spending profile (1) per account and (2) merchant category code and account.\n\u25e6 Applied Clustering algorithm to separate patterns\n\u25e6 Applied Principal Component Analysis reducing dimensions and built control charts (Statistical Process Control) around principal components to detect any unusual patterns.\n\u2022 Created ACH accounts' profiles and applied clustering algorithms to establish finite number of patterns.\n\u2022 Performed Network Analysis where the memo contains certain keywords that could be involved in fraud activities and visualized the fraud network using Gephi software.\n\u2022 Improved the logic to categorize what the ACH transaction is for, i.e., income, card payment, etc. The results were used to help all other Analytic projects to create features for what each banking transaction is for.\n\nPeter J. Chung pchung3000@yahoo.com\n\nEARLY WARNING SERVICES )\n\u2022 Supported Common Purchase Point (CPP) project, scoring merchants where credit cards were compromised by tracing all common merchants visited by compromised credit / debit cards. Created features such as velocity (physical distance from one card present transaction to next per time) and applied multiple machine learning models to fit the data.\n\u2022 Performed quarterly / yearly model maintenances to validate if the models still fit the data after a few years of use.\n\u2022 Supported Income Verification project to verify an account holder's income.\n\u2022 Performed Marketing Analysis to know whether traditional media or twitter have higher impact on #P2P transactions for competitors. So, analyzed each competitors' #P2P transactions identified in ACH data against periodic tweet reaches and periodic traditional media impressions."", u'Data Engineer & Data Analyst\nMERRILL LYNCH / BANK OF AMERICA - Scottsdale, AZ\nJanuary 2006 to January 2012\nPerformed various statistical analyses and data ETL for various formats.\n\u2022 Executed Monte Carlo Simulation, simulating stock prices to valuate stock options and market performance awards.']","[u'PhD', u'Master of Science in Industrial Engineering', u'Bachelor of Science in Mathematics']","[u'Arizona State University Tempe Tempe, AZ', u'Arizona State University Tempe, AZ', u'New Mexico State University Las Cruces, NM']","degree_1 : PhD, degree_2 :  Master of Science in Indstrial Engineering, degree_3 :  Bachelor of Science in Mathematics"
0,https://resumes.indeed.com/resume/5dbbc578cce5e0c5,"[u'Data Scientist Intern\nSEGA Networks - San Francisco, CA\nOctober 2017 to October 2017\n\u2022 Classified mobile games for product development purpose by scraping mobile games data from appannie.com and developing a Na\xefve Bayes model\n\u2022 Detected suspicious behavior using XGBoost model\n\u2022 Predicted future purchases and churn rate utilizing Buy Till You Die model', u'Data Analyst Intern\nMarsh Limited Liability Company - Shanghai, CN\nJune 2016 to August 2016\n\u2022 Analyzed and visualized data to distinguish potential clients, and conducted Shenzhen city property insurance market research\n\u2022 Assisted with employee insurance cross-selling, recorded meetings, and collaborated with colleagues to collect data', u'Tech Support Consultant\nIndiana University Information Technology Service - Bloomington, IN\nJanuary 2015 to August 2016\nBloomington, IN Jan 2015-Aug 2016\nrole: Tech Support Consultant\n\u2022 Assisted customers with technical problems related to software and computer peripheral equipment\n\u2022 Introduced 3D printing services to customers, and managed 3D printing services']","[u'M.S. in Data Science', u'B.S. in Applied Mathematics']","[u'University of San Francisco\nJuly 2017 to June 2018', u'Indiana University\nDecember 2016']","degree_1 : M.S. in Data Science, degree_2 :  B.S. in Applied Mathematics"
0,https://resumes.indeed.com/resume/561d3348d0b34fe0,"[u'Data Scientist Intern\nGE Aviation - Cincinnati, OH\nMay 2017 to August 2017\nPython and R: Performed statistical analysis, built data visualizations, and investigated new data sources to answer\nquestions about manufacturing processes\n- Apache Spark: Developed an event detection algorithm in Apache Spark to process millions of observations and clean\ndata for further analysis\n- Machine Learning: Utilized clustering methods to investigate data sources and perform pattern recognition', u'Data Analyst Intern\nOhio Valley Medical Center - Wheeling, WV\nMay 2016 to August 2016\nDashboards and Visualizations: Created performance dashboards and built interactive reports to drive information\nbased decision making and improve patient care\n- SQL: Used store procedures and advanced queries to interact with massive medical databases and solve problems\n- Automation: Automated more than thirty routine reports using command line scripts removing the need for propriety\ntools and reducing costs']","[u""Bachelor's""]",[u''],"degree_1 : ""Bachelors"""
0,https://resumes.indeed.com/resume/38bdd548487a399b,"[u'Data Scientist\nGENERAL ELECTRIC - Atlanta, GA\nOctober 2016 to Present\nHelped develop an algorithm in Python using machine learning techniques to match turbine and generator records within 2\npairs of datasets\no Used boosting and fuzzy matching to assess similarity\n\u2022 Helped extend an optimization algorithm in Python for market share allocation with a predicted savings of $4 million\no Incorporated new business constraints for combustion commodities\n\u2022 Analyzed non-conformance parts using clustering techniques and decision tree and logistic regression modeling\n\u2022 Coded a Monte Carlo simulation in Python to predict turbine outages\n\u2022 Developed machine learning algorithms to predict prices for parts and to aid in price negotiation\no Explored linear regression, regression trees, kernel regression, and boosting\n\u2022 Developed an algorithm to classify parts based on text fields using TF-IDF, random forest, naive bayes, and logistic\nregression', u'Data Scientist\nNCR CORPORATION - Duluth, GA\nJune 2016 to October 2016\nUsed 2 different techniques to perform anomaly detection on ATM and POS machine work orders\no Used DBSCAN clustering with 6 metrics and created Tableau dashboards to assess results\no Used the AnomalyDetection R Package to pinpoint anomalous work order volumes', u'Teaching Assistant for Probability and Random Processes\nCARNEGIE MELLON UNIVERSITY - Pittsburgh, PA\nAugust 2015 to May 2016\no Graded weekly homework and quizzes\no Held weekly office hours to answer questions involving homework and course content', u""Big Data Engineer Intern\nNCR CORPORATION - Duluth, GA\nJune 2014 to August 2014\n\u2022 Used Google's Geocoding API and Python to obtain geocoding information and save $25,000\n\u2022 Analyzed 1 month of Twitter data using the Twitter API and Python for marketing research\no Ran Sentiment Analysis, LDA, TF-IDF, and clustering SQL-MapReduce functions on the data"", u'Inventory Planning & Replenishment Supply Chain Intern\nTHE HOME DEPOT - STORE SUPPORT CENTER - Atlanta, GA\nMay 2013 to August 2013\n\u2022 Shadowed Import Analysts to learn manual inventory transfer process across distribution centers\n\u2022 Optimized the manual transfer process and reduced time spent by Import Analysts from 4 hours to 2 hours']","[u""Master's of Statistical Practice in Statistical Methods"", u'Bachelor of Science in Industrial Engineering in Industrial Engineering']","[u'CARNEGIE MELLON UNIVERSITY Pittsburgh, PA\nAugust 2015 to May 2016', u'GEORGIA INSTITUTE OF TECHNOLOGY Atlanta, GA\nAugust 2011 to May 2015']","degree_1 : ""Masters of Statistical Practice in Statistical Methods"", degree_2 :  Bachelor of Science in Indstrial Engineering in Indstrial Engineering"
0,https://resumes.indeed.com/resume/0a924ab2f979d6e6,"[u""Data Analyst\nStatistical Consulting Center - Los Angeles, CA\nSeptember 2016 to December 2017\n\u2022 Performed data analysis to create metrics and performance dashboards for City of LA Mayor's Office and LAPD Controllers Office\n\u2022 Developed data mapping which encompassed time reports and police response times through geographic coding of datasets\n\nPROJECTS & LEADERSHIP"", u'Career Development Chair\nStatistics Club, UCLA - Los Angeles, CA\nJune 2016 to December 2017\nOrganized academic programs including career fairs, resume workshops, and mock interviews sessions for 100+ students\n\n\u2022 Facilitated weekly tutoring sessions and mentored students in lower-division statistics courses as well as R programming', u'Participant\nKaggle Competition - Los Angeles, CA\nMarch 2017 to June 2017\n\u2022 Built boosting tree with Xgboost; parameters tuning through Bayesian optimization methods into prediction with an accuracy of 81%\n\u2022 Successfully completed the competition and ranked top 10% amongst all competitors', u'Data Scientist Intern\nCity of Los Angeles - Los Angeles, CA\nAugust 2016 to January 2017\n\u2022 Partnered with 10+ universities and city officials to create project objectives, charts, and guidelines for the Data Science Federation\n\n\u2022 Facilitated meetings with city departments and council; manipulated datasets to create innovative dashboards using R and Tableau\n\u2022 Developed project timelines, identifying metrics to measure activities and progress to formulate a sustainable program within ITA\n\n\u2022 Oversaw 150+ students and interns during various data projects; presented findings to city officials which led to implementation', u""Data Analyst\nLos Angeles, CA\nJanuary 2016 to June 2016\n\u2022 Collaborated with four team members to develop a 15-page business plan for an educational mobile learning application startup\n\n\u2022 Researched financial projections, value proposition, CAC, competition analysis, revenue stream, pricing, and market analysis\n\u2022 Delivered a 5-minute pitch to investors at the TEKONE 2016 entrepreneurship conference as sole student group among 15 groups\n\u2022 Conducted quantitative market research; applied Bass Diffusion model to generate 5-year financial projection and break-even analysis\n\nProject Manager/ Participant\nDataFest / Los Angeles, CA / May 2017\n\n\u2022 Analyzed 2GB+ of raw data from Expedia to provide a comparative sales analysis in comparison to Airbnb to measure performance\n\u2022 Performed statistical analysis using Random Forest method to calculate significant variables affecting Expedia's customer acquisition\n\n\u2022 Introduced solutions and recommendations for Expedia to optimize and streamline marketing strategies based on findings""]",[u'Bachelor of Science in Statistics'],"[u'University of California Los Angeles, CA\nDecember 2017']",degree_1 : Bachelor of Science in Statistics
0,https://resumes.indeed.com/resume/404950f566637da6,"[u'Data Scientist Intern\nBooxby\nDecember 2017 to Present\n\u2022 Booxby is a machine learning platform solving the book discovery problem of publishing industry\n\u2022 Leading the project of predicting the success of books to help publishers allocate marketing budget\n\u2022 Using machine learning and natural language processing to help authors and publishers find the right market for their books, and help readers find the right books', u""Data Analyst\nGausscode Technology Inc - Santa Clara, CA\nFebruary 2016 to May 2017\n\u2022 Worked as the company's first data analyst, responsible for developing automatic data analytics product\n\u2022 Led the development of key metrics that allow clients monitor operation, visualized the metrics through 50+ reports and dashboards using MySQL and Tableau, frequently presented to senior management\n\u2022 Developed user personas for a retail industry client, stimulated client's sales through precision marketing"", u'Data Analyst\nGoogle - Mountain View, CA\nAugust 2015 to February 2016\n\u2022 Developed strategic metrics for Google Express to exceed SLA in fulfillment and customer experience through analyzing large transactional datasets by SQL and R\n\u2022 Converted analytical insights into reports and dashboards, from which cross-functional colleagues can effectively act upon\n\u2022 Highly specialized in ad-hoc data analysis and reporting, frequently presented to senior management']","[u'MS Business in Data Science', u'BA in Economics']","[u'University of Illinois at Urbana-Champaign, Champaign Urbana-Champaign, IL\nAugust 2014 to August 2015', u'Wuhan University Wuhan, CN\nSeptember 2009 to June 2013']","degree_1 : MS Bsiness in Data Science, degree_2 :  BA in Economics"
0,https://resumes.indeed.com/resume/dd70bdfd53e57164,"[u""Data Scientist\nMercy's Health and Technology\nMay 2016 to Present\nResponsibilities:\n\u2022 Experience working in Data Requirement analysis for transforming data according to business requirements.\n\u2022 Worked thoroughly with data compliance teams such as Data Analysts and Data Engineers to gathered require raw data and define source fields in Hadoop.\n\u2022 Applied Forward Elimination and Backward Elimination for data sets to identify most statically significant variables for Data analysis; the implementation of Bagging and Boosting to enhance the model performance in various datasets\n\u2022 Utilized Label Encoders in Python to create dummy variables for geographic locations to identify their impact on pre-acquisition and post acquisitions by using 2 sample paired t test.\n\u2022 Hands on experience on R packages and libraries like ggplot2, C5.0, h2o, dplyr, reshape2, plotly, RMarkdown, caret, caTools etc.\n\u2022 Expertise in enhancing model performance by using kfold cross validation, hyperparameter tunning using grid search to increase accuracies of different machine learning algorithms\n\u2022 Expertise in transforming business requirements into designing algorithms, analytical models, building models, developing data mining, and reporting solutions that scales across massive volume of structured and unstructured data.\n\u2022 Provided and created data presentation to reduce biases and telling true story to people by pulling millions of rows of data using SQL and performed Exploratory Data Analysis.\n\u2022 Applied Wilcoxon sign test to patient and treatment data for pre-acquisition and post-acquisition for different sectors to find the statistical significance in R programming.\n\u2022 Handled importing data from various data sources, performed transformations using Hive, Map Reduce, and loaded data into HDFS.\n\u2022 Interaction with Business Analyst, SMEs, and other Data Architects to understand Business needs and functionality for various project solutions.\n\u2022 Created sparksession, worked with VectorAssembler, stringindexer, onehotencoding to build different models using spark.sql"", u'Associate Data Scientist /Analyst\nQUASH COSULTING LLC\nOctober 2014 to May 2016\nResponsibilities:\n\u2022 Implemented public segmentation using unsupervised machine learning algorithms by implementing k-means algorithm using Pyspark.\n\u2022 Explored and Extracted data from source XML in HDFS, preparing data for exploratory analysis using data munging.\n\u2022 Used R and python for Exploratory Data Analysis, A/B testing, Anova test and Hypothesis test to compare and identify the effectiveness of Creative Campaigns.\nUsed Spark for test data analytics using MLLib and Analyzed the performance to identify bottlenecks.\n\u2022 Worked on different data formats such as JSON, XML and performed machine learning algorithms in R.\n\u2022 Worked on Linux shell scripts for business process and loading data from different interfaces to HDFS.\n\u2022 Addressed over fitting by implementing of the algorithm regularization methods like L1, L2 and dropout\n\u2022 Used Python, R, SQL to create Statistical algorithms involving Multivariate Regression, Linear Regression, Logistic Regression, PCA, Random forest models, Decision trees, Support Vector Machine for estimating the risks of welfare dependency.\n\u2022 Identified and targeted welfare high-risk groups with Machine learning algorithms.', u'Hadoop Developer\nHCC Insurance Holdings, Inc - Houston, TX\nNovember 2012 to February 2015\nResponsibilities:\n\u2022 Involved in implementation of Hadoop Cluster and Hive for Development and Test Environment.\n\u2022 Analyzed the data as per the business requirements using Hive queries.\nUsed Spark API over Cloudera Hadoop YARN to perform analytics on data in Hive.\nInstalled and configured Hadoop MapReduce, HDFS and Developed MapReduce jobs in Java for data preprocessing.\n\u2022 Collected and aggregated large amounts of log data using Apache Flume and staging data in HDFS for further analysis.\n\u2022 Created Hive Tables, loaded values and generated adhoc-reports using the table data to display strong understanding on Hadoop architecture including HDFS, MapReduce, Hive, Pig, Sqoop and Oozie.\n\u2022 Used spark with Yarn and got performance results compared with map reduce.\nLoaded existing data warehouse data from Oracle database to Hadoop Distributed File System (HDFS)\n\u2022 Loaded data into Hive Tables from Hadoop Distributed File System (HDFS) to provide SQL-like access on Hadoop data.', u'MySQL Database Administrator\nDiscover Financial\nMay 2010 to November 2012\n\u2022 Built and maintained High transactional (OLTP) and multi-dimensional Data Warehouses (OLAP)\n\u2022 Created Database Objects like Schemas, Tables, Views, Stored Procedures, Indexes (cluster/Non-cluster) and Triggers, Buffer Pools, User Roles, User Defined Types (UDT) and functions.', u'Mathematics/science Teacher\nEast High School Anchorage AK - Anchorage, AK\nJune 2004 to May 2010']",[u'Master in Data Science'],[u'Mathematics and Computer Science University of Yaound\xe9\nJanuary 2000'],degree_1 : Master in Data Science
0,https://resumes.indeed.com/resume/5fb5e631342aa042,"[u""Data Scientist\nCGI Group Inc - New York, NY\nFebruary 2017 to Present\nCompany Description:CGI is turning data into diamonds. Big data is becoming a reality for many businesses and government organizations. Those that are successful in leveraging data through analytics are outperforming their peers. Getting more value from data is a key priority, both to gain insights about customers, citizens, employees and operations, and to reduce the costs and complexity of managing ever-growing volumes of data.CGI can help with expertise, solutions and partnerships using our Data2Diamonds approach to be simplifying data management and realizing value from analytics. This framework provides a blueprint for success in putting information to work.\nResponsibilities:\n\u2022 Interact with finance team to build a prediction model for Q1, Q2, Q3, Q4 (Government FY)\n\u2022 Extending company's data with third-party sources of information like GOVWIN, FedBizOpps, etc.\n\u2022 Responsible for quantitative analysis of structured, semi-structured, and unstructured data working in small teams to develop, test, and harden advanced analytical models as required.\n\u2022 Responsible for performing Machine-learning techniques regression/classification to predict the outcomes.\n\u2022 Responsible for design and development of Python programs to prepare transform and harmonize data sets in preparation for modeling.\n\u2022 Handled importing data from various data sources(GOVWIN), performed transformations using Spark, and loaded data into HDFS.\n\u2022 Interaction with Business Analyst, SMEs and other Data Architects to understand Business needs and functionality for various project solutions.\n\u2022 Created custom visualizations for ZOHO Dashboards shared amongst team of data scientists and analysts.\n\u2022 Created visualization and trends for Government spending on Defense DoD, Navy DoN, FDA, DOA, NIH, etc.\n\u2022 Opposition research to build data-sets to perform Triage and BNB analysis.\n\u2022 Developed and maintained data dictionary to create metadata reports for technical and business purpose.\n\u2022 Involved with Data Analysis Primarily Identifying Data Sets, Source Data, Source Meta Data, Data Definitions and Data Formats\n\u2022 Wrote simple and advanced scripts to create standard and adhoc reports for senior managers.\n\u2022 Collaborate the data mapping document from source to target and the data quality assessments for the source data.\n\u2022 Participated in Business meetings to understand the business needs & requirements.\n\u2022 Create workflow using Apache Oozie\n\u2022 Prepare ETLarchitect& design document which covers ETLarchitect, SSISdesign, extraction, transformation and loading of data into dimensional model.\n\u2022 Provide technical & requirement guidance to the team members for ETL -SSISdesign.\n\u2022 Participated in Architect solution meetings & guidance in Dimensional Data Modeling design.\n\u2022 Provided detailed customized representation of data, which helped and improved decision-making process.\n\nEnvironment:Linux, Hadoop, Python, Machine learning, AWS, Spark, HDFS, Python Libraries (Scikit-Learn/SciPy/NumPy/Pandas), SSIS, ETL, Tableau, ZOHO, Apache Oozie, GIT, PyCharm, GOVWIN, FBO\n\nProject 2"", u'Data Scientist\nWalt Disney - Glendale, CA\nOctober 2015 to January 2017\nCompany Description:The Walt Disney Co. is a diversified international family entertainment and media enterprise. It operates through four business segments: Media Networks, Parks & Resorts, Studio Entertainment and Consumer Products & Interactive Media.\nResponsibilities:\n\u2022 Performed Data Profiling to learn about behavior with various features such as caller ID, traffic pattern, location, number validity.\n\u2022 Application of various machine learning algorithms like decision trees, regression models, neural networks, SVM, clustering to identify fraudulent profiles using scikit-learn package in python.\n\u2022 Used clustering technique K-Means to identify outliers and to classify unlabeled data.\n\u2022 Evaluated models using Cross Validation, Log loss function, ROC curves and used AUC for feature selection.\n\u2022 Analyze traffic patterns by calculating autocorrelation with different time lags.\n\u2022 Ensured that the model has low False Positive Rate.\n\u2022 Used Principal Component Analysis in feature engineering to analyze high dimensional data.\n\u2022 Created and designed reports that will use gathered metrics to infer and draw logical conclusions of past and future behavior.\n\u2022 Performed Multinomial Logistic Regression, Random forest, Decision Tree, SVM to classify Scammer, Telemarketer.\n\u2022 Performed data analysis by using Hive to retrieve the data from Hadoop cluster, SQL to retrieve data from Oracle database.\n\u2022 Implemented rule-based expertise system from the results of exploratory analysis and information gathered from the people from different departments.\n\u2022 Performed Data Cleaning, features scaling, features engineering using pandas and NumPy packages in Python.\n\u2022 Developed MapReduce pipeline for feature extraction using Hive.\n\u2022 Created DataQualityScripts using SQL and Hive to validate successful data load and quality of the data. Created several types of data visualizations using Python and Tableau.\n\u2022 Communicated the results with operations team for taking best decisions.\n\u2022 Collected data needs and requirements by Interacting with the other departments within Walt Disney\n\nEnvironment:Linux, Hadoop, Python, Machine learning, MapReduce, HDFS, Python Libraries (NumPy/Pandas), Tableau, GIT, NLP, Neural Network, Hive, SQL\n\nProject 3', u'Data Scientist\nValley National Bank - Wayne, NJ\nDecember 2014 to September 2015\nCompany Description:Valley National Bancorp is a regional bank holding company headquartered in Wayne, New Jersey. Its principal subsidiary, Valley National Bank, operates 209 branches in 30 counties in northern and central New Jersey, Manhattan, Brooklyn, Queens, Long Island and Florida\nResponsibilities:\n\u2022 Supported MapReduce Programs running on clusters.\n\u2022 Evaluated business requirements and prepared detailed specifications that follow project guidelines required to develop written programs.\n\u2022 Configured Hadoop cluster with Name node and slaves and formatted HDFS.\n\u2022 Used Oozie workflow engine to run multiple Hive and Pig jobs.\n\u2022 Performed MapReduce Programs those are running on the cluster.\n\u2022 Developed multiple MapReduce jobs for data cleaning and preprocessing.\n\u2022 Analyzed the partitioned and bucketed data and compute various metrics for reporting.\n\u2022 Involved in loading data from RDBMS and web logs into HDFS using Sqoop and Flume.\n\u2022 Developed Hive queries for Analysis across different banners.\n\u2022 Extracted data from Twitter using Java and Twitter API. Parsed JSON formatted twitter data and uploaded to database.\n\u2022 Launching AmazonEC2 Cloud Instances using Amazon Images (Linux) and Configuring launched instances with respect to specific applications.\n\u2022 Exported the result set from Hive to MySQL using Sqoop after processing the data.\n\u2022 Analyzed the data by performing Hive queries and running Pig scripts to study customer behavior.\n\u2022 Used Hive to partition and bucket data.\n\u2022 Wrote Pig Scripts to perform ETL procedures on the data in HDFS.\n\u2022 Created HBase tables to store various data formats of data coming from different portfolios.\n\u2022 Worked on improving performance of existing Hive Queries.\n\nEnvironment:Linux, Hadoop, Python, MapReduce, HDFS, Python, GIT, Hive, SQL, Pig, Scoop, Flume, AWS, EC2, Twitter API, Oozie\n\nProject 4', u""Data Analyst / Scientist\nNetApp - Wichita, KS\nSeptember 2013 to November 2014\nCompany Description:NetApp knows storage backwards and forwards and on premise and in the cloud. The company makes data storage systems used by businesses for archiving and backup. It offers products for hybrid cloud storage, extending customers' IT infrastructure to the cloud environments of Amazon, Google, IBM, and Microsoft. NetApp enables customers' use of flash storage, another relatively new market for the company\nResponsibilities:\n\u2022 Worked on data cleaning and reshaping, generated segmented subsets using NumPyand Pandas in Python.\n\u2022 Generated cost-benefit analysis to quantify the model implementation comparing with the former situation.\n\u2022 Conducted model optimization and comparison using stepwise function.\n\u2022 Wrote and optimized SQL queries involving multiple joins and advanced analytical functions to perform data extraction and merging from large volumes of historical data stored in Oracle, validating the ETL processed data in target database.\n\u2022 Worked on model selection based on confusion matrices, minimized the Type II error\n\u2022 Developed Python scripts to automate data sampling process. Ensured the data integrity by checking for completeness, duplication, accuracy, and consistency.\n\u2022 Applied various machine learning algorithms and statistical modeling like decision tree, logistic regression, Gradient Boosting Machine to build predictive model using scikit-learn package in Python\n\u2022 Identified the variables that significantly affect the target\n\u2022 Continuously collected business requirements during the whole project life cycle.\n\u2022 Generated data analysis reports using Matplotlib, Tableau, successfully delivered and presented the results for C-level decision makers.\n\nEnvironment:Windows, Hadoop, Python, MapReduce, Oracle, NumPy, Pandas, Matplotlib, Tableau, ETL, ML algorithms.\nProject 5"", u""Data Analyst/Data Modeler\nInfotech Enterprises IT Services Pvt. Ltd - Hyderabad, Telangana\nJuly 2011 to August 2013\nCompany Description:Infotech Enterprises IT Services Pvt. Ltd. (Infotech IT), part of the $ 260 million Infotech Enterprises group, leverages its business process knowledge, technological competence, strategic alliances and strong global presence to offer innovative IT solutions to the Retail Industry.\nResponsibilities:\n\u2022 Data analysis and reporting using MySQL, MS Power Point, MS Access and SQL assistant.\n\u2022 Involved in MySQL, MS Power Point, MS Access Database design and design new database on Netezza which will have optimized outcome.\n\u2022 Involved in writing T-SQL, working on SSIS, SSRS, SSAS, Data Cleansing, Data Scrubbing and Data Migration.\n\u2022 Involved in writing scripts for loading data to target data Warehouse using Bteq, Fast Load, Multiload\n\u2022 Create ETL scripts using Regular Expressions and custom tools (Informatica, Pentaho, and Sync Sort) to ETL data.\n\u2022 Developed SQL Service Broker to flow and sync of data from MS-I to Microsoft's master database management (MDM).\n\u2022 Involved in loading data between Netezza tables using NZSQL utility.\n\u2022 Worked on Data modeling using Dimensional Data Modeling, Star Schema/Snow Flake schema, and Fact& Dimensional, Physical&Logicaldatamodeling.\n\u2022 Generated Stats pack/AWR reports from Oracle database and analyzed the reports for Oracle wait events, time consuming SQL queries, table space growth, and database growth.\n\nEnvironment:MySQL, MS Power Point, MS Access, MY SQL, MS Power Point, MS Access, Netezza, DB2, T-SQL, DTS, SSIS, SSRS, SSAS, ETL, MDM, Teradata, Oracle, Star Schema and Snow Flake Schema.\n\nProject 6"", u'Data Analyst\nNetBlade Solutions - Mumbai, Maharashtra\nMay 2009 to June 2011\nCompany Description:Netblade Solution - Service Provider of web design solutions, website development & mobile development in Thane, Maharashtra. Company Factsheet. Nature of Business Service Provider; Legal Status of Firm Private Limited Company. Know More \u2022 Sure Quote; Hashtag Planet. Our Services. Web Design Solutions.\nResponsibilities:\n\u2022 Collaborating with business and technology teams.\n\u2022 Data Analysis-Data collection, data transformation and data loading the data using different ETL systems like SSIS and Informatica.\n\u2022 Performed source to target mapping as part of data migration from JD Edwards system to Agile PDM system.\n\u2022 Data Migration testing and implementation activities using SSIS and SSRS tools of Microsoft SQL Server 2008.\n\u2022 Responsible for accuracy of the data collected and stored in the corporate support system.\n\u2022 Performed data review, evaluate, design, implement and maintain company database.\n\u2022 Involved in construction of data flow diagrams and documentation of the processes.\n\u2022 Interacted with end users for requirements study and analysis by JAD (Joint Application Development).\n\u2022 Performed gap analysis between the present data warehouse to the future data warehouse being developed and identified data gaps and data quality issues and suggested potential solutions.\n\u2022 Participated in system and use case modeling like activity and use case diagrams.\n\u2022 Analyzed user requirements & worked with data modelers to identify entities and relationship for data modeling.\n\u2022 Actively participated in the design of data model like conceptual, logical models using Erwin. Used Exception handling application block for checking errors/exceptions across the website.\n\u2022 Developed Report Component, so that it retrieves the data by executing Stored Procedures throw Data Access component.\n\nEnvironment:Windows, Oracle, MS Excel, SSIS, Informatica, GAP Analysis, ERWIN']","[u""Bachelor's""]",[u''],"degree_1 : ""Bachelors"""
0,https://resumes.indeed.com/resume/7e089524f5508416,"[u'Data Science Consultant\nNYC Data Science Academy - New York, NY\nMarch 2017 to Present\n- Collaborated with John Jay College & National Network data team on engineering a classification algorithm to detect specific type of violent behavior from police calls reports\n- Devised methods to find patterns within unstructured data\n- Led team of seven to deliver a predictive model and tools John Jay can use to apply to future data sets', u'R&D Applications Engineer\nSolvay SA - Stamford, CT\nNovember 2015 to November 2017\n- Used large data set to identify key operational parameters negatively impacting production goals at customer site\n- Planned exploratory tests at customer sites to gather data, developing a working relationship for future tests\n- Technical team lead on new product development\n- Interfaced with marketing and sales teams to crystallize customer needs and define scope of project\n- Researched and categorized local environmental regulatory policies that drive customer need for sustainable solutions', u'Post-Doctorate Researcher\nJanuary 2015 to October 2015', u'Research Associate\nColumbia University - New York, NY\nSeptember 2008 to December 2014\n- Planned and executed novel experiments to gather data on complex interactions between green-chemicals\n- Extrapolated findings from data, presenting findings at symposia and papers\n- Played a key role in securing multi-year research agreement with industry partner\n- Planned and executed independent research project, publishing papers on novel findings', u'R&D Scientist I\nCiba Specialty Chemicals - Tarrytown, NY\nApril 2005 to August 2008\n- Effectively coordinated with other scientists and analytical departments to reach development targets\n- Presented projects and findings to supervisors and peers for evaluation and feedback of progress and goals\n- Coauthored invention disclosures to be filed for future patent application\n- Coauthor of patented process for creating hydrophilic surfaces']","[u'PhD in Earth and Environmental Engineering', u'MS in Materials Science', u'BS in Chemistry']","[u'Columbia University\nSeptember 2009 to August 2014', u'SUNY Binghamton\nDecember 2003 to June 2004', u'SUNY Binghamton\nSeptember 1999 to December 2003']","degree_1 : PhD in Earth and Environmental Engineering, degree_2 :  MS in Materials Science, degree_3 :  BS in Chemistry"
0,https://resumes.indeed.com/resume/81a930d801477fab,"[u""Machine Learning/Data Scientist\nCanon - Jamesburg, NJ\nJune 2017 to Present\nDescription:Canon U.S.A., Inc. is a leading provider of consumer, business-to-business, and industrial digital imaging solutions to the United States and to Latin America and the Caribbean markets.\n\nResponsibilities:\n\u2756 Assisted the project with Python programming, coding and running QA on the same from time to time.\n\u2756 As an Architect design conceptual, logical and physical models using Erwin and build datamarts using hybrid Inmon and Kimball DW methodologies.\n\u2756 Exploratory data analysis using R to deep dive into internal and external data to diagnose areas of improvement to increase efficiency\n\u2756 Utilized Boosted Decision Tree, Linear and Bayesian Linear Regression Machine Learning models in Microsoft Azure to develop and implement interactive Webservice predictive models\n\u2756 Lead the company's machine learning and statistical modeling effort including building predictive models and generate data products to support customer segmentation, product recommendation and allocation planning; prototyping and experimenting ML/DL algorithms and integrating into production system for different business needs.\n\u2756 Acquired and cleaned using Talend and structure data from multiple source including external and internal databases.\n\u2756 Implemented end-to-end systems for DataAnalytics, DataAutomation and integrated with custom visualization tools using R, Mahout, Hadoop and MongoDB.\n\u2756 Worked with several R packages including knitr, dplyr, SparkR, CausalInfer, spacetime.\n\u2756 Worked with Data Governance, Data quality, data lineage, Data architect to design various models and processes.\n\u2756 Perform a proper EDA, Univariate and bivariateanalysis to understand the intrinsic effect/combined effects.\n\u2756 Created SQLscripts and analyzed the data in MS Access/Excel and Worked on SQL and SAS script mapping.\n\u2756 Responsible for design and development of Python programs to prepare transform and harmonize data sets in preparation for modeling.\n\u2756 Handled importing data from various data sources(GOVWIN), performed transformations using Spark, and loaded data into HDFS.\n\u2756 Interaction with Business Analyst, SMEs and other Data Architects to understand Business needs and functionality for various project solutions.\n\u2756 Created visualization and trends for Government spending on Defense DoD, Navy DoN, FDA, DOA, NIH, etc.\n\u2756 Worked on different data formats such as JSON, XML and performed machine learning algorithms in Python.\n\u2756 Work independently or collaboratively throughout the complete analytics project lifecycle including data extraction/preparation, design and implementation of scalable machine learning analysis and solutions, and documentation of results.\n\u2756 Developed and maintained data dictionary to create metadata reports for technical and business purpose.\n\u2756 Involved with Data Analysis Primarily Identifying Data Sets, Source Data, Source Meta Data, Data Definitions and Data Formats\n\u2756 Designed data models and data flow diagrams using Erwin and MSVisio.\n\u2756 Independently coded new programs and designed Tables to load and test the program effectively for the given POC's using with Big Data/Hadoop.\n\u2756 Developed, Implemented & Maintained the Conceptual, Logical&Physical Data Models using Erwin for Forward/ReverseEngineered Databases.\n\u2756 As an Architect implemented MDM hub to provide clean, consistent data for a SOA implementation.\n\u2756 Lead the development and presentation of a dataanalytics data-hub prototype with the help of the other members of the emerging solutions team\n\u2756 Established Data architecture strategy, best practices, standards, and roadmaps.\n\u2756 Worked with Hadoop ecosystem covering HDFS, HBase, YARN and MapReduce.\n\u2756 Involved in business process modeling using UML.\n\u2756 Performed datacleaning and imputation of missing values using R.\n\nEnvironment: Teradata 13.1, Informatica 6.2.1, Ab Initio, Business Objects, Oracle 9i, PL/SQL, Microsoft Office Suite (Excel, Vlookup, Pivot, Access, PowerPoint), Visio, VBA, Micro Strategy, Tableau, ERWIN, Machine Learning, Microsoft Azure."", u'Data Scientist/ Machine Learning Engineer\nAmerican Family Insurance - Madison, WI\nMarch 2016 to May 2017\nDescription:American Family Insurance, also abbreviated as AmFam, is a private mutual company that focuses on property, casualty, and auto insurance, and offers commercial insurance, life, health, and homeowners coverage as well as investment and retirement-planning products.\n\nResponsibilities:\n\u2756 Developed the logical data models and physical data models that confine existing condition/potential status data fundamentals and data flows using ERStudio\n\u2756 Reviewed and implemented the naming standards for the entities, attributes, alternate keys, and primary keys for the logical model.\n\u2756 Responsible for performing Machine-learning techniques regression/classification to predict the outcomes.\n\u2756 Used Python and Spark to implement different machine learning algorithms including Generalized Linear Model, SVM, Random Forest, Boosting and Neural Network\n\u2756 Build Machine Learning models in predicting product quality as early as possible\n\u2756 Created the conceptualmodel for the datawarehouse using Erwindatamodeling tool.\n\u2756 Used External Loaders like Multi-Load, TPump and FastLoad to load data into Oracle and Database analysis, development, testing, implementation and deployment.\n\u2756 Used R, Python and Spark to develop variety of models and algorithms for analytic purposes\n\u2756 Performed Multinomial Logistic Regression, Random forest, Decision Tree, SVM to classify package is going to deliver on time for the new route.\n\u2756 Performed data analysis by using Hive to retrieve the data from Hadoop cluster, SQL to retrieve data from Oracle database.\n\u2756 Utilized Spark, Scala, Hadoop, HBase, Kafka, Spark Streaming, MLLib, python, a broad variety of machine learning methods including classifications, regressions, dimensionally reduction etc.\n\u2756 Used Natural Language Processing (NLP) for response modeling and fraud detection efforts for credit cards.\n\u2756 Prepared multi-class classification data for modeling using one hot encoding, applied unsupervised and supervised learning methods in analyzing high-dimensional data.\n\u2756 Implemented public segmentation using unsupervised machinelearning algorithms by implementing k-means algorithm using Pyspark.\n\u2756 Explored and Extracted data from source XML in HDFS, preparing data for exploratory analysis using data munging.\n\u2756 Design and model the reporting data warehouse considering current and future reportingrequirement\n\u2756 Worked with data compliance teams, Data governance team to maintain data models, Metadata, Data Dictionaries; define source fields and its definitions.\n\u2756 Created stored procedures using PL/SQL and tuned the databases and backend process.\n\u2756 Worked with Data Scientist in order to create a Data marts for data science specific functions.\n\u2756 Performed data analysis and data profiling using complex SQL on various sources systems including Teradata, SQLServer.\n\u2756 Involved in analysis of Businessrequirement, Design and Development of Highlevel and Low-leveldesigns, Unit and Integration testing.\n\nEnvironment: Erwin 8, Teradata 13, SQL Server 2008, Oracle 9i, SQL*Loader, PL/SQL, ODS, OLAP, OLTP, SSAS, Informatica Power Center 8.1, Spark, Scala, Hadoop, HBase.', u""Data Scientist\nJP Morgan - San Antonio, TX\nNovember 2014 to February 2016\nDescription: JPMorgan Chase & Co. is an American multinational banking and financial services holding company headquartered in New York City. It is the largest bank in the United States, the world's sixth largest bank by total assets, with total assets of US$2.5 trillion, and the world's second most valuable bank by market capitalization\n\nResponsibilities:\n\u2756 Worked as a Data Modeler/Analyst to generate Data Models using Erwin and developed relational database system.\n\u2756 Analyzed the business requirements of the project by studying the Business Requirement Specification document.\n\u2756 Extensively worked on DataModeling tools ErwinDataModeler to design the datamodels.\n\u2756 Designedmapping to process the incremental changes that exists in the source table. Whenever source data elements were missing in source tables, these were modified/added inconsistency with third normal form based OLTP source database.\n\u2756 Designed tables and implemented the naming conventions for Logical and PhysicalData Models in Erwin 7.0.\n\u2756 Designedlogical and physical data models for multiple OLTP and Analytic applications.\n\u2756 Extensively used the Erwin design tool &Erwin model manager to create and maintain the DataMart.\n\u2756 Wrote simple and advanced SQLqueries and scripts to create standard and adhoc reports for senior managers.\n\u2756 Collaborated the data mapping document from source to target and the data quality assessments for the source data.\n\u2756 Identified and targeted welfare high-risk groups with Machine learning algorithms.\n\u2756 Conducted campaigns and run real-time trials to determine what works fast and track the impact of different initiatives.\n\u2756 Used Graphical Entity-Relationship Diagramming to create new database design via easy to use, graphical interface.\n\u2756 Created multiple custom SQLqueries in TeradataSQL Workbench to prepare the right data sets for Tableau dashboards\n\u2756 Perform analyses such as regression analysis, logistic regression, discriminant analysis, cluster analysis using SAS programming.\n\u2756 Used Expert level understanding of different databases in combinations for Data extraction and loading, joiningdata extracted from different databases and loading to a specific database.\n\u2756 Co-ordinate with various business users, stakeholders and SME to get Functional expertise, design and business test scenarios review, UAT participation and validation of financial data.\n\nEnvironment: Erwin r7.0, SQL Server 2000/2005, Windows XP/NT/2000, Oracle 8i/9i, MS-DTS, UML, UAT, SQL Loader, OOD, OLTP, PL/SQL, MS Visio, Informatica."", u'Data Scientist/Data Modeler\nBetchel corporation - San Jose, CA\nMarch 2013 to October 2014\nDescription: Bechtel is one of the most respected global engineering, construction, and project management companies. Together with our customers, we deliver landmark projects that create long-term progress and economic growth.\n\nResponsibilities:\n\u2756 Configured the project on WebSphere 6.1 application servers\n\u2756 Communicated with other Health Care info by using Web Services with the help of SOAP, WSDL JAX-RPC\n\u2756 Deployed GUI pages by using JSP, JSTL, HTML, DHTML, XHTML, CSS, JavaScript, AJAX\n\u2756 Used SAX and DOM parsers to parse the raw XML documents\n\u2756 Used RAD as Development IDE for web applications.\n\u2756 Translated business requirements into working logical and physical data models for Staging, Operational Data Store and Data marts applications.\n\u2756 Implemented metadata management as one part of data governance. Worked with the business users to populate business glossary.\n\u2756 Imported model to business glossary and performed integration of the technical and business terms.\n\u2756 Maintenance in the testing team for System testing/Integration/UAT\n\u2756 Used Log4J logging framework to write Log messages with various levels.\n\u2756 Conducted Design reviews and Technical reviews with other project stakeholders.\n\u2756 Implemented Microsoft Visio and Rational Rose for designing the Use Case Diagrams, Class model, Sequence diagrams, and Activity diagrams for SDLC process of the application\n\u2756 Work on SQL Data warehouse using Azure for designing services to handle computational and data-intensive queries in database.\n\u2756 Reverse Engineered existing Relational and data vault database systems as there were no existing data models for them.\n\u2756 Created test plan documents for all back-end database modules\n\u2756 Guaranteeing quality in the deliverables.\n\u2756 Implemented the project in Linux environment.\n\nEnvironment: R 3.0, Erwin 9.5, Tableau 8.0, MDM, QlikView, MLlib, PL/SQL, HDFS, Teradata 14.1, JSON, HADOOP (HDFS), Map Reduce, Pig, Spark, R Studio, MAHOUT, JAVA, HIVE, AWS.', u""Business Analyst /Data Analyst\nICICI Bank - Hyderabad, Telangana\nOctober 2011 to February 2013\nDescription: ICICI Bank, stands for Industrial Credit and Investment Corporation of India, is an Indian multinational banking and financial services company headquartered in Mumbai, Maharashtra, India, with its registered office in Vadodara. In 2017, it is the third largest bank in India in terms of assets and third in term of market capitalization.\n\nResponsibilities:\n\u2756 Analyze business information requirements and model class diagrams and/or conceptual domain models.\n\u2756 Managed the project requirements, documents and use cases by IBM Rational RequisitePro.\n\u2756 Assisted in building an Integrated logical data design, propose physical database design for building the data mart.\n\u2756 Gather & Review Customer Information Requirements for OLAP and building the data mart.\n\u2756 Responsible for defining the key identifiers for each mapping/interface\n\u2756 Responsible for defining the functional requirement documents for each source to target interface.\n\u2756 Coordinated meetings with vendors to define requirements and system interaction agreement documentation between client and vendor system.\n\u2756 Enterprise Metadata Library with any changes or updates.\n\u2756 Document data quality and traceability documents for each source interface.\n\u2756 Performed document analysis involving creation of Use Cases and Use Case narrations using Microsoft Visio, in order to present the efficiency of the gathered requirements.\n\u2756 Analyzed business process workflows and assisted in the development of ETL procedures for mapping data from source to target systems.\n\u2756 Worked with BTEQ to submit SQL statements, import and export data, and generate reports in Terra-data.\n\u2756 Worked with ETL Developers in creating External Batches to execute mappings, Mapplets using Informatica workflow designer to integrate Shire's data from varied sources like Oracle, DB2, flat files and SQL databases and loaded into landing tables of Informatica MDM Hub.\n\u2756 Responsible for full data loads from production to AWS Redshift staging environment.\n\u2756 Responsible for creating Hive tables, loading data and writing hive queries.\n\u2756 Calculated and analyzed claims data for provider incentive and supplemental benefit analysis using Microsoft Access and Oracle SQL.\n\u2756 Establish standards of procedures.\n\u2756 Generate weekly and monthly asset inventory reports.\n\u2756 Document all data mapping and transformation processes in the Functional Design documents based on the business requirements\n\nEnvironment: SQL Server 2008R2/2005 Enterprise, SSRS, SSIS, Crystal Reports, Windows Enterprise Server 2000, DTS, SQL Profiler, and Query Analyzer."", u""Data Analyst\nHindustan Zinc Limited - Udaipur, Rajasthan\nApril 2009 to September 2011\nDescription: Hindustan Zinc is a Vedanta Group company in zinc, lead and silver business. We are one of the world's largest integrated producers of zinc and are among leading global lead and silver producers. We are one of the lowest cost producers in the world and are well placed to serve the growing demand of Asian countries.\n\nResponsibilities:\n\u2756 Developed the logical data models and physical data models that confine existing condition/potential status data fundamentals and data flows using ER Studio\n\u2756 Involved in analysis of Business requirement, Design and Development of High level and Low-level designs, Unit and Integration testing\n\u2756 Reviewed and implemented the naming standards for the entities, attributes, alternate keys, and primary keys for the logical model.\n\u2756 Performed data analysis and data profiling using complex SQL on various sources systems including Teradata, SQL Server.\n\u2756 Designed, Build the Dimensions, cubes with star schema and Snow Flake Schema using SQL Server Analysis Services (SSAS).\n\u2756 Created the conceptual model for the data warehouse using Erwin data modeling tool.\n\u2756 Worked with Data Scientist in order to create a Data marts for data science specific functions.\n\u2756 Determined data rules and conducted Logical and Physical design reviews with business analysts, developers and DBAs.\n\u2756 Translate business and data requirements into Logical data models in support of Enterprise DataModels, ODS, OLAP, OLTP, Operational Data Structures and Analytical systems.\n\nEnvironment: Erwin r9.0, Informatica 9.0, ODS, OLTP, Oracle 10g, Hive, OLAP, DB2, Metadata, MS Excel, Mainframes MS Visio, Rational Rose, Requisite Pro, Hadoop, PL/SQL, etc.""]","[u""Bachelor's""]",[u''],"degree_1 : ""Bachelors"""
0,https://resumes.indeed.com/resume/7cff6fadc81b6ae6,"[u'Data Scientist\nDynegy Inc - Houston, TX\nJune 2016 to Present\nDevised a load forecasting model with support vector regression (SVR) that gave better precision compared to other already existing classical models.\n\u2022 Trading recommendation system for prediction of day ahead- real time spreads using ANN and advanced deep\nneural network with multiple GPU. This model has reduced the training time of 4 years data from 1 week to 30\nminutes. It has also significantly improved the prediction accuracy.', u""Data Analyst\nBengaluru, Karnataka\nAugust 2011 to July 2014\nWas responsible for establishing reporting databases, analyzing data using specialized statistical software's like\nSAS, R and MATLAB, and presenting research results in numerical and graphic formats.\n\u2022 Modeling and time series forecasting techniques such as Seasonal and Non-Seasonal ARIMA Models.\n\u2022 Developed a logistic regression model that predicts the trial and repeat prescribers for a recently launched drug;\nthese results aided the brand team in optimizing marketing activities and in designing efficient messaging\nplatforms\n\u2022 Text-Mining using Natural Language Processing using openNLP, Relationship Analysis using Latent Symantec\nAnalysis and Random Indexing, Text Categorization and Topic Identification using Latent Dirichlet Allocation. I\nhave also worked on clustering algorithms such as K-Means, Agglomerative Clustering.""]","[u'Master of Science in Applied Mathematics', u'Master of Science in Statistics', u'Bachelor of Science in Mathematics, Statistics and Computer Science']","[u'University of Houston Houston, TX\nJanuary 2015 to January 2016', u'Maharajas College, Mahatma Gandhi University\nJanuary 2010 to January 2012', u'St. Teresas College, Mahatma Gandhi University\nJanuary 2007 to January 2010']","degree_1 : Master of Science in Applied Mathematics, degree_2 :  Master of Science in Statistics, degree_3 :  Bachelor of Science in Mathematics, degree_4 :  Statistics and Compter Science"
0,https://resumes.indeed.com/resume/7b293cfe97277805,"[u""Data Scientist/Data Analyst\nShoptaki - New York, NY\nMarch 2017 to Present\n\u2022 Created smart chain framework to help new entrepreneurs to grow at global scale by using disruptive technology of data science, block-chain, artificial intelligence, multilateral netting and cryptography.\n\u2022 Handled the current and historical foreign exchange data to find patterns and helped company gain insights from it using machine learning and advanced excel tools.\n\u2022 Developed a machine learning framework to analyze customer data, payment method using block chain technology.\n\u2022 Created AI bots for Multilateral netting code for real time conversion and forecasting through Yahoo finance API.\n\u2022 Improved content curation by incorporating user recommendation system using SQL and Google Analytics\n\u2022 Used Distributed TensorFlow data parallelism to set up parallel processes on GPU's resulting reduced processing time.\n\u2022 Coordinate with the business users and stake holders providing appropriate, effective and efficient way to adapt new technologies and use cases with the existing functionality."", u'Data Analyst\nAbhyudaya Multimedia - Indore, Madhya Pradesh\nMarch 2014 to August 2015\n\u2022 Extracted, analyzed and synthesized data from data mart using SQL, prepared reports and provided actionable insights using Tableau.\n\u2022 Implemented ETL strategies for processing data while working with key users to derive reports used by high level management.\n\u2022 Dealt with the database designing and development issues and proposed solutions to improve system efficiency and reduce total expenses.\n\u2022 Used R and advanced excel tools to interpret data to draw conclusion for managerial action and strategy.\n\u2022 Used agile method by developing user and small stories to complete projects efficiently in particular amount of time.\n\u2022 Work with users to identify the most appropriate source of record and profile the data required for sales and service.\n\u2022 Document the complete process flow to describe program development, logic, testing, and implementation, application integration, coding.', u'Data Analyst\nPractolearn Solution Pvt. Ltd - Indore, Madhya Pradesh\nMarch 2013 to February 2014\n\u2022 Executed data repositories strategies and maintained the existing database systems by keeping them available, secure and stable.\n\u2022 Shape and disseminate analyses and syntheses to help decision makers to drive business activity, translate recommendations for internal plans and business plans\n\u2022 Support the domain ""business owners"" in the definition of their needs and translate them into recommendations for development and improvement of model applications.\n\u2022 Provided SQL management, installation, monitoring, tuning, optimization, backup and recovery.\n\u2022 Executed data migration procedures for moving data between database platforms without service disruption.']","[u'Master of Science', u'Bachelor of Engineering in Electronics and Communication']","[u'Pace University, Seidenberg School of Computer Science and Information Systems New York, NY\nMay 2017', u'RGPV University, Rishiraj Institute of Technology Indore, Madhya Pradesh\nMay 2013']","degree_1 : Master of Science, degree_2 :  Bachelor of Engineering in Electronics and Commnication"
0,https://resumes.indeed.com/resume/b6844d4be537f274,"[u""Data Scientist/Data Analyst\nJP Morgan & Chase - Columbus, OH\nApril 2017 to Present\nJP Morgan Chase & Co. (JPMC) is an American multinational banking and financial services holding company headquartered in New York City. It is the largest bank in the United States, the world's sixth largest bank by total assets, with total assets of US$2.534 trillion, and the world's second most valuable bank by market capitalization.\n\nJPMC serves consumers and small businesses with a broad range of financial services, including personal banking, small business banking and lending, mortgages, credit cards, payments, auto finance and investment advice. Consumer & Community Banking Risk Management partners with each CCB sub-line of business to identify, assess, prioritize and remediate risk. Types of risk that occur in consumer businesses include fraud, reputation, operational, credit, market and regulatory, among others\n\nAs a Data Scientist, I was a key contributor on the Fraud Modeling team responsible for developing and implementing best-in-class fraud prevention and detection models and analytical tools. The JPMC Fraud Modeling team is an analytical center of excellence to all fraud risk managers and operations across the bank. The team provides diverse models and analytical tools used to identify potentially fraudulent transactions across different lines of business (card, retail, auto, merchant services).\n\nAs the analytical expert, I identified and retooled suitable machine learning algorithms to enhance the fraud risk ranking of particular transactions and/or applications for new products. This included a balance of feature engineering, feature selection, and developing and training machine learning algorithms to extract predictive models/patterns from data gathered for hundreds of millions transactions. The project effectively utilize big data platforms, data assets, and analytical capabilities to control fraud loss and improve customer experience.\n\nResponsibilities\n* Review suspicious activity and complex fraud cases to help identify and resolve fraud risk trends and issues.\n* Clearly and thoroughly document investigation findings and conclusions.\n* Offline analysis of customer data to tune rules, exposes patterns, research anomalies, reduce false positives, and build executive and project-level reports.\n* Identify meaningful insights from chargeback data. Interpret and communicate findings from analysis to engineers, product and stakeholders.\n* Analyze high-volume data to investigate, identify and report trends linked to fraudulent transactions.\n* Utilize Sqoop to ingest real-time data. Used analytics libraries Sci-Kit Learn, MLLIB and MLxtend.\n* Extensively use Python's multiple data science packages like Pandas, NumPy, matplotlib, Seaborn, SciPy, Scikit-learn and NLTK.\n* Performed Exploratory Data Analysis, trying to find trends and clusters.\n* Built models using techniques like Regression, Tree based ensemble methods, Time Series forecasting, KNN, Clustering and Isolation Forest methods.\n* Work on data that was a combination of unstructured and structured data from multiple sources and automate the cleaning using Python scripts.\n* Extensively perform large data read/writes to and from csv and excel files using pandas.\n* Tasked with maintaining RDD's using SparkSQL.\n* Communicate and coordinate with other departments to collection business requirement.\n* Tackle highly imbalanced Fraud dataset using undersampling with ensemble methods, oversampling and cost sensitive algorithms.\n* Improved fraud prediction performance by using random forest and gradient boosting for feature selection with Python Scikit-learn.\n* Implemented machine learning model (logistic regression, XGboost) with Python Scikit- learn.\n* Optimize algorithm with stochastic gradient descent algorithm Fine-tuned the algorithm parameter with manual tuning and automated tuning such as Bayesian Optimization.\n* Develop a technical brief based on the business brief. This contains detailed steps and stages of developing and delivering the project including timelines.\n* After sign-off from the client on technical brief, started developing the SAS codes.\n* Write the data validation SAS codes with the help of Univariate, Frequency procedures.\n* Summarising the data at customer level by joining the datasets of customer transaction, dimension and from 3rd party sources.\n* Separately calculated the KPIs for Target and Mass campaigns at pre-promo-post periods with respective to their transactions, spend and visits.\n* Also measure the KPIs at MoM (Month on Month), QoQ (Quarter on Quarter) and YoY (Year on Year) with respect to pre-promo-post.\n* Measure the ROI based on the differences pre-promo-post KPIs.\n* Extensively use SAS procedures like IMPORT, EXPORT, SORT, FREQ, MEANS, FORMAT, APPEND, UNIVARIATE, DATASETS and REPORT.\n* Standardise the data with the help of PROC STANDARD.\n* Implement cluster analysis (PROC CLUSTER and PROC FASTCLUS) iteratively.\n* Work extensively with data governance team to maintain data models, Metadata and dictionaries.\n* Use Python to preprocess data and attempt to find insights.\n* Iteratively rebuild models dealing with changes in data and refining them over time.\n* Create and publish multiple dashboards and reports using Tableau server.\n* Extensively use SQL queries for legacy data retrieval jobs.\n* Task with migrating the django database from MySQL to PostgreSQL.\n* Gain expertise in Data Visualization using matplotlib, Bokeh and Plotly.\n* Responsible for maintaining and analyzing large datasets used to analyze risk by domain experts.\n* Develop Hive queries that compared new incoming data against historic data. Built tables in Hive to store large volumes of data.\n* Use big data tools Spark (Sparksql, Mllib) to conduct the real time analysis of credit card fraud based on AWS.\n* Perform Data audit, QA of SAS code/projects and sense check of results.\n\nEnvironment: Spark, Hadoop, AWS, SAS Enterprise Guide, SAS/MACROS, SAS/ACCESS, SAS/STAT, SAS/SQL, ORACLE, MS-OFFICE, Python (scikit-learn, pandas, Numpy), Machine Learning (logistic regression, XGboost), Gradient Descent algorithm, Bayesian optimization, Tableau."", u""Data Scientist/Data Analyst\nUHG - CT\nJanuary 2016 to April 2017\nOptum is focused on helping physicians make informed treatment and patient care decisions using data science. The idea is to use historic data from multiple cancer treatment centers and patient's individual EHR information to provide personalized treatment recommendations, depending on the type of cancer, the patient's previous health records and current condition.\n\nClassification. The goal of this task is to define a category of a data object. Supervised machine learning with a classification algorithm can be used to answer this question: Is there a probability that a patient will experience heart failure?\n\nRegression. This machine learning task is aimed at modeling and forecasting numeric values. With regression algorithm, a scientist can predict, for instance, how many months will be needed to treat a patient.\n\nClustering. Using Clustering to spot patterns in data, unsupervised machine learning with applied clustering algorithms allows for grouping patients' health records by similarity and get answers to questions like What are the qualitative and quantitative similarities among high-utilizing patients.\n\nResponsibilities\n* Application of various machine learning algorithms and statistical modeling like decision trees, regression models, neural networks, SVM, clustering to identify Volume using scikit-learn package in python, Matlab.\n* Utilized Apache Spark with Python to develop and execute Big Data Analytics and Machine learning applications, executed machine Learning use cases under Spark ML and Mllib.\n* Led technical implementation of advanced analytics projects, Defined the mathematical approaches, developer new and effective analytics algorithms and wrote the key pieces of mission-critical source code implementing advanced machine learning algorithms utilizing caffe, TensorFlow, Scala, Spark, MLLib, R and other tools and languages needed.\n* Built analytical data pipelines to port data in and out of Hadoop/HDFS from structured and unstructured sources and designed and implemented system architecture for Amazon EC2 based cloud-hosted solution for client.\n* Performed K-means clustering, Multivariate analysis and Support Vector Machines in Python and R.\n* Professional Tableau user (Desktop, Online, and Server), Experience with Keras and Tensor Flow.\n* Created mapreduce running over HDFS for data mining and analysis using R and Loading & Storage data to Pig Script and R for MapReduce operations and created various types of data visualizations using R, and Tableau.\n* Worked on machine learning on large size data using Spark and MapReduce.\n* Performed data analysis by using Hive to retrieve the data from Hadoop cluster, SQL to retrieve data from Oracle database.\n* Developed Spark/Scala, Python for regular expression (regex) project in the Hadoop/Hive environment with Linux/Windows for big data resources.\n* Performed Multinomial Logistic Regression, Random forest, Decision Tree, SVM to classify package is going to deliver on time for the new route.\n* Stored and retrieved data from data-warehouses using Amazon Redshift.\n* Responsible for planning & scheduling new product releases and promotional offers.\n* Used pandas, numpy, seaborn, scipy, matplotlib, scikit-learn, NLTK in Python for developing various machine learning algorithms.\n* Worked on NOSQL databases like MongoDB, HBase.\n* Created Data Quality Scripts using SQL and Hive to validate successful data load and quality of the data. Created various types of data visualizations using Python and Tableau.\n* Worked on data pre-processing and cleaning the data to perform feature engineering and performed data imputation techniques for the missing values in the dataset using Python.\n* Extracted data from HDFS and prepared data for exploratory analysis using data munging.\n* Worked on Text Analytics, Naive Bayes, Sentiment analysis, creating word clouds and retrieving data from Twitter and other social networking platforms.\n* Worked on different data formats such as JSON, XML and performed machine learning algorithms in Python.\n\nEnvironment: Python, MongoDB, JavaScript, SQL Server, HDFS, Pig, Hive, Oracle, DB2, Tableau, ETL (Informatica), SQL, T-SQL, EC2, EMR, Teradata, Hadoop Framework, AWS, Spark SQL, Scala, SparkMllib, NLP, SQL, Matlab, HBase, Cassandra, R, Pyspark, Tableau Desktop, Excel, Linux, CDH5"", u""Data Scientist/Data Analyst\nTJX Companies Inc\nFebruary 2014 to December 2015\nThe TJX Companies, Inc. is an American multinational off-price department store corporation, headquartered in Framingham, Massachusetts. Of its banners, HomeGoods, Marshalls, TJ Maxx, and Sierra Trading Post operate in the United States\n\nWith an increasing amount of shopping done online, traditional retailers like TJX must innovate to stay relevant. The Global Strategy and Data Labs Team at TJX is tackling these challenges by leveraging our deep data assets to provide a best in class, worldwide capability to drive actionable customer understanding. As a Data Scientist, my goal was to use the power of advanced analytics to drive financial results and improve customer experiences. We work collaboratively with business partners across the organization to define the questions to ask, pursue the best opportunities, share insights, and develop practical business recommendations.\n\nResponsibilities\n* Built statistical analysis and machine learning models using rich, multi-brand, longitudinal data sets covering over 65 million customers globally.\n* Communicated with cross-functional business partners to find the right questions, the right data, and the right approaches needed to reach project goals.\n* Worked at all levels of a multi-billion dollar fashion retailer (design, marketing, customer retention and acquisition, pricing, inventory, logistics, ecommerce, wherever we can make an impact).\n* Buyer Propensity Model: Designed a Bayesian model to estimate buyers' propensity for different deal categories; gives statistically significant (+7%) lift in clicks, purchases and revenue across North America and European markets.\n* Statistical/Mining tools used: R, Gaussian Modeling, Hive, Tableau.\n* Co-Click Model: Augmented TJX deal recommendation using deal co-clicks similarity; provides statistically significant lifts (+5%) for clicks, purchases and revenue across majority of the countries. Statistical/Mining tools used: R, Tableau, Hive, Matrix Factorization.\n* Ads Keyword Optimization: Designed keyword generation, using a combination of statistical and linguistic signals, from merchant pages, deal description etc.; obtained higher CTRs than non-linguistic approaches.\n* Statistical/Mining tools used: Statistical Parsing, StanfordNLP.\n* General Data Analysis: General & exploratory data analysis and statistical tests over a variety of TJX datasets/experiments. Statistical/Mining tools used: Various statistical tests, R, Tableau, Shiny, Hive, Teradata.\n* Managing end to end pipeline: As a senior member of data science team, helped manage the end to end pipeline from ETL (extraction, transform and load) to EDA (exploratory data analysis) and algorithm development to final business decision and product design.\n\nEnvironment Exploratory Data Analysis (EDA), Statistical Tests (A/B, A/A etc.), Stochastic Optimization, Bayesian Modeling, Time Series Analysis, Gaussian Processes, Recommendation Engines, Latent Space Models for Text, Large Scale Distributed Learning, Generative & Discriminative Models (clustering, regression etc.)."", u""Data Scientist\nLife Insurance Corporation\nAugust 2012 to February 2014\nLife Insurance Corporation of India (LIC) is an Indian state-owned insurance group and investment company headquartered in Mumbai. It is the largest insurance company in India with an estimated asset value of ?1,560,482 crore (US$240 billion).\n\nAs a Data Scientist on the Finance Advanced Analytics team in UIIC's Global Retail Markets, I was responsible for driving advanced operational insight for our billing and payments function that processed over $16B worth of revenue annually. I supported the development of predictive model-enabled solutions and drove process efficiencies to improve customer experience and business results. I also oversaw the delivery of critical business intelligence analysis for billing and payment operations, leveraging the latest analytics tools.\n\nResponsibilities\n* Developed predictive models using appropriate statistical and machine-learning methodology to support strategic business decisions and front-line operations.\n* Designed and managed experiments and pilots to test hypotheses or fgenerate observation data.\n* Proactively seek continuous improvement opportunities and support cross-functional analytic projects with business stakeholders for\n* implementation.\n* Identify data needs and assists in designing billing & payment operations data roadmap.\n* Collaborated with our underwriters, product managers, software engineers, designers, and data/business intelligence teams.\n* Identified growth opportunities through data (ROI evaluation, LTV, allowables and other analyses) to significantly improve business performance across channels, programs and purchasing modes.\n* Under minimal supervision, ascertain and execute plans to build, implement and maintain predictive models (Stepwise regression, Markov Chain, Time series forecasting, Multiple and logistic regression models, etc.) using available tools and programming languages. Monitor and provide feedback on model performance and recalibrate model as necessary.\n* Performed analysis to identify areas of improvement for conversion rate, user experience, media mix, cross sell and upsell. Lead recommendation implementation and adoption across areas.\n* Independently gathers requirements from appropriate business partners for project, including necessary data for analysis to be performed. Develops project plans, executes on deliverables within agreed upon timeframes, manages deadlines, communicate progress and makes recommendations to address issues.\n* Translated analytical findings and statistical models into measures of business impact and actionable recommendations for the marketing team.\n* Built Decision Trees in Python to represent segmentation of data and identified key variables to be used in predictive modeling and also used Regression analysis, ANOVA and Z test.\n* Modeling Helped Business and Engineering team to make decision based on data in Policy and Claims System by using Data Visualization technique to transform Unstructured data into structure format to do data visualization for business decision.\n* Worked on Insurance Healthcare Claim System.\n* Used data to develop new statistical models to extract insights from large volume of data using the concept of Cluster Analysis, Neural Network, Random Forest and ARIMA modelling for identifying pattern in Time Series analysis.\n* Developed and automated various modeling steps to make project-process faster and more accurate using Machine Learning Technique like Linear Regression, Non- Linear Regression, Logistic regression, Na\xefve Bayes Classification, Support Vector Machine, KNN etc.\n* Created innovative algorithms behind variety of services ranging from Insurance Underwriting Risk to deciding right Insurance structure for all claims.\n* Performed ad hoc statistical, data mining, and machine learning analysis, Develop and Design advance predictive analysis models using Python.\n* Performing Goodness of fit for various distribution and the best distribution which models the claims data.\n* Developed logistic regression model in R to predict whether policy is going to claim or not when hit by hailstorm (weather event).\n* Extracting data from the database using ETL Concept.\n* Validated the models on out-of-sample and out-of-time data.\n* Spatial Analysis to determine possible relationships between claims and policies in force.\n\nEnvironment: Python, Regression analysis, ANOVA and Z test, Linear Regression, Non- Linear Regression, Logistic regression, Na\xefve Bayes Classification, Support Vector Machine, KNN, ETL, Cluster Analysis, Neural Network, Random Forest and ARIMA modelling."", u'Jr. Data Analyst\nLife Insurance Corporation - Hyderabad, Telangana\nJune 2011 to August 2012\nResponsibilities\n* Developed and implemented predictive models using Natural Language Processing Techniques and machinelearning algorithms such as linear regression, classification, multivariate regression, Naive Bayes, RandomForests, K-means clustering, KNN, PCA and regularization for data analysis.\n* Designed and developed Natural Language Processing models for sentiment analysis.\n* Applied clustering algorithms i.e. Hierarchical, K-means with help of Scikit and Scipy.\n* Developed visualizations and dashboards using ggplot, Tableau.\n* Worked on development of data warehouse, Data Lake and ETL systems using relational and non relationaltools like SQL, No SQL.\n* Built and analyzed datasets using R, SAS, Matlab and Python (in decreasing order of usage).\n* Participated in all phases of datamining; datacollection, datacleaning, developingmodels, validation, visualization and performed Gapanalysis.\n* DataManipulation and Aggregation from different source using Nexus, Toad, BusinessObjects, PowerBI and SmartView.\n* Implemented Agile Methodology for building an internal application.\n* Good knowledge of HadoopArchitecture and various components such as HDFS, Job Tracker, Task Tracker, Name Node, Data Node, Secondary Name Node, and MapReduce concepts.\n* As Architect delivered various complex OLAPdatabases/cubes, scorecards, dashboards and reports.\n* Programmed a utility in Python that used multiple packages (scipy, numpy, pandas).\n* Implemented Classification using supervised algorithms like LogisticRegression, Decisiontrees, KNN, NaiveBayes.\n* Used Teradata15 utilities such as FastExport, MLOAD for handling various tasks data migration/ETL from OLTP Source Systems to OLAP Target Systems\n* Maintenance in the testing team for System testing/Integration/UAT.\n* Involved in preparation & design of technical documents like Bus Matrix Document, PPDM Model, and LDM & PDM.\n* Understanding the client business problems and analyzing the data by using appropriate Statistical models to generate insights.\n\nEnvironment: R 3.0, Erwin 9.5, Tableau 8.0, MDM, QlikView, ML Lib, PL/SQL, HDFS, Teradata 14.1, JSON, HADOOP (HDFS), MapReduce, PIG, Spark, R Studio, MAHOUT, JAVA, HIVE, AWS.']",[u'Bachelor of Technology in Technology'],"[u'Gokaraju Rangaraju Institute of Technology Hyderabad, Telangana']",degree_1 : Bachelor of Technology in Technology
0,https://resumes.indeed.com/resume/fd8d77d51176dc0e,"[u'Data Scientist\nSoothsayer Analytics - Livonia, MI\nJuly 2017 to Present\no Kids Read Now (Non-profit Organization): Developed predictive analytics models for the utility of summer reading program on the reading score of 100,000 Pre-K to fourth-grade kids of the schools in Ohio state.\no Data cleaning, pre-processing, imputation, transformation, scaling, feature engineering, data aggregation, merge data-frames, descriptive statistics, data visualization, regular expressions, report client with weekly Tableau dashboards.\no Decision trees, Random Forest, linear & logistic regression, KNN models to classify quartiles & predict scores.\no Achieved accuracy, precision, recall in the range of 75-80 % on average for the validated models.\no Implemented agile methodology, project management, and Burndown charts to execute the scrum units for the sprint.\no The program showed improvement in the reading score of 75.7% students from Fall 2016 to Fall 2017.\no Steelcase: Analyzed human mobility from embedded-analytics IoT sensors, accuracy stats prediction.\no Markov Chains, Data visualization, Tableau dashboards, proof of concept for the IoT sensors.\no An average accuracy of 73.8% for spatially accurate human prediction in a room was estimated.\no Kaggle: Demand forecasting, inventory mgmt. using RNN & LSTM for a food processing company - TensorFlow, Keras\no Genetic Algorithms for Job shop scheduling optimization - Google Or-tools, Constraint Programming\no Reinforcement learning for Alpha Zero AI - Keras, Python.', u'Graduate Engineer\nVoltas Limited - Mumbai, Maharashtra\nJuly 2014 to July 2015\nBusiness Development: cost estimations, Supply Chain & Procurement: supplier mapping, HVAC Design: Auto-CAD, MS Project\no Derived KPIs like Fill Rate and Perfect Order, derived business development metrics for in a cross-functional team.\no SQL reports for mapping suppliers, supplier segmentation with Kraljic matrix, improved supply chain lead times by 19%.\no Estimated costs with MS Excel (Pivot tables/V-Lookups/VBA/Macros) & analyzed the Critical to Quality (CTQ) outputs.\no Lean Six Sigma DMAIC tool to streamline logistics, tracking production spreadsheet with project milestone dates.', u'Process Analyst Intern\nReliance Infrastructure Limited - Mumbai, Maharashtra\nDecember 2012 to January 2013\no Kaizen, lean manufacturing principles, 5S, six sigma methodologies for process optimization.']","[u'M.S. in Industrial Engineering', u'B.E. in Mechanical Engineering']","[u'Arizona State University\nAugust 2015 to May 2017', u'University of Mumbai Mumbai, Maharashtra\nAugust 2010 to June 2014']","degree_1 : M.S. in Indstrial Engineering, degree_2 :  B.E. in Mechanical Engineering"
0,https://resumes.indeed.com/resume/fdd2b861623f01c7,"[u""Data Scientist\nBoeing\nJanuary 2015 to Present\n-Data and text mining structured, semi structured and unstructured data\n-Create statistical models to predict machine failure and non-conformances\n-Create Shiny applications in R for customers\n-Perform safety analysis and track KPI's to help give direction to the company\n-Extract meaningful and actionable items from structured and unstructured data\n-Present and convey meaningful results to management and customers\n-Visually display results in tableau"", u'Teaching Associate\nCalifornia State University\nMay 2012 to May 2013', u'Teacher\nCalifornia State University\nJanuary 2011 to May 2013\n-Taught business calculus, calculus, and college algebra\n-Responsible for creating curriculums, tests, quizzes, and homework\n-Initiated and implemented a group work environment to encourage independent thinking while working with others\n\u2022 Honors and Awards:']",[u'Master of Science in Applied Mathematics'],[u'California State University\nJanuary 2008 to January 2014'],degree_1 : Master of Science in Applied Mathematics
0,https://resumes.indeed.com/resume/649d6e43732da5c9,"[u'Data Scientist I\nOptum - Eden Prairie, MN\nJanuary 2017 to Present\n* Utilize machine learning and statistics to create models for innovation and research team\n* Query large, complex data sets from a variety of databases\n* Create reports and presentations for communicating analysis results to project/product teams\n* Use text-analysis to cluster topics with semi-structured survey data to improve voting algorithm\n* Implement strategies for device assignment and tracking by cleaning and assessing existing mobile asset data, saving company approximately 15,000 dollars per month', u'Academic Analyst\nSouth Dakota State University - Brookings, SD\nAugust 2016 to December 2016\n* Independently prepared data and designed scaffolding plan for analyzing institutional data to find actionable items for student success in their first year of college\n* Created predictive model for student success', u'Business Analyst\nBrookings, SD\nOctober 2014 to May 2016\n* Collected, cleaned and analyzed costs data to improve the strategy for dynamic service-agreement pricing\n* Created an automated tool for pricing service agreements in order to implement new pricing strategies\n* Utilized math skills and critical thinking skills to accurately price service agreements for customers']",[u'BS in Mathematics (Emphasis in Statistics and Data Science)'],[u'South Dakota State University'],degree_1 : BS in Mathematics (Emphasis in Statistics and Data Science)
0,https://resumes.indeed.com/resume/66dc6137d7a7152b,"[u'Data Scientist\nRealNetworks - New York, NY\nApril 2017 to Present\nResponsibilities:\n\u2022 Lead a team of Data Scientists for machine learning and advise a Business Intelligence team for reporting & data visualization\n\u2022 Collaborate with business stakeholders and other technical teams to drive growth of the data science capabilities in the company\n\u2022 Research and application of machine learning & natural language processing for text analytics for live fraud detection on distributed systems supporting massive rates of data transfer\n\u2022 Analyze large amount of data from offline and real-time traffic and communicate results with various stake-holders', u'Data Scientist (Consultant)\nChubb - Township of Warren, NJ\nAugust 2015 to March 2017\nResponsibilities:\n\u2022 Consolidate customer information from different data sources (CRM data, clickstream data, others) to better understand purchase patterns\n\u2022 Group customers with similar patterns into segments for effective marketing activities and communication\n\u2022 Empower sales teams in the field by identifying and prioritizing leads for Cross-Selling B2B insurance products\n\u2022 Translate business challenges into math/statistical hypotheses and prepare analysis plans to highlight potential value for the business\n\u2022 Take end-to-end ownership of designing, developing and deploying predictive models (data preparation => variable selection => model building and evaluation => deployment)', u'Graduate Teaching Assistant\nDepartment of Chemistry & Biochemistry, The University of Texas at Arlington - Arlington, TX\nSeptember 2011 to May 2015\n\u2022 Taught undergraduate labs (Quantitative analysis and General Chemistry) and worked with the Lab Instructors and other Teaching Assistants to improve learning experience in undergraduate program\n\u2022 Acquired practical skills of teaching, mentoring, working with diverse categories of people', u'Junior Research Fellow\nIndian Association for the Cultivation of Science - Kolkata, West Bengal\nJanuary 2010 to June 2011\n\u2022 Created simulations with Molecular Mechanics Force Fields (Monte Carlo and Molecular Dynamics) to study various properties of molecules\n\u2022 Identified correlations between chemical structures and properties (quantitative structure-property relationship (QSPR) and quantitative structure-activity relationship (QSAR)).']","[u'Master of Science in Analytics', u'Master of Science in Chemistry', u'M.Sc. in Chemistry', u'B.Sc. in Mathematics, Physics, Chemistry']","[u'HARRISBURG UNIVERSITY OF SCIENCE AND TECHNOLOGY Harrisburg, PA\nMarch 2018 to December 2019', u'UNIVERSITY OF TEXAS AT ARLINGTON Arlington, TX\nAugust 2011 to May 2015', u'VIDYASAGAR UNIVERSITY\nJune 2007 to November 2009', u'VIDYASAGAR UNIVERSITY\nJune 2004 to May 2007']","degree_1 : Master of Science in Analytics, degree_2 :  Master of Science in Chemistry, degree_3 :  M.Sc. in Chemistry, degree_4 :  B.Sc. in Mathematics, degree_5 :  Physics, degree_6 :  Chemistry"
0,https://resumes.indeed.com/resume/ac2a4f393d79da76,"[u'Data Scientist\nThe Meet Group - San Francisco, CA\nDecember 2017 to March 2018\n-- Designed and implemented a Bayesian AB testing methodology, allowing for improved estimates of the effect of new features in the mobile app\n-- Supervised a digital economy in which users send and receive gifts; used marketing mixed modeling techniques to predict purchases of a premium feature', u'Data Scientist\nJP Research - Mountain View, CA\nJanuary 2014 to December 2017\n-- Developed a fully customizable survival analysis package using flexsurv and KMsurv, allowing clients to alter assumptions and assess the sensitivity of their projections\n-- Used survey-weighted logistic regression smoothing to fix small sample issues, improving prediction accuracy\n-- Analyzed concussion data using a random forest and Naive Bayes classifier, advising the NFL that certain helmets models are not as effective as advertised\n-- Built automated tools for data visualization using ggplot, improving efficiency across platforms\n-- Added functionality to Excel\u2019s evolutionary solver, allowing clients to create custom optimization scenarios and track underlying data', u'Pricing Analyst\nEsurance - San Francisco, CA\nApril 2013 to December 2013\n-- Discovered flaws in the methodology being used to calculate weather peril zone relativities; designed and implemented a 3D algorithm based on k-means clustering and Monte Carlo methods\n-- Observed groups of customers being neglected by the pricing algorithm; used hierarchical clustering to improve customer segmentation and increase profits by 6%']","[u'Certificate Program in Data Science', u'Master of Arts in Applied Statistics', u'Bachelor of Science in Mathematics']","[u'University of California Berkeley Berkeley, CA\nJanuary 2017 to January 2018', u'University of California Santa Barbara Santa Barbara, CA\nJanuary 2012', u'Clemson University Clemson, SC\nJanuary 2011']","degree_1 : Certificate Program in Data Science, degree_2 :  Master of Arts in Applied Statistics, degree_3 :  Bachelor of Science in Mathematics"
0,https://resumes.indeed.com/resume/eaa4f04458909235,"[u""Data Scientist\nAmadeus - Waltham, MA\nApril 2017 to Present\nDescription: Amadeus powers travel. Amadeus' solutions connect travelers to the journeys they want, linking them via travel agents, search engines and tour operators to airlines, airports, hotels, cars and railways. Technology has always been critical to developing global travel, increasing scale, choice and access. We have developed our technology in partnership with the travel industry. We combine a deep understanding of how people travel with the ability to design and deliver the most complex, trusted, and critical systems our customers need.\n\nResponsibilities:\n\u2022 Identifying the Customer and account attributes required for MDM implementation from disparate sources and preparing detailed documentation.\n\u2022 Performing data profiling and analysis on different source systems that are required for Customer Master.\n\u2022 Worked closely with the Data Governance Office team in assessing the source systems for project deliverables.\n\u2022 Used T-SQL queries to pull the data from disparate systems and Data warehouse in different environments.\n\u2022 Used Data Quality validation techniques to validate CriticalDataelements (CDE) and identified various anomalies.\n\u2022 Extensively used open source tools - RStudio(R) and Spyder(Python) for statistical analysis and building the machine learning.\n\u2022 Assessed existing EDW technologies and methods to ensure our EDW/BI architecture meet the needs of the business, enterprise and allows for business growth.\n\u2022 Worked extensively with Sqoop for importing and exporting the data from HDFS to Relational Database systems like Oracle and vice-versa.\n\u2022 Developed MapReduce programs to parse the raw data, populate staging tables and store the refined data in partitioned tables in the EDW.\n\u2022 Performing Data Validation / Data Reconciliation between disparate source and target systems (Salesforce, Cisco-UIC, Cognos, DataWarehouse) for various projects.\n\u2022 Writing complexSQL queries for validating the data against different kinds of reports generated by Cognos.\n\u2022 Interaction with Business Analyst, SMEs, and other Data Architects to understand Business needs and functionality for various project solutions\n\u2022 Researched, evaluated, architected, and deployed new tools, frameworks, and patterns to built sustainable Big Data platforms for the clients\n\u2022 Used Amazon Cloud EC2 along with Amazon SQS to upload and retrieve project history.\n\u2022 Used Python and Django to interface with the jQuery UI and manage the storage and deletion of content.\n\u2022 Rewrite existing Python/Django modules to deliver certain format of data.\n\u2022 Extracting data from different databases as per the business requirements using Sql Server Management Studio.\n\u2022 Interacting with the ETL, BIteams to understand / support on various ongoing projects.\n\u2022 Extensively using MS Excel for data validation.\n\u2022 Generating weekly, monthly reports for various business users according to the business requirements. Manipulating/mining data from database tables (Redshift, Oracle, Data Warehouse).\n\u2022 Providing analytical network support to improve quality and standard work results.\n\u2022 Create statistical models using distributed and standalone models to build various diagnostics, predictive and prescriptive solution.\n\u2022 Interface with other technology teams to load (ETL), extract and transform data from a wide variety of data sources\n\u2022 Utilize a broad variety of statistical packages like SAS, R , MLIB, Graphs, Hadoop, Spark , Map Reduce and others\n\u2022 Provides input and recommendations on technical issues to Business&DataAnalysts, BIEngineers and DataScientists.\n\nEnvironment:Data Governance, SQL Server, ETL, MS Office Suite - Excel (Pivot, VLOOKUP), DB2, R, Python, Visio, HP ALM, Agile, Sypder, Word, Azure, MDM, SharePoint, Data Quality, Tableau and Reference Data Management."", u""Data Scientist\nLogMeIn - Boston, MA\nJanuary 2016 to March 2017\nDescription: LogMeIn, Inc, is a provider of software as a service and cloud-based remote connectivity services for collaboration, IT management and customer engagement. The company's products give users and administrators access to remote computers.\n\nResponsibilities:\n\u2022 Developing Data Mapping, Data Governance, Transformation and Cleansing rules for the Master Data Management (MDM) Architecture involving OLTP, ODS and OLAP.\n\u2022 Providing source to target mappings to the ETL team to perform initial, full, and incremental loads into the target data mart.\n\u2022 Conducting JAD sessions, writing meeting minutes, collecting requirements from business users and analyze based on the requirements.\n\u2022 Involved in defining the source to target data mappings, business rules, and data definitions.\n\u2022 Transformation on the files received from clients and consumed by Sql Server.\n\u2022 Working closely with the ETL, SSIS, SSRSDevelopers to explain the complex Data Transformation using Logic.\n\u2022 Worked on DTS Packages, DTS Import/Export for transferring data between SQLServer2000 to 2005.\n\u2022 Worked on moving data from Hive tables into HBase for real time analytics on Hive tables.\n\u2022 Handled importing of data from various data sources, performed transformations using Hive. (External tables, partitioning)\n\u2022 Responsible for creating Hive tables, loading the structured data resulted from Map Reduce jobs into the tables and writing hive queries to further analyze the logs to identify issues and behavioral patterns.\n\u2022 Exploring with Spark improving the performance and optimization of the existing algorithms in Hadoop using Spark context, Spark-SQL, Data Frame, pair RDD's, Spark YARN.\n\u2022 Developed Spark code and Spark-SQL/Streaming for faster testing and processing of data.\n\u2022 Experience in deploying data from various sources into HDFS and building reports using Tableau.\n\u2022 Developed a data pipeline using Kafka and Strom to store data into HDFS.\n\u2022 Performing Data Profiling, Cleansing, Integration and extraction tools (e.g. Informatica).\n\u2022 Utilizing Informatica toolset (Informatica Data Explorer, and Informatica Data Quality) to analyze legacy data for Data Profiling.\n\u2022 A highly immersive Data Science program involving Data Manipulation and Visualization, Web Scraping, Machine Learning, GIT, SQL, Unix Commands, Python programming, NoSQL, MongoDB, Hadoop.\n\u2022 Defining the list codes and code conversions between the source systems and the data mart using Reference Data Management (RDM).\n\u2022 Applying data cleansing/data scrubbing techniques to ensure consistency amongst data sets.\n\u2022 Extensively using ETL methodology for supporting data extraction, transformations and loading processing, in a complex EDW using Informatica.\n\nEnvironment: Power Center 9.x/8.1, Informatica Analyst, MDM, MS Excel, Agile, IDD, Data Governance, Oracle 11g, Meta Data, Sql Server, SOA, SSIS, SSRS, IDQ, Data Lineage, ETL, UNIX, T-SQL, HP Quality Center 11, RDM (Reference Data Management)."", u""Data Analyst\nNetApp - Wichita, KS\nJune 2014 to December 2015\nDescription:NetApp knows storage backwards and forwards and on premise and in the cloud. The company makes data storage systems used by businesses for archiving and backup. It offers products for hybrid cloud storage, extending customers' IT infrastructure to the cloud environments of Amazon, Google, IBM, and Microsoft. NetApp enables customers' use of flash storage, another relatively new market for the company\n\nResponsibilities:\n\u2022 Worked on data cleaning and reshaping, generated segmented subsets using Numpy and Pandas in Python.\n\u2022 Generated cost-benefit analysis to quantify the model implementation comparing with the former situation.\n\u2022 Designed both 3NF data models for ODS, OLTP systems and dimensional data models using Star and Snowflake Schemas.\n\u2022 Developed Teradata SQL scripts using OLAP functions like rank and rank () Over to improve the query performance while pulling the data from large tables.\n\u2022 Conducted model optimization and comparison using stepwise function based on AIC value\n\u2022 Wrote and optimized complex SQL queries involving multiple joins and advanced analytical functions to perform data extraction and merging from large volumes of historical data stored in Oracle 11g, validating the ETL processed data in target database.\n\u2022 Worked on model selection based on confusion matrices, minimized the Type II error\n\u2022 Developed Pythonscripts to automate data sampling process. Ensured the data integrity by checking for completeness, duplication, accuracy, and consistency.\n\u2022 Applied various machine learning algorithms and statistical modeling like decision tree, logistic regression, Gradient Boosting Machine to build predictive model using scikit-learn package in Python.\n\u2022 Developed MapReduce Python modules for machine learning & predictive analytics in Hadoop on AWS.\n\u2022 Performed Smoke test to do the primary checks like record counts, Column matching for database and Dashboard testing.\n\u2022 Performed testing at SIT(System Integration Testing) level and UAT(User Acceptance testing) level.\n\u2022 Gathered requirements from the development team and database developers to analyze the tables and entity relationships for understanding the database.\n\u2022 Maintenance of large data sets, combining data from various sources by Excel, Enterprise, and SAS Grid, Access and SQL queries.\n\u2022 Developed and implemented SSIS, SSRS and SSAS application solutions for various business units across the organization.\n\u2022 Identified the variables that significantly affect the target\n\u2022 Continuously collected business requirements during the whole project life cycle.\n\u2022 Generated data analysis reports using Matplotlib, Tableau, successfully delivered and presented the results for C-level decision makers.\n\nEnvironment: SQL Server 2008, Teradata 13, Erwin 8, Oracle 9i, PL/SQL, SQL*Loader, ODS, OLAP, SSAS, Informatica Power Center, OLTP."", u""Data Modeler\nMedStar Franklin Square Hospital - Baltimore, MD\nMarch 2013 to May 2014\nDescription: The HI-Exchange Project dealt with development of an online HIE (Health information exchange) and a secure web portal to enable authorized Franklin Square Hospital providers to have fast and easy access to patient's electronic health record. The HI-Exchange web portal features EMR functions and decision Support tools for better care management.\n\nResponsibilities:\n\u2022 Installation of Talend Studio.\n\u2022 Architecting and design of data warehouse ETL processes.\n\u2022 Design and implement the ETL Data model and create staging, source and Target tables in SQLserverdatabase.\n\u2022 Demo of POC built for the prospective customer and provide guidance and gather the feedback to backend ETL testing on SQL Server 2008 using SSIS.\n\u2022 Gathering and analysis requirements definition meetings with business users and document meeting outcomes.\n\u2022 Create the Operational manual Document.\n\u2022 Develop Integrations jobs to transfer data from source system to Hadoop.\n\u2022 Task allocation for the ETL and Reporting team.\n\u2022 Create Integration Jobs to backup a copy of data in network file system.\n\u2022 Application of business rules on the data being transferred.\n\u2022 Technical design documents for Transformation processes.\n\u2022 Communicate effectively with client and their internal development team to deliver product functionality requirements.\n\nEnvironment: MS Office, Hadoop, ETL, ODS, OLAP, SQL Server 2008 and Talend Studio."", u'Java & J2EE developer\nJDA Software Group, Inc - Hyderabad, Telangana\nApril 2011 to January 2013\nDescription: JDA Software Group, Inc. is an American software and consultancy company (owned by New Mountain Capital), providing supply chain management, manufacturing planning, retail planning, store operations and collaborative category management solutions headquartered in Scottsdale, Arizona\n\nResponsibilities:\n\u2022 Design patterns of BusinessDelegates, ServiceLocator and DTO are used for designing the web module of the application.\n\u2022 Implemented various J2EE design patterns for designing this application.\n\u2022 Developed the Web Interface using Struts, JavaScript, HTML and CSS.\n\u2022 Used Factory, Singleton design patterns for implementing enterprise modules/DTO\'s.\n\u2022 Extensively used the struts application resources properties file for error codes, views labels and for Product Internationalization.\n\u2022 Extensively used the Struts controller component classes for developing the applications.\n\u2022 Struts 1.2 has provided its own Controller component and integrates with other technologies to provide the Model and the View for the Model, used Struts to interact with standard data access technologies, like JDBC and EJB.\n\u2022 Used RAD (Rational Application Developer 7.0) as a Development platform\n\u2022 Struts Framework provided the functionality to validate the form data. It\'s used to validate the data on the users browser as well as on the server side. Struts Framework emits the java scripts and it\'s used to validate the form data on the client browser.\n\u2022 JavaBeans were used to store in a number of different collections of ""attributes"". The JavaServer Pages (JSP) Specification defines scope choices.\n\u2022 Consumed webservices using AxisWebservices.\n\u2022 Used JavaScript for the web page validation and Struts Validator for server side validation of data.\n\u2022 Developed SQLstoredprocedures and prepared statements for updating and accessing data from database.\n\u2022 Used JDBC and Hibernate to connect to the database using Oracle.\n\nEnvironment: HTML, CSS, JavaScript, Struts, J2EE, JDBC, oracle 10g.', u'Java Developer\nTejas Networks - Bengaluru, Karnataka\nJune 2009 to March 2011\nDescription:Tejas Networks is an India-based optical and data networking products company. Tejas designs, develops, and sells high-performance and cost-competitive products to telecommunications service providers, internet service providers, utilities, defence and government entities in over 60 countries. Tejas products utilize a programmable software-defined hardware architecture with a common software code-base that delivers an app-like ease of development and upgrades of new features and technology standards. Tejas is ranked amongst top-10 suppliers in the global optical aggregation segment.\n\nResponsibilities:\n\u2022 Involved in implementingJ2EEdesignpatterns for designing this application.\n\u2022 Used Factory, Singleton design patterns for implementing enterprise modules/DTO\'s.\n\u2022 Extensively used the struts application resources properties file for error codes, views labels and for Product Internationalization.\n\u2022 Developed the Web Interface using Struts, HibernatesJavaScript, HTML and CSS.\n\u2022 Extensively used the Struts controller component classes for developing the applications.\n\u2022 Struts 1.2 has provided its own Controller component and integrates with other technologies to provide the Model and the View for the Model, used Struts to interact with standard data access technologies, like JDBC and EJB.\n\u2022 Used RAD (Rational Application Developer 7.0) as a Development platform\n\u2022 Used SVN for source code control and JUNIT for unit testing.\n\u2022 JavaBeans were used to store in a number of different collections of ""attributes"". The JavaServer Pages (JSP) Specification defines scope choices.\n\nEnvironment: HTML, CSS, JavaScript, Struts, J2EE, JDBC, Rational Application Developer, JSP, SVN, Oracle 10g.']","[u""Bachelor's""]",[u''],"degree_1 : ""Bachelors"""
0,https://resumes.indeed.com/resume/72415ecc252cc3a9,"[u'Data Analyst / Scientist\nVerizon - Township of Warren, NJ\nJanuary 2017 to January 2018\nResponsibilities:\n\u2022 Writing test cases in Python for statistical analysis and created predictive analysis reports in different domains.\n\u2022 Created a Machine Learning model to predict the business that will be most successful in region wise.\n\u2022 Performing data analysis using statistical and machine learning methodologies to advance unstructured data sets.\n\u2022 Working on consensus algorithms using Python.\n\u2022 Created interactive dashboards in tableau to assist end-user decision making.\n\u2022 Building, publishing customized interactive reports and dashboards, report scheduling using Tableau server.\n\u2022 Extensively worked Tableau Server and created report schedules, backups, and restoring backups into the repository.\n\u2022 Involved in the design and development of user interfaces and customization of Reports using Tableau and designed cubes for data visualization, forecasting reports using historical data.\n\u2022 Generated various presentable reports and documentation using report designer.\n\u2022 Analyze data model, write business algorithms using stored procedures and queries, and perform proof of concepts for new data architecture.\n\u2022 Created databases, tables, views & schemas. Performed users access authorization, defined locations, populated tables with data and performed unload/copy/replicate data from one database into another after detail analysis and design.\n\u2022 Collected, reviewed and analyzed quarterly and annual financial asset data.\n\u2022 Preparing Dashboards using calculations, parameters in Tableau Desktop and Server.\n\u2022 Evaluated data profiling, quality, cleansing, integration and extraction tool.\n\u2022 Created reports to show quality and quantitative trends to define key metrics.\n\u2022 Verified new deployments and contributed to evolution of various tools to improve data quality.\n\u2022 Worked with cloud based technology like Redshift, S3, AWS, EC2 Machine, etc. and extracting the data from the Oracle financials and the Redshift database.\nEnvironment:\nTableau, Python, Statistics, Artificial Intelligence, multi-threading, MATLAB R2017a, Amazon Web Services, No SQL, PL/SQL.', u""Data Analyst\nAltria\nMay 2014 to February 2016\nResponsibilities:\n\u2022 Provided assistance for defining of SAP roles and new business processes.\n\u2022 Reviewed and maintained data structures in SAP modules.\n\u2022 Extracted data from SAP BI using OLTP and OLAP connections to do modelling and designing in Universe level.\n\u2022 Utilized SAP ETL toolset (Data Explorer, and Data Quality) to analyze legacy data for data profiling.\n\u2022 Generated Xcelsius dashboards and reports using BEx, WEBI, DESKI, and Crystal and performed analysis for decision making.\n\u2022 Performed modeled data extraction process from universe to Xcelsius to generate dashboards using QAAWS.\n\u2022 Building, publishing customized interactive reports and dashboards, report scheduling using Tableau server.\n\u2022 Generated Tableau ad-hoc reports using excel sheet, flat files, CSV files to.\n\u2022 Generated tableau dashboards with combination charts for clear understanding.\n\u2022 Monitored end - to - end workflow process to enter, create, or change the data.\n\u2022 Conducted or performed data modeling exercises in support of subject areas and/or specific client needs for data, reports, or analyses, with a concern towards reuse of existing data elements, alignment with existing data assets and target enterprise data architecture.\n\u2022 Tested various ETL transformation rules based on log files, data movement and with help of SQL.\n\u2022 Involved with Business Unit for Gap Analysis to check the compatibility of the existing system infrastructure with the new business requirements. Executed SQL Queries to check the data table updates after test executions.\n\u2022 Tested various ETL transformation rules based on log files, data movement and with help of SQL.\n\u2022 Developed conceptual, logical, and physical Enterprise Data Model based on industry standards.\n\u2022 Involved in preparing logical data models and conducted controlled brain-storming sessions with project focus groups.\n\u2022 Involved in designing Context Flow Diagrams, Structure Chart and ER- diagrams and worked on database features and objects such as partitioning, change data capture, indexes, views, indexed views to develop optimal physical data mode.\n\u2022 Designed the Logical Model into Dimensional Model using Star Schema and Snowflake Schema.\n\u2022 Executed enterprise data governance strategies in line with the organization's strategic plan and objectives.\n\u2022 Assessed data repositories for compliance with data governance policies and standards. Works with all areas of the organization to ensure data quality and integrity.\n\u2022 Involved in the data profiling activities for the column assessment and natural key study.\n\u2022 Developed complex SQL queries, and perform execution validation for remediation and Analysis.\n\u2022 Used Normalization methods up to 3NF and De-normalization techniques for effective performance in OLTP systems.\nEnvironment:\nSAP NetWeaver, SAP Business Intelligence, Business Objects, Crystal Reports, Excelsius Dashboards, Rational Requisite Pro, MSOffice (Word, Excel and PowerPoint), MS Project, MS Access, CSV files, UML, Erwin, MS Visio, Oracle 11g, RDBMS, SQL Server 2008, SQL."", u'Data Analyst\nFibcom India Limited\nSeptember 2009 to April 2014\nResponsibilities:\n\u2022 Gathered business requirements through interviews, surveys, prototyping and observing from account managers and UI (User Interface) of the existing Broker Portal system.\n\u2022 Translated the business requirements into workable functional and non-functional requirements at detailed production level using Workflow Diagrams, Sequence Diagrams, Activity Diagrams and Use Case Modelling.\n\u2022 Identified the objects and relationships between the Healthcare objects to develop a logical model in Erwin tool.\n\u2022 Implemented Slowly Changing Dimensions Type2 and Type3 for accessing history of reference Healthcare data changes.\n\u2022 Identified entities and attributes and developed conceptual, logical and physical models using Erwin.\n\u2022 Used reverse engineering to create Graphical Representation (E-R diagram) and to connect to existing database.\n\u2022 Writing Complex SQL queries in Teradata to pull the records with data issues reported by the users using complex joins, partitions, ordered analytical functions, aggregate functions etc.\n\u2022 Defined and implemented Data Governance Management tools.\n\u2022 Created and implemented the Data Governance framework that meets data objectives for the organization.\n\u2022 Established and executed the Data Quality Governance Framework, which includes end-to-end process.\n\u2022 Quality framework for assessing decisions that ensure the suitability of data for its intended purpose.\n\u2022 Developed Healthcare data mapping documents between Legacy, Production, and User Interface Systems.\n\u2022 Creating or modifying the T-SQL queries as per the business requirements.\n\u2022 Designed and Developed Oracle database Tables, Views, Indexes with proper privileges and Maintained and updated the database by deleting and removing old data.\nEnvironment:\nOracle 11g, SQL, Erwin, ODBC, RDBMS, SQL Server, Rational Rose, HTML, Agile methodology.']","[u'MS in Information Systems and Technology', u""Bachelor's degree in Reproducible Research""]","[u'Wilmington University\nMay 2017', u'Nagarjuna University\nMay 2008']","degree_1 : MS in Information Systems and Technology, degree_2 :  ""Bachelors degree in Reprodcible Research"""
0,https://resumes.indeed.com/resume/10f6cf98ba444de5,"[u'Data Scientist\nProcter & Gamble - Cincinnati, OH\nJanuary 2018 to Present\n\u2022 Project to improve Demand forecasting accuracy and to predict Anomaly in Amazon regular orders towards P&G key products using machine learning algorithms\n\u2022 Transformation of unstructured and semi-structured data into time series data, variable selection using Autocorrelation and regression in R\n\u2022 Prediction model building using Dynamic regression and Arima models, Anomaly detection using Random forest classification', u""Supply Chain Analytics lead\nDr. Reddy's Laboratories\nMay 2009 to June 2017\n\u2022 Led supply chain analytics team for Project \u201cintelligent integration\u201d for building KPI dashboards, Demand Analytics, transportation & procurement analytics reducing overall supply chain system inventory and cost of poor execution (COPE)\n\u2022 Transformed structured data from SAP ECC and APO into SAP BI, Designed KPI queries for different modules e.g. Forecasting Accuracy, Procurement and Delivery OTIF, Transportation cost optimization, Process lead times, Production planned vs Actuals\n\u2022 Created KPI and Analytics Dashboard on Tableau for senior leadership decision making and S&OP process\n\u2022 Sales Analytics & Budgeting for LATAM market \u2013 Analyzed internal, IMS semi-structured data for promotion and distribution strategy, Competitor analysis, Sales incentive planning, Long term product portfolio planning, Discount/ Bonus decision\n\u2022 Designed and Implemented transactional data analytics tools by developing queries in internal database for various business functions sales, supply chain, warehousing, imports, logistics cost in Excel VBA and Tableau\n\u2022 Aligned distribution strategy with Market promotion strategy after analyzing primary, secondary and tertiary sales data\n\u2022 Designed and Implemented SAP MM and SD modules in LATAM and CIS market to improve Data quality index and data integrity, Transformed MIS system by generating key automatic reports\n\u2022 Optimized warehouse operations by reducing manpower overtime requirement, optimizing transportation cost by better forecast of customer orders using simulation modeling on excel\n\u2022 Produced sales and operational analyses from internal/external (IMS) data sources for use in decision making improved gaining market share from 0.5% to 2% (USD 38 M to USD 140 M in two years)""]","[u'MS in Business in Analytics', u'MBA in Supply Chain Operations', u""Bachelor's in engineering in Marine Engineering and Naval Architecture""]","[u'University of Cincinnati, Lindner School of Business Cincinnati, OH\nApril 2018', u'Indian Institute of Technology Madras Chennai, Tamil Nadu\nMay 2009', u'Marine Engineering & Research Institute Kolkata, West Bengal\nOctober 2001']","degree_1 : MS in Bsiness in Analytics, degree_2 :  MBA in Spply Chain Operations, degree_3 :  ""Bachelors in engineering in Marine Engineering and Naval Architectre"""
0,https://resumes.indeed.com/resume/5660181539bfe2f8,"[u""Founder/Artificial Intelligence Application Engineer\nNTM Corporation - McDonough, GA\nAugust 2017 to Present\n\u2022 Develop and test proprietary artificial intelligence algorithms\n\u2022 Develop MySQL databases for AI to access data for analysis and store results.\n\u2022 Lead application of NTM Corp. artificial intelligence algorithm for the analysis of human genome\ndata.\n\u2022 Wrote computer code to reformat genomic data to be readable by the AI.\n\u2022 Additional developed applications for the AI include financial market and professional sports\nanalysis.\n\u2022 Designed company website and wrote informational articles to communicate the company's work\nwith the public. Articles were shared through social media."", u'Assistant Professor, Mathematics\nLimited Term - Marietta, GA\nAugust 2016 to Present\n\u2022 Teach Calculus I, II, Precalculus, and College Trigonometry.\n\u2022 Develop and implement lesson plans for undergraduate students in mathematics; track and analyze student performance.\n\u2022 Create online materials for students to access and use as study guides and for assignments.\n\u2022 Conduct group and one-on-one office hours.\n\u2022 Foster a teaching environment conducive to learning and promoting excellent student/teacher\ninteraction; strive for student engagement in learning.\n\u2022 Apply various teaching methods for differing learning styles.', u'Founder/Data Scientist\nMedEquality LLC - McDonough, GA\nApril 2015 to August 2016\nProgrammed, along with co-founder Jonathan Charlton, the web application MedEquality.com,\nwhich allows users to comparison-shop for medical procedures locally and nationally, based on pricing and quality metrics.\n\u2022 Wrote code to develop visualizations and store user activity such as button clicks, searches, and preferences into the MedEquality database.\n\u2022 Programmed web-scraping algorithms into the MedEquality web application, to collect\ngeographical, IP address, time, and date data of users, to allow for attribution of user preferences and pattern recognition.\n\u2022 To augment the R Shiny functionality, programmed JavaScript and HTML functions into the user\ninterface, allowing features such as social media connectivity, graphics displays, and Disqus\nmessage boards.\n\u2022 Performed statistical modeling validation on proprietary machine learning algorithms. Algorithm\ncollected live data from Yahoo Finance APIs and made predictions on market behavior.\nProgrammed model to return performance measures in real time such as accuracy, predictive\nvalue, sensitivity, and specificity.\n\u2022 Programmed web application as a subcontractor for Alvarez & Marsal LLC. Web application was\ndesigned to allow members of the Utah state legislature to visualize different scenarios and consequences as they shifted money to different programs throughout the state.\n\u2022 Created the MedEquality corporate page and blog (MedEqualityLLC.com). Wrote and shared\narticles about technology and medical innovations to build brand awareness and increase\nengagement on LinkedIn and MedEquality.com.', u'Consultant/Business and Statistical Analyst\nVeterans Health Administration - Atlanta, GA\nAugust 2014 to June 2015\nProgrammed in R to mine databases for the United States Department of Veterans Affairs (VA).\nThis data mining supported the Veterans Transportation Program, which provides transportation for ambulatory and non-ambulatory patients to and from appointments at the VA hospitals\nnationwide.\n\u2022 Programmed in R to access MS SQL Server to retrieve data from VA servers.\n\u2022 Built cost models to compare expected costs with actual costs to assist program managers with oversight and to predict future costs.\n\u2022 Determined cost per mile based on fuel consumption and maintenance for a variety of VA\nMedical Centers across the nation. This was the first instance of their having such analysis.\n\u2022 Performed cluster analysis on datasets to segment VA hospitals by key demographics. This\nanalysis will assist the VA in predicting future costs involved with expanding the program.\n\u2022 Produced a demo of an executive dashboard and presented it to the upper management. The\ndemo incorporated a searchable map of VA hospitals across the United States, mileage metrics,\ncost modeling, and veteran demographics.\n\u2022 Assisted the VA in establishing data standards and guidelines to collect and analyze data more\nefficiently as the program matures and expands.', u""Data Scientist\nBooz Allen Hamilton - Atlanta, GA\nJanuary 2014 to June 2014\nProgrammed in R to perform data analysis for projects in support of the Centers for Disease\nControl and Prevention. Projects focused primarily on digital disease detection.\n\u2022 As part of an Agile scrum team, helped develop a web-based platform to analyze CDC data\ncollected from New York, Houston, and Indianapolis. Platform was designed to provide analytic and predictive modeling for detection of bacteriological and epidemiological pathogens.\n\u2022 Created web-based applications, using R Shiny, as tools for data analysis and visualization.\nApplications allowed users to input and manipulate data and download results. Incorporated\nsearchable maps, graphs, models, and JavaScript/HTML objects into the applications.\n\u2022 As a part of an Agile scrum team, helped to develop an R Shiny phone log for the Atlanta\nVolunteer Lawyers Foundation. The phone log allowed the law firm to track their incoming calls\nand collect accurate referral and intake data. The phone log produced histograms and cumulative distributions to help the group visualize their activity throughout the years. These\ndata have a direct impact on the firm's referral fees and grant reports.\n\u2022 Contributed to the development of a low-level NLP program to analyze Twitter data feeds.\nTwitter application produced word clouds and frequency data based on user search terms.\n\nBIOTECHNOLOGY SKILLS\n\u2022 Process development for nanoparticle-based gene therapy experiments. Applications include treatment\nof \u2022-thalassemia, cancer, and cystic fibrosis.\n\u2022 3-dimensional force microscopy to probe nanoscale mechanics of biomolecules and polymers.\n\u2022 Characterization of polymers on the nanoscale by measuring diffusion and active transport.\n\u2022 Design and development of novel instrumentation for biomedical research.\n\u2022 Process development for nanotechnology-based biomedical experiments.\n\u2022 3D printing to create instrumentation for lab experiments and prototypes.\n\u2022 Gaussmeters to characterize magnetic fields and field gradients.\n\u2022 Sterile cell culture technique, culturing a variety of cancer cells, lung cells, and stem cells. Sterile\nhandling of reagents. Cryopreservation to maintain cell lines.\n\u2022 Experience following and developing protocols to use biological reagents/chemicals.\n\u2022 Experience handling and experimenting with patient-derived bodily tissues, including synovial\nfluid, mucus, and sputum.\n\u2022 Cell transfection using nanoparticles, lipid-based reagents, and baculoviruses. Analysis of cytotoxicity.\n\u2022 Fluorescent cell staining to image cell features (e.g., organelles, cytoskeleton, membrane).\n\u2022 UV-Vis spectrophotometry, fluorescence assays, BCA protein assay, confocal and widefield microscopy.\n\u2022 Process development to achieve binding of polymers, proteins, and nucleic acids to different substrates,\nincluding nanoparticles and glass.\n\u2022 Microcontroller programming to design biomedical instrumentation. (e.g. heart rate monitors,\ndigital thermometers).\n\u2022 Fe2O3/Fe3O4 and FFPDMS (ferrofluid) nanoparticle fabrication.\n\u2022 Setup and maintenance of laboratory equipment, including computers, incubators, sterile hoods,\nand microscopes."", u'Graduate Research Assistant\nUniversity of North Carolina - Chapel Hill, NC\nAugust 2006 to May 2013\nThesis: Applied Magnetic Forces Enhance Nanoparticle Based Gene Delivery and Characterize\nIntracellular Rheology and Transport. Advisor: Dr. Richard Superfine\n\u2022 Designed a platform for applying oscillating magnetic fields for paramagnetic nanoparticle\ntransfections for gene delivery, and calculated theoretical forces exerted on nanoparticles.\n\u2022 Nanoparticle-based gene delivery corrected aberrant splicing mutations in cells. Potential\napplications include treatment of \u2022-thalassemia, cancer, and cystic fibrosis.\n\u2022 Platform improved drug delivery efficiency by a factor of 2.3X.\n\u2022 Results of data analysis produced the discovery of endocytic pathways which are sensitive to oscillating force stimulation for treatment of cancer cells.\n\u2022 Used paramagnetic nanoparticles to deliver splice-correcting antisense oligonucleotides to \u2022- thalassemic HeLa cells.\n\u2022 Determined the frequency dependence of force oscillation for magnetic drug delivery in cervical\ncancer cells. This serves to optimize drug/gene therapy for non-viral vectors.\n\u2022 Performed process development to functionalize nanoparticles to make them effective for gene\ntherapy or as probes for cancer cells.\n\u2022 Designed motorized stages for all the 3-dimensional force microscopy (3DFM) instruments in our\nlab. These stages greatly improved the ability to adjust the z-resolution of the pole tip, a key\nfactor in modulating the force applied in biomechanics experiments.\n\u2022 Maintained stocks of DMEM, PBS, DMSO, solvents, cell culture dishes, fluorescent stains, and other reagents for the lab.\n\u2022 Was responsible for maintaining sterile hoods, incubators, microscopes, and other lab equipment\nthat I and other members of the lab used.']","[u'Ph.D. in Biomedical Engineering', u'B.S. in Mathematics']","[u'University of North Carolina Chapel Hill, NC\nMay 2013', u'Hampton University Hampton, VA\nMay 2001']","degree_1 : Ph.D. in Biomedical Engineering, degree_2 :  B.S. in Mathematics"
0,https://resumes.indeed.com/resume/70f126c3afb08d84,"[u'Data Scientist\nIWhere Co., Ltd - Pleasanton, CA\nDecember 2016 to Present', u'Data Scientist - Consultant\nIWhere(Beijing) Co., Ltd - Beijing, CN\nJune 2015 to December 2016\n\u2022 Identified key metrics to measure the success of products and features, design A/B test or hypothesis test plan, processed data sets with advanced querying, analyzed data with analytics packages in Python and R, and interpreted results to the cross-functional team.\n\u2022 Indicated and diagnosed problems with KPIs variations, located effective variables with statistical models, and recommended improvement solutions.\n\u2022 Built regression, clustering, and classification models to evaluate and optimize business actions. Boosted 35% on user acquisition, extended life-circle by 20%.\n\u2022 Cooperated with product and engineering team on feeds recommender with Bayes and Markov techniques, lifted user engagement by 15%.\nData Scientist - Consultant 2015.6-2016.12\nIWhere(Beijing) Co., Ltd. Beijing, China', u'Data Analyst\nAHome LLC - Arcadia, CA\nFebruary 2014 to June 2015']","[u'Master of Science in Statistics', u'Bachelor of Science in Statistics']","[u'San Jose State University San Jose, CA\nAugust 2011 to December 2014', u'Capital University of Economics and Business Beijing, CN\nAugust 2007 to July 2011']","degree_1 : Master of Science in Statistics, degree_2 :  Bachelor of Science in Statistics"
0,https://resumes.indeed.com/resume/cbb67b6356600313,"[u""Data Scientist/ Machine Learning\nFIS - Jacksonville, FL\nJanuary 2017 to Present\nDescription: FIS provides financial software, world-class services and global business solutions. Let us help you compete and win in today's chaotic marketplace. Fidelity National Information Services Inc., better known by the abbreviation FIS, is an international provider of financial services technology and outsourcing services. FIS is the world's largest global provider dedicated to financial technology solutions. FIS empowers the financial world with software, services, consulting and outsourcing solutions focused on retail and institutional banking, payments, asset and wealth management, risk and compliance, trade enablement, transaction processing and record-keeping.\n\nResponsibilities:\n\u2022 Extracted data from HDFS and prepared data for exploratory analysis using data munging\n\u2022 Built models using Statistical techniques like Bayesian HMM and Machine Learning classification models like XGBoost, SVM, and Random Forest.\n\u2022 Participated in all phases of data mining, data cleaning, data collection, developing models, validation, visualization and performed Gap analysis.\n\u2022 A highly immersive Data Science program involving Data Manipulation&Visualization, Web Scraping, Machine Learning, Python programming, SQL, GIT, MongoDB, Hadoop.\n\u2022 Setup storage and data analysis tools in AWS cloud computing infrastructure.\n\u2022 Installed and used Caffe Deep Learning Framework\n\u2022 Worked on different data formats such as JSON, XML and performed machine learning algorithms in Python.\n\u2022 Worked as Data Architects and IT Architects to understand the movement of data and its storage and ER Studio 9.7\n\u2022 Used pandas, numpy, seaborn, matplotlib, scikit-learn, scipy, NLTK in Python for developing various machine learning algorithms.\n\u2022 Data Manipulation and Aggregation from different source using Nexus, Business Objects, Toad, Power BI and Smart View.\n\u2022 Implemented Agile Methodology for building an internal application.\n\u2022 Focus on integration overlap and Informatica newer commitment to MDM with the acquisition of Identity Systems.\n\u2022 Coded proprietary packages to analyze and visualize SPCfile data to identify bad spectra and samples to reduce unnecessary procedures and costs.\n\u2022 Programmed a utility in Python that used multiple packages (numpy, scipy, pandas)\n\u2022 Implemented Classification using supervised algorithms like Logistic Regression, Decision trees, Naive Bayes, KNN.\n\u2022 As Architect delivered various complex OLAPdatabases/cubes, scorecards, dashboards and reports.\n\u2022 Updated Python scripts to match training data with our database stored in AWS Cloud Search, so that we would be able to assign each document a response label for further classification.\n\u2022 Used Teradata utilities such as Fast Export, MLOAD for handling various tasks data migration/ETL from OLTP Source Systems to OLAP Target Systems\n\u2022 Data transformation from various resources, data organization, features extraction from raw and stored.\n\u2022 Validated the machine learning classifiers using ROC Curves and Lift Charts.\n\nEnvironment:Unix, Python 3.5.2, MLLib, SAS, regression, logistic regression, Hadoop 2.7.4, NoSQL, Teradata, OLTP, random forest, OLAP, HDFS, ODS, NLTK, SVM, JSON, XML and MapReduce."", u""Data Scientist\nCBRE - Dallas, TX\nOctober 2015 to November 2016\nDescription: CBRE Group, Inc. is the largest commercial real estate services and investment firm in the world. It is based in Los Angeles, California and operates more than 450 offices worldwide and has clients in more than 100 countries.\n\nResponsibilities:\n\u2022 Utilized Spark, Scala, Hadoop, HQL, VQL, oozie, pySpark, Data Lake, TensorFlow, HBase, Cassandra, Redshift, MongoDB, Kafka, Kinesis, Spark Streaming, Edward, CUDA, MLLib, AWS, Python, a broad variety of machine learning methods including classifications, regressions, dimensionally reduction etc.\n\u2022 Utilized the engine to increase user lifetime by 45% and triple user conversations for target categories.\n\u2022 Application of various machine learning algorithms and statistical modeling like decision trees, text analytics, natural language processing (NLP), supervised and unsupervised, regression models, social network analysis, neural networks, deep learning, SVM, clustering to identify Volume using scikit-learn package in python, Matlab.\n\u2022 Worked onanalyzing data from Google Analytics, AdWords, Facebook etc.\n\u2022 Evaluated models using Cross Validation, Log loss function, ROC curves and used AUC for feature selection and elastic technologies like ElasticSearch, Kibana.\n\u2022 Performed Data Profiling to learn about behavior with various features such as traffic pattern, location, Date and Time etc.\n\u2022 Categorized comments into positive and negative clusters from different social networking sites using Sentiment Analysis and Text Analytics\n\u2022 Performed Multinomial Logistic Regression, Decision Tree, Random forest, SVM to classify package is going to deliver on time for the new route.\n\u2022 Performed data analysis by using Hive to retrieve the data from Hadoop cluster, Sql to retrieve datafrom Oracle database and used ETL for data transformation.\n\u2022 Performed Data Cleaning, features scaling, features engineering using pandas and numpy packages in python.\n\u2022 Exploring DAG's, their dependencies and logs using AirFlow pipelines for automation\n\u2022 Performed data cleaning and feature selection using MLlib package in PySpark and working with deep learning frameworks such as Caffe, Neon.\n\u2022 Developed Spark/Scala,R Python for regular expression (regex) project in the Hadoop/Hive environment with Linux/Windows for big data resources.\n\u2022 Used clustering technique K-Means to identify outliers and to classify unlabeled data.\n\u2022 Tracking operations using sensors until certain criteria is met using AirFlowtechnology.\n\u2022 Responsible for different Data mapping activities from Source systems to Teradata using utilities like TPump, FEXP,BTEQ, MLOAD, FLOADetc\n\u2022 Analyze traffic patterns by calculating autocorrelation with different time lags.\n\u2022 Ensured that the model has low False Positive Rate and Text classification and sentiment analysis for unstructured and semi-structured data.\n\u2022 Addressed over fitting by implementing of the algorithm regularization methods like L1 and L2.\n\u2022 Used Principal Component Analysis in feature engineering to analyze high dimensional data.\n\u2022 Used MLlib, Spark's Machine learning library to build and evaluate different models.\n\u2022 Implemented rule based expertise system from the results of exploratory analysis and information gathered from the people from different departments.\n\u2022 Created and designed reports that will use gathered metrics to infer and draw logical conclusions of past and future behavior.\n\u2022 Developed MapReduce pipeline for feature extraction using Hive and Pig.\n\u2022 Created Data Quality Scripts using SQL and Hive to validate successful data load and quality of the data. Created various types of data visualizations using Python and Tableau.\n\u2022 Communicated the results with operations team for taking best decisions.\n\u2022 Collected data needs and requirements by Interacting with the other departments.\n\nEnvironment:Python 2.x, CDH5, HDFS, Hadoop 2.3, Hive, Impala, AWS, Linux, Spark, Tableau Desktop, SQL Server 2014, Microsoft Excel, Matlab, Spark SQL, Pyspark."", u'Data Analyst\nWalgreens - Deerfield, IL\nDecember 2013 to September 2015\nDescription:The Walgreens Companyis an American company that operatesas the second-largest pharmacy store chain in the United States. It specializes in filling prescriptions, health and wellness products, health information, and photo services\n\nResponsibilities:\n\u2022 Worked with BI team in gathering the report requirements and also Sqoop to export data into HDFS and Hive\n\u2022 Involved in the below phases of Analytics using R, Python and Jupyter notebook.\n\u2022 a. Data collection and treatment: Analysed existing internal data and external data, worked on entry errors,classification errors and defined criteria for missing values\nb. Data Mining: Used cluster analysis for identifying customer segments, Decision trees used for profitable and non-profitable customers, Market Basket Analysis used for customer purchasing behaviour and part/product association.\n\u2022 Developed multiple Map Reduce jobs in Java for data cleaning and preprocessing.\n\u2022 Assisted with data capacity planning and node forecasting.\n\u2022 Installed, Configured and managed Flume Infrastructure\n\u2022 Administrator for Pig, Hive and HBase installing updates patches and upgrades.\n\u2022 Worked closely with the claims processing team to obtain patterns in filing of fraudulent claims.\n\u2022 Worked on performing major upgrade of cluster from CDH3u6 to CDH4.4.0\n\u2022 Developed Map Reduce programs to extract and transform the data sets and results were exported back to RDBMS using Sqoop.\n\u2022 Patterns were observed in fraudulent claims using text mining in R and Hive.\n\u2022 Exported the data required information to RDBMS using Sqoop to make the data available for the claims processing team to assist in processing a claim based on the data.\n\u2022 Developed Map Reduce programs to parse the raw data, populate staging tables and store the refined data in partitioned tables in the EDW.\n\u2022 Created tables in Hive and loaded the structured (resulted from Map Reduce jobs) data\n\u2022 Using HiveQL developed many queries and extracted the required information.\n\u2022 Created Hive queries that helped market analysts spot emerging trends by comparing fresh data with EDW reference tables and historical metrics.\n\u2022 Was responsible for importing the data (mostly log files) from various sources into HDFS using Flume\n\u2022 Enabled speedy reviews and first mover advantages by using Oozie to automate data loading into the Hadoop Distributed File System and PIG to pre-process the data.\n\u2022 Provided design recommendations and thought leadership to sponsors/stakeholders that improved review processes and resolved technical problems.\n\u2022 Managed and reviewed Hadoop log files.\n\u2022 Tested raw data and executed performance scripts.\n\nEnvironment:HDFS, PIG, HIVE, Map Reduce, Linux, HBase, Flume, Sqoop, R, VMware, Eclipse, Cloudera, Python.', u""Data Scientist\nJohnson and Johnson - Raritan, NJ\nSeptember 2012 to November 2013\nDescription:Transamerica is the US-based brand of Aegon, a Dutch financial services firm. Aegon is one of the world's leading providers of life insurance, pensions and asset management and is helping approximately 30 million customers globally to achieve a lifetime of financial security.\n\nResponsibilities:\n\u2022 Manipulating, cleansing & processing data using Excel, Access and SQL.\n\u2022 Responsible for loading, extracting and validation of client data.\n\u2022 Modelled clean data into the Kafka servers for use over the spark engine.\n\u2022 Zookeeper along with Kafka was used to stream data and end-to-end client communication.\n\u2022 Performed transformations over the warehoused data using Scala& Python and modelled the data back into the servers for iterative transformations into KAFKA.\n\u2022 Modelled data using Machine learning libraries(Sci-kit learn) apart from SVN and KNN based classificationto create a training dataset for use in a predictive model.\n\u2022 Liaising with end-users and 3rd party suppliers.\n\u2022 Analyzing raw data, drawing conclusions & developing recommendations\n\u2022 Writing T-SQL scripts to manipulate data for data loads and extracts.\n\u2022 Developing data analytical databases from complex financial source data.\n\u2022 Performing daily system checks.\n\u2022 Data entry, data auditing, creating data reports & monitoring all data for accuracy.\n\u2022 Designing, developing and implementing new functionality.\n\u2022 Monitoring the automated loading processes.\n\u2022 Advising on the suitability of methodologies and suggesting improvements.\n\u2022 Carrying out specified data processing and statistical techniques.\n\u2022 Supplying qualitative and quantitative data to colleagues & clients.\n\u2022 Using Informatica& SAS to extract, transform & load source data from transaction\nsystems.\n\u2022 Performed sequential analytics using SAS Enterprise miner using jobs fed by the SAS Grid Manager.\n\u2022 Loaded packages and stored procedures using Base SAS and integrated functional and business requirements using the EBI suite.\n\u2022 Creating data pipelines using big data technologies like Hadoop, spark etc.\n\u2022 Creating statistical models using distributed and standalone models to build various diagnostics, predictive and prescriptive solution.\n\u2022 Utilize a broad variety of statistical packages like SAS, R , MLIB, Graphs,Hadoop, Spark , MapReduce, Pig\n\u2022 Performed a check using quality parameters fed using the SAS QC engine.\n\u2022 Created a UI dashboard for end users and performed prototype testing using Tableau.\n\u2022 Refine and train models based on domain knowledge and customer business objectives\n\u2022 Deliver or collaborate on delivering effective visualizations to support the client business objectives\n\u2022 Communicate to your peers and managers promptly as and when required.\n\u2022 Produce solid and effective strategies based on accurate and meaningful data reports and analysis and/or keen observations.\n\u2022 Establish and maintain communication with clients and/or team members; understand needs, resolve issues, and meet expectations\n\u2022 Developed web applications using .net technologies; work on bug fixes/issues that arise in the production environment and resolve them at the earliest\n\nEnvironment:Cloudera, HDFS, Pig, Hive, Map Reduce, python, Sqoop, Storm, Kafka, LINUX, Hbase, Impala, Java, SQL, Cassandra, MongoDB, SVN."", u'Data Architect/Data Modeler\nInfotech Enterprises IT Services Pvt. Ltd - Hyderabad, Telangana\nDecember 2010 to August 2012\nDescription: Infotech Enterprises IT Services Pvt. Ltd. (Infotech IT), part of the $ 260 million Infotech Enterprises group, leverages its business process knowledge, technological competence, strategic alliances and strong global presence to offer innovative IT solutions to the Retail Industry.\n\nResponsibilities:\n\u2022 Worked with large amounts of structured and unstructured data.\n\u2022 Knowledge in Machine Learning concepts (Generalized Linear models, Regularization, Random Forest, Time Series models, etc.)\n\u2022 Worked in Business Intelligence tools and visualization tools such as Business Objects, Tableau, ChartIO, etc.\n\u2022 Deployed GUI pages by using JSP, JSTL, HTML, DHTML, XHTML, CSS, JavaScript, AJAX.\n\u2022 Configured the project on WebSphere 6.1 application servers\n\u2022 Implemented the online application by using Core Java, Jdbc, JSP, Servlets and EJB 1.1, Web Services, SOAP, WSDL.\n\u2022 Handled end-to-end project from data discovery to model deployment.\n\u2022 Monitoring the automated loading processes.\n\u2022 Communicated with other Health Care info by using Web Services with the help of SOAP, WSDLJAX-RPC\n\u2022 Used Singleton, factory design pattern, DAO Design Patterns based on the application requirements\n\u2022 Used SAX and DOM parsers to parse the raw XML documents\n\u2022 Used RAD as Development IDE for web applications.\n\u2022 Preparing and executing Unit test cases\n\u2022 Used Log4J logging framework to write Log messages with various levels.\n\u2022 Involved in fixing bugs and minor enhancements for the front-end modules.\n\u2022 Implemented Microsoft Visio and Rational Rose for designing the Use Case Diagrams, Class model, Sequence diagrams, and Activity diagrams for SDLC process of the application\n\u2022 Doing functional and technical reviews\n\u2022 Maintenance in the testing team for System testing/Integration/UAT.\n\u2022 Guaranteeing quality in the deliverables.\n\u2022 Conducted Design reviews and Technical reviews with other project stakeholders.\n\u2022 Was a part of the complete life cycle of the project from the requirements to the production support\n\u2022 Created test plan documents for all back-end database modules\n\u2022 Implemented the project in Linux environment.\n\nEnvironment: R 3.0, Erwin 9.5, Tableau 8.0, MDM, QlikView, MLLib, PL/SQL, HDFS, Teradata 14.1, JSON, HADOOP (HDFS), MapReduce, PIG, Spark, R Studio, MAHOUT, JAVA, HIVE, AWS.', u""Data Analyst/ Data Modeler\nGlobalLogic Technologies - Hyderabad, Telangana\nApril 2009 to November 2010\nDescription:GlobalLogic provides experience design, Digital Product Engineering Services, and Agile Software Development to the world's top brands by leveraging UX UI Design, next-gen technologies, and cloud software, with end-to-end solution by the best Software Development Company.\n\nResponsibilities:\n\u2022 Developed Internet traffic scoring platform for ad networks, advertisers and publishers (rule engine, site scoring, keyword scoring, lift measurement, linkage analysis).\n\u2022 Responsible for defining the key identifiers for each mapping/interface.\n\u2022 Clients include eBay, Click Forensics, Cars.com, Turn.com, Microsoft, and Looksmart.\n\u2022 Designed the architecture for one of the first analytics 3.0. online platforms: all-purpose scoring, with on-demand, SaaS, API services. Currently under implementation.\n\u2022 Web crawling and text mining techniques to score referral domains, generate keyword taxonomies, and assess commercial value of bid keywords.\n\u2022 Developed new hybrid statistical and data mining technique known as hidden decision trees and hidden forests.\n\u2022 Reverse engineering of keyword pricing algorithms in the context of pay-per-click arbitrage.\n\u2022 Implementation of Metadata Repository, Maintaining Data Quality, Data Cleanup procedures, Transformations, Data Standards, Data Governance program, Scripts, Stored Procedures, triggers and execution of test plans\n\u2022 Performed data quality in Talend Open Studio.\n\u2022 Coordinated meetings with vendors to define requirements and system interaction agreement documentation between client and vendor system.\n\u2022 Automated bidding for advertiser campaigns based either on keyword or category (run-of-site) bidding.\n\u2022 Creation of multimillion bid keyword lists using extensive web crawling. Identification of metrics to measure the quality of each list (yield or coverage, volume, and keyword average financial value).\n\u2022 Enterprise Metadata Library with any changes or updates.\n\u2022 Document data quality and traceability documents for each source interface.\n\u2022 Establish standards of procedures.\n\u2022 Generate weekly and monthly asset inventory reports.\n\nEnvironment: Erwin r7.0, SQL Server 2000/2005, Windows XP/NT/2000, Oracle 8i/9i, MS-DTS, UML, UAT, SQL Loader, OOD, OLTP, PL/SQL, MS Visio, Informatica.""]","[u""Bachelor's""]",[u''],"degree_1 : ""Bachelors"""
0,https://resumes.indeed.com/resume/431765cd1c3d2f8f,"[u'Data Scientist\nSpire Global Inc - Boulder, CO\nJanuary 2016 to Present', u'Project Scientist II\nNational Center for Atmospheric Research (NCAR) - Boulder, CO\nJanuary 2004 to January 2016', u'Research Scientist II\nUniversity of Colorado - Boulder, CO\nMarch 1997 to December 2003', u'Research Assistant\nThe University of Michigan - Ann Arbor, MI\nJanuary 1991 to December 1996']","[u'Ph.D. in Atmospheric and Space Sciences', u'B.S. in Meteorology']","[u'The University of Michigan Ann Arbor, MI\nJanuary 1996', u'Nanjing Institute of Meteorology Nanjing, CN\nJanuary 1984']","degree_1 : Ph.D. in Atmospheric and Space Sciences, degree_2 :  B.S. in Meteorology"
0,https://resumes.indeed.com/resume/e522f6397e39c0d7,"[u""Data Scientist\nRegions Bank\nNovember 2016 to Present\n- Part of the new data science team (4 members) created under the CDO (Chief Data Officer) for developing predictive analytics and augmented intelligence applications to help the bank in process improvement and for finding new opportunities\n- As a team, assigned to create and architect an enterprise big data environment with Hadoop echo system and IBM Power Minskey\n(Nvidia P100 GPU's) to facilitate data science team and quant squad in the bank\n- Demonstrated the core values of data science and its use cases to various people at the executive level and presented the computational differences between the GPU's, Spark and CPU's through Monte Carlo Simulations to different quant squads\n- Successfully developed applications like Intelligent Web Scrapper for getting information on potential customers from the web, Regions Business Universe which facilitates various departments of the bank to take quick customer decisions and delivered quick process for CIP (Customer Identification Program) which decrease the process time from 20 min to 3 min\n- Acting as lead role for determining the POCO (Probability of Charge-off) for an overdraft account, initial results are promising and seem to provide huge savings to the bank (80% completed)\n- Ongoing Projects: Customer Churn Identification, Financial Shocks Prediction"", u'Big Data Intern\nBig Data Analytics and Research Lab - Birmingham, AL\nJuly 2016 to October 2016\n- Deployed a 5 node Hadoop cluster with all the big data daemons like Spark, Hive, Pig, Avro, YARN and Sqoop with 60 Cores, 160\nGB ECC Ram and 25TB of storage\n- Developed a ""Click Through Rate Prediction Model"" (2014 Kaggle competition model) for calculating the click probability of the user using Logistic Regression (L2 Regularization) with Spark ML pipeline (3 stages)\n\u2217 Final Test Accuracy: 0.86 \u2217 ROC AUC: 0.88']","[u""Master's in Electrical and Computer Engineering""]","[u'University of Alabama at Birmingham Birmingham, AL\nAugust 2015 to April 2017']","degree_1 : ""Masters in Electrical and Compter Engineering"""
0,https://resumes.indeed.com/resume/c8e87ab11fa147b6,"[u'Data science fellow\nInsight Data Science - New York, NY\nJanuary 2018 to Present\nConsulted with a startup company that produces and sells couches\n\nManaged regular communications with the client\n\nQuantified return on online marketing investments analyzing Facebook ads data\n\nUsed several supervised classification algorithms (e.g., random forest, logistic regression) to predict the outcome of future online advertising given targeting choices\n\nDelivered data driven recommendations for targeting choices to use in future marketing campaigns', u'Postdoctoral Research Scientist\nColumbia University - Palisades, NY\nSeptember 2015 to December 2017\nInvestigated the causes of decadal climate variability over the global oceans\n\nAnalyzed and aggregated sparse data from numerical climate model simulations, satellite and surface observations, accounting for missing data\n\nDemonstrated that human and natural emissions contribute to ~70% of North Atlantic temperature change on decadal timescales using a large ensemble of earth model simulations and statistical analysis\n\nBuilt a predictive ARIMAX model that provided comparable predictive skill as operational earth system models reducing computational costs and simulation time from days to minutes\n\nAuthor/co-author of 14 peer-reviewed scientific papers', u'Research Assistant\nRosenstiel School of Marine and Atmospheric Science at University of Miami - Miami, FL\nSeptember 2010 to August 2015\nDemonstrated the impact of cloud radiative feedbacks in increasing the persistence of global climate variability and change\n\nPerformed numerical simulations tuning radiative properties of clouds in a earth system model written in Fortran and running on a supercomputer\n\nDeveloped ad-hoc NCL, Matlab and Python scripts to analyze future climate change scenarios in 40+ climate model simulations from the international CMIP5 archive\n\nTeaching assistant for 3 undergraduate classes\n\nDelivered talks at several international conferences, winning an outstanding presentation award from the American Meteorological Society']","[u'Ph.D. in Meteorology and Physical Oceanography', u'M.S. in Physics', u'B.S. in Physics']","[u'University of Miami Miami, FL\nJanuary 2015', u'University of Torino\nJanuary 2010', u'University of Torino\nJanuary 2008']","degree_1 : Ph.D. in Meteorology and Physical Oceanography, degree_2 :  M.S. in Physics, degree_3 :  B.S. in Physics"
0,https://resumes.indeed.com/resume/3d9d26eaa03255f3,"[u'Data Scientist\nValere Enterprise - Dhaka\nJanuary 2016 to Present\nData Scientist | Quantitative Analyst\n\nProviding data-driven, action-oriented solutions to challenging business problems\n\nBusiness-minded data scientist with a demonstrated ability to deliver valuable insights via data analytics and advanced data-driven methods. Relied on as a key advisor in driving global, multibillion-dollar growth; gains in customer loyalty; and record-setting profit improvements.']","[u'B.Sc In C.S.E in Computer Science and Engineering', u'Bachelor of Science in Computer and Engineering in Data mining and data warehouse']","[u'American International University Bangladesh Dhaka', u'American International University Bangladesh dhaka']","degree_1 : B.Sc In C.S.E in Compter Science and Engineering, degree_2 :  Bachelor of Science in Compter and Engineering in Data mining and data warehose"
0,https://resumes.indeed.com/resume/0a4f6175d8f588f6,"[u'Data Scientist\nBaker Hughes a GE Company - San Francisco Bay Area, CA\nApril 2017 to Present\n\u2022 Developed, coded, and deployed deep learning techniques for anomaly detection and fault prediction from noisy time-series data.\n\u2022 Combined detailed physics models with operational data to predict part failure, prevent downtime, and optimize day-to-day operations for critical industrial equipment.\n\u2022 Evangelized scalable code development from the ground up while working with research engineers from mechanical, chemical, and electrical engineering backgrounds.\n\u2022 Built deep learning analytic products which drive half a million dollars of revenue a month.', u'Staff Research Associate\nUC Davis Medical Center & Health System - Sacramento, CA\nOctober 2012 to August 2016\n\u2022 Acquired and interpreted clinical and laboratory data for a hospital-wide research and quality improvement project involving over 18,000 patients.\n\u2022 Reduced the prevalence of hospital-acquired infections by empowering clinicians with granular health metrics.\n\u2022 Managed large and complex data sets ranging from ADT (Admission/Transfer/Discharge) information, to LIS (Laboratory Information System) records, to databases of MALDI (Matrix-assisted laser desorption/ionization) spectra.']","[u""Master's in Data Science"", u""Bachelor's in Biochemistry""]","[u'UC Berkeley Berkeley, CA\nJanuary 2015 to August 2016', u'UC Davis Davis, CA\nSeptember 2008 to April 2012']","degree_1 : ""Masters in Data Science"", degree_2 :  ""Bachelors in Biochemistry"""
0,https://resumes.indeed.com/resume/a5c39db53053693c,"[u""Senior Data Scientist\nSOCIALFLOW - New York, NY\nJanuary 2015 to January 2017\nCo-led company's largest product initiative to develop a groundbreaking Artificial Intelligence (A.I.) system\nthat optimizes and measures performance of social content. Developed machine learning model and trained\nalgorithms, resulting in publishers earning instant insights across content categories and social platforms.\n\u2022 Spearheaded SocialFlow's customer insights project to analyze and quantify global reach of its customers'\ncontent. Oversaw all facets of project, from performing data analyses to creating dynamic visualizations and insights. Produced first quantifiable results, which elevated SocialFlow's leadership position in the industry.\n\u2022 Performed advanced data science analysis on Twitter conversations on a variety of topics including national\nelections, sports, innovation, among others, yielding a 105% improvement in interactions for the company.\n\u2022 Authored data science reports on these topics featured in articles on The Wall Street Journal, Business Insider\nand, most recently, on Buzzfeed.\n\u2022 Advanced sales and marketing efforts, and formulated brand positioning and messaging for ROAR, company's\nlargest annual conference with 250+ participants. Increased ROI measured in prospective customers by 25%.\n\u2022 Directed CRM sales initiative to organize, track, and manage leads. Drove high conversation rates via scoring\nand analyzing the marketing and sales funnel across different revenue teams."", u""Analyst, Strategy & Analysis Group\nDIGITASLBI - Boston, MA\nJanuary 2014 to January 2015\nOversaw data and analytics initiatives across social platforms for Digitas' largest client, a global financial\ninstitution. Created 100+ advertising programs and oversaw campaign execution on behalf of the client.\n\u2022 Identified and engaged with emerging technology platforms to enhance client's social marketing and brand\nperformance. Established six successful partnerships among third party vendors, Digitas, and its largest client.\n\u2022 Led a cross-border team for a prominent campaign for company's largest client during the World Economic\nForum. Leveraged advanced analytics to increase brand sentiment and engagement on social media by 210%.\n\u2022 Received Digitas' highest annual award, the Arch Street Award for work on the project.\n\u2022 Ideated advertising strategy for a client's largest campaign with a $20 million advertising budget. Helped lead\nthe team on campaign implementation on social channels and drove high conversion rates for client.\n\nADDITIONAL DATA""]","[u'', u'Master of Business Administration degree in Marketing and Pathway']","[u'KELLOGG SCHOOL OF MANAGEMENT Evanston, IL\nJanuary 2017 to Present', u'NORTHWESTERN UNIVERSITY\nJune 2019']","degree_1 : , degree_2 :  Master of Bsiness Administration degree in Marketing and Pathway"
0,https://resumes.indeed.com/resume/aa52241c0e46ac6c,"[u'Data Scientist\nVandelay Education - Austin, TX\nOctober 2016 to Present\nResponsible for data collection, analysis and communication for a 40 for data science\nperson office operating nationwide. Duties include collection and secure\nstorage of sensitive student data, defining key metrics, creation of\nautomated workflows based on said metrics, and creating and presenting\nvisualizations of company performance.', u'Self-Employed\nSelf-Employed - Berlin, DE\nMarch 2014 to April 2016\nDreams are stubborn things.']","[u'Bootcamp in data science and R programming', u'BA in English, Economics']","[u'Signal Data Science Berkeley, CA\nJanuary 2016', u'Tulane University New Orleans, LA\nJanuary 2006 to January 2010']","degree_1 : Bootcamp in data science and R programming, degree_2 :  BA in English, degree_3 :  Economics"
0,https://resumes.indeed.com/resume/5e367e3c9588cbd4,"[u'National Control System Data Editor\nBNSF Railway (via Thinkfind Corp.) - Fort Worth, TX\nFebruary 2015 to February 2017\nResponsible for railroad Engineering Asset and National Control System data accuracy and reconciliation within a variety of systems including asset databases and multiple GIS models. The GIS models are used to allow Positive Train Control on locomotive on-board safety systems. Evaluated existing asset data for accuracy against multiple sources including aerial imagery, field collected GPS data, engineering design plans, and LiDAR.', u""Project Scientist/GIS Analyst\nNewFields Engineering & Environmental, LLC - Lewisville, TX\nJune 2005 to February 2015\nTen years of experience in environmental consulting, project management, due diligence investigations including subsurface investigations and contaminated site delineation investigations, environmental regulatory compliance assessment, remediation oversight, GIS analysis, database design, and data/database management. Field experience also included safety management and behavior based safety programs.\n\nKey Projects & Achievements:\n\nProject management of large and small scale environmental remediation, site restoration, and construction projects including initial evaluation, design, proposal writing, cost estimation, scheduling, site mobilization, subcontractor management, analysis and reporting of sampling results, and project close-out.\n\nReview and development of a comprehensive plan for remedial measures performed at U.S. Air Force bases across the United States. Analysis included review of historic site investigation data, performance monitoring, performance of trend analysis, identification of existing and potential remedial actions based on the data, and completing a Decision Consequence Analysis to assess the expected cost for each alternative remedial action to identify the most cost effective strategy.\n\nPerformed Spill Prevention, Control, and Countermeasure regulatory compliance investigation and implementation for over 100 telecommunication facilities.\n\nPerformed site assessment and preparation of Stormwater Permits for sites in Texas following TCEQ permitting guidelines for industrial and cement manufacturing clients.\n\nPerformed soil vapor monitoring activities as a part of due diligence soil vapor intrusion screening for property transaction requirements at several facilities in Texas.\n\nAssisted cement manufacturing clients with Emergency Planning and Community Right-to-Know Act compliance including Texas Tier II reporting requirements, review of Safety Data Sheets for classification of stored chemicals as hazardous substances or extremely hazardous substances, and review and development of Hazard Communication Programs.\n\nCertified ExxonMobil Loss Prevention System (LPS) and Shell Oil Safe System of Work trainer. Conducted multiple 12-hour initial Loss Prevention System (LPS) training sessions and four-hour LPS refresher training sessions as an ExxonMobil certified LPS Trainer.\n\nCompleted two 32-hour LPS Trainer certification classes conducted by ExxonMobil's Safety, Health, and Environment initiative in Houston, Texas.\n\nManaged Health & Safety coordination for OSHA compliance within the office and for field projects. Duties included development of comprehensive Standard Operating Procedures, Health & Safety Plans for each project, Job Safety Analysis for each job activity, incident investigation and reporting, and tracking the training compliance of employees.\n\nResearched and developed environmental regulatory compliance manuals for applications such as underground storage tanks, aboveground storage tanks, hazardous substance reporting, job safety analysis, and site health & safety plans.\n\nPrepared regulatory technical reports for activities including groundwater monitoring, monitored natural attenuation, corrective measures studies, and subsurface investigations.\n\nCompleted investigations and reports for over 70 Phase I Environmental Site Assessments for retail, residential and light industrial properties in 12 states.\n\nDeveloped and implemented geodatabases for several environmental litigation projects which enabled detailed and precise analysis of environmental impacts.\n\nCreated analysis maps and performed GIS analysis for expert testimony reports for environmental litigation projects.\n\nCreated oil and natural gas well feature datasets including key field data, domains, and subtypes, and created feature classes using geolocation data.\n\nCreated analysis maps for expert testimony reports using ArcMap showing the georeferenced locations of sampling points, oil and natural gas wells, property parcels, roads, and wind speed and wind direction. The maps included titles, legends, compass rose, map descriptions, sources, coordinate system or projection used, and scales.""]","[u'Certificate of Completion in Geographic Information Science', u'Bachelor of Science in Environmental Science']","[u'TCCD Fort Worth, TX\nJanuary 2011 to January 2013', u'Texas A&M University Commerce, TX\nJanuary 2005']","degree_1 : Certificate of Completion in Geographic Information Science, degree_2 :  Bachelor of Science in Environmental Science"
0,https://resumes.indeed.com/resume/6237bb557d9ea2f7,"[u'Data Scientist\nAdobe, California\nJanuary 2017 to Present\nDescription: Adobe Systems Incorporated (/\u0259\u02c8do\u028abi\u02d0/ \u0259dohbee) is an American multinational computer software company. The company is headquartered in San Jose, California, United States. Adobe has historically focused upon the creation of multimedia and creativity software products, with a more recent foray towards rich Internet application software development.\n\nResponsibilities:\n\u2022 Extracted data from HDFS and prepared data for exploratory analysis using data munging\n\u2022 Built models using Statistical techniques like Bayesian HMM and Machine Learning classification models like XGBoost, SVM, and Random Forest.\n\u2022 Participated in all phases of data mining, data cleaning, data collection, developing models, validation, visualization and performed Gap analysis.\n\u2022 A highly immersive Data Science program involving Data Manipulation & Visualization, Web Scraping,\n\u2022 Machine Learning, Python programming, SQL, GIT, MongoDB, Hadoop.\n\u2022 Setup storage and data analysis tools in AWS cloud computing infrastructure.\n\u2022 Installed and used Caffe Deep Learning Framework.\n\u2022 Developing Models on scala and Spark for users, prediction models, sequential algorithms\n\u2022 Worked on different data formats such as JSON, XML and performed machine learning algorithms in Python.\n\u2022 Worked as Data Architects and IT Architects to understand the movement of data and its storage and ER Studio 9.7\n\u2022 Used pandas, numpy, seaborn, matplotlib, scikit-learn, scipy, NLTK in Python for developing various machine learning algorithms.\n\u2022 Performed the ongoing delivery, migrating client mini-data warehouses or functional data-marts from different environments to MS SQL server.\n\u2022 Developed SSIS packages to export data from Excel (Spreadsheets) to SQL Server, automated all the SSIS packages and monitored errors using SQL Job daily\n\u2022 Developed Hive queries and UDFS to analyze/transform the data in HDFS\n\u2022 Data Manipulation and Aggregation from different source using Nexus, Business Objects, Toad, Power BI and Smart View.\n\u2022 Experience in handling multiple relational databases like SQL Server, Oracle\n\u2022 Implemented Agile Methodology for building an internal application.\n\u2022 Focus on integration overlap and Informatica newer commitment to MDM with the acquisition of Identity Systems.\n\u2022 Good knowledge on Spark components like Spark SQL, MLib, Spark Streaming and GraphX,\n\u2022 Extensively worked on Spark Streaming and Apache Kafka to fetch live stream data.\n\u2022 Implemented novel algorithm for test and control team using Spark /Scala, Oozie, HDFS and Python on P&G Yarn cluster.\n\u2022 Skilled in using dplyr and pandas in R and Python for performing exploratory data analysis.\n\u2022 Developed scalable model using Spark (RDD, MLlib, Ml, Data frames) in Scala\n\u2022 Integrated Tesseract, ghost script with Spark to access data in hdfs and saving data in hive table\n\u2022 Programmed a utility in Python that used multiple packages (numpy, scipy, pandas)\n\u2022 Implemented Classification using supervised algorithms like Logistic Regression, Decision trees, Naive Bayes, KNN.\n\u2022 Performed Importing and exporting data into HDFS and Hive using Sqoop\n\u2022 As Architect delivered various complex OLAPdatabases/cubes, scorecards, dashboards and reports.\n\u2022 Updated Python scripts to match training data with our database stored in AWS Cloud Search, so that we would be able to assign each document a response label for further classification.\n\u2022 Used Teradata utilities such as Fast Export, MLOAD for handling various tasks data migration/ETL from OLTP Source Systems to OLAP Target Systems\n\u2022 Data transformation from various resources, data organization, features extraction from raw and stored.\n\u2022 Validated the machine learning classifiers using ROC Curves and Lift Charts.\n\nEnvironment: Python, BI, ER Studio 9.7, JSON XMA, XGBoost, HDFX, OLTP etc', u'Data Scientist\nBBH, Newyork\nNovember 2015 to December 2016\nDescription: We make ambitious ideas for ambitious clients. Using the power of creativity for outsized impact on culture and business growth, we work with some of the most innovative marketers in the world and are consistently honored with the highest strategic and creative accolades as a result.\n\nResponsibilities:\n\u2022 Responsible for analyzing large data sets to develop multiple custom models and algorithms to drive innovative business solutions.\n\u2022 Perform preliminary data analysis and handle anomalies such as missing, duplicates, outliers, and imputed irrelevant data.\n\u2022 Remove outliers using Proximity Distance and Density based techniques.\n\u2022 Involved in Analysis, Design and Implementation/translation of Business User requirements.\n\u2022 Experienced in using supervised, unsupervised and regression techniques in building models.\n\u2022 Performed Market Basket Analysis to identify the groups of assets moving together and recommended the client their risks\n\u2022 Experience in determine trends and significant data relationships using advanced Statistical Methods.\n\u2022 Implemented techniques like forward selection, backward elimination and step wise approach for selection of most significant independent variables.\n\u2022 Performed Feature selection and Feature extraction dimensionality reduction methods to figure out significant variables.\n\u2022 Used RMSE score, Confusion matrix, ROC, Cross validation and A/B testing to evaluate model performance in both simulated environment and real world.\n\u2022 Performed Exploratory Data Analysis using R. Also involved in generating various graphs and charts for analyzing the data using Python Libraries.\n\u2022 Involved in the execution of multiple business plans and projects Ensures business needs are being met Interpret data to identify trends to go across future data sets.\n\u2022 Developed interactive dashboards, created various Ad Hoc reports for users in Tableau by connecting various data sources.\n\nEnvironment: Python, SQL server, Hadoop, HDFS, HBase, MapReduce, Hive, Impala, Pig, Sqoop, Mahout, Spark MLlib, MongoDB, Tableau, ETL, Unix/Linux.', u""Data Scientist\nSafeway - Pleasanton, CA\nJanuary 2014 to October 2015\nDescription: United Market is independent organization owned by Albertsons companies. Albertson's Marketing team need visibility into Sales, Promotional and marketing data to me merged with rest of the Organization. EDW data from United markets was extracted, mapped and integrated with Albertson's data.\n\nResponsibilities:\n\u2022 Responsible for performing Machine-learning techniques regression/classification to predict the outcomes.\n\u2022 Responsible for design and development of advanced R/Python programs to prepare transform and harmonize data sets in preparation for modeling.\n\u2022 Created data visualization with ggplot2 in R to understand annual sales pattern.\n\u2022 Applied concepts of probability, distribution and statistical inference on given dataset to unearth interesting findings through use of comparison, T-test, F-test, R-squared, P-value etc.\n\n\u2022 Applied linear regression, multiple regression, ordinary least square method, mean-variance, theory of large numbers, logistic regression, dummy variable, residuals, Poisson distribution, Bayes, Naive Bayes, fitting function etc. to data with help of Scikit, Scipy, Numpy and Pandas module of Python.\n\u2022 Applied clustering algorithms i.e. Hierarchical, K-means with help of Scikit and Scipy anddeveloped visualizations and dashboards using ggplot2, Tableau.\n\u2022 Python and R scripting to wrangle and aggregate a war dataset consisting of 2+ million records and inconsistent formats. Functions used such as is.na, median and filters like which ().\n\u2022 Reset data frame index in R for misaligned data and generate qplot for data visualization.\n\u2022 Developed large data sets from structured and unstructured data. Perform data mining.\n\u2022 Partnered with modelers to develop data frame requirements for projects and converting vector data into matrices by using rbind () and nbind () functions.\n\u2022 Performed Ad-hoc reporting/customer profiling, segmentation using R/Python.\n\u2022 Tracked various campaigns, generating customer profiling analysis and data manipulation.\n\u2022 Provided R/SQL programming, with detailed direction, in the execution of data analysis that contributed to the final project deliverables. Responsible for data mining.\n\u2022 Analyzed large datasets to answer business questions by generating reports and outcome.\n\u2022 Worked in a team of programmers and data analysts to develop insightful deliverables that support data- driven marketing strategies.\n\u2022 Executed SQL queries from R/Python on complex table configurations.\n\u2022 Retrieving data from database through SQL as per business requirements.\n\u2022 Prepared data frames by using Gsub () function in R for identifying missing data that used for production data analysis.\n\u2022 Create, maintain, modify and optimize SQL Server databases and troubleshoot server problems.\n\u2022 Data collection, cleaned, filtered and transformed data in the specified format.\n\u2022 Prepared the workspace for Markdown.\n\u2022 Accomplished Data analysis, statistical analysis, generated reports, listings, and graphs.\n\u2022 Worked on R and Python to identify business performance via Classification, tree map, and regression models along with visualizing data for interactive understanding and decision-making.\n\u2022 Documented all programs and procedures to ensure an accurate historical record of work completed on an assigned project, which improved quality and efficiency of process by 15%.\n\u2022 Adhering to best practices for project support and documentation.\n\u2022 Managing the Reporting/Dash boarding for the Key metrics of the business.\n\nEnvironment: MS Excel, PL/SQL, R, Python, SAS, SQL, MS Word, MS Excel, Hadoop, and Tableau"", u""Data Analyst\nMedtronic Inc - Santa Ana, CA\nOctober 2012 to December 2013\nDescription: Medtronic Public Limited Company is a medical device company. Its headquarters are in Dublin, Ireland. Its operational headquarters are in Fridley, Minnesota. Medtronic is among the world's largest medical equipment development companies.\n\nResponsibilities:\n\u2022 Experience in working on Spark SQL queries, Data frames, import data from Data sources, perform transformations, perform read/write operations, save the results to output directory into HDFS.\n\u2022 Worked on installation of Kafka on Hadoop cluster and to use it for streaming & cleansing of raw data and have extracted useful information using Hive and stored the results in Hbase.\n\u2022 R and Python languages used to identify chemical performance via Classification, tree map and regression models along with visualizing data for interactive understanding and decision-making.\n\u2022 Identified outliers, anomalies and trends in any given data sets by using R.\n\u2022 Provided daily change management process support, ensuring that all changes to program baselines are properly documented and approved, maintained, managed and issue change schedules.\n\u2022 Developed, installed, maintained and monitored company databases in high performance/high availability environment with supported configuration, performance tuning to ensure optimal resource usage.\n\u2022 Documented all programs and procedures to ensure an accurate historical record of work completed on assigned project as well as to improve quality and efficacy\n\n\u2022 Produced quality reports for management for decision-making and Participated in all phases of research including data collection, data cleaning, data mining, developing models and visualizations.\n\u2022 Performed data imputation using Scikit-learn package in Python.\n\n\u2022 Performed data processing using Python libraries like Numpy and Pandas and Worked with data analysis usingggplot2 library in R to do data visualizations for better understanding of customers' behaviors.\n\nEnvironment: Machine learning, AWS, MS Azure, Cassandra, Spark, HDFS, Hive, Pig, Linux, Python (Scikit-Learn/Scipy/ Numpy/Pandas), R, SAS, SPSS, MySQL, Eclipse, PL/SQL, SQL connector, Tableau 14."", u""Data Analytics\nezDI LLC - Ahmedabad, Gujarat\nJanuary 2011 to September 2012\nDescription: ezDI helps hospitals reduce and maintain their DNFB at or below 4 days and eliminates their dependence on costly and outdated coding software by automating and optimizing the entire coding workflow - allowing CDI staff, coders and auditors to work together in perfect unison on a single platform.\n\nResponsibilities:\n\u2022 Worked with several R packages including dplyr, Spark, Causal Infer, spacetime.\n\u2022 Implemented end-to-end systems for Data Analytics, Data Automation and integrated with custom visualization tools using R and Hadoop.\n\u2022 Gathering all the data that is required from multiple data sources and creating datasets that will be used in analysis.\n\u2022 Performed Exploratory Data Analysis and Data Visualizations using R, and Tableau.\n\u2022 Worked with Data governance, Data quality, data lineage, Data architect to design various models and processes.\n\u2022 Independently coded new programs and designed Tables to load and test the program effectively for the given POC's using with Big Data/Hadoop.\n\u2022 Designed data models and data flow diagrams using Erwin and MS Visio.\n\u2022 Reviewed the logical model with Business users, ETL Team, DBA's and testing team to provide\n\n\u2022 Information about the data model and business requirements.\n\u2022 Extensively worked in Oracle SQL, PL/SQL, SQL*Loader, Query performance tuning, created DDL scripts, created database objects like Tables, Views Indexes, Synonyms and Sequences.\n\n\u2022 Strong programming skills using R, Elastic Search & Machine Learning Algorithms.\n\u2022 Designed and implemented machine learning algorithms to enhance existing data mining capabilities.\n\u2022 Used variety of analytical tools and techniques (regression, logistic, GLM, decision trees, machine learning etc.) to carry out analysis and derive conclusions.\n\u2022 Visualize, interpret, report findings and develop strategic uses of data.\n\nEnvironment: Unix, Python 3.5, MLLib, SAS, regression, logistic regression, Hadoop 2.7, NoSQL, Teradata, OLTP, random forest, OLAP, HDFS, ODS, NLTK, SVM, JSON, XML."", u""Python Developer\nEchidna Software Pvt. ltd - Bengaluru, Karnataka\nJune 2009 to December 2010\nDescription: Echidna began when a small group of commerce leaders knew there had to be a better way to do commerce. So, they branched off and created a new kind of agency -- one that combines amazing UX, enterprise-level technology implementation, and value-added marketing and analytics services.\n\nResponsibilities:\n\u2022 Involved in the design, development and testing phases of application using AGILE methodology.\n\u2022 Designed and maintained databases using Python and developed Python based API (RESTful Web Service) using Flask, SQL Alchemy and PostgreSQL.\n\u2022 Designed and developed the UI of the website using HTML, XHTML, AJAX, CSS and JavaScript.\n\u2022 Participated in requirement gathering and worked closely with the architect in designing and modeling.\n\u2022 Worked on Restful web services which enforced a stateless client server and support JSON few changes from SOAP to RESTFUL Technology Involved in detailed analysis based on the requirement documents.\n\u2022 Involved in writing SQL queries implementing functions, triggers, cursors, object types, sequences, indexes etc.\n\u2022 Created and managed all hosted or local repositories through Source Tree's simple interface of GIT client, collaborated with GIT command lines and Stash.\n\u2022 Responsible for setting up Python REST API framework and spring frame work using Django\n\u2022 Developed consumer-based features and applications using Python, Django, HTML, behavior Driven Development (BDD) and pair-based programming.\n\u2022 Designed and developed components using Python with Django framework. Implemented code in python to retrieve and manipulate data.\n\u2022 Involved in development of the enterprise social network application using Python, Twisted, and Cassandra.\n\u2022 Used Python and Django creating graphics, XML processing of documents, data exchange and business logic implementation between servers.\n\u2022 orked closely with back-end developer to find ways to push the limits of existing Web technology.\n\u2022 Designed and developed the UI for the website with HTML, XHTML, CSS, Java Script and AJAX\n\u2022 Used AJAX&JSON communication for accessing RESTfulweb services data payload.\n\u2022 Designed dynamic client-side JavaScript codes to build web forms and performed simulations for web application page.\n\u2022 Created and implemented SQL Queries, Stored procedures, Functions, Packages and Triggers in SQL Server.\n\u2022 Successfully implemented Auto Complete/Auto Suggest functionality using JQuery, Ajax, Web Service and JSON.\n\nEnvironment: Python 2.5, Java/J2EE, Django1.0, HTML, CSS Linux, Shell Scripting, Java Script, Ajax, JQuery, JSON, XML, PostgreSQL, Jenkins, ANT, Maven, Subversion, Python""]","[u""Bachelor's""]",[u''],"degree_1 : ""Bachelors"""
0,https://resumes.indeed.com/resume/6611db0b87c9691d,"[u'Business Intelligence Analyst\nT-Mobile - Bellevue, WA\nJanuary 2017 to January 2018\n\u2022 Worked in a Team of 7 at Slalom Consulting where our objective is to retire SAP Business Objects for T-Mobile Engineering Team and move all their reports to Power BI which will save them 6 Million USD on licensing\n\u2022 Built and supported end users in the Site Development team at T-Mobile with data models and reports in Power BI to allow quick analysis of frequently asked questions\n\u2022 Shaped, mined and modeled data using Power Query and Power Pivot\n\u2022 Developed reports and dashboards showcasing KPIs to support strategic recommendations\n\u2022 Customized the Power BI reports to meet different client use cases\n\u2022 Wrote advanced functions in DAX for different calculated columns and measures\n\u2022 Built and analyzed surveys using Typeform to improve our deliverables to T-Mobile Engineering Team\n\u2022 Built a Power BI report in hours to help T-Mobile quickly respond and repair affected cell sites when Hurricane Harvey caused massive damage\n\u2022 Created tool, database and dashboard training documents\n\u2022 Wrote, updated, and ran SQL queries to answer business questions\n\u2022 Wrote guides and Delivering demos of software tools for T-Mobile employees across North America\n\u2022 Documented processes and system design\n\u2022 Developed User Acceptance Testing(UAT) criteria\n\u2022 Translated business requirements into technical development requirements', u'Proctor and Trainer\nMicrosoft - Redmond, WA\nJanuary 2017 to January 2018\nProctoring Power BI trainings and provided 1:1 tutoring/instructions for MSFT employees as part of ""BI@Microsoft"" program', u'Business Intelligence Analyst\nMicrosoft - Redmond, WA\nNovember 2015 to December 2016\n\u2022 Built and maintained visual reports using Power BI for Microsoft Office Demand Insights team.\n\u2022 Performed ad hoc analysis and supported stakeholder requests for data\n\u2022 Built and maintained over 140 visual reports using Power BI and PowerPivot for Microsoft mid-year and quarterly reviews\n\u2022 Implemented row level security for Power BI reports for sales routed through Dublin\n\u2022 Analyzed new data sources across multiple databases and servers\n\u2022 Worked closely with data engineers to operationalize insights as needed\n\u2022 Collected and analyzed data from public information, field reports and purchased source to assist stakeholders with valuable information\n\u2022 Created visual scripts using R statistical software as per client requirements\n\u2022 Interviewed key stakeholders and members of the Sales and finance department to decide the KPIs and Key Metrics for the reports\n\u2022 Wrote advanced SQL scripts to bring in the relevant information to build the reports\n\u2022 Built OLAP cubes using SSAS to improve the performance of the reports and to get in real time data\n\u2022 Used advanced Power BI features such as scheduled refreshes, email subscriptions etc', u'Data Scientist\nThomson Reuters - Boston, MA\nJanuary 2015 to October 2015\n\u2022 Extracted information, patterns of various kinds from a very large dataset about worldwide financial crime using R\n\u2022 Manipulated and geocoded the data to produce data visualizations that would help customers understand the scope and content of the data\n\u2022 Worked closely with the experts from the business unit who owned the data to understand the nuances of the data that was to be extracted and to make sure that the data was aggregated accurately\n\u2022 Performed text mining, ad hoc analysis and assisted in building reports to support the Thomson-Reuters weekly blog\n\u2022 Created new database objects like Sequences, Procedures, Functions, Packages, Triggers, Indexes and Views using T-SQL in Development and Production environment for SQL Server.\n\u2022 Generated DDL scripts for database schema and created all other database objects.\n\u2022 Involved in merging existing databases and designed new data models to meet the requirements.\n\u2022 Create joins and sub-queries for complex queries involving multiple tables.\n\u2022 Designed and developed SSIS (ETL) packages to validate, extract, transform and load data from OLTP system to the Data warehouse and Report-Data mart.\n\u2022 Built regression model to predict, validate different output variables and fortified the models by designing a completely randomized 2^4 full factorial experiment\n\u2022 Implemented the significant interactions between the critical factors in the model and calculated ANOVA interactions, main effects of the predictor variables and optimized the response variables by 15%', u""Data Migration Intern\nCONNX Solutions - Redmond, WA\nSeptember 2014 to January 2015\n\u2022 Designed, created data models and data flows for the data synchronization of corporate relational, cloud, 'Big Data' and legacy database systems. Constructed instructional videos for use on corporate website.\n\u2022 Collaborated with a team of three interns, sales, engineering and IT. Worked with executive management and IT to understand specifications, logical processes and architecture.\n\u2022 Synchronized customer list of 100,000+ records from flat file to multiple platforms, including SQL Server 2012 and Microsoft Azure\n\u2022 Implemented Error Handling, Load Statistics, Slowly Changing Dimensions and Currency Conversion for Data Integration process using Talend 6.1 and SSIS\n\u2022 Reduced the data integration duration by 80% using bulk output connections\n\u2022 Performed data discovery and visualization by creating dashboards with KPIs and reports to analyze trends in the resultant warehouse using QlikView, QlikSense, Tableau and PowerPivot\n\u2022 Presented an executive overview during bi-weekly status meetings during the development phase.""]",[u'Bachelor of Technology in Industrial/Mechanical Engineering'],"[u'Vellore Institute of Technology Vellore, Tamil Nadu\nJune 2010 to May 2014']",degree_1 : Bachelor of Technology in Indstrial/Mechanical Engineering
0,https://resumes.indeed.com/resume/32cf523eec79d9c3,"[u'Software design engineer, machine learning data scientist\nTATA CONSULTANCY SERVICE - Redmond, WA\nApril 2015 to Present\nDuties and Technologies used:\n\u2022 Azure Subscription Automation VM CPU usage and auto scaling machine learning modeling.\nUsing algorisms:\nARIMA (auto regression integrated moving average),\nNNet (feed-forward neural networks with a single hidden layer, and for multinomial log-linear models)\nMulti-Classes Neural Networks classifications\nWelch two samples statistics test for model evaluations\nTime Series Anomaly Detection\nK-Means clustering\nSigmoid Activation\nPlatform:\nAzure Machine Learning Studio with R Scripts, SQLite, Storage Blob\nData analysis exploration visualization:\nMS Power BI\n\u2022 Cosmos Scope big data extracting and parsing in huge telemetry JSON format raw data stream to build\nsearch traffic & revenue analysis funnels. Providing simple and clear data visualization for search team. Post - sale monetization customer behave analysis based on customer purchase daily cosmos big data.', u'Software design engineer III\nTRYGSTAD TECHNICAL SERVICE - Bellevue, WA\nMay 2008 to August 2014\nDuties\n\u2022 Continue Integration Service Central Database & Reporting Site design / development\n\u2022 Continue Integration Service Automation System development and data base performance improvement.', u'Data engineer\nSHANGHAI HUAWEI MICROSYSTEM CO., LTD - SHANGHAI, CN\nJanuary 1999 to January 2006\nDuties / Technologies used:\nProject / product - civic information management application system\n\u2022 Leading an agile development team working closely with customers, area experts and data scientists to provide supervision system analysis, total solutions, data standards, distribution architectures for Shanghai\nMunicipal & Civic Archiving Information Center.\nTechnologies used:\n\u2022 Front: HTML + CSS + ASP / JSP + OWC web report components\n\u2022 Middle part: Java class EJB, COM wrappers\n\u2022 Back end: T-SQL/PL SQL + SQL Server DTS, OLAP\n\u2022 Animation: Adobe Flash Scenes & action scripts']",[u'in Computer Software Engineering'],"[u'Shanghai Computer Manufacturing College Shanghai, CN']",degree_1 : in Compter Software Engineering
0,https://resumes.indeed.com/resume/df778f9f538706ff,"[u'Data Scientist\nApplied Materials, Inc - Santa Clara, CA\nJuly 2017 to Present\nProject: HR Analytics\nDescription: Client had to understand if copy of company confidential data is leading to attrition of their employees. Project was about analyzing big data which existed in Hadoop, get data insights and build attrition model which would help in determining non-compliance activity leading to employee attrition.\nResponsibilities:\n\u2022 Identified patterns and major factors related to non-compliance leading to employee attrition.\n\u2022 Captured non-compliance activity of employees (e.g. copy confidential data, detect anomalies etc.) and see patterns/trends in relation to employee attrition.\n\u2022 Performed analysis & developed attrition model for prediction based on complete data in Impala. Data preparation steps followed in Impala and most tuned model implemented for prediction on newer data.\n\u2022 Performed POC on installation of SparkR on Spark 1.6.0.\n\u2022 Algorithms: Classification Algorithms (Bayesian Belief Networks, Decision Trees, Logistic Regression, Random Forest, XGBoost and Support Vector Machines)\n\nProject: Cost minimization for parts cleaning\nDescription: Applied Materials supplies equipment to enable manufacture of semiconductor chips for electronics, flat panel displays for computers, smartphones and televisions & solar products. Each & every part supplied is cleaned (application of alloy coating of metals) & tested. Project was understanding which parts can be sampled to reduce testing costs and understand irregularities in the process.\nResponsibilities:\n\u2022 Client had to minimize part cleaning costs (testing to see if alloy coating of metals applied over parts are within specified threshold limits).\n\u2022 Identify parts which frequently gets cleaned in 1st iteration; those parts to be sampled to reduce expenses of testing each & every part.\n\u2022 Identify patterns for which set of metals frequently lead to multiple iterations leading to increased cleaning costs. Provided high end visuals to engineering team so they could understand flaws in the process and take corrective action.\n\u2022 Algorithms: Multivariate analysis\n\nProject: Minimize ticket resolution time\nDescription: Automation of ticket assignment to right queue using Machine\nLearning based solution.\n\nResponsibilities:\n\u2022 Client had to minimize ticket resolution time by allocation to right department, by avoiding hopping.\n\u2022 Text description provided by user analyzed preparing corpus using techniques as Latent Dirichlet Allocation.\n\u2022 Probabilities for each of the topics used in the model to route ticket to the most probable department.\n\u2022 Algorithms: Text Mining, Latent Dirichlet Allocation (LDA) & Classification techniques.\nEnvironment: R language, R Shiny, Spark 1.6.0 (Python), SparkR, Hadoop, Hive, Impala & Tableau.', u'Big Data Analyst\nNavistar Inc\nNovember 2013 to June 2017\nProject: Gamification Design\n\nDescription: Navistar is a truck company which perform truck sales and provide driver performance feedback. Project is about integrating data which spits from devices installed in truck with driver, vehicle level details and performing calculations which would help in rewarding drivers based on their record along certain parameters.\nResponsibility:\n\u2022 Performed POC on data sourced in ADLS can be processed in HDInsight using pipeline with Hive activity.\n\u2022 Used Hive for creation of ORC formatted tables and used ADF for data orchestration to Azure database. Data copied from ADLS to Azure SQL database using ADF pipelines invoked using PowerShell scripting.\n\u2022 Developed SQOOP queries to export HIVE tables to Azure SQL database scheduled using CRON tab.\n\u2022 Analyzed gamification design and developed calculations in Hive.\n\u2022 Metrics developed & dashboard built using Power BI, which would reward drivers for safe driving and keep checks on health of the vehicle.\nEnvironment: HDInsight, Azure database, Azure Data Lake, Azure Data Factory, Hive, SQOOP, PowerShell & Power BI.\n\nClient: Microsoft Corporation\nRole: Data Scientist\nProject: Product Recommendation Engine\n\nDescription: GMO Predictive Analytics Platform is an enterprise scalable data mining and reporting platform built for developing and publishing predictive models at scale. These models span sales and marketing lifecycle (from acquisition to churn) and cover all global subsidiaries.\n\nResponsibilities:\n\u2022 Performed data exploration and analysis of Sales & Marketing data using R.\n\u2022 Cross-sell/Up-sell products (Microsoft range of products) to customers based on patterns identified for similar set of customers, thereby improving overall revenue.\n\u2022 Developed ADF pipelines to move data from on - premise source systems to COSMOS, from COSMOS (with data transformation) to Azure Warehouse (staging), from Azure warehouse to Azure ML (for scoring) and appending scores back to the data in Azure warehouse.\n\u2022 Involved in automating end-2-end model refresh process using COSMOS, Azure Data Factory (ADF) pipelines, Azure ML and PowerShell scripting.\n\u2022 Developed dashboards using Power BI as per requirements.\n\u2022 Algorithms: Collaborative filtering & Market Basket Analysis.\n\nData Scientist | Opportunity Wins Classification\n\u2022 Predict an opportunity will be win/loss based on set of parameters.\n\u2022 Helped business identify opportunities which have better chances of win and accordingly identify correct resources & take corrective steps. Developed visualizations which would derive insights into the data.\n\u2022 Algorithms: Decision Trees, Logistic Regression, Random Forest, Time series forecasting.\nEnvironment: Azure Stack (HDInsight, Data Lake, ADF, AML), R scripting, PowerShell scripting & PowerBI.\n\nACADEMIC ANALYTICS EXPERIENCE\n\u2022 Delivered trainings to a batch on Statistics and Machine Learning algorithms using R.\n\u2022 Analyzed soap brands on basis of ratings for unlabeled features by grouping into appropriate dimensions using Correspondence analysis. Developed bi-plots of unlabeled features as well as data points on different dimensions and derived insights on soap brand perceptions as defined by the customer.\n\u2022 Categorized stores on basis of store features using cluster analysis. Used the analysis for assortment planning at store level, improved performance of existing stores (with comparatively less sales) and provided suitable sites for opening of new stores.\n\u2022 Developed classification model using techniques as Decision trees, Logistic Regression, Random Forest, GBM and SVM to predict credit defaulting customers and income classification based on certain features.\n\u2022 Developed linear regression models for predicting price of used cars & rent of house in a locality.\n\u2022 Experience performing different statistical tests as Hypothesis testing of means, ANOVA for comparison of means of two groups, Chi-Square for measures of association and goodness of fit.\n\u2022 Engaged in developing LDA model that will classify whether Churn has taken place or not based on telecommunication data using R and test model performance using measures as Confusion matrix.\n\u2022 Created models using techniques as PCA & FA to reduce number of dimensions for better prediction.\n\u2022 Formulated problems for business cases as optimizing delivery routes, minimize cost of operations & maximize profits using optimization techniques of linear programming, simplex, northwest corner & least cost methods.\n\nMSBI EXPERIENCE\nClient: Microsoft Corporation\nRole: Lead BI Developer\nProject: Enterprise Services\n\u2022 Worked in a multi-team environment, coordinating activities with onsite team and delegating duties to team.\n\u2022 Incorporated self-service BI model in cube with development of complex calculated members, implementing PII (Personnel Information Identifier) security and improved processing & query performance of the cube.\n\u2022 Performing Database and ETL development per new requirements as well as actively involved in improving overall system performance by optimizing slow running/resource intensive queries.\n\u2022 Developed complex SSIS packages as to automate table usage for dimension processing, so as to create multiple OLAP databases to maintain version history of security.\n\u2022 Migrated fully functional OLAP cube along with its security to a Tabular model using SSAS tabular.\nEnvironment: SQL Server 2016/2014/2012 (SSIS, SSAS, SSRS), PowerBI & Excel 2016/2013.', u'Sr. BI Developer\nCITRIX - Fort Lauderdale, FL\nMay 2012 to August 2013\nProject: Sales & Channel Analytics\n\nResponsibilities:\n\u2022 Developed complex SQL queries as to retrieve dimension counts from Olapquerylog to retrieve dimension usage and queries to retrieve measure count usage from profiler logs.\n\u2022 Performed cube development tasks as implementing Time Intelligence using shell dimension, Security and use of Many-2-Many dimensions approach to resolve Many-2-Many discrepancies.\n\u2022 Developing complex ETL packages using Integration Services (SSIS) as automated partition processing of measure groups in cubes, sending automated emails to business users for job failures and packages involving slowly changing dimensions (SCDs) using SSIS expressions and C-Sharp (C#) using scripting component.\nEnvironment: SQL Server 2012 (SSAS, SSIS & SSRS), SQL Sentry and Excel 2013/2010.', u'BI Analyst\nMicrosoft Corporation - Redmond, WA\nJune 2010 to May 2012\nProject: MS Sales\n\nResponsibilities:\n\u2022 Developed audits to ensure health of system is satisfactory and minimize any downtimes.\n\u2022 Worked on a migration project in which migrated all of the SQL agent jobs, stored procedures and tables on one environment onto a new environment. Experience migrating CLR stored procedures and .Net assemblies.\n\u2022 Designed OLAP cube and measures/dimensions in defining KPIs on Performance Point Server.\nEnvironment: SQL Server 2008 R2 (SSAS, SSRS), Excel 2010, Excel Macros, VBA, Batch Files, PPS 2007.\n\nClient: Microsoft Corporation\nRole: BI Analyst\nProject: EDW Scorecard Platform\n\nResponsibilities:\n\u2022 Conducted meetings with business teams to on-board new scorecards on existing platform.\n\u2022 Designed complex calculated members using MDX for Excel scorecards to meet business requirements of time/geography rollup/non-rollups different per metric.\n\u2022 Worked on cube write back feature, where changes were configured directly in cube using excel macros.\nEnvironment: SQL Server 2008 R2 (SSAS & SSIS), SharePoint 2010, Excel 2010, Excel Macros, VBA, Java Scripting.', u'BI Developer\nAT&T - San Ramon, CA\nAugust 2008 to May 2010\nProject: U-verse Reporting\n\nResponsibilities:\n\u2022 Automated reports using Visual Basic for Applications (VBA) to zip, unzip files, automatically upload to SharePoint, send emails to users & download reports from SharePoint.\n\u2022 Worked on migration project to move database objects from multiple environments into single environment.\n\u2022 Interacted with business users to understand requirements and accordingly designed reports.\nEnvironment: SQL Server 2005, Teradata v12.0, Oracle 9i, VBA & Batch Files.', u'BI Developer\nSupervalu Inc - Salt Lake City, UT\nApril 2007 to July 2008\nProject: Retail Reporting\n\nResponsibilities:\n\u2022 Performed ETL development using Informatica and developed shell scripts in UNIX.\n\u2022 Developed adhoc reports using SQL for data retrieval from Teradata.\n\u2022 Fine-tuned dimensions by choosing right key attributes, avoiding deployment of unnecessary attributes, defining hierarchies for proper rollup.\nEnvironment: SQL Server 2005 (SSIS, SSAS, SSRS), Teradata v12, Informatica 7.1.3 & Unix.\n\nClient: Harris Bank\nRole: BI Developer\nProject: Finance Reporting\n\nResponsibilities:\n\u2022 Developed SSRS reports using cascaded parameters, sub-reports, drill down & drill through reports.\n\u2022 Performed tuning of database for slow running SQL queries.\n\u2022 Experience creating different chart types, parameterized reports with use of expressions & global variables.\nEnvironment: SQL Server 2005 (SSRS), Excel 2007, Oracle 9i, TOAD 6.0']","[u'in Business Analytics', u""Master's in engineering"", u""Bachelor's in engineering""]","[u'Great Lakes University\nOctober 2016', u'SYRACUSE University\nDecember 2006', u'PUNE University Pune, Maharashtra\nAugust 2003']","degree_1 : in Bsiness Analytics, degree_2 :  ""Masters in engineering"", degree_3 :  ""Bachelors in engineering"""
0,https://resumes.indeed.com/resume/72711299fbc3f0b3,"[u'Data Scientist\nUnisys - San Francisco Bay Area, CA\nJune 2017 to Present\n\u2022 Lead analytics consultant responsible for design and implementing solutions in variety of projects focused on risk management, digital marketing, consumer segmentation, resource management, fraud detection, and geospatial and time series applications.\n\u2022 Predictive analytics production using AWS machine learning API, PHP, Python, MySQL and JavaScript.\n\u2022 Extracting, cleaning and manipulating data in Linux environment using scripting and database query languages (Python, Perl, SQL, Hive, Spark, Oracle and Teradata)\n\u2022 Designed and implemented BI solutions and automated dashboards for real-time reporting of KPIs using Flask, AWS Quicksight and Power BI.', u'Data Scientist\nSelling Source - Las Vegas, NV\nOctober 2015 to June 2017\n\u2022 Backend design and implementation of data pipelines and machine learning algorithms along with providing data driven insights for content marketing, ping tree segmentation, fraud detection and evaluating credit worthiness of loan applications.\n\u2022 Applied data mining techniques such as dimensionality reduction and correlation analysis to discover underlying trends and providing quantitative basis to address business decisions.\n\u2022 Managing architecture, accessibility and security of real-time accumulated structured and unstructured data. Automating summarization reports, visualizations, evaluation and accessibility to complex data indexes.\n\u2022 Designed, implemented and interpreted results of hypothesis testing (AB testing and Bayesian Bandits).\n\u2022 Production using machine learning APIs, PHP, Python, SAS, MySQL and JavaScript.', u'Research Assistant\nUniversity of California - Irvine, CA\nJune 2009 to June 2014\nStudied stochastic transport in fracture networks. Proposed and verified mathematical schema for large- scale anomalous transport. Developed distributed computing models using MPI, Fortran, Matlab, R, Python\nand C++ to simulate flow and transport in fractured rocks and porous media.\n\u2022 Performed Monte Carlo analysis on stochastic fracture networks, revealing effects of micro-scale network\nproperties on large-scale transport. Simulated streamline-scale flow in networks of fractures using Finite\nElement Analysis, and reported algorithms with enhanced computational efficiency as alternative to direct\nsolution of convective-diffusive PDEs.']","[u'Ph.D. in Computational Geoscience', u'M.Sc. in Civil Engineering', u'B.Sc. in Civil Engineering']","[u'University of California Irvine Irvine, CA\nJanuary 2014', u'Temple University Philadelphia, PA\nJanuary 2009', u'Sharif University of Technology Tehran, IR\nJanuary 2007']","degree_1 : Ph.D. in Comptational Geoscience, degree_2 :  M.Sc. in Civil Engineering, degree_3 :  B.Sc. in Civil Engineering"
0,https://resumes.indeed.com/resume/0bf94bb5889b1366,"[u""Business Intelligence Engineer Intern\nAmazon, WA\nAugust 2017 to November 2017\nSupply Chain Optimization Analysis\n\u2022 Analyzed the adaptive transportation system to find the causes for transportation capacity breaches to help lessen its negative\nimpact on business performance\n\u2022 Developed live tools in Tableau to track and analyze transportation resources' capacity consumption that helped manage the resources efficiently during Prime Day and Holiday Season for entire NA region"", u""Predictive Analytics Intern\nCNA, IL\nMay 2017 to August 2017\nCustomer Growth Prediction\n\u2022 Predicted the agencies' growth for the next year with 83% accuracy by combining two different GLMs; one for retention and one for new business generation\n\u2022 Large and dirty data with nearly 800 variables has been cleaned and feature extracted using Correlation, AIC, lift charts and domain knowledge"", u""Senior Data Analyst\nLatentView | Client: PayPal\nApril 2014 to July 2016\nConsumer Marketing Analytics, Digital Marketing Analytics\n\u2022 Estimated sample sizes and designed strategies for email and banner marketing campaigns using statistical sampling techniques in SAS which generated an incremental purchase volume of ~$30M\n\u2022 Developed key campaign metrics and performed post-campaign lift analysis through A/B testing methodology using statistical\nsignificance test\n\u2022 Built campaign reports to track key campaign metrics that were consumed on weekly basis by the head of PayPal's marketing\nanalytics for Canada in making key business decisions\n\u2022 Maximized campaign's revenue by implementing closed-loop marketing techniques through performing consumer profiling on post campaign effectiveness results\nWeb Analytics\n\u2022 Built a Conversion-funnel report to perform web analysis of signup flow pages and identified areas of improvement which led to a significant increase in customer acquisition rates\n\u2022 Performed A/B testing on a web experiment between a static home page and video background and validated the latter to have\n2% higher signup rate\n\u2022 Designed and built reports, and automated processes like post-campaign analysis, campaign monitoring, website event tracking and keywords measurement in paid search(SEM) which decreased human effort of 10hrs per week"", u""Decision Scientist\nMuSigma - IN\nMay 2013 to April 2014\n\u2022 Segmented the B2B Customers of Dell based on their buying cycle, analyzed these segments to find the reasons behind attrition and provided actionable business recommendations\n\u2022 Identified the physicians who are most likely to switch from AbbVie's drug with 82% accuracy by analysing brand switching\npatterns of physicians\n\u2022 Generated actionable insights from weekly and YOY trends of client's and its competitor drugs at various geographical levels and at physician level to pinpoint the areas/physicians to be targeted by medical representatives""]","[u'Master of Science in Data Science', u'Bachelor of Technology in Chemical Engineering']","[u'Indiana University Bloomington, IN\nMay 2018', u'National Institute of Technology Warangal, Telangana\nMay 2013']","degree_1 : Master of Science in Data Science, degree_2 :  Bachelor of Technology in Chemical Engineering"
0,https://resumes.indeed.com/resume/a90cf398cd85e064,"[u'Data Scientist\nNew York Data Science Academy - New York, NY\nApril 2016 to July 2016\nAttended a 12-week immersive program that covers topics such as data visualization, data analysis, web scraping, and machine learning. Completed coursework on Simple and Multiple Linear Regression, Generalized Linear Models, Ridge and Lasso Regression, Principal Component Analysis, Clustering, Random Forests, Support Vector Machines, Neural Networks, Association Rules, Na\xefve Bayes.\n\u2022 Velocity of Citi Bike - Engineered a velocity feature on the Citi Bike data consisting of over 500,000 observations to analyze where in NYC people were traveling fast or slow using R.\n\u2022 NBA Stats 1947-2015 - Used Shiny to make an interactive dashboard to visualize the 1947-2015 NBA stats and the current held records\n\u2022 What can r/Technology tell us? - Webscraped r/technology using Scrapy and Python to see what tech sites are referenced the most and what tech topics received the most upvotes or comments. Companies can focus their advertisement on the top most frequent tech sites mentioned on r/Technology.\n\u2022 Higgs Boson Kaggle Competition - Collaborated with a team of 4 to apply machine learning algorithms such as random forest, gbm, and neural networks on the Higgs Boson Kaggle training set consisting of 220,000 observations and 30 features and testing it on over 500,000 observations in R.\n\u2022 Predicting Number of Members for Meetup - Retrieved Meetup data using APIs. Engineered features to help predict the number of members for each Meetup group. To generate the most members, the top three features one need to focus on is location, rating, and length of description.', u""Sales Assistant/Data Analyst - Institutional Sales\nThieme Medical Publishers - New York, NY\nJanuary 2014 to October 2015\nCollaborated with a team of 4 in New York to penetrate the western hemisphere with the Thieme's entire e-product portfolio and generated more than one-forth of the institutional sales revenue globally in 2014. Managed the team's database\n\u2022 Analyzed customer data and visualized sales trends using Microsoft Excel\n\u2022 Administered Thieme's entire medical e-product portfolio for the institutional market in the western hemisphere\n\u2022 Managed the sales flow from the initial sale and licensing to chasing late payments\n\u2022 Redefined quarterly reports and statistical reports using SalesForce and Excel\n\u2022 Learned and implemented both SalesForce and Mission<one> to generate and organize leads\n\u2022 Collaborated with accounting, marketing, editorial and IT departments to successfully manage accounts\n\u2022 Presented possible campaigns and learned about the company's global performance in yearly meetings in Germany""]",[u'Bachelor of Science in Physics'],"[u""St. John's University Queens, NY\nMay 2013""]",degree_1 : Bachelor of Science in Physics
0,https://resumes.indeed.com/resume/9d711cc1ea4a8591,"[u'Data Scientist\nKensho Technologies Inc - Cambridge, MA\nApril 2016 to Present\nProject: Credit Card Fraud\n\nResponsibilities:\n\u2022 Communicated and coordinated with other departments to collect business requirement\n\u2022 Worked on miss value imputation, outliers identification with statistical methodologies using Pandas, Numpy\n\u2022 Participated in features engineering such as feature creating, feature scaling and One-Hot encoding with Scikit-learn\n\u2022 Tackled highly imbalanced Fraud dataset using undersampling with ensemble methods, oversampling and cost sensitive algorithms\n\u2022 Improved fraud prediction performance by using random forest and gradient boosting for feature selection with Python Scikit-learn\n\u2022 Implemented machine learning model (logistic regression, XGboost) with Python Scikit- learn\n\u2022 Validated and selected models using k-fold cross validation, confusion matrices and worked on optimizing models for high recall rate\n\u2022 Implemented Ensemble Models with majority votes to enhance the efficiency and performance\n\u2022 Designed rich data visualizations with Tableau 9.4\n\nEnvironment: Python (scikit-learn, pandas, Numpy), Machine Learning (logistic regression, XGboost), Gradient Descent algorithm, Bayesian optimization, Tableau', u'Data Engineer\nNorth Shore Medical Center - Salem, MA\nSeptember 2015 to April 2016\nResponsibilities:\n\u2022 Implement advanced ETL processes to load and integrate large datasets (SSIS, MSSQL server, R)\n\u2022 Cleaned and manipulated complex healthcare datasets in order to create the data foundation for further analytics and the development of key insights (MSSQL server, R, Tableau, Excel)\n\u2022 Modelled the data relationship to provide assistance for business decision making (R, Tableau)\n\u2022 Reported and dash boarded analytical results to client (R, Tableau, Excel)\n\nEnvironment: SSIS, MS-SQL Server, R, Tableau, MS-Excel', u'Data Engineer\nInternastic Technologies Inc - Mumbai, Maharashtra\nJanuary 2012 to August 2015\nProject 1: Overall Satisfaction Analysis\n\u2022 Identified what factors could influence the overall satisfaction of consumers. Range (1-5)\n\u2022 considered the SMG (Service Management Group) database survey results in analyzing the impact on overall customer satisfaction\n\u2022 Used Ordinal logistic regression methodology in explaining the importance of features\n\u2022 The analysis involved predicting the overall satisfaction - ordinal rating, by analyzing the impact of each independent factors in explaining the output\n\u2022 Packages used: MASS package, polr function for Ordinal logistics regression model\nEnvironment: R Studio, SQL Server, Dplyr, Tidyr, ggplot2, Tableau, MASS package - polr function\nProject 2: Annual Marketing Budget Allocation\n\u2022 Responsible for Data collection and data preparation and normalizing the data\n\u2022 Used SQL, ETL tool, R Studio, and Python for data preparation\n\u2022 Supported data consultants in the data modeling phase\n\u2022 Used Constraint optimization algorithms (USED EXCEL) to optimize the marketing budget\n\u2022 Used Time Series Models - decomposition of time series, trend, and seasonality detection, forecasting and exponential smoothing in predicting the market share and brand share to allocate the Marketing budget\nEnvironment: R (dplyr, Tidyr, ggplot2, reshape2, fpp, forecast packages), Python (pandas, NumPy, scikit-learn),SQL Server, Excel\nProject 3: Sentiment Analysis\n\u2022 Used SMG (Service Management Group) database for most of the analysis used SQL for data preparation\n\u2022 Sentiment analysis using public comments from SMG database (POC Stage- Used Python (NLTK) for analysis)\n\u2022 Monthly Media Activity intelligence & Competitor Pricing reports for top level management\n\u2022 Overall satisfaction analysis using descriptive statistics\nEnvironment: SQL, Python (NLTK), Excel']","[u""Master's""]",[u''],"degree_1 : ""Masters"""
0,https://resumes.indeed.com/resume/a95bb1f6f834ae69,"[u'Data Scientist\nCapital One - McLean, VA\nAugust 2017 to February 2018\nForecasting Stock Prices - Time Series Analysis\nForecasting of Apple, Google and other fortune 500 companies stock price using information from Yahoo Finance and Google Trend\n\u2022 Performed ARIMA time series analysis on the weekly close prices of stocks, transformed the raw data to stabilize the variance and plotted autocorrelation and partial autocorrelation functions\n\u2022 Quantified positive, negative and neutral news related to the company and used it for prediction calculations\n\u2022 Designed dashboards with Tableau 10 and provided complex reports, including summaries, charts, and graphs to interpret findings to team and stakeholders\n\u2022 Performed mathematical manipulations to improve model accuracy, allowing better prediction of stock prices\n\u2022 Designing and developing various machine learning frameworks using R and python.\nEnvironment: Excel 2014, R 3.X, Python 2.7, Hadoop 2.X, MS SQL Server 2008, Tableau 9.2', u'H3 Technologies - Florence, KY\nOctober 2016 to February 2018', u'Data Scientist\nDorel Sports - Madison, WI\nMarch 2017 to August 2017\nCustomer Segmentation for Retail Marketing for Cannondale Bicyles\nK-means clustering to segment customers into distinct groups based on their purchasing habits.\n\u2022 Merged customers, products and orders data frames from Cannondale Bicycle company\n\u2022 Utilized SQL to extract data from SQL Server 11.0, and MongoDB, to prepare data for analysis.\n\u2022 Conducted analysis on assessing customer consuming behaviors and discover value of customers with RMF analysis; applied customer segmentation with clustering algorithms such as K-Means Clustering and Hierarchical Clustering.\n\u2022 Developed personalized products recommendation with Machine Learning algorithms, including Collaborative filtering and Gradient Boosting Tree, to better meet the needs of existing customers and acquire new customers.\n\u2022 Used R packages such as tidyr, dplyr, broom, ggplot2, tibble etc. for data cleaning and feature engineering.\n\u2022 Determined customer satisfaction and helped enhance customer experience using NLP.\n\u2022 Identified customers in each segment by determining the preferences of the customer segments\n\u2022 Provided bi-weekly reviews to director and VP of Cannondale.\n\u2022 Selected customers with high value and improved their retention rate by sending ads and coupons\nEnvironment: R 3.X, Oracle 12c, Hadoop 2.X, Git 2.X', u'Data Scientist\nMicrosoft Corporation - Seattle, WA\nOctober 2016 to March 2017\nTwitter - Text Analytics and NLP\nMachine Learning model for building Fake-Real Classifier Based on relevant twitter accounts\n\n\u2022 Project included preprocessing the data and using natural language processing(NLP) techniques\n\u2022 Extracted tweets and followers from the Twitter website using R twitter package\n\u2022 Preprocessed the data with tm package and built a term-document matrix\n\u2022 Used topic models, sentiment140, igraph packages to analyze - topics, sentiments and relationships\n\u2022 Evaluated and optimized performance of models, tuned parameters with K-Fold Cross Validation.\n\u2022 Using Random Forest, accuracy on test set was 85.5% with Sensitivity 90% and Specificity 73.7%\n\u2022 Used Git 2.X to apply version control. Tracked changes in files and coordinated work on the files among multiple team members.\nEnvironment: R 3.X, Oracle 10g, Hadoop 1.X, HDFS, Spark 1.4, Tableau 9, Git 2.X', u'Project Scientist\nTheiss Research - La Jolla, CA\nJuly 2014 to October 2016\n\u2022 Used machine learning and statistical techniques to predict the nature of contamination in materials.\n\u2022 Used several regression and classification models to find the best solution.\n\u2022 Used Re-sampling Methods as well as Model Selection and Regularization techniques to resolve bias-variance and other issues.\n\u2022 Figured out critical features required to improve the accuracy of the model. Collected data under different conditions to build optimal feature space.\n\u2022 Obtained, processed, visualized, and analyzed data by plotting relevant graphs and merging the data to different statistical models to explain the underlying physical phenomena in materials\n\u2022 Demonstrated mastery by publishing of results, preparation of reports, and presentations to a less technical audience\n\u2022 Evidenced strong interpersonal and leadership abilities while training colleagues on statistical techniques and provided guidance throughout their research work', u'Researcher\nNational Institute of Standards and Technology (NIST) - Gaithersburg, MD\nJuly 2011 to July 2014\n\u2022 Diligently, and in a timely manner, carried out independent research where analysis of a large amount of data was required\n\u2022 Collected data, built analytical models and made recommendations regarding material selections.\n\u2022 Worked on different machine learning algorithms to build an optimum predictive model.\n\u2022 Used R to perform root cause analysis for the issues encountered in production.\n\u2022 Computed several parameters based on collected data-largely consisting of time-dependent noise.\n\u2022 Performed time series analysis and figured out ways to find patterns in noisy data.']","[u'PhD in Electrical & Computer Engineering', u'B.E in Electronics & Communications Engineering']","[u'George Mason University\nJanuary 2014', u'Visvesvaraya Technological University\nJanuary 2006']","degree_1 : PhD in Electrical & Compter Engineering, degree_2 :  B.E in Electronics & Commnications Engineering"
0,https://resumes.indeed.com/resume/ebeb5490552786e4,"[u""Data Scientist\nCVS - Woonsocket, RI\nAugust 2017 to Present\nDescription:CVS Health Corporation (previously CVS Corporation and CVS Caremark Corporation) (stylized as Heart Corazon's (CVS Health) is an American retail pharmacy and health care company headquartered in Woonsocket, Rhode Island. The company began in 1964 with three partners who grew the venture from a parent company, Mark Steven, Inc., that helped retailers manage their health and beauty aid product lines.\n\nResponsibilities:\n\u2022 Conducted qualitative and quantitative research to gather data from more than 30 insurance companies\n\u2022 Responsible for data identification, collection, exploration & cleaning for modeling, participate in model development\n\u2022 Visualize, interpret, report findings and develop strategic uses of data.\n\u2022 Understand transaction data and develop Analytics insights using Statistical models using Machine learning.\n\u2022 Involved in gathering requirements while uncovering and defining multiple dimensions. Extracted data from one or more source files and Databases.\n\u2022 Analyzed data from both competitive market and the monopolistic market\n\u2022 Collected Database of sales of items in all aspects. Cleaned, filtered and transformed data to specified format.\n\u2022 Cleaned data using R, then visualize the data, and derive statistical modeling plots\n\u2022 Interacted with the other departments to understand and identify data needs and requirements and work with other members of the IT organization to deliver data visualization and reporting solutions to address those needs.\n\u2022 Developed data solutions to support strategic initiatives, improve internal processes, and assist with strategic decision-making and design SWOT Analysis.\n\u2022 Extraction by developing a pipeline using Hive (HQL) to retrieve the data from Hadoop cluster, SQL to retrieve data from Oracle database and used ETL for data transformation.\n\u2022 Performed Data Cleaning, features scaling, features engineering using pandas and NumPy packages in python.\n\u2022 Replacement of missing data and perform a proper EDA, Univariate and bi-variate analysis to understand the intrinsic effect/combined effects.\n\u2022 Responsible for providing reporting, analysis and insightful recommendations to business leaders on key performance metrics pertaining to sales & marketing\n\u2022 Worked on various theorems related to determining various insurance pricing (insurance coverages and costs)\n\u2022 Used R to identify product performance via Classification, tree map and regression models along with visualizing data for interactive understanding and decision making.\n\u2022 Accomplished multiple tasks from collecting data to organizing data and interpreting statistical information.\n\u2022 Created dynamic linear models to perform trend analysis on customer transactional data in R.\n\u2022 Conducted exploratory and descriptive data analysis of large data sets\n\nEnvironment:R-Programming, Statistical/Regression Analysis, Microsoft Excel, Machine Learning, SQL, PowerPoint."", u""Data Scientist\nAIM Speciality Health - Chicago, IL\nMay 2016 to July 2017\nDescription:AIM Specialty Health\xae (AIM) provides clinical solutions that drive appropriate, safe, and affordable care. Serving more than 50 million members across 50 states, D.C. and U.S. territories, AIM promotes optimal care through use of evidence-based clinical guidelines and real-time decision support for both providers and their patients. The AIM platform delivers significant cost-of-care savings across an expanding set of clinical domains, including radi-ology, cardiology, oncology, specialty drugs, sleep medicine, musculoskeletal care, and genetic testing\n\nResponsibilities:\n\u2022 Involved in the entire data science project life cycle and actively involved in all the phases including data extraction, data cleaning, statistical modeling, and data visualization with large data sets of structured and unstructured data.\n\u2022 Involved required gathering, function analysis and development of the ETL.\n\u2022 Used R to manipulate data, develop and validate quantitate models.\n\u2022 Fundamental analysis was conducted on the data to understand the basic insights of data.\n\u2022 Performed exploratory analysis on the data provided by the client.\n\u2022 Cleansed the data by eliminating duplicate and inaccurate data in R and Python.\n\u2022 Identified the missing values, pattern recognition techniques were used to find the pattern for missing values and handled them with K-NN imputation.\n\u2022 Unstructured data was scaled and combined with structured data to apply statistical methods.\n\u2022 Dummy variables created for categorical data.\n\u2022 Data was visualized using different visualization (scatter plot, box plots, histograms) techniques from ggplot2 package in R.\n\u2022 Involved working on different databases like Json, Scala, SPARK/HADOOP, XML, NoSQL and SQL of different platforms etc.\n\u2022 Involved working in Data science using Python on different data transformation and validation techniques like Dimensionality reduction using Principal Component Analysis () and Factor Analysis, testing and validation using ROC plot, K- fold cross validation, statistical significance testing.\n\u2022 Evaluated models for feature selection and elastic technologies like Elasticsearch, Kibana etc.\n\u2022 Used predictive analytics and machine learning algorithms to forecast key metrics in the form of designed dashboards on to AWS and Django platform for the company's core business.\n\u2022 Sorted attributes into categorical and numeral variables.\n\u2022 Machine learning algorithms like Random forest and logical regression modelswerebuilt on correlated data sets.\n\u2022 Performed data cleaning and feature selection using MLLib package in PySpark and working with deep learning frameworks such as Caffe, Neon etc.\n\u2022 Design of Dashboards showing Categories and product-based reports with key performance indicators.\n\u2022 Understanding and implementing the process of MapReduce using various Big Data platforms like Hadoop/Spark SQL API in python.\n\u2022 Extraction of data from different database and warehouses and transforming them as per the requirements.\n\u2022 Used functional programming languages like Scala on Hadoop platforms.\n\u2022 Performed data profiling to learn about the behavior of various features and finding dependencies with in them using Python.\n\u2022 Fine-tuned models to obtain more recall than accuracy. Tradeoff between False Positives and False Negatives.\n\u2022 Evaluated models using Recall, Cross Validation and ROC.\n\u2022 Z-score standardization, Laplace estimator and other techniques was applied on the model for performance improvement.\n\nEnvironment:R, Python, MS Excel, Tableau, Power point,Caffe, Neon, Scala, Hadoop platforms,Recall, Cross Validation."", u""Data Scientist\nLash Group Inc - Fort Mill, SC\nJanuary 2015 to April 2016\nDescription:Lash Group started more than 20 years ago around a kitchen table with a unique idea and a passion for patients. Our pioneering vision to connect patients more quickly and efficiently to better care has transformed an industry, and we remain dedicated to helping people access, afford, and remain on the treatments they need. More than a patient services company. We're part of AmerisourceBergen, one of the world's largest pharmaceutical sourcing and distribution services companies. We partner with our network before, during, and after launch to help our clients succeed.\n\nResponsibilities:\n\u2022 Played key role in System Development Life Cycle (SDLC) Process consisting of: Design and Gap Analysis, Business Requirements, Systems Requirements, Test Criteria and Implementation to have the output of project.\n\u2022 Conducted Joint Application Development (JAD) sessions with development and other IT teams to define the requirements and supported developers in the use of Change Management and Change Control tools and procedures.\n\u2022 Supported the project teams in the use of the selected tools and architectures, providing continuity and consistency across projects.\n\u2022 Analyzed the business and functional requirements and translated them into technical specifications and data rules required for the ETL process.\n\u2022 Various versions of the documents generated during the project were maintained and managed using Rational Clear Case and performed defect tracking using Rational Clear Quest.\n\u2022 Created Requirement Traceability Matrix linking Business Requirements to Functional requirements and thereby to Systems Requirements.\n\u2022 Reviewing data and tools standards, including tools for use in data acquisition and preparation, data modeling, metadata management, structured queries and online analytical processing (OLAP).\n\u2022 Performed data analysis and used SQL extensively for analysis and business logic validation needed to resolve production issues.\n\u2022 Performed Data mapping between source systems to Target systems, logical data modeling, created class diagrams and ER diagrams.\n\u2022 Planned and defined Use Cases; created Use Case diagrams, Scenarios and Use Case Narratives using the UML methodologies.\n\u2022 Developed User Accepting Testing (UAT) plans and test cases based on the business requirements/System Requirements.\n\u2022 Conducted training programs for the BI users and defined the overall corporate data model on which the BI is based.\n\nEnvironment: Agile Methodology, Rational Requisite Pro, MS Visio, SQL server, Rational Clear Quest, MS Office, Azure, Share Point, OIBEE, MS Office, MS excel, Rally, MS Excel, HP ALM"", u""Business Analyst\nAT&T - Dallas, TX\nMay 2013 to December 2014\nDescription:AT&T Inc is engaged in provision of communications and digital entertainment services in the United States and the world. It provides fixed-line services, including voice, data, and television services to consumers and small businesses.\n\nResponsibilities:\n\u2022 Participated in all phases of research including requirement gathering, data cleaning, data mining, developing model and visualization.\n\u2022 Collaborated with Data Engineers and Technical Architect to get insights and understanding of the data.\n\u2022 Used R to manipulate and analyze data for solution. Packages like 'tm', 'wordcloud', were used for test mining.\n\u2022 Analyzed data by creating vector corpus from the text.\n\u2022 Cleansed and transformed the data by removing stop words, white spaces, punctuations,\n\u2022 Converted the corpus into lower case, lemmatized the corpus.\n\u2022 Created Document Term Matrix (DTM) from the corpus, which make model building possible.\n\u2022 Word frequencies were analyzed as a part of mining the corpus.\n\u2022 Applied inverse document frequencies to eliminate words that occur large fraction of corpus.\n\u2022 Calculated cumulative frequencies, identified potential classification terms.\n\u2022 Converted matrix as data frame and visualized and normalized the data for better model performance.\n\u2022 Visualized high frequency words using word cloud.\n\u2022 Build K-means algorithm to cluster the data into 10 groups.\n\u2022 Grouped based on the past issues and each group was assigned with specific set of issues.\n\u2022 Random forests were built on the existing data and results were observed.\n\u2022 The model could predict 89% accurate.\n\nEnvironment:R, Python, Analytics Dashboard, Web services, Text Mining, VSTS, MS Office"", u'Data Analytics\nSmart Edge Software ltd\nJanuary 2011 to April 2013\nResponsibilities:\n\u2022 Worked closely with internal team like design, development and HR management to understand the requirement of the company.\n\u2022 Built and analyzed datasets using R, and Python.\n\u2022 Developed ETL based systems for data acquisition and data consumption by stakeholders.\n\u2022 Developed data mining, data analytics data collection, data cleaning, developing models, validation, visualization and performed Gap analysis.\n\u2022 Built Multivariate business and resource forecasting using machine learning algorithms.\n\u2022 Applied linear regression, multiple regression, ordinary least square method, mean-variance, theory of large numbers, logistic regression, dummy variable, residuals, Poisson distribution, Bayes, Naive Bayes, fitting function etc. to data with help of Scikit, SciPy, Numpy and Pandas module of Python.\n\u2022 Applied linear regression in Python and SAS to understand the relationship between different attributes of dataset and causal relationship between them\n\u2022 Applied clustering algorithms on market data to study the underlying data patterns. Methodologies used were PCA, Factor analysis, Hierarchical, K-means through Scikit/SciPy, R for projecting market\n\u2022 Performs complex pattern recognition of financial time series data and forecast of returns through the ARMA and ARIMA models and exponential smoothening for multivariate time series data.\n\u2022 Developed Clustering algorithms and Support Vector Machines that improved Customer segmentation and Market Expansion.\n\u2022 Documented the visualizations and results and submitted to HR management.\n\nEnvironment: R, Python, Analytics Dashboard, Web services, SAS, VSTS, MS Office', u'Business Analyst\nData Point - Hyderabad, Telangana\nMay 2009 to November 2011\nResponsibilities:\n\u2022 Analyze and determine risks to help clients make sound financial decisions.\n\u2022 Determine solutions to minimize or eliminate risks.\n\u2022 Lead interviews and meetings with business associates to gather business requirements.\n\u2022 Assist customer care in problems and issues resolution.\n\u2022 Leading member in interviews (JADs) and meetings to engage business associates and subject matter experts, in business planning, business analysis, and business requirements definitions.\n\u2022 Act as a subject matter expert in requirements planning/gathering sessions.\n\u2022 Lead Research and gathered information regarding a business problem/opportunity.\n\u2022 Analyzed and translated information for management and IT teams.\n\u2022 Used Unified Modeling Language (UML) methodologies to design Use Case Diagrams, Activity Diagrams, and Sequence Diagrams.\n\u2022 Working closely with the development team to clarify and ensure adherence to business requirements.\n\u2022 Prepared Functional Requirement Specification and Business Use Cases.\n\u2022 Identified for business process improvement and initiated efforts to make improvements and problem resolution.\n\u2022 Analyze financial statements such as profit and loss, company budget and employee headcount reports.\n\u2022 Compile reports showing the proposed plan of action for existing and potential clients\n\u2022 Develop test scripts utilizing comprehensive business requirements, functional documentation and processes.\n\u2022 Interact with various resources at workspace like business, development as well as analyst teams.\n\u2022 Implement test scripts, record actual results and identify defects and examine statistical reports.\n\u2022 Suggest creative and active solutions for problem and resolve issues and express ingenuity as required.\n\u2022 Assist in test defect management process, monitor and report on defect resolution to management.\n\u2022 Identify and suggest changes to predetermined quality guidelines and policies.\n\u2022 Review other risk factors including problems with the physical location of the organization, potential for robberies and the likelihood of employees being injured on the job.\n\nEnvironment:Hybrid Framework, SQL server, Jupiter, Lexus Nexus, Global Automatic Call Distributor (GACD) and Leeds.']","[u""Bachelor's""]",[u''],"degree_1 : ""Bachelors"""
0,https://resumes.indeed.com/resume/d286bdc521bbe870,"[u'Data Scientist\nLynden, Inc - Seattle, WA\nJanuary 2017 to Present\n\u2022 Built classifier which identifies phone calls requiring human review\n\u2022 Utilized IBM Watson Speech-to-Text API to transcribe 17,500 calls / 600+ hours of customer service calls\n\u2022 Implemented several NLP tools to gather sentiment and build a classifier system to determine which calls lead to positive or negative customer service experiences\n\u2022 Linked call surveys, previously graded by a human, as labels for training models', u""Data Science Student\nGalvanize, Inc - Seattle, WA\nJanuary 2017 to January 2017\nCapstone Project: Created a classifier to predict whether companies will beat their\nprojected quarterly revenues based on the contents of their earnings conference calls.\nUsed NLP via TF-IDF and NLTK's Vader sentiment analyzer to create an ensemble\nof a gradient boosted classifier with a logistic regressor. https://goo.gl/x7UNCX\n\u2022 Case Study: Predicted churn in a ridesharing app. Used profit curves, LTV, and a Gradient Boosting Classifier to optimally set thresholds of churn prediction.\n\u2022 Case Study: Used text to classify app descriptions as sports related/not sports related.\nUsed NLP via TF-IDF with Multinomial Naive Bayes."", u""Senior Financial Consultant\nTD Ameritrade - Seattle, WA\nJanuary 2014 to January 2017\nRanked #1 within entire company in increased assets under management (AUM),\n$25.7 million, during last quarter of employment\n\u2022 Worked directly with clients - oftentimes face to face. Gained understanding of their\nfinancial goals and delivered customized solutions\n\u2022 Lead company-wide sales guidance conference call - coached junior Financial\nConsultants on best practices\n\u2022 Created automated trading algorithms using Python\n\u2022 Frequently conducted Monte Carlo simulations on clients' portfolios to assign\nconfidence intervals of asset longevity\n\u2022 Managed $300 million for roughly 450 clients\n\u2022 Promoted to Senior Financial Consultant from Financial Consultant role after 2 years\nof exceeding expectations"", u'Financial Advisor\nAmeriprise - Fort Lauderdale, FL\nJanuary 2013 to January 2014\n\u2022 Utilized Python, BeautifulSoup, and Selenium to automate client gathering by scraping FL.gov layoff reports for 401k rollovers\n\u2022 Raised AUM by $5 million within first 12 months of hire', u'Financial Advisor\nMerrill Lynch - Miami, FL\nJanuary 2011 to January 2012\n\u2022 Co-managed $200 million for 100 affluent households\n\u2022 Recognized in the Big Bull Rankings - the daily top 5 highest grossing advisors in southern Florida\n\u2022 Awarded the Silver Bull for opening a $250,000+ relationship within first three\nmonths of hire', u'Financial Advisor\nEdward Jones - Springfield, MO\nJanuary 2009 to January 2011\n\u2022 Designated as a Segment Leader for having highest gross production in segment\n\u2022 Raised AUM by $3 million within first 12 months of hire']",[u'Certificate in Data Science'],"[u'University of Missouri Seattle, WA\nJanuary 2017']",degree_1 : Certificate in Data Science
0,https://resumes.indeed.com/resume/176bbd984619ac9a,"[u'Data Scientist\nSystematrix Solutions - Atlanta, GA\nNovember 2017 to Present\n\u2022 Implemented graph algorithms for network analysis on spark, neo4j and python\n\u2022 Presented and Coded new algorithms for graph analytics using graphX and scala.\n\u2022 Created, modified, and benchmarked machine learning algorithms for statistical inference on network properties and money laundering prediction\n\u2022 Determined new data sources and implemented relevant contextual data sources and processes\n\u2022 Installed programs on and adjusted code for multi-core parallel processing on computer clusters\n\u2022 Routinely provided qualitative insights into upcoming roadblocks to meeting projects and customers needs before it was a noticed problem\n\u2022 Took the initiative to develop and present data privacy policies, standards, processes, and local and international legal requirements\n\u2022 Translated SME\u2019s requirements into concrete queries and programs to detect fraud\n\u2022 Prescribed strategic approach to changing algorithmic regulations\n\u2022 Constructed visualizations for KPI dashboards and testing networks', u""Operational Intelligence Analyst\nStanford University - Palo Alto, CA\nDecember 2015 to July 2017\n\u2022 Used mathematical techniques, and fitted statistical models to analyze data related to business problems.\n\u2022 Visualized and Identified contextual data that was needed, patterns, summary statistics and trends using (but not limited to): non-parametric ensemble models, Bayesian inference and Natural Language Processing (NLP).\n\u2022 Adjusted code for multi-core parallel processing on computer clusters and used MapReduce functions to aggregate data for customer profile.\n\u2022 Automated system to categorize any text using an unsupervised model and leveraged glove vectors (or Word2Vec) to classify risk.\n\u2022 Constructed statistical frameworks and code utilizing new machine learning programs and then presented it at conferences and expos.\n\u2022 Transferred, aggregated and updated data on approvers of advances, credit cards, purchase orders, and other financial and banking transactions in NoSQL database (mongoDB) using JavaScript, and Python.\n\u2022 Collaborated on multiple high-priority projects, was a key contributor to the team's long term strategy meetings; solved problems with user friendly explanation of the methodology and with minimal oversight."", u'Testing Technician\nPhillips-Invivo - Orlando, FL\nJune 2011 to August 2014\n\u2022 Developed and tested new product improvements and Product Test Plans in the R&D department.\n\u2022 With engineering verified and validated Quality Requirements, and Product Specifications.\n\u2022 Investigated product non-conformance thus preventing in-field breakdown and product recalls.\n\u2022 Created and managed instructional guideline material, reports, and created excel spreadsheets to track requirements for regulators and auditors.', u'Student Assistant\nUCF-Department of Industrial Engineering - Orlando, FL\nSeptember 2009 to June 2011\n\u2022 Managed industrial engineering graduate student records system and created MS Access database to file/organize student records.\n\u2022 Operated front desk support operations with staff meetings, information switchboard, FAQ, data entry of student records and department forms.']","[u'Master of Science in Statistical Computing in (MS), Credits', u'Bachelors of Science in Industrial Engineering in Operations Research']","[u'University of Central Florida Orlando, FL\nAugust 2013 to March 2015', u'University of Central Florida Orlando, FL\nMay 2013']","degree_1 : Master of Science in Statistical Compting in (MS), degree_2 :  Credits, degree_3 :  Bachelors of Science in Indstrial Engineering in Operations Research"
0,https://resumes.indeed.com/resume/f0170e071e849d98,"[u'Intern Data Scientist\nAutodesk, MA\nJune 2017 to December 2017\n\u2022 Developed a command flow search tool to provide easy way to explore, visualize using d3.js and understand user workflow\n\u2022 Designed complex HiveQL queries in MySQL to extract, integrate and manipulate critical data accurately and efficiently\n\u2022 Leveraged product usage data to identify user persona by applying Multinomial Logistic Regression algorithm on the dataset\n\u2022 Used deep analytics techniques to gain more insight into product usage and market adoption, and created Oozie workflow\n\u2022 Devised HiveQL queries as a part of Oozie workflow and fed the results into Looker to further enrich it for visualization\n\u2022 Created visually impactful dashboards in Looker to help management and development teams in decision making\n\u2022 Assisted the product and business analytics teams to migrate their dashboards from Splunk to Looker', u'Northeastern University\nJanuary 2017 to April 2017\n\u2022 Developed a scalable web application for Bloggers on Spring Boot platform with Amazon EC2 & Dynamo DB database\n\u2022 Designed secured and robust cloud architecture using VPCs, Load Balancers, and Auto Scaling Configuration\n\u2022 Conducted load and performance testing to analyze and measure the performance of web application using JMeter subnet', u'Big Data Analysis on Yelp Dataset\nSeptember 2016 to December 2016\n\u2022 Performed Sentiment analysis, Bloom Filter, Chaining MapReduce, Secondary Sorting to analyze data in various dimensions\n\u2022 Applied Joins, Partitioner, and Combiner in MapReduce to improve efficiency and visualized the results in Tableau\n\u2022 Used neo4j graph database to recommend users based on historic data and indexing in HBase for faster query processing', u""Big Data Analysis on Yelp Dataset\nSeptember 2016 to December 2016\n\u2022 Predicted the anticipated popularity of a News article using R and Microsoft Azure Machine Learning as data mining tool\n\u2022 Implemented feature selection to select most important features, Decision Tree, Random Forest, Neural Network, and Poisson Regression for Regression and Classification, K-means and Hierarchical Clustering for Clustering for model building\nExcess Food Share May 2016 - June 2016\n\u2022 Developed an application to share surplus food in vicinity using J2EE Spring MVC and Hibernate ORM\n\u2022 Secured the application against cross-site scripting using Interceptors and RegEx, and SQL injection to secure user's data\n\u2022 Used Hibernate annotation based mapping with MySQL Workbench for database transactions and frontend in JSP Servlet"", u'Immortal Lungs (IOT)\nJanuary 2016 to April 2016\n\u2022 Designed a database application to cure and predict occurrence of lung cancer with the help of nanobots and tracking device\n\u2022 Expanded the system to be centralized between doctor and patient using MySQL and normalized to 3NF to avoid anomalies\n\u2022 Implemented views, triggers, stored procedures and scheduled backup recovery points', u'Apprentice PHP Developer\nExa Digital Solutions Pvt. Ltd\nMay 2014 to June 2015\n\u2022 Built web analysis application using Google APIs, PHP, JSON, HTML5, CSS3, jQuery, and Ajax to improve client business\n\u2022 Developed dashboard for office to be used by different departments to have live update on all ongoing projects\n\u2022 Accomplished the milestone of saving 200 man hours/week by automating a process which consisted of web scraping data\nfrom Google Webmaster, AdWords and post processing to convert scraped data into human readable format']","[u'MS in Information Systems in Database Management', u'BE in Information Technology in Warehouse and Mining, Artificial Intelligence']","[u'Northeastern University Boston, MA\nMay 2018', u'University of Mumbai Mumbai, Maharashtra\nMay 2013']","degree_1 : MS in Information Systems in Database Management, degree_2 :  BE in Information Technology in Warehose and Mining, degree_3 :  Artificial Intelligence"
0,https://resumes.indeed.com/resume/55258f76f450c5d7,"[u'Data Scientist / Engineer Intern - Advanced Advertisement\nViacom Media Networks - New York, NY\nJanuary 2018 to Present\n\u2022 Analyzing social data to predict best influencers for advertisement thus impacting business.\n\u2022 Architected AWS pipeline to ingest raw data (AWS S3), perform ETL by parsing JSON (AWS Glue - Spark), and perform analytics\n(AWS Athena) after concluding with the team.\n\u2022 Utilized API gateway to built swagger defined REST API to extract data from backend.\n\u2022 Built API connector to extract information from social data platforms (Facebook, YouTube, Instagram) in Python.\n\u2022 Actively promoted continuous integration, testing and technical documentation through Github, Zenhub and unit testing.', u""Data Scientist Intern\nGenscape, Inc - Boulder, CO\nJune 2017 to August 2017\nAcquired and interpreted US Natural gas, and Power market data using Machine Learning models, performed time series analysis\nARIMA model to improve Genscape's products.\n\u2022 Performed ETL using R and SQL, developed and scaled models to estimate the gas consumption of gas - fired power plants.\n\u2022 Impacted growth by improving prediction accuracy of 200 plants (accuracy from 50% to 88%), which is then posted on Bloomberg\nterminal for clientele to generate US Gas Market analytics."", u'Developer Intern\nBayesquare Inc - New York, NY\nOctober 2016 to December 2016\n(Technologies: Python, Predictive Analytics, KNN Classifier) October 2016 - December 2016\n\u2022 Architected statistical and machine - learning model (KNN Classifier) to perform predictive analysis on real-time stock data,\nfollowed by a comprehensive testing for accuracy on quantopian.com with real-time training data.']","[u'Master of Science in Computer Science', u'Bachelor of Engineering in Information Technology']","[u'NEW YORK UNIVERSITY\nAugust 2016 to May 2018', u'UNIVERSITY OF MUMBAI\nAugust 2012 to June 2016']","degree_1 : Master of Science in Compter Science, degree_2 :  Bachelor of Engineering in Information Technology"
0,https://resumes.indeed.com/resume/9447b20f6b353735,"[u'Associate Data Scientist\nCalm Tea Management\nAugust 2015 to Present\nResponsibilities:\n\u2022 Designed applications of Statistical Analysis and Data visualizations with challenging large data processing problems dealing with Health care clinic data along with Investment risk analysis.\n\u2022 Involved writing the mapping specifications for converting the legacy building and warehouse datasets\n\u2022 Advanced skills with various databases and performed the computations, log transformations, Data exploration to identify the insights and conclusions from customer medical history data using R- programming in R-studio\n\u2022 Performed in-depth statistical analysis and data mining methods using R, including Cluster analysis, Logistic Regression, and boosting models\n\u2022 Extensively used Azure Machine Learning to set up the experiments and creating Web services for the predictive analytics\n\u2022 Performed feature scaling, feature engineering and statistical modeling.\n\u2022 Worked on writing complex SQL queries in performing Data analysis using window functions, joins, improving performance by creating partitioned tables,\n\u2022 Prepared multiple dashboards using Tableau to visualize Health care data and worked with all aspects of regression models to determine the payments of medical procedures as well as investment risk classification\n\u2022 Responsible for working with stakeholders, SMEs to troubleshoot issues, communicate desired goals, issues, and findings to ensure all team members are on the same page', u'Data Quality Analyst\nQualcomm Technologies\nMarch 2014 to July 2015\nResponsibilities:\n\u2022 Performed Data Transformation method for Rescaling and Normalizing variables.\n\u2022 Regularly generated Data Models using R transform data into useful information for the Stocks team to make stock predictions and analysis\n\u2022 Applied different Machine Learning algorithms/methods on data sets to predict target marketing, marketing analysis, cost analysis, etc.\n\u2022 Worked on data to increase cross-& up-sell revenues, enhance customer value or reduce manufacture losses and provide market analysis\n\u2022 Analyzed, transformed, and contextualized a variety of ingested telecommunications engineering, marketing data, and some consumer behavior data for building direct marketing predictive models using Machine Learning\n\u2022 Designed, modeled, validated, and tested statistical algorithms against various data sets including behavioral data and deployed predictive models using R-studio and Python.\n\u2022 Analyzed customer consuming behavior and discover value of customers. Furthermore, applied customer segmentation with Clustering algorithms and develop geo-demographic customer segmentation models.\n\u2022 Developed personalized products recommendation with Machine Learning algorithms including Collaborative filtering and Boosting Tree, to better meet the needs of existing customers and acquire new customers.\n\u2022 Worked closely with marketing team to deliver actionable insights from huge volume of data, coming from different marketing campaigns and customer interaction matrices such as web portal usage, email campaign responses, public site interaction, and other customer specific parameters.\n\u2022 Visualized data for patterns, anomalies, and forecast using Tableau, Python, and ggplot.', u'Business Data Analyst\nAvion Systems\nAugust 2012 to February 2014\n\u2022 As a Business Data Analyst, performed detailed business requirement analysis for the Data Mart Enhancement and Performance Improvement.\n\u2022 Applied various statistical concepts of in the evaluation stage to extract significant findings through comparisons.\n\u2022 Prepared comprehensive documentation, analyses and understandings of results including technical visualization, reports, protocols, and quantitative analyses.\n\u2022 Gathered, analyzed & translated raw data into relevant analytic approaches & shared for peer review in context of the business requirement\n\u2022 Proficient in the entire CRISP-DM life cycle and actively involved in all the phases of project life cycle including data acquisition, data cleaning, data engineering.\n\u2022 Contributed to Risk Management team, Marketing team, as well as Operations team, helping them analyze and gather information according to their needs\n\u2022 Design, model, validate and test statistical algorithms against various real-world data sets including behavioral data and deploy models in the backend\n\u2022 Gained experience transforming data by the method of Normalizing variables.\n\u2022 Co-ordinate with stakeholders, operatives, colleagues, and SME to get business scenario review, validation of data, and confirmation of required results']","[u""Master's""]",[u''],"degree_1 : ""Masters"""
0,https://resumes.indeed.com/resume/a920fae8993e4c58,"[u'Data Scientist\nMetis - New York, NY\nApril 2017 to Present\nMetis is an intensive 12 week program designed to equip aspiring data scientists with a wide range NoSQL (MongoDB) of tools for practical applications in industry. The curriculum places an emphasis on the use of modern Machine Learning techniques for prediction, classi cation, NLP and deep learning tasks, through the use of libraries such as sci-kit learn and Keras (Theano + Tensorflow).', u'Programmer Analyst\nMerrill Lynch - Hopewell, NJ\nJanuary 2005 to September 2008\nServed as a software consultant for the Investment Proposal, MLPA and Wealth Outlook teams. Was responsible for analysis, design, development, and testing of enterprise software applications.\nDeveloped a reporting windows service for the financial planning applications using C#, Visual .NET', u'Graduate Assistant\nSyracuse - Syracuse, NY\nJanuary 2004 to August 2004\nDeveloped and maintained Hindi website for the Maxwell School using ASP pages, Visual Interdev', u'Intern\nTop choice digital - Brooklyn, NY\nMay 2003 to December 2003\nPart of the team that developed templates for e-bay auctions using Dreamweaver, HTML, JavaScript. Maintained the sales database in MS Access.', u'Intern\nBusiness Technology Solutions Pvt. Limited - Hyderabad, ANDHRA PRADESH, IN\nApril 2001 to May 2002\nPart of the team that developed, tested and documented papersinvited.com. Worked with the content management team to design the front-end using ASP/VB Script.']","[u'MS in Computer Science', u'BS in Computer Science']","[u'Syracuse University Syracuse, NY', u'JNTU Hyderabad, ANDHRA PRADESH, IN']","degree_1 : MS in Compter Science, degree_2 :  BS in Compter Science"
0,https://resumes.indeed.com/resume/2dfe8c2f3aa62f70,"[u'Data Scientist\nJ.BHUNT - Lowell, AR\nAugust 2017 to Present\nDescription:J.B. Hunt Transport Services, Inc. is a trucking and transportation company that was founded by Johnnie Bryan Hunt and based in the Northwest Arkansas city of Lowell. J.B. Hunt Transport Services, Inc. was incorporated in Arkansas on August 10, 1961 and originally started with five trucks and seven refrigerated trailers to support the original rice hull business. By 1983, J.B. Hunt had grown into the 80th largest trucking firm in the U.S. and earned $63 million in revenue.\n\nResponsibilities:\n\u2022 Performing feature extraction and selection using Latent Semantic Analysis, Principal Components\n\u2022 Analysis, Factor Analysis, and Singular Value Decomposition, using KNIME version 2.12.2 and Rapid Miner with R and Python Programming Tools.\n\u2022 Performing correlation analysis and hypothesis tests on email metadata to establish any statistically.\n\u2022 Significant relationships between individual (or combinations) email metadata attributes and document categorization (responsiveness and privilege) using MINITAB, R, and Python programming.\n\u2022 Lead the efforts on kicking off data inventory engagements involving close to over 100 different data sets being utilized within Coke like Neilson, Crest, DINE, 360i, AHOLD, SMG(Digital Campaign Data),Social Media, Matrix(Teradata),Freestyle& Consumer Rewards data and ingested them under the Data Lake making it the first time, different Coke teams have access to various different data streams at one unified location.\n\u2022 Lead the effort on doing various POC on the various Big Data stacks and aligned them with Coke requirements and provide them with various recommendations on a robust architecture and roadmap.\n\u2022 Established Data architecture strategy, best practices, standards, and roadmaps.\n\u2022 Lead the development and presentation of a data analytics data-hub prototype with the help of the other members of the emerging solutions team.\n\u2022 Involved in analysis of Business requirement, Design and Development of High level and Low-level designs, Unit and Integration testing.\n\u2022 Responsible for loading data from UNIX file systems to HDFS. Installed and configured Hive and written Pig/Hive UDFs.\n\u2022 Involved in creating Hive Tables, loading with data and writing Hive queries which will invoke and run MapReduce jobs in the backend.\n\u2022 Created Airflow Scheduling scripts in Python to automate the process of Sqooping wide range of data sets.\n\u2022 Worked on migrating MapReduce programs into Spark transformations using Spark and Scala, initially done using Python (PySpark).\n\u2022 Experienced in querying data using SparkSQL on top of Spark engine for faster data sets processing.\n\u2022 Involved in file movements between HDFS and AWS S3 and extensively worked with S3 bucket in AWS\n\u2022 Created data pipeline for different events of ingestion, aggregation and load consumer response data in AWS S3 bucket into Hive external tables in HDFS location to serve as feed for tableau dashboards.\n\u2022 Designed & Implemented the next generation Lambda Architecture to manage Real Time/Micro Batch/Batch Analytics use cases as well as IOT use cases using Spark Core /Spark Streaming/Kafka/NIFI-Hortonworks Data Flow/Hortonworks HDP\n\u2022 Designed and created highly scalable and sub-second response time data access patterns like Pivotal HAWQ/HDP, Spark SQL/HDFS/S3 & MPP like Redshift.\n\u2022 Created roadmap, strategies & implementations around data governance, data lineage, data tagging, metadata management, security cutting across data lake platform using Apache Atlas/Waterline Data.\n\u2022 Lead the effort on managing/designing/architecting/implementing use cases for the Data Lake initiative and do presentations with C-level Executives to build confidence, capability and to drive data analytics vision for the future\n\u2022 Lead & Implemented various predictive analytics POC using Data Science/Machine Learningalgorithms (R, Python, Scala, SparkR, Spark MLlib) to demonstrate capabilities around use cases like Diet Coke Market Share reduction/Shopping Basket Analysis/Customer\n\u2022 Segmentation/Upselling/Product Mix/Trade Promotions/Channel Optimizations in the US market and build different predictive models for future forecasting.\n\u2022 Lead the efforts to introduce the concept of Data Lake Architecture so that data from various sources like Teradata/Social Media/Sensor Data/Survey Data and Unstructured Data can be stored and process in Hadoop/Spark and provide a roadmap to provide timelines and steps to replace Data warehouse/Data Marts.\n\u2022 Performed Exploratory Data Analysis and Data Visualizations using R, and Tableau.\n\u2022 Perform a proper EDA, Univariate and bivariate analysis to understand the intrinsic effect/combined.\n\u2022 Worked with several R packages including knitr, dplyr, SparkR, Causal Infer, spacetime.\n\u2022 Interacted with the other departments to understand and identify data needs and requirements.\nEnvironment:JAVA 1.8/1.7,Hadoop 2/1.3,HBase 0.96,Apache Cassandra 1.2.6,Apache Hive, R 3.3,Apache Pig 0.9, Apache Flume 1.1, Zookeeper, Active MQ 5.0,Apache Camel 2.7,Apache Solr, Apache Lucene, Hibernate Search, MySQL 5.5, ApacheAxis2, SOAP, WSDL, JAX-WS, JAX-RS, JAXB, JAXP, Spring-WS, Storm, Spark, Python-Scikit Learn, Python, Scala, Akka, NLP, OpenNLP, NLTK.', u""Data Scientist\nBarclays IB\nMay 2016 to July 2017\nDescription:Barclays Investment Bank (formerly known as Barclays Capital) is the investment banking division of the British multinational Barclays and is headquartered in London. It provides advisory, financing and risk management services to large companies, institutions and government clients.\n\nResponsibilities:\n\u2022 As an Architect design conceptual, logical and physical models using Erwin and build datamarts using hybrid Inmon and Kimball DW methodologies.\n\u2022 Worked closely with business, data governance, SMEs and vendors to define data requirements.\n\u2022 Worked with data investigation, discovery and mapping tools to scan every single data record from many sources.\n\u2022 Designed the prototype of the Data mart and documented possible outcome from it for end-user.\n\u2022 Involved in business process modeling using UML\n\u2022 Developed and maintained data dictionary to create metadata reports for technical and business purpose.\n\u2022 Designed and automated the process of score cuts that achieve increased close and good rates using advanced R programming.\n\u2022 Involved on Prediction model building, Machine Learning, Business process improvements, Visualization & Process implementation with R Programming and Deep See\n\u2022 Redesigned and developed SAS Applications with Netezza Database to the Netezza Applications reducing run time of Applications from 40 hours to 20 sec using PostgreSQL, nzsql, Aginity Workbench, SAS.\n\u2022 Managed datasets using Panda data frames and MySQL, queried MYSQL relational database(RDBMS) queries from python using Python-MySQL connector MySQL dB package to retrieve information.\n\u2022 Created SQLtables with referential integrity and developed queries using SQL, SQL*PLUS and PL/SQL.\n\u2022 Utilized standard Python modules such as csv, itertools and pickle for development.\n\u2022 Formulated procedures for integration of R programming plans with data sources and delivery systems and R language was used for prediction.\n\u2022 Implementing SparkMlib utilities such as including classification, regression, clustering, collaborative filtering and dimensionality reduction.\n\u2022 Design, coding, unit testing of ETL package source marts and subject marts using Informatica ETL processes for Oracledatabase.\n\u2022 Developed Statistical Analysis and Response Modeling for Analytical Database contributors (logistic regression).\n\u2022 Developed various QlikView Data Models by extracting and using the data from various sources files, DB2, Excel, Flat Files and Bigdata.\n\u2022 Handled importing data from various data sources, performed transformations using Hive, Map Reduce, and loaded data into HDFS.\n\u2022 Tech stack is Python 2.7/PyCharm/Anaconda/pandas/NumPy/unit test/R/Oracle\n\u2022 Applied unsupervised and supervised learning methods in analyzing high-dimensional data. Proficient use of Python Scikit-learn, pandas, and NumPy packages.\n\u2022 Interaction with Business Analyst, SMEs and other Data Architects to understand Business needs and functionality for various project solutions.\n\u2022 Performed data modeling operations using Power Bi, Pandas, and SQL.\n\u2022 Utilized Python libraries wxPython, NumPy, Twisted and matplotlib Used python libraries like Beautiful Soup and matplotlib.\n\u2022 Researched, evaluated, architected, and deployed new tools, frameworks, and patterns to build sustainable Big Data platforms for the clients\n\u2022 Identifying and executing process improvements, hands-on in various technologies such as Oracle, Informatica, BusinessObjects.\n\u2022 Generated graphical reports using python package NumPy and matplotlib.\n\u2022 Built various graphs for business decision making using Python matplotlib library.\n\u2022 Designed both 3NF data models for ODS, OLTP systems and dimensionaldata models using Star and SnowflakeSchemas. Independently coded new programs and designed Tables to load and test the program effectively for the given POC's using with Big Data/Hadoop.\n\u2022 Interaction with Business Analyst, SMEs, and other Data Architects to understand Business needs and functionality for various project solutions\n\u2022 Researched, evaluated, architected, and deployed new tools, frameworks, and patterns to build sustainable Big Data platforms for the clients\n\u2022 Identifying and executing process improvements, hands-on in various technologies such as Oracle, Informatica, Business Objects.\n\u2022 Environment:R 9.0, Informatica 9.0, ODS, OLTP, Oracle 10g, Hive, OLAP, DB2, Metadata, MS Excel, Mainframes MS Visio, Rational Rose, Requisite Pro., Hadoop, PL/SQL, SAS etc.\nDesigned both 3NF data models for ODS, OLTP systems and dimensional data models using Star and Snowflake Schemas."", u""Bigdata/Hadoop Developer\nVerizon - New York, NY\nMay 2013 to December 2014\nDescription:Verizon Communications otherwise known as Verizon is an American multinational telecommunications conglomerate and a corporate component of the Dow Jones Industrial Average. The company is based at 1095 Avenue of the Americas in Midtown Manhattan, New York City, but is incorporated in Delaware.\n\nResponsibilities:\n\u2022 Analyzed Business Requirements and Identified mapping documents required for system and functional testing efforts for all test scenarios.\n\u2022 Implemented of Core concepts of Java, J2EE Technologies: JSP, Servlets, JSF, JSTL, EJB transaction implementation (CMP, BMP, and Message-Driven Beans), JMS, Struts, Spring, Swing, Hibernate, Java Beans, JDBC, XML, Web Services, JNDI, Multi-Threading, Drools, etc.\n\u2022 Handled importing of data from various data sources, performed transformations using Hive, MapReduce, loaded data into HDFS and Extracted the data from MySQL into HDFS using Sqoop\n\u2022 Installed/Configured/Maintained Apache Hadoop clusters for Analytics, application development and Hadoop tools like Hive, HSQL Pig, HBase, OLAP, Zookeeper, Avro, parquet,andSqoop on Linux ARCH.\n\u2022 Installed and configured Hive, Pig, Sqoop, Flume and Oozie on the Hadoop cluster\n\u2022 Installed and configured Hive and implemented various business requirements by writing HIVE UDF.\n\u2022 The configuration of theapplication using spring, Struts, Hibernate, DAO's, Actions Classes, Java Server Pages.\n\u2022 Configuring Hibernate Struts and Tiles related XML files.\n\u2022 Developed the application using Struts Framework that uses Model View Controller (MVC) architecture with JSP as the view.\n\u2022 Developed Hive queries to join clickstream data with the relational data for determining the interaction of search guests on the website\n\u2022 Worked on importing and exporting data from Oracle and DB2 into HDFS and HIVE using Sqoop.\n\u2022 Worked on NoSQL databases including HBase, MongoDB, and Cassandra.\n\u2022 Used Spark with Yarn and got performance results compared with MapReduce\n\u2022 Involved in implementation of Hadoop Cluster and Hive for Development and Test Environment\n\u2022 Developed MapReduce programs in Java to search production logs and web analytics logs for use cases like application issues, measure page download performance respectively\n\u2022 Migrated traditional MR jobs to Spark MR Jobs and worked on Spark SQL and Spark Streaming\n\u2022 Responsible for developing data pipeline using HDInsight, flume, Sqoop,and pig to extract the data from weblogs and store in HDFS.\n\u2022 Used Zookeeper along with HBase\n\u2022 Loaded data into Hive Tables from Hadoop Distributed File System (HDFS) to provide SQL-like access to Hadoop data\n\u2022 Used Hive/HQL or Hive queries to provide Adhoc-reports for data in Hive tables in HDFS\n\u2022 Involved in admin related issues of HBase and other NoSQL databases\n\u2022 Integrated Hadoop with Tableau and SAS analytics to provide end users analytical reports\n\u2022 Handling the documentation of data transfer to HDFS system from various sources. (SQOOP, Flume,and FALCON)\n\u2022 Cluster coordination services through Zookeeper.\n\u2022 Load and transform large sets of structured, semi-structured and unstructured data.\n\u2022 Exported the analyzed data to the relational databases using Sqoop for visualization and to generate reports for the BI team.\n\u2022 Created custom user-defined functions in Python language for Pig.\n\u2022 Worked with different team in ETL, Data Integration,and Migration to Hadoop\n\u2022 Implement POC with Hadoop. Extract data with Spark into HDFS.\n\u2022 Used different file formats like Text files, Sequence Files, Avro.\n\u2022 Assisted in creating and maintaining Technical documentation to launching HADOOP Clusters and even for executing Hive queries and Pig Scripts.\n\u2022 Documented the entire process."", u'Bigdata/Hadoop Developer\nDignity Health, SFO - CA\nFebruary 2012 to December 2013\nDescription:Dignity Health is a California-based not-for-profit public-benefit corporation that operates hospitals and ancillary care facilities in 3 states. As such, it is exempt from federal and state income taxes. Dignity Health is the fifth largest hospital system in the nation and the largest not-for-profit hospital provider in California. Dignity Health was founded in 1986 by the Sisters of Mercy under the name Catholic Healthcare West.\n\nResponsibilities:\n\u2022 Designed and developed multiple MapReduce jobs in Java for complex analysis. Importing and exporting the data using Sqoop from HDFS to Relational Database systems and vice-versa.\n\u2022 Integrated Apache Storm with Kafka to perform web analytics. Uploaded clickstream data from Kafka to HDFS, HBase,and Hive by integrating with Storm.\n\u2022 Configured Flume to transport web server logs into HDFS. Also used Kite logging module to upload webserver logs into HDFS.\n\u2022 Developed UDF functions for Hive and wrote complex queries in Hive for data analysis\n\u2022 Performed Installation of Hadoop in fully and Pseudo Distributed Mode for POC in early stages of the project.\n\u2022 Analyze, develop, integrate, and then direct the operationalization of new data sources.\n\u2022 Regression (linear, multivariate) analysis using R language and plotting graphs of regression results using Shiny R framework.\n\u2022 Generating Scala and Java classes from the respective APIs so that they can be incorporated into the overall application.\n\u2022 Responsible for working with different teams in building Hadoop Infrastructure\n\u2022 Gathered business requirements in meetings for successful implementation and POC and moving it to Production and implemented POC to migrate map reduce jobs into Spark RDD transformations using Scala.\n\u2022 Implemented different machine learning techniques in Scala using Scala machine learning library.\n\u2022 Developed Spark applications using Scala for easy Hadoop transitions.\n\u2022 Successfully loaded files to Hive and HDFS from Oracle, Netezza and SQL Server using SQOOP\n\u2022 Uses Talend Open Studio to load files into Hadoop HIVE tables and performed ETL aggregations in Hadoop Hive.\n\u2022 Developed Simple to Quebec and Python MapReduce streaming jobs using Python language that is implemented using Hive and Pig.\n\u2022 Designing & Creating ETL Jobs through Talend to load huge volumes of data into Cassandra, Hadoop Ecosystem,and relational databases.\n\u2022 Worked on analyzing, writing Hadoop Map Reducejobs using Java API, Pig,and Hive.\n\u2022 Developed some machine learning algorithms using Mahout for data mining for the data stored in HDFS\n\u2022 Used Flume extensively in gathering and moving log data files from Application Servers to a central location in Hadoop Distributed File System (HDFS)\n\u2022 Worked with Oozie Workflow manager to schedule Hadoop jobs and high intensive jobs\n\u2022 Responsible for cluster maintenance, adding and removing cluster nodes, cluster monitoring,and troubleshooting, manage and review data backups, manage and review Hadoop log files.\n\u2022 Extensively used Hive/HQL or Hive queries to query data in Hive Tables and loaded data into HIVE tables.\n\u2022 Creating UDF functions in Pig &Hive and applying partitioning and bucketing techniques in Hive for performance improvement\n\u2022 Creating indexes and tuning the SQL queries in Hive and Involved in database connection by using Sqoop\n\u2022 Involved in Hadoop Name node metadata backups and load balancing as a part of Cluster Maintenance and Monitoring\n\u2022 Used File System Check (FSCK) to check the health of files in HDFS and used Sqoop to import data from SQL server to Cassandra.\n\u2022 Monitored Nightly jobs to export data out of HDFS to be stored offsite as part of HDFS backup\n\u2022 Used Pig for analysis of large datasets and brought data back to HBase by Pig\n\u2022 Worked with various Hadoop Ecosystem tools like Sqoop, Hive, Pig, Flume, Oozie, Kafka\n\u2022 Developed Python Mapper and Reducer scripts and implemented them using Hadoop streaming.\n\u2022 Created schema and database objects in HIVE and developed Unix Scripts for data loading and automation\n\u2022 Environment:Hadoop , MapReduce, Sqoop , AWS, Hive , Flume , Oozie , Pig , HBase, Scala, Zookeeper 3.4.3, Talend Open Studio, Talend, Oracle 11g, Apache Cassandra, SQL Server 2012, MySQL, Java, SQL, PL/SQL, UNIX shell script, Eclipse Kepler IDE, Microsoft Office 2010, MS Outlook 2010., MySQL, SQLite, Cassandra, Docker, AWS (EC2, S3), PyUnit, Karma, Jenkins, Selenium Automation Testing.\n\nInvolved in thetraining of big data ecosystem to end-users.', u'Java Developer\nZessta Software Services - Hyderabad, Telangana\nDecember 2011 to April 2013\nEnvironment:Hadoop, Hive, Pig, Kafka, Scala, Spark, Cassandra, HBase, MongoDB, Scoop, Flume, Falcon, Storm, Oracle 11g, Java, SQL, HBase, Oozie, YARN, Zookeeper, Python, Eclipse Kepler IDE, Microsoft Office 2007, UNIX, MS Outlook 2007.\n\nDescription:We are a high energy IT Specialist Company offering Smart Solutions and Mobile Applications worldwide. Established in 2011, our expertise lies in devising holistic and end-to-end solutions in contemporary technologies including m-Commerce, NFC, Tap and Mobile Technology, Predictive Analytics and Smart solutions. We also offer secure contactless solutions in the field of financial, payment and mobile technology.\n\nResponsibilities:\n\u2022 Involved in all phases of Designing and Development of application.\n\u2022 Created Class diagrams and Use Case diagrams from design specification using Rational Rose.\n\u2022 Worked in Waterfall Methodology and involved in the project discussions.\n\u2022 Implemented action classes, form beans and JSP pages interaction with these components.\n\u2022 Designed and developed the application using Struts Model View Controller (MVC) design Pattern.\n\u2022 Developed Struts Action Forms, Action classes and templates and performed action mapping in struts.\n\u2022 Develop GUI related changes using JSP, HTML and client validations using JavaScript.\n\u2022 Coded JavaScript for AJAX and client-side data validation.\n\u2022 Extensive use of EJBs for middle tier component to implement the business logic.\n\u2022 Developed an automated application using JMS for messaging and JNDI to interact with the Server.\n\u2022 Implemented Service locator pattern to invoke EJBs through Struts.\n\u2022 Used JMS API for asynchronous communication by putting the messages in the Message queue.\n\u2022 Configured the data mapping between Oracle and SQL Server and tested performance accuracy-related queries under SQL Server.\n\u2022 Extensively used the JDBC Prepared Statement to embed the SQL queries into the java code.\n\u2022 Implemented logging using log4j.\n\u2022 Used CVS for version control.\n\u2022 Tomcat Application Server was used for deploying the application.\n\u2022 Created SQL queries, PL/SQL Stored Procedures and Functions. Developed additional UI Components and implemented an asynchronous, AJAX (jQuery) based rich client to improve customer experience.\n\u2022 Built ANT scripts for automated deployment and for the build operation of the entire application.\n\u2022 Developed web pages using HTML5/CSS and JavaScript, Angular JS', u'Java Developer\nInfo Edge solutions - Hyderabad, Telangana\nMay 2009 to November 2011\nEnvironment: Java/J2EE, JSP, Servlets, SQL, JDBC, Eclipse, HTML, MS Office, Windows, AJAX, EJB, JMS, JPA annotations, Firefox, JavaScript, JMS, PL/SQL, Oracle 9i, TOMCAT, log4j, Exception Handling, collections, HTML, CSS.\n\nDeveloped many JSP pages, used Dojo in JavaScript Library, jQuery UI for client-side validation.\n\nDescription:Info Edge has an in-depth understanding of the Indian consumer internet domain. With years of experience in the domain, strong cash flow generation and a diversified business portfolio, it one of the very few profitable pure-play internet companies in the country.\n\nResponsibilities:\n\u2022 Analyzed the requirements, design documents and developed the Java, JSP, Struts Components.\n\u2022 Technical interaction withtheclient in requirement gathering and developing the project.\n\u2022 Developed Java modules, action classes and JSPs for retrieval of information from the error logs with different directories for different jobs\n\u2022 Involved in design and documentation of various process flows for services as well as for various class loading functionalities.\n\u2022 Involved in application level database programming using JDBC/SQL\n\u2022 Developed stored procedures for directly retrieving data from the Static data server, Trade server and Risk Server.\n\u2022 Used Putty to connect to the remote server in deploying the beans.\n\u2022 Involved in testing and integration of all the modules.\n\u2022 Created Java classes to communicate withthedatabase using JDBC.\n\u2022 Develop GUI related changes using JSP, HTML and client validations using Javascript.\n\u2022 Developed Java beans, helper classes and Servlets for interacting with UI written in JSP.\n\u2022 Used Java Messaging Services (JMS) and Backend messaging forthereliable and asynchronous exchange of important information such as payment status report.\n\u2022 Develop GUI related changes using JSP, HTML and client validations using Javascript.\n\u2022 Developed front-end using HTML, JSP, Struts Taglibs and Tiles.\n\u2022 More involved in writing SQL Packages, Stored Procedures, and functions.\n\u2022 Experience with Oracle Supplied Packages such as DBMS_SQL, DBMS_JOB, and UTL_FILE.\n\u2022 Developed PL/SQL triggers and master tables forthe automatic creation of primary keys.\n\u2022 Setup JDBC connectivity and databases.\n\nEnvironment: Java, JSP, Servlets, Struts, Oracle, Eclipse TOAD2.0, Ant, Windows.']","[u""Bachelor's""]",[u''],"degree_1 : ""Bachelors"""
0,https://resumes.indeed.com/resume/453f9948759e043e,"[u""Data Scientist\nFiat Chrysler Automobiles - Auburn Hills, MI\nAugust 2017 to Present\nDescription:\nFiat Chrysler Automobiles N.V. (FCA), incorporated on April 1, 2014, is an international automotive group. The Company is engaged in designing, engineering, manufacturing, distributing and selling vehicles, components and production systems. The Company's segments include regional mass-market vehicle segments, which include NAFTA , LATAM, APAC and EMEA, Maserati, its luxury brand segment, and a global components segment.\n\nResponsibilities:\n\u2022 Transformation of data using SSIS.\n\u2022 Build analytical solutions and models by manipulating large data sets.\n\u2022 A highly immersive Data Science program involving Data Manipulation&Visualization, Web Scraping, Machine Learning, Python programming, SQL, GIT, MongoDB, Hadoop.\n\u2022 Applied machine learning and statistical techniques to large datasets to find actionable insights.\n\u2022 Involved in complete Software Development Life Cycle (SDLC) process by analyzing business requirements and understanding the functional work flow of information from source systems to destination systems.\n\u2022 Played critical role in collecting data from different data sources and data system like SAP, JDE, Lubes, Hadoop, etc.\n\u2022 Created ETL packages using SSIS to extract data from relational database and then transform and load into the data mart.\n\u2022 Transforming and merging all the weekly client data into yearly file using ETL SSIS\n\u2022 Used Visual Team Foundation server for version control, source control and reporting.\n\u2022 KT with the client to understand their various Data Management systems and understanding the data.\n\u2022 Worked on different data formats such as JSON, XML and performed machine learning algorithms in Python.\n\u2022 Creating meta-data and data dictionary for the future data use/ data refresh of the same client.\n\u2022 Structuring the Data Marts to store and organize the customer's data.\n\u2022 Used pandas, numpy, seaborn, matplotlib, scikit-learn, scipy, NLTK in Python for developing various machine learning algorithms.\n\u2022 Mapping flow of trade cycle data from source to target and documenting the same.\n\u2022 Performing QA on the data extracted, transformed and exported to excel.\n\nEnvironment: Hadoop, Oracle 11g, MS Office, SSMS, SSIS, Power BI, Microsoft reporting tools, Big Data."", u""Data Scientist\nBecton Dickinson\nMay 2016 to July 2017\nDescription:\nBecton, Dickinson & Co. is a global medical technology company. The company is engaged in the development, manufacture and sale of medical devices, instrument systems and reagents used by healthcare institutions, life science researchers, clinical laboratories, the pharmaceutical industry and the general public.\nResponsibilities:\n\n\u2022 A highly immersive Data Science program involving Data Manipulation & Visualization, Web Scraping, Machine Learning, Python programming, SQL, GIT, Unix Commands, NoSQL, MongoDB, Hadoop.\n\u2022 Setup storage and data analysis tools in Amazon Web Services cloud computing infrastructure.\n\u2022 Used pandas, numpy, seaborn, scipy, matplotlib, scikit-learn, NLTK in Python for developing various machine learning algorithms.\n\u2022 Installed and used CaffeDeepLearning Framework\n\u2022 Worked on different data formats such as JSON, XML and performed machine learning algorithms in Python.\n\u2022 Utilized Spark, Scala, Hadoop, HQL, VQL, oozie, pySpark, Data Lake, TensorFlow, HBase, Cassandra, Redshift, MongoDB, Kafka, Kinesis, Spark Streaming, Edward, CUDA, MLLib, AWS, Python, a broad variety of machine learning methods including classifications, regressions, dimensionally reduction etc.\n\u2022 Worked as Data Architects and IT Architects to understand the movement of data and its storage and ER Studio 9.7\n\u2022 Participated in all phases of data mining; data collection, data cleaning, developing models, validation, visualization and performed Gap analysis.\n\u2022 Data Manipulation and Aggregation from different source using Nexus, Toad, Business Objects, Power BI and Smart View.\n\u2022 Implemented Agile Methodology for building an internal application.\n\u2022 Focus on integration overlap and Informatica newer commitment to MDM with the acquisition of Identity Systems.\n\u2022 Used MLlib, Spark's Machine learning library to build and evaluate different models.\n\u2022 Good knowledge of Hadoop Architecture and various components such as HDFS, JobTracker, Task Tracker, NameNode, DataNode, Secondary NameNode, and MapReduce concepts.\n\u2022 As Architect delivered various complex OLAP databases/cubes, scorecards, dashboards and reports.\n\u2022 Programmed a utility in Python that used multiple packages (scipy, numpy, pandas)\n\u2022 Implemented Classification using supervised algorithms like Logistic Regression, Decision trees, KNN, Naive Bayes.\n\u2022 Used Teradata15 utilities such as Fast Export, MLOAD for handling various tasks data migration/ETL from OLTP Source Systems to OLAP Target Systems\n\u2022 Experience in Hadoop ecosystem components like Hadoop MapReduce, HDFS, HBase, Oozie, Hive, Sqoop, Pig, Flume including their installation and configuration.\n\u2022 Updated Python scripts to match training data with our database stored in AWS Cloud Search, so that we would be able to assign each document a response label for further classification.\n\nEnvironment: regression, logistic regression, Hadoop, Teradata, OLTP, Unix, Python, MLLib, SAS, random forest, OLAP, HDFS, NLTK, SVM, JSON and XML."", u'Data Scientist\nHarvard Pilgrim Health Care - Boston, MA\nJanuary 2015 to April 2016\nDescription: Harvard Pilgrim Health Plan is a not-for-profit health services company based in the New England region of the United States.\n\nResponsibilities:\n\u2022 Review and determine risk profiles of data based on metadata and underlying data elements\n\u2022 Worked on Developing Fraud Detection Platform using various machine learning algorithms in Python\n\u2022 Worked on Linear discriminant analysis, Greedy Forward Selection, Greedy Backward Selection and Feature reduction algorithms like Principal Component Analysis (PCA) and Factor Analysis\n\u2022 Manipulating and Cleaning data using missing value treatment in Pandas and performed standardization\n\u2022 Implemented Classification using Supervised algorithms like Logistic Regression, Decision trees, KNN, NaiveBayes\n\u2022 Worked on customer segmentation using an unsupervised learning technique - clustering\n\u2022 Performed Exploratory Data Analysis and Data Visualizations using Python\n\u2022 Strong skills in data visualization like matplotlib and seaborn library\n\u2022 Create different charts such as Heat maps, Bar charts, Line charts etc.,\n\u2022 Experienced in working with SVM-Kernel method like RBF, polynomial, linear\n\u2022 Implemented Ensemble models like Boosting and Bagging\n\u2022 Worked with cross validation technique and grid search to improve project model results\n\nEnvironment:Python, SQL, Microsoft Excel', u'Data Analyst\nSprint corporation Inc - Overland Park, KS\nMay 2013 to December 2014\nDescription: Sprint Inc., incorporated June 19, 1899, is a holding company. The Company and its subsidiaries provide communications and digital entertainment services in the United States and the world. The Company operates through four segments: Business Solutions, Entertainment Group, Consumer Mobility and International\n\nResponsibilities:\n\u2022 Generated cost-benefit analysis to quantify the model implementation comparing with the former situation\n\u2022 Worked on model selection based on confusion matrices, minimized the Type II error\n\u2022 Worked on data cleaning and reshaping, generated segmented subsets using Numpy and Pandas in Python\n\u2022 Wrote and optimized complex SQL queries involving multiple joins and advanced analytical functions to perform data extraction and merging from large volumes of historical data stored in Oracle 11g, validating the ETL processed data in target database\n\u2022 Applied various machine learning algorithms and statistical modeling like decision tree, logistic regression, Gradient Boosting Machine to build predictive model using scikit-learn package in Python\n\u2022 Developed Python scripts to automate data sampling process. Ensured the data integrity by checking for completeness, duplication, accuracy, and consistency\n\u2022 Identified the variables that significantly affect the target\n\u2022 Continuously collected business requirements during the whole project life cycle.\n\u2022 Conducted model optimization and comparison using stepwise function based on AIC value\n\u2022 Generated data analysis reports using Matplotlib, Tableau, successfully delivered and presented the results for C-level decision makers\n\nEnvironment:SAP ERP, SCM APO GATP Software, User acceptance testing (UAT), Rational clear case, Clear quest, Use cases, UML, MS Office, Requisite Pro.', u""Data Modeler\nInfogain India Pvt Ltd - Noida, Uttar Pradesh\nDecember 2011 to April 2013\nDescription: Infogain Implements Oracle Retail Store Solutions for Healthcare Leader using Agile SCRUM ERP a Necessary Tool for Indian SMEs: CRO, Infogain The Infogain team will be in Las Vegas for the National Workers' Compensation and Disability Conference\xae & Expo, which brings together thousands of professionals.\n\nResponsibilities:\n\u2022 Worked with large amounts of structured and unstructured data.\n\u2022 Knowledge in Machine Learning concepts (Generalized Linear models, Regularization, Random Forest, Time Series models, etc.)\n\u2022 Worked in Business Intelligence tools and visualization tools such as Business Objects, Tableau, ChartIO, etc.\n\u2022 Deployed GUI pages by using JSP, JSTL, HTML, DHTML, XHTML, CSS, JavaScript, AJAX.\n\u2022 Configured the project on WebSphere 6.1 application servers\n\u2022 Implemented the online application by using Core Java, Jdbc, JSP, Servlets and EJB 1.1, Web Services, SOAP, WSDL.\n\u2022 Handled end-to-end project from data discovery to model deployment.\n\u2022 Monitoring the automated loading processes.\n\u2022 Communicated with other Health Care info by using Web Services with the help of SOAP, WSDLJAX-RPC\n\u2022 Used Singleton, factory design pattern, DAO Design Patterns based on the application requirements\n\u2022 Used SAX and DOM parsers to parse the raw XML documents\n\u2022 Used RAD as Development IDE for web applications.\n\u2022 Preparing and executing Unit test cases\n\u2022 Used Log4J logging framework to write Log messages with various levels.\n\u2022 Involved in fixing bugs and minor enhancements for the front-end modules.\n\u2022 Implemented Microsoft Visio and Rational Rose for designing the Use Case Diagrams, Class model, Sequence diagrams, and Activity diagrams for SDLC process of the application\n\u2022 Doing functional and technical reviews\n\u2022 Maintenance in the testing team for System testing/Integration/UAT.\n\u2022 Guaranteeing quality in the deliverables.\n\u2022 Conducted Design reviews and Technical reviews with other project stakeholders.\n\u2022 Was a part of the complete life cycle of the project from the requirements to the production support\n\u2022 Created test plan documents for all back-end database modules\n\u2022 Implemented the project in Linux environment.\n\nEnvironment: R 3.0, Erwin 9.5, Tableau 8.0, MDM, QlikView, MLLib, PL/SQL, HDFS, Teradata 14.1, JSON, HADOOP (HDFS), MapReduce, PIG, Spark, R Studio, MAHOUT, JAVA, HIVE, AWS"", u'Data Analyst\nPidilite Industries - IN\nMay 2009 to November 2011\nDescription: Pidilite Industries Limited is an Indian-based adhesives manufacturing company. It also sells art material, construction chemicals and other industrial chemicals. Pidilite markets the Fevicol range of adhesives.\n\nResponsibilities:\n\u2022 Performed data profiling in the source systems that are required for New Customer Engagement (NCE) Data mart.\n\u2022 Document the complete process flow to describe program development, logic, testing, and implementation, application integration, coding.\n\u2022 Manipulating, cleansing & processing data using Excel, Access and SQL.\n\u2022 Responsible for loading, extracting and validation of client data.\n\u2022 Liaising with end-users and 3rd party suppliers. Analyzing raw data, drawing conclusions & developing recommendations writing SQLscripts to manipulate data for data loads and extracts.\n\u2022 Developing data analytical databases from complex financial source data. Performing daily system checks. Data entry, data auditing, creating data reports & monitoring all data for accuracy. Designing, developing and implementing new functionality.\n\u2022 Monitoring the automated loading processes. Advising on the suitability of methodologies and suggesting improvements.\n\u2022 Involved in defining the source to target data mappings, business rules, and business and data definitions.\n\u2022 Responsible for defining the key identifiers for each mapping/interface.\n\u2022 Coordinate with the business users in providing appropriate, effective and efficient way to design the new reporting needs based on the user with the existing functionality.\n\u2022 Document data quality and traceability documents for each source interface.\n\u2022 Designed and implemented data integration modules for Extract/Transform/Load (ETL) functions.\n\u2022 Documented the complete process flow to describe program development, logic, testing, and implementation, application integration, coding.\n\u2022 Worked with internal architects and, assisting in the development of current and target state data architectures.\n\u2022 Worked with project team representatives to ensure that logical and physical ER/Studio data models were developed in line with corporate standards and guidelines.\n\nEnvironment:SQL/Server, Oracle10 &11g, MS-Office, Netezza, Teradata, Enterprise Architect, Informatica Data Quality, ER Studio, TOAD, Business Objects, Green plum Database, PL/SQL.']","[u""Bachelor's""]",[u''],"degree_1 : ""Bachelors"""
0,https://resumes.indeed.com/resume/491ce2b9fd89a49f,"[u""Data Scientist (Data Product Manager)\nFracta Inc - Redwood City, CA\nOctober 2016 to Present\nFracta is a SaaS company using machine learning to provide a solution to optimize water main replacement for U.S. water utilities. It was awarded second place in the Imagine H2O accelerator.\n\n\u2022 Drove the development of Fracta's automated wrangling software which improved the quality of utility water main and break\ndata, eliminating the need for human input and reducing data wrangling time by 80%\n\u2022 Defined need for an efficient method of data processing and geolocation after conducting customer discovery interviews with 5\nwater companies, incorporating this feedback into product requirements and roadmap\n\u2022 Led a cross-functional and cross-cultural team in the adoption of agile methodology (Kanban) which reduced communication\ncosts and allowed greater transparency between team members\n\u2022 Built automated water utility data wrangling prototype using Python, QGIS, and PostgreSQL\n\u2022 Spearheaded pivot away from robotics, saving $100K from being used for the acquisition of additional robotics technology"", u'Founder\nHangster - Stanford, CA\nSeptember 2014 to August 2016\nHangster revolutionizes the way friends make plans and meet up, matching free friends together based on availability and preferred activity. It reached thousands of users across 3 campuses and received mentions in TechCrunch and The New York Times.\n\n\u2022 Successfully raised $75K in angel funding after developing initial application prototype / minimum viable product\n\u2022 Conducted Voice of the Customer (VOC) interviews on 100s of users in order to develop list of key product requirements\n\u2022 Designed and iterated on application user interface and experience (UI/UX) based on insights gained from customer feedback\n\u2022 Implemented Mixpanel in order to track user interactions and analyze retention', u'Operations Intern\nRaytheon Space and Airborne Systems - El Segundo, CA\nJune 2014 to August 2014\n\u2022 Created environmental testing schedule using Microsoft Project as a part of the $15.92 billion E-2D Hawkeye Program\n\u2022 Wrote technical instructions for engineering technicians to conduct thermal testing of radar assembly\n\u2022 Designed vibration testing apparatus using PTC Creo Computer-aided design software']",[u'BS in Mechanical Engineering'],"[u'Stanford University Stanford, CA\nSeptember 2012 to June 2016']",degree_1 : BS in Mechanical Engineering
0,https://resumes.indeed.com/resume/503e06bf4948db4f,"[u'Data Scientist\nMassMutual Financial Group - Amherst, MA\nJanuary 2014 to Present\nInternal consultant to Massmutual and its subsidiaries. Consulting across several areas of the business, particularly with predictive modeling and interactive data visualization.\nDetailed achievements:\nA primary contributor to an Asset Retention project. Retained another $90 million of assets. Estimated annual value of over $400 million.\nHelped conduct an observational study of prescription drug history relationship to mortality.\nDesigned and implemented sales and distribution reporting interactive visualization.\nDesigned and implemented a parallelized Twitter crawler.\nUsed over 300 million tweets to develop a sentiment analysis model.', u""Business Analytics Consultant\nMassMutual Financial Group\nFebruary 2014 to August 2014\nSpringield, MA. Internal business consultant facilitating understanding and promoting action between internal\ndata science consultants and business stakeholders, or subsidiary brands.\nDetailed achievements:\nConsulted on investment and underwriting projects.\nLiaison between MassMutual's real estate team and external firm IDEO for the Society of Grownups.\nConsulted on brand and customer experience for Society of Gownups.""]",[u'Bachelor of Arts in Economics'],"[u'University of Connecticut Storrs, CT\nJanuary 2011 to January 2013']",degree_1 : Bachelor of Arts in Economics
0,https://resumes.indeed.com/resume/1cfc9d7e5917ca7b,"[u'Data Scientist\nSantander Bank - Holmdel, NJ\nJune 2017 to Present\nSantander Bank is based in Boston and its principal market is the northeastern United States. The Bank offers financial services and products including retail banking, mortgages, corporate banking, cash management, credit card, capital markets, trust and wealth management, and insurance.\n\nThe project was to build predictive models for the identification/detection of fraudulent transactions by applying machine learning methods, principle component analysis, and logistic regression on large dataset.\n\nResponsibilities:\n\u2022 Participated in all phases of data acquisition, data cleaning, developing models, validation, and visualization to deliver data science solutions.\n\u2022 Worked on fraud detection analysis on payments transactions using the history of transactions with supervised learning methods.\n\u2022 Collected data in Hadoop and retrieved the data required for building models using Hive.\n\u2022 Developed Spark Python modules for machine learning & predictive analytics in Hadoop.\n\u2022 Used Pandas, Numpy, Seaborn, Matplotlib, Scikit-learn in Python for developing various machine learning models and utilized algorithms such as Decision Trees, Logistic regression, Gradient Boosting, SVM and KNN.\n\u2022 Used cross-validation to test the models with different batches of data to optimize the models and prevent overfitting.\n\u2022 Used PCA and other feature engineering techniques for high dimensional datasets while maintaining the variance of most important features.\n\u2022 Created Transformation Pipelines for preprocessing large amount of data with methods such as imputing, scaling, selecting, etc.\n\u2022 Ensemble methods were used to increase the accuracy of the training model with different Bagging and Boosting methods.\n\nEnvironment:\nHadoop 2.x, HDFS, Hive, Pig Latin, Python 3.x (Numpy, Pandas, Scikit-learn, Matplotlib), Jupyter, GitHub, Linux', u""Data Analyst/Data Scientist\nSCIO Health Analytics - Hartford, CT\nApril 2015 to May 2017\nSCIO Health Analytics provides analytics solutions and services that turns data into actionable insights for health care providers in the United States and globally. Services also include medical and pharmacy claims auditing, inpatient data pursuits, care gaps closure, and commercial analytics.\n\nI was part of the team that worked with Subrogation claims of Healthcare Providers such as Humana. The objective was to load data, analyze, and provide monthly reports for the predictions on a claim's potential of a third-party recovery. Tableau and SSRS were used to build claim and recovery reports.\n\nResponsibilities:\n\u2022 Assembled a Predictive Modelling module by using supervised learning for Subrogation Claim Prediction to identify which claims would be classified as having Subrogation potential.\n\u2022 Implemented models such as Logistic Regression and Na\xefve Bayes, in Python using scikit-learn, to predict the claim potential outcome.\n\u2022 Dimensionality Reduction techniques applied to refine the attribute lists and feature selection applied to rank selected features to generate accurate results.\n\u2022 Gathered requirements and business rules from business users to implement Predictive Modelling.\n\u2022 Designed and developed ETL packages using SSIS to create Data Warehouses from different tables and file sources like Flat and Excel files, with different methods in SSIS such as derived columns, aggregations, Merge joins, count, conditional split and more to transform the data.\n\u2022 Designed reporting solutions for different stakeholders from mock-up till deployment in different areas such as Potential Subrogation claims, Monthly Revenue from Subrogation & Transactions.\n\u2022 Performed data visualization and designed dashboards with Tableau, and provided complex reports, including charts, summaries, and graphs to interpret the findings for Adjustors to view various claim information.\n\u2022 Optimized queries in T-SQL by removing unnecessary columns and redundant data, normalized tables, established joins and indices; developed complex SQL queries, stored procedures, views, functions and reports that meet customer requirements.\n\nEnvironment:\nPython 3.x (Scikit-learn, Matplotlib), Jupyter, SQL Server 2012, MS SQL Server Management Studio, MS BI Suite (SSIS/SSRS), T-SQL, Visual Studio BIDS, Tableau"", u'SQL BI Developer\nADP - Chennai, Tamil Nadu\nFebruary 2012 to March 2015\nADP is a leading provider of human resources management software and services worldwide.\n\nThis project was done for an internal business unit (ADP France) to comply with French statutory requirements for employee training. Goal was to develop a web-based SQL application built upon a baseline HRMS application to generate/support the Report development for training plans, budget preparation, cost tracking and 2483 reporting.\n\nResponsibilities:\n\u2022 Collected requirements from business users, and designed report models to meet business requirements.\n\u2022 Directed and managed meetings with clients, tracked document changes and ensured sign-off from clients.\n\u2022 Maintained and developed complex SQL queries, stored procedures, views, functions and reports that meet customer requirements using Microsoft SQL Server 2008 R2.\n\u2022 Created Views and Table-valued Functions, Common Table Expression (CTE), joins, complex subqueries to provide the reporting solutions.\n\u2022 Optimized the performance of queries with modification in T-SQL queries, removed the unnecessary columns and redundant data, normalized tables, established joins and created index.\n\u2022 Created, managed, and delivered interactive web-based reports to support daily operations.\n\u2022 Validated reports and resolve issues in a timely manner.\n\u2022 Developed and implemented several types of Reports (Training Reports, Schedules, Costs Summary Reports and Annual 2483 Report) by using features of SSRS such as sub-reports, drill down reports, summary reports and parameterized reports.\n\u2022 Designed and developed new reports and maintained existing reports for the Human Resource Management System Dashboards using Tableau, Qlikview and Microsoft Excel to support the business strategy and management.\n\u2022 Identified process improvements that significantly reduce workloads or improve quality.\n\nEnvironment:\nSQL Server 2008 R2, MS SQL Server Management Studio, SSRS, T-SQL, Visual Studio BIDS, Tableau, Qlikview']",[u'Master of Science in Business & Information Systems'],"[u'New Jersey Institute of Technology Newark, NJ']",degree_1 : Master of Science in Bsiness & Information Systems
0,https://resumes.indeed.com/resume/da965b49e8b5aa68,"[u'Data Scientist\nComcast Business - Philadelphia, PA\nJanuary 2018 to Present\nImplemented Spark using Scala, and Spark SQL API for faster testing and processing data.\nInvolved in converting Hive/SQL queries into Spark transformations using Spark RDDs, Scala.\nCombine and synthesize data from multiple sources\nPerform time-series analyses, hypothesis testing, and causal analyses to statistically assess relative impact and extract trends\nTools: Python(Scikit, Pandas, Matplotlib), MySQL, Apache Spark, Hadoop, Hive .', u'Graduate Teaching Assistant\nOklahoma State University\nJanuary 2017 to Present\nActively involved in Research work, Grading and Proctoring for the subject.', u""Data Scientist Intern\nComcast Business - Philadelphia, PA\nMay 2017 to August 2017\n\u2022 Developed Automated Model-Enhancement Code as a subpart of a complete End-to-End Modelling process using Apache Spark, Pyspark for handling BigData (Participated in the Hackathon event to present my product to the Senior Leadership Team)\n\u2022 Developed a Binary classification model to predict premium channel upgrade for comcast video customers in the next 90 days using Python and Pyspark based on RDD's for better efficiency.\n\u2022 Worked on scoring and validating already developed models using Inhouse tool UC4 for stimulation study of tool"", u'Jr. Data Analyst\nApsidata Solutions Pvt Ltd\nMarch 2015 to July 2016\n\u2022 Worked on CBIR algorithm/Image analysis using medical Imagedata. Special emphasis on clustering techniques and classification procedures using MATLAB, SAS.\n\u2022 Developed churn rate prediction models of a telecom company using logistic regression and determining the socio-economic factors that were affecting the business using R, SAS.\n\u2022 Market Survey Research Data Analysis (7K variables) and complete automation for the dash boarding using Tableau, SAS, MS-Excel.']","[u'MS in Business Analytics', u'B.Tech in Electronics and Communications']","[u'Oklahoma State University Stillwater, OK\nJune 2016 to May 2018', u'Uttar Pradesh Technical University Noida, Uttar Pradesh\nJune 2011 to May 2015']","degree_1 : MS in Bsiness Analytics, degree_2 :  B.Tech in Electronics and Commnications"
0,https://resumes.indeed.com/resume/61fccbd166f2198c,"[u'Teaching Assistant\nStevens Institute of Technology - Hoboken, NJ\nFebruary 2018 to Present\n\u2022 Assist with marking and correcting work\n\u2022 Provide support and guidance to students', u'Data Scientist intern\nSirius XM Radio Inc - New York, NY\nJune 2017 to December 2017\n\u2022 Process large dataset and obtain necessary data with apache drill for requests from marketing departments\n\u2022 Generate large scale data automatically for visualization and reporting with Tableau\n\u2022 Collect and clean audio data to improve data quality in the preprocessing phase\n\u2022 Assist with employing Random Forest Machine Learning for the silence detection system with Java\n\u2022 Investigate 24 TB BI data during Exploratory Data Analysis with R and Apache Drill\n\u2022 Ingest batch metadata from an external party, preprocess the categorical data into numeric data with information theory\n\u2022 Use ANN to cluster millions of music tracks and check the clustering accuracy with different metrics\n\u2022 Create an interactive web app with Shiny which could search for features and provide basic information in datasets\n\u2022 Ingest streaming data with Kafka, process data with Spark and save results into Hadoop Distributed File System', u'Project Assistant\nHopefluent Group Holdings Limited\nJuly 2015 to July 2015\n\u2022 Communicated with sellers, clients and co-workers to ensure real estate projects ran smoothly\n\u2022 Collected data about competitors and summarized data about projects to keep track of the changeable market\n\u2022 Analyzed data to offer improvement solution plans to increase profits\n\u2022 Wrote reports and created presentation materials for senior management\n\u2022 Directed and supervised workers to promote improvement plans that attracted more customers to projects']","[u'Master of Business Intelligence and Analysis in Data Science', u'Bachelor of Land Resources Management in Real Estate Development/Property Management']","[u'Stevens Institute of Technology Hoboken, NJ\nMay 2018', u'South China Agricultural University Guangzhou, CN\nJune 2015']","degree_1 : Master of Bsiness Intelligence and Analysis in Data Science, degree_2 :  Bachelor of Land Resorces Management in Real Estate Development/Property Management"
0,https://resumes.indeed.com/resume/0ddd16c176a659b5,"[u'MDM Solution Architect, Data Governor\nVerizon Telematics\nMay 2014 to Present\nArchitect data driven advance marketing campaign application for various telematics offerings, analytics, and strategy. MDM application, data profiling, data lineage, data governor, and data steward performance for the data HUB. Used data security analyzer for encryption/decryption of cloud based campaign data repository. Subsequently, contributed to design marketing campaign automation, and segmentation based on MDM ecosystem.\n\nEloqua, ADOBE Campaign Manager used the ingested data from Siebel, Salesforce, and Teradata to maintain MDM ecosystem; design market segmentation, marketing campaign; predictive, and forecasting model; and delivered Tableau centered visualization, and reporting system.\n\nSoftware\nOracle, Teradata, ERWIN, Informatica (Powercenter, IDD, IDQ, and MDM), OBIEE, Siebel, Eloqua, ADOBE Analytics, ADOBE Campaign, Tableau.', u""Enterprise Data Architect, Solution Architect, Analyst, and Data Scientist\nNirvanatech, Inc\nJanuary 2004 to April 2014\nFor this consortium, over 10+years, worked for five different projects in five different line of businesses. They were, Macy's Technologies (Retail system), STIBO (PIM software vendor), John Hancock (Finance, Insurance), Alere Healthcare (Managed care), and CDC (Public healthcare). Accomplishments and contributions for each project are listed below.""]","[u'MS in Software Engineering', u'MBA in MIS', u'MBA in Finance and Accounting', u'in Certified Management Accountant']","[u'Rochester Institute of Technology Rochester, NY', u'University of Houston Houston, TX', u""Memorial University of Newfoundland St. John's, NL"", u'ICWAI']","degree_1 : MS in Software Engineering, degree_2 :  MBA in MIS, degree_3 :  MBA in Finance and Acconting, degree_4 :  in Certified Management Accontant"
0,https://resumes.indeed.com/resume/5a2ed62bef553dd1,"[u""Data Scientist Intern\nPublicis Media Group - Wuhan, CN\nJune 2017 to August 2017\n- Integrated and preprocessed data in Python('re', 'jieba', 'zhon') for FOX movies comments from 17 mainstream social media websites (600k records for 5 movies per week, 88M)\n- Built model to classify sentiments using labeled dataset (Na\xefve Bayesian, Logistic Regression, LSTM) and get the accuracy of 75%, extracted 7 kinds of topics with exist topic dataset, and counted most frequent keywords\n- Generated report in terms of sentiment, topic, resource website, keywords, date."", u'Data Scientist Intern\nNeutron Mobile Inc - Monterey Park, CA\nJune 2016 to August 2016\n- Analyzed 2014 Lending Club long-term loan data (235k records about personal financial & loan status, 37M), EDA (plot histogram and box-plot to detect outlier) and select closed long-term loan data (24 or 36 months)\n- Conducted feature validation (remove invalid or useless), engineering & cleaning (fill n/a, ordinal feature encoding (e.g. grade to number), high cardinality feature encoding (e.g. zip code, state, employment title to frequent encoding), one hot encoding (e.g. term, home_ownership, verification_status, purpose))\n- Built underwriting model(using every last season as test) with XGBoost and got AUC train/test 0.88/0.74\n- Validated model and solved data leakage problem and got final AUC 0.81/0.72']","[u'M.S. in Analytics', u'M.S. in Structural Engineering', u'B.E. in Civil Engineering']","[u'University of Chicago Chicago, IL\nSeptember 2016 to December 2017', u'Northwestern University\nJanuary 2015 to January 2016', u'South China University of Technology\nJanuary 2011 to January 2015']","degree_1 : M.S. in Analytics, degree_2 :  M.S. in Strctral Engineering, degree_3 :  B.E. in Civil Engineering"
0,https://resumes.indeed.com/resume/f5815efd7e72a2d3,"[u""Research Assistant\nIllinois Institute of Technology Chicago Chicago - Chicago, IL\nAugust 2017 to February 2018\nUnder Dr. Irina Matveeva, IIT Chicago\n\u25cf Performed a Text Analysis of Federal Reserve meeting minutes using LDA and LSA methods.\n\u25cf Published the outcomes in the applied track of ACM SIGKDD'18 conference"", u""Data Scientist\nIllinois Institute of Technology - Chicago, IL\nAugust 2017 to December 2017\nAutomated the data collection process from USA's news media websites and ranked those websites based on the net positivity of the words in the article."", u""Data Scientist\nIllinois Institute of Technology - Chicago, IL\nAugust 2017 to December 2017\nPerformed community detection on certain twitter accounts using its API and developed a heuristic to identify whether the tweet is related to 'Data science' tweets using the SVM, supervised learning model"", u'Data Scientist\nIllinois Institute of Technology - Chicago, IL\nJanuary 2017 to May 2017\nBuilt a Decision tree classifier with an accuracy of 81.29 percent to predict the crime rate based on demographic and economic information of the locality obtained from the city of Chicago data portal.', u'SQL Developer\nDatabase for Online Grocery - Chicago, IL\nAugust 2016 to December 2016\nCreated a ER schema for an online grocery store application to store information about products, availability of products in the stock, and customers of the store and executed it through SQL.', u'Python Engineer\nExperdex Pte Ltd India\nJuly 2015 to July 2016\n\u25cf Built a model to collect and store data from the Fitbit API to track the employees activities\n\u25cf Brainstormed with the Team Head to launch a prototype version of food recommender based on the users\nactivity tracker data.', u'Junior Design Engineer\nVaata Infra Ltd India\nJune 2014 to April 2015\n\u25cf Accelerated the load planning design to increase the shipping of turbine modules from 6 to 10 per container\n\u25cf Collaborated with the Construction Engineer and the Research and Development team manager, to deliver a fully\nconstructed Concrete-Tower on time.']","[u'in Architecture', u'']","[u'Illinois Institute of Technology Chicago, IL\nSeptember 2016 to March 2018', u'National Institute of Technology Tiruchchirappalli, Tamil Nadu\nJanuary 2010 to January 2014']","degree_1 : in Architectre, degree_2 :  "
0,https://resumes.indeed.com/resume/aebfc01cbd34056e,"[u""Data Scientist\nArgus Information & Advisory Services, LLC - White Plains, NY\nSeptember 2016 to Present\nArgus Information and Advisory Services is a Subsidiary of Verisk Analytics Company and the leading provider of analytics, information and solutions to consumer banks and their regulators. The company's clients range from financial institutions to retailers and tech companies. The project focused on detecting anti-money laundering violation using Big Data and Data Science tools and improving customer's transaction monitoring system.\nResponsibilities:\n\u2022 Collected and analyzed the business requirements, understood the particular Fraud/AML challenges that our client faces.\n\u2022 Participated in Data integration job with Data Engineer team to gather traditional transaction data and external source data together.\n\u2022 Transformed data from SQL Server database to Hadoop Clusters which is set up by using AWS EMR.\n\u2022 Conducted data cleansing and feature engineering job through python NumPy and Pandas.\n\u2022 Implemented Naive Bayes, Logistic Regression, SVM, Random Forest and Gradient boosting with weighted loss function by using Python Scikit-learn.\n\u2022 Implemented mulit-layers Neural Networks by using Google Tensorflow and Spark.\n\u2022 Performed extensive Behavioral modeling and Customer Segmentation to discover behavior patterns of customers by using K-means Clustering.\n\u2022 Managed and scheduled models by using Oozie for batch processing.\n\u2022 Updated and saved Fraud predictions to AWS S3 for application team.\n\u2022 Tested the business performance of the AML models by evaluating detection rate and false positive rate and worked on continuous improvement on model.\n\u2022 Created reports and dashboards, by using Tableau, to explain and communicated data insights, significant features, model's score and performance of new transaction monitoring system to both technical and business teams.\n\u2022 Used GitHub for version control with Data Engineer team and Data Scientists colleagues.\nEnvironment: SQL Server 2014, Hadoop 2.0, Hive 2.0, Spark (PySpark, SparkSQL), Python 3.X, Tensorflow, Oozie 4.2, Tableau 10.X, AWS S3/EC2/EMR, Github"", u'Data Scientist\nCenterLight Health System - Bronx, NY\nApril 2015 to July 2016\nCenterLight Health System, a not-for-profit organization, has evolved into a leader in serving the elderly, chronically ill and disabled. CenterLight is one of the largest long-term care providers in New York State, serving all of New York City, Westchester, Nassau, Rockland and Suffolk Counties. This project aimed to predict the billing cycles and accounting related issues to increase the efficiency of enterprise claim processing.\n\nResponsibilities:\n\u2022 Conducted reverse engineering based on demo reports to understand the data without documentation.\n\u2022 Generated new data mapping documentations and redefined the proper requirements in detail.\n\u2022 Generated different Data Marts for gathering the tables needed (Member info, Claim info, Transaction info, Appointment info, Diagnose info) from SQL Server Database.\n\u2022 Created ETL packages to transform data into the right format and join tables together to get all features required using SSIS.\n\u2022 Processed data using Python pandas to examine transaction data, identify outliers and inconsistencies.\n\u2022 Conducted exploratory data analysis using python NumPy and Seaborn to see the insights of data and validate each feature through different charts and graphs.\n\u2022 Built predictive models including Linear regression, Lasso Regression, Random Forest Regression and Support Vector Regression to predict the claim closing gap by using python scikit-learn.\n\u2022 Used GridSearchCV to evaluate each model and to find best parameters set for each model.\n\u2022 Created reports and an app demo using Tableau to show client how prediction can help the business.\n\u2022 Deployed and hosted our models by using Azure Machine Learning Studio and share an API with application development team.\n\u2022 Used Confluence to share and collaborate on projects with team members, and keep track of up to date documentations.\n\nEnvironment: SQL Server 2012, SQL Server Data Tools 2010, SQL Server Integration Services, Python 2.7/3.3, Tableau 9.4, Azure Machine Learning Studio', u""Junior Data Scientist\nAtlantic Health - Morristown, NJ\nJanuary 2014 to March 2015\nAtlantic Health System is one of the leading non-profit health care systems in New Jersey, providing a wide array of health care services to the residents of Northern and Central regions of the state as well as Pike County, PA, and southern Orange County, NY. Project was to build a predictive model to predict the readmission case. The main objective was to reduce the risk of being wrongly diagnosed and the risk of being involved in the legal disputes.\n\nResponsibilities:\n\u2022 Communicated and coordinated with other departments to gather business requirements.\n\u2022 Gathered data information from multiple sources, and performed resampling method to handle the issue of imbalanced data.\n\u2022 Worked with ETL Team and Doctors to understand the data and define the uniform standard format.\n\u2022 Conducted data cleansing by using advanced SQL queries in SQL Server Database.\n\u2022 Split the data into different smaller dataset based on different diagnoses, in charge of conducting exploratory data analysis for three of diagnoses datasets (Diabetes, cold/flu, allergy).\n\u2022 Created the whole pipeline of data preprocessing (imputing, scaling, label encoding) through python pandas to get data ready to modeling part.\n\u2022 Built predictive models, using python scikit-learn, including Support Vector Machine, Decision tree, Naive Bayes Classifier, Neural Network to predict a potential readmitted case.\n\u2022 Performed Ensemble methods, including Gradient Boosting, Random Forest, customized ensemble method to produce more accurate solutions.\n\u2022 Designed and implemented cross-validation and statistical tests including Hypothesis testing, AVOVA, Chi-square test to verify models' significance.\n\u2022 Created a API by using Flask and shared the idea with application team and help them define the requirements of new application.\n\u2022 Used Agile methodology and Scrum process for project developing.\n\nEnvironment: SQL server 2012, SQL Server Integration Services, Python 2.7, Jupyter notebook, Flask 0.10, SharePoint 2013"", u'BI Developer\nFulton Financial Corporation - Lancaster, PA\nDecember 2012 to October 2013\nFulton is a financial company based in Lancaster, Pennsylvania. They provide a wide range of financial products and personalized services in Pennsylvania, Maryland, Delaware, Virginia and New Jersey. They are comprised of several different banking subsidiaries. The main job of this project was to provide ETL solutions for data migration and provide data quality and micro strategy solutions.\n\nResponsibilities:\n\u2022 Involved in gathering user/project requirements from business users and IT managers, translated it into functional and non-functional specifications needed and created documentations for the project.\n\u2022 Assisted in design and data modeling efforts of Data Marts and Enterprise Data Warehouse.\n\u2022 Used T-SQL in SQL Server to develop complex stored procedures, triggers, clustered index & non-clustered index, Views, and User-defined Functions (UDFs).\n\u2022 Designed SSIS packages to extract, transform and load existing data into SQL Server, used lots of components of SSIS, such as Pivot Transformation, Fuzzy Lookup, Derived Columns, Condition Split, Aggregate, Execute SQL Task, Data Flow Task and Execute Package Task.\n\u2022 Created SSIS Packages that involved dealing with different source formats (Text files, XML, Database Tables)\n\u2022 Debugged and troubleshot the ETL packages by using breakpoint, analyzing process, catching error information by SQL command in SSIS.\n\u2022 Create reports with the use of SSRS to generate different types of reports such as tabular, matrix, drill down and charts reports with accordance with user requirement.\n\u2022 Maintained and updated existing reports, analyzed the SQL queries and logic behind them to improve the performance.\n\u2022 Helped deploy the report with scheduling, subscription, history snapshot configured and set up.\n\u2022 Developed in Agile environment throughout the project.\n\nEnvironment: SQL server 2008/2012, SQL Server Management Studio (SSMS), MS BI Suite (SSIS, SSRS)']",[u'Master of Science in Electrical Engineering'],[u'Stevens Institute of Technology'],degree_1 : Master of Science in Electrical Engineering
0,https://resumes.indeed.com/resume/fe819f41b1ad0fdf,"[u'Sr. Data Scientist\nEli Lilly and Company - Indianapolis, IN\nJanuary 2016 to Present\nEli Lilly and Company is an American global pharmaceutical company with annual revenue of over $28 billion. Lilly is considered in the top 10 pharmaceutical companies across the globe for pharma sales. Being a pharma company, most of the statisticians deal with the clinical trial data for different types of drugs development. Worked on several projects with Machine Learning approach as well as Bayesian approach (Automatic Diabetic Retinopathy Detection, BATMAN, Prior-Elicitation, Referral network, Treatment Specify Develop Tool (typical subgroup problem))\n\nAutomatic Diabetic Retinopathy Detection\nDiabetic retinopathy is an eye disease affecting diabetes suffers. It has few early-warning signs and left untreated it can cause blindness. Regular screening is recommended for at risk patients. The current standard is manual grading of retinal fundus images. However, there is a shortage of trained professionals and very high variability. If ophthalmologist graders do manual grading on retinal images only 60-65% of consistency can be achieved.\nAutomatic detection of diabetic retinopathy could have benefits increasing diagnostic availability to patients, decreasing human variability in outcomes, and lowering the time for diagnotic analysis. Applied the task automating diabetic retinopathy grading using a neural network model. Neural networks are powerful models that can be trained to perform a wide variety of tasks. They are well suited to leanring from large amounts of data like images. This network is based on a convolutional neural network architecture and trained on over 30,000 hand-labeled images. Used TensorFlow, Keras and Python tools for implementation.\nResposibilities:\n\u2022 Went through data cleaning and manipulation phase on labeled and unlabeled image data set.\n\u2022 Handled unbalanced data set problem such as models were not learning and label imbalance issues.\n\u2022 Several resampling methods were implemented.\n\u2022 Overfitting issue was present when model failed to generalize using resampled data. Data augmentation, batch normalization, 12 norm, dropout helped to overcome this issue.\n\u2022 Resnet algorithm was used which uses smaller network\n\u2022 Used Keras for implementation and trained using cyclic learning rate schedule.\n\u2022 Using cyclic learning rate automatic schedule was implemented for training in three cycles for about 20 hours of training time.\n\u2022 Accuracy, Kappa, precision and F1 score were calculated for comparing the results of four different algorithms: Na\xefve, Resampled, weighted and Resnet\n\u2022 About 80% accuracy was achieved using Resnet\nEnvironment: TensorFlow, Keras, Python, HPC\n\nTreatment Specific subgroup Detection:\n\u2022 The objective of this project is to evaluate treatment effect on the overall population and identify subgroups that have significantly better or worse outcome than the overall population\n\u2022 I was involved with prototyping machine learning algorithms for POC(Proof of Concept) , clinical data pooling.\n\u2022 Implemented ensemble learning using decision trees, random forest.\n\u2022 Sampling the data set, splitting based on categories and building tree-based methods to identify preliminary Subgroups.\n\u2022 Aggregating over the similar preliminary sub groups and calibrating to classify subgroup\n\u2022 Evaluated model performance by creating a confusion matrix. Used precision, recall and F1 score for accuracy.\n\nText classification:\n\u2022 Used Topic Modelling to build categories that accurately capture scientific documents in therapeutic department categories, using the data available on all historical articles and other available variables from discrete applications.\n\u2022 Performed tokenization, stop word removal, Lexicon Normalization, Stemming, and tf-idf.\n\u2022 Text to features (Featured engineering on text data) by performing SVD\n\u2022 Perform topic modeling (latent semantic analysis - LSA, Probabilistic Latent Semantic Analysis-PLSA) to identify topics from clustered documents.\n\nBATMAN: involved the development of tool for performing Bayesian Network Meta-Analysis in a regulated environment. The tool has a capability of analyzing most of the generalized linear modeling framework for pairwise and network meta-analysis of randomized controlled trials. Lilly internal R package is been developed in agile scrum environment. The tool is been used by most of the statisticians in the third phase of a drug development life cycle. This project helps to save approximately 6 months of the drug development process, which can help to reduce the time frame for patent filing and eventually in billions of profits.\nOne of the other projects I worked on involved building a referral network shiny app using neural networks. This project was initiated to help marketing team to push Lilly product to key leaders in the regions. This helped Lilly to improve their sales by 60%. Same project is been used to increase sales of different products.\n{More details cannot be provided due to company policies and confidentiality purposes}\n\nResponsibilities:\n\u2022 Went through the data cleaning and manipulation phase on clinical trial data sets of different drugs\n\u2022 R functions were written for data piping and manipulation before data was feed inside The Bayesian models for meta-analysis\n\u2022 Included several likelihood models such as Normal, Binomial, TTE, cLogLog, survival models, Poisson etc.\n\u2022 Data parsing was done using DOCOPT package.\n\u2022 MCMC sampling was implemented using JAGS sampler.\n\u2022 WINBUGS code was included for data processing and model implementation\n\u2022 Several visualizations (density plots, forest plots, leverage plots, network plots, covariant adjustment plots etc) were made using packages such as GGPLOT2, GGMCMC, animation etc\n\u2022 Customized reports and presentations were generated autonomously using tool for different models using r packages e.g. rmarkdown, animation, knitr, ReporteRs etc\n\u2022 Eventually everything was put in a package for Lilly internal use.\n\u2022 Tool was tested under system testing and user acceptance testing in a regulated environment.\nEnvironment: R, Matlab, HPC, Java Script, JAVA, SQL, C++\n\nPrior Elicitation Tool: An essential element of Science Driven Adaptive Program is broad technical knowledge and computational ability to elicit prior distributions via modeling/leveraging data and/or subjective elicitation of expert opinion. These priors are key inputs into trial and program simulation tools, informing trial design/clinical planning. Along with the designs and decision rules, the priors elicited play a key role in estimating the probability of study success for various clinical plan options and are essential for optimizing trial designs. These priors also help make and inform clinical development decisions by understanding the probability of the drug working.\n\nResponsibilities:\n\u2022 Created a shiny dashboard app with a capability of saving different sessions.\n\u2022 These sessions can be saved and reactivated for later use.\n\u2022 Worked on an internally developed package ""GLMCMP""\n\u2022 Integrated jqPlots charts and graphs for jQuery within shiny for drag gable plots.\n\u2022 added a capability to add up to 500 single/reverse distributions using modules.\n\u2022 Summary reports were made in different formats using custom plots.\nEnvironment: R, Shiny, java Script, jQuery,', u""Principal Data Scientist\nLowe's - Mooresville, NC\nMarch 2015 to August 2015\nWith over 100 million customers and annual revenue of $55 billion, Lowes generates a huge amount of data regarding its customers both who shop in store and online. The scope of the project is to use the customer's transaction data in store and online over a period to recommend items to customers; that they will find engaging and improve their experience with Lowes at the same time encourage them to buy and try products. Further, the recommender engine introduces new products to a customer, which they may not have seen before.\nIn order to achieve these dual goals we performed data exploration to learn more about users unique behavior .We performed feature engineering to create new features from existing features that truly reflect the signals in the data and avoid noise. These features were used to create utility matrix. Later on we use Collaborative Filtering with Latent Factors model to create recommender engine that achieves scalability, accuracy & speed.\n\nResponsibilities\n\u2022 Performed Data Profiling to learn about user behavior\n\u2022 Merged user data from multiple data sources\n\u2022 Performed Exploratory Data Analysis using R and Hive on Hadoop HDFS\n\u2022 Prototype machine learning algorithm for POC (Proof of Concept)\n\u2022 Performed Data Cleaning, features scaling, features engineering,\n\u2022 Developed novel approach to build machine learning algorithm and implement it in production environment\n\u2022 Performed ad-hoc data analysis for customer insights using Hive\n\u2022 Developed Performance metrics to evaluate Algorithm's performance\n\u2022 Used RMSE score, F-SCORE, PRECISION, RECALL, A/B testing to evaluate recommender's performance in both simulated environment and real-world.\n\u2022 Fine tune the algorithm using regularization term to overcome the problem of over fitting\nEnvironment: TERADATA, Oracle, HADOOP (HDFS), PIG, MySQL, RStudio, Python, JAVA, MAHOUT, HIVE, PIG, SPARK"", u'Data Scientist\nNorthern Trust Corporation - Chicago, IL\nAugust 2014 to March 2015\nWorked on, multiple projects to leverage statistical learning/machine learning algorithms to automate Alternate Asset Servicing. The automation helped NT to reduce errors and improve operational efficiency. Further, developed BI reports that provided predictive analytics & Reporting with dashboards on analyst performance, client activity, future workload & anomalies based on data collected from discrete applications.\nResponsibilities\n\u2022 Defined Project Scope, project Charter & Business Case\n\u2022 Prototype machine learning algorithm for POC (Proof Of Concept)\n\u2022 Performed Data Cleaning, features scaling, features engineering,\n\u2022 Developed predictive models for use in machine learning platform using the scikit-learn python framework\n\u2022 Improved statistical models using learning curves, parameter curves, feature selection, and regularization.\n\u2022 Performed ad-hoc data analysis for customer insights using SQL using Amazon AWS Hadoop cluster\n\u2022 Developed MapReduce pipeline for feature extraction\n\u2022 Implemented Support Vector Machine (lite)\n\u2022 Performed Principal Component Analysis (PCA) & Linear Discriminate Analysis(LDA)\n\u2022 Fine-tuned low bias & High variance trade off \u2022 Defined the technical requirements of the analytic solutions.\n\u2022 Defined the data requirements of the analytic solution.\n\u2022 Worked on commercial data from desperate source systems, built data models and transformed data to provide added value in IT applications by streamlining processes, reducing cost , maximizing profits & rolling out business solutions that met one of the objectives\n\u2022 Worked closely with subject-matter experts and business analysts from SAP and non-SAP systems and platforms, and investigating statistical and predictive and prescriptive patterns in the data to build business solutions\n\u2022 Made iterative changes to analytic/predictive models and decision logic embedded in operational applications and business process platforms\n\u2022 Worked with multiple relational, dimensional, and OLAP databases\nEnvironment: MS SQL, Oracle, HADOOP (HDFS), PIG, MySQL, SAP Sybase, RStudio, Python, JAVA,.NET, HIVE, HADOOP HDFS, PIG, MAHOUT', u'Data Scientist\nAllstate - Northbrook, IL\nJanuary 2013 to August 2014\nThe project was to build smart categories that accurately capture customer interactions. Based on, the existing data available on all historical conversations and other available variables from discrete applications. Further, in 2nd phase of the project we developed a learning algorithm that process all events and interactions between Allstate and its customer and provide a one page story about a customer. This write up helps Allstate agents & customer service partner to enrich customer experience\nResponsibilities:\n\u2022 Data analysis and visualization (Python, R,)\n\u2022 Designed, implemented and automated modeling and analysis procedures on existing and experimentally created data\n\u2022 Increased pace & confidence of learning algorithm by combining state of the art technology and statistical methods; provided expertise and assistance in integrating advanced analytics into ongoing business processes\n\u2022 Parsed data, producing concise conclusions from raw data in a clean, well-structured and easily maintainable format\n\u2022 Implemented Topic Modelling, PASSIVE AGGRESSIVE & other linear classifier models\n\u2022 Perform tfidf weighting, normalize\n\u2022 Performed scheduled and adhoc data driven statistical analysis, supporting existing processes\n\u2022 Developed clustering models for customer segmentation using R\n\u2022 Created dynamic linear models to perform trend analysis on customer transactional data in R\n\u2022 Performed Topic modeling\nEnvironment: R, SQL, Python, TABLEU, SAP HANA, SAS, JAVA, PCA & LDA, regression, logistic regression, random forest, neural networks , Topic Modeling, NLTK, SVM(Support Vector Machine), JSON, XML, HIVE, HADOOP, PIG, MAHOUT', u""Data Scientist\nBank Of America - Princeton, NJ\nJanuary 2012 to December 2012\nThe project was to build an algorithm that accurately classifies credit card holders among multiple classes based on the historical data available on multiple variables. Further, the aim was to improve bank's efficiency by reducing default rate while offering new products.\nMoreover, I was Involved in a project to identify the employees' access level, based on his/her current & historical tasks and duties.\nResponsibilities:\n\u2022 Responsible for predictive analysis of credit scoring to predict whether or not credit extended to a new or an existing applicant will likely result in profit or losses.\n\u2022 Primarily used R packages for the data mining tasks.\n\u2022 Participated in all phases of data mining; data collection, data cleaning, developing models, validation and visualization.\n\u2022 Data for modeling was collected using SQL by querying several tables. The extracted tables were further appended or merged to create tables for modeling using SAS PROC MERGE and PROC SET procedures.\n\u2022 Adopted principal component analysis, PROC PRINCOMP to reduce the dimension of the data. The missing values were replaced if applicable with the group average using proc means.\n\u2022 Computed Credit Risk Parameters such as Probability of Default (PD) and Loss Given Default (LGD) and Exposure at Default (EAD).\n\u2022 Used logistic regression (PORC LOGISTIC), clustering (PROC CLUSTER) and multivariate modeling to provide valuable analytical insights.\n\u2022 Used Kolmogorov-Smirnov test (K-S test or KS test) to measure the quality of the models.\n\u2022 Used PROC GRAPH for generating various graphs and charts for analyzing the different features.\n\u2022 Used k-fold cross validation to avoid over fitting.\nEnvironment: .Net, SAS, R, Python, Oracle, IBM DB2, MS SQL, HIVE, HADOOP, PIG, MAHOUT""]","[u""Bachelor's""]",[u''],"degree_1 : ""Bachelors"""
0,https://resumes.indeed.com/resume/5b25a89c6814510d,"[u'Data Scientist\nAlbeado,Inc. - Santa Clara, CA\nOctober 2017 to Present\n\uf06c Diving in millions of health insurance claims and developing fraud\ndetection engine for preventing fraud, waste and abuse\n\uf06c Real-time manufacturing data exploration and visualization for monitory\nanalysis\n\uf06c Building intermittent time series forecasting models for inventory\noptimization', u'Assistant researcher\nShanghai Jiao Tong University - Shanghai, China\nNovember 2013 to June 2016\n\uf06c Responsible for physical/mathematical model design, numerical simulation\nand data analysis and interpretation.\n- representative work ""Plasma optical modulators for intense lasers"" was\npublished in one of the world\u2019s top academic journals Nature\nCommunications.\n- representative work ""Control of focusing fields for positron acceleration in\nnonlinear plasma wakes using multiple laser modes"" was highlighted by\nPhysics of Plasmas and was one of the most-accessed articles from this\njournal published from October to December 2014.\n\uf06c Responsible for project coordination and funding acquisition\n\uf06c Teaching and supervising undergraduate and graduate students', u'Postdoctoral Researcher\nUniversity of California - Berkeley, CA - Berkeley, CA\nJuly 2012 to June 2013\n\uf06c Working in one of the world\u2019s top research group of laser plasma\naccelerators at the Lawrence Berkeley National Laboratory. Responsible for\nphysical/mathematical model design, numerical simulation\nand data analysis and interpretation.\n- representative work ""Two-color laser-ionization injection"" was published\nin the most prestigious journal in the field of physics Physical Review\nLetters.']","[u'PhD in Physics', u'PhD in Physics', u""Bachelor's in Optical Information Sciences and Technology""]","[u'Institute of Physics Chinese Academy of Sciences Beijing, China\nSeptember 2007 to July 2012', u'Lawrence Berkeley National Laboratory Berkeley, CA\nDecember 2010 to June 2012', u'Central University for Nationalities Beijing, China\nSeptember 2003 to July 2007']","degree_1 : PhD in Physics, degree_2 :  PhD in Physics, degree_3 :  ""Bachelors in Optical Information Sciences and Technology"""
0,https://resumes.indeed.com/resume/53a3f7ba643d3b21,"[u""Data Science Researcher\nNortheastern University - Boston, MA\nDecember 2016 to Present\nCalculated and Predicted the reply rate for each e-mail sent to various users for Yesware using Python and R by creating linear models using several\nRegression algorithms, evaluated and deployed the model.\n\u2756 Classified hand-written text using Recursive Neural Network and LSTM Frameworks using Natural language Processing techniques in Python.\n\u2756 Performed Logistic regression to predict if an employee would be attrited or not using IBM's HR dataset that included multivariate analysis using R.\n\u2756 Performed Hypothesis testing on incomplete data and structured in a way that the data could be used for analysis.\n\u2756 Performed Classification analysis on the cancellation status of flights based on weather conditions using Classification tress and Random Forests.\n\u2756 Designed a gym relational database model and collected, stored and retrieved data using SQL queries.\n\u2756 Predicted the future Per Capita Income for various counties in USA and classified them using Neural Networks.\n\u2756 Performed sentimental analysis on various newspapers' online posts and comments using R."", u""Data Scientist Co-op\nNortheastern University - Boston, MA\nApril 2017 to January 2018\nDetected inappropriate records and columns and cleansed 2 TB Electronic Health Record(EHR) and All Payers Claims Data(APCD) from various Public\nHealth Centers around Massachusetts using R and performed statistical data analysis.\n\u2756 Performed Predictive analysis to predict the level of cholesterol and the probability of cardiac arrests within next 10 years using multiple linear regression\ntechniques that included data gathering, requirements specification, processing, analysis and presentations and evaluated and deployed the model.\n\u2756 Performed Computer Vision Classification analysis on images with tumor using Convolution Neural Network Frameworks in Python.\n\u2756 Performed Clustering analysis to group people into categories based on their smoking habits using Clustering ML algorithms and deployed the model.\n\u2756 Determined the Association rules for patients who had undergone a specific diagnosis using Association algorithms in R.\n\u2756 Retrieved data from several hospitals' database for researchers that fit a specific diagnosis criteria using various SQL queries.\n\u2756 Performed Classification analysis to classify people who would not show up on the day of appointment using Classification ML algorithms and evaluated the models to choose the best algorithm.\n\u2756 Detected patterns of the patients loyal to a particular hospital for SHRINE and stored and retrieved CRC data locally using various SQL queries for further\nanalysis."", u'Associate Engineer\nCaterpillar Inc - Chennai, Tamil Nadu\nJanuary 2015 to July 2016\nDeveloped an independent tool to automate the development process for Display Systems software that reduced 40% of the development time using Visual\nBasic.NET.\n\u2756 Analyzed the data from various analog and digital sensors for various Electronic Control Modules using Excel and R.\n\u2756 Created visualization reports and dashboards for sensor outputs and analyzed data.\n\u2756 Modified the existing Perl and Python scripts to automate the flash file build process that saved 30% of the development time.\n\u2756 Integrated, tested and built the new features into a flash file as per the request of the customers using C programing.']","[u'M.S in Data Analytics Engineering', u'B.E. in Electronics and Communication']","[u'Northeastern University Boston, MA\nSeptember 2016 to Present', u'P.S.G. College of Technology\nJuly 2011 to May 2015']","degree_1 : M.S in Data Analytics Engineering, degree_2 :  B.E. in Electronics and Commnication"
0,https://resumes.indeed.com/resume/120668eb2e071a99,"[u""Senior Data Scientist\nThePSI - Gurgaon, Haryana\nAugust 2016 to July 2017\nCoordinated as Sr. Data Scientist on many projects for Boston Consulting Group across different domains such as marketing, operations and pricing. Designed and pioneered production-ready machine-learning models and feature\nextraction systems. Lead the Analytics Team Training and Onboarding Committee, including many internal workshop\nseries. Awarded Employee of the Year and Innovation and Leadership awards for my work and for streamlining research and development across departments.\nPROJECT 1: Marketing, Sales and Pricing Catalyst (Client: BCG and Diageo)\nCharted and spearheaded the design and development of Analytical Engine of BCG's million dollar analytcial product\nMSP (Marketing, Sales and Pricing Catalyst). Implemented machine learning and deep learning AI models to forecast\nand optimize marketing investment, product pricing and sales. Analyzed 15 years of data and collaborated with stakeholders to formulate strategy for analytical engine integration with overall system. Tools used: R, Python, Hadoop,\nSpark, Machine Learning, AI, Excel, SQL, Tableau, Time series Forecasting and GitHub.\nPROJECT 2: Sentiment Analysis for Samsung English\nIntegrated Hadoop, Flume and python Twitter API to fetch relevant tweets related to brand and products to understand\ncustomer views and aspirations. Deployed python nltk library for data mining. Applied machine learning algorithm\nto analyze and model sentiments and performed descriptive analytics on text. Presented the findings and recommendations to Samsung on Tableau. Tools used: Python, Hadoop, ML, Tableau and Excel"", u""Data Scientist\nLibreosoft - Mumbai, Maharashtra\nNovember 2014 to July 2016\nWorked as Data Scientist and Business Analyst for McKinsey on projects such as Retail and Churn Analytics and Project\nManagement Tool Excalibur respectively. Coordinated with different departments, clients and stakeholders to gather\ndata and requirements. Programmed and orchestrated real-time statistical and machine learning models and feature\nextraction system. Helped the client to increase customer retention by 45% and improve target marketing thus\ndecreased marketing expenditure by 34%. Mentored team to design product of highest standard with very quick\ndeliverables. Reading\nPROJECT 1: Retail and Churn analytics (Client Dmart India)\nDeveloped machine learning supervised and unsupervised models to segment and cluster customers based on different\nbusiness matrices and customer buying patterns. Created RFM models based on customer purchase data and used\nmathematical algorithms to evaluate customer life time value for next 10 years. The application of machine learning\nyielded company a high retention rate of 45%. Tools used: R language, R on Hadoop, SQL, Excel, Tableau and Machine\nLearning.\nPROJECT 2: Excalibur Tool Building Lego\nSupervised as Sr. Business Analyst on McKinsey's Idea Managment tool Excalibur. Foresaw the development of project\nfrom initiation and requirement gathering to testing and delivery phase. Coordinated with technical team and stakeholders to sustain hassle-free development. Handled ad-hoc requests inteligently and shared valuable and\ninnovatives ideas to make better, user friendly and optimized product. Tools used: Microsoft Excel, Microsoft\nPowerpoint, GitHub, Python and Tableau""]","[u'Masters in Business Analytics in Business Analytics', u'Post Graduation Diploma in Data Science and Big Data in Data Science and Big Data', u'in Technology']","[u'University of Cincinnati Cincinnati, OH\nAugust 2017 to May 2018', u'SP Jain School of Global Management Mumbai, Maharashtra\nAugust 2015 to July 2016', u'Rajasthan Technical University\nJuly 2011 to May 2015']","degree_1 : Masters in Bsiness Analytics in Bsiness Analytics, degree_2 :  Post Gradation Diploma in Data Science and Big Data in Data Science and Big Data, degree_3 :  in Technology"
0,https://resumes.indeed.com/resume/f962bf8e4aa16f8e,"[u'Business Manager II\nCiti\nOctober 2017 to Present\nWorking as a Business Manager for Citi Cards Decision management Customer Acquisition Team. Managing a team that builds Profit and loss models for various Citi Credit Cards in Partnership with American Airlines.Reviewing the offers every quarter thus ensuring maximum customer acquisition and profitability.', u""Graduate Summer Fellow\nMassachusetts Institute of Technology - Cambridge, MA\nMay 2017 to August 2017\no Visiting student, researching about implementing various data science techniques to resolve sustainability issues for the Cambridge area. Building models and visualizations for one of the MIT's 'Living Lab'\no Analytics consultant for MBA student of MIT-Sloan school of Business for their summer Project"", u'Analytics Associate\nCapital One - Bangalore, Karnataka\nMay 2013 to August 2016\n\u2022 Forecasting and Resource Management Analytics:\no Built a Time Series Forecasting model to predict complaint ticket volumes and count of resources required for their\nresolution. Reduced cost behind extra resources by 15%\n\u2022 Customer Analytics using Text Mining:\no Performed sentiment analysis for the customer responses using text mining in Python. (NLTK Package)\no Calculated Customer satisfaction and Net promoter score and segmented the customers per these scores.\no Designed a Tableau dashboard which provide insights about customers who are Promoters, Passives and Detractors\nthus helping the business to develop actionable strategies to address each customer segment.\n\u2022 Compliance analytics using Big Data and Visualization\no Analyzed business metrics in SQL and R for 100+ bank processes to ensure that the metrics are meeting six sigma\nquality standards and built a Tableau Dashboard for 1000+ users to visualize the metrics at high and detailed level.\no Visited US office to meet 30+ process owners to understand the Business Process and metrics, identifying scope of merging and automation of the metrics.\no Created a data infrastructure in Hadoop to house 5000+ Business metrics. Improved the quality of data aggregation and automated ~350 metrics resulting 35% faster report generation and 9 FTE equivalent reduction of cost/effort.\no Recognized by an Annual award (top 1% of employees) for outstanding contribution to this project', u'Data Scientist\nMu Sigma Business Solutions - Bangalore, Karnataka\nMarch 2012 to May 2013\nPredicted the effectiveness of upcoming promotions for US electronic retailer every week by building linear regression\nmodel resulting into scrapping of 9 promotional items which increased the profit by 14%\n\u2022 Segmented the US Physicians for a Healthcare company by K means Clustering in SAS to help the marketing team to strategize their marketing techniques, target the right Physicians and optimize the investment behind marketing.\nReduced the cost behind marketing by 28%']","[u'MS in Business Analytics and Project Management', u'Bachelor of Engineering in Electronics']","[u'University of Connecticut school of Business\nAugust 2017', u'Sardar Patel Institute of Technology\nJune 2011']","degree_1 : MS in Bsiness Analytics and Project Management, degree_2 :  Bachelor of Engineering in Electronics"
0,https://resumes.indeed.com/resume/fe3d5da01fba9590,"[u'Data Scientist and Statistician\nUCLA - Rice University - Baylor College of Medicine Research Group\nJanuary 2016 to Present\n\u2022 Applied GARCH-DCC model for estimating dynamic functional connectivity among resting-state networks in temporal lobe epilepsy from functional MRI (fMRI) data\n\u2022 Engineered temporal and spectral features and applied various machine learning algorithms for temporal epilepsy classification\n\u2022 Currently working on classification algorithms for repeated measures with applications to voxel-wise resting-state fMRI data', u'Data Scientist and Statistician\nBureau of Economic Geology - Austin, TX\nSeptember 2014 to Present\n\u2022 Developed and applied a penalized model-based recursive partitioning algorithm for predicting oil and gas expected ultimate recovery for the Eagle Ford shale play\n\u2022 Applied random forest, gradient boosting and deep neural networks for predicting oil and gas production from geologic and operator completion practice information\n\u2022 Applied machine learning algorithms for shale well log curve predictions\n\u2022 Currently working on using deep learning for predicting decline curves and lithofacies classification for oil and gas wells in all major US shale plays', u'Graduate Researcher\nRice University - Houston, TX\nAugust 2008 to August 2015\n\u2022 Developed a novel particle Markov chain Monte Carlo algorithm for time series data\n\u2022 Developed a particle filter to account for heavy tails and skewed distributions\n\u2022 Modeling of time series data, including ARIMA, GARCH and stochastic volatility models', u'Market Risk Consultant\nBP - Houston, TX\nAugust 2010 to August 2014\n\u2022 Developed and assisted in the integration of a Monte Carlo based Value-at-Risk (VaR) engine for Natural Gas and Liquids (NGL) and Power portfolios in a production environment saving the company over one million dollars yearly\n\u2022 Developed and implemented various exotic options models into the VaR engine\n\u2022 Developed, automated and deployed a software application for estimating the risk / reward of portfolio positions based on historical prices for NGL and Power portfolios']","[u'Postdoctorate Fellow in Baker Institute for Public Policy', u'Ph.D. in Statistics', u'B.A. in Mathematics and Economics', u'']","[u'Baker Institute\nSeptember 2015 to Present', u'Rice University\nAugust 2009 to August 2015', u'Berea College\nAugust 2003 to June 2007', u'Harvard University\nMay 2005 to August 2005']","degree_1 : Postdoctorate Fellow in Baker Institte for Pblic Policy, degree_2 :  Ph.D. in Statistics, degree_3 :  B.A. in Mathematics and Economics, degree_4 :  "
0,https://resumes.indeed.com/resume/2d8b6a057d9902f6,"[u""Data Scientist\nVerveba Analytics Inc\nJanuary 2016 to Present\nJan 2016 - July 2016\n\nProject Overview:\n\u2022 Study of various RF parameters in the several terabytes of telecom data and total test time taken by each event for stationary testing.\n\u2022 Exploratory analysis, data wrangling and development of algorithms in R for data mining and analysis.\n\u2022 Worked with RF engineers to understand the relationship and importance of various variables and then looked for anomalies in data.\n\u2022 Helped to pin down on low performing sites and test equipment that needed recall.\n\u2022 Modified and created complex SQL-based queries to access explore and pull data from multiple sources, responsible for data acquisition and maintenance of in-house Tapas data stored in a Hadoop database.\n\u2022 Worked on front-end dashboard design and layout using Spark data streaming in IntelliJ, SQL query fetching, HTML, Java Web Service SOAP, REST APIs etc. Worked on MS SQL server management studio for creating DB & writing scripts for analysis and filtering.\n\u2022 Big data lake streaming automation using batch job submission using HIVE query fetching by JSON protocol and map-reduce RDD's in spark."", u'Data Analyst & Big Data Developer\nVerveba Analytics Inc - Richardson, TX\nAugust 2015 to Present\n1. Telecom Data Analysis - Richardson, TX - Client: AT&T August 2015 - Present\n\nProject Overview: Studies of telecom data collected on handsets for various events and develop a model to classify the data points as indoor and outdoor.\n\u2022 Tested and developed several machine learning predictive models to sort out long standing issue of indoor vs. outdoor data classification.\n\u2022 Cleaned, merged & organized data sets with thousands of variables.\n\u2022 Tested many machine learning techniques like decision tree, random forest, artificial neural network, na\xefve Bayes probability model, regression models for classification and prediction.\n\u2022 Performed extensive exploratory statistical analysis and used un-supervised machine learning clustering methods to understand data.\n\u2022 Developed computing algorithms in R and python, then worked together with software engineers to provide analytics solutions into automated dashboards.\n\u2022 Project management of system architecture and design for various businesses cases.\n\u2022 Written several scripts in PIG, HIVE to filter/map/aggregate data and used scoop to transfer data from (and to) hadoop data-lake.\n\u2022 Developed code in Spark framework using Scala and py-spark for UI and dashboard display analysis. Used Apache Zeppelin for visualization.', u""Data Analyst\nVerveba Analytics Inc\nAugust 2016 to March 2017\nProject Overview: Applied math and statistics to determine areas for improvement in telecom coverage in Latin America and developed rate indications to keep the company profitable and operating using actuarial models.\n\u2022 Researched new markets for telecom usage in Latin America, created risk-based pricing models\n\u2022 Processed and validated international data required for reserving analyses\n\u2022 Queried company databases using SAS and SQL, wrote programs to manipulate data as needed\n\u2022 Conceived of method for simulating extension of exposure using a random sample of existing telecom data, which resulted in the most reliable estimate of current cell phone coverage.\n\u2022 Created and maintained automatic rate generators for comparison of competitors' prices.\n\u2022 Developed Excel system that estimated the impact of various rate or algorithm changes on the established book of business.\n\u2022 Provided year-end loss development exhibits for Actuarial Opinion for a newly acquired company using Excel based Affinity software.\n\u2022 Data and IT support for Actuarial Predictive Modeling projects. SAS Dataflux, DB2, Teradata, SQL"", u'Student Researcher\nPurdue University - West Lafayette, IN\nJanuary 2014 to August 2016\nProject Overview: Sleep quality research through data collected, cleaned and analyzed using various mathematical and statistical tools.\n\u2022 Developed statistical models as a process that includes exploratory data analysis, data quality check, model identification, model validation and testing, and model selection. Built and tested numerous time series models, various regression methods and machine learning models.\n\u2022 Conducted sleep quality research using R and SAS to analyze research data acquired from Mechanical Turks to study the effect of technology in the bedroom on sleep quality amongst different age groups. Results were optimized using complex nonlinear fitting.\n\u2022 Used visualization tools such as MATLAB and Microsoft Excel to present the results of the study to best support the findings and research techniques employed.\n\u2022 Created backup databases of sleep study data on Blackboard Academic Suite using MS SQL Server.']",[u'Bachelor of Science in Mathematics'],[u'Purdue University\nJanuary 2016'],degree_1 : Bachelor of Science in Mathematics
0,https://resumes.indeed.com/resume/c5119f8b7fbbb088,"[u'Data Science Intern\nMyklovr - New York, NY\nJanuary 2018 to Present\n\u2022Achieve higher than 90% accuracy by demonstrating the algorithm and real-time use by developing the GPA Trend Classification and user interface for the official company website.\n\u2022Clean big raw data from college database using EDA analysis on students\u2019 selection preferences, grades, financial situation, geographic information; developed/aggregated students feature.\n\u2022Develop algorithms to help students get into the target school by providing academic recommendations. using Random Forest, KNN, Na\xefve Bayes methods.', u'Team mentor\nFRC First Robotics Competition - New Hampshire\nJune 2016 to Present\n\u2022 Worked extensively with team members and coaches during the build season, elected team mentor to assist the team and make decisions.\n\u2022 Updated event information, ordered robotics parts and made the winning strategy for the team.\n\u2022 Fund-raised 6,000 dollars\u2019 scholarship through social media, FRC headquarter and local government to support team.', u""Business Development Intern\nBoston Scientific Company International Medical Trading, CO.LTD, Shanghai, China - Shanghai\nJuly 2016 to August 2016\nImproved sales representatives' and managers' understanding in the latest sales strategy, products\ninformation, products feature and regional sales people focus by translating the latest files from the headquarters of BSC.\n\u2022 Collected all the mailing information and sent the demo to all the regional sales people on the lists by using Excel spreadsheets, Pivot tables, and data analysis tools to make the managers' work easier.\n\u2022 Classified all the bidding information of BSC products and handed them to the managers."", u'Data Scientist Intern\nBOSTON SCIENTIFIC CORP - Shangha\nMay 2016 to August 2016\n\u2022Cleaned and analyzed customer database with 100,000 records to reveal customers\u2019 income level, orders and sales growth for future sales improvement\n\u2022Implemented data mining and statistical machine learning solutions to various business problems such as sales prediction, demand forecasting, marketing segment and targeted marketing', u""Teaching Assistant\nIndiana University Economics Department - Bloomington, IN\nAugust 2014 to December 2015\n08/ 2014-12/ 2015\n\u2022 Selected by faculty for academic excellence to further 4 students' comprehension of statistical\nanalysis.\n\u2022 Improved students' performance with upcoming lab activities, homework, and examinations by conducting office hours.\n\u2022 Handed out all the materials during the class session and collected them by category.\nShanghai Municipal Food and Drug Administration, Shanghai, China"", u'Intern\nShanghai Municipal Food and Drug Administration - Shanghai\nJune 2013 to August 2013\n\u2022 Collected balance sheets, classified financial data, filled in the correct section.\n\u2022 Proficient in Microsoft Office Suite (Word, Excel, Outlook, PowerPoint) and managing, arranging, and coordinating executive calendars, travel, events, and contacts.\n\u2022 Work closely with the administration manager to ensure that the day to day functions run smoothly.']","[u'Master of Science in Business Intelligence & Analytics', u'Bachelor of Arts in Economics & Mathematics']","[u'Stevens Institute of Technology Hoboken, NJ\nDecember 2017', u'Indiana University Bloomington, IN\nDecember 2015']","degree_1 : Master of Science in Bsiness Intelligence & Analytics, degree_2 :  Bachelor of Arts in Economics & Mathematics"
0,https://resumes.indeed.com/resume/0b14ee4f576ebbe1,"[u""Machine Learning/Data Scientist\nCitibank - Irving, TX\nFebruary 2017 to Present\nDescription: Citibank is the consumer division of financial services multinational Citigroup. Citibank was founded in 1812 as the City Bank of New York, later First National City Bank of New York. Citibank provides credit cards, mortgages, personal loans, commercial loans, and lines of credit.\nResponsibilities:\n\n\u2022 Design and develop state-of-the-art deep-learning / machine-learning algorithms for analyzing image and video data among others.\n\u2022 Develop and implement innovative AI and machine learning tools that will be used in the Risk\n\u2022 Experience with TensorFlow, Caffe and other Deep Learning frameworks.\n\u2022 Effective software development processes to customize and extend the computer vision and image processing techniques to solve new problems for Automation Anywhere.\n\u2022 Develop and implement innovative data quality improvement tools.\n\u2022 Will demonstrate cross-functional resource interaction to accomplish your goals.\n\u2022 Involved in Peer Reviews, Functional and Requirement Reviews.\n\u2022 Develop project requirements and deliverable timelines; execute efficiently to meet the plan timelines.\n\u2022 Creating and support a data management workflow from data collection, storage, analysis to training and validation.\n\u2022 Created Natural Language Processing model in R using OpenNLP to classify customer attitude by their reviews.\n\u2022 Hands on experience on MongoDB Replica Set and Sharding.\n\u2022 Installed and configured NoSQL databases like MongoDB and Cassandra.\n\u2022 Having experience on managing life cycle of MongoDB including sizing, automation, monitoring and tuning.\n\u2022 Developed Spark applications by using Scala and Python and implemented Apache Spark for data processing from various streaming sources.\n\u2022 Worked with Spark for improving performance and optimization of the existing algorithms in Hadoop using Spark Context, Spark-SQL, Data Frames, Pair RDD's.\n\u2022 Developed Shiny and R application showcasing machine learning for improving business forecasting.\n\u2022 Understanding requirements, significance of weld point data, energy efficiency using large datasets\n\u2022 Develop necessary connectors to plug ML software into wider data pipeline architectures.\n\u2022 Creating and support a data management workflow from data collection, storage, analysis to training and validation.\n\u2022 Wrote Django APIs for two large datasets from Portland's TriMet as part of broader plan to expose public data via easily consumable REST APIs.\n\u2022 Identify and assess available machine learning and statistical analysis libraries (including regressors, classifiers, statistical tests, and clustering algorithms).\n\u2022 Design and build scalable software architecture to enable real-time / big-data processing.\n\u2022 Acquire business knowledge in the Firm's risk management processes.\n\u2022 Be very passionate about quality and have a strong sense of ownership on the work accomplished.\n\u2022 Be quick to learn new technologies as well as deliver on them in short order.\n\u2022 Taking responsibility for technical problem solving, creatively meeting product objectives and developing best practices.\n\u2022 Worked on requirements gathering for multiple functionality enhancements by engaging with business users and ascertaining their demands.\n\u2022 Involved in maintaining and uploading the Test Scripts.\n\u2022 Have a high sense of urgency to deliver projects as well as troubleshoot and fix data queries/ issues.\n\u2022 Work independently with R&D partners to understand requirements.\n\u2022 Understanding business process and Business problems thoroughly and forecasting the business using data science techniques.\n\u2022 Gathering required data from business users to achieve accurate training data for analysis.\n\u2022 Coordinate and communicate with technical teams for any data requirements.\n\u2022 Providing status of project to project manager and business team up to date.\n\u2022 Understanding the Business requirements based on Functional specification to design the ETL methodology in technical specifications.\n\u2022 Consolidation, standardization, matching Trillium for the unstructured flat file data.\n\u2022 Responsible for developing, support and maintenance for the ETL (Extract, Transform and Load) processes using Informatica Power Center 8.5.\n\nEnvironment:Microsoft Azure HDInsight -Hadoop, Hive, HBase, Azure SQL Data Warehouse, SQL Server 2012, Integration Service (SSIS), Analysis Service (SSAS), Reporting Service (SSRS), Power BI 2.3, Share Point 2010, Naveego, Telerik, Mongo DB, Spot Fire, Tableau 10."", u""Machine Learning/Data Scientist\nDocuSign - San Francisco, CA\nDecember 2015 to January 2017\nDescription:DocuSign is a San Francisco-based company that provides electronic signature technology and digital transaction management services for facilitating electronic exchanges of contracts and signed documents. DocuSign's features include authentication services, user identity management and workflow automation.\nResponsibilities:\n\n\u2022 Participated in stake holder's meetings to understand the business needs & requirements.\n\u2022 Involved in preparation & design of technical documents like Bus Matrix Document, PPDM Model, and LDM & PDM.\n\u2022 Designed framework for sales requirements & Lead team of 5.\n\u2022 Provided technical solutions on MS Azure HDInsight, Hive, HBase, Mongo DB, Telerik, Power BI, Spot Fire, Tableau, Azure SQL Data Warehouse Data Migration Techniques using BCP, Azure Data Factory, and Fraud prediction using Azure Machine Learning.\n\u2022 Participated in Business meetings to understand the business needs & requirements.\n\u2022 Prepared & designed technical documents like OLAPDesignDocument, ConceptualModel, and LDM&PDM.\n\u2022 Prepared scripts to ensure proper data access, manipulation and reporting functions with R programming languages.\n\u2022 Helping automate data analysis by automating pre-processing and incorporating helper functions into API codebase.\n\u2022 Forwarded the Cassandra system logs and gc logs to Splunk and configured the dashboards and alerts for Cassandra on it.\n\u2022 Knowledge on set up Cassandra wide monitoring scripts and alerting system.\n\u2022 Developed preprocessing job using Spark Data frames to transform Json documents to flat file.\n\u2022 Expertise On optimizing spark Jobs when dealing with Huge joins and data Skew.\n\u2022 Formulated procedures for integration of R programming plans with data sources and delivery systems.\n\u2022 Built prototype applications in Python Flask and R Shiny for internal users to interact with predictive models and visualize statistical analysis result.\n\u2022 Up skilled / Trained team on SQL Server 2012 for incoming new requirements.\n\u2022 Provided technical solutions for OLAP design and reporting requirements.\n\u2022 Participated in Business meetings to understand the business needs & requirements.\n\u2022 Prepare ETLarchitect& design document which covers ETLarchitect, SSISdesign, extraction, transformation and loading of Duck Creek data into dimensional model.\n\u2022 Provide technical & requirement guidance to the team members for ETL -SSISdesign.\n\u2022 Participated in Business meetings to understand the business needs & requirements.\n\u2022 Design ETL framework and development.\n\u2022 Design Logical & Physical Data Model using MS Visio 2003 data modeler tool.\n\u2022 Prepare High Level Design (HLD) &Low Level Design (LLD) documents.\n\u2022 Provide technical & requirement guidance to the team members.\n\u2022 Manage development & enhancement of Project Change Request from client.\n\u2022 Participated in Architect solution meetings & guidance in Dimensional Data Modeling design.\nEnvironment: Apache, Spark MLlib, TensorFlow, Oryx 2, Accord.NET, Amazon Machine Learning (AML)Python, Django, Flask, ORM, Jinja 2, Mako, Naive Bayes, SVM, K- means, ANN, Regression, R, Spark, Cassandra."", u""Data Scientist\nCabot Corporation\nFebruary 2014 to November 2015\nDescription:Cabot Corporation is an American specialty chemicals and performance materials company headquartered in Boston, Massachusetts.\n\nResponsibilities:\n\u2022 Design Dimensional Data Modeling using Erwintool.\n\u2022 Design ETLArchitect for SSIS&CubeArchitect for SSAS.\n\u2022 Performed Different type of Optimizations in spark such as Broadcast-join, repartition and kryo serial serialization.\n\u2022 Loaded D-Stream data into Spark RDD and did in-memory data computation to generate output response.\n\u2022 Estimation of work/task using MS Project Plan and allocation of task/work among team members.\n\u2022 Onsite team coordination - Daily / Weekly Status report / meeting.\n\u2022 Responsible for Business Analysis and Requirements Collection and Understanding Business Problems\n\u2022 Interact regularly with Business leaders to set and manage expectations aligned to group capabilities.\n\u2022 Understanding business process and Business problems thoroughly and forecasting the business using data science techniques.\n\u2022 Moved the Cassandra data from old cluster to new cluster on production and lower environments with minimal application downtime.\n\u2022 Configured Performance Tuning and Monitoring for Cassandra Read and Write processes for fast I/O operations and low latency time.\n\u2022 Enhancing the team's data science architecture by developing an internal R package, writing technical documentation and tutorials\n\u2022 Gathering required data from business users to achieve accurate training data for analysis.\n\u2022 Coordinate and communicate with technical teams for any data requirements.\n\u2022 Guided team to implement EDA part for given sales data and analyze the results. Involved in Gathering, exploring and cleaning the data.\n\u2022 Use of cutting edge data mining, machine learning techniques for building advanced customer solutions.\n\u2022 Assigning tasks to analytics team and reporting team and gathering inputs from them on regular basis.\n\u2022 Implemented techniques from artificial intelligence/machine learning to solve supervised and unsupervised learning problems.\n\u2022 Present results of analysis and prediction model evaluation to business executives.\n\u2022 Design, develop, maintain and communicate visual dashboards / reports and analysis based on business requirement needs\n\u2022 Providing status of project to project manager and business team up to date.\n\u2022 Understanding the Business requirements based on Functional specification to design the ETLmethodology in technical specifications.\nEnvironment: Erwin r9.0, Informatica 9.0, ODS, OLTP, Oracle 10g, Hive, OLAP, DB2, Metadata, MS Excel, Mainframes MS Visio, Rational Rose, Requisite Pro, Hadoop, PL/SQL, R, Spark, Cassandra etc."", u""Data Scientist/Machine learning\nPlacingIT - Palo Alto, CA\nNovember 2012 to January 2014\nDescription:PlacingIT is a Dallas-based IT staffing and recruiting company. We take tremendous pride in building responsive, lasting partnerships so we can present to our clients the highest-quality, perfect-fit candidates by focusing on efficiency, precision, and outstanding customer service.\nResponsibilities:\n\n\u2022 Developing propensity models for Retail liability products to drive proactive campaigns.\n\u2022 Extraction and tabulation of data from multiple data sources using R, SAS.\n\u2022 Data cleansing, transformation and creating new variables using R.\n\u2022 Built predictive scorecards for Cross-selling Car loan, Life Insurance, TD and RD.\n\u2022 Scoring predictive models as per regulatory requirements & ensuring deliverables with PSI.\n\u2022 Data modeling and formulation of statistical equations using advanced statistical forecasting techniques.\n\u2022 Provide guidance and mentoring to team members.\n\u2022 Setting up test cluster with new services like Grafana and integrating with Kafka and HBase for intense monitoring.\n\u2022 Benchmarking in HBaseperformance evaluation, tuning and capacity scheduling.\n\u2022 Establish scalable, efficient, automated processes for large scale data analyses, model development, model validation and model implementation.\n\u2022 Formulate and test hypotheses, extract signals from peta-byte scale, unstructured data sets, and ensure that our display advertising business delivers the highest standards of performance.\n\u2022 Lead a multi-functional project team.\n\u2022 Develop necessary connectors to plug ML software into wider data pipeline architectures.\n\u2022 Applied association rule mining & chaid model to identify hidden patterns and rules in remedy ticket analysis which aid in decision making.\n\u2022 Understanding the client business problems and analyzing the data by using appropriate Statistical models to generate insights.\n\u2022 Integrated Teradata with R for BI platform and also implemented corporate business rules\n\u2022 Participated in Business meetings to understand the business needs & requirements.\n\u2022 Arrange and chair Data Workshops with SME's and related stake holders for requirement data catalogue understanding.\n\u2022 Design Logical Data Model which will fit and adopt the Teradata Financial Logical Data Model (FSLDM11) using Erwin data modeler tool.\n\u2022 Present and approve designed Logical Data Model in Data Model Governance Committee (DMGC).\n\nEnvironment: R Studio, Machine learning, Informatica 9.0, Scala, Amazon Machine Learning (AML)Python, Django, SAAS, R, Spark, HBase."", u'Data Scientist\nEssar Groups - Mumbai, Maharashtra\nFebruary 2011 to October 2012\nDescription:Essar Global Fund Limited is an Indian conglomerate group based in Mumbai, India. The Fund is a global investor, controlling a number of world-class assets diversified across the core sectors of Energy, Metals & Mining, Infrastructure and Services\nResponsibilities:\n\u2022 Worked as Data Expert on a data mining ETL development project using SAS Enterprise Guide.\n\u2022 Created test plan documents for all back-end database modules.\n\u2022 Worked with large amounts of structured and unstructured data.\n\u2022 Responsible for data collection, cleansing andANOVA. Designed technical solution roadmap to deal with noise in sales data.\n\u2022 Worked on loading the data from MySQL to HBase where necessary using Sqoop.\n\u2022 Handled end-to-end project from data discovery to model deployment\n\u2022 Knowledge in Business Intelligence tools and visualization tools such as Business Objects, Tableau, ChartIO, etc.\n\u2022 Knowledge in Machine Learning concepts (Generalized Linear models, Regularization, Random Forest, Time Series models, etc.).\n\u2022 Deployed GUI pages by using JSP, JSTL, HTML, DHTML, XHTML, CSS, JavaScript, AJAX.\n\u2022 Configured the project on WebSphere 6.1 application servers.\n\u2022 Implemented the online application by using Core Java, Jdbc, JSP, Servlets and EJB 1.1, Web Services, SOAP, WSDL.\n\u2022 Communicated with other Health Care info by using Web Services with the help of SOAP, WSDLJAX-RPC.\n\u2022 Used Singleton, factory design pattern, DAO Design Patterns based on the application requirements\n\u2022 Used SAX and DOM parsers to parse the raw XML documents\n\u2022 Used RAD as Development IDE for web applications.\n\u2022 Used Log4J logging framework to write Log messages with various levels.\n\u2022 Involved in fixing bugs and minor enhancements for the front-end modules.\n\u2022 Implemented Microsoft Visio and Rational Rose for designing the Use Case Diagrams, Class model, Sequence diagrams, and Activity diagrams for SDLC process of the application.\n\u2022 Maintenance in the testing team for System testing/Integration/UAT.\n\u2022 Conducted Design reviews and Technical reviews with other project stakeholders.\n\u2022 Planned, coordinated, and monitored project levels of performance and activities to ensure project completion in time.\n\u2022 Involved in defining the source to target data mappings, business rules, and business and data definitions.\n\nEnvironment: R 3.0, Erwin 9.5, Tableau 8.0, MDM, QlikView, MLLib, PL/SQL, HDFS, Teradata 14.1, JSON, HADOOP (HDFS), MapReduce, PIG, Spark, R Studio, MAHOUT, JAVA, HIVE, AWS.', u'Data Analyst\nFirst Indian Corporation Private Limited - Hyderabad, Telangana\nFebruary 2009 to October 2011\nDescription:First Indian Corporation pioneered mortgage process offshoring in India, and in the last 15 years has grown into the largest mortgage industry focused technology\nResponsibilities:\n\u2022 Responsible for defining the key identifiers for each mapping/interface.\n\u2022 Clients include eBay, Click Forensics, Cars.com, Turn.com, Microsoft, and Looksmart.\n\u2022 Implementation of Metadata Repository, Maintaining Data Quality, Data Cleanup procedures, Transformations, Data Standards, Data Governance program, Scripts, Stored Procedures, triggers and execution of test plans.\n\u2022 Designed the architecture for one of the first analytics 3.0. Online platforms: all-purpose scoring, with on-demand, SaaS, API services. Currently under implementation.\n\u2022 Web crawling and text mining techniques to score referral domains, generate keyword taxonomies, and assess commercial value of bid keywords.\n\u2022 Developed new hybrid statistical and data mining technique known as hidden decision trees and hidden forests.\n\u2022 Reverse engineering of keyword pricing algorithms in the context of pay-per-click arbitrage.\n\u2022 Coordinated meetings with vendors to define requirements and system interaction agreement documentation between client and vendor system.\n\u2022 Automated bidding for advertiser campaigns based either on keyword or category (run-of-site) bidding.\n\u2022 Creation of multimillion bid keyword lists using extensive web crawling. Identification of metrics to measure the quality of each list (yield or coverage, volume, and keyword average financial value).\n\u2022 Enterprise Metadata Library with any changes or updates.\n\u2022 Document data quality and traceability documents for each source interface.\n\u2022 Establish standards of procedures.\n\u2022 Generate weekly and monthly asset inventory reports.\n\nEnvironment:Erwin r, SQL Server 2000/2005, Windows XP/NT/2000, Oracle, MS-DTS, UML, UAT, SQL Loader, OOD, OLTP, PL/SQL, MS Visio, Informatica..']","[u""Bachelor's""]",[u''],"degree_1 : ""Bachelors"""
0,https://resumes.indeed.com/resume/08b035377257946f,"[u'Associate Data Scientist\nDycom Industries Inc - North Palm Beach, FL\nFebruary 2015 to May 2015\nDescription: This role mirrors my initial employment with Dycom from December 2013 to June 2014. Performed complex analysis and built sophisticated predictive models to generate\nknowledge about customers, products and operations answering strategic and operational\nquestions that require advanced data-driven approaches.\nResponsibilities:\n\u2022 Created fitted security group roles for an internal payroll system using Excel and R.\n\u2022 Developed and maintained predictive models using data mining and R/SPSS statistical\ntools\n\u2022 Building Queries utilizing SQL to create reports and prepare large integrate datasets.\n\u2022 Using V lookup and other tools to build small macros for data clean up.\n\u2022 Working with analysis tools within Excel and SQL analysis service to analysis data and make actionable.\n\u2022 Validated data sources to meet requirements of the effort and outcomes\n\u2022 Analyzed data quality identifying improvements and solutions for data gaps\n\u2022 Developed a deep understanding and working knowledge of all Dycom data sources\npurpose, structure and definitions\n\u2022 Regularly utilized complex data and algorithms\n\u2022 Performed complex data analyses, segmentation and profiling of expansive data sources\n\u2022 Applied predictive modeling to identify underlying trends\n\u2022 Collaborated across the organization to translate manual data analysis into automated\nanalytics, resulting in faster more meaningful data extraction for users\n\u2022 Established model schedules or invoke real time through workflow and other application\nintegration\n\u2022 Maintained and enhanced predictive models. Identify and recommend improvements.\n\u2022 Prepared and delivered presentations effectively detailing results of analytics\n\u2022 Provided analytics and model recommendation insight\n\u2022 Adhered to and recommend improvements to data governance', u'Graduate Teaching Associate\nPurdue University - West Lafayette, IN\nAugust 2014 to January 2015\nDescription: Involved in all activities related to the instruction of undergraduate students. Served as both an instructor and tutor; as well as providing assistance to faculty member in the instruction of a course. Accountable for such tasks as grading papers, preparing handouts,\nplacing materials on the Web, and/or assisting with assignments\nResponsibilities:\n\u2022 Primary instructor for six sections of undergraduate statistics\n\u2022 Graded student exams and quizzes\n\u2022 Developed quizzes and exam prep worksheets\n\u2022 Facilitated group and individual review sessions for students\n\u2022 Taught using Excel/SPSS and utilizing SQL components of Excel/SPSS.', u'Associate Data Scientist\nDycom Industries Inc - North Palm Beach, FL\nDecember 2013 to June 2014\nDescription: Performed complex analysis and built sophisticated predictive models to generate\nknowledge about customers, products and operations answering strategic and operational\nquestions that require advanced data-driven approaches\nResponsibilities:\n\u2022 Applied predictive modeling to identify underlying trends for fleets of commercial\ndrivers within multiple subsidiary businesses using Excel and Microsoft SQL.\n\u2022 Developed and maintained predictive models using data mining and R/SPSS statistical\ntools\n\u2022 Building Queries utilizing SQL to create reports and prepare large integrate datasets.\n\u2022 Using V lookup and other tools to build small macros for data clean up.\n\u2022 Working with analysis tools within Excel and SQL analysis service to analysis data and make actionable.\n\u2022 Validated data sources to meet requirements of the effort and outcomes\n\u2022 Analyzed data quality identifying improvements and solutions for data gaps\n\u2022 Developed a deep understanding and working knowledge of all Dycom data sources\npurpose, structure and definitions\n\u2022 Regularly utilized complex data and algorithms\n\u2022 Performed complex data analyses, segmentation and profiling of expansive data sources\n\u2022 Collaborated across the organization to translate manual data analysis into automated\nanalytics, resulting in faster more meaningful data extraction for users\n\u2022 Established model schedules or invoke real time through workflow and other application\nintegration\n\u2022 Maintained and enhanced predictive models. Identify and recommend improvements.\n\u2022 Prepared and delivered presentations effectively detailing results of analytics\n\u2022 Provided analytics and model recommendation insight\n\u2022 Adhered to and recommend improvements to data governance']","[u'Master Science in Computer Science', u'in Mining of Massive Data Sets', u'', u'in Statistics', u'Bachelor of Mathematical Science in Mathematical Science', u'Associate of Arts']","[u'Georgia Institute of Technology-Main Campus Boca Raton, FL\nJanuary 2016 to January 2020', u'Stanford University\nApril 2015', u'Johns Hopkins University\nMarch 2015', u'Purdue University West Lafayette, IN\nJune 2014 to January 2015', u'Florida Atlantic University Boca Raton, FL\nMay 2013', u'Palm Beach State College-Boca Raton Boca Raton, FL\nAugust 2011']","degree_1 : Master Science in Compter Science, degree_2 :  in Mining of Massive Data Sets, degree_3 :  , degree_4 :  in Statistics, degree_5 :  Bachelor of Mathematical Science in Mathematical Science, degree_6 :  Associate of Arts"
0,https://resumes.indeed.com/resume/b6d3ba45fdce4c5f,"[u'Data Scientist\nMetis\nJune 2017 to September 2017\nDeveloped 5 data science projects in an accredited 12-week immersive program in machine learning, statistics, and data analytics', u'Operations Analyst\nMET International, Inc\nJanuary 2017 to May 2017\nExtracted online sales data of thousands of computer products for visualization and marketing analytics\n\nDeveloped and maintained Python applications that improved sales workflow and data processing efficiency by 50%', u'Systems Analyst Intern\nMET International, Inc.\nJune 2016 to August 2016\nAssembled and tested refurbished servers for distributed data storage\n\nOversaw E-commerce storefronts for wholesale and retail computer hardware sales', u'Cybersecurity Intern\nData Sentry\nMay 2015 to August 2015\nCo-taught and reviewed curricula and software for professional cybersecurity training programs\n\nRevised training materials to be comprehensible for new users\n\nTested website and database vulnerabilities using Wireshark and Kali Linux']","[u'MS in Electrical Engineering', u'BS in Electrical Engineering']","[u'Washington University in St. Louis\nJanuary 2016 to December 2016', u'Washington University in St Louis\nAugust 2012 to May 2016']","degree_1 : MS in Electrical Engineering, degree_2 :  BS in Electrical Engineering"
0,https://resumes.indeed.com/resume/c7e566531fbe5607,"[u'Data Scientist\nFORD MOTOR COMPANY\nOctober 2017 to March 2018\n\u2022 Designed and developed new dashboards from scratch using Qlikview and Tableau for BDD and MIAMI AV Taas projects.\n\n\u2022 Developed applications using different components of QlikView Enterprise like List boxes, Multiboxes, slider, current selections box, buttons, charts, text objects, Calendar, Cyclic and Drill down groups, bookmarks, etc.\n\u2022 Created Tableau scorecards, dashboards using stack bars, bar graphs, scattered plots, geographical maps, Gantt charts.\n\u2022 Usage of Pivot Tables, created side by side bars, Scatter Plots, Stacked Bars, Heat Maps, Filled Maps and Symbol Maps according to deliverable specifications.\n\u2022 Created Rich Graphic visualization/dashboards to enable a fast read on taxi data and key business drivers and to direct attention to key area.\n\u2022 Extracted the data from various data sources such as traditional databases, Excel documents and CSV files, transforming the data into standard format and load the data so that it is available for analyzing and creating reports.\n\u2022 Used Alteryx for Joining, Preparing and Blending data for Chariot Shuttle services project.\n\u2022 Created Data Histograms for New York, Chicago, SFO and DC to know the quality of public transport.\n\n\u2022 Used MS Project and MS Excel to create and manage project schedules, summaries, tasks, milestones, resources, resource utilization, and overall project.\n\u2022 Created Excel and MS Project plans and timelines of software deployments. Created weekly status reports, monthly summaries and attended weekly and bi-weekly core project team meetings. Created project organization charts using VISIO\n\u2022 Created project dashboards on project progress and prepared reports for Sr. Management.\n\u2022 Effectively communicate project expectations and progress to team members and stakeholders in a timely and clear fashion.\n\u2022 Provided strong organization skills for frequent project tracking, reporting, and presentation development.\n\u2022 Status Reporting- directed and prepared status reports for management, client, project personnel or others and modifies schedules or plans as required.\n\nProject Environment: MS Project, MS Office, MS Visio, MS Excel, SharePoint, MS Visio', u'Sr Data Analyst\nHyderabad, Telangana\nJuly 2011 to August 2015\n\u2022 Developed project charter by determining scope, milestones, and product deliverables.\n\u2022 Manage project activities, such as issues tracking, project timelines and risk mitigation, facilitating project team meetings and preparing/delivering status reports using MS Project, MS Excel & MS Word.\n\u2022 Tracked project through all SDLC phases (Project Initiation, Project Planning and Coordination, Project Control and Project Reporting and Communication) using MS Project Professional.\n\u2022 Managed triage of multiple incoming priorities effectively by understanding customer needs and meeting service level requirements. Identify potential system problems and escalated to department contact for resolution.\n\u2022 Lead and manage a team of coding analyst and effectively implemented process for key sector deliverables\n\u2022 Facilitate IT Review meetings with customer and the IT Leads, review the customer current environment and future needs.\n\u2022 Was part of Agile team and assisted Project Manager with tracking and reporting of project status, resources and costs to ensure accuracy and minimize project delays in MS Project.\n\u2022 Created and maintained project schedules and other required project documentation using MS Project.\n\u2022 Evaluated competitors and reviewed prices, sales, marketing, distribution, products and financial feasibility to ensure optimal product offerings and elevate market presence.\n\u2022 Collected customer demographics, needs/preferences, and purchasing habits to capitalize on new markets and identify methods to boost product demand.\n\u2022 Maintained and Updated SharePoint Site Documents (Excel Spreadsheets, Word Documents, Visio and PowerPoint charts.\n\u2022 Responsible for logging and tracking calls using the current ticketing database, and maintaining history records and related problem documentation.\n\u2022 Developed comprehensive reports based on marketing, sales trends, and demographic data analysis.\n\u2022 Conducted cross team training sessions on best practices which reduced the turn-around time of different deliverables across the research teams.\n\nProject Environment: MS Project, MS Office, MS Visio, MS Excel, SharePoint, MS Visio\n\nProjects Handled:', u'Data Analyst\nKellogg\'s, Pepsi, Coca Cola, General Mills, Campbell, P&G - Hyderabad, Telangana\nJuly 2007 to July 2011\n\u2022 TELECOM: AT&T, T-MOBILE\n\u2022 BANKS: Bank of America, HSBC\n\u2022 OTHERS: General Motors, ESPN\n\nKantar Operations, Hyderabad, India July 2007 - July 2011\nData Analyst\n\n\u2022 Defined project scope, goals and deliverables that support business goals in collaboration with senior management and stakeholders.\n\u2022 Managed activities that support the successful completion of projects including assisting with project schedules, deliverables, assignments, tasks, project meetings, status reporting, communication and action items, as required.\n\u2022 Provided weekly written and verbal communications reports, and milestones throughout project completion outlining project status and progress to project manager, communications director, regional managers, and communications technicians.\n\u2022 Lead meetings via agendas, publish meeting notes and track action items\n\u2022 Tracked project through all project phases (Project Initiation, Project Planning and Coordination, Project Control and Project Reporting and Communication).\n\u2022 Handled coding process for the study on General Motors, and University of Phoenix.\n\u2022 Been a single point of contact for ""Main idea section"" for FMCG deliverables.\n\u2022 Coordinated with the quality leads in preparing process documentation for FMCG deliverables.\n\u2022 Worked on building strong client relationship and obtained one of the highest Customer-Satisfaction score in the team.\n\u2022 Pro-actively assisted client in preparing the questionnaire for a FMCG study.\n\u2022 Team lead for Greening initiative for the firm.\n\u2022 Event coordinator for company\'s annual meet.\n\nProject Environment: MS Project, MS Office, MS Visio, MS Excel, SharePoint, MS Visio']","[u""Master's in Information Systems in Data Mining and Data Modelling"", u""Master's in Business Administration in Marketing"", u'Bachelors in Commerce in Commerce']","[u'University of North Texas Denton, TX', u'Wesley Business School, Osmania University', u'Siddhanti College, Osmania University']","degree_1 : ""Masters in Information Systems in Data Mining and Data Modelling"", degree_2 :  ""Masters in Bsiness Administration in Marketing"", degree_3 :  Bachelors in Commerce in Commerce"
0,https://resumes.indeed.com/resume/f660725b180c565f,"[u'Data Science Intern\nKeller Williams Realty - Dallas, TX\nMay 2017 to Present\n\u2022 Worked with Keller Williams, a major real-estate firm to realize latest data science and machine learning capabilities to predict rents, rehab costs and after repaired values(ARV) of properties\n\u2022 Prospected and acquired $6 million worth of investment properties. Analysed feasibility of investment properties and market conditions using several metrics including absorption rates, market price history, and investment ratios\n\u2022 Gathered and filtered data acquired from listing sources to present to Managers, Investors, and clients\n\u2022 Clustered real-estate data in to 3 regions in Dallas/Fort Worth area using k-means clustering with 80% homogeneous clusters through R and assigned ranks to each region\n\u2022 Implemented and tuned machine learning models using various algorithms like regression, support vector machines and decision trees\n\u2022 Maintain client relations, prospect for off-market deals, and manage investment sales pipeline', u""Data Scientist\nThe University of Texas at Dallas - Dallas\nAugust 2016 to May 2018\nthat enabled managers to perform end to end analysis which resulted in a simplified report management, reduced support costs & improved efficiency by 20%\n\nDATA SCIENCE ACADEMIC PROJECTS\nMachine Learning Using R\n\u2022 Predicted telecom customers' churn with 96% accuracy based on information about their accounts using various machine learning algorithms like logistic regression and random forests\n\u2022 Classified injury-type in to 4 classes using logistic regression and one-vs-all strategy to build a multi-class classification model with 71% accuracy\nSentiment Analysis of Amazon reviews - Python (NLP)\n\u2022 Trained and classified text data from Amazon reviews and developed a sentiment analysis predictive model using Random Forests with 87% accuracy.\nBig Data Analytics with Hadoop\n\u2022 By using Hadoop, Hive and Pig in the study of Dallas Police Data (~200,000 rows), analysed the patterns in crimes\n\u2022 Visualized the transformed data using excel and tableau to show patterns in crimes\nCredit Card Fraud detection\n\u2022 Estimated the likelihood of credit card transactions being fraudulent by training an ensemble model with Random Forests and plotted suitable visualizations (Precision-Recall curve, confusion matrix, etc.)\nData Visualization using Tableau\n\u2022 Developed visualizations on UN dataset, natural disasters dataset using Tableau and created Tableau stories"", u'Data Analyst\nInfosys Limited - Hyderabad, Telangana\nApril 2013 to July 2016\n\u2022 To help our clients - various teams in Microsoft HR portfolio in providing Enterprise Data of Microsoft. Our team is responsible for data acquisition, data providing and reporting\n\u2022 Designed and developed relational databases using various datasets and deployed them on servers\n\u2022 Examined data, identified outliers, inconsistencies and manipulated data to ensure data quality and integration\n\u2022 Created and edited T-SQL queries, stored procedures for data retrieval from MS SQL Server 2012\n\u2022 Increased query performance, by optimizing the performance of various SQL scripts, stored procedures and triggers by identifying slow running queries using SQL Profiler']","[u'MS Business in Data Science', u'BE (Honors in Mechanical Engineering']","[u'The University of Texas at Dallas Richardson, TX\nAugust 2016 to May 2018', u'BITS Pilani Pilani, Rajasthan\nAugust 2008 to May 2012']","degree_1 : MS Bsiness in Data Science, degree_2 :  BE (Honors in Mechanical Engineering"
0,https://resumes.indeed.com/resume/ce4aa87fa10c8845,"[u""Data Science Consultant\nCarlson Analytics Lab - Minneapolis, MN\nJuly 2017 to Present\n\u2022 Data-driven marketing strategy for a Minneapolis based CPA firm - Developing go-to-market strategy to demonstrate financial success of the firm's clients; using causal inference techniques to benchmark performance of ~1000\nprivate companies vs. their competitors using 5-year revenue and taxable income\n\u2022 Improving Customer experience at Mall of America (MoA) - Identified ~$100k potential revenue loss due to fraud;\nemployed topic modeling (LDA) and association rules mining to find distinct groups of customers based on ridership\npatterns that helped pinpoint customers indulging in fraudulent activities\n\u2022 Understanding Consumer Perception using Social Media - Declared winner out of 18 teams by industry experts,\nfaculty and peers for implementing a cloud-based solution using AWS to mine user perception from Twitter conversations\n\u2022 Identifying low performing customer segments for PwC - Increased profitability of an auto-insurance client by identifying clusters of low performing customers using K-prototypes technique"", u""Consultant - Analytics & Product Design\nC1 INDIA PVT. LTD - Gurgaon, Haryana\nOctober 2016 to May 2017\nAchieved 200% improvement in search performance, as measured by the baseline performance of two e-tendering web\napplications, by introducing Elasticsearch as a company-wide used search engine\n\u2022 Reduced software development time from a proposed 1 year to 6 months using Agile methodology, thereby\nconceptualizing a new B2B e-marketplace product designed to facilitate e-auctions between 50k+ vendors\n\u2022 Improved data management by reducing the number of tables in the company's database from ~500 to under 150;\nimplemented a new database containing vendor and product data for ~40 applications, using PostgreSQL"", u""Co-Founder, Technology Lead\nEXCELERATION - Gurgaon, Haryana\nJune 2016 to May 2017\nExceleration is an EdTech initiative to facilitate personalization in school education using data science\n\u2022 Delivered an application prototype consisting of 40 mobile interface screens and proposal for a business model for a\nlearning analytics solution; led a team of 3 graduate students as part of an experiential learning program\n\u2022 Developed a proprietary repository consisting of 50+ insights on student learning outcomes, that automates the monitoring of students' academic performances, by leveraging assessment samples sourced from a school\n\u2022 Conceptualized the technology architecture; implemented a database to store scholastic and non-scholastic information,\ndeveloped framework to write application logic and led user experience design for a mobile app"", u'Jr. Data Scientist\nORKASH SERVICES - Gurgaon, Haryana\nJune 2015 to June 2016\nAchieved 4x increase in productivity of software developers and analysts by designing an in-house intelligence\nmanagement system using Apache Accumulo that reduced time spent on data acquisition and pre-processing\n\u2022 Led 4 cross-functional teams of analysts and software engineers to deliver solutions for automotive, banking, politics and social services sectors; analyzed ~50 million data points and created dashboards using Kibana', u'Business Analyst\nJune 2013 to June 2015\n\u2022 Delivered a database of 5k potential customers and 30k analyzed grievances to an automotive giant based on sentiment and network analysis of ~25 million social media posts from Facebook and Twitter\n\u2022 Generated $20k in additional revenue for a financial company by enriching 5k rows of CRM data with data from job\nportals and professional networking sites, enabling cross-selling and upselling of financial products']","[u'Master of Science in Business Analytics', u'Bachelor of Engineering in Information Technology']","[u'UNIVERSITY OF MINNESOTA Minneapolis, MN\nMay 2018', u'MANIPAL UNIVERSITY\nMay 2013']","degree_1 : Master of Science in Bsiness Analytics, degree_2 :  Bachelor of Engineering in Information Technology"
0,https://resumes.indeed.com/resume/2d37e0c440694486,"[u'Graduate Data Analyst\nArizona State University - Tempe, AZ\nSeptember 2017 to Present\n\u2022 Performed text mining using Python on enquiry emails and came up with categories to improve the response time of emails\n\u2022 Accelerated the audit process of International students by automating the reports using Excel and VBA\n\u2022 Helped optimize \'I-20 issuance process\' by finding bottlenecks using Six Sigma DMAIC methodology which helped in minimizing the\nI-20 issuance turnaround time by 60%\n\u2022 Assisted in defining metrics for ""Front desk Check In"" process and built a dashboard to track the performance', u'Decision Scientist\nMu Sigma - Bengaluru, Karnataka\nMarch 2017 to May 2017\n\u2022 Identified 20 business problems for the client in Home Insurance and came up with a roadmap after carrying out in-depth analysis\n\u2022 Built a model (GBM, GLM) for Commercial Insurance to check if 3rd party credit risk data can be used to optimize the premium which resulted in saving of $300,000\n\u2022 Led a team of 3 to create a data warehouse in Amazon Redshift which integrates multiple data sources to improve the reliability of reports for different stakeholders from Life Insurance business', u'Trainee Decision Scientist\nMu Sigma - Bengaluru, Karnataka\nSeptember 2015 to March 2017\n\u2022 Generated insights by carrying out \u201cOverdraw and Churn analysis\u201d for an Australian Bank by doing customer segmentation in R to help business quantify the $ impact caused due to these customers\n\u2022 Performed text mining on the Assessor reports to find out the reasons for the increasing claims cost\n\u2022 Liaised with the client to build a predictive model which predicted the customers likely to file a claim\n\u2022 Developed interactive dashboards in Tableau and its backend in Redshift using Agile approach which captured several metrics in Home Insurance used by managers to drive their decisions\n\u2022 Carried out comprehensive research about \u201cGraph Analytics\u201d and helped in rebuilding the Fraud Detection model using GBM for Auto Insurance by adding new features which improved the accuracy by 5%', u'Product Manager, Intern\nAmuse Labs - Dharwad, Karnataka\nJuly 2015 to August 2015\n\u2022 Conceptualized, designed and built a multimedia gamification framework for businesses to engage with the users\n\u2022 Derived user behavior insights from the log data for the Puzzle Me platform\n\u2022 Oversaw the research about developing new features, marketing and content development for the Puzzle Me platform']","[u""Master's in Business Analytics"", u""Bachelor's in Computer Science""]","[u'Arizona State University-Tempe Tempe, AZ\nAugust 2017 to May 2018', u'University of Pune Pune, Maharashtra\nMay 2011 to May 2015']","degree_1 : ""Masters in Bsiness Analytics"", degree_2 :  ""Bachelors in Compter Science"""
0,https://resumes.indeed.com/resume/ff4133ff3094d2bd,"[u""Data Scientist\nComcast - Philadelphia, PA\nOctober 2017 to Present\nPerformed technical and analytical work for the company's diversity programs involving data retrieval, data validation, trend analysis, and database management\n\u25e6 Performed statistical analysis on diversity program results, and Supported Strategic initiatives\n\u25e6 Partnered with various stakeholders across Comcast, NBCUniversal, and Comcast Spectacor to gather diversity data and metrics, and aggregate and analyze data on a company-wide level\n\u25e6 Provided data analysis for external diversity surveys, executive requests and presentations, and Joint Diversity Council meetings.\n\u25e6 Maintained and enhanced data archive & SQL database for the metrics reporting of the company's key diversity initiatives\n\u25e6 Supported ad hoc reporting in relation to the company's diversity program initiatives\n\u25e6 Designed and lead implementation of analytics pipelines and visualization by collaborating with other data/informatics teams.\n\u25e6 Scouted, tested and recommended new analytics algorithms and models to the data analytics."", u'Data Scientist\nWillis Towers Watson - Philadelphia, PA\nOctober 2015 to October 2017\nImported, cleaned, transformed, validated and/or modeled data with the purpose of understanding or making conclusions from the data for decision making purposes\n\u25e6 Analyzed trends and developed customer segmentation algorithm in R which scored sales leads for clients and lead to increased market share\n\u25e6 Implemented demand forecasting models which improved upon forecast accuracy and reduced supply chain inefficiencies\n\u25e6 Conducted data preparation, and outlier detection using MS SQL server; built the model using R.\n\u25e6 Created and presented executive dashboards and scorecards to show the trends in the data using Excel and VBA-Macros.\n\u25e6 Assisted the HR director in talent assessment of new Data Scientists.\n\u25e6 Interpreted data, analyzing results using statistical techniques and provided ongoing reports for Machine learning\n\u25e6 Produced analysis on datum using SQL, Excel, Access, Python, and other data analytical tools\n\u25e6 Used analytical rigor and statistical methods to analyze large amounts of data, extracting actionable insights using advanced statistical techniques such as data analysis, data mining, optimization tools, and machine learning techniques and statistics\n\u25e6 Created graphical representation of data and reports using Tableau\n\u25e6 Generated questions and identify inadequacies based on supplied data sets\n\u25e6 Identified, analyzed, and interpreted trends or patterns in complex data sets', u'Data Analyst\nWakely Consulting Group - Denver, CO\nJuly 2014 to October 2015\nUtilized Excel, SAS, VBA and Machine learning processes to analyze and interpret insurance trends and transform complex data files into meaningful and presentable charts and tables.\n\u25e6 Analyzed given statistical data to create a premium portfolio and calculated cost estimates using CMS guidelines\n\u25e6 Learned in-depth about Medicare and Medicaid processes. Developed, priced and analyzed Healthcare products and services.\n\u25e6 Helped create various unique tools to simplify Risk adjustment, Risk corridor, and Reinsurance.\nColumbia Business School New York, NY\nResearch Intern: Hypothesis testing and market research 0.5 years\n\u2022 Used advanced data mining, statistical analysis, machine-learning and visualization techniques using R, Excel, and SAS to create solutions to challenging real-world problems.\n\u2022 Recruited test subjects, gathered and recorded data through interviews and/or field observation and conduct experiments\n\u2022 Worked with diverse data sets, identified and developed valuable new sources of data and collaborated with product teams to ensure successful integration\n\u2022 Used the latest cluster-based technologies to implement real-time solutions that scale to massive data sets\nDesired Entertainment Services:\nFounder\n\u25e6 CEO of a business that provides an array of entertainment services for any event with a budget of $500-$7000\n\u25e6 Managed over 20 employees ranging from DJs, clowns, etc. and worked with over 20 suppliers and retailers\n\u25e6 Implemented business tactics learned in the classroom to optimize customer satisfaction and create higher margins']",[u'Bachelors of Science in Statistics'],"[u'The Pennsylvania State University University Park, PA']",degree_1 : Bachelors of Science in Statistics
0,https://resumes.indeed.com/resume/bc0a714e4ff4c5a6,"[u""Data Scientist\nCity of New York - New York, NY\nJune 2017 to Present\nCity of New York hopes to improve employee turnover rates by 1) Enriching traditional resume variables with third-party data, 2) Using big-data and/or intelligent search technologies and algorithms to construct an optimal, non-discriminatory model predicting employee tenure, and 3) hiring only candidates the model predicts will have good tenure.\n\nResponsibilities:\n* Perform Data Profiling to learn about behavior with various features turnover before the hiring decision, when one has no on-the-job behavioral data.\n* Extracted the data from hive tables by writing efficient Hive queries.\n* Performed preliminary data analysis using descriptive statistics and handled anomalies such as removing duplicates and imputing missing values.\n* Analyze Data and Performed Data Preparation by applying historical model on the data set in AZUREML.\n* Application of various machine learning algorithms and statistical modeling like decision trees, text analytics, natural language processing (NLP), supervised and unsupervised, regression models, social network analysis, neural networks, deep learning, SVM, clustering to identify Volume using scikit-learn package in python, Matlab.\n* Performed data cleaning and feature selection using MLlib package in PySpark and working with deep learning frameworks such as Caffe, Neon etc.\n* Conducted a hybrid of Hierarchical and K-means Cluster Analysis using IBM SPSS and identified meaningful segments of through a discovery approach.\n* Develop Spark/Scala, Python, R for regular expression (regex) project in the Hadoop/Hive environment with Linux/Windows for big data resources. Used clustering technique K-Means to identify outliers and to classify unlabeled data.\n* Evaluate models using Cross Validation, Log loss function, ROC curves and used AUC for feature selection and elastic technologies like ElasticSearch, Kibana etc.\n* Work with NLTK library to NLP data processing and finding the patterns.\n* Categorize comments into positive and negative clusters from different social networking sites using Sentiment Analysis and Text Analytics.\n* Ensure that the model has low False Positive Rate and Text classification and sentiment analysis for unstructured and semi-structured data.\n* Addressed over fitting by implementing of the algorithm regularization methods like L2 and L1.\n* Use Principal Component Analysis in feature engineering to analyze high dimensional data.\n* Create and design reports that will use gathered metrics to infer and draw logical conclusions of past and future behavior.\n* Perform data analysis by using Hive to retrieve the data from Hadoop cluster, Sql to retrieve data from Oracle database and used ETL for data transformation.\n* Use MLlib, Spark's Machine learning library to build and evaluate different models.\n* Perform Data Cleaning, features scaling, features engineering using pandas and numpy packages in python.\n* Create Data Quality Scripts using SQL and Hive to validate successful data load and quality of the data. Create various types of data visualizations using Python and Tableau.\n* Communicate the results with operations team for taking best decisions.\n* Collect data needs and requirements by Interacting with the other departments.\n\nEnvironment: Python 2.x, R, CDH5, HDFS, Hadoop 2.3, Hive, Linux, Spark, IBM SPSS, Tableau Desktop, SQL Server 2012, Microsoft Excel, Matlab, Spark SQL, Pyspark."", u""Data Scientist\nUSPS - Eagan, MN\nMay 2016 to June 2017\nThe United States Postal Service is an independent agency of the United States federal government responsible for providing postal service in the United States, including its insular areas and associated states. It is one of the few government agencies explicitly authorized by the United States Constitution.\n\nCustomer 360\nCustomer 360 refers to summarization information related to customer, at every digital touch point, which describes the behavior of customers and predicts what can happen with them in the future. It can be created using mega-table, which maintains information of all products a customer holds, summary of all customers transactions, demographic features, and CRM information.\n\nData Lake Creation\nData was collected from different sources as flat files, and exported to cloud storage using AWS and Hadoop technologies. Data analyzed, reported and presented using Tableau dashboards. Worked on creation of Orchestration engine that was controlling the complete flow of data ingestion using Sqoop pipelines dumping data into Hive and DQM layers were enabled. Data processed in Hive and end results were reported using Tableau dashboards.\n\nCustomer Segmentation\nDeveloped 11 customer segments using unsupervised learning techniques like KMeans and Gaussian mixture models; The clusters helped business simplify complex patterns to manageable set of 11 patterns that helped set strategic and tactical objectives pertaining to customer retention, acquisition, spend and loyalty.\n\nResponsibilities:\n* Implemented Data Exploration to analyze patterns and to select features using Python SciPy.\n* Built Factor Analysis and Cluster Analysis models using Python SciPy to classify customers into different target groups.\n* Built predictive models including Support Vector Machine, Random Forests and Na\xefve Bayes Classifier using Python Scikit-Learn to predict the personalized product choice for each client.\n* Using R's dplyr and ggplot2 packages, performed an extensive graphical visualization of overall data, including customized graphical representation of revenue reports, specific item sales statistics and visualization.\n* Designed and implemented cross-validation and statistical tests including Hypothetical Testing, ANOVA, Auto-correlation to verify the models' significance.\n* Designed an A/B experiment for testing the business performance of the new recommendation system.\n* Supported MapReduce Programs running on the cluster.\n* Evaluated business requirements and prepared detailed specifications that follow project guidelines required to develop written programs.\n* Configured Hadoop cluster with Namenode and slaves and formatted HDFS.\n* Used Oozie workflow engine to run multiple Hive and Pig jobs.\n* Participated in Data Acquisition with Data Engineer team to extract historical and real-time data by using Hadoop MapReduce and HDFS.\n* Performed Data Enrichment jobs to deal missing value, to normalize data, and to select features by using HiveQL.\n* Developed multiple MapReduce jobs in java for data cleaning and pre-processing.\n* Analyzed the partitioned and bucketed data and compute various metrics for reporting.\n* Involved in loading data from RDBMS and web logs into HDFS using Sqoop and Flume.\n* Worked on loading the data from MySQL to HBase where necessary using Sqoop.\n* Developed Hive queries for Analysis across different banners.\n* Extracted data from Twitter using Java and Twitter API. Parsed JSON formatted twitter data and uploaded to database.\n* Launching Amazon EC2 Cloud Instances using Amazon Images (Linux/ Ubuntu) and Configuring launched instances with respect to specific applications.\n* Developed Hive queries for analysis, and exported the result set from Hive to MySQL using Sqoop after processing the data.\n* Analyzed the data by performing Hive queries and running Pig scripts to study customer behavior.\n* Created HBase tables to store various data formats of data coming from different portfolios.\n* Worked on improving performance of existing Pig and Hive Queries.\n* Created reports and dashboards, by using D3.js and Tableau 9.x, to explain and communicate data insights, significant features, models scores and performance of new recommendation system to both technical and business teams.\n* Utilize SQL, Excel and several Marketing/Web Analytics tools (Google Analytics, Bing Ads, AdWords, AdSense, Criteo, Smartly, SurveyMonkey, and Mailchimp) in order to complete business & marketing analysis and assessment.\n* Used Git 2.x for version control with Data Engineer team and Data Scientists colleagues.\n* Used Agile methodology and SCRUM process for project developing.\n\nEnvironment: HDFS, Hive, Scoop, Pig, Oozie, Amazon Web Services (AWS), Python 3.x (SciPy, Scikit-Learn), Tableau 9.x, D3.js, SVM, Random Forests, Na\xefve Bayes Classifier, A/B experiment, Git 2.x, Agile/SCRUM."", u'Data Scientist\nComerica Incorporated - Dallas, TX\nSeptember 2013 to May 2016\nis a financial services company headquartered in Dallas, Texas. It has retail banking operations in Texas, Michigan, Arizona, California and Florida, with select business operations in several other U.S. states, as well as in Canada and Mexico. The purpose of this project was to fight against credit card fraud. My team mainly focused on rebuilding credit card fraud detection model, monitoring the model in production, taking action if model performance degrades and working closely with business team to on-board a new model.\n\nResponsibilities:\n* Involved in defining the source to target data mappings, business rules, and data definitions.\n* Coordinated with team on designing and implementing data solutions as per project requirements.\n* Detected fraudulent transactions quickly and to find or predicting future customers in the customer relationship management market.\n* Communicated and coordinated with other departments to collect business requirement.\n* Handled importing data from various data sources, performed transformations using Hive, Map Reduce, and loaded data into HDFS.\n* Performed data analysis by using Hive to retrieve the data from Hadoop cluster, Sql to retrieve data from Oracle database.\n* Used pandas, numpy, seaborn, scipy, matplotlib, scikit-learn in Python for developing various machine learning algorithms.\n* Developed MapReduce pipeline for feature extraction using Hive.\n* Used Statistical testing to evaluate Model performance.\n* Improved fraud prediction performance by using random forest and gradient boosting for feature selection with Python Scikit-learn.\n* Implemented machine learning model (logistic regression, XGboost) with Python Scikit- learn.\n* Designed rich data visualizations with Tableau and Python.\n\nEnvironment: Machine learning, Cassandra, NLTK, Spark, HDFS, Hive, Pig, Linux, Python (Scikit-Learn/Scipy/Numpy/Pandas), R, SAS, SPSS, MySQL, PL/SQL, Jupyter notebook, Tableau.', u'Data Scientist\nTata Consultancy Services - Noida, Uttar Pradesh\nAugust 2012 to July 2013\nResponsibilities:\n* Involved in complete Software Development Life Cycle (SDLC) process by analyzing business requirements and understanding the functional work flow of information from source systems to destination systems.\n* A highly immersive Data Science program involving Data Manipulation & Visualization, Web Scraping, Machine Learning, Python programming, SQL,Unix Commands, NoSQL, Hadoop.\n* Used pandas, numpy, seaborn, scipy, matplotlib, scikit-learn, NLTK in Python for developing various machine learning algorithms.\n* Worked on different data formats such as JSON, XML and performed machine learning algorithms in Python.\n* Analyzed sentimental data and detecting trend in customer usage and other services.\n* Analyzed and Prepared data, identify the patterns on dataset by applying historical models.\n* Collaborated with Senior Data Scientists for understanding of data.\n* Used Python and R scripting by implementing machine algorithms to predict the data and forecast the data for better results.\n* Used Python and R scripting to visualize the data and implemented machine learning algorithms.\n* Experience in developing packages in R with a shiny interface.\n* Used predictive analysis to create models of customer behavior that are correlated positively with historical data and use these models to forecast future results.\n* Predicted user preference based on segmentation using General Additive Models, combined with feature clustering, to understand non-linear patterns between user segmentation and related monthly platform usage features (time series data).\n* Perform data manipulation, data preparation, normalization, and predictive modeling.\n* Improve efficiency and accuracy by evaluating model in Python and R.\n* Used Python and R script for improvement of model.\n* Application of various machine learning algorithms and statistical modeling like Decision Trees, Random Forest, Regression Models, neural networks, SVM, clustering to identify Volume using scikit-learn package\n* Performed Data cleaning process applied Backward - Forward filling methods on dataset for handling missing values.\n* Developed a predictive model and validate Neural Network Classification model for predict the feature label.\n* Performed Boosting method on predicted model for the improve efficiency of the model.\n* Presented Dashboards to Higher Management for more Insights using Power BI and Tableau.\n* Hands on experience in using HIVE, Hadoop, HDFS and Bigdata related topics.\n\nEnvironment: R/R studio, Python, Tableau, Hadoop, Hive, MS SQL Server, MS Access, MS Excel, Outlook, Power BI.', u'Jr. Data Scientist\nHSBC Bank - Noida, Uttar Pradesh\nJanuary 2011 to July 2012\nResponsibilities:\n* Created physical and logical models and used Erwin9.1 for Dimensional Data Modeling.\n* Designed and Developed Oracle PL/SQL Procedures and UNIX Shell Scripts for Data Import/Export and data Conversions.\n* Performed legacy application data cleansing, data anomaly resolution and developed cleansing rule sets for ongoing cleansing and data synchronization.\n* Extensively used Star Schema methodologies in building and designing the logical data model into Dimensional Models.\n* Involved in project cycle plan for the data warehouse, source data analysis, data extraction process, transformation and loading strategy designing.\n* Worked on Conceptual, Logical Modeling and Physical Database design for OLTP and OLAP systems.\n* Designed a STAR schema for sales data involving shared dimensions (Conformed) using Erwin Data Modeler.\n* Designed and build the OLAP cubes using Star schema and Snow Flake Schema using native OLAP Service Manager.\n* Extensively used Teradata utilities (BTEQ, Fast load, Multiload, TPUMP) to import/export and load the data from oracle, flat files.\n* Performed Data Analysis tasks on warehouses from several sources like Oracle, DB2, and XML etc and generated various reports and documents.\n* Created Database Maintenance Plans for the performance of SQL Server which covers Database Integrity checks, update database Statistics and Re-indexing.\n* Involved in workflows and monitored jobs using Informatica tools. Developed SQL, PL/SQL and ETL scripts on UNIX.\n* Migrated data from SAS environment to SQL Server 2008 via SQL Integration Services (SSIS).\n* Used External Loaders like Multi Load, T Pump and Fast Load to load data into Teradata Database, Involved in analysis, development, testing, implementation and deployment.\n* Used SSIS to create ETL packages to Validate, Extract, Transform and Load data into Data Warehouse and Data Mart.\n* Actively involved in Normalization (3NF) & De-normalization of database.\n* Developed multiple processes for Daily Data Ingestion from Client associated data vendors and Production Team, Client site employees using SSIS and SSRS.\n* Created multiple custom SQL queries in Teradata SQL Workbench to prepare the right data sets for Tableau dashboards. Queries involved retrieving data from multiple tables using various join conditions that enabled to utilize efficiently optimized data extracts for Tableau workbooks.\n* Resolved the data type inconsistencies between the source systems and the target system using the mapping documents and analyzing the database using SQL queries.\n* Extensively used Extract Transform Loading (ETL) tool of SQL Server to populate data from various data sources.\n* Extensively used SQL Loader to load data from the Legacy systems into Oracle databases using control files and used Oracle External Tables feature to read the data from flat files into Oracle staging tables.\n\nEnvironment: ERwin9.1, Data Modeling, Informatica Power Center9.6, Taradata SQL, PL/SQL, BTEQ, DB2, Oracle, Agile, ETL, Tableau, Cognos, Business Objects, UNIX, SQL Server2008, Azure, TOAD, SAS, SSRS, SSIS, T-SQL etc.']","[u'Masters of Science in Computer Science in Computer Science', u'in Technology']","[u'University of Texas at Arlington Arlington, TX\nMay 2016', u'SRM University\nMay 2011']","degree_1 : Masters of Science in Compter Science in Compter Science, degree_2 :  in Technology"
0,https://resumes.indeed.com/resume/cdfbcc942ce890bd,"[u'Data Scientist\nForeal Spectrum - Fremont, CA\nAugust 2017 to Present\nDesigned multiple real-time prediction models based on both supervised and unsupervised learning.\n\u25e6 Performed image segmentation on food image data using pyrMeanShiftFiltering in OpenCV.\n\u25e6 Implemented fine-tuned InceptionV3 and InceptionResNetV2 models with Keras to do food recognition on school cafeteria food data set with over 100 classes and achieved 93 % accuracy for top 5 classes.\n\u25e6 Applied Faster R-CNN using TensorFlow with VGG16 pre-trained model on food images with multiple dishes\nto detect location of food and recognized food type simultaneously to save time.', u'Assistant Data Analyst\nOverseas Chinese Fund - Hangzhou, CN\nJuly 2016 to August 2016\nWrote SQL queries to get age, investment amount and geographic distribution for investors throughout the whole year. Improved customer experience by analyzing collected information.\n\u25e6 Found potential investors of all ages to help company further expand the business regions.\n\u25e6 Determined the peak time to invest every day to help engineering team to make proper arrangement.\n\u25e6 Found the most popular range of investment amount then eliminated 3 redundant investment options.']","[u'Master of Science in Data Science in Data Science', u'Bachelor of Science in Applied Mathematics in Applied Mathematics']","[u'New York University New York, NY\nSeptember 2015 to May 2017', u'ZheJiang University Hangzhou, CN\nSeptember 2011 to June 2015']","degree_1 : Master of Science in Data Science in Data Science, degree_2 :  Bachelor of Science in Applied Mathematics in Applied Mathematics"
0,https://resumes.indeed.com/resume/d0c7af5e3c065757,"[u""Data Scientist\nColaberry - Boston, MA\nFebruary 2017 to December 2017\nOverview: Colaberry http://colaberry.com/ | RefactorEd https://www.refactored.ai/\n\nResponsibilities:\n\xa7 Designed, built and tuned weekly Content Analysis for client using data from Google analytics to measure the relationship between various article types and authors\n\xa7 Optimized Digital Marketing strategies based on Revenue reports from Facebook and Programmatic sources to increase\ntraffic and revenue through various social media channels leading to 25% increase in Facebook and Google Ad revenue\n\xa7 Increased company's web traffic and revenue by 12% by deriving insights on company's internal data, applying data\nscience modelling techniques and advance statistical models\n\xa7 Worked on large datasets, transformed information from raw data into meaningful analysis that identifies trends and predicts outcomes using various linear and nonlinear models\n\xa7 Developed a web-scrapper from scratch using python, to collect data from relevant blogs/articles/news on internet and performed various analysis on the data using NLP techniques to get insights\n\xa7 Leveraged data to provide strategic business decisions to increase sales and effectiveness of marketing efforts of the team\n\xa7 Created dashboards to provide information on current marketing KPIs and actionable marketing insights using tableau."", u""Business Operation Associate\nZS Associates - Gurgaon, Haryana\nJuly 2014 to May 2016\nOverview: Consultant | Analytics | Data Strategy\n\xa7 Led teams in various client engagements, managed client relationships.\n\xa7 Designed, built and fine-tuned predictive models to determine sales-rep goals, helped the client with the decision making,\ncutting their losses substantially and making them 7% more profitable.\n\xa7 Conducted market research for client; analyzed the survey data to strategize client's product launch using data science\n\xa7 Engineered complex incentive compensation model using SQL based tool for a client's business unit having 1000+ sales\nrepresentatives.\n\xa7 Secured $12 million-dollar funding of client renewal business for the company for which was awarded Operational\nExcellence award\n\xa7 Managed to retain my project team with zero attrition and 100% client satisfaction.\n\nMarketing Insights and Data Science Projects:\nCredit Data Analysis (Scikit-Learn, Seaborn, Pandas, Statsmodels)\n\xa7 Implemented and compared various machine learning algorithms such as - KNN, Decision tree, Na\xefve Bayes, Support\nVector machines etc. with respect to their accuracy to classify customers having Good/Bad credit.\nChurn Analysis Using R programming (e1071, randomForest, tree)\n\xa7 Implemented various tree-based algorithm such as Decision tree, Random forest, XG-Boost and Bagging to find out the various parameters leading to churn of customers.\nMarketing Predictive Analysis on Transaction Data using Python\n\xa7 Performed Predictive analysis on 50000+ observations from Grocery and Drugstore dataset to identify consumer behavior\nAnalyzed various demographics to understand characteristics of different consumers and performed RFM analysis to identify most profitable consumer segments\nMarketing Research project for a Real Estate firm in Dallas (SPSS, Survey data, R)\n\xa7 Designed and distributed survey for a local real estate firm's new student housing project to understand consumer choices and utility of various attributes performed conjoint and other quantitative analysis on survey data using SPSS to identify\ntarget markets and recommend product positioning strategies""]","[u'M.S. in Business Analytics', u'Bachelor of Engineering in Electrical and Telecommunication Engineering']","[u'University of Texas at Dallas Dallas, TX\nAugust 2016 to May 2018', u'Army Institute of Technology\nJune 2010 to July 2014']","degree_1 : M.S. in Bsiness Analytics, degree_2 :  Bachelor of Engineering in Electrical and Telecommnication Engineering"
0,https://resumes.indeed.com/resume/0679158651ada276,"[u""Data Scientist\nArgus Information & Advisory Services, LLC - White Plains, NY\nSeptember 2016 to Present\nArgus Information and Advisory Services is a Subsidiary of Verisk Analytics Company and the leading provider of analytics, information and solutions to consumer banks and their regulators. The company's clients range from financial institutions to retailers and tech companies. The project focused on detecting anti-money laundering violation using Big Data and Data Science tools and improving customer's transaction monitoring system.\nResponsibilities:\n\u2022 Collected and analyzed the business requirements, understood the particular Fraud/AML challenges that our client faces.\n\u2022 Participated in Data integration job with Data Engineer team to gather traditional transaction data and external source data together.\n\u2022 Transformed data from SQL Server database to Hadoop Clusters which is set up by using AWS EMR.\n\u2022 Conducted data cleansing and feature engineering job through python NumPy and Pandas.\n\u2022 Implemented Naive Bayes, Logistic Regression, SVM, Random Forest and Gradient boosting with weighted loss function by using Python Scikit-learn.\n\u2022 Implemented mulit-layers Neural Networks by using Google Tensorflow and Spark.\n\u2022 Performed extensive Behavioral modeling and Customer Segmentation to discover behavior patterns of customers by using K-means Clustering.\n\u2022 Managed and scheduled models by using Oozie for batch processing.\n\u2022 Updated and saved Fraud predictions to AWS S3 for application team.\n\u2022 Tested the business performance of the AML models by evaluating detection rate and false positive rate and worked on continuous improvement on model.\n\u2022 Created reports and dashboards, by using Tableau, to explain and communicated data insights, significant features, model's score and performance of new transaction monitoring system to both technical and business teams.\n\u2022 Used GitHub for version control with Data Engineer team and Data Scientists colleagues.\nEnvironment: SQL Server 2014, Hadoop 2.0, Hive 2.0, Spark (PySpark, SparkSQL), Python 3.X, Tensorflow, Oozie 4.2, Tableau 10.X, AWS S3/EC2/EMR, Github"", u'Data Scientist\nCenterLight Health System - Bronx, NY\nApril 2015 to July 2016\nCenterLight Health System, a not-for-profit organization, has evolved into a leader in serving the elderly, chronically ill and disabled. CenterLight is one of the largest long-term care providers in New York State, serving all of New York City, Westchester, Nassau, Rockland and Suffolk Counties. This project aimed to predict the billing cycles and accounting related issues to increase the efficiency of enterprise claim processing.\n\nResponsibilities:\n\u2022 Conducted reverse engineering based on demo reports to understand the data without documentation.\n\u2022 Generated new data mapping documentations and redefined the proper requirements in detail.\n\u2022 Generated different Data Marts for gathering the tables needed (Member info, Claim info, Transaction info, Appointment info, Diagnose info) from SQL Server Database.\n\u2022 Created ETL packages to transform data into the right format and join tables together to get all features required using SSIS.\n\u2022 Processed data using Python pandas to examine transaction data, identify outliers and inconsistencies.\n\u2022 Conducted exploratory data analysis using python NumPy and Seaborn to see the insights of data and validate each feature through different charts and graphs.\n\u2022 Built predictive models including Linear regression, Lasso Regression, Random Forest Regression and Support Vector Regression to predict the claim closing gap by using python scikit-learn.\n\u2022 Used GridSearchCV to evaluate each model and to find best parameters set for each model.\n\u2022 Created reports and an app demo using Tableau to show client how prediction can help the business.\n\u2022 Deployed and hosted our models by using Azure Machine Learning Studio and share an API with application development team.\n\u2022 Used Confluence to share and collaborate on projects with team members, and keep track of up to date documentations.\n\nEnvironment: SQL Server 2012, SQL Server Data Tools 2010, SQL Server Integration Services, Python 2.7/3.3, Tableau 9.4, Azure Machine Learning Studio', u""Junior Data Scientist\nAtlantic Health - Morristown, NJ\nJanuary 2014 to March 2015\nAtlantic Health System is one of the leading non-profit health care systems in New Jersey, providing a wide array of health care services to the residents of Northern and Central regions of the state as well as Pike County, PA, and southern Orange County, NY. Project was to build a predictive model to predict the readmission case. The main objective was to reduce the risk of being wrongly diagnosed and the risk of being involved in the legal disputes.\n\nResponsibilities:\n\u2022 Communicated and coordinated with other departments to gather business requirements.\n\u2022 Gathered data information from multiple sources, and performed resampling method to handle the issue of imbalanced data.\n\u2022 Worked with ETL Team and Doctors to understand the data and define the uniform standard format.\n\u2022 Conducted data cleansing by using advanced SQL queries in SQL Server Database.\n\u2022 Split the data into different smaller dataset based on different diagnoses, in charge of conducting exploratory data analysis for three of diagnoses datasets (Diabetes, cold/flu, allergy).\n\u2022 Created the whole pipeline of data preprocessing (imputing, scaling, label encoding) through python pandas to get data ready to modeling part.\n\u2022 Built predictive models, using python scikit-learn, including Support Vector Machine, Decision tree, Naive Bayes Classifier, Neural Network to predict a potential readmitted case.\n\u2022 Performed Ensemble methods, including Gradient Boosting, Random Forest, customized ensemble method to produce more accurate solutions.\n\u2022 Designed and implemented cross-validation and statistical tests including Hypothesis testing, AVOVA, Chi-square test to verify models' significance.\n\u2022 Created a API by using Flask and shared the idea with application team and help them define the requirements of new application.\n\u2022 Used Agile methodology and Scrum process for project developing.\n\nEnvironment: SQL server 2012, SQL Server Integration Services, Python 2.7, Jupyter notebook, Flask 0.10, SharePoint 2013"", u'BI Developer\nFulton Financial Corporation - Lancaster, PA\nDecember 2012 to October 2013\nFulton is a financial company based in Lancaster, Pennsylvania. They provide a wide range of financial products and personalized services in Pennsylvania, Maryland, Delaware, Virginia and New Jersey. They are comprised of several different banking subsidiaries. The main job of this project was to provide ETL solutions for data migration and provide data quality and micro strategy solutions.\n\nResponsibilities:\n\u2022 Involved in gathering user/project requirements from business users and IT managers, translated it into functional and non-functional specifications needed and created documentations for the project.\n\u2022 Assisted in design and data modeling efforts of Data Marts and Enterprise Data Warehouse.\n\u2022 Used T-SQL in SQL Server to develop complex stored procedures, triggers, clustered index & non-clustered index, Views, and User-defined Functions (UDFs).\n\u2022 Designed SSIS packages to extract, transform and load existing data into SQL Server, used lots of components of SSIS, such as Pivot Transformation, Fuzzy Lookup, Derived Columns, Condition Split, Aggregate, Execute SQL Task, Data Flow Task and Execute Package Task.\n\u2022 Created SSIS Packages that involved dealing with different source formats (Text files, XML, Database Tables)\n\u2022 Debugged and troubleshot the ETL packages by using breakpoint, analyzing process, catching error information by SQL command in SSIS.\n\u2022 Create reports with the use of SSRS to generate different types of reports such as tabular, matrix, drill down and charts reports with accordance with user requirement.\n\u2022 Maintained and updated existing reports, analyzed the SQL queries and logic behind them to improve the performance.\n\u2022 Helped deploy the report with scheduling, subscription, history snapshot configured and set up.\n\u2022 Developed in Agile environment throughout the project.\n\nEnvironment: SQL server 2008/2012, SQL Server Management Studio (SSMS), MS BI Suite (SSIS, SSRS)']",[u'Master of Science in Electrical Engineering'],[u'Stevens Institute of Technology'],degree_1 : Master of Science in Electrical Engineering
0,https://resumes.indeed.com/resume/6734fceeeab868cc,"[u'Data Engineer\nThird Eye Data - Santa Clara, CA\nJune 2017 to February 2018\nResponsibilities:\n\u2022 Joined on the team of an ongoing project. Discussed with my mentor for better optimal implementation of Spark, AWS and Cassandra.\n\u2022 Implemented the idea where spark ML libraries were used to run various models on the No-SQL data and do that using AWS and Cassandra to see if there is any change in performance.\n\u2022 Assisted in some problem solving for the ongoing project.\n\u2022 Built relational databases in SQL server of several flat files of partner information from several large (5-10 GB) flat files in Python. Used logistics regression and random forests models in R/Python to predict the likelihood of partner participation in various marketing programs. Designed and developed visualizations and dashboards in R /Tableau that surfaced the primary factors that drove program participation and identified the best targets for future targeted marketing efforts.\n\u2022 Worked on a cloud-based application that will be using Amazon Web Services(AWS) EC2 as the service and will execute SQL commands on the SQL database installed on the EC2 instance. All of the code was written in Java. And perform the time performance taken to execute the commands 1000, 5000 and 10,000 times.\n\u2022 Used Python and SOAP protocol that would get the data from ndfd XML servers in a XML format and print that data in a understandable format.\n\u2022 Involved in software Unit, functional, system, User Acceptance, UI testing.', u'Data Scientist\nGroupon - Chicago, IL\nJanuary 2016 to December 2016\n\u2022 Responsibilities:\n\u2022 Collaborated with business to understand company needs and devise possible solutions\n\u2022 Analyzed and solved business problems, and found patterns and insights within structured and unstructured data\n\u2022 Cleaned, analyzed and selected data to gauge customer experience\n\u2022 Implemented new statistical and mathematical methodologies as needed for specific models or analysis\n\u2022 Used algorithms and programming to efficiently go through large datasets and apply treatments, filters, and conditions as needed\n\u2022 Created meaningful data visualizations to communicate findings and relate them back to how they create business impact\n\u2022 Implemented various Machine Learning modeling techniques with Java, Python and Matlab as the programming language:', u'Data Engineer\nDesign Innova, New Delhi - New Delhi, Delhi\nAugust 2013 to August 2014\n\u2022 Developed a queue management system that would fetch queue numbers and display them on the screen.\n\u2022 Worked on developing a front-end that would get that data in JSON format using Java and display that data on the screens using HTML, Javascript and Jquery.\n\u2022 Worked on various Data modeling techniques.', u'Software Engineer Intern\nAT&T - Noida, Uttar Pradesh\nMay 2013 to August 2013\n\u2022 Project based on technologies such as Raspberry Pi, Java, Pi4J api and PHP/Javascript.\n\u2022 Creating a LAMP server in order to have the server record values from the raspberry pi and upload them on a website using PHP/Javascript and those values can be accessed from any device having connection to internet.']","[u'Master of Computer Science and Engineering in Computer Science and Engineering', u'Bachelor of Computer Science and Engineering in Computer Science and Engineering']","[u'University of Tex as at Arlington Arlington, TX', u'Amity University Noida, Uttar Pradesh']","degree_1 : Master of Compter Science and Engineering in Compter Science and Engineering, degree_2 :  Bachelor of Compter Science and Engineering in Compter Science and Engineering"
0,https://resumes.indeed.com/resume/06fd18405518ec64,"[u'Lead iOS Developer\nSelf Employed - Issaquah, WA\nDecember 2016 to Present\nCreator of a mobile application for a fast paced commercial maintenance company looking to expand nationally. Skills: Swift, Cocoa Touch, GCD, delegation, Firebase, notifications, MapKit, PDFKit, geofencing, MVC, animation, Database design, HTTP/S, XCode\nDesigned and implemented mobile application that streamlines key processes and enables visualization of key performance metrics enabling an increase in productivity and profit.\nEnables instant/clear communication between client and company that will spur national growth.\nLead in customer communication, database design/structuring, front facing design and technical requirements development.', u'Lead Data Scientist/Analyst\nUnited States Air Force - Seattle, WA\nMay 2016 to Present\nAwarded Employee of the Month Dec 2016 (#1/153), Test Team of the Year 2016, Test Team of the Quarter 2017. Lead Analyst for multidisciplinary operational test team supporting $52 Billion KC-46A program. Lead of data management and analytics including coordinating the test design process and incorporating experimental design and reliability growth principles to manage cost and schedule.\n-Saved $500K by optimizing test design to reduce data test points by 40% while maintaining statistical significance.\n-Communicated technical requirements to multiple levels of leadership influencing key management decisions.\n-Guided 12-man team in metric development, data management and analysis planning for $7.1M Operational Test of KC-46A ensuring all required and necessary data is collected and planned data analysis/modeling is technically sound and defendable.\n-Automated test documentation creation and data collection processes reducing man-hours required to complete document creation and collection tasks by 70%.', u'Data Scientist - Senior Data Scientist/Manager\nUnited States Air Force (Active Duty) - Vandenberg AFB, CA\nAugust 2013 to August 2016\nAwarded Air Force Achievement Medal for leading a six-man operations assessment team for semi-annual global Combatant Command Exercise and supporting the creation of assessment framework for linkages of operational space objectives to tactical tasks and quantitative measures.\nLed 6-member operational assessment team that developed and analyzed 90+ metrics and briefed critical results daily to Generals during four semi-annual high octane two-week global exercises.\nSpearheaded an in-depth analysis into physical therapy clinics that resulted in an $8K/qtrly savings and resulted in a framework that received recognition from Surgeon General of the Air Force.\nCoded VBA Excel tool that streamlined in-processing/out-processing checklists for 1000+ airmen unit. Streamlining resulted in a savings of 300+ man-hrs/yr and is still in use 4 years later.\nAnalyzed 5,000 data points and identified 100+ anomalies near real time which increased training reality for 29,000 participants in multi-national one-week exercise.\nInvented analytic automation framework for 190+ metrics that streamlined the analytic process by 80% and was implemented at the Global Combatant Command Level.', u'Unit Fitness Program Manager\nUnited States Air Force (Active Duty) - Vandenberg AFB, CA\nAugust 2014 to May 2016\nScheduled individuals for fitness assessments. Initiated and maintained fitness program case files. Informed airmen of BE WELL programs and requirements. Ensured airmen followed through to completion. Provided fitness metrics and unit status report to the Unit Commander on a monthly basis. Devised and led weekly unit PT sessions. Augmented the fitness assessment cell in administering fitness assessments as well as conducted practice fitness assessments.\nLed 120+ member unit handling all members score tracking and BE WELL programs.\nLed weekly fitness sessions of 120+ member to ensure each member maintained physical fitness standards']","[u'BS in Computer Science', u'BS in Operations Research']","[u'Oregon State University Corvallis, OR\nAugust 2015 to December 2017', u'United States Air Force Academy Colorado Springs, CO\nJune 2009 to May 2013']","degree_1 : BS in Compter Science, degree_2 :  BS in Operations Research"
0,https://resumes.indeed.com/resume/37da632c4ec7cd6b,"[u'Research Assistant\nStevens Institute of Technology - Hoboken, NJ\nSeptember 2017 to Present\n\u2022 Contribute to research in multimedia data analysis. Design an Methodology for multimedia data analysis for voice data, image data, category data, text data.\n\u2022 Design video sentiment analysis model will apply to commercial public speech. Plan to combine it with text\nsentiment analysis model, to mine alternative data from video.\n\u2022 Design and implement of text sentiment analysis ML components applied to the commercial conference\ncall; Automatically abstract alternative data from text file.\n\u2022 Implementation and develop the Cross-Section of Stock Return analysis model applied to annual\naccounting data come from CRSP/COMPUSTAT files (Pyspark, MapReduce)', u'Data Scientist\nHangzhou Testing Alliance Technology - Hangzhou, CN\nSeptember 2016 to February 2018\n\u2022 Contributed to developing image recognition research\nproject.\n\u2022 Developed and implemented fine-grained image classification model applied to the agricultural product\nimage.\n\u2022 Designed, implemented and developed the auto-news system. Include auto-news mining system, news\nclassification system, news popularity calculation system. This program will automatically mine news from websites, then do analysis and classification. Finally, calculate the popularity of each news and save the\ndata to the database.\n\u2022 News classification system implemented by RNN and w2v encoding model. Finally, achieved 92.5% accuracy of the validation set.\n\u2022 Implemented web crawler, utilize machine learning mining data from the web source']","[u'Master of Science in Computer Science', u'Master of Engineering in Computer Engineering', u'Bachelor of Engineering in Electrical and Electronics']","[u'Stevens Institute of Technology Hoboken, NJ\nSeptember 2017 to May 2018', u'Stevens Institute of Technology Hoboken, NJ\nSeptember 2016 to May 2017', u'Stevens Institute of Technology Hoboken, NJ\nSeptember 2012 to May 2016']","degree_1 : Master of Science in Compter Science, degree_2 :  Master of Engineering in Compter Engineering, degree_3 :  Bachelor of Engineering in Electrical and Electronics"
0,https://resumes.indeed.com/resume/bcf71df73a040ea6,"[u'Data Scientist\nNew York City Health and Hospitals - New York, NY\nJanuary 2018 to January 2018\n- May 2018\n\u25cf No show predictive analysis using SVM with RBF Kernel to predict if a patient would show up for his appointment or not,\nUsed Principal component analysis to reduce the dimensionality, Gradient decent, Min-max normalization technique\n\u25cf Contextual intent based Chatbot developed using DNN, Tflearn, numpy, tensorflow, Attention mechanism and flask', u'Data Analyst\nXtilytics Health Care - New York, NY\nMay 2017 to August 2017\nMerged Psoriasis disease dataset and weather dataset of 4 major cities in the United States\n\u25cf ETL the datasets using open refine and Pentaho, handled missing values and found co-relation values for variables\n\u25cf Analyzed if weather has any effect on psoriasis disease using data from Past 7 years in R language and visualized outputs by plotting time-series graphs for better understanding using library such as GGplot2, dplyr, lubridate, Zoo', u""Web Developer\nElasticsearch (ELK) Stack - Mumbai, Maharashtra\nJanuary 2015 to July 2016\nCreated a product based search engine cluster using elasticsearch consisting of four nodes on AWS cloud service thereby\nimproving user experience and increasing websites overall traffic by 20%\n\u25cf Used Logstash to gather and transport data to elasticsearch cluster and use Kibana for data visualization which helped\nanalyze customer, their behavior, location details and top revenue generating products\n\u25cf Concept covered such as Fuzzy logic, Pattern matching, Phase matching, Pre-fix Query, Post-fix Query, sharding\n\u25cf Scrapping data using scrappy.org to extract content from select websites using python for business gains\n\u25cf Communicated directly with Potential Clients across the state and outlined our website's fundamental properties thereby\nConvincing them to get listed on the website increasing the total listing\n\u25cf Converted our website to a mobile friendly technology using bootstrap and hence gained experience in HTML, CSS, MySQL\nDatabases, and JavaScript""]","[u'Masters of Science', u'', u'Bachelor of Engineering in Information Technology', u'', u'']","[u'Pace University, Seidenberg School of Computer Science and Information Systems New York, NY\nMay 2018', u'Pace University New York, NY\nFebruary 2017', u'University of Mumbai Mumbai, Maharashtra\nMay 2015', u'Don Bosco Institute of Technology\nJanuary 2015', u'SGD']","degree_1 : Masters of Science, degree_2 :  , degree_3 :  Bachelor of Engineering in Information Technology, degree_4 :  , degree_5 :  "
0,https://resumes.indeed.com/resume/526e3bfd3ba725bc,"[u""Data Scientist\nKotak Mahindra Bank\nSeptember 2016 to July 2017\nTechnologies: Python, SAS, MySQL, MATLAB, Microsoft Revolution R\n\u2022 Helped build a 'Customer Persona' based approach for digital marketing of the bank's products. It was accepted as the bank's analytics motto for the Financial Year 2017-18.\n\u2022 Developed a Logistic Regression model to predict whether a customer will pay income tax through the bank's facilities\n\u2022 Implemented an unsupervised learning model to cluster the text of customer complaints to improve customer\nexperience.\n\u2022 Analyzed cash withdrawal transactions at an ATM level to find non performing machines and locations.\n\u2022 Developed a strategy to implement a customer feedback system based on the demography of each and every branch\n\u2022 Analysis of demonetization of 500 and 1000 Rupee Notes on the bank's Digital Channels and Call Centers"", u""D.C.S. Engineer\nEmerson Process Management\nJune 2015 to March 2016\nInterfaced and tested third party PLC with Distributed Control System(DCS) using Modbus Protocol\n\nPROJECTS\n\u2022 Artistic Neural Style Transfer Using Convolutional Neural Networks Jan '18 - Present\n- Currently working on transferring the content information and the texture information from two separate\nimages and morph them into one image by minimizing the style and content loss simultaneously using VGG19\narchitecture.\n\u2022 Collaborative Filter based Recommender System Sep '17 - Dec '17\n- Developed User based and Item based filters to build a Movie recommender on the MovieLens dataset. Different\nmodels were built by tweaking the similarity metric(Pearson's correlation and Cosine similarity) and the number of nearest neighbors that go into the predicted rating of a movie by a user. Achieved a RMSE of 0.86.\n\u2022 Email Fraud Analysis May '16 - Aug '16\n- Developed a Naive Bayes bag of words model to predict whether an incoming email is a fraud and spam email or not. Compared this algorithm with other classification algorithms like logistic regression, KNN classification and\ndecision trees.""]","[u""Master's of Science in Computer Engineering"", u'Certificate', u'Bachelor of Technology in Instrumentation & Control Engineering']","[u'Rochester Institute of Technology Rochester, NY\nAugust 2017', u'S.P. Jain School of Global Management Mumbai, Maharashtra\nFebruary 2016 to August 2016', u'Vishwakarma Institute of Technology Pune, Maharashtra\nAugust 2011 to May 2015']","degree_1 : ""Masters of Science in Compter Engineering"", degree_2 :  Certificate, degree_3 :  Bachelor of Technology in Instrmentation & Control Engineering"
0,https://resumes.indeed.com/resume/c4ef0298ff7bad62,"[u'Senior Data Scientist\nSynPon - Mountain View, CA\nJune 2013 to Present\n- Natural language understanding (NLU) systems which read and answer questions with Wikipedia\n- System identification and state estimation for humanoid robotics with MuJoCo\n- Deep reinforcement learning agents that play Atari with Tensorflow\n- Image caption generation using Caffe and the MS COCO dataset\n- Open source and commercial software development and consulting services', u'Machine Learning Scientist\nASTOUND - Menlo Park, CA\nDecember 2017 to March 2018\n\u2022 Development and Implementation of AutoML systems\n\u2022 Applied Natural Language Understanding', u'Data Analyst\nIntel - Hillsboro, OR\nMay 2017 to August 2017\n\u2022 Creation of automated ETL tools using Pandas and MongoDB\n\u2022 Strategic insight through analysis of time series data\n\u2022 Developed novel web scrapers for data acquisition.', u'Senior Data Analyst\nMicrosoft - Seattle, WA\nJanuary 2016 to February 2016\n\u2022 Text Categorization and Sentiment Analysis with AzureML\n\u2022 Data Munging with SQL and Python\n\u2022 Data Science training and consultation', u'Software Engineer\nSportswear, Inc. - Seattle, WA\nDecember 2014 to May 2015\n\u2022 Image processing and digital embroidery software engineering with CUDA and C++\n\u2022 Build and test engineering with Python\n\u2022 Data analysis and strategic insight with Python', u'Research Scientist\nUniversity of Washington - Seattle, WA\nApril 2013 to May 2013\n\u2022 Developed software to repurpose FDA approved compounds through combining atomic level knowledge of compound-protein interactions with compound-disease associations which was able to identify a known use for over one in four FDA approved drugs which passed an initial screening against the human genome.', u'Research Fellow\nUniversity of Washington - Seattle, WA\nJuly 2011 to August 2012\n\u2022 Created method to simulate single nucleotide polymorphisms whose ROC curve was substantially above the identity function through Gaussian anomaly detection to identify mutations which effected gene function.']","[u'MS in Applied Mathematics', u'BS in Physics']","[u'University of Washington Seattle, WA\nJanuary 2011 to January 2013', u'Lamar University Beaumont, TX\nJanuary 2000 to January 2007']","degree_1 : MS in Applied Mathematics, degree_2 :  BS in Physics"
0,https://resumes.indeed.com/resume/0225942b10cf4572,"[u'Data Science Consultant\nCARLSON ANALYTICS LAB, Carlson School of Management - Minneapolis, MN\nJuly 2017 to Present\nClient: Fortune 200 CPG firm\n\u2022Aiding to devise store retention strategies to stabilize profit across sales channels; Defining personalized churn threshold using K-means clustering to predict future churners with tuned random forest model in scikit-learn(Python)\n\u2022Examining drivers of profit across sales channels i.e. velocity and distribution by leveraging PowerBI\n\nClient: Mall of America Live Case Competition(Finalist)\n\u2022Led to ease in assessment of customer experience by formulating key metrics like mall and park engagement score in R; Designed personalized promotions using K-prototypes clustering to improve customer satisfaction\n\u2022Recognized customer concern of ride queue waiting time by web scraping TripAdvisor user reviews with R\u2019s rvest package; Applied LDA algorithm to categorize reviews\n\u2022Reduced daily and weekly park ridership prediction error by 10% by generating new features and adopting stacked predictive models in R\u2019s caret package\n\nClient: PwC Live Case Competition(Finalist)\n\u2022Constructed methodology to increase profitability of auto-insurance business by $2.1M by creating effective Tableau reports and explanatory logistic models in RapidMiner which helped in finding factors leading to declining profit', u'Decision Scientist\nMu Sigma Business Solutions PVT. LTD - Bengaluru, Karnataka\nAugust 2014 to March 2017\n\u2022Analysed impact of personalized email marketing campaigns directed towards physicians\u2019 prescribing behaviour of drugs through A/B testing; drove faster results by automating A/B testing process in SAS and R\n\u2022Built decision tree models in R to determine factors affecting physician\u2019s response to email campaigns in Teradata SQL\n\u2022Constructed one stop solution for product teams utilising Teradata SQL and Tableau leading to reduction in response time for common email campaign performance requests by ~95% by combining 6+ data sources\n\u2022Orchestrated team of 3 to acquire $0.7M project proposal by collaborating with multiple stakeholders and peers to design project mock-ups tailored to business\u2019s needs\n\u2022Uncovered opportunity worth $500M for one of the largest retailer in US by implementing KPIs that identified under-performing items in Greenplum SQL and providing an interactive Tableau dashboard as end product\n\u2022Eliminated human effort worth 14 days per month in refreshes of high importance reports by automating refresh and quality check process in DSS (Data Science Studio)\n\u2022Conducted Tableau training for directors and senior managers of largest pharmaceutical manufacturer in US']","[u'Master of Science in Business Analytics', u'Bachelor of Engineering in Information Technology']","[u'University Of Minnesota - Carlson School of Management Minneapolis, MN\nMay 2017 to May 2018', u'UNIVERSITY OF PUNE Pune, Maharashtra\nMay 2014']","degree_1 : Master of Science in Bsiness Analytics, degree_2 :  Bachelor of Engineering in Information Technology"
0,https://resumes.indeed.com/resume/bffd0416345526a6,"[u""Chief Clinical Data Scientist\nAllscripts - Denver, CO\nAugust 2016 to Present\nPerformed exploratory data analysis and implemented various machine learning techniques on clinical patient data\non over 43 million Electronic Medical Records from over 3,000 hospitals and clinics across the United States.\n\u2022 Created a python tool that accurately maps dirty problem descriptions to correct ICD9 / ICD10 / SNOMED codes by merging various Natural Language Processing techniques.\n\u2022 Lead specific projects to understand our data in diseases, such as Opioid addiction, Diabetes, Chronic Kidney\nDisorder, Renal Failure, and Chronic Obstructive Kidney Disorder.\n\u2022 Proposed and implemented a project to predict Kidney Failure as a complication of Type II Diabetes Mellitus.\n\u2022 Managed and helped interns across the country with their various projects.\n\u2022 Statistically analyzed public health data to solve clients' problems, then provided clean graphs / charts for easy\ninterpretability."", u""Staffing Coordinator (Temporary)\nMacy's - Hicksville, NY\nOctober 2015 to January 2016\nProgrammed macros using VBA in Microsoft Excel to organize data for affiliated stores, to assist in developing\nstaffing strategies for upcoming holiday seasons, and to update forms to become more efficient and user-friendly.\n\u2022 Assigned holiday schedules to over two thousand seasonal new-hires nationwide."", u'Legal Recruiting Assistant (Temporary)\nPaul, Weiss, Rifkind, Wharton & Garrison LLP - New York, NY\nJuly 2015 to September 2015\nMaintained recruitment database: created and updated applicant profiles to reflect acceptances and rejections;\nscanned resumes and transcripts; tracked follow-up correspondence with Attorneys.', u'Staffing Coordinator\nFive Star Staffing Services - Uniondale, NY\nFebruary 2014 to June 2015\nFulfilled staffing needs for nursing homes throughout Nassau County, reducing their overtime by over 80%.']","[u'Master of Science in Data Science in Data Science', u'Bachelor of Science in Mathematics in Economics, Sociology, Statistics']","[u'New Haven University San Francisco, CA\nJanuary 2016 to December 2016', u'University at Buffalo, State University of New York, Buffalo Buffalo, NY\nAugust 2009 to May 2013']","degree_1 : Master of Science in Data Science in Data Science, degree_2 :  Bachelor of Science in Mathematics in Economics, degree_3 :  Sociology, degree_4 :  Statistics"
0,https://resumes.indeed.com/resume/cca144832d8ddf36,"[u""Data Scientist\nCisco - San Jose, CA\nMarch 2017 to Present\nDevelop predictive models on large scale datasets to address various business problems through leveraging advanced statistical modeling, machine learning and deep learning.\n\nIdentify key processes within Cisco's Supply chain which can be improved significantly using advanced analytics / data science and thereby strive for continuous improvement.\n\nExtracting meaning from huge volumes of data to help improve decision making and to provide business intelligence through data driven solutions.\n\nWork closely with other analysts, data engineers to develop data infrastructure (data pipelines, reports, dashboards etc.) and other tools to make analytics more effective.\n\nGuiding and teaching junior analysts and helping them with their data science training."", u'Research Assistant\nKnoesis Research Center - Dayton, OH\nJanuary 2015 to December 2016\nInvestigated, characterized, and developed predictive models in the analysis of a large scale social network with millions of edges and a big dataset of user attributes. Research methods emphasize exploratory data analysis, time series analysis, statistical inference, reproducible research, predictive modeling, natural language processing, graph mining and social network analysis.\n\nResearch published in two papers, including one best paper award at ACM/IEEE ASONAM 2015.']",[u'Masters in Computer Engineering in Data Science'],"[u'Wright State University Dayton, OH\nAugust 2014 to December 2016']",degree_1 : Masters in Compter Engineering in Data Science
0,https://resumes.indeed.com/resume/644adeab7aacd848,"[u'Senior Geophysicist/Data Scientist\nCGG US Imaging\nMay 2013 to November 2017\nDeveloped in-house machine learning solutions to solve the cutting-edge seismic imaging problems with 2 publications on top conferences in the industry\n\u25cf Innovatively Identified and developed machine learning applications to automate and improve the production efficiency, such as automatic fault delineation using Deep Neural Networks and automatic rock facies classification using unsupervised classification algorithms\n\u25cf Led a team and participated in high-profile seismic imaging projects that helped major oil companies making multi-billion decisions on well drilling plan']","[u'PhD in Applied Physics', u'M.S. in Space Physics', u'B.S. in Physics']","[u'Rice University Houston, TX\nJanuary 2013', u'Chinese Academy of Science\nJanuary 2008', u'University of Science & Technology of China\nJanuary 2005']","degree_1 : PhD in Applied Physics, degree_2 :  M.S. in Space Physics, degree_3 :  B.S. in Physics"
0,https://resumes.indeed.com/resume/1c5a77763c4f05d1,"[u'Chief Data Scientist/ R Programmer(Remote)\nQuantFaram - Cupertino, CA\nJune 2017 to Present\nProjects and Responsibilities:\nMajor focus is to implement AI/ML in health and life science projects. I am also working of some sideline projects based on fraud detection using various AI/ML tools.I remotely helped to establish data science healthcare and life science division. I am responsible for advanced statistical modeling using informatics tools such as machine learning framework, programing (R, Python, Scala) and business analytics tools.I am actively involved in a process of wrangling, massaging and transformation of data. I sure to successful completion projects.', u'R Programmer/Data Scientist\nArkansas Children Nutrition Center\nSeptember 2015 to Present\nUAMS, Little Rock\nProjects and Responsibilities:\nCorrelation of neonatal diet driven gut microbiota on intestinal gene expression, immune response, development and metabolome.\n\u27a2 Designing protocol, data collection, cleaning, analysis and management of data\n\u27a2 Ability to make a story from analyzed data, wrote reports,\n\u27a2 Highly productive: Published manuscript (3) and presented in many conferences (5). Award received: 3\n\u27a2 Hands on experience of R studio, Python, SQL management studio, markdown, shiny, QIIME, base-space, ggplot2, graph pad, machine learning, big database tools;\n\u27a2 Statistical modeling and Data visualization - heat map, NMDA plot, PCA plot, box plot, bubble plot, 2D or 3D graphs, pi-chart, map etc\n\u27a2 Built my own code bank using Python and R script for many machine learning problems', u'R Programmer/ Research Data Scientist\nInternal Medicine/Biochemistry/Surgery, UTMB - Galveston, TX\nJuly 2009 to August 2015\nProjects and Responsibilities:\nI worked on 5+ major projects such browning project, propranolol hypermetabolic project, ENPP obesity projects etc and got hand on experience of following task\n\u27a2 Data production, collection, integration, management, data analysis, report drafting, and editing.\n\u27a2 Regression, Correlation, Cluster analysis, Classification, Heatmap generation, Prediction analysis and several others.\n\u27a2 Statistical modeling and Data visualization\n\u27a2 I got experience IRB and FDA rules and regulation and patient consenting.\n\u27a2 Made story from analyzed data, wrote reports.\n\u27a2 I used Environment: R, graph pad, SPSS, ms-excess, Epic, InfoEd, SPSS, Hadoop, Spark, Pig, Hive, bioinformatics tools.\n\u27a2 Productivltiy: I received 5 award, published >10 papers and presented in multiple conference and meetings.\n\u27a2 Clinical and preclinical studies experience.\n\nPUBLICATION\nRelated to neural network\nSaraf MK and Saraf G. Artificial Neural Network (ANN): An Intelligent Computational', u'R Programmer/ Research Data Scientist\nApproach - Nice, FR\nJanuary 2008 to June 2011', u'R Programmer/ Research Data Scientist\nGarg G\nJanuary 2007 to January 2007\nBedi JS, Baveja N, Saraf MK. A computational approach to analyze the degree of short term and long term memory deficit by stress in human. 36th Annual meeting of Society for Neurosciences 2007, US.\n\nRelated to heath sector\nClinical and Animal Research: 22']","[u'M Pharm', u'B Pharm in Machine learning A to Z using Python and R']","[u'Panjabi University\nJanuary 1999 to January 2002', u'Dr. HS Gour University\nJanuary 1993 to January 1997']","degree_1 : M Pharm, degree_2 :  B Pharm in Machine learning A to Z sing Python and R"
0,https://resumes.indeed.com/resume/9dc49fb77142798e,"[u'Data Scientist\nVerizon - Dallas, TX\nJanuary 2016 to Present\nDescription:\nThe Project was mainly focused on reducing customer churn by understanding the customer behavior using Statistical Modelling, Machine Learning techniques and take necessary steps to reduce customer churn as much as possible.\nResponsibilities:\n\u2022 Working closely with marketing team to deliver actionable insights from huge volume of data, coming from different marketing campaigns and customer interaction matrices such as web portal usage, email campaign responses, public site interaction, and other customer specific parameters.\n\u2022 Characterizing false positives and false negatives to improve a model for predicting customer churn rate.\n\u2022 Consumer segmentation and characterization to predict behavior. Analyzing promoters and detractors (defined using Net Promoter Score).\n\u2022 Outlier detection using high-dimensional historical data. Acquiring, cleaning and structuring data from multiple sources and maintain databases/data systems. Identifying, analyzing, and interpreting trends or patterns in complex data sets.\n\u2022 Developing, prototype and test predictive algorithms. Filtering and ""cleaning"" data and review computer reports, printouts, and performance indicators to locate and correct code problems.\n\u2022 Developing and implementing data collection systems and other strategies that optimize statistical efficiency and data quality.\n\u2022 Used different statistical models like regression and classification models to create contact scoring models. Also used clustering to the customer data profiles to do customer segmentation and analysis.\n\u2022 Interpreting data, analyze results using statistical techniques and provide ongoing reports.\n\u2022 Building a recommender system based on client\'s past renewal history to upsell and cross-sell them other related products or services. Created a recommendations engine that finds related customers, products.\nEnvironment: Python- Pandas, Numpy, Scikit-Learn, TensorFlow - ANN, SciPy, Seaborn, Matplotlib, SQL, Machine Learning, Deep Learning. R-Foreign, ggplot, igraph, lattice, MASS, mice and logit.', u'Data Science Pilot Project\nPitt Plastic - Pittsburg, KS\nMay 2015 to December 2015\nDescription:\nThe project was focused on sales prediction by using Sales training data into supervised classification algorithm to predict customer churn.\nResponsibilities:\n\u2022 Supported sales forecasting & planning team by improving time series & principal component analysis.\n\u2022 Utilized machine learning techniques for predictions & forecasting based on the Sales training data.\n\u2022 Executed overall data aggregation/alignment & process improvement reporting within the sales dept.\n\u2022 Managed Data quality & integrity using skills in Data Warehousing, Databases & ETL.\n\u2022 Monitored and maintained elevated levels of data analytic quality, accuracy, and process consistency.\n\u2022 Assisted sales management in data modeling.\n\u2022 Ensured on-time execution and implementation of sales planning analysis and reporting objectives.\n\u2022 Worked with sales management team to refine predictive methods & sales planning analytical process.\n\u2022 Executed and monitored the accuracy and efficiency for sales forecasts & reporting.\n\u2022 Prepared Dashboards using calculations, parameters in QlikView.\n\u2022 Supported consistent implementation of company reporting and sales process initiatives.\n\u2022 Used Python to identify customer classification, tree map, and regression models.\n\u2022 Performed forecasting and time series analysis of customer likes and dislikes.\n\nEnvironment: ETL, QlikView, Python, Machine Learning, SQL', u'Data Analyst / Data Engineer\nCigna - Hyderabad, Telangana\nMay 2013 to December 2014\nDescription:\nDealing with scenarios related to Healthcare fraud, waste and abuse detection; denial claims management; clinical pathways optimization; and health plan member profitability- Understanding and managing clinical variation across a hospital system, for patients undergoing specific types of surgery.\nResponsibilities:\n\u2022 Characterizing false positives and false negatives to improve a model for predicting overpaid claims, Consumer segmentation and characterization to predict behavior.\n\u2022 Outlier detection using high-dimensional historical data.\n\u2022 Analyzing promoters and detractors (defined using Net Promoter Score).\n\u2022 Installed and configured Hadoop MapReduce, HDFS, Developed multiple MapReduce jobs in Python for data cleaning and preprocessing.\n\u2022 Supported Map Reduce Programs those are running on the cluster. Involved in loading data from UNIX file system to HDFS.\n\u2022 Experienced in managing and reviewing Hadoop log files.\n\u2022 Involved in writing Hive queries to load and process data in Hadoop File System.\n\u2022 Exported data from Impala to Tableau reporting tool, created dashboards on live connection.\n\u2022 Gained very good business knowledge on health insurance, claim processing, fraud suspect identification, appeals process etc.\n\u2022 Maintained integrity of the database by implementing different validation techniques to the data uploading procedures.\n\u2022 Defended conjunction between ERP system and database using SQL reporting services.\n\u2022 Developed dashboards in Excel with SQL connections to ensure smooth procedural flow and analysis.\n\u2022 Identified and proposed process enhancements.\n\u2022 Quantified multiple stocking proposals and contracts with customers which increased business by approximately 20% per customer.\n\u2022 Developed ad-hoc reports upon requests using MS Access and MS Excel.\nEnvironment: Hadoop, MapReduce, Hive, Impala, Python (pandas, NumPy, scikit-learn), SQL Server, Excel, Tableau', u'Data Analyst\nHexaware Technologies Hyderabad - Hyderabad, Telangana\nJune 2011 to April 2013\nProject 1: Customer Satisfaction Analysis\nCustomer Satisfaction Analysis Using Regression Models\nResponsibilities:\n\u2022 Identifying what factors could influence the overall satisfaction of consumers. Range (1-5).\n\u2022 Considered the SMG (Service Management Group) database survey results in analyzing the impact on overall customer satisfaction.\n\u2022 Analyzed business process workflows and assisted in the development of ETL procedures for mapping data from source to target systems\n\u2022 We used Ordinal logistic regression methodology in explaining the importance of features.\n\u2022 The analysis involved predicting the overall satisfaction - ordinal rating, by analyzing the impact of each independent factors in explaining the output.\n\u2022 Packages used: MASS package, plot function for Ordinal logistics regression model.\nEnvironment: R Studio, SQL Server, Dplyr, Tidyr, ggplot2, Tableau, MASS package - plot function.\nProject 2: Annual Marketing Budget Allocation.\nAnnual Marketing Budget Allocation.\nResponsibilities:\n\u2022 Responsible for Data collection and data preparation and normalizing the data.\n\u2022 Used SQL, ETL tool, R Studio, and Python for data preparation.\n\u2022 Supported data consultants in the data modeling phase.\n\u2022 Used Constraint optimization algorithms (USED EXCEL) to optimize the marketing budget.\n\u2022 Used Time Series Models - decomposition of time series, trend, and seasonality detection, forecasting and exponential smoothing in predicting the market share and brand share to allocate the Marketing budget\nEnvironment: R , Python (pandas, NumPy, scikit-learn), SQL Server, Excel , ETL.']",[],[],degree_1 : 
0,https://resumes.indeed.com/resume/0e3cd733a51a6f48,"[u'Data Scientist\nHexistech Inc - North Brunswick, NJ\nJuly 2017 to Present\nHexistech is a growing Product development company involved in providing wide range of application and development & Technical support to the various clients for past decade. The project is migrating to Big Data world from RDBMS for storing and analyzing consumer information. The challenges are moving consumer data from RDBMS to HDFS perform analysis on the data and forward the analyzed results to BI teams to make decisions.\n\u2022 Used Pandas, NumPy, Seaborn, SciPy, Matplotlib, Scikit-learn, NLTK in Python for developing various machine learning algorithms and utilized machine learning algorithms such as linear regression, multivariate regression, naive Bayes, Random Forests, K-means, &KNN for data analysis.\n\u2022 Responsible for design and development of advanced R/Python programs to prepare transform and harmonize data sets in preparation for modeling.\n\u2022 Demonstrated experience in design and implementation of Statistical models, Predictive models, enterprise data model, metadata solution and data lifecycle management in both RDBMS, Big Data environments.\n\u2022 Developed MapReduce Python modules for machine learning & predictive analytics in Hadoop on AWS. Implemented a Python-based distributed random forest via Python streaming.\n\u2022 Created SSIS packages (ETL) to migrate data from heterogeneous sources such as MS Excel, Flat Files, CSV files.\n\u2022 Improved fraud prediction performance by using random forest and gradient boosting for feature selection with Python Scikit-learn.\n\u2022 Designed and implemented system architecture for Amazon EC2 based cloud-hosted solution for the client.\n\u2022 Analysed large data sets apply machine learning techniques and develop predictive models, statistical models and developing and enhancing statistical models by leveraging best-in-class modelling techniques.\n\u2022 Wrote Python modules to extract/load asset data from the MySQL source database.\n\u2022 Experience in writing custom User Defined Functions (UDF) in Python for Hadoop (Hive and Pig).\n\u2022 Developed Spark code using Python for faster processing of data on Hive (Hadoop). Developed MapReduce jobs in Python for data cleaning and data processing.\n\u2022 Used spark cluster to manipulate RDDS (Resilient Distributed Datasets) and used concepts of RDD partitions.\n\u2022 Having experienced in Agile Methodologies, Scrum stories and sprints experience in a Python based environment, along with data analytics, data wrangling and Excel data extracts.\n\u2022 Participated in writing scripts for test automation.', u""Junior Data Scientist\nAsk Mobi-Apps & Consulting Services Pvt. Ltd - Hyderabad, Telangana\nAugust 2013 to September 2015\n\u2022 Perform Data Profiling to learn about user behaviour and merge data from multiple data sources.\n\u2022 Implemented big data processing applications to collect, clean and normalization large volumes of open data using Hadoop ecosystems such as PIG, and HIVE.\n\u2022 Designing and developing various machine learning frameworks using Python and Scala.\n\u2022 Programmed a utility in Python that used multiple packages (SciPy, NumPy, pandas).\n\u2022 Independently coded new programs and designed Tables to load and test the program effectively for the given POC's using with Big Data/Hadoop.\n\u2022 Worked on Clustering and classification of data using machine learning algorithms\n\u2022 Implemented Classification using supervised algorithms like Logistic Regression, Decision trees, KNN, Naive Bayes.\n\u2022 Implemented Text mining to transposing words and phrases in unstructured data into numerical values\n\u2022 Handled importing data from various data sources, performed transformations using Hive and loaded data into HDFS.\n\u2022 Collaborate with data engineers to implement ETL process, write and optimized SQL queries to perform data extraction from Cloud and merging from Oracle 12c.\n\u2022 Collect unstructured data from MongoDB and completed data aggregation.\n\u2022 Experience in working with Hadoop clusters using Cloud era distributions.\n\u2022 Converting the existing relational database model to Hadoop ecosystem.\n\u2022 Storing training features are stored in AWS S3.\n\u2022 Validating and publishing-built model into s3.\n\u2022 Ability to build ML pipelines in Python or Scala in Spark.\n\u2022 Perform data visualization with Tableau and generate dashboards to present the findings.\n\u2022 Recommend and evaluate marketing approaches based on quality analytics of customer consuming behaviour.\n\u2022 Determine customer satisfaction and help enhance customer experience using NLP.\n\u2022 Work on Text Analytics, Naive Bayes, Sentiment analysis, creating word clouds and retrieving data from Twitter and other social networking platforms.""]",[u'MS in Computer science in Object-Oriented Design'],"[u'American College of Commerce and Technology Falls Church, VA\nJune 2017']",degree_1 : MS in Compter science in Object-Oriented Design
0,https://resumes.indeed.com/resume/ead6658dc6dcdf8b,"[u'A distinguished Technology Lead, Manager and Data Scientist\n16 Plus years\nJanuary 2005 to January 2008\nof experience specializes in large scale project implementation.\n\u2022 Proven ability to bridge technology and business goals to provide productive solutions using various technologies and platforms\n\u2022 Professional experience in analysis, design, Development and implementing Object Oriented Programming Concepts (OOPS), Service Oriented Architecture (SOA) based Applications, client-server applications, N-tier applications, Web and Desktop applications.\n\u2022 Experience in C# 3.0, 4.0, ASP.NET MVC4/5, ASP.NET 2.0/3.5/4.0, WCF, WPF, LINQ, ADO.NET, Ajax, JQuery, Angular JS, Bootstrap, SQL Server 2000/2005/2008/2012, MySQL, MongoDB, XML and Web Services, SOAP, RESTful WEB API, WEB API2 Services, SSIS, SSRS,VB.NET (9/8/7), VB (4/5/6)\n\u2022 Experiencein BizTalk , publish and subscribe, request and reply, message correlation, orchestrations, pipelines and filters, exposing orchestration as service endpoints, Guaranteed Delivery, File transfer, message transformation , content based routing\n\u2022 Extensively implemented and consumed WEB API 1 and WEB API 2 versions, used HTTP methods (verbs-GET, PUT, POST, PATCH, DELETE) to do CRUD operations, supporting JSON or/and XML data exchange formats and also implemented Security, Caching features.\n\u2022 Full stack .Net Developer with knowledge of Project Lifecycle including Requirements Analysis, Architectural Design, Development, Debugging, Integration, scheduling, delivery, documentation, Product Maintenance and Production support using Agile Methodology with SCRUM. Translating business requirements into technology solutions. Excellent technical/non-technical documentation experience. Worked with Re-engineering and Refactoring tasks. Experience using Dependency Injection.\n\u2022 Experience in Software Configuration Management (Daily Build, Release and Testing methodology) using tools like TFS\n\u2022 Strong abilities in Database Design, Normalization, writing Stored Procedures, handling SQL CLR, Triggers, Cursors, Views, Functions in MS SQL Server and involved in all the stages of System Development Life Cycle. Consumed ADO.NET Entity Frame Work for Entity Data Model, Entities, Relationship Mapping, Querying Data through LINQ.\n\u2022 Hands on experience in Developing Rich User Interface on web using Angular JS, HTML, Web forms, MVC and CSS with Webservers IIS 6.0/7.0/8.0.\n\u2022 Expert on fixing the product bugs, tracking bugs and maintain whole project source code using TFS. Implemented Unit and Integrated Testing with NUnit and custom applications. Implemented the quality practices, Process areas in CMMI Level 5.\n\u2022 Domain Experience includes Utilities, Insurance, Banking, Retail, Technology and Hi-tech.\n\u2022 Good Experience with RDBMS T-SQL Queries, Stored Procedures, functions, Triggers and Cursors.\n\u2022 Excellent communication & inter-personal skills and an ability to establish rapport with user groups.\n\u2022 Passionate about learning and working in new technologies or challenging work assignments as Individual or Lead Role. Ability to perform at a high level, meet deadlines, adaptable to ever changing priorities.\n\u2022 Business Analytics from Indian Schools of Business\n\u2022 Trained in Machine Learning, BIGDATA tools like Hadoop, Spark and Scala, MapReduce, PIG, Sqoop, etc.\n\u2022 Hands onData Analytics, Machine Learning and Statistical methods.\n\u2022 Main asset - presence of high degree of ""n-ACH"" (the need for Achievement).\n\u2022 Worked in different Geographical locations (USA, UK).']","[u'Microsoft certified application Developer', u'in Mathematics, Applied Mathematics, Statistics']","[u'Indian School of Business(ISB)', u'University of California San Diego, CA']","degree_1 : Microsoft certified application Developer, degree_2 :  in Mathematics, degree_3 :  Applied Mathematics, degree_4 :  Statistics"
0,https://resumes.indeed.com/resume/f5c09b218e2f8024,"[u'Data Scientist\nAT&T - Dallas, TX\nJuly 2017 to Present\nDescription:\nAT&T Inc is engaged in provision of communications and digital entertainment services in the United States and the world. It provides fixed-line services, including voice, data, and television services to consumers and small businesses.\nResponsibilities:\n\u2022 Involved in defining the source to target data mappings, business rules, and data definitions.\n\u2022 Performing data profiling on various source systems that are required for transferring data to ECH using\n\u2022 Defining the list codes and code conversions between the source systems and the data mart using Reference Data Management (RDM).\n\u2022 Involved in data collection and induction to Teradata\n\u2022 Conducted data cleaning, data preparation, and outlier detection\n\u2022 Finding insights from millions of customer chat and calls records\n\u2022 Gathering requirements from business\n\u2022 Reviewing business requirements and analyzing data sources\n\u2022 Developed predictive models for sales and Finance teams using various ML and DL algorithms\n\u2022 Utilizing Informatica toolset (InformaticaData Explorer, and Informatica Data Quality) to analyze legacy data for Data Profiling.\n\u2022 Worked on DTS Packages, DTS Import/Export for transferring data between SQL Server 2000 to 2005\n\u2022 Involved in upgrading DTS packages to SSIS packages (ETL).\n\u2022 Involved in Training and Testing the ML Supervised and Unsupervised models\n\u2022 Researching on Deep Learning to implement NLP\n\u2022 Presented to the higher management the discovered trends and analysis, forecast data, recommendations, model results and risks identified\n\u2022 Performing an end to end InformaticaETL Testing for these custom tables by writing complex SQL Queries on the source database and comparing the results against the target database.\n\u2022 Using HP Quality Center v 11 for defect tracking of issues.\n\u2022 Involved in applying data mining techniques and optimization techniques in B2B and B2C industries and proficient in Machine Learning, Data/Text Mining, Statistical Analysis and Predictive Modeling.\n\u2022 Created and presented executive dashboards to show the patterns & trends in the data using Tableau Desktop\n\u2022 Developed NLP models for Topic extraction, Sentiment Analysis\n\u2022 Developed Executive Summary KPI, Key value programs, NPI dashboards in Tableau\n\u2022 Created customized Calculations, Conditions and Filters (Local, Global) for various analytical reports and dashboards\n\u2022 Was able to identify emerging issues using the models\n\u2022 Developing & evaluating Machine Learning models\n\u2022 Developed different visualizations using advanced features and deep analytics in Tableau\n\u2022 Used algorithms and programming to efficiently go through large datasets and apply treatments, filters, and conditions as needed\n\u2022 Developed Cross Tab, Chart, Funnel charts, Donut charts, Heat Maps, Tree Maps and Drill Through Reports, 100% stacked bar charts etc. in Tableau Desktop\n\u2022 Involved in publishing, scheduling and subscriptions with Tableau Server and creating and managing users, groups, sites in Tableau Server.\n\u2022 Involved in developing and testing the SQL Scripts for report development, Tableau reports, Dashboards and handled the performance issues effectively\n\u2022 Tested dashboards to ensure data was matching as per the business requirements and if there were any changes in underlying data\nEnvironment: Data Governance, SQL Server, ER Studio 9.7, Tableau 9.03, AWS, Teradata 15, ETL, MS Office Suite - Excel(Pivot, VLOOKUP), DB2, R, Python, Visio, HP ALM, Agile, Azure, Data Quality, Tableau and Reference Data Management.', u'Data Scientist\nHealthfirst, NY\nApril 2016 to June 2017\nDescription:\nHealth First Inc. is a not-for-profit integrated health system in Central Florida. It offers cancer, fitness, heart and vascular, maternity, neurosciences, orthopedics and sports medicine, open surgery, robotic surgery, urogynecology, vascular and vein, and weight loss services, as well as brevard and cosmetic dermatology services.\nResponsibilities:\n\u2022 A highly immersive DataScience program involving DataManipulation&Visualization, Web Scraping, MachineLearning, Python programming, SQL, GIT, Unix Commands, NoSQL, MongoDB, Hadoop.\n\u2022 Installed and used CaffeDeepLearningFramework\n\u2022 Worked on different data formats such as JSON, XML and performed machinelearningalgorithms in Python.\n\u2022 Participated in all phases of datamining; datacollection, datacleaning, developingmodels, validation, visualization and performed Gapanalysis.\n\u2022 Developing Voice Bot using AI (IVR ), improving the interaction between Human and the Virtual Assistant\n\u2022 Implemented Event Task for execute Application Automatically.\n\u2022 Involved in developing Patches & Updates Module.\n\u2022 Setup storage and dataanalysis tools in AmazonWebServices cloud computing infrastructure.\n\u2022 Used pandas, numpy, seaborn, scipy, matplotlib, scikit-learn, NLTK in Python for developing various machinelearningalgorithms.\n\u2022 Development and Deployment using Google Dialogflow Enterprise.\n\u2022 Worked as Data Architects and IT Architects to understand the movement of data and its storage and ERStudio9.7\n\u2022 Data visualizationusingElasticsearch , Kibana and Logstash in python.\n\u2022 Used Kibana an open source plugin for Elasticsearch in analytics and Data visualization.\n\u2022 DataManipulation and Aggregation from different source using Nexus, Toad, BusinessObjects, PowerBI and SmartView.\n\u2022 Implemented Agile Methodology for building an internal application.\n\u2022 Extracting the source data from Oracle tables, MS SQL Server, sequential files and excel sheets.\n\u2022 Migrating Informatica mappings from SQL Server to Netezza Foster culture of continuous engineering improvement through mentoring, feedback, and metrics\n\u2022 Broad knowledge of programming, and scripting (especially in R / Java / Python)\n\u2022 Developing and maintaining Data Dictionary to create metadata reports for technical and business purpose.\n\u2022 Predictive modeling using state-of-the-art methods\n\u2022 Developed MapReduce/Spark Python modules for machine learning & predictive analytics in Hadoop on AWS.\n\u2022 Parse and manipulate raw, complex data streams to prepare for loading into an analytical tool.\n\u2022 Build and maintain dashboard and reporting based on the statistical models to identify and track key metrics and risk indicators.\n\u2022 Proven experience building sustainable and trustful relationships with senior leaders\n\u2022 Focus on integration overlap and Informatica newer commitment to MDM with the acquisition of Identity Systems.\n\u2022 Data analysis using regressions, data cleaning, excel v-look up, histograms and TOAD client and data representation of the analysis and suggested solutions for investors\n\u2022 Rapid model creation in Python using pandas, numpy, sklearn, and plot.ly for data visualization. These models are then implemented in SAS where they are interfaced with MSSQL databases and scheduled to update on a timely basis.\n\u2022 Attained good knowledge in Hadoop Data Lake Implementation and HADOOP Architecture for client business data management.\n\u2022 Extracted data from HDFS and prepared data for exploratory analysis using datamunging\n\nEnvironment: ER Studio 9.7, Tableau 9.03, AWS, Teradata 15, MDM, GIT, Unix, Python 3.5.2, , MLLib, SAS, regression, logistic regression, Hadoop, NoSQL, Teradata, OLTP, random forest, OLAP, HDFS, ODS, NLTK, SVM, JSON, XML, MapReduce, Google Dialog Flow.', u'Data Analyst/Data Scientist\nPearson New Jersey, New Jersey\nDecember 2014 to March 2016\nDescription: Pearson is based on educational platform, which is one of the leading educational and professional publishers around the globe. The rapid growth in usage of Tablets, Laptops and smart phones which transforms education to everyone virtually anytime and anywhere, this project focuses on e-learning web portal and application development.\nResponsibilities:\n\u2022 Assisting business by being able to deliver a machine learning project from beginning to end, aggregating and exploring data, building and validating predictive models and deploying completed models to deliver business impacts to the organization\n\u2022 Created data modeling and data mapping document containing source, formulate transformational rules to populate target fields\n\u2022 Created impact & gap analysis documents specifying changes introduced as part of the program and lead the business process team\n\u2022 Work with big data consultants to analyze, extract, normalize and label relevant data using Statistical modeling techniques like Logistic regression, decision trees, Support vector machine, Random forest, Naive Bayes and neural networks\n\u2022 Developed ETLs for data sources used in production reporting for marketing and operations teams.\n\u2022 Write SQL queries to perform data analysis, data modeling and prepare data mapping documents to explain the transformation rules from source to target tables\n\u2022 Led the Change Management stream of an HR/Payroll project resulting from a $16.5 billion acquisition and the formation of UTAS, created Change Management Plan, and ensured team was on target to deliver both communication and training to HR, finance and Payroll staff.\n\u2022 Review business data for trends, patterns or casual analysis to assist in identifying model drift and retraining models\n\u2022 Created customized reports and processes in SAS and Tableau Desktop\n\u2022 Performed data analysis to create reporting requirements by specifying inclusion & exclusion criteria, conditions, business rules and data elements to be included into the report\n\u2022 Scheduled and facilitated requirements gathering with HR, Payroll, finance and accounting teams to implement ADP eTime and ADP Enterprise v5 and ADP General Ledger and drove requirements for data collection and data modeling with data engineers\n\u2022 Performed SQL query for data analysis and integration\n\u2022 Support PMO governance activities; defining and maintaining Project Management standards.\n\u2022 Responsible for generating ideas for product changes that improve key metrics\n\u2022 Provided data analytics of the web-portal to the team for feedback and improvement.\n\nEnvironment: Python, HTML5, CSS3, AJAX, Teradata, OLTP, random forest, OLAP, HDFS, ODS, JSON, jQuery, MySQL, NumPy, SQL Alchemy, Matplotlib, Hadoop, Pig Scripts.', u'Data Analyst/Data Modeler\nSUNTRUST BANK - Richmond, VA\nApril 2013 to November 2014\nDescription: SunTrust provides the financial services for consumers, small business and commercial banking, financial transaction processing, asset management and private equity Involved in the scope discussions with the Business Analysts and the Business users to identify the technical requirements\n\nResponsibilities:\n\u2022 Involved in defining the source to target data mappings, business rules, data definitions.\n\u2022 Involved in defining the business/transformation rules applied for sales and service data.\n\u2022 Worked with project team representatives to ensure that logical and physical ER/Studio data models were developed in line with corporate standards and guidelines.\n\u2022 Define the list codes and code conversions between the source systems and the data mart.\n\u2022 Coordinate with the business users in providing appropriate, effective and efficient way to design the new reporting needs based on the user with the existing functionality.\n\u2022 Worked with BTEQ to submit SQL statements, import and export data, and generate reports in Teradata.\n\u2022 Responsible for defining the key identifiers for each mapping/interface.\n\u2022 Responsible for defining the key identifiers for each mapping/interface.\n\u2022 Implementation of Metadata Repository, Maintaining Data Quality, Data Cleanup procedures, Transformations, Data Standards, Data Governance program, Scripts, Stored Procedures, triggers and execution of test plans\n\u2022 Performed data quality in TalendOpenStudio.\n\u2022 Coordinated meetings with vendors to define requirements and system interaction agreement documentation between client and vendor system.\n\u2022 Responsible for defining the functional requirement documents for each source to target interface.\n\u2022 Remain knowledgeable in all areas of business operations in order to identify systems needs and requirements.\n\u2022 Document the complete process flow to describe program development, logic, testing, and implementation, application integration, coding.\n\u2022 Enterprise Metadata Library with any changes or updates.\n\u2022 Generate weekly and monthly asset inventory reports.\nEnvironment: Erwin r7.0, SQL Server 2012/2008, Windows XP/NT/2000, Oracle 10g/9i, MS-DTS, UML, UAT, SQL Loader, OOD, OLTP, PL/SQL, MS Visio, Informatica.', u'Business Analyst /Data Analyst\nICICI Bank - Hyderabad, Telangana\nNovember 2011 to March 2013\nDescription: ICICI Bank, stands for Industrial Credit and Investment Corporation of India, is an Indian multinational banking and financial services company headquartered in Mumbai, Maharashtra, India, with its registered office in Vadodara. In 2017, it is the third largest bank in India in terms of assets and third in term of market capitalisation.\nResponsibilities:\n\u2022 Analyze business information requirements and model class diagrams and/or conceptual domain models.\n\u2022 Managed the project requirements, documents and use cases by IBM Rational RequisitePro.\n\u2022 Assisted in building an Integrated LogicalDataDesign, propose physical database design for building the data mart.\n\u2022 Gather & Review Customer Information Requirements for OLAP and building the data mart.\n\u2022 Responsible for defining the key identifiers for each mapping/interface\n\u2022 Responsible for defining the functional requirement documents for each source to target interface.\n\u2022 Coordinated meetings with vendors to define requirements and system interaction agreement documentation between client and vendor system.\n\u2022 Enterprise Metadata Library with any changes or updates.\n\u2022 Document data quality and traceability documents for each source interface.\n\u2022 Performed document analysis involving creation of Use Cases and Use Case narrations using Microsoft Visio, in order to present the efficiency of the gathered requirements.\n\u2022 Analyzed business process workflows and assisted in the development of ETL procedures for mapping data from source to target systems.\n\u2022 Worked with BTEQ to submit SQL statements, import and export data, and generate reports in Terra-data.\n\u2022 Calculated and analyzed claims data for provider incentive and supplemental benefit analysis using Microsoft Access and Oracle SQL.\n\u2022 Establish standards of procedures.\n\u2022 Generate weekly and monthly asset inventory reports.\n\u2022 Document all data mapping and transformation processes in the Functional Design documents based on the business requirements\n\nEnvironment: SQL Server 2008R2/2005 Enterprise, SSRS, SSIS, Crystal Reports, Windows Enterprise Server 2000, DTS, SQL Profiler, and Query Analyzer.', u'BI Developer/Data Analyst\nResilient - Hyderabad, Telangana\nApril 2009 to October 2011\nDescription: Tax and Banking and Full Level Tax and Banking (FLTII) are web-based applications, which hold the information and documents for processing payroll tax and banking details.\nResponsibilities:\n\u2022 Developed an Object modeling in UML for Conceptual Data Model using Enterprise Architect.\n\u2022 Developed logical and Physical data models using Erwin to design OLTP system for different applications.\n\u2022 Facilitated transition of logical data models into the physical database design and recommended technical approaches for good data management practices.\n\u2022 Worked with DBA group to create Best-Fit Physical Data Model with DDL from the Logical Data Model using Forward engineering.\n\u2022 Worked with the ETL team to document the transformation rules for data migration from OLTP to Warehouse environment for reporting purposes.\n\u2022 Developed Data Migration and Cleansing rules for the Integration Architecture (OLTP, ODS, DW).\n\u2022 Performed K-means clustering, Multivariate analysis, and Support Vector Machines in R.\n\u2022 Extensive system study, design, development and testing were carried out in the Oracle environment to meet the customer requirements.\n\u2022 Written complex Hive and SQL queries for data analysis to meet business requirements.\n\u2022 Written complex SQL queries for implementing business requirements\n\nEnvironment: DB2, Teradata, SQL-Server 2008, Enterprise Architect, Power Designer, MS SSAS, Crystal Reports, SSRS, ER Studio, Lotus Notes, Windows XP, MS Excel, word and Access.']","[u""Bachelor's""]",[u''],"degree_1 : ""Bachelors"""
0,https://resumes.indeed.com/resume/5b369099018051fd,"[u'PBN Data\nFounder and CEO - Bellevue, WA\nJanuary 2018 to Present\n\u2022Establish business: develop business plan and marketing strategy.\n\u2022Implement and maintain company website, acquire company phone number and email.\n\u2022Aggressively seek out new clients by following marketing strategy.\n\u2022Analyze data and financial statements to create data visualizations and reports for clients.', u'Selected Projects\nBellevue, WA\nAugust 2017 to Present\n\u2022Natural language processing (NLP) in R to classify Indeed.com job listings as Data Scientist or Data Analyst using Naive Bayes, KNN, Random Forest, LASSO, and Ridge Regression\n\u2022Image classification in Python with Keras using convolutional neural network (CNN) and transfer learning to find Waldo\n\u2022Custom Snapchat filter iOS app \u2013 use Python, dlib, and OpenCV to detect facial landmarks\n\u2022Options price prediction and arbitrage using Python and scikit-learn', u""Data Scientist: Master's Thesis\nBaylor University - Waco, TX\nAugust 2016 to August 2017\nIn-depth economic data analysis and research report required for graduation. Studied the relationship between payday lending laws and personal bankruptcy:\n\u2022Collected and managed multiple comprehensive datasets on payday lending, bankruptcy filings, and state-level economic metrics such as gross state product and unemployment rates.\n\u2022Utilized Excel, R, and Stata to analyze data using differences-in-differences approach.\n\u2022Communication and Design: Created custom charts and tables using Tableau and Adobe Illustrator to visually demonstrate that the prohibition of payday loans decreases personal bankruptcy by approximately 15.6%."", u'Data Scientist: Health Care Analyst\nBaylor University Practicum in Portfolio Management - Waco, TX\nJanuary 2013 to May 2013\n\u2022Successfully managed $5.2MM fund in a team environment. Collaborated with 11 other students to determine portfolio composition.\n\u2022Studied and utilized Bloomberg and FactSet analytic software to perform fundamental financial analysis.']","[u""Master's of Science in Economics in Economics"", u""Bachelor's of Business Administration in Finance""]","[u'Baylor University Waco, TX\nAugust 2014 to August 2017', u'Baylor University Waco, TX\nAugust 2010 to May 2014']","degree_1 : ""Masters of Science in Economics in Economics"", degree_2 :  ""Bachelors of Bsiness Administration in Finance"""
0,https://resumes.indeed.com/resume/9887f125ebcec937,"[u'Data Scientist, Consultant\nConstant Contact - Waltham, MA\nSeptember 2017 to Present\n\u2022 Performed the root-cause analysis on large amount of spam emails data using NLP and Text mining and then built a classification model to reduce improve the overall false negative rate.\n\u2022 Collaborated with BI and Analytics teams to leverage clickstream data with various other sources to build predictive modeling of visitor quality, conversion, retention.\n\u2022 Integrated Tableau with Spark SQL; loaded data in Tableau Server, and created and deployed BI dashboards.\n\nEnvironment: Amazon S3, Hadoop Hive Spark, Python, R, Java, SQL, NLP, Google Analytics', u'Data Scientist, Consultant\nDassault Systems - Waltham, MA\nFebruary 2017 to August 2017\nProject: Digitization and automation of all kinds of customers, marketing and pipeline data in order to provide a full customer 360 and events 360 view. Apply machine learning techniques to get actionable customer insight, improve forecasting, create affinity models personalize advocacy marketing, and increase ROMI.\nResponsibilities:\n\u2022 Extracted and transformed customers and marketing datasets obtained from multiple sources (Oracle, PostgreSQL), loading them into EXALEAD for indexing, and developed a web based front end/search bases applications using a combination of Grails/Java + JQuery + d3.js.\n\n\u2022 Consolidated CRM and events data and applied data mining techniques to perform CLV, segmentation, profiling and other scoring models.\n\n\u2022 Built and tested Time Series model in Spark R for trends and predictive sales pipeline analytics, used various smoothing methods to improve the model, and integrated the algorithm with the sales dashboard using D3.js, resulting 98.5% accuracy in FY17Q1.\n\u2022 Created a k-mean clustering model using Python to facilitate product product marketing campaigns.\n\u2022 Built the response model for email marketing using Logistic Regression resulting in a 350% increase in response rate.\n\u2022 Analyzed a vast amount of text data from various sources for NLP use cases, used Apache Spark for Massively Parallel NLP and Logistic Regression to predict the probability of the survey questions.\n\u2022 Developed a few dynamic BI dashboard and produced regular reporting to track key KPIs, sales and performance matrixes across multiple channels, used Tableau and D3.js.\n\u2022 Generated hypothesis for A/B testing for Banner Ads on Google, additionally, for multiple alternatives, choose multi-armed bandit algorithm to create more value in spending - saving 17% on budget with 750% ROI on PPC channel.\n\u2022 Involved with the R&D team in conceptualizing of a new Deep Learning framework with 3D simulation, revolutionizing the Data-Driven Design for PLM/CAD solutions.\n\nEnvironment: Azure, Hadoop Hive Spark, Python, R, Java, SQL, EXALEAD Cloudview, SQL, HTML, D3.js, JQuery', u""Data Mining Scientist\nDassault Systems - Waltham, MA\nSeptember 2016 to January 2017\nProject: Assisting product marketing team with segmentation, positioning, and overall go-to-market strategy for 2017 product launch. Building appropriate analytical models for the classification and text-mining problems, reporting services using Azure data warehouse and data visualization per business requirement.\nResponsibilities:\n\u2022 Created a product portfolio application using previous years of revenue data, using EXALEAD for indexing and HTML CSS JavaScript and Java to create an application.\n\u2022 Finding outliers, errors, trends, missing values, and distribution in the data. Utilized techniques like Histogram, Bar plot, pie-chart, scatter chart, box plots to determine the initial conditions of the data.\n\u2022 Loaded data from Azure HDInsight to Spark RDD and created a K-mean clustering model using Spark MLlib in order to segmented customers for advocacy marketing, used D3.js for visualization.\n\u2022 Built a regression model for revenue and channel analytics with detailed correlation and causation analysis, using Spark MLlib, integrated with global marketing dashboard using JavaScript.\n\u2022 Analyzed usage trends for content with Adobe analytics and CRM Data Extracts with Siebel Analytics.\n\u2022 Analyzed customer's feedback data using text mining package spark-ts in Spark, helping the team to know the sentiments of the customers about the product, ENOVIA.\n\u2022 Improved UX|UI of ENOVIA's webpage by collaborating with the design and engineering team.\n\n\u2022 Developed and maintained the BI dashboard and produced regular reporting to track key KPIs, sales and performance matrixes across multiple channels, used Tableau and D3.js.\n\nEnvironment: Azure, Hadoop Hive Spark, Python, JavaScript, Tableau, Adobe Analytics, EXALEAD"", u""Data Analyst\nUniversity of Massachusetts Boston - Boston, MA\nMarch 2014 to August 2016\nThe project was to enable the purchasing/finance department with periodic (weekly/monthly/quartly/yearly) reporting, data modeling, regression, and visualization solutions. Maintain and manage Contract & Compliance and asset management database. Also, Optimization of $70 million budget for two consecutive FYs.\nResponsibilities:\n\u2022 Analyzed business requirements, system requirements, data mapping requirement specifications, and responsible for documenting functional requirements and supplementary requirements.\n\u2022 Initial pattern recognition and data cleansing using dpylr package in R. Worked on data manipulation on raw purchasing data in different formats from multiple sources (oracle, byways) and prepared the data for further analysis using ggvis and ggplot2 packages.\n\u2022 Prepared annual/semi-annual/quarterly/monthly reports for the department using SQL server reporting service (SSRS) and advanced excel functions like Vlookups, PivotTables, Merging, Sorting.\n\u2022 Analyzed reports of data duplicates or other errors based on monthly or daily data reports.\n\u2022 Collected commodities data from multiple websites such as Target, Amazon and Walmart by using web scrapping with Python for price comparison of products, reducing annual expenditures by 7%.\n\u2022 Provided visual spend analytics using Tableau to identify opportunities to reduce cost, track contract compliances, and measure supplier's performance - 11% cost reduction.\n\u2022 Strategic contributions to finance and contract and compliance board meetings regarding optimization of university's budget and new projects.\nEnvironment: Excel (Vlookups, PivotTables etc), Tableau, PeopleSoft, SQL Server 2012, R Shiny, R Studio, Python\nVisual IQ, Needham MA\nData Scientist, Marketing\nProject: Building a recommendation system for a media streaming and retail clients. Further, develop web-apps and dashboards to showcase the results for the stakeholders for creating the appropriate strategies.\nResponsibilities\n\u2022 Built a real-time streaming model using Spark Streaming - Loaded streaming data from Amazon S3 to Spark RDDs, wrote a program to take the live streaming of Tweets, built a machine learning model using Spark MLlib TF-IDF in R, invoked it from SQL to build a dashboard with Apache Zeppelin that uses a live uses this machine learning model and filter out things for users.\n\u2022 Spatial data analysis and geo clustering using spatial libraries (sp, rgdal, maptools, spatstat, dbscan) - combining geo location data and search history to build recommendation systems.\n\u2022 Designed custom reports, charts, tables and dashboards in Shiny Apps to help the marketing and operations teams in their decision making process.\n\u2022 Created focused reports and dashboards on content/channel performance, lead generation and conversion rates using Google Analytics.\n\nEnvironment: Amazon S3, Apache Zeppelin, R, Python, Shiny, Google Analytics.\nBoston Children Hospital\nData Scientist/Project Manager\nProject: To create a project plan to develop a digital platform called KidsMD for Boston Children's Hospital to be a centralized technology platform of tools that parents/caregivers and patients can confidently refer to for interactive healthcare content.\nResponsibilities:\n\u2022 Worked with different teams (survey, pharmaceutical and research and development, claims etc.) to gain insights about the data concepts behind their health symptoms and business.\n\u2022 Designed ETL packages dealing with different data sources (SQL Server, Flat Files, and XMLs etc.) and loaded the data into target data sources by performing different kinds of transformations using USQL in Azure data lake analytics, and then created DataViz reports using Power BI.\n\n\u2022 Built the predictive model using multiple logistic regression using Spark MLlib in order to classify EMR and Non-EMR cases, analyzed the accuracy of the model using Receiver Operating Characteristic (ROC curve) and Confusion Matrix.\n\u2022 Provided recommendations regarding Voice Artificial Intelligence (voice-assistant) and collaborated with Amazon Alexa team for to partner with BCH platform.\n\u2022 Designed custom reports, charts, tables and dashboards using Tableau and for marketing and operations teams for their decision-making process.\n\u2022 Implement and customize web analytics and website optimization tools based on business needs, specifically Google Analytics.\n\u2022 Analyzed generic search data from GoogleAdwords to identify the trends in child health care.\nEnvironment: MS-Project, Microsoft Azure, Apache Spark, Python, SQL, Tableau, Google Analytics."", u'Software Engineer/Product Manager\nBeing Newton Group - New Delhi, Delhi\nMay 2012 to December 2013\nProject: Development of a computer-adaptive-testing (CAT) software.\n\u2022 Worked in a team of a few software developers to build Computer Adaptive Testing (CAT) application in C and Java using Agile SDLC methodology. Also, conducted JAD sessions and brainstorming.\n\u2022 Performed Unit Testing, Load Testing and Performance Testing in Java.\n\u2022 Designed a real-time intelligent chat bot in Python to improve the customer service and staff efficiency by 60%.\n\u2022 Strategic involvement in pricing strategy and overall go-to-market plan for Indian and global market.\nEnvironment: Agile, C, Java, Python, JIRA, PowerPoint', u'Data Analyst Intern\nBSNL - New Delhi, Delhi\nJanuary 2011 to April 2012\n\u2022 Worked with different teams to gain insights about the data concepts behind their business.\n\u2022 Performed data analysis primarily Identifying data sets, source data, source meta data, data definitions and data formats. Also, performed the root cause analysis for unexpected billings scores.\n\u2022 Gathered all data, cleaning and Analysis the data from different Districts inside of cities using time series analysis using R.\n\u2022 Used R Programing language to simple and complicate data analysis. Mainly, implement a wide variety of statistical and graphical techniques, including linear and nonlinear modeling, statistical tests, simple time-series analysis using IDE- R studio.\nEnvironment: R-Studio, SQL Server 2008, MS Excel, MS Access, PowerPoint, Statistics']","[u'MBA', u'BS']","[u'University of Massachusetts Boston Boston, MA', u'Information Technology Uttar Pradesh Technical University']","degree_1 : MBA, degree_2 :  BS"
0,https://resumes.indeed.com/resume/bd385b2b2a6e5c07,"[u'Data Scientist / Principal Engineer\nSix Sigma Green Belt at NextEra Energy Resources\nJuly 2017 to Present\nAnalyze large data sets, conduct data mining studies, validate current optimizer tools and design, and develop new algorithms/solutions that help continually improve the effectiveness of our\nmaintenance strategy.\n\nMine data being used for work prioritization to determine hidden trends and areas of opportunity.\nContinually evaluate existing algorithms to improve precision and accuracy of tools that solve for lowest cost options.\nWork with operations and maintenance teams to define expected value outcomes from existing\napplications, and translate new needs into data solutions.\nCollaborate effectively with other teams developing data science solutions to seamlessly integrate into the CWE data systems being developed for Wind.\nDevelop prototype working software as proof of concepts that identify next needed application.', u'Data Scientist/Sr PGD Engineer\u2212 Performance Monitoring, Six Sigma Green Belt\nNextEra Energy\nMarch 2015 to July 2017\nPerform statistical analysis and machine learning of power generation data for performance and online monitoring to support predictive monitoring and efficient operation of power plant\ncomponents and associated systems.', u'Research Scientist\nMistras\nSeptember 2012 to February 2015\nWork in the area of Nondestructive Evaluation (NDE) applied specifically to the development and implementation of On-Line Monitoring (OLM). Responsible for research, development of advanced\nOLM methods and strategies for civil infrastructure, power utilities including traditional fossil,\nnuclear and alternative plants. Help Research Contracts & Application to secure research contracts from federal agencies and others, and work as Principal Investigators in those projects. Develop\nadvanced analysis techniques, methods and algorithms for automated OLM data analysis. Be in charge of setup data bases for customer data management and archive. Work with OLM systems\ninstallation crews to determine systems sensitivity and develop protocols for installation. Develop\nphysics- based computational models/methods and generate application software where no\nmethod or historic program has been available. Develop diagnosis and prognosis models that use data to determine asset condition and predict remaining life.', u'Post-Doctoral Fellow\nUniversity of South Carolina - May, South Carolina, US\nMay 2009 to August 2012\nWork on the development of a real-time system for health monitoring of steel structural elements\nsubjected to fatigue in bridges. The development of the real-time health monitoring system\nincludes the work on mathematical and software tools for modeling fatigue crack growth in steel elements, filtering of Acoustic Emission data generated by the crack growth, and the\nprobabilistic assessment of the remaining fatigue life of the structural element using the filtered\ndata. Conditioning of hardware tools to acquire Acoustic Emission data during cyclic loading tests.\nWork on Creation of finite element models of pile-cap connections for prestressed piles. Write\npeer reviewed journal and conference papers on the filtering of Acoustic Emission data and the prognosis of crack growth in steel structural elements, and provide service to the department,\nuniversity and profession.']","[u'Doctor of Philosophy (Ph.D.) in Civil Engineering', u""Master's in Civil Engineering"", u""Bachelor's in Civil Engineering""]","[u'University of South Carolina Columbia, SC\nJanuary 2007 to January 2009', u'University of South Carolina Columbia, SC\nJanuary 2005 to January 2007', u'Universidad del Valle\nJanuary 1999 to January 2005']","degree_1 : Doctor of Philosophy (Ph.D.) in Civil Engineering, degree_2 :  ""Masters in Civil Engineering"", degree_3 :  ""Bachelors in Civil Engineering"""
0,https://resumes.indeed.com/resume/90696a104c422e1b,"[u'Bioinformatics Team Manager - Research Scientist\nBOSTON BIOMEDICAL INC\nAugust 2014 to Present\nSuccessfully lead research team for bioinformatics and computational designs of RNA based therapeutics to boost efficacy levels and implement innovations to improve results. Collaborated between clinical, pre-clinical and research on a number of early and late stage research pipeline projects.\n\u25c6 Led team that developed internal RNAi design algorithm and web interface\n\u25c6 Produced valuable research data for use in drug design initiatives\n\u25c6 Developed a proprietary algorithm to expand proprietary aiRNA technology and research capabilities\n\u25c6 Designed and implemented key research innovations, including small molecule inhibitors for research targeted proteins\n\u25c6 Developed budgets, training materials, and maintained documentation across multiple projects\n\u25c6 Post-Doctoral work (described below)', u'Bioinformatics Manager, Research Scientist\nBOSTON BIOMEDICAL INC - Cambridge, MA\nJanuary 2012 to January 2018\nSuccessfully lead research team for bioinformatics and computational designs of RNA based therapeutics to boost efficacy levels and implement innovations to improve results. Collaborated between clinical, pre-clinical and research on a number of early and late stage research pipeline projects.\n\u25c6 Led team that developed internal RNAi design algorithm and web interface\n\u25c6 Produced valuable research data for use in drug design initiatives\n\u25c6 Developed a proprietary algorithm to expand proprietary aiRNA technology and research capabilities\n\u25c6 Designed and implemented key research innovations, including small molecule inhibitors for research targeted proteins\n\u25c6 Developed budgets, training materials, and maintained documentation across multiple projects\n\u25c6 Post-Doctoral work (described below)', u'Clinical Research Scientist, Clinical Trial Data Manager\nBOSTON BIOMEDICAL INC\nJanuary 2015 to June 2016\nInterdepartmental cross-training)\nManage clinical trial data to ensure a high level of accuracy, efficiency, and organization during collection and reporting processes.\n\u25c6 Oversaw development, build and deployment of clinical oncological clinical trials in collaboration with large vendor (Oracle)\n\u25c6 Led multi-stakeholder meetings, coordinated vendors, negotiated agreements.', u""Postdoctoral Scientist\nBOSTON BIOMEDICAL INC\nMay 2012 to August 2014\nPerformed high-level research to support team's ongoing biomedical trial development.\n\u25c6 Successfully developed and implemented docking, SAR, and computational methods to boost drug discovery and validation processes\n\u25c6 Supported research team in effectively expressing, purifying, and characterizing oncologically targeted proteins from E. coli and insect cells\n\u25c6 Independently developed new methods and protocols which were acknowledged by senior management\n\u25c6 Recognized for exemplary performance and promoted to leadership role""]",[u'Ph.D. in Chemistry'],"[u'University of Massachusetts Amherst, MA\nJanuary 2011']",degree_1 : Ph.D. in Chemistry
0,https://resumes.indeed.com/resume/5803a781a9cd4383,"[u""Data Scientist\nIBM - Dublin, OH\nJune 2015 to Present\nBBCI (Behavior Based Customer Insights)\nIBM's Behavior Based Customer Insight is a suite of products that leverages predictive analytics to personalize customer engagement and deliver customized actions. The product suite involves\npredictive models ranging from churn prediction to cross sell models.\n\n\u2022 Data Exploration and Mining using tools like SPSS and R.\n\u2022 Wrote Complex SQL queries for data extraction.\n\u2022 Used techniques like supervised and unsupervised machine learning, statistical analysis and predictive\nmodeling to deliver business insights to customers based on data stored in DB2 databases.\n\u2022 Developed Churn, Cross Sell and Upsell models for Banking, Insurance and Wealth industry.\n\u2022 Experienced full life cycle of statistical model development i.e. conception, data mining, model\nbuilding and deployment.\n\u2022 Leveraged other statistical packages as third party plugins e.g. R in SPSS streams.\n\u2022 Conducted product training for technical and sales seminar."", u'Senior Software Developer\nUnion Pacific - Boulder, CO\nDecember 2010 to April 2013\nHMS & CMTS\nUPRR utilizes HMS (Hotel Management System) and (CMTS) Crew Management And Timekeeping\nSystems for Crew Scheduling, Fatigue Management and Position Assignment. These systems are built on Java and Mainframe technologies.\n\n\u2022 Wrote business modules in Java, JSP and JS\n\u2022 Integrated mainframe APIs to feed to Java systems.', u'Software Developer\nTech Mahindra - Pune, Maharashtra\nDecember 2006 to November 2010\niTF(iTrackforce) and TFE\niTF project handles assignment of positions to bidders through java based decision engine.\n\n\u2022 Developed UI screens using custom spring framework.\n\u2022 Wrote automated scripts for functional testing.']","[u'Masters of Science in Business Analytics', u""Bachelor's in Electronics & Communication""]","[u'University of Cincinnati, Carl H. Lindner College of Business Cincinnati, OH\nAugust 2014 to August 2015', u'GLA University Mathura, Uttar Pradesh\nJune 2002 to June 2006']","degree_1 : Masters of Science in Bsiness Analytics, degree_2 :  ""Bachelors in Electronics & Commnication"""
0,https://resumes.indeed.com/resume/f7f07991e97eef45,"[u""Data Scientist/ Machine Learning\nCardinal Health - Dublin, OH\nAugust 2017 to Present\nDescription:\nCardinal Health, a drug wholesaler that also makes gloves and surgical apparel, has been on a buying spree lately in a bid to shore up future earnings. It's also further expanding into services and support for customers that are moving from the traditional hospital model to larger integrated systems across various sites of care.\n\nResponsibilities:\n\u2022 Performed Data Profiling to learn about behavior with various features such as traffic pattern, location, Date and Time etc.\n\u2022 Application of various machine learning algorithms and statistical modeling like decision trees, regression models, neural networks, SVM, clustering to identify Volume using scikit-learn package in Python, Matlab.\n\u2022 Used clustering technique K-Means to identify outliers and to classify unlabeled data.\n\u2022 Evaluated models using Cross Validation, Log loss function, ROC curves and used AUC for feature selection.\n\u2022 Analyze traffic patterns by calculating autocorrelation with different time lags.\n\u2022 Ensured that the model has low False Positive Rate.\n\u2022 Addressed overfitting by implementing the algorithm regularization methods like L2 and L1.\n\u2022 Used Principal Component Analysis in feature engineering to analyze high dimensional data.\n\u2022 Created and designed reports that will use gathered metrics to infer and draw logical conclusions of past and future behavior.\n\u2022 Performed Multinomial Logistic Regression, Random forest, Decision Tree, SVM to classify package is going to deliver on time for the new route.\n\u2022 Performed data analysis by using Hive to retrieve the data from Hadoop cluster, SQL to retrieve data from Oracle database.\n\u2022 Used MLlib, Spark's Machine learning library to build and evaluate different models.\n\u2022 Implemented rule-based expert system from the results of exploratory analysis and information gathered from the people from different departments.\n\u2022 Performed Data Cleaning, features scaling, features engineering using pandas and numpy packages in python.\n\u2022 Developed MapReduce pipeline for feature extraction using Hive.\n\u2022 Created Data Quality Scripts using SQL and Hive to validate successful data load and quality of the data. Created various types of data visualizations using Python and Tableau.\n\u2022 Communicated the results with operations team for taking best decisions.\n\u2022 Collected data needs and requirements by Interacting with the other departments.\n\nEnvironment: Python 2.x, CDH5, HDFS, Hadoop 2.3, Hive, Impala, Linux, Spark, Tableau Desktop, SQL Server 2012, Microsoft Excel, Matlab, Spark SQL, Pyspark."", u'Data Scientist\nPinnacle Financial Partners - Nashville, TN\nMay 2016 to July 2017\nDescription:\nPinnacle Financial Partners, Inc. operates as a bank holding company for Pinnacle Bank that provides various banking products and services in the United States. The company accepts various deposits, including savings, checking, interest-bearing checking, money market, and certificate of deposit accounts. Its loan products include commercial loans, such as equipment and working capital loans; commercial real estate loans comprising investment properties and business loans secured by real estate; and loans to individuals consisting of secured and unsecured installment and term loans, lines of credit, residential first mortgage loans, and home equity loans and lines of credit.\n\nResponsibilities:\n\u2022 Responsible for analyzing large data sets to develop multiple custom models and algorithms to drive innovative business solutions.\n\u2022 Perform preliminary data analysis and handle anomalies such as missing, duplicates, outliers, and imputed irrelevant data.\n\u2022 Remove outliers using Proximity Distance and Density-based techniques.\n\u2022 Involved in Analysis, Design and Implementation/translation of Business User requirements.\n\u2022 Experienced in using supervised, unsupervised and regression techniques in building models.\n\u2022 Performed Market Basket Analysis to identify the groups of assets moving together and recommended the client their risks\n\u2022 Experience in determine trends and significant data relationships using advanced Statistical Methods.\n\u2022 Implemented techniques like forwarding selection, backward elimination and stepwise approach for selection of most significant independent variables.\n\u2022 Performed Feature selection and Feature extraction dimensionality reduction methods to figure out significant variables.\n\u2022 Used RMSE score, Confusion matrix, ROC, Cross-validation and A/B testing to evaluate model performance in both simulated environment and the real world.\n\u2022 Performed Exploratory Data Analysis using R. Also involved in generating various graphs and charts for analyzing the data using Python Libraries.\n\u2022 Involved in the execution of multiple business plans and projects Ensures business needs are being met Interpret data to identify trends to go across future data sets.\n\u2022 Developed interactive dashboards, Created various AdHoc reports for users in Tableau by connecting various data sources.\n\nEnvironment: Python, SQL server, Hadoop, HDFS, HBase, MapReduce, Hive, Impala, Pig, Sqoop, Mahout, Spark MLLib, MongoDB, Tableau, ETL, Unix/Linux.', u'Data Analyst\nFleetCor Technologies Inc - Norcross, GA\nJanuary 2015 to April 2016\nDescription: FleetCor Technologies, Inc. provides specialized payment products and services to Dataes, commercial fleets, oil companies, petroleum marketers, and government entities in North America, Europe, South Africa, and Asia.\nResponsibilities:\n\n\u2022 Involved in Analysis, Design and Implementation/translation of Business User requirements.\n\u2022 Worked on large sets of Structured and Unstructured data.\n\u2022 Actively involved in designing and developing data ingestion, aggregation, and integration in Hadoop environment.\n\u2022 Developed Sqoop scripts to import-export data from relational sources and handled incremental loading on the customer, transaction data by date.\n\u2022 Experience in creating Hive Tables, Partitioning and Bucketing.\n\u2022 Performed data analysis and data profiling using complex SQL queries on various sources systems including Oracle 10g/11g and SQL Server 2012.\n\u2022 Identified inconsistencies in data collected from different source.\n\u2022 Worked with business owners/stakeholders to assess Risk impact, provided a solution to business owners.\n\u2022 Experienced in determine trends and significant data relationships Analyzing using advanced Statistical Methods.\n\u2022 Carrying out specified data processing and statistical techniques such as sampling techniques, estimation, hypothesis testing, time series, correlation and regression analysis Using R.\n\u2022 Applied various data mining techniques: Linear Regression & Logistic Regression, classification, clustering.\n\u2022 Took personal responsibility for meeting deadlines and delivering high-quality work.\n\u2022 Strived to continually improve existing methodologies, processes, and deliverable templates.\n\nEnvironment: R, SQL server, Oracle, HDFS, HBase, MapReduce, Hive, Impala, Pig, Sqoop, NoSQL, Tableau, Unix/Linux, Core Java, Log 4j.', u'System Analyst\nAccenture - Bengaluru, Karnataka\nMay 2009 to November 2011\nDescription: Accenture PLC is a global management consulting and professional services company that provides strategy, consulting, digital, technology and operations.\n\nResponsibilities:\n\u2022 OABS Analytics team is mainly into reporting and creation of dashboards and Digitization of reports for all the Practices.\n\u2022 Design and develop the Headcount reports for worldwide regions and for all the practices.\n\u2022 Validation of data to check the accuracy of it.\n\u2022 Analytics team is mainly into reporting and creation of dashboards and Digitization of reports.\n\u2022 Design and develop the sales standard reports for APJ regions and area across the client locations.\n\u2022 Prepare the Weekly monthly and fortnight sales report.\n\u2022 Involved in automation of reports, was a go-to person for all automation on all MS office tools.\n\u2022 Worked on VBA projects as the integration of Excel-PowerPoint and Excel-Access etc.\n\u2022 Used charts, Pivots, and other complex excel function to automate and improve the productivity of the team.\n\u2022 Frequent Ad-hoc request to support key business requirement.\n\u2022 Providing Content and Data Management services to Ford of Europe being in Marketing Sales & Service team.\n\u2022 Validated data to ensure the quality, validity, and accuracy of content.\n\u2022 Claim processing for both Indian and European market (Retail Claims, Fleet Claims, and Warranty Claims etc.).\n\nEnvironment: Ad hoc, VBA, SQL/Server, Oracle 9i, MS-Office, Teradata.']",[u'Bachelor of Computer Science in TOOLS AND SKILLS'],[u'Informatica Power Centre'],degree_1 : Bachelor of Compter Science in TOOLS AND SKILLS
0,https://resumes.indeed.com/resume/12bbc88ccd976353,"[u'Melrose Pharmacy\nFebruary 2018 to Present', u'Data entry\nWalgreens\nAugust 2015 to Present\n\u2022 Filling.\n\u2022 Dispensing.\n\u2022 Prescription transfers in and out.\n\u2022 Immunizations.\n\u2022 Counseling.\n\u2022 Prescription deletes.\n\u2022 Efficiently managing both the counter as well as the drive-through.\nPage |3\n\n\u2022 Reminder calls.\n\u2022 New therapy and adherent calls.', u'Melrose Pharmacy\nPresent', u'Avella Specialty Pharmacy\nJanuary 2018 to January 2018\n\u2022 MTM of specialty drugs involved in various cancers, HCV and HIV.\n\u2022 Prior authorizations.\n\u2022 Transfers in and out.\n\u2022 Prescription monitoring.', u""Cancer Treatment Center of America\nNovember 2017 to November 2017\n\u2022 Participated in compounding IV chemo drugs for infusion.\n\u2022 Checked patient's profile for drug-drug interactions.\n\u2022 Participated in operating the pyxis machine."", u'Midtown VA Clinic - Midtown, VA, US\nOctober 2017 to October 2017\nPharmacological and non-pharmacological management of chronic disease\nstates like hypertension, diabetes and hyperlipidemia through face-to-face and telephonic patient interviews.', u""Banner Boswell Medical Center\nAugust 2017 to August 2017\n\u2022 Collecting patient's med history and documenting in Cerner.\n\u2022 Participated in rounds in the ED.\n\u2022 Performed vancomycin/aminoglycoside dosing calculations.\n\u2022 Participated in Geriatric in-patient case discussions.\n\u2022 Shadowed at Cardiac solutions."", u""Select Specialty Hospital\nJuly 2016 to July 2016\nGood experience in compounding creams and pastes in a hospital setting.\nPage |2\n\n\u2022 Co-ordinated with the preceptor to prepare a manuscript justifying the necessity\nof frequent INR monitoring for patients on warfarin therapy based on the hospital records.\n\u2022 Active participation in patient rounds.\n\u2022 Refilling medications and discarding expired ones in nurses' wows.\n\nii) APPE rotations-"", u""Cigna Retail Pharmacy\nJune 2016 to June 2016\n\u2022 Successfully filled and dispensed around 150 prescriptions per day.\n\u2022 Active participation in patient counseling.\n\u2022 Successfully shadowed the pharmacist at Cigna's coumadin clinic."", u'research scientist\nCenter for Structural Biology-Vanderbilt University - Nashville, TN\nNovember 2009 to June 2011\nParticipated in the project: ""Computational study of binding affinity of Streptavidin mutants to biotin""-Published in Biochemistry 2012']","[u'Doctor of Pharmacy in Pharmacy', u'Doctor of Philosophy in Chemistry', u'Masters in Science in Physical Chemistry', u'Bachelors in Science in Chemistry', u'PharmD']","[u'Midwestern University College of Pharmacy Glendale, AZ\nJune 2015 to Present', u'Indian Institute of Technology Mumbai, Maharashtra\nOctober 2004 to May 2009', u'University of Calcutta Kolkata, West Bengal\nAugust 2001 to August 2003', u""St. Xaviers' College, University of Calcutta Kolkata, West Bengal\nAugust 1998 to July 2001"", u'Midwestern University']","degree_1 : Doctor of Pharmacy in Pharmacy, degree_2 :  Doctor of Philosophy in Chemistry, degree_3 :  Masters in Science in Physical Chemistry, degree_4 :  Bachelors in Science in Chemistry, degree_5 :  PharmD"
0,https://resumes.indeed.com/resume/cca11c011a9f3ac6,"[u'Data Scientist/Machine Learning\nCapital One - Wilmington, DE\nMarch 2017 to Present\nDescription:\nResponsibilities:\n\u2022 Built models using Statistical techniques like Bayesian HMM and Machine Learning classification models like XG Boost, SVM, and Random Forest.\n\u2022 A highly immersive Data Science program involving Data Manipulation & Visualization, Web Scraping, Machine Learning, Python programming, SQL, GIT, Unix Commands, NoSQL, MongoDB, Hadoop.\n\u2022 Setup storage and data analysis tools in Amazon Web Services cloud computing infrastructure.\n\u2022 Used pandas, numpy, seaborn, scipy, matplotlib, scikit-learn, NLTK in Python for developing various machine learning algorithms.\n\u2022 Installed and used Caffe Deep Learning Framework\n\u2022 Worked on different data formats such as JSON, XML and performed machine learning algorithms in Python.\n\u2022 Worked as Data Architects and IT Architects to understand the movement of data and its storage and ER Studio 9.7\n\u2022 Participated in all phases of data mining; data collection, data cleaning, developing models, validation, visualization and performed Gap analysis.\n\u2022 Data Manipulation and Aggregation from a different source using Nexus, Toad, Business Objects, Power BI and Smart View.\n\u2022 Implemented Agile Methodology for building an internal application.\n\u2022 Focus on integration overlap and Informatica newer commitment to MDM with the acquisition of Identity Systems.\n\u2022 Good knowledge of Hadoop Architecture and various components such as HDFS, JobTracker, Task Tracker, NameNode, DataNode, Secondary NameNode, and MapReduce concepts.\n\u2022 As Architect delivered various complex OLAP databases/cubes, scorecards, dashboards and reports.\n\u2022 Programmed a utility in Python that used multiple packages (scipy, numpy, pandas)\n\u2022 Implemented Classification using supervised algorithms like Logistic Regression, Decision trees, KNN, Naive Bayes.\n\u2022 Used Teradata15 utilities such as Fast Export, MLOAD for handling various tasks Data Migration/ETL from OLTP Source Systems to OLAP Target Systems\n\u2022 Experience in Hadoop ecosystem components like Hadoop MapReduce, HDFS, HBase, Oozie, Hive, Sqoop, Pig, Flume including their installation and configuration.\n\u2022 Updated Python scripts to match training data with our database stored in AWS Cloud Search, so that we would be able to assign each document a response label for further classification.\n\u2022 Data transformation from various resources, data organization, features extraction from raw and stored.\n\u2022 Validated the machine learning classifiers using ROC Curves and Lift Charts.\n\u2022 Extracted data from HDFS and prepared data for exploratory analysis using data munging.\nEnvironment: ER Studio 9.7, Tableau 9.03, AWS, Teradata 15, MDM, GIT, Unix, Python 3.5.2, , MLLib, SAS, Regression, Logistic Regression, Hadoop, NoSQL, Teradata, OLTP, Random Forest, OLAP, HDFS, ODS, NLTK, SVM, JSON, XML, MapReduce.\n\nClient: Target, MD', u""Data Scientist/Data Architecture\nMLLib\nOctober 2015 to February 2017\nDescription:\nResponsibilities:\n\u2022 Responsible for performing Machine-learning techniques regression/classification to predict the outcomes.\n\u2022 Coded R functions to interface with Caffe Deep Learning Framework.\n\u2022 Working in Amazon Web Services cloud computing environment\n\u2022 Used Tableau to automatically generate reports, Worked with partially adjudicated insurance flat files, internal records, 3rd party data sources, JSON, XML and more.\n\u2022 Identified and evaluated various distributed machine learning libraries like Mahout, MLLib (Apache Spark) and R.\n\u2022 Evaluated the performance of Various Classification and Regression algorithms using R language to predict the future power.\n\u2022 Worked with several R packages including knitr, dplyr, SparkR, CausalInfer, spacetime.\n\u2022 Involved in Detecting Patterns with Unsupervised Learning like K-Means Clustering.\n\u2022 Implemented end-to-end systems for Data Analytics, Data Automation and integrated with custom visualization tools using R, Mahout, Hadoop, and MongoDB.\n\u2022 Gathering all the data that is required from multiple data sources and creating datasets that will be used in the analysis.\n\u2022 Performed Exploratory Data Analysis and Data Visualizations using R, and Tableau.\n\u2022 Perform a proper EDA, Univariate and bi-variate analysis to understand the intrinsic effect/combined effects.\n\u2022 Worked with Data governance, Data quality, data lineage, Data architect to design various models and processes.\n\u2022 Independently coded new programs and designed Tables to load and test the program effectively for the given POC's using with Big Data/Hadoop.\n\u2022 Designed data models and data flow diagrams using Erwin and MS Visio.\n\u2022 As an Architect implemented MDM hub to provide clean, consistent data for an SOA implementation.\n\u2022 Developed, Implemented & Maintained the Conceptual, Logical & Physical Data Models using Erwin for forwarding/Reverse Engineered Databases.\n\u2022 Established Data Architecture Strategy, Best Practices, Standards, and Roadmaps.\n\u2022 Lead the development and presentation of a data analytics data-hub prototype with the help of the other members of the emerging solutions team.\n\u2022 Performed data cleaning and imputation of missing values using R.\n\u2022 Worked with Hadoop eco system covering HDFS, HBase, YARN and Map Reduce.\n\u2022 Take up ad-hoc requests based on different departments and locations.\n\u2022 Used Hive to store the data and perform data cleaning steps for huge datasets.\n\u2022 Created dash boards and visualization on regular basis using ggplot2 and Tableau.\n\u2022 Creating customized business reports and sharing insights to the management.\n\u2022 Worked with BTEQ to submit SQL statements, import and export data, and generate reports in Teradata.\n\u2022 Interacted with the other departments to understand and identify data needs and requirements and work with other members of the IT organization to deliver data visualization and reporting solutions to address those needs.\nEnvironment: R 3.0, Erwin 9.5, Tableau 8.0, MDM, QlikView, MLLib, PL/SQL, HDFS, Teradata 14.1, JSON, HADOOP (HDFS), MapReduce, PIG, Spark, R Studio, MAHOUT, JAVA, HIVE, AWS."", u'Data Scientist\nIPG Photonics Corp - Oxford, MA\nDecember 2014 to September 2015\nDescription: IPG Photonics Corporation develops and manufactures fiber lasers, fiber amplifiers, and diode lasers. Its laser products include low, medium, and high output power lasers from 0.5 to 2 microns in wavelength, fiber pigtailed packaged diodes and fiber coupled direct diode laser systems.\n\nResponsibilities:\n\u2022 As an Architect design conceptual, logical and physical models using Erwin and build data marts using hybrid Inmon and Kimball DW methodologies\n\u2022 Interaction with Business Analyst, SMEs, and other Data Architects to understand Business needs and functionality for various project solutions\n\u2022 Researched, evaluated, architected, and deployed new tools, frameworks, and patterns to build sustainable Big Data platforms for the clients\n\u2022 Identifying and executing process improvements, hands-on in various technologies such as Oracle, Informatica, and Business Objects.\n\u2022 Worked closely with business, data governance, SMEs and vendors to define data requirements.\n\u2022 Worked with data investigation, discovery and mapping tools to scan every single data record from many sources.\n\u2022 Designed the prototype of the Data mart and documented possible outcome from it for end-user.\n\u2022 Involved in business process modeling using UML\n\u2022 Developed and maintained data dictionary to create metadata reports for technical and business purpose.\n\u2022 Created SQL tables with referential integrity and developed queries using SQL, SQL*PLUS and PL/SQL\n\u2022 Experience in maintaining database architecture and metadata that support the Enterprise Data warehouse.\n\u2022 Developed various QlikView Data Models by extracting and using the data from various sources files, DB2, Excel, Flat Files and Big data.\n\u2022 Handled importing data from various data sources, performed transformations using Hive, Map Reduce, and loaded data into HDFS.\n\u2022 Designed both 3NF data models for ODS, OLTP systems and dimensional data models using Star and Snow flake Schemas.\n\u2022 Design, coding, unit testing of ETL package source marts and subject marts using Informatica ETL processes for Oracle database.\nEnvironment: Erwin r9.0, Informatica 9.0, ODS, OLTP, Oracle 10g, Hive, OLAP, DB2, Metadata, MS Excel, Mainframes MS Visio, Rational Rose, Requisite Pro, Hadoop, PL/SQL, etc.', u'Java Developer\nNBCC - New Delhi, Delhi\nFebruary 2010 to October 2013\nDescription: NBCC (India) Limited (formerly National Buildings Construction Corporation Limited), a Navratna organization under category I, is a Central Public Sector undertaking which trades publicly in the market and is largely owned by Government of India.\n\nResponsibilities:\n\u2022 Participating in system design, planning, estimation, and implementation.\n\u2022 Involved in developing Use Case Diagrams, Class Diagrams, Sequence Diagrams and Process Flow Diagrams for the modules using UML and Rational Rose.\n\u2022 Developed the presentation layer using JSP, AJAX, HTML, XHTML, CSS and client validations using JavaScript.\n\u2022 Developed and implemented the MVC Architectural Pattern using Spring Framework.\n\u2022 Effective usage of J2EE Design Patterns Namely Session Facade, Factory Method, Command, and Singleton to develop various base framework components in the application.\n\u2022 Modified Account View Functionality to enable display of blocked accounts details that have tags. This involved modifying beans, JSP changes, and middle tier enhancements.\n\u2022 Worked on generating the web services classes by using WSDL, UDDI, and SOAP.\n\u2022 Consumed Web Services using WSDL, SOAP, and UDDI from the third party for authorizing payments to/from customers.\n\u2022 Involved in Units integration using JUnit, bug fixing, and User acceptance testing with test cases.\n\u2022 Used CVS for version control and Maven as a build tool.\n\u2022 Designed and developed systems based on JEE specifications and used Spring Framework with MVC architecture.\n\u2022 Used Spring Roo Framework Design/Enterprise Integration patterns and REST architecture compliance for design and development of applications.\n\u2022 Involved in the application development using Spring Core, Spring Roo, Spring JEE, Spring Aspects modules and Java web-based technologies such as Web Service (REST /SOA /micro services) including micro services implementations and Hibernate ORM.\n\u2022 Used LDAP and Microsoft active directory series for authorization and authentication services.\n\u2022 Implemented different design patterns such as singleton, Session Fa\xe7ade, Factory, and MVC design patterns such as Business delegate, session fa\xe7ade and DAO design patterns.\n\u2022 Used JPA - Object Mapping for the backend data persistence.\n\nEnvironment: R 3.0, Erwin 9.5, Tableau 8.0, MDM, QlikView, MLLib, PL/SQL, HDFS, Teradata 14.1, JSON, HADOOP (HDFS), MapReduce, PIG, Spark, R Studio, MAHOUT, JAVA, HIVE, AWS.']","[u""Bachelor's""]",[u''],"degree_1 : ""Bachelors"""
0,https://resumes.indeed.com/resume/244a007e4aaa3816,"[u'Sr Data scientist /Machine learning\nCVS Health - Buffalo Grove, IL\nJune 2017 to Present\nCVS Health Corporation (previously CVS Corporation and CVS Caremark Corporation) (stylized as Heart coraz\xf3n.svgCVSHealth) is an American retail pharmacy and health care company headquartered in Woonsocket, Rhode Island. The company began in 1964 with three partners who grew the venture from a parent company, Mark Steven, Inc., that helped retailers manage their health and beauty aid product lines.', u""Data Scientist\nAIM Specialty Health - Chicago, IL\nMarch 2016 to May 2017\nDescription: AIM Specialty Health\xae (AIM) provides clinical solutions that drive appropriate, safe, and affordable care. Serving more than 50 million members across 50 states, D.C. and U.S. territories, AIM promotes optimal care through use of evidence-based clinical guidelines and real-time decision support for both providers and their patients. The AIM platform delivers significant cost-of-care savings across an expanding set of clinical domains, including radiology, cardiology, oncology, specialty drugs, sleep medicine, musculoskeletal care, and genetic testing.\n\nResponsibilities:\n\u2022 Utilized Spark, Scala, Hadoop, HQL, VQL, oozie, pySpark, Data Lake, TensorFlow, HBase, Cassandra, Redshift, MongoDB, Kafka, Kinesis, Spark Streaming, Edward, CUDA, MLLib, AWS, Python, a broad variety of machine learning methods including classifications, regressions, dimensionally reduction etc.\n\u2022 Utilized the engine to increase user lifetime by 45% and triple user conversations for target categories.\n\u2022 Application of various machine learning algorithms and statistical modeling like decision trees, text analytics, natural language processing (NLP), supervised and unsupervised, regression models, social network analysis, neural networks, deep learning, SVM, clustering to identify Volume using scikit-learn package in python, Matlab.\n\u2022 Worked onanalyzing data from Google Analytics, AdWords, Facebook etc.\n\u2022 Evaluated models using Cross Validation, Log loss function, ROC curves and used AUC for feature selection and elastic technologies like ElasticSearch, Kibana.\n\u2022 Performed Data Profiling to learn about behavior with various features such as traffic pattern, location, Date and Time etc.\n\u2022 Categorized comments into positive and negative clusters from different social networking sites using Sentiment Analysis and Text Analytics\n\u2022 Performed Multinomial Logistic Regression, Decision Tree, Random forest, SVM to classify package is going to deliver on time for the new route.\n\u2022 Performed data analysis by using Hive to retrieve the data from Hadoop cluster, Sql to retrieve datafrom Oracle database and used ETL for data transformation.\n\u2022 Performed Data Cleaning, features scaling, features engineering using pandas and numpy packages in python.\n\u2022 Exploring DAG's, their dependencies and logs using AirFlow pipelines for automation\n\u2022 Performed data cleaning and feature selection using MLlib package in PySpark and working with deep learning frameworks such as Caffe, Neon.\n\u2022 Developed Spark/Scala,R Python for regular expression (regex) project in the Hadoop/Hive environment with Linux/Windows for big data resources.\n\u2022 Used clustering technique K-Means to identify outliers and to classify unlabeled data.\n\u2022 Tracking operations using sensors until certain criteria is met using AirFlowtechnology.\n\u2022 Responsible for different Data mapping activities from Source systems to Teradata using utilities like TPump, FEXP,BTEQ, MLOAD, FLOADetc\n\u2022 Analyze traffic patterns by calculating autocorrelation with different time lags.\n\u2022 Ensured that the model has low False Positive Rate and Text classification and sentiment analysis for unstructured and semi-structured data.\n\u2022 Addressed over fitting by implementing of the algorithm regularization methods like L1 and L2.\n\u2022 Used Principal Component Analysis in feature engineering to analyze high dimensional data.\n\u2022 Used MLlib, Spark's Machine learning library to build and evaluate different models.\n\u2022 Implemented rule based expertise system from the results of exploratory analysis and information gathered from the people from different departments.\n\u2022 Created and designed reports that will use gathered metrics to infer and draw logical conclusions of past and future behavior.\n\u2022 Developed MapReduce pipeline for feature extraction using Hive and Pig.\n\u2022 Created Data Quality Scripts using SQL and Hive to validate successful data load and quality of the data. Created various types of data visualizations using Python and Tableau.\n\u2022 Communicated the results with operations team for taking best decisions.\n\u2022 Collected data needs and requirements by Interacting with the other departments.\n\nEnvironment:Python 2.x, CDH5, HDFS, Hadoop 2.3, Hive, Impala, AWS, Linux, Spark, Tableau Desktop, SQL Server 2014, Microsoft Excel, Matlab, Spark SQL, Pyspark."", u'Data Analyst\nBank of New York Mellon, NYC\nNovember 2014 to February 2016\nDescription:Bank of New York Mellon is one of the secured financial institutions that serves huge domain of customers. Bank offers various financial and banking services to its customers. The current application is a part of online banking that allows a customer to pay bills securely as well as enroll in monthly automatic recurring bill payment. The functionalities involved in e-bill payment are add a payee, make a payment, set up automatic payments, receive bills electronically, request e-mail notifications and review payments.\n\nResponsibilities:\n\u2022 Involved in the below phases of Analytics using R, Python and Jupyter notebook.\n\u2022 Worked with BI team in gathering the report requirements and also Sqoop to export data into HDFS and Hive\n\u2022 Data Mining: Used cluster analysis for identifying customer segments, Decision trees used for profitable and non-profitable customers, Market Basket Analysis used for customer purchasing behaviour and part/product association.\n\u2022 Data collection and treatment: Analysed existing internal data and external data, worked on entry errors,classification errors and defined criteria for missing values.\n\u2022 Assisted with data capacity planning and node forecasting.\n\u2022 Developed multiple Map Reduce jobs in Java for data cleaning and preprocessing.\n\u2022 Administrator for Pig, Hive and HBase installing updates patches and upgrades.\n\u2022 Installed, Configured and managed Flume Infrastructure\n\u2022 Worked on performing major upgrade of cluster from CDH3u6 to CDH4.4.0\n\u2022 Worked closely with the claims processing team to obtain patterns in filing of fraudulent claims.\n\u2022 Patterns were observed in fraudulent claims using text mining in R and Hive.\n\u2022 Developed Map Reduce programs to extract and transform the data sets and results were exported back to RDBMS using Sqoop.\n\u2022 Developed Map Reduce programs to parse the raw data, populate staging tables and store the refined data in partitioned tables in the EDW.\n\u2022 Exported the data required information to RDBMS using Sqoop to make the data available for the claims processing team to assist in processing a claim based on the data.\n\u2022 Using HiveQL developed many queries and extracted the required information.\n\u2022 Created tables in Hive and loaded the structured (resulted from Map Reduce jobs) data\n\u2022 Was responsible for importing the data (mostly log files) from various sources into HDFS using Flume\n\u2022 Created Hive queries that helped market analysts spot emerging trends by comparing fresh data with EDW reference tables and historical metrics.\n\u2022 Provided design recommendations and thought leadership to sponsors/stakeholders that improved review processes and resolved technical problems.\n\u2022 Enabled speedy reviews and first mover advantages by using Oozie to automate data loading into the Hadoop Distributed File System and PIG to pre-process the data.\n\u2022 Tested raw data and executed performance scripts.\n\u2022 Managed and reviewed Hadoop log files.\n\nEnvironment:HBase, Flume, Sqoop, R, VMware, Eclipse, Cloudera, Python,HDFS, PIG, HIVE, Map Reduce, Linux.', u""Data Analyst / Scientist\nEsri - Redlands, CA\nMarch 2013 to October 2014\nDescription:Esri is an international supplier of geographic information system (GIS) software, web GIS and geodatabase management applications\n\nResponsibilities:\n\u2022 Data analysis and reporting using MySQL, MS Power Point, MS Access and SQL assistant.\n\u2022 Involved in MySQL, MS Power Point, MS Access Database design and design new database on Netezza which will have optimized outcome.\n\u2022 Involved in writing T-SQL, working on SSIS, SSRS, SSAS, Data Cleansing, Data Scrubbing and Data Migration.\n\u2022 Involved in writing scripts for loading data to target data Warehouse using Bteq, Fast Load, Multiload\n\u2022 Create ETL scripts using Regular Expressions and custom tools (Informatica, Pentaho, and Sync Sort) to ETL data.\n\u2022 Developed SQL Service Broker to flow and sync of data from MS-I to Microsoft's master database management (MDM).\n\u2022 Involved in loading data between Netezza tables using NZSQL utility.\n\u2022 Worked on Data modeling using Dimensional Data Modeling, Star Schema/Snow Flake schema, and Fact& Dimensional, Physical&Logicaldatamodeling.\nGenerated Stats pack/AWR reports from Oracle database and analyzed the reports for Oracle wait events, time consuming SQL queries, table space growth, and database growth.\n\nEnvironment:, MySQL, MS Power Point, MS Access, MY SQL, MS Power Point, MS Access, Netezza, DB2, T-SQL, DTS, SSIS, SSRS, SSAS, ETL, MDM, Teradata, Oracle, Star Schema and Snow Flake Schema."", u""Data Analyst/Data Modeler\nNetBlade Solutions - Mumbai, Maharashtra\nOctober 2011 to February 2013\nDescription: Netblade Solution - Service Provider of web design solutions, website development & mobile development in Thane, Maharashtra. Company Factsheet. Nature of Business Service Provider; Legal Status of Firm Private Limited Company. Know More \u2022 Sure Quote; Hashtag Planet. Our Services. Web Design Solutions.\n\nResponsibilities:\n\u2022 Data analysis and reporting using MySQL, MS Power Point, MS Access and SQL assistant.\n\u2022 Involved in MySQL, MS Power Point, MS Access Database design and design new database on Netezza which will have optimized outcome.\n\u2022 Involved in writing T-SQL, working on SSIS, SSRS, SSAS, Data Cleansing, Data Scrubbing and Data Migration.\n\u2022 Involved in writing scripts for loading data to target data Warehouse using Bteq, Fast Load, Multiload\n\u2022 Create ETL scripts using Regular Expressions and custom tools (Informatica, Pentaho, and Sync Sort) to ETL data.\n\u2022 Developed SQL Service Broker to flow and sync of data from MS-I to Microsoft's master database management (MDM).\n\u2022 Involved in loading data between Netezza tables using NZSQL utility.\n\u2022 Worked on Data modeling using Dimensional Data Modeling, Star Schema/Snow Flake schema, and Fact& Dimensional, Physical&Logicaldatamodeling.\nGenerated Stats pack/AWR reports from Oracle database and analyzed the reports for Oracle wait events, time consuming SQL queries, table space growth, and database growth.\n\nEnvironment: MySQL, MS Power Point, MS Access, MY SQL, MS Power Point, MS Access, Netezza, DB2, T-SQL, DTS, SSIS, SSRS, SSAS, ETL, MDM, Teradata, Oracle, Star Schema and Snow Flake Schema."", u'Data Analyst\nSun Pharmaceutical - Mumbai, Maharashtra\nMarch 2009 to September 2011\nDescription: Sun Pharmaceutical Industries Limited is an Indian multinational pharmaceutical company headquartered in Mumbai, Maharashtra that manufactures and sells pharmaceutical formulations and active pharmaceutical.\n\nResponsibilities:\n\u2022 Collaborating with business and technology teams.\n\u2022 Data Analysis-Data collection, data transformation and data loading the data using different ETL systems like SSIS and Informatica.\n\u2022 Performed source to target mapping as part of data migration from JD Edwards system to Agile PDM system.\n\u2022 Data Migration testing and implementation activities using SSIS and SSRS tools of Microsoft SQL Server 2008.\n\u2022 Responsible for accuracy of the data collected and stored in the corporate support system.\n\u2022 Performed data review, evaluate, design, implement and maintain company database.\n\u2022 Involved in construction of data flow diagrams and documentation of the processes.\n\u2022 Interacted with end users for requirements study and analysis by JAD (Joint Application Development).\n\u2022 Performed gap analysis between the present data warehouse to the future data warehouse being developed and identified data gaps and data quality issues and suggested potential solutions.\n\u2022 Participated in system and use case modeling like activity and use case diagrams.\n\u2022 Analyzed user requirements & worked with data modelers to identify entities and relationship for data modeling.\n\u2022 Actively participated in the design of data model like conceptual, logical models using Erwin. Used Exception handling application block for checking errors/exceptions across the website.\nDeveloped Report Component, so that it retrieves the data by executing Stored Procedures throw Data Access component.\n\nEnvironment: Windows, Oracle, MS Excel, SSIS, Informatica, GAP Analysis, ERWIN']","[u""Bachelor's""]",[u''],"degree_1 : ""Bachelors"""
0,https://resumes.indeed.com/resume/4b829133510d95d9,"[u'Data Scientist/Analyst\nCoretech, Greater New York Area\nFebruary 2017 to December 2017\n\u2022 Provided statistical solutions for analysing operational challenges in software development for CoreTech.\n\u2022 Conducted exploratory analysis, hypothesis testing and linear modelling with R.\n\u2022 Built a K-means clustering model to classify the developers with identified features.\n\u2022 Performed unsupervised classification using k-means clustering in R to to classify the developers with identified features.\n\u2022 Performed ETL operations to extract data from master data, transform in to business defined readable data, and load it to reporting data systems\n\u2022 Created data models and generated DDL scripts with the help of Oracle SQL Developer\n\u2022 Implemented automated data extraction and cleaning scripts with UNIX.\n\u2022 Represented insights to clients through visualizations built with Tableau & R.\n\u2022 Performed Hypothesis tests, Chi-squared, Student t-test, ANOVA & Regression.', u""Data Analyst (Retail)\nEste\xe9 Lauder, Greater New York Area\nMay 2016 to December 2016\n\u2022 Created & updated Business requirements, converting them into reports, visualizations and design documents.\n\u2022 Performed data analysis and built predictive models (MySQL, Excel, R, ANOVA, Regression)\n\u2022 Wrote SQL queries to pull the North Americas sales data and visualized the sales statistics.\n\u2022 Time Series Forecasting to predict product demand using R Programming and integrating with Tableau using the 'RServe' in R package to deliver predicted results in dynamic visualizations.\n\n\u2022 Utilized 'forecast' package in R to run ARIMA, and Exponential Smoothing models\n\n\u2022 Developed Visualizations using Cross tables, Bar Charts, Pie Charts, Donuts Charts, Maps, Line Charts, Scatter Plots and Filters, Marking, Drill Down, Hierarchies and Colouring. (Tableau, R - ggplot)\n\u2022 Joined tables, created charts and performed advanced calculations using Excel VLOOKUP and pivot table.\n\u2022 Used ETL process and developed OLAP cube to analyse the region wise product sales.\n\u2022 Created test plans, test cases, test scripts for User Acceptance Testing, Regression testing & Integration testing.\n\u2022 Delivered UAT documentation to recommended changes to applications. (MS Visio, PowerPoint)\n\u2022 Verified that Development & Testing teams adhere to Functional Requirements and performed root cause analysis on delays in fulfilment of deliverables.\n\u2022 Addressed ah-hoc requests from business stakeholders that involved tasks ranging from data analysis and report generation to backend testing of data to adhere to the business standards\n\u2022 Assisted product managers, fellow data analysts and other business teams in completing business deliverables."", u'Data Analyst/Tableau Developer\nNew York University, Greater New York Area\nSeptember 2015 to May 2016\n\u2022 Applied sentiment analysis to analyse the feedback attributes of 4000 students to improve the student engagement and facilities.\n\u2022 Developed a suite of 50+ annual institutional reports about financial aid & future projects on Tableau and MySQL to build a more efficient data handling system for 10 diverse departments.\n\u2022 Developed Tableau visualizations and dashboards to visualize the University enrolment trends.\n\u2022 Performed Predictive Analysis & Use Case Analysis on enrolment and hiring data to increase federal funding.', u'Business Data Analyst (Financial Services)\nWipro Technologies\nDecember 2012 to July 2015\n\u2022 Created predictive models for the stock prices using R and integrated it with Tableau.\n\n\u2022 Created dashboards to present summary statistics of the claims and plotted their geographic locations.\n\n\u2022 Performed Statistical Analysis and presented the region-wise trends of the claims.\n\n\u2022 Connected Tableau server to publish dashboard to a central location for portal integration.\n\n\u2022 Implemented software development process based on Agile methodology.\n\n\u2022 Designed/developed tables, utilized SQL queries to update new customers in the client database, create script, gather and manipulate information from Oracle.\n\n\u2022 Worked on manual testing of data for ETL process to verify the validity of the data from the external source system and the internal database systems\n\n\u2022 Exposure to data warehousing concepts like ETL, aggregation and staging of data, dimensional reporting\n\n\u2022 Performed discussions with data modeler to update the data model for new requirements\n\u2022 Moderated our team as a Scrum Master and effectively communicated the project information.\n\u2022 Elicited requirements from stakeholders analysed and transformed them into BRD and SRS documents for several database related change requests']","[u'Master of Science', u'Bachelor of Engineering in Computer Science & Engineering']","[u'New York University\nMay 2017', u'Anna University\nMay 2012']","degree_1 : Master of Science, degree_2 :  Bachelor of Engineering in Compter Science & Engineering"
0,https://resumes.indeed.com/resume/6c8a2645025aa0bb,"[u'Data Scientist\nObjectNet Technologies\nMay 2017 to Present\nDeveloped an effective machine learning model to estimate the risk score of Mortgage-Backed Security (MBS) Data. Implemented K-Fold\nCross-Validation to effectively train the model. Utilized powerful models like Regression models, Logistic Regression &amp; Random Forest.\n\u2022 Significantly Improved model performance by modelling in Spark (PySpark) to fit the entire dataset containing 95 million records.\n\u2022 Visualized and Presented valuable insights to the executive team in Model Governance reviews.\n\u2022 Processed complex and large data sets using advanced Big Data querying and visualization tools.', u'Access Database Analyst\nArizona State University\nNovember 2016 to May 2017\nGenerated reports and forms from Query results (SQL) from over 25 administrative databases.', u'Data Scientist\nStatron India\nMay 2016 to May 2017\n\u2022 Analyzed sales data using highly-tuned intricate machine learning models.\n\u2022 Used Clustering and Association Rule Learning techniques for identification and segmentation of customers for upselling and cross- selling marketing strategies.\n\u2022 Extracted meaningful data by implementing Statistical Preprocessing techniques on raw and heterogeneous large-scale data.Adopted the best performing model based on meticulous Parameter Tuning and Stratified K-Fold Cross-Validation techniques. Utilized powerful\nmodels like Regression models, Logistic Regression, Random Forest (ensemble models), SVR, SVM, K-NN, K-Means and Hierarchical\nClustering, NLP, Neural Networks. Visualized and presented the results using interactive dashboards.\n\u2022 Collated valuable data from complex and large data sets using advanced Querying and Big Data tools.', u'Operations Analyst\nAhlada Group of Companies\nJanuary 2014 to April 2015\n\u2022 Achieved a reduction in rejection rate from 3.81% to 1.24% (saving approx.$ 40,000) using Statistical modelling and machine learning.\n\u2022 Implemented Regression models in Cost Analysis and Defect Estimation.\n\u2022 Queried complex and large in-house data and furnished valuable insights to the management team using advanced visualizing tools.']","[u'Master of Science in Industrial Engineering', u'Bachelor of Engineering in Mechanical Engineering']","[u'Arizona State University Tempe, AZ\nAugust 2015 to May 2017', u'Osmania University Hyderabad, Telangana\nAugust 2011 to June 2015']","degree_1 : Master of Science in Indstrial Engineering, degree_2 :  Bachelor of Engineering in Mechanical Engineering"
0,https://resumes.indeed.com/resume/6334bd56f4d2f3fd,"[u""Data Scientist - SAS, SQL, Qlikview,R\nAIG - IN\nJune 2014 to March 2017\n\u2022 Developed a predictive model in SAS for estimation of annual projects cost of finance department and obtained an overall reduction of 2.1 million dollars in annual budgeting for projects from the existing model\n\u2022 Modeled Factor analysis in SAS which determined the key attributes of commercial insurance data and extrapolated the variables responsible for decrease of data quality score in QlikView, resulted in a clean data consequently improving the decision-making process for business stakeholders\n\u2022 Implemented a star schema data model for consumer insurance data using relational database techniques and lead a team of 3 in design and development of dashboards which comprised of sun-burst charts, what-if analysis, and Qlik maps to analyze trends of revenue generated for a period of 10 years for various KPI's and was pivotal in reconstructing the business decisions by stakeholders\n\nHIGHLIGHTS AND ACHIVEMENTS:\n\u2022 SAS Certified Base Programmer for SAS 9.\n\u2022 Received the Certificate for Employee of the Quarter Star Award from CEO in 2015 and 2016.\n\nPROJECTS:\nEducation Dataset: Statistics Method - Built a classification predictive model (Logistic Regression) in SAS to classify student's decision to enroll in a Computer Science department. Used variable reduction techniques, cross validation, and AUC metrics to measure the model [performance. Obtained an overall accuracy of 87% which did better than naive rule of 70%.\nHighway Safety Research Group (Louisiana Police Department): Statistics method - Built a boosted Neural Network classification model in SAS to predict impaired drivers involved in fatal accidents. Obtained an overall accuracy of 97%, which is 8% improvement from the existing model. As a result, police department were able to classify the missing values of impaired driver more accurately than before and could possibly implement stricter rules.\nHome Builders Missing Data Analysis and Imputation: Analytics - Analyzed Home Builders Association of Tennessee data for missing pattern. Ran MAR, MCAR test which identified the reason the of missingness and used multiple imputation techniques such as MCMC (Markov chain Monte Carlo) and Fully specified condition (FCS). As a result, 40 % of the missing data were imputed which increased the predictive accuracy of the model by 7%.\nHuman Resources Analytics: Built a neural network model to predict the employees\u2019 attrition rate with an overall accuracy of 96.8 %. Identified a 3-key metrics which are highly correlated with the attrition rate and provided plausible recommendation.""]","[u'Master of Science in Analytics', u""Bachelor's in Computer Science and Engineering""]","[u'Louisiana State University Baton Rouge, LA\nMay 2017 to May 2018', u'Sri Jayachamarajendra College of Engineering\nMay 2014']","degree_1 : Master of Science in Analytics, degree_2 :  ""Bachelors in Compter Science and Engineering"""
0,https://resumes.indeed.com/resume/3fe4b431cc76d977,[],[],[],degree_1 : 
0,https://resumes.indeed.com/resume/cf1eeb337120c431,"[u""Clinical Software Specialist\nHemaTerra Technologies - Baltimore, MD\nJanuary 2016 to January 2017\nProject Lead:\n\u2022 Lead all clinical software training though WebEx and from client sites.\n\u2022 Participates in all stages of software validation.\n\u2022 Conduct perspective client demo presentations.\n\u2022 Excel in presenting software configuration details towards each client's specific needs.\n\u2022 Knowledge of each state's regulatory requirements.\n\u2022 Proactively peruse knowledge on clinical and software industry updates per continuing education classes and reading\nmaterial.\n\u2022 Monitor project timelines for multiple clients to ensure successful go live dates are met.\n\u2022 Serve all post live clients with ongoing support and new project request."", u'Clinical Operations Manager\nCONFIRMATRIX - Lawrenceville, GA\nJanuary 2012 to January 2016\nProject Management:\n\u2022 Facilitate mile stone meetings to ensure projects are meeting assigned time frames.\n\u2022 Participates and delegate in user acceptance testing and testing of new system functionality.\n\u2022 Conduct mediation and conflict resolution between staff members.\n\u2022 Track and communicate with new and existing clients to ensure the laboratory is meeting all their needs.\n\u2022 Designed a process flow to better achieve customer turnaround time.\n\u2022 Track proficiency samples and ensure proper testing has taken place.\n\u2022 Gather and review correlation studies into report form for Director Review.\n\u2022 Achieved high staff morale and retention through effective communication, prompt problem resolution, proactive supervisory\npractices and facilitating a proactive work environment.\n\u2022 Serve as escalation point expert to subordinates.', u'Education Coordinator, Data Analysis\nCONFIRMATRIX\nJanuary 2012 to January 2015\n\u2022 Created an on board training program for in house and off site laboratories.\n\u2022 Mentor and monitor employee work performance.\n\u2022 Coordinated and conducted semi-annual and yearly competencies.\n\u2022 Log into client laboratory sites to help interpret questionable data.\n\u2022 Administrator for the program, Medial Lab in order to facilitate CE credits are being maintained.\n\u2022 Review laboratory reports to ensure test interpretation is clear.\n\u2022 Conduct end user testing when new test codes and rules have been implemented.\n\u2022 Perform data audits on analyst to ensure they are following SOP and taking corrective action.', u'Clinical Laboratory Scientist\nQUALTEX - Norcross, GA\nJanuary 2011 to January 2012\n\u2022 Perform ABO and antibody testing on donor units.\n\u2022 Perform ELISA and protein extraction testing.\n\u2022 Conduct quality review protocols and serve as team lead to ensure Quality Assurance is up to SOP.\n\u2022 Validate new reagents.\n\u2022 Document and perform proficiency test.\n\u2022 Act as primary liaison responsible for the communication between donor organization and testing site.', u'Medical Technologist\nGWINNETT MEDICAL HOSPITAL - Lawrenceville, GA\nJanuary 2008 to January 2011\n\u2022 Performed laboratory testing used in the diagnosis, treatment and prevention of disease.\n\u2022 Cross match units of blood and conduct antibody testing ID.\n\u2022 Review microbiology and blood smear slides.\n\u2022 Conduct urinalysis and chemistry testing.\n\u2022 Evaluate QC to ensure they pass before results are released.\n\u2022 Participate in proficiency testing and ensure proper procedures have been followed according to regulatory body inspections.']",[u'BA in Biomedical Science'],"[u'Medical College of Georgia Augusta, GA']",degree_1 : BA in Biomedical Science
0,https://resumes.indeed.com/resume/559ecb667acd564a,"[u'Data Scientist - Technical Sales\nIBM - Armonk, NY\nAugust 2015 to Present\nOver four periods, averaged 100% of sales quota and exceeded quota twice.\nGenerated over $1.5M\n\n\u2022 Focused on both Open Source and Proprietary Data Science Offerings.\n\u2022 Delivered 10 or more Proof of Concepts each year to multiple clients on multiple platforms.\n\u2022 Averaged 6 client meetings per month.\n\u2022 Demonstrated Technology and proposed solutions to complicated problems.\n\u2022 Responded to RFPs\n\n1812 Emerson Lane Denton, TX 76209 - 940-453-0975 - Shad.Griffin@us.ibm.com', u'Managing Consultant\nIBM - Armonk, NY\nAugust 2013 to August 2015\nAveraged more than 110% of Utilization Goal.', u'Director\nResearch and Modeling, Javelin Marketing Group - Irving, TX\nJanuary 2011 to August 2013\nMajor contributor on a $15M+ reporting and analysis contract with Large US Tele-Communication Company.\n\u2022 Led a team of diverse individuals that converted an out dated and overly complex process into a SQL server/SAS process decreasing production times by more than 90%.\n\u2022 Constructed Media Mix time-series models estimating the impact of media expenditures on product sales using SAS-ETS Software.\n\u2022 Generated weekly ROI reports based on media-mix models using SAS Enterprise Guide and SQL Server.', u'Senior Manager\nVerizon\nMay 1999 to January 2011\n- Modeling and Analysis, Supermedia, LLC (Verizon). DFW Airport, Texas\n\u2022 Worked with partners to enact positive change in a multi-billion-dollar publishing, internet and advertising organization.\n\u2022 Managed employees, projects and vendors.\n\nSample of Major Accomplishments:\nDirect Sales Channel Optimization - Used marketing analytics to guide the deployment of 2500 Premise and Telephone Sales Reps - Increased sales rep productivity by 8%\n\nLead/Click/Call Prediction - Utilized advanced knowledge of data, statistics and programming to estimate, within 5%, the number of leads an ad will generate if placed on-line or in a printed yellow pages directory.\n\nPredicting Internet Traffic -- Estimated the demand for search based advertising in all category-geography groupings throughout the United States. Decreased the number of accounts requiring manual input by 75%.']","[u'MS in Economics in Economics', u'BA in Economics in Economics']","[u'University of North Texas\nJanuary 1994', u'Louisiana State University\nJanuary 1992']","degree_1 : MS in Economics in Economics, degree_2 :  BA in Economics in Economics"
0,https://resumes.indeed.com/resume/d1d986310d14434f,"[u'Data Scientist Intern\nIBM - Shanghai, CN\nJune 2017 to August 2017\nProject: Prediction Model of High Risk Customers in Corporate Loans\n\u2022 Exported 14GB data from database by SQL, manipulated data and built a prediction model via SPSS Modeler to predict high risk customers.\n\u2022 Created 85 features independently based on business insights. Selected 21 features by forward selection method, which is superior than PCA after comparison. Took feature transformations to enhance the impacts of the inputs on model while preventing overfitting.\n\u2022 Used package ROSE in R to deal with imbalanced data. Applied 5 methods (logistic regression, SVM, decision\ntree, random forest, and neural network) to build the prediction model.\n\u2022 Improved the accuracy rate from 7% to 45% and recall rate from 47% to 90%, compraring to the previous model.\n\u2212 Project: Profiling Customers in Corporate Loans\n\u2022 Exported 15GB data from database by SQL, manipulated data and created customer profiles via SPSS Modeler.\n\u2022 Detected outliers by standard deviation and replaced outliers with boundary values within the desired range.\n\u2022 Created 40 features by RFM and PCA. Derived 36 tags from static info and cluster analysis using K-Means.', u'Advisor\nResearch Project\nFebruary 2015 to January 2016\nProf. Kyle Mandli\n\u2022 Introduced an entropy-satisfying scheme for hyperbolic PDEs, especially for two-layer shallow water equations.\n\u2022 Conducted numerical tests under different boundary and initial conditions via P ython and F ORT RAN\n\u2022 Analyzed the numerical properties (entropy-satisfying, well-balanced) of the new numerical scheme.']","[u'Ph.D. (not complete) in Mathematics', u'M.S. in Applied Mathematics', u'B.S. in Engineering Mechanics']","[u'Carnegie Mellon University\nAugust 2016 to January 2018', u'Columbia University New York, NY\nSeptember 2014 to December 2015', u'Harbin Institute of Technology\nAugust 2010 to June 2014']","degree_1 : Ph.D. (not complete) in Mathematics, degree_2 :  M.S. in Applied Mathematics, degree_3 :  B.S. in Engineering Mechanics"
0,https://resumes.indeed.com/resume/b8afa91d534d559d,"[u'Graduate Research Assistantship\nGeorgia State University\nJanuary 2017 to May 2017\nWeb Developer\n\u2022 Developed internal applications (student admission management) for the computer science department\n\u2022 Technologies: PHP, MySQL, JavaScript, jQuery', u""Data Scientist\nWebPlus - Ahmedabad, Gujarat\nJanuary 2013 to December 2016\n\u2022 Data warehousing and data mining of large financial data\n\u2022 Software tools creation for portfolio analytics, decision making, financial risk management\n\u2022 Experience with machine learning techniques; decision forest, Na\xefve Bayes, SVM\n\u2022 Development of front office pricing models across multiple derivative products for investment decisions\n\u2022 Experience within buy side and sell side in equity, derivatives & debt segment\n\u2022 Technologies/Tools: Matlab, C++, PHP, MySQL, MS Excel\n\nSr. Software Developer\n\u2022 Led the team of 10 developers for successful project (Mobile app Development with web based admin section & Web portals with editable admin section) execution\n\u2022 Worked on content management systems, E-commerce store, Web Portals and Mobile Apps (Drupal, Magento, Android& iOS)\n\u2022 Design and development of software development documentation\n\u2022 Business lead generation and client acquisition for web and mobile apps\n\u2022 Improved operational efficiency of the software development team by leveraging project management skills\n\u2022 Technologies/Tools/Frameworks:PHP, MySQL, Android Studio, C, C++, HTML, CSS\nSoftware Developer\n\u2022 3rd party API (Google, PayPal etc..) integration Experience\n\u2022 Complex web-portal development (features like complex database, membership site) and operations/maintenance\n\u2022 Development of web services and admin section of mobile apps (iOS/Android)\n\u2022 Worked with mobile app developers for creating backend section in PHP\n\u2022 Mentored the team of 5 PHP developers\n\u2022 Technologies/Tools/Frameworks: PHP, MySQL, Laravel, YII, CodeIgniter\nProject-1 - Machine learning technology in Investment Portal ( Jan'14 to Dec'16)\n\u2022 Machine learning technology development in MATLAB\n\u2022 Three-tier application development in MATLAB, MySQL & PHP environment\n\u25e6 Utilization of deep neural networks for decision support system\n\u25e6 Parallel computing used for deep neural networks training\n\u25e6 UI design, data warehousing development in PHP\n\u2022 Full lifecycle of data science followed from data acquisition, data preprocessing, data warehousing, data mining, data visualization & model evaluation/validation\n\u2022 Multiple version of systems were developed for different asset classes like equity, debt, options & futures.\n\u25e6 System worked on global markets with data collected from worldwide exchanges\nProject 2 - Auxiliary applications in Investment Portal ( Jan'14 to Dec'16)\n\u2022 Various three-tier application development surrounding machine learning based decision support system\n\u25e6 Portfolio tracking, CRM, data collection and warehousing, financial risk management, portfolio management related application development\n\u2022 Open-ended system for further modifications and experimentation with other machine learning methods\n\u2022 Multiple version of systems were developed for different asset classes like equity, debt, options & futures\n\u25e6 System worked on global markets with data collected from worldwide exchanges\n\u2022 Technologies/Tools/Frameworks: MATLAB, MySQL & PHP""]","[u'MS in Computer Science in Computer Science', u'Masters of Science in Finance in Finance & Management', u'BS in Computer Engineering']","[u'Georgia State University\nDecember 2017', u'Cranfield School of Management\nJanuary 2012', u'L D College of Engineering, Gujarat University\nJanuary 2011']","degree_1 : MS in Compter Science in Compter Science, degree_2 :  Masters of Science in Finance in Finance & Management, degree_3 :  BS in Compter Engineering"
0,https://resumes.indeed.com/resume/2e95afac3ce9b3a1,"[u'Senior Analyst\nCredigy Solution Inc - Atlanta, GA\nJanuary 2016 to Present\nA U.S. subsidiary of National Bank of Canada, focusing on consumer finance investment\nIdentify risk factors, conduct risk-based ranking/segmentation, and translate calculated risk to expected loss\nProcess bureau data via SQL and SAS, report KPIs via Tableau and monitor risk profiles of assets over $4.8B\nDevelop statistical models (2M+row and 1k+columns):\no Probability of default model using non-parametric survival logistics regression (ROC 0.87)\no Status transition model using multinomial logistic regression and portfolio simulation\no Account selection model using decision tree model, reducing 32% risk for target pool\nImplement risk models to provide forward-looking prediction cooperated with macro-economics forecast\nRecommend investment strategy based on risk quantification, credit policy review, and deal negation feedback', u'Data Scientist Intern\nMazliah Analytics - Atlanta, GA\nAugust 2015 to December 2015\nAnalytics consulting firm specializing in travel industry\nPrepared quarterly destination recommendation for TripAdvisor.com users and refreshed scoring algorithm\nManipulated user data and applied collaborative filtering algorithms to over 2 million users via SQL and Hive\nRevised calculation of similarity in the algorithm and increased user clicks by 5% in 30 days', u'Analyst\nRecovery Enhancement Project, Emory Healthcare - Atlanta, GA\nMay 2015 to August 2015\nIntroduced wearable device (Fitbit) to patients to enhance ambulation for recovery after colorectal surgery\nCollaborated with physicians, nurses and medical residents and designed a standard workflow\nConducted training and monitored the execution of workflow among all surgical patients for 2 months']","[u'Master of Science in Industrial & Systems Engineering', u'Bachelor of Science in Industrial Engineering']","[u'Georgia Institute of Technology Atlanta, GA\nDecember 2015', u'Tsinghua University Beijing, CN\nJuly 2014']","degree_1 : Master of Science in Indstrial & Systems Engineering, degree_2 :  Bachelor of Science in Indstrial Engineering"
0,https://resumes.indeed.com/resume/55ccacd3f1eba00b,"[u'Data Scientist\nBBVA Compass - Birmingham, AL\nFebruary 2015 to Present\nResponsibilities:\n\u2022 Implemented end-to-end systems for Data Analytics, Data Automation and Integration.\n\u2022 Responsible for data identification, collection, exploration & cleaning for modeling, participated in model development\n\u2022 Handled ad-hoc requests for business needs and provided high quality and accurate results.\n\u2022 Integrated with other departments to extract data from complex sources for reports and analysis.\n\u2022 Built standard reports for company presentation, provided ad-hoc query and analysis support, and created requirements documents by interacting with customers.\n\u2022 Used Python and Spark to implement different machine learning algorithms including Generalized Linear Model, SVM, Random Forest, Boosting and Neural Network.\n\u2022 Implement various statistical techniques to manipulate data (missing data imputation, principle component analysis and sampling) and build predictive models.\n\u2022 Writing detailed analysis plans and descriptions of analyses and findings for research protocols regulatory reports and healthcare manuscripts.\n\u2022 Developed MapReduce/Spark Python modules for machine learning & predictive analytics in Hadoop. Implemented a Python-based distributed random forest via Python streaming.\n\u2022 Record and maintain meta-analyses and analyses of systematic reviews of medical literature\n\u2022 Successfully built models to predict of the surgery based on the pre-complications of the patient using the Logistic regression\n\u2022 Worked closely with end-user clients to understand their reporting needs and provided necessary solution.\n\nTools and Techniques used: R, Clustering, Regressions Analysis, Singular Vector Decomposition -SVD, Minitab, Oracle, and Tableau', u""Data Scientist\nUHG, Piles grove NJ\nAugust 2014 to January 2015\nResponsibilities:\n\u2022 Build multiple POC's for a major hospitality chain.\n\u2022 Used R, Python and Spark to develop variety of models and algorithms for analytic purposes\n\u2022 Identifying the energy consumption parameters and building the model to identify what causes more consumption.\n\u2022 Performed Data Cleaning, features scaling, features engineering using pandas and numpy packages in python.\n\u2022 Responsible for design and development of advanced R/Python programs to prepare transform and harmonize data sets in preparation for modeling.\n\u2022 Created Data Quality Scripts using SQL and Hive to validate successful data load and quality of the data. Created various types of data visualizations using Python and Tableau.\n\u2022 Responsible for design and development of advanced R/Python programs to prepare transform and harmonize data sets in preparation for modeling.\n\u2022 Overview: Client wanted to identify the drivers of hourly energy consumption from the different parameters and also to control the energy consumption it is paramount to understand its behavior and the main possible factors that may drive the same. The usage of the HVAC systems is typically driven by ambient temperature. There are elaborate frameworks that are available for anyone to be able to investigate the role of ambient temperature towards energy consumption\n\nTools and Techniques used: R, Logistic Regression, clustering, Hypothesis, Random Forest and Tableau."", u""Data Scientist/ Research Analyst\nMacy's - Wayne, NJ\nSeptember 2011 to July 2013\nResponsibilities:\n\u2022 Provided statistical solutions for multiple projects for US & UK based retail chain, FMCG, CPG companies.\n\u2022 Participated in data acquisition with Data Engineer team to extract historical and real-time data.\n\u2022 Conducted Exploratory Data Analysis using R and carried out visualizations with Tableau reporting.\n\u2022 Designed and implemented cross-validation and continuous statistical tests.\n\u2022 Created reports and dashboards to explain and communicate data insights, significant features, models scores and performance of new recommendation system to both technical and business teams.\n\u2022 Applied deep learning and machine learning algorithms to automate portfolio collection and aggregation process, access to appropriate market information and utilization of different pricing methodologies to estimate fair value.\n\u2022 Have experience working in Rapid miner studio and Python.\n\u2022 Used GIT for version control with Data Engineer team and Data Scientists colleagues.\n\u2022 Used agile methodology and SCRUM process for project developing.\n\nAnalytical implementation:\n\u2022 Operations Analytics comprises store Scorecard, Cluster based analysis, Store and Resource productivity analysis, Growth-Trend analysis, Like-for-Like Store analysis.\n\u2022 Merchandising and Inventory Analytics - Merchandise Plan Performance, Category scorecard,\nCategory tactics including Assortment planning, OTB planning, Buying Plan, Allocation planning and Promotion planning.\n\u2022 Customer Analytics - Demographic and purchase behavior segmentation, Customer Churn-Acquisition-Retention, Market Basket Analysis, Loyalty based analysis, RFM Scoring, Campaign Analysis, Customer Concentration Analysis and Customer Purchase Behavior Analysis.\n\u2022 Market Assessment (External Data Analytics) - Retailer vs. Competitor/ Market - Share & trend\nanalysis, Channel Assessment, Retail format based analysis, Pricing Comparison between regions, Event comparisons, Category mix comparison, Private label/ National brand analysis, Promotion mix, Customer buying behavior.\n\u2022 Documentation - provide functional documents to support Sales and Marketing teams.\n\nTools and Techniques used: R, Minitab, Multi-Class Logistics Regression Classifier, Boosted Regression Tree, Random Forest, Association Rules, Support Vector Machine, Clustering Analysis, Collaborative Recommended System, Time-series Analysis, Tableau, Excel-Miner.""]","[u""Bachelor's""]",[u''],"degree_1 : ""Bachelors"""
0,https://resumes.indeed.com/resume/768cd9dcfa0db1d4,"[u'Data Scientist\nSpecialization in Analytical tools like R, Oracle - SQL and all-time favourite Excel.\n\u27a2 Using Python and implementing Machine learning models.\n\u27a2 Experience in building data models, collection and consolidation of data, data cleaning, exploratory analysis of model, development, documentation and monitoring\n\u27a2 Experience to perform descriptive statistics of data like univariate analysis, quantile plot, percentile distribution, removing outliers and extreme values and missing values in the data\n\u27a2 Knowledge on Concept of Data Science, Data Representation and Visualization techniques\n\u27a2 Experience in Machine Learning Techniques, Statistical Analysis and Modelling, Big Data Analytics, Advanced Visualization, Data Science, Statistical Computation and Numerical Analysis.\n\u27a2 Rich experience in Statistical Analysis and Modelling Techniques including: Probability and Statistical models, Predictive Models and Forecasting techniques, Stochastic Process, Time series and Trend Analysis, Regression and Auto-correlation, Analysis of variance and multivariate analysis, Factor Analysis, Hypothesis Testing, Logistic regression, Clustering methods like k-means, hierarchical.\n\u27a2 I have participated on kaggle data sets to know and learn the new concepts of machine learning.\n\u27a2 Knowledge of technologies like Java, C#, Html5, JQuery, JavaScript, Angular JS.\n\nTypes of Projects handled:\nMarketing Analytics\n1. Regression Modelling - To identify the right housing price of a investment firm basis on City Category, Parking area and other important characteristics of the property.\n\n2. Market basket analysis: identifying products and content that go well together.\n3. Word Cloud and Sentiment Analytics on Reviews of Product Categories using.\n\nBanking and Finance\n\n1. Predict Loan Status using various classification techniques - Company wants to automate the loan eligibility process based on customer details like Gender, Marital Status and Income etc. To automate this process, we need to predict the loan status of each customer.']",[u'Bachelors of Engineering in Engineering'],[u'JNTU'],degree_1 : Bachelors of Engineering in Engineering
0,https://resumes.indeed.com/resume/2e45844078281a18,"[u""Data Scientist\nOpera Solutions, New Jersey\nOctober 2017 to Present\nDescription: Opera Solutions, LLC is a technology and analytics company mainly focused on big data. The firm uses a combination of machine learning science, advanced predictive analytics, technology, large-scale data management, and human expertise. Opera Solutions delivers predictive analytics as a service, and offers hosted, cloud-based systems for specific business problems, e.g., predicting the behavior of individual consumers, stopping revenue leakage in hospitals, warning of threats to corporate security or brand health, etc.\n\nResponsibilities:\n\u2022 Performed Data Profiling to learn about behavior with various features of USMLE examinations of various student patterns.\n\u2022 Evaluated models using Cross Validation, Log loss function, ROC curves and used AUC for feature selection and elastic technologies like ElasticSearch, Kibana etc\n\u2022 Addressed overfitting by implementing of the algorithm regularization methods like L2 and L1.\n\u2022 Implemented statistical modeling with XGBoost machine learning software package using Python to determine the predicted probabilities of each model.\n\u2022 Created master data for modelling by combining various tables and derived fields from client data and students LORs, essays and various performance metrics.\n\u2022 Formulated a basis for variable selection and GridSearch, KFold for optimal hyperparameters\n\u2022 Utilized Boosting algorithms to build a model for predictive analysis of student's behaviour who took USMLE exam apply for residency.\n\u2022 Used numpy, scipy, pandas, nltk(Natural Language Processing Toolkit), matplotlib to build the model.\n\u2022 Formulated several graphs to show the performance of the students by demographics and their mean score in different USMLE exams.\n\u2022 Application of various Artificial Intelligence(AI)/machine learning algorithms and statistical modeling like decision trees, text analytics, natural language processing(NLP), supervised and unsupervised, regression models.\n\u2022 Used Principal Component Analysis in feature engineering to analyze high dimensional data.\n\u2022 Performed Data Cleaning, features scaling, features engineering using pandas and numpy packages in python and build models using deep learning frameworks.\n\u2022 Created deep learning models using Tensorflow and keras by combining all tests as a single normalized score and predict residency attainment of students.\n\u2022 Used XGB classifier if the feature is an categorical variable and XGB regressor for continuous variables and combined it using FeatureUnion and FunctionTransfomer methods of Natural Language Processing.\n\u2022 Used OnevsRest Classifier to fit each classifier against all other classifiers and used it on multiclass classification problems.\n\u2022 Implemented application of various machine learning algorithms and statistical modeling like Decision Tree, Text Analytics, Sentiment Analysis, Naive Bayes, Logistic Regression and Linear Regression using Python to determine the accuracy rate of each model.\n\u2022 Created and designed reports that will use gathered metrics to infer and draw logical conclusions of past and future behavior.\n\u2022 Generated various models by using different machine learning and deep learning frameworks and tuned the best performance model using Signal Hub.\n\u2022 Created data layers as signals to Signal Hub to predict new unseen data with performance not less than the static model build using deep learning framework.\n\nEnvironment: Python 2.x,3.x, Hive, AWS, Linux, Tableau Desktop, Microsoft Excel, NLP, Deep learning frameworks such as TensorFLow, Keras, Boosting algorithms etc"", u""Data Scientist\nMercedes Benz Financial Services, Michigan\nJanuary 2016 to September 2017\nDescription: Mercedes-Benz Financial Services is a leading, captive financial services provider and the global financial services company of Daimler AG. Doing business as Mercedes-Benz Financial Services and Daimler Truck Financial, we provide financing for automotive and commercial vehicle dealers and their retail consumers in the United States, Canada, Mexico, Brazil and Argentina.\n\nResponsibilities:\n\u2022 Performed Data Profiling to learn about behavior with various features such as traffic pattern, location, time, Date and Time etc.\n\u2022 Application of various Artificial Intelligence(AI)/machine learning algorithms and statistical modeling like decision trees, text analytics, natural language processing(NLP), supervised and unsupervised, regression models, social network analysis, neural networks, deep learning, SVM, clustering to identify Volume using scikit-learn package in python, Matlab.\n\u2022 Utilized Spark, Snowflake, Scala, Hadoop, HQL, VQL, oozie, pySpark, Data Lake, TensorFlow, HBase, Cassandra, Redshift, MongoDB, Kafka, Kinesis, Spark Streaming, Edward, CUDA, MLLib, AWS, Python, a broad variety of machine learning methods including classifications, regressions, dimensionally reduction etc. and Utilized the engine to increase user lifetime by 45% and triple user conversations for target categories.\n\u2022 Created and connected SQL engine through C# to connect database, developed API libraries and business logic using C#, XML and Python\n\u2022 Exploring DAG's, their dependencies and logs using AirFlow pipelines for automation\n\u2022 Performed data cleaning and feature selection using MLlib package in PySpark and working with deep learning frameworks such as Caffe, Neon etc\n\u2022 Developed Spark/Scala, Python, R for regular expression (regex) project in the Hadoop/Hive environment with Linux/Windows for big data resources. Used clustering technique K-Means to identify outliers and to classify unlabeled data.\n\u2022 Created user friendly interface for quick view of reports by using C#, JSP, XML and developed expandable menu that show drilldown data on graph click\n\u2022 Evaluated models using Cross Validation, Log loss function, ROC curves and used AUC for feature selection and elastic technologies like ElasticSearch, Kibana etc\n\u2022 Categorised comments into positive and negative clusters from different social networking sites using Sentiment Analysis and Text Analytics\n\u2022 Tracking operations using sensors until certain criteria is met using AirFlow technology.\n\u2022 Responsible for different Data mapping activities from Source systems to Teradata using utilities like TPump, FEXP, MLOAD, BTEQ, FLOAD etc\n\u2022 Analyze traffic patterns by calculating autocorrelation with different time lags.\n\u2022 Ensured that the model has low False Positive Rate and Text classification and sentiment analysis for unstructured and semi-structured data.\n\u2022 Addressed overfitting by implementing of the algorithm regularization methods like L2 and L1.\n\u2022 Used Principal Component Analysis in feature engineering to analyze high dimensional data.\n\u2022 Created and designed reports that will use gathered metrics to infer and draw logical conclusions of past and future behavior.\n\u2022 Performed Multinomial Logistic Regression, Random forest, Decision Tree, SVM to classify package is going to deliver on time for the new route.\n\u2022 Performed data analysis by using Hive to retrieve the data from Hadoop cluster, Sql to retrieve data from Oracle database and used ETL for data transformation.\n\u2022 Used MLlib, Spark's Machine learning library to build and evaluate different models.\n\u2022 Implemented rule based expertise system from the results of exploratory analysis and information gathered from the people from different departments.\n\u2022 Performed Data Cleaning, features scaling, features engineering using pandas and numpy packages in python and build models using SAP Predictive Analytics.\n\u2022 Developed MapReduce pipeline for feature extraction using Hive and Pig.\n\u2022 Created Data Quality Scripts using SQL and Hive to validate successful data load and quality of the data. Created various types of data visualizations using Python and Tableau/Spotfire.\n\u2022 Communicated the results with operations team for taking best decisions.\n\u2022 Collected data needs and requirements by Interacting with the other departments.\n\nEnvironment: Python 2.x, CDH5, HDFS, C#, Hadoop 2.3, Hive, Impala, AWS, Linux, Spark, Tableau Desktop, SQL Server 2012, Microsoft Excel, Matlab, Spark SQL, Pyspark."", u""Data Scientist\nFirst Data - Atlanta, GA\nJanuary 2015 to December 2015\nDescription: First Data Corporation is a global payment processing company headquartered in Atlanta, Georgia, United States. The company's portfolio includes merchant transaction processing services; credit, debit, private-label, gift, payroll and other prepaid card offerings; fraud protection and authentication solutions.\nResponsibilities:\n\n\u2022 Provided Configuration Management and Build support for more than 5 different applications, built and deployed to the production and lower environments.\n\u2022 Implemented public segmentation using unsupervised machine learning algorithms by implementing k-means algorithm using Pyspark.\n\u2022 Using AirFlow to keep track of job statuses in repositories like MySQl and Postgre databases.\n\u2022 Explored and Extracted data from source XML in HDFS, used ETL for preparing data for exploratory analysis using data munging.\n\u2022 Responsible for different Data mapping activities from Source systems to Teradata, Text mining and building models using topic analysis, sentiment analysis for both semi-structured and unstructured data.\n\u2022 Handled importing data from various data sources, performed transformations using Hive, Map Reduce, and loaded data into HDFS\n\u2022 Used R and python for Exploratory Data Analysis, A/B testing, HQL, VQL, Data Lake, AWS Redshift, oozie, pySpark, Anova test and Hypothesis test to compare and identify the effectiveness of Creative Campaigns.\n\u2022 Computing A/B testing frameworks, clickstream and time spent databases using Airflow\n\u2022 Created clusters to Control and test groups and conducted group campaigns using Text Analytics.\n\u2022 Created positive and negative clusters from merchant's transaction using Sentiment Analysis to test the authenticity of transactions and resolve any chargebacks.\n\u2022 Analyzed and calculated the lifetime cost of everyone in the welfare system using 20 years of historical data.\n\u2022 Created and developed classes and web page elements using C# and AJAX. JSP was used for validating client side responses and connected C# to database to retrieve SQL data\n\u2022 Developed LINUXShell scripts by using NZSQL/NZLOAD utilities to load data from flat files to Netezza database.\n\u2022 Developed triggers, stored procedures, functions and packages using cursors and ref cursor concepts associated with the project using Pl/SQL\n\u2022 Created various types of data visualizations using R, C#, python and Tableau/Spotfire also connected Pipeline Pilot with Spotfire to create more interactive business driven layouts.\n\u2022 Used Python, R, SQL to create Statistical algorithms involving Multivariate Regression, Linear Regression, Logistic Regression, PCA, Random forest models, Decision trees, Support Vector Machine for estimating the risks of welfare dependency.\n\u2022 Identified and targeted welfare high-risk groups with Machine learning/deep learning algorithms.\n\u2022 Conducted campaigns and run real-time trials to determine what works fast and track the impact of different initiatives.\n\u2022 Developed Tableau visualizations and dashboards using Tableau Desktop.\n\u2022 Used Graphical Entity-Relationship Diagramming to create new database design via easy to use, graphical interface.\n\u2022 Created multiple custom SQL queries in Teradata SQL Workbench to prepare the right data sets for Tableau dashboards\n\u2022 Perform analyses such as regression analysis, logistic regression, discriminant analysis, cluster analysis using SAS programming.\n\nEnvironment: R 3.x, HDFS, C#, Hadoop 2.3, Pig, Hive, Linux, R-Studio, Tableau 10, SQL Server, Ms Excel, Pypark."", u""Data Scientist\nTripAdvisor - New York, NY\nMay 2013 to December 2014\nDescription: TripAdvisor, Inc. is an American travel website company providing reviews of travel-related content. It also includes interactive travel forums. TripAdvisor was an early adopter of user-generated content. The website services are free to users, who provide most of the content, and the website is supported by an advertising business model.\n\nResponsibilities:\n\u2022 Involved in Design, Development and Support phases of Software Development Life Cycle (SDLC)\n\u2022 Performed data ETL by collecting, exporting, merging and massaging data from multiple sources and platforms including SSRS/SSIS (SQL Server Integration Services) in SQL Server.\n\u2022 Programming experience with .NET framework, C#, Visual Studio 2005/2008 to build web based, client/server architecture and to produce reports with C# and JSP.\n\u2022 Worked with cross-functional teams (including data engineer team) to extract data and rapidly execute from MongoDB through MongDB connector for Hadoop.\n\u2022 Performed data cleaning and feature selection using MLlib package in PySpark.\n\u2022 Performed partitional clustering into 100 by k-means clustering using Scikit-learn package in Python where similar hotels for a search are grouped together.\n\u2022 Used Python to perform ANOVA test to analyze the differences among hotel clusters.\n\u2022 Implemented application of various machine learning algorithms and statistical modeling like Decision Tree, Text Analytics, Sentiment Analysis, Naive Bayes, Logistic Regression and Linear Regression using Python to determine the accuracy rate of each model.\n\u2022 Determined the most accurately prediction model based on the accuracy rate.\n\u2022 Used text-mining process of reviews to determine customers' concentrations.\n\u2022 Delivered analysis support to hotel recommendation and providing an online A/B test.\n\u2022 Designed Tableau bar graphs, scattered plots, and geographical maps to create detailed level summary reports and dashboards.\n\u2022 Developed hybrid model to improve the accuracy rate.\n\nEnvironment: Python, PySpark, C#, Tableau, MongoDB, Hadoop, SQL Server, SDLC, ETL, SSIS, recommendation systems, Machine Learning Algorithms, text-mining process, A/B test"", u""Data Scientist\nBank of America - Wilmington, DE\nAugust 2012 to April 2013\nDescription: Bank of America is a multinational banking and financial services corporation. It is ranked 2nd on the list of largest banks in the United States by assets. As of 2016, Bank of America was the 26th largest company in the United States by total revenue.\n\nResponsibilities:\n\u2022 Participated in all phases of research including data collection, data cleaning, data mining, developing models and visualizations.\n\u2022 Collaborated with data engineers and operation team to collect data from internal system to fit the analytical requirements.\n\u2022 Redefined many attributes and relationships and cleansed unwanted tables/columns using SQL queries.\n\u2022 Utilized Spark SQL API in PySpark to extract and load data and perform SQL queries and also used C# connector to perform SQL queries by creating and connecting to SQL engine.\n\u2022 Performed data imputation using Scikit-learn package in Python.\n\u2022 Performed data processing using Python libraries like Numpy and Pandas.\n\u2022 Worked with data analysis using ggplot2 library in R to do data visualizations for better understanding of customers' behaviors.\n\u2022 Implemented statistical modeling with XGBoost machine learning software package using R to determine the predicted probabilities of each model.\n\u2022 Delivered the results with operation team for better decisions.\n\nEnvironment: Python, R, SQL, Tableau, Spark, Machine Learning Software Package, recommendation systems."", u""Python Developer\nCenvien Technologies - Hyderabad, Telangana\nJanuary 2011 to July 2012\nDescription: Cenvien technologies gather the requirements by listening and understanding to the client's business requirement to deliver quality products. It is highly qualified and strongly dedicated developing team that produces unique solutions.\n\nResponsibilities:\n\u2022 Developed entire frontend and backend modules using Python on Django Web Framework.\n\u2022 Implemented the presentation layer with HTML, CSS and JavaScript.\n\u2022 Involved in writing stored procedures using Oracle.\n\u2022 Optimized the database queries to improve the performance.\n\u2022 Designed and developed data management system using Oracle.\n\nEnvironment: MySQL, ORACLE, HTML5, CSS3, JavaScript, Shell, Linux & Windows, Django, Python"", u'Programmer Analyst\nPennar Industries Limited - Hyderabad, Telangana\nMarch 2009 to December 2010\nDescription: As a backend developer of web applications and data science infrastructure. The main area of focus is to come up with comprehensive solutions that need massive capacity and throughput.\n\nResponsibilities:\n\u2022 Effectively communicated with the stakeholders to gather requirements for different projects\n\u2022 Used MySQL db package and Python-MySQL connector for writing and executing several MYSQL database queries from Python.\n\u2022 Implemented Client/Server applications using C#, JSP and SQL\n\u2022 Created functions, triggers, views and stored procedures using My SQL.\n\u2022 Worked closely with back-end developer to find ways to push the limits of existing Web technology.\n\u2022 Involved in the code review meetings.\n\nEnvironment: Python, MySQL, C#.']",[u'Bachelor of Computer Science in TOOLS AND TECHNOLOGIES'],[u'Informatica Power Centre'],degree_1 : Bachelor of Compter Science in TOOLS AND TECHNOLOGIES
0,https://resumes.indeed.com/resume/f9ba48b27e8817a6,"[u'Consultant Data Modeller\nHCL America and MIT - CSAIL(Contract)\nJanuary 2017 to Present\n\u2022 Working a technical adviser managing a team of 10 developers and an MSc students at MIT in a project that involves Document Image Analysis\n\u2022 Developing Deep Learning based approaches (AlexNet, VGG16) to document type classification utilizing tensorflow and bidirectional LSTM NNs\n\u2022 Utilizing various machine learning algorithms (SVM, FCM, RCF) for document layout classification\n\u2022 OpenNLP via Python (NLTK and NERTagger) for information extraction\n\u2022 Developing novel IE strategies using fusion methods for IE and document analysis using natural language and related approaches', u'Senior Data Scientist/Neuromarketing\nViralGains, Inc\nMarch 2016 to October 2016\n\u2022 Performed predictive and descriptive analytics in a real - time - bidding context for digital adware.\n\u2022 Worked on developing a reinforcement learning based approach to automate bidding, along with sentiment analysis, engagement analysis, real - time bidding (RTB) cost optimization, and digital video delivery optimization.\n\u2022 Developed neuromarketing strategies for digital adtech video delivery based on the BIG5 model, brand labeling models, external cookie data, natural language processing, visual analytics and other data based resources\n\u2022 Worked in Agile with two week sprints, focusing on dev using R and Python\n\u2022 The data resided on S3 for temporary storage, brought the data in to a Hadoop cluster (five worker nodes currently), and processed the data within a Spark - ML/scala and Java environment.\n\u2022 Designed, implemented and tested large scale (TBs of data) datasets from a series of over 20 data tables, acquired from the RTB server in real - time, providing auction decision making logic based on machine learning, within a 50 ms time scale.\n\u2022 Derived contextual information from landing pages and host URLs that were mapped onto a set of video inventory, with the goal of enhancing user engagement through mapping videos to pages, with some information available from cookies/Pixels for custom audience targeting modeling.', u'Data Scientist\nBayesian Networks\nMay 2015 to December 2015\n\u2022 Served as part of Predictive modeling group; analyzed data for the mobile phone industry.\n\u2022 Developed descriptive analytical models such as reinforcement learning, deep learning, HMMs, SVMs, Bayesian Networks, CART/random forests, and a variety of other ML based approaches for descriptive analytics such as reason for data/line drop occurrences, role of signal strength on UX, update statistics for KPIs, cross - dimensional analysis of a variety of indicators such as geobin location and QoS (coded in R and Python).\n\u2022 Integrated machine learning algorithms for KPIs into the software platform (coded in Java/Eclipse/Maven/P4).\n\u2022 Developed predictive analytical models to provide seasonality models, churn models, recommender data, battery life predictability, and likelihood of switching models.', u'Development Editor (Fixed term assignment)\nPearson Education\nJanuary 2015 to August 2015\n- work from home assignment\n\u2022 Served as the development editor for a Pearson contract for the development of instructional materials for teachers of advanced High School mathematics.\n\u2022 Supervised a small group (5) of technical writers.\n\u2022 Provided training materials for a set of international teachers that are contracted to teach overseas.\n\u2022 Responsible for technical writing (1, 000+ pages), providing the base learning materials, reference PPT decks, and off - line training materials.', u'Cognitive Scientist/Subject matter expert\nCarnegie Learning, Inc - Pittsburgh, PA\nNovember 2014 to April 2015\nUX data analytics Consultation\n\u2022 Developed cognitive models for Carnegie Learning on - line instructional materials for the University of Phoenix.\n\u2022 Implemented Section 508 compliance into their touch based (tablets and smart phones) UIs for their educational products.\n\u2022 Involved in developing problem sets, documentation, and related promotional material for their on - line educational products.\n\u2022 Acquired and analyzed testing data from a cohort of 1, 000 students in order to determine whether the modifications to the UI were effective.\n\u2022 Responsible for Statistical modeling using SAS and/or R for descriptive analytics and student responses - ANOVA, MANOVAS, and descriptive statistics.\n\u2022 Utilized clickstream tracking to evaluate the UI from an efficiency and UX perspective.\n\u2022 Developed cognitive models for the CarnegieLearning suite of online learning educational material.\n\u2022 Utilized a suite of tools, most of which were custom built, to implement the learning and skill acquisition components to educational material geared at the advanced high school and University of Phoenix remedial math and science programs.\n\u2022 Developed a suite of models that were designed to test for and deliver suitable materials for students with specific types of learning issues, such as dyslexia.', u'R & D Director/Senior Scientist\nRKI Systems Ltd - London\nAugust 2013 to August 2014\n\u2022 Data mined large volume foreign exchange datasets (technical data) using custom developed machine learning algorithms written in R, C++ and RHadoop.\n\u2022 Implemented neural networks, genetic algorithms, artificial immune systems, learning classifier systems, liquid state machines/echo state neural networks, clustering algorithms, random forests, change point management, and deep learning, among other machine learning algorithms.\n\u2022 Deployed a variety of classical financial modeling and statistical modeling (SAS based) processes such as: VARIMA models, eGARCH type models, along with more classical MCMC models.\n\u2022 Integrated natural language processing for the automated acquisition of external announcements which may affect trading behavior patterns.\n\u2022 Implemented time series predictive analytics which incorporated both technical and fundamental data into a coherent financial modeling back - end system for an on - line FX trading system.', u'Associate Professor and Research Director\nLoughborough University\nSeptember 2010 to June 2013\n\u2022 Implemented robotics programming laboratory, where we implemented EKF, particle filters for automated robotic landmark utilization and navigation.\n\u2022 Responsible for acquisition of ECG/EEG/GSR/EMG data for person identification/authentication.\n\u2022 Developed applications that utilized the affective state of users (happy, sad, frustrated, bored, disgusted, and angry) to dynamically adjusting user interfaces and provide contextual online assistance using ECG, heart rate variability, and galvanic skin response.\n\u2022 Analyzed large EEG datasets in order to determine differences between individuals.\n\u2022 Established an MSc program in health informatics.\n\u2022 Developed a cognitive robotics lab - which emphasized AI and psychophysiology for robotic control.\n\u2022 Installed and curated the only Middle Eastern MGED gene expression data repository.\n\u2022 Established a cloud based personalized medicine system.\n\u2022 Developed an MRI based data analysis system, utilizing the MNI BIC dataset production tools.\n\u2022 Implemented an online weight control application (apk).', u'Senior Lecturer\nUniversity of Westminster, School of Electronics & Computer Science\nSeptember 2001 to June 2010\n\u2022 Established an MSc in Neuroscience, Bioinformatics, and Biometrics.\n\u2022 Implemented a remote access Lego Mindstorms robotic control system, which allowed users to issue control commands via the internet, using a web - camera and some minor controls.\n\u2022 Developed novel machine learning approaches to mining EEG datasets.\n\u2022 Developed data conversion protocols for converting proprietary MRI data sources into DICOM compliant and HLA7 CDA R2 format, in order to facilitate the merging of a variety of clinical data into standardized EHR formats, for ultimate storage on secure data serves.\n\u2022 Utilized rule based machine learning algorithms for extraction of clinically relevant data from biomedical datasets.\n\u2022 Web scraped for the extraction of scientific data using customized ontologies, pattern matching, and natural language processing.\n\u2022 Developed and patented a keystroke dynamics based user authentication scheme (joint Patent with University of Westminster, UK).\n\u2022 Developed applications (Windows based, C++/C#) for face recognition and motion capture technologies for biometrics and conative computing.\n\u2022 Published 3 books - including the first textbook of behavioral biometrics (Wiley and Sons in 2008) along with 170+ scientific publications\n\u2022 Researched enhanced computer security utilizing not only keystroke dynamics, but mouse - stroke dynamics, pupillometry, and biosignals generally.']","[u'Ph.D. in Computational Neuroscience', u'B.Sc. in Biochemistry', u'B.Sc. in Neurophysiology', u'B.Sc. in Computer Science']","[u'University of Maryland, College Park College Park, MD', u'University of Maryland, College Park College Park, MD', u'University of Maryland, College Park College Park, MD', u'University of Maryland Adelphi, MD']","degree_1 : Ph.D. in Comptational Neroscience, degree_2 :  B.Sc. in Biochemistry, degree_3 :  B.Sc. in Nerophysiology, degree_4 :  B.Sc. in Compter Science"
0,https://resumes.indeed.com/resume/cf1ca000e34f17f2,"[u'Actuarial Assistant\nAIG - GA\nMarch 2017 to Present\n\u2022 Conduct profitability studies and other analysis for various North American Financial Lines business segments and present results of analyses to senior managers and business decision makers\n\u2022 Assist with identifying and developing advanced techniques (GLM) for risk segmentation and pricing for D&O business\n\u2022 Collaborate with other colleagues and partner with other department to build internal central database\n\u2022 Develop actuarial reports to help business effectively manage existing portfolios', u""Data Scientist Intern\nInsight Sourcing Group - GA\nMay 2015 to December 2016\nEnterprise Energy Solution\n\u2022 Analyzed regulated energy market across industries and utility vendors. Recommended saving plan to clients by optimizing\nutility rates and tariffs\n\u2022 Developed and quantified risk hedging model for internal analysis by estimating natural gas futures prices and simulating\nsettlement prices\nSpendHQ\n\u2022 Assisted with 15 clients' regular data refresh and normalized and categorized company spending on daily basis\n\u2022 Conducted client portfolio analysis based on client geography, industry type, and revenue size (Presented insight findings to senior associates and administrators)"", u'Graduate Research Assistant\nGeorgia State University - GA\nSeptember 2014 to December 2016\n\u2022 Collaborated with 10 other students and performed predictive modeling to predict hotel cancellation rate\n\u2022 Managed projects for the analytic institute including internal announcements, weekly meetings, and external company\nprojected projects\n\u2022 Assisted with psychology department and conducted pre-school learning curve analysis']","[u'Master of Science in Risk Management & Insurance', u'Bachelor of Science in Business Administration']","[u'Georgia State University Atlanta, GA\nAugust 2014 to December 2016', u'Drake University Des Moines, IA\nAugust 2010 to May 2013']","degree_1 : Master of Science in Risk Management & Insrance, degree_2 :  Bachelor of Science in Bsiness Administration"
0,https://resumes.indeed.com/resume/63596d992f8a33ff,"[u'Principal Data Scientist\nEdison Energy - Boston, MA\nMarch 2016 to Present\nLead team of six data scientists & developers responsible for research, design, and implementation of derivatives pricing and price simulation algorithms, focused on renewable energy, power, and natural gas markets.\nDesigned, developed, and deployed portfolio management analytical product offering, booking $1.7 million in sales to Fortune 100 clients in first two years of product rollout.\nCoinventor on filed patent \u201dSystems and methods for energy management\u201d', u'Data Engineer\nAltenex - Boston, MA\nApril 2015 to March 2016\nDeveloped methodology and software implementation of long-term stochastic power system scenario simulation.\nDeveloped and deployed highly parallel and distributed implementation of price risk engine to AWS\nLead analytics for over 1GW of constructed on-site and off-site renewable energy projects, yielding millions of dollars in savings to clients.\nResearched and implemented copula-based methodology for risk-adverse renewable energy system portfolio optimization.', u'Senior Consultant\nNavigant - Washington, DC\nFebruary 2014 to March 2015\nWorked with the Global Energy Division in the Market Intelligence group. Focused on Transmission Modeling and Planning, Energy Forecasting, proprietary model development, and Environmental Policy. Sample engagements include:\n\nResearch and preparation of transmission components of present and forecasted North American energy market reference case.\n\nQuantified the Transmission System effects of distributed energy resource integration for IOU in FRCC and WECC.\n\nConducted a deliverability analysis using PSSE for an IOU in PJM to determine thermal overloads under N-1 contingency scenarios.\n\nCreation of a web data extraction tool to monitor updates to critical power systems infrastructure and market frameworks.\n\nDevelopment of algorithms to check integrity and validity of Bulk Electric System models.\n\nAuthored portions of environmental policy section in market forecast report.\n\nConducted transmission expansion planning analyses for an IOU in SPP through single and double contingency simulations to identify most at-risk corridors, recommended options to satisfy NERC reliability requirements.\n\nFull development of web scraping platform and web application for sales leads generation using Openshift, Python, Flask, and Import.io']","[u""Master's in Power Systems Engineering & Optimization"", u'BS in Environmental Engineering']","[u'Cornell University Ithaca, NY\nJanuary 2014 to May 2014', u'Cornell University Ithaca, NY\nAugust 2009 to May 2013']","degree_1 : ""Masters in Power Systems Engineering & Optimization"", degree_2 :  BS in Environmental Engineering"
0,https://resumes.indeed.com/resume/77a0bcc3e5c982d8,"[u'Sr. Data Analyst\nCeres Environmental Services, Inc - Sarasota, FL\nFebruary 2018 to Present\nUnderstand data (both constraints & abilities), data requirements, and data merging for purposes of completing analyses that will create\nactionable insights; create and communicate compelling recommendation based on these insights, that will ultimately drive operational\nefficiency and an improved customer experience.\n\u2022 Daily data uploads from various monitor sites into ticket tracker.\n\u2022 Check source documents against entered data to ensure data integrity at every stage and assist in developing and maintaining improved\nrecords within the database system.\n\u2022 Generating statistical reports based on maintained data on a periodic basis.', u'Data Scientist\nFCCI Insurance - Sarasota, FL\nMay 2017 to November 2017\nDevelop predictive models to determine a consumers propensity to commit an action towards FCCI products through the various offline\nand online channels using linear and logistic regression in R, SAS.\n\u2022 Experience in designing rich data visualization to communicate to customers or management.\n\u2022 Handle large amount of datasets, build classification models for predictions & forecasting based on the training data using Machine\nlearning (Random Forest, Regression, Clustering).\n\u2022 Experience in dealing with Apache Hadoop component like HDFS, MapReduce and Hive', u""Business Intelligence Analyst\nLarsen & Toubro Infotech Ltd\nNovember 2013 to June 2016\nOver 3 years of solid SAS programming experience in developing applications, data warehousing, statistical analysis and report\ngeneration using SAS 9.x and SAS 8.x.\n\u2022 Experience in developing and debugging SAS/MACROS to extract, modify, merge and analyze data to generate statistical analysis\noutputs as well as controlled batch jobs.\n\u2022 Built regression models (including generalized linear models and gradient boosting machines) to predict risk using driving\ncharacteristics and behaviors in support of usage-based insurance\n\u2022 Applied technical skills using SAS, Tableau and SQL in data collection, data analysis and reporting to procure data from database\nstructures to report and provide solutions to client requests in a timely manner.\n\u2022 Expertise in Informatica ETL and reporting tools. Deep understanding of the Data Warehousing SDLC and architecture of ETL,\nreporting and BI tools\n\u2022 Worked in the Insurance field gathered and analyzed business requirements and produced specification documents for new project\nrequests. Developed ad-hoc queries and reports for clients and provide analytical support and recommendation as necessary. Acted as liaison between business unit and report developer to develop, communicate and modify existing report\n\u2022 Applied technical skills using SAS, Tableau and SQL in data collection, data analysis and reporting to procure data from database\nstructures to report and provide solutions to client requests in a timely manner\n\nINOLVEMENT, VOLUNTEERING WORK\n\u2022 Active participation in company's social activities: Teaching at schools in rural areas, Blood donation camps.\n\u2022 Hosted the L&T's Annual Technical Service Group (TSG) Day.""]","[u'Master of Science in Management Information System in Management Information System', u'Bachelor of Technology in Electronics and Communication']","[u'Muma School of Business, University of South Florida Tampa, FL\nAugust 2016 to December 2017', u""Lingaya's University Faridabad, Haryana\nAugust 2009 to May 2013""]","degree_1 : Master of Science in Management Information System in Management Information System, degree_2 :  Bachelor of Technology in Electronics and Commnication"
0,https://resumes.indeed.com/resume/9e697ff517500853,"[u'Data Scientist Intern\nSri SRM Fabricators - Chennai, Tamil Nadu\nApril 2017 to July 2017\nIndia.\n\u2022 Used Machine Learning algorithms to determine whether a customer buys or not and a prediction accuracy of 84% was\nobtained.\n\u2022 Developed and implemented data collection systems and analytics technique to optimize the statistical efficiency and designed experiments to draw actionable conclusions.\n\u2022 Worked cross-functionally with other data scientists to monitor the prediction efficiency and validate the effectiveness.', u'Data Analyst Intern\nAlbonair India Pvt Ltd - Chennai, Tamil Nadu\nAugust 2016 to September 2016\nIndia.\n\u2022 Cleaned data in SQL, created reports using Excel to measure the ROI and worked closely with the management to prioritize\nbusiness needs.\n\u2022 Participated in team projects to maintain the process and extracted insights by visualizing the data using Tableau.\n\u2022 Performed data validation to ensure reliability and carried out SWOT analyses to perform self-evaluation\n\u2022 Developed queries based on business requests using MySQL and PostgreSQL.', u'Data Analyst and Marketing Lead\nVelammal Engineering College - Chennai, Tamil Nadu\nFebruary 2016 to May 2016\nIndia\n\u2022 Obtained data on the history of student enrollment in previous years and performed data cleaning using SQL(PostgreSQL)\nto find the potential location of student participation. Facebook Ad Campaigning was used to target those students.\n\u2022 Assisted in creating reports to track and analyze the success of the promo. Produced data strategies and made\nrecommendations regarding the market, competitors and current market place.']","[u'Master of Science in Industrial Engineering in Industrial Engineering', u'Bachelor of Engineering in Mechanical Engineering in Statistics and Numerical Methods']","[u'ARIZONA STATE UNIVERSITY\nAugust 2017 to Present', u'VELAMMAL ENGINEERING COLLEGE, ANNA UNIVERSITY Chennai, Tamil Nadu\nMay 2017']","degree_1 : Master of Science in Indstrial Engineering in Indstrial Engineering, degree_2 :  Bachelor of Engineering in Mechanical Engineering in Statistics and Nmerical Methods"
0,https://resumes.indeed.com/resume/ee83ba0c546d52ea,"[u'Data Scientist/Machine Learning\nEquifax - Atlanta, GA\nFebruary 2017 to Present\nDescription: Equifax is a global information solutions company that uses unique data, innovative analytics, technology and industry expertise to power organizations and individuals around the world by transforming knowledge into insights that help make more informed business and personal decisions. Equifax operates or has investments in 24 countries in North America, Central and South America, Europe and the Asia Pacific region.\nResponsibilities:\n\u2022 A highly immersive DataScience program involving DataManipulation&Visualization, Web Scraping, MachineLearning, Python programming, SQL, GIT, Unix Commands, NoSQL, MongoDB, Hadoop.\n\u2022 Setup storage and dataanalysis tools in AmazonWebServices cloud computing infrastructure.\n\u2022 Used pandas, numpy, seaborn, scipy, matplotlib, scikit-learn, NLTK in Python for developing various machinelearningalgorithms.\n\u2022 Installed and used CaffeDeepLearningFramework\n\u2022 Worked on different data formats such as JSON, XML and performed machinelearningalgorithms in Python.\n\u2022 Developing Voice Bot using AI (IVR ), improving the interaction between Human and the Virtual Assistant\n\u2022 Development and Deployment using Google Dialogflow Enterprise.\n\u2022 Worked as Data Architects and IT Architects to understand the movement of data and its storage and ERStudio9.7\n\u2022 Participated in all phases of datamining; datacollection, datacleaning, developingmodels, validation, visualization and performed Gapanalysis.\n\u2022 DataManipulation and Aggregation from different source using Nexus, Toad, BusinessObjects, PowerBI and SmartView.\n\u2022 Implemented Agile Methodology for building an internal application.\n\u2022 Focus on integration overlap and Informatica newer commitment to MDM with the acquisition of Identity Systems.\n\u2022 Rapid model creation in Python using pandas, numpy, sklearn, and plot.ly for data visualization. These models are then implemented in SAS where they are interfaced with MSSQL databases and scheduled to update on a timely basis.\n\u2022 Data analysis using regressions, data cleaning, excel v-look up, histograms and TOAD client and data representation of the analysis and suggested solutions for investors\n\u2022 Attained good knowledge in Hadoop Data Lake Implementation and HADOOP Architecture for client business data management.\n\u2022 Extracted data from HDFS and prepared data for exploratory analysis using datamunging.\nEnvironment:ER Studio 9.7, Tableau 9.03, AWS, Teradata 15, MDM, GIT, Unix, Python 3.5.2, , MLLib, SAS, regression, logistic regression, Hadoop, NoSQL, Teradata, OLTP, random forest, OLAP, HDFS, ODS, NLTK, SVM, JSON, XML, MapReduce, Google Dialog Flow', u""Data Scientist\nJohnson &Johnson - Raritan, NJ\nDecember 2015 to January 2017\nDescription: Johnson & Johnson Helps Keep the Littlest Patients Safe Around the World \u2022 Read the Story. The Johnson & Johnson Institute: 9 Facts about a Program That Trains 125,000 Healthcare Providers Each Year\nResponsibilities:\n\u2022 Worked with several R packages including knitr, dplyr, SparkR, CausalInfer, spacetime.\n\u2022 Implemented end-to-end systems for DataAnalytics, DataAutomation and integrated with custom visualization tools using R,Mahout, Hadoop and MongoDB.\n\u2022 Gathering all the data that is required from multiple data sources and creating datasets that will be used in analysis.\n\u2022 Performed Exploratory DataAnalysis and DataVisualizations using R, and Tableau.\n\u2022 Performa proper EDA, Univariate and bi-variate analysis to understand the intrinsic effect/combined effects.\n\u2022 Worked with Data governance, Data quality, data lineage, Data architect to design various models and processes.\n\u2022 Independently coded new programs and designed Tables to load and test the program effectively for the given POC's using with Big Data/Hadoop.\n\u2022 Designed data models and data flow diagrams using Erwin and MSVisio.\n\u2022 Research on improving IVR used internally in J&J.\n\u2022 Developing IVRFor clinics so that the callers can receive anonymous access to test results.\n\u2022 Performed datacleaning and imputation of missing values using R.\n\u2022 Worked with Hadoop eco system covering HDFS, HBase, YARN and MapReduce\n\u2022 Take up ad-hoc requests based on different departments and locations.\n\u2022 Determined regression model predictors using Correlation matrix for Factor analysis in R\n\u2022 Built Regression model to understand order fulfillment time lag issue using Scikit-learn in Python\n\u2022 Utilized Spark, Scala, Hadoop, HBase, Kafka, Spark Streaming, MLlib, Python, a broad variety of machine learning methods including classifications, regressions, dimensionally reduction etc\n\u2022 Empowered decision makers with data analysis dashboards using Tableau and Power BI\n\u2022 Interface with other technology teams to extract, transform, and load (ETL) data from a wide variety of data sources\n\u2022 Own the functional and non-functional scaling of software systems in your ownership area.\n\u2022 Provides input and recommendations on technical issues to BIEngineers, Business&DataAnalysts and Data Scientists.\n\u2022 As an Architect implemented MDM hub to provide clean, consistent data for a SOA implementation.\n\u2022 Developed, Implemented & Maintained the Conceptual, Logical&Physical Data Models using Erwin for Forward/ReverseEngineered Databases.\n\u2022 Established Data architecture strategy, best practices, standards, and roadmaps.\nLead the development and presentation of a dataanalytics data-hub prototype with the help of the other members of the emerging solutions team.\n\nEnvironment:R 3.0, Erwin 9.5, Tableau 8.0, MDM, QlikView, MLLib, PL/SQL, HDFS, Teradata 14.1, JSON, HADOOP (HDFS), MapReduce, PIG, Spark, R Studio, MAHOUT, JAVA, HIVE, AWS."", u'Data Scientist/Machine Learning\nDPSG - Plano, TX\nFebruary 2014 to November 2015\nDescription: Dr Pepper Snapple Group Inc. is an American soft drink company, based in Plano, Texas. Formerly called Cadbury Schweppes Americas Beverages,\n\nResponsibilities:\n\u2022 Involved in defining the source to target data mappings, business rules, and data definitions.\n\u2022 Performing data profiling on various source systems that are required for transferring data to ECH using\n\u2022 Defining the list codes and code conversions between the source systems and the data mart using Reference Data Management (RDM).\n\u2022 Utilizing Informatica toolset (InformaticaData Explorer, and Informatica Data Quality) to analyze legacy data for Data Profiling.\n\u2022 Worked on DTS Packages, DTS Import/Export for transferring data between SQL Server 2000 to 2005\n\u2022 Involved in upgrading DTS packages to SSIS packages (ETL).\n\u2022 Performing an end to end InformaticaETL Testing for these custom tables by writing complex SQL Queries on the source database and comparing the results against the target database.\n\u2022 Using HP Quality Center v 11 for defect tracking of issues.\n\u2022 Expertise in applying data mining techniques and optimization techniques in B2B and B2C industries and proficient in Machine Learning, Data/Text Mining, Statistical Analysis and Predictive Modeling.\n\u2022 Extracting the source data from Oracle tables, MS SQL Server, sequential files and excel sheets.\n\u2022 Developing and maintaining Data Dictionary to create metadata reports for technical and business purpose.\n\u2022 Predictive modeling using state-of-the-art methods\n\u2022 Build and maintain dashboard and reporting based on the statistical models to identify and track key metrics and risk indicators.\n\u2022 Developed MapReduce/Spark Python modules for machine learning & predictive analytics in Hadoop on AWS.\n\u2022 Parse and manipulate raw, complex data streams to prepare for loading into an analytical tool.\n\u2022 Migrating Informatica mappings from SQL Server to Netezza Foster culture of continuous engineering improvement through mentoring, feedback, and metrics\n\u2022 Broad knowledge of programming, and scripting (especially in R / Java / Python)\n\u2022 Implemented Event Task for execute Application Automatically.\n\u2022 Involved in developing Patches & Updates Module.\n\u2022 Proven experience building sustainable and trustful relationships with senior leaders.\n\nEnvironment:Erwin 8, Teradata 13, SQL Server 2008, Oracle 9i, SQL*Loader, PL/SQL, ODS, OLAP, OLTP, SSAS, Informatica Power Center 8.1.', u'BI Developer/Data Analyst\nCitrix Systems - Fort Lauderdale, FL\nNovember 2012 to January 2014\nResponsibilities:Citrix Systems, Inc. is an American multinational software company that provides server, application and desktop virtualization, networking, software as a service, and cloud computing technologies.\nDescription: Dr Pepper Snapple Group Inc. is an American soft drink company, based in Plano, Texas. Formerly called Cadbury Schweppes Americas Beverages,\n\u2022 Involved in detail designing of data marts by using Star Schema and plan data marts involving shared dimensions.\n\u2022 Conducted one-to-one sessions with business users to gather data for Data Warehouse requirements.\n\u2022 Part of team analyzing database requirements in detail with the project stakeholders through Joint Requirements Development (JRD) sessions.\n\u2022 Developed an Object modeling in UML for Conceptual Data Model using Enterprise Architect.\n\u2022 Developed logical and Physical data models using Erwin to design OLTP system for different applications.\n\u2022 Facilitated transition of logical data models into the physical database design and recommended technical approaches for good data management practices.\n\u2022 Worked with DBA group to create Best-Fit Physical Data Model with DDL from the Logical Data Model using Forward engineering.\n\u2022 Worked with the ETL team to document the transformation rules for data migration from OLTP to Warehouse environment for reporting purposes.\n\u2022 Extensive system study, design, development and testing were carried out in the Oracle environment to meet the customer requirements.\n\u2022 Developed Data Migration and Cleansing rules for the Integration Architecture (OLTP, ODS, DW)\n\u2022 Used Teradata utilities such as Fast Export, Multi LOAD for handling various tasks.\n\u2022 Involved in migration projects to migrate data from data warehouses on Oracle/DB2 and migrated those to Teradata.\n\u2022 Demonstrated experience in design and implementation of Statistical models, Predictive models, enterprise data model, metadata solution and data life cycle management in both RDBMS, Big Data environments\n\u2022 Tested Complex ETL Mappings and Sessions based on business user requirements and business rules to load data from source flat files and RDBMS tables to target tables\n\u2022 Created entity process association matrices using Zachman Framework, functional decomposition diagrams and data flow diagrams from business requirements documents.\n\u2022 Used Model Manager Option in Erwin to synchronize the data models in Model Mart approach.\n\u2022 Gather various reporting requirements from Business Analysts.\n\u2022 Worked on enhancements to the Data Warehouse model using Erwin as per the business reporting requirements.\n\u2022 Hands on experience in implementing Naive Bayes and skilled in Random Forests, Decision Trees, Linear and Logistic Regression, SVM, Clustering, neural networks, Principle Component Analysis.\n\u2022 Performed K-means clustering, Multivariate analysis, and Support Vector Machines in R.\n\u2022 Written complex Hive and SQL queries for data analysis to meet business requirements.\n\u2022 Written complex SQL queries for implementing business requirements\n\u2022 Reverse Engineering the reports and identified Data Elements (in the source system) . Dimensions, Facts and Measures required for reports.\n\u2022 Developed data mapping documents between Legacy, Production, and User Interface Systems.\n\u2022 Generated comprehensive analytical reports by running SQL queries against current databases to conduct data analysis.\n\u2022 Generated ad-hoc repots using Crystal Reports 9and SQL Server Reporting Services (SSRS).\nEnvironment:Erwin r9.5, DB2, Teradata, SQL-Server2008, Informatica 9.1, Enterprise Architect, Power Designer, MS SSAS, Crystal Reports, SSRS, ER Studio, Lotus Notes, Windows XP, MS Excel, word and Access.', u'Data Analyst/Data Modeler\nInnova Infotech - Bengaluru, Karnataka\nFebruary 2011 to October 2012\nResponsibilities:\n\u2022 Worked with project team representatives to ensure that logical and physical ER/Studio data models were developed in line with corporate standards and guidelines.\n\u2022 Involved in defining the source to target data mappings, business rules, data definitions.\n\u2022 Worked with BTEQ to submit SQL statements, import and export data, and generate reports in Teradata.\n\u2022 Responsible for defining the key identifiers for each mapping/interface.\n\u2022 Responsible for defining the functional requirement documents for each source to target interface.\n\u2022 Document the complete process flow to describe program development, logic, testing, and implementation, application integration, coding.\n\u2022 Involved in defining the business/transformation rules applied for sales and service data.\n\u2022 Define the list codes and code conversions between the source systems and the data mart.\n\u2022 Worked with internal architects and, assisting in the development of current and target state data architectures.\n\u2022 Coordinate with the business users in providing appropriate, effective and efficient way to design the new reporting needs based on the user with the existing functionality.\n\u2022 Remain knowledgeable in all areas of business operations in order to identify systems needs and requirements.\n\u2022 Responsible for defining the key identifiers for each mapping/interface.\n\u2022 Implementation of Metadata Repository, Maintaining Data Quality, Data Cleanup procedures, Transformations, Data Standards, Data Governance program, Scripts, Stored Procedures, triggers and execution of test plans\n\u2022 Performed data quality in TalendOpenStudio.\n\u2022 Coordinated meetings with vendors to define requirements and system interaction agreement documentation between client and vendor system.\n\u2022 Enterprise Metadata Library with any changes or updates.\n\u2022 Generate weekly and monthly asset inventory reports.\nEnvironment:Erwin r7.0, SQL Server 2000/2005, Windows XP/NT/2000, Oracle 8i/9i, MS-DTS, UML, UAT, SQL Loader, OOD, OLTP, PL/SQL, MS Visio, Informatica.', u'Data Analyst\nAmtex Software Solutions Pvt Ltd - Chennai, Tamil Nadu\nAugust 2009 to January 2011\nResponsibilities:\n\u2022 Analyze business information requirements and model class diagrams and/or conceptual domain models.\n\u2022 Gather & Review Customer Information Requirements for OLAP and building the data mart.\n\u2022 Performed document analysis involving creation of Use Cases and Use Case narrations using Microsoft Visio, in order to present the efficiency of the gathered requirements.\n\u2022 Calculated and analyzed claims data for provider incentive and supplemental benefit analysis using Microsoft Access and Oracle SQL.\n\u2022 Analyzed business process workflows and assisted in the development of ETL procedures for mapping data from source to target systems.\n\u2022 Worked with BTEQ to submit SQL statements, import and export data, and generate reports in Terra-data.\n\u2022 Responsible for defining the key identifiers for each mapping/interface\n\u2022 Responsible for defining the functional requirement documents for each source to target interface.\n\u2022 Coordinated meetings with vendors to define requirements and system interaction agreement documentation between client and vendor system.\n\u2022 Enterprise Metadata Library with any changes or updates.\n\u2022 Document data quality and traceability documents for each source interface.\n\u2022 Establish standards of procedures.\n\u2022 Generate weekly and monthly asset inventory reports.\n\u2022 Managed the project requirements, documents and use cases by IBM Rational RequisitePro.\n\u2022 Assisted in building an Integrated LogicalDataDesign, propose physical database design for building the data mart.\n\u2022 Document all data mapping and transformation processes in the Functional Design documents based on the business requirements.\nEnvironment:SQL Server 2008R2/2005 Enterprise, SSRS, SSIS, Crystal Reports, Windows Enterprise Server 2000, DTS, SQL Profiler, and Query Analyzer.']",[u'Bachelor of Computer Science & Technology in TOOLS AND TECHNOLOGIES'],[u'Stanford NLP'],degree_1 : Bachelor of Compter Science & Technology in TOOLS AND TECHNOLOGIES
0,https://resumes.indeed.com/resume/1b7927add7b21ecb,"[u'Associate Analyst\nCapital One - Bengaluru, Karnataka\nApril 2016 to June 2017\nAdvised Ops department by forecasting volumes and performing statistical process control, notified managers\nto prevent failure in resource allocation, decreased operational cost by 5%\n\u2022 Created one stop shop for Business Risk Office for tracking all matrices, collaborated with VPs to monitor\nmetrics efficiently and minimised time to skim different excel reports\n\u2022 Led a team of 4 to automate complete reporting framework, cutting time from 9 to 0 hours with 0 manual errors', u'Data Scientist\nHarman International - Bengaluru, Karnataka\nNovember 2014 to March 2016\nDeveloped machine learning algorithms for Jaguar Land Rover to predict real time failure in car, built Random\nForests, SVM, Logistic Regression Ensemble model, had 92% True Positives\n\u2022 Managed project and designed shelf layout, suggested bundled products to REWE, used decision trees, affinity\nanalysis, association rules, lifted sales by 3.5% in FMCG category', u""Business Analyst\nJune 2013 to October 2014\n\u2022 Generated targeting models for high propensity buyers, enhanced conversion rate by 20%\n\u2022 Analysed drivers effecting NPS score by performing regression model for an advertising Company, resulted in reduction of attrition rate from 16% to 12%\n\u2022 Performed text mining to enable client to understand reasons for attrition and bad NPS score\n\u2022 Converted pilot project by identifying low SFDC compliance for supply chain team of world's largest\ntechnology client, elevated large order compliance from 63% to 74%""]","[u'Master of Science in Business Analytics and Project Management', u'Bachelor of Technology in Chemical Engineering']","[u'University of Connecticut School of Business Hartford, CT\nDecember 2018', u'Malaviya National Institute of Technology Jaipur, Rajasthan\nMay 2013']","degree_1 : Master of Science in Bsiness Analytics and Project Management, degree_2 :  Bachelor of Technology in Chemical Engineering"
0,https://resumes.indeed.com/resume/e7cf378c77056857,"[u'Data scientist\nlife Scan Inc\nJune 2016 to Present\nDescription: Life Scan is an American multinational medical devices, pharmaceutical and consumer packaged goods manufacturing company founded in 1886. Its common stock is a component of the Dow Jones Industrial Average and the company is listed among the Fortune 500. Johnson & Johnson is headquartered in New Brunswick, New Jersey, the consumer division being located in Skillman, New Jersey\n\nResponsibilities:\n\u2022 Built models using Statistical techniques like Bayesian HMM and Machine Learning classification models like XG Boost, SVM, and RandomForest.\n\u2022 Worked with data compliance teams, data governance team to maintain data models, Metadata, data Dictionaries, define source fields and its definitions\n\u2022 Developed MapReduce/Spark Python modules for machine learning & predictive analytics in Hadoop on AWS. Implemented a Python-based distributed random forest via Python streaming.\n\u2022 Setup storage and data analysis tools in Amazon Web Services cloud computing infrastructure.\n\u2022 A highly immersive Data Science program involving Data Manipulation & Visualization, Web Scraping, Machine Learning, Python programming, SQL, GIT, Unix Commands, NoSQL, MongoDB, Hadoop.\n\u2022 Transformed Logical Data Model to Erwin, Physical Data Model ensuring the Primary Key and Foreign Key relationships in PDM, Consistency of definitions of Data Attributes and Primary Index Considerations.\n\u2022 Developed Oracle10g stored packages, procedures, functions and database triggers using PL/SQL for ETL process, data handling, logging, archiving and to perform Oracle back-end validations for batch processes.\n\u2022 Documented logical, physical, relational and dimensional data models. Designed the Data Marts in dimensional data modeling using star and snowflake schemas.\n\u2022 Worked as Data Architects and IT Architects to understand the movement of data and its storage and ER Studio 9.7\n\u2022 Worked with the UNIX team and installed TIDAL job scheduler on QA and Production Netezza environment.\n\u2022 Handled importing data from various data sources, performed transformations using Hive, Map Reduce, and loaded data intoHDFS.\n\u2022 Worked with BTEQ to submit SQL statements, import and export data, and generate reports in Teradata.\n\u2022 Designed and documented Use Cases, Activity Diagrams, Sequence Diagrams, OOD (Object Oriented Design) using UML and Visio.\n\u2022 Created Hive queries that helped analysts spot emerging trends by comparing fresh data with EDW reference tables and historical metrics and processed the data using HQL (like SQL) on top of Map-reduce.\n\u2022 Handson development and maintenance using Oracle SQL, PL/SQL, SQL Loader, and Informatica Power Center9.1.\n\u2022 Designed the ETL process to Extract translates and load data from OLTPOracle database system to Teradata data warehouse.\n\u2022 Created tables, sequences, synonyms, joins, functions and operators in Netezza database.\n\u2022 Created and implemented MDM data model for Consumer/Provider for HealthCareMDM product from Variant.\n\u2022 Built and published customized interactive reports and dashboards, report scheduling using Tableau server.\n\u2022 Hands on Oracle External Tables feature to read the data from flat files into Oracle staging tables.\n\u2022 Analyzed the web log data using the HiveQL to extract number of unique visitors per day, page views, visit duration, most purchased product on website and managed and reviewed Hadoop log files.\n\u2022 Used Erwin9.1 for effective model management of sharing, dividing and reusing model information and design for productivity improvement.\n\u2022 Designed and developed user interfaces and customization of Reports using Tableau and OBIEE and designed cubes for data visualization, mobile/web presentation with parameterization and cascading.\n\u2022 Performed DataAnalysis and Data Profiling and worked on data transformations and data quality rules.\n\u2022 Created SSIS Packages using Pivot Transformation, Execute SQL Task, Data Flow Task, etc to import data into the data warehouse.\n\u2022 Developed and implemented SSIS, SSRS and SSAS application solutions for various business units across the organization.\n\nEnvironment:ERwin9.x, Teradata, Oracle10g, Hadoop, HDFS, Pig, Hive, MapReduce, PL/SQL, UNIX, Informatica Power Center, MDM, SQL Server, Netezza, DB2, Tableau, Aginity, Architecture, SAS/Graph, SAS/SQL, Tableau, SAS/Connect and SAS/Access.', u'Data scientist\nCardinal health - Dublin, OH\nJanuary 2015 to December 2015\nDescription: The company specializes in distribution of pharmaceuticals and medical products, serving more than 100,000 locations. The company also manufactures medical and surgical products, including gloves, surgical apparel and fluid management products. In addition, it operates the largest network of radio pharmacies in the Cardinal Health provides medical products to over 75 percent of hospitals in the United States.\n\nResponsibilities:\n\u2022 Defined and document the functional and technical business requirements.\n\u2022 Provided the architectural leadership in shaping strategic, business technology projects, with an emphasis on application architecture.\n\u2022 Utilized domain knowledge and application portfolio knowledge to play a key role in defining the future state of large, business technology programs.\n\u2022 Participated in all phases of data mining, data collection, data cleaning, developing models, validation, and visualization and performed Gap analysis.\n\u2022 Developed MapReduce/Spark Python modules for machine learning & predictive analytics in Hadoop on AWS. Implemented a Python-based distributed random forest via Python streaming.\n\u2022 Performed Source System Analysis, database design, data modeling for the warehouse layer using MLDM concepts and package layer using Dimensional modeling.\n\u2022 Created ecosystem models (e.g. conceptual, logical, physical, canonical) that are required for supporting services within the enterprise data architecture (conceptual data model for defining the major subject areas used, ecosystem logical model for defining standard business meaning for entities and fields, and an ecosystem canonical model for defining the standard messages and formats to be used in data integration services throughout the ecosystem).\n\u2022 Used Pandas, NumPy, seaborn, SciPy, Matplotlib, Scikit-learn, NLTK in Python for developing various machine learning algorithms and utilized machine learning algorithms such as linear regression, multivariate regression, naive Bayes, Random Forests, K-means, &KNN for data analysis.\n\u2022 Demonstrated experience in design and implementation of Statistical models, Predictive models, enterprise data model, metadata solution and data life cycle management in both RDBMS, Big Data environments.\n\u2022 Developed LINUX Shell scripts by using NZSQL/NZLOAD utilities to load data from flat files to Netezza database.\n\u2022 Designed and implemented system architecture for Amazon EC2 based cloud-hosted solution for client.\n\u2022 Tested Complex ETL Mappings and Sessions based on business user requirements and business rules to load data from source flat files and RDBMS tables to target tables.\n\u2022 Analyzed large data sets apply machine learning techniques and develop predictive models, statistical models and developing and enhancing statistical models by leveraging best-in-class modeling techniques.\n\u2022 Hands on database design, relational integrity constraints, OLAP, OLTP, Cubes and Normalization (3NF) and De-normalization of database.\n\u2022 Developed MapReduce/Spark Python modules for machine learning & predictive analytics in Hadoop on AWS.\n\u2022 Worked on customer segmentation using an unsupervised learning technique - clustering.\n\u2022 Worked with various Teradata15 tools and utilities like Teradata Viewpoint, Multi Load, ARC, Teradata Administrator, BTEQ and other Teradata Utilities.\n\u2022 Utilized Spark, Scala, Hadoop, HBase, Kafka, Spark Streaming, MLlib, Python, a broad variety of machine learning methods including classifications, regressions, dimensionally reduction etc.\n\nEnvironment: Erwin r9.6, Python, SQL, Oracle 12c, Netezza, SQL Server, Informatica, Java, SSRS, PL/SQL, T-SQL, Tableau, MLlib, regression, Cluster analysis, Scala NLP, Spark, Kafka, MongoDB, logistic regression, Hadoop, Hive, Teradata, random forest, OLAP, Azure, MariaDB, SAP CRM, HDFS, ODS, NLTK, SVM, JSON, Tableau, XML, Cassandra, Map Reduce, AWS.', u'Data Scientist\nFIS Global, Omaha, NB\nMay 2013 to December 2014\nDescription: FIS, is an international provider of financial services technology and outsourcing services. Headquartered in Jacksonville, Florida, FIS employs more than 55,000 people worldwide. FIS provides payment processing and banking software, services and outsourcing of the associated technology\n\nResponsibilities:\n\u2022 Implemented end-to-end systems for Data Analytics, Data Automation and integrated with custom visualization tools using R, Mahout, Hadoop and MongoDB.\n\u2022 Performed Market-Basket Analysis and implemented decision trees, random forests and K- fold cross validation.\n\u2022 Responsible for data identification, collection, exploration & cleaning for modeling, participate in model development\n\u2022 Visualize, interpret, report findings and develop strategic uses of data.\n\u2022 Understand transaction data and develop Analytics insights using Statistical models using Azure Machine learning.\n\u2022 Selection of statistical algorithms - (Random Forest, SVM, Bagged CART, GBM etc)\n\u2022 Interacted with the other departments to understand and identify data needs and requirements and work with other members of the IT organization to deliver data visualization and reporting solutions to address those needs\n\u2022 Responsible for provide reporting, analysis and insightful recommendations to business leaders on key performance metrics pertaining to sales & marketing\n\u2022 Knowledge of other relational database platforms such as Oracle, DB2, IMS-DB, NoSQL (Elastic Search) etc.\n\u2022 Used Decision trees and Random forests to find employee attrition rate.\n\u2022 Worked on Time Series forecasting using R.\n\u2022 Converted raw data to processed data by merging, finding outliers, errors, trends, missing values and distributions in the data.\n\u2022 Accomplished Data analysis, Statistical analysis, generated reports, listings and graphs.\n\nEnvironment: R, Python, Mahout, MySQL, Hadoop, Pig, MongoDB.', u""Data Scientist\nBank of America - Princeton, NJ\nAugust 2012 to April 2013\nDescription: The project was to build an algorithm that accurately classifies credit card holders among multiple classes based on the historical data available on multiple variables. Further, the aim was to improve bank's efficiency by reducing default rate while offering new products. Moreover, I was Involved in a project to identify the employees' access level, based on his/her current & historical tasks and duties.\n\nResponsibilities:\n\u2022 Responsible for predictive analysis of credit scoring to predict whether or not credit extended to a new or an existing applicant will likely result in profit or losses.\n\u2022 Primarily used R packages for the data mining tasks.\n\u2022 Participated in all phases of data mining; data collection, data cleaning, developing models, validation and visualization.\n\u2022 Data for modeling was collected using SQL by querying several tables. The extracted tables were further appended or merged to create tables for modeling using SAS PROC MERGE and PROC SET procedures.\n\u2022 Adopted principal component analysis, PROC PRINCOMP to reduce the dimension of the data. The missing values were replaced if applicable with the group average using proc means.\n\u2022 Computed CreditRisk Parameters such as Probability of Default (PD) and Loss Given Default (LGD) and Exposure at Default (EAD).\n\u2022 Used logistic regression (PORC LOGISTIC), clustering (PROC CLUSTER) and multivariate modeling to provide valuable analytical insights.\n\u2022 Used Kolmogorov-Smirnov test (K-S test or KS test) to measure the quality of the models.\n\u2022 Used PROC GRAPH for generating various graphs and charts for analyzing the different features.\n\u2022 Used k-fold cross validation to avoid over fitting.\n\nEnvironment: SAS, R, Python, MS SQL, HIVE, HADOOP, PIG, MAHOUT."", u""Data Analyst\nVelvan Soft Solution Pvt Ltd, Hyd\nJanuary 2011 to July 2012\nDescription: Velvan Soft Solutions Private Limited - Service Provider of project & program management with special prominence, custom j2ee and .net development & advanced web portal development in Hyderabad, Telangana\n\nResponsibilities:\n\u2022 Queried data from SQL server, imported other formats of data and performed data checking, cleansing, manipulation and reporting using SAS (Base and Macro) and SQL.\n\u2022 Built loss forecast models using multitude credit data, census data and insurance data.\n\u2022 Built claim duration model for worker's comp indemnity loss reserve using survival analysis.\n\u2022 Researched and developed fraud detection model strategy. Planned and documented strategy in white paper and presentation to management.\n\u2022 Used extreme value theory and generalized Pareto distribution to fit excess liability loss data.\n\u2022 Performed ad hoc data analysis and reporting using SAS and Excel.\n\u2022 Provide guidance, training and sharing SAS programming and predictive modeling methodologies to team members"", u""Systems Analyst\nExcell Media limited\nMarch 2009 to December 2010\nDescription Excel Entertainment is an Indian film studio based in Mumbai. It was created by Farhan Akhtar, and Ritesh Sidhwani in 1999. The two have gone on to create several films under Excel's banner. Riva Games was founded by Paul Roy, the founder and Chairman of The Riva Group of Companies. Established in 2002.\n\nResponsibilities:\n\u2022 Analyze business information requirements and model class diagrams and/or conceptual domain models.\n\u2022 Gather & Review Customer Information Requirements for OLAP and building the data mart.\n\u2022 Performed document analysis involving creation of Use Cases and Use Case narrations using Microsoft Visio, in order to present the efficiency of the gathered requirements.\n\u2022 Calculated and analyzed claims data for provider incentive and supplemental benefit analysis using Microsoft Access and OracleSQL.\n\u2022 Analyzed business process workflows and assisted in the development of ETL procedures for mapping data from source to target systems.\n\u2022 Worked with BTEQ to submit SQL statements, import and export data, and generate reports in Terra-data.\n\u2022 Responsible for defining the key identifiers for each mapping/interface\n\u2022 Responsible for defining the functional requirement documents for each source to target interface.\n\u2022 Document, clarify, and communicate requests for change requests with the requestor and coordinate with the development and testing team.\n\u2022 Coordinated meetings with vendors to define requirements and system interaction agreement documentation between client and vendor system.\n\u2022 Enterprise Metadata Library with any changes or updates.\n\u2022 Document data quality and traceability documents for each source interface.\n\u2022 Establish standards of procedures.\n\nEnvironment: SQL Server 2008R2/2005 Enterprise, SSRS, SSIS, Crystal Reports, Windows Enterprise Server 2000, DTS, SQL Profiler, and Query Analyzer""]",[u'Bachelor of Computer Science in SAP Business Objects'],[u'Informatica Power Centre\nJanuary 2009'],degree_1 : Bachelor of Compter Science in SAP Bsiness Objects
0,https://resumes.indeed.com/resume/d81d1f5391893423,"[u""Baltimore, MD\nTransamerica - Baltimore, MD\nMay 2016 to November 2017\n\u2022 Work independently and collaboratively throughout the complete analytics project lifecycle including data extraction/preparation, design and implementation of scalable machine learning analysis and solutions, and documentation of results.\n\u2022 Responsible for launching Amazon EC2 instances using Amazon Web Services (Windows & Linux).\n\u2022 Created roles for EC2, S3 and EBS resources to communicate within the team using IAM.\n\u2022 Responsible for S3 bucket creation, policies and the IAM role based policies and creating alarms and notifications for EC2 hosts using Cloud Watch.\n\u2022 Build and configure Virtual Data Center in AWS cloud to support EDW hosting including Virtual Private Cloud (VPC), Public and Private Subnets, Security Groups, Route Tables and launching EC2, RDS instances in the defined virtual private connection.\n\u2022 Installed Rstudio server on AWS Linux AMI as a free tier.\n\u2022 Application of various machine learning algorithms and statistical modeling like decision trees, regression models, clustering, SVM to identify Volume using scikit-learn package in R and Python.\n\u2022 Hands on experience in implementing Naive Bayes and skilled in Random Forests, Decision Trees, Linear and Logistic Regression, SVM, Clustering, Principle Component Analysis.\n\u2022 Have knowledge on A/B Testing, ANOVA, Multivariate Analysis, Association Rules and Text Analysis using R.\n\u2022 Have Knowledge on Hadoop ecosystem framework, Pig and Hive.\n\u2022 Extensive data cleansing and analysis, using pivot tables, formulas (V-lookup and others), data validation, conditional formatting, and graph and chart manipulation using excel.\n\u2022 Collaborate with technical and non-technical resources across the business to leverage their support and integrate our efforts.\n\u2022 Training and Testing of Data under data modelling process for each machine learning algorithm.\n\u2022 Created pivot tables and charts using worksheet data and external resources, modified pivot tables, sorted items and group data, and refreshed and formatted pivot tables.\n\u2022 Analyzed the Root cause in a code and incorporated changes in programs as cost-effective solution.\n\u2022 Worked on different data formats such as JSON, XML and performed machine-learning algorithms in Python.\n\u2022 Preparing the Test Documents, Results and delivery assuring completion of the user story.\n\nClient: Computer Sciences Corporation India Pvt Ltd\nIndore, Madhya Pradesh June 2014 to Januar2016\n\n\u2022 Responsible for Retrieving data using SQL/Hive Queries and perform Analysis enhancements and documentation of the system.\n\u2022 Used R, SAS and SQL to manipulate data, and develop and validate quantitative models.\n\u2022 Generated reports of more than 100 Agent Fraud Investigation cases based on the client requirement and making sure the data is accurate.\n\u2022 Worked as a RLC Team Member and undertook user stories (tasks) with strict deadlines in Agile Environment.\n\u2022 Involved in Analyzing system failures, identifying root causes, and recommended course of actions. Documented the systems processes and procedures for future references.\n\u2022 Analysis performed on more than 100000 rows of data in excel sheet to identify the fraud agents in the system.\n\u2022 Scanned and created processes in the Data Warehouse to import retrieve and analyze data from the Cyber Life database.\n\u2022 Heavily involved in Testing and Modelling the data in order to migrate it to production environment.\n\u2022 Successfully implemented migration of client's requirement application from Test/DSS/Model regions to Production.\n\u2022 Have written numerous codes in R and python to retrieve the data from a .csv or .xl file.\n\u2022 Analyzed data collected in stores (JCL jobs, stored-procedures and queries) and provided reports to the Business team by storing the data in excel/SPSS/SAS file.\n\u2022 Performed Analysis and Interpretation of the reports on various findings.\n\u2022 Performing Exploratory Data Analysis on the data provided by the Client.\n\u2022 Prepared Test documents for Zap before and after changes in Model, Test and Production regions.\n\u2022 Responsible for production support Abend Resolution and other production support activities and comparing the seasonal trends based on the data by Excel.\n\u2022 Used advanced Microsoft Excel functions such as Pivot tables and VLOOKUP in order to analyze the data and prepare programs.\n\u2022 Brainstorming sessions and propose hypothesis, approaches and techniques.\n\u2022 Various statistical tests performed for clear understanding to the client.\n\u2022 Provided training to Beginners regarding the CyberLife system and other basics.\n\u2022 Complete support to all regions. (Test/Model/System/Regression/Production).\n\u2022 Actively involved in Analysis, Development and Unit testing of the data.\n\u2022 Complete delivery assurance of the project."", u'Jr. Data Scientist\nCSC Technologies\nMay 2011 to May 2014\nKey Responsibilities:\n\u2022 Responsible for collecting patients data from various sources including hospitals, clinics etc.\n\u2022 Prepared regular patient reports by collecting samples of Diagnosed Patients using Excel spreadsheets.\n\u2022 Ensure that there are no missing values in the dataset and can be used for further Analysis.\n\u2022 Cleaned data by analyzing and eliminating duplicate and inaccurate data (outliers) using R.\n\u2022 Worked in Agile Environment and responsible for designing analytic frameworks for data mining, ETL, analysis, and reporting under the supervision of the Manager.\n\u2022 Trained in Basics of Data Scientist and implemented those software applications in collecting and managing patient data in Excel/SPSS.\n\u2022 Assisted in performing statistical analysis of the data and storing them in a database.\n\u2022 Worked with Quality Control Teams to develop Test Plan and Test Cases.\n\u2022 Involved in designing and implementing the data extraction (XML DATA stream) procedures.\n\u2022 Generated graphs and reports using ggplot, ggplot2 in R Studio for analyzing models.\n\u2022 Generating the Results and predicting the Accuracy.\n\u2022 Preparing the Final Documents and ensure delivery to the Client before EOD.']",[u'Masters in Computer Science'],[u'Indiana State University'],degree_1 : Masters in Compter Science
0,https://resumes.indeed.com/resume/a679783301079129,"[u'Data Scientist, Intern\nCPHandheld Technologies\nSeptember 2016 to November 2016\nApplied data mining and other data modeling in Python to develop better support-based predictive models, customer experience improvements, and greater product adoption strategies. (Python, SQL, Jupyter)\n\u2212 Defined key metrics and conducted A/B testing which helped to illuminate common support issues. (Tableau)\n\u2212 Helped to develop a clustering machine learning model to analyze customer churn rates, and their respective lifetime values. (Python, Scikit-learn, Pandas, SQL)', u'Product Quality Analyst, Intern\nDitto Technologies\nJune 2016 to August 2016\nUsed SQL queries to conduct data collection within relational databases to attain product-based information.\n\u2212 Worked within an agile development team conducting brand compliance tests against the physical product.\n\u2212 Wrote and added features to a large database filled with sales data, and product design defects.']","[u""Master's in Data Analytics"", u'Bachelor of Science in Economics']","[u'University of Southern Indiana Evansville, IN\nJanuary 2018 to December 2018', u'University of Southern Indiana Evansville, IN\nMay 2017']","degree_1 : ""Masters in Data Analytics"", degree_2 :  Bachelor of Science in Economics"
0,https://resumes.indeed.com/resume/ae1a5a4a5bbf2789,"[u'Data Scientist\nTransUnion - Chicago, IL\nJune 2017 to Present\nDescription: TransUnion is a consumer credit reporting agency. TransUnion mission is to help people around the world and organizations in optimizing their risk-based decisions and enabling consumers to understand and manage their personal information.\nResponsibilities:\n\u2022 Built data pipelines for reporting, alerting, and data mining. Experienced with table design and data management using HDFS, Hive, Impala, Sqoop, MySQL, Mem SQL, Grafana/Influx DB, and Kafka.\n\u2022 Worked with statistical models for data analysis, predictive modelling, machine learning approaches and recommendation and optimization algorithms.\n\u2022 Working in Business and Data Analysis, Data Profiling, Data Migration, Data Integration and Metadata Management Services.\n\u2022 Worked extensively on Databases preferably Oracle 11g/12c and writing PL/SQL scripts for multiple purposes.\n\u2022 Built models using Statistical techniques like Bayesian HMM and MachineLearning classification models like XGBoost, SVM, and Random Forest using R and Python packages.\n\u2022 Worked with data compliance teams, data governance team to maintain data models, Metadata, data Dictionaries, define source fields and its definitions.\n\u2022 Worked with BigData Technologies such Hadoop, Hive, MapReduce\n\u2022 Developed MapReduce/Spark Python modules for machine learning & predictive analytics in Hadoop on AWS. Implemented a Python-based distributed random forest via Python streaming.\n\u2022 Setup storage and data analysis tools in Amazon Web Services cloud computing infrastructure.\n\u2022 A highly immersive Data Science program involving Data Manipulation & Visualization, Web Scraping, Machine Learning, Python programming, SQL, GIT, Unix Commands, NoSQL, MongoDB, Hadoop.\n\u2022 Performed scoring and financial forecasting for collection priorities using Python, R and SAS machine learning algorithms.\n\u2022 Handled importing data from various data sources, performed transformations using Hive, MapReduce, and loaded data into HDFS\n\u2022 Managed existing team members, lead the recruiting and on boarding of a larger Data Science team that addresses analytical knowledge requirements.\n\u2022 Worked directly with upper executives to define requirements of scoring models.\n\u2022 Developed a model for predicting repayment of debt owed to small and medium enterprise (SME) businesses.\n\u2022 Developed a generic model for predicting repayment of debt owed in the healthcare, large commercial, and government sectors.\n\u2022 Created SQLscripts and analyzed the data in MS Access/Excel and Worked on SQL and SAS script mapping.\n\u2022 Developed a legal model for predicting which debtors respond to litigation only.\n\u2022 Created multiple dynamic scoring strategies for adjusting the score upon consumer behaviour such as payment or right-party phone call.\n\u2022 Rapid model creation in Python using pandas, NumPy, sklearn, and plot.ly for data visualization. These models are then implemented in SAS where they are interfaced with MSSQL databases and scheduled to update on a timely basis.\n\u2022 Data analysis using regressions, data cleaning, excel v-look up, histograms and TOAD client and data representation of the analysis and suggested solutions for investors\n\u2022 Attained good knowledge in Hadoop Data Lake Implementation and HADOOP Architecture for client business data management.\n\u2022 Identifying relevant key performing factors; testing their statistical significance\n\u2022 Above scoring models resulted in millions of dollars of added revenue to the company and a change in priorities of the entire company.\nEnvironment:-R, SQL, Python 2.7.x, SQL Server 2014, regression, logistic regression, random forest, neural networks, Topic Modeling, NLTK, SVM (Support Vector Machine), JSON, XML, HIVE, HADOOP, PIG, Sklearn, SciPy, GraphLab, No SQL, SAS, SPSS, Spark, Hadoop, Kafka, HBase, MLib.', u'Data Scientist\nCiti - Irving, TX\nMarch 2016 to May 2017\nDescription: -Citi provide consumers, corporations, governments and institutions with a broad range of financial services and products. It strives to create the best outcomes for clients and customers with financial ingenuity that leads to simple, creative and responsible solutions.\nResponsibilities:\n\u2022 Utilize a broad variety of statistical packages like SAS, R, MLIB, Graphs, Hadoop, Spark, MapReduce, Pig and others\n\u2022 Refine and train models based on domain knowledge and customer business objectives\n\u2022 Deliver or collaborate on delivering effective visualizations to support the client business objectives\n\u2022 Extensive understanding of the BI and analytics space with special focus on the consumer and customer space\n\u2022 Converted time lag problems in order fulfillment into Data mining tasks\n\u2022 Performed Data Profiling to assess data quality using SQL through complex internal database\n\u2022 Improved sales and logistic data quality by data cleaning using NumPy, SciPy, Pandas in Python\n\u2022 Built Data warehouse to support end-user queries with Oracle and MS Visual Studio\n\u2022 Designed and implemented Dimensional DataModeling for order fulfillment process\n\u2022 Deployed SSIS packages to complete ETL and DataMapping process\n\u2022 Transformed data through methods like Aggregation, Slowly Changing Dimension, Splitting\n\u2022 Derived business intelligence report for order fulfilment using MS SSAS and SSRS\n\u2022 Determined regression model predictors using Correlation matrix for Factor analysis in R\n\u2022 Built Regression model to understand order fulfilment time lag issue using Scikit-learn in Python\n\u2022 Optimized predictive model by reducing insignificant variables using Stepwise Regression\n\u2022 Empowered decision makers with data analysis dashboards using Tableau and Power BI\n\u2022 Interface with other technology teams to extract, transform, and load (ETL) data from a wide variety of data sources\n\u2022 Own the functional and non-functional scaling of software systems in your ownership area.\n\u2022 Provides input and recommendations on technical issues to BIEngineers, Business&DataAnalysts and Data Scientists.\n\u2022 Outstanding analytical and problem solving skills are essential.\nEnvironment:- Python, Hive, C/C++, C# , Java or Python, Bash, HTML5, PERL, Processing, Python and J Query, SOAPUI, WCF, WPF, VSO, TFS, GIT, XML, XSD, SQL Server 2008, Oracle 10/11g, ANGULAR JS.', u'Data Scientist\nDPSG - Plano, TX\nNovember 2014 to February 2016\nDescription:- On May 7, 2008, DPS became a stand-alone, publicly traded company on the New York Stock Exchange as the result of a spin-off by Cadbury, plc, which held the Cadbury Schweppes Americas Beverages business group of entities.\nResponsibilities:\n\u2022 Supported MapReduce Programs running on the cluster.\n\u2022 Evaluated business requirements and prepared detailed specifications that follow project guidelines required to develop written programs.\n\u2022 Configured Hadoop cluster with Name node and slaves and formatted HDFS.\n\u2022 Used Oozie workflow engine to run multiple Hive and Pig jobs.\n\u2022 Performed MapReduce Programs those are running on the cluster.\n\u2022 Developed multiple MapReduce jobs in java for data cleaning and pre-processing.\n\u2022 Analyzed the partitioned and bucketed data and compute various metrics for reporting.\n\u2022 Involved in loading data from RDBMS and web logs into HDFS using Sqoop and Flume.\n\u2022 Worked on loading the data from MySQL to HBase where necessary using Sqoop.\n\u2022 Developed Hive queries for Analysis across different banners.\n\u2022 Extracted data from Twitter using Java and Twitter API.Parsed JSON formatted twitter data and uploaded to database.\n\u2022 Launching AmazonEC2 Cloud Instances using Amazon Images (Linux/ Ubuntu) and Configuring launched instances with respect to specific applications.\n\u2022 Exported the result set from Hive to MySQL using Sqoop after processing the data.\n\u2022 Analyzed the data by performing Hive queries and running Pigscripts to study customer behavior.\n\u2022 Have hands on experience working on Sequence files, AVRO, HAR file formats and compression.\n\u2022 Used Hive to partition and bucketdata.\n\u2022 Experience in writing MapReduce programs with JavaAPI to cleanse Structured and unstructured data.\n\u2022 Wrote Pig Scripts to perform ETL procedures on the data in HDFS.\n\u2022 Created HBase tables to store various data formats of data coming from different portfolios.\n\u2022 Worked on improving performance of existing Pig and Hive Queries.\nEnvironment: -SQL/Server, Oracle 9i, MS-Office, Teradata, Informatica, ER Studio, XML, Business Objects.', u'Data Analyst/Modeler\nCitrix Systems - Fort Lauderdale, FL\nMarch 2013 to October 2014\nDescription: - Citrix Systems, Inc. is an American multinational software company that provides server, application and desktop virtualization, networking, software as a service, and cloud computing technologies.\nResponsibilities:\n\u2022 Participated in JAD sessions, gathered information from Business Analysts, end users and other stakeholders to determine the requirements.\n\u2022 Developed the logical data models and physical data models that confine existing condition/potential status data fundamentals and data flows using ER Studio.\n\u2022 Created Data warehousing methodologies/Dimensional Data modelling techniques such as Star/Snowflake schema using ERWIN9.1.\n\u2022 Extensively used Aginity Netezza workbench to perform various DDL, DML etc. operations on Netezza database.\n\u2022 Designed the Data Warehouse and MDM hub Conceptual, Logical and Physical data models.\n\u2022 Performed Daily Monitoring of Oracle instances using Oracle Enterprise Manager, ADDM, TOAD, monitor users, table spaces, memory structures, rollback segments, logs and alerts.\n\u2022 Involved in Teradata SQL Development, Unit Testing and Performance Tuning and to ensure testing issues are resolved on the basis of using defect reports.\n\u2022 Customized reports using SAS/MACRO facility, PROC REPORT, PROC TABULATE and PROC.\n\u2022 Translate business and data requirements into Logical data models in support of Enterprise DataModels, ODS, OLAP, OLTP, Operational Data Structures and Analytical systems.\n\u2022 Worked on database testing, wrote complex SQL queries to verify the transactions and business logic like identifying the duplicate rows by using SQL Developer and PL/SQL Developer.\n\u2022 Used Teradata SQL Assistant, Teradata Administrator, PMONand data load/export utilities like BTEQ, FastLoad, Multi Load, Fast Export, TPumpon UNIX/Windows environments and running the batch process for Teradata.\n\u2022 Worked on data profiling and data validation to ensure the accuracy of the data between the warehouse and source systems.\n\u2022 Hands on Data warehouse concepts like Data warehouse Architecture, Star schema, Snowflake schema, and Data Marts, Dimension and Fact tables.\n\u2022 Developed SQL Queries to fetch complex data from different tables in remote databases using joins, database links and Bulk collects.\n\u2022 Migrated database from legacy systems, SQL server to Oracle and Netezza.\n\u2022 Reviewed the logical model with application developers, ETL Team, DBAs and testing team to provide information about the data model and business requirements.\n\u2022 Worked on SQL Server concepts SSIS (SQL Server Integration Services), SSAS (Analysis Services) and SSRS (Reporting Services).\nEnvironment:-ER Studio, Teradata13.1, SQL, PL/SQL, BTEQ, DB2, Oracle, MDM, Netezza, ETL, RTF UNIX, SQL Server2010, Informatica, SSRS, SSIS, SSAS, SAS, Aginity.', u'Data Analyst/Data Modeler\nInnovaInfotech - Bengaluru, Karnataka\nOctober 2011 to February 2013\nDescription: - SYS INNOVA Infotech is an offshore software services and IT consulting company based in Bangalore, India. As a committed outsourcing partner and an IT vendor, our goal is to ensure cost effective, technical excellence and on-time deliveries. While we take care of their end-to-end programming and consulting needs, our clients focus on core business activities which correlate directly to their revenues and profitability. Strategic partnership with us gives our clients the access to latest technology, skilled manpower and scalable team which ultimately results in lower risk and higher ROI.\nResponsibilities:\n\u2022 Worked with project team representatives to ensure that logical and physical ER/Studio data models were developed in line with corporate standards and guidelines.\n\u2022 Involved in defining the source to target data mappings, business rules, data definitions.\n\u2022 Worked with BTEQ to submit SQL statements, import and export data, and generate reports in Teradata.\n\u2022 Responsible for defining the key identifiers for each mapping/interface.\n\u2022 Responsible for defining the functional requirement documents for each source to target interface.\n\u2022 Document the complete process flow to describe program development, logic, testing, and implementation, application integration, coding.\n\u2022 Involved in defining the business/transformation rules applied for sales and service data.\n\u2022 Define the list codes and code conversions between the source systems and the data mart.\n\u2022 Worked with internal architects and, assisting in the development of current and target state data architectures.\n\u2022 Coordinate with the business users in providing appropriate, effective and efficient way to design the new reporting needs based on the user with the existing functionality.\n\u2022 Remain knowledgeable in all areas of business operations in order to identify systems needs and requirements.\n\u2022 Responsible for defining the key identifiers for each mapping/interface.\n\u2022 Implementation of Metadata Repository, Maintaining Data Quality, Data Cleanup procedures, Transformations, Data Standards, Data Governance program, Scripts, Stored Procedures, triggers and execution of test plans\n\u2022 Performed data quality in Talend Open Studio.\n\u2022 Coordinated meetings with vendors to define requirements and system interaction agreement documentation between client and vendor system.\n\u2022 Enterprise Metadata Library with any changes or updates.\n\u2022 Generate weekly and monthly asset inventory reports.\nEnvironment:-Erwin r7.0, SQL Server 2000/2005, Windows XP/NT/2000, Oracle 8i/9i, MS-DTS, UML, UAT, SQL Loader, OOD, OLTP, PL/SQL, MS Visio, Informatica.', u'Data Analyst\nStyles You, Amtex Software Solutions Pvt Ltd - Chennai, Tamil Nadu\nMarch 2009 to September 2011\nDescription:- Amtex provides high-quality end-to-end software solutions across industries through unique models and methodologies, to deliver time, cost, quality and full-service advantages best-of breed in nature. Amtex assists clients in making informed business decisions through high-impact insight, advice, and research. An international network of industry experts enables us to carry out global and country-specific projects.\nResponsibilities:\n\u2022 Analyze business information requirements and model class diagrams and/or conceptual domain models.\n\u2022 Gather & Review Customer Information Requirements for OLAP and building the data mart.\n\u2022 Performed document analysis involving creation of Use Cases and Use Case narrations using Microsoft Visio, in order to present the efficiency of the gathered requirements.\n\u2022 Calculated and analyzed claims data for provider incentive and supplemental benefit analysis using Microsoft Access and Oracle SQL.\n\u2022 Analyzed business process workflows and assisted in the development of ETL procedures for mapping data from source to target systems.\n\u2022 Worked with BTEQ to submit SQL statements, import and export data, and generate reports in Terra-data.\n\u2022 Responsible for defining the key identifiers for each mapping/interface\n\u2022 Responsible for defining the functional requirement documents for each source to target interface.\n\u2022 Coordinated meetings with vendors to define requirements and system interaction agreement documentation between client and vendor system.\n\u2022 Enterprise Metadata Library with any changes or updates.\n\u2022 Document data quality and traceability documents for each source interface.\n\u2022 Establish standards of procedures.\n\u2022 Generate weekly and monthly asset inventory reports.\n\u2022 Managed the project requirements, documents and use cases by IBM Rational RequisitePro.\n\u2022 Assisted in building an Integrated LogicalDataDesign, propose physical database design for building the data mart.\n\u2022 Document all data mapping and transformation processes in the Functional Design documents based on the business requirements.\nEnvironment:-SQL Server 2008R2/2005 Enterprise, SSRS, SSIS, Crystal Reports, Windows Enterprise Server 2000, DTS, SQL Profiler, and Query Analyzer.']",[],[],degree_1 : 
0,https://resumes.indeed.com/resume/ca9f357eb38a81d3,"[u'Data Insight and Operational Lead - Practicum Consulting Project\nWells Fargo Advisors - St. Louis, MO\nJanuary 2018 to Present\nFor the moment, I am working as a Data Lead on an analytics project with Wells Fargo in finding the driving factors for financial product demand based on monthly data accumulated in the past three years. As a data lead, I am tasked to communicate between my team mates, the client, and advisors constantly in order to understand the business problem, align the expectation, and more importantly, seek the most appropriate solution and methodology using data analytics skills and knowledge.', u""Data Scientist Intern\nDELOITTE ANALYTICS INSTITUTE (DAI) - Shanghai, CN\nDecember 2016 to June 2017\n\u2022 Tidied and visualized 2 Million consumers' demographics and behavioral data using R ('dplyr', 'ggplot2', 'reshape') and Tableau, delivering 360\xba customer profile by performing ETL\n\u2022 Created K-medoids model to discern high-value segments and tailored CRM activities based on clustering features\n\u2022 Constructed Logistic Regression and CART on 40K+ customer-level credit data with 489 predictors in R ('glmnet', 'randomForest', 'gbm') and SPSS Modeler; Implemented Machine Transfer Learning on loan delinquency prediction\n\u2022 Developed introductory-level study dashboards on data modeling and R&D projects (Predictive Analysis, Sentimental Analysis, Robotic Process Automation) for data education\n\u2022 Created Wechat account feeds; Increased unique visitor viewing by 30% and page review by 400% within 7 days"", u""Tax Consultant Intern\nKPMG - Shanghai, CN\nSeptember 2016 to November 2016\n\u2022 Innovated an online paperless VAT data system and streamlined monthly tax verification process, reducing GMS team's time commitment on it from 1 day to 10 minutes (2300% improvement)\n\u2022 Created XML template for monthly report of companies' custom duties and tax, optimizing tax refunding process from 1 day to 2 hours (300% improvement)\n\u2022 Filed the accounting voucher to get familiar with the clients' business operation; Completed all the unfinished filing left by the former intern and bound them in an orderly way"", u""Marketing Intern\nSHANGHAI MEDIA GROUP - Shanghai, CN\nJuly 2015 to September 2015\n\u2022 Inspected episode data of 20+ Disney TV programs with selected features by multi-dimensional analysis using Excel; Proposed favorable programming based on the analyses; Increase youngsters' viewership by 1% year-on-year\n\u2022 Increased 0.4% in rating within a week by devising ICS's Wechat public account feeds\n\u2022 Used non-linear editing application (Sobey) to produce ICS programs' promos, varying from 30 seconds to 2 minutes""]","[u'Master of Science in Customer Analytics', u'Bachelor of Arts in Communication Science / Applied Math']","[u'WASHINGTON UNIVERSITY, OLIN BUSINESS SCHOOL St. Louis, MO\nAugust 2017 to December 2018', u'FUDAN UNIVERSITY Shanghai, CN\nSeptember 2013 to June 2017']","degree_1 : Master of Science in Cstomer Analytics, degree_2 :  Bachelor of Arts in Commnication Science / Applied Math"
0,https://resumes.indeed.com/resume/673b88f3753e895e,"[u'Data Scientist/ Machine Learning\nNissan Motors, Tennessee\nAugust 2015 to Present\nDescription: The mission of the Nissan Foundation is to build community through valuing cultural diversity. The Nissan Foundation is part of Nissan North America\'s commitment to ""enrich people\'s lives"" by helping to meet the needs of communities throughout the U.S. through philanthropic investments, corporate outreach sponsorships.\n\nResponsibilities:\n\u2022 Extracted data from HDFS and prepared data for exploratory analysis using data munging\n\u2022 Built models using Statistical techniques like Bayesian HMM and Machine Learning classification models like XGBoost, SVM, and Random Forest.\n\u2022 Participated in all phases of data mining, data cleaning, data collection, developing models, validation, visualization and performed Gap analysis.\n\u2022 A highly immersive Data Science program involving Data Manipulation&Visualization, Web Scraping, Machine Learning, Python programming, SQL, GIT, MongoDB, Hadoop.\n\u2022 Setup storage and data analysis tools in AWS cloud computing infrastructure.\n\u2022 Installed and used Caffe Deep Learning Framework\n\u2022 Worked on different data formats such as JSON, XML and performed machine learning algorithms in Python.\n\u2022 Worked as Data Architects and IT Architects to understand the movement of data and its storage and ER Studio 9.7\n\u2022 Used pandas, numpy, seaborn, matplotlib, scikit-learn, scipy, NLTK in Python for developing various machine learning algorithms.\n\u2022 Data Manipulation and Aggregation from different source using Nexus, Business Objects, Toad, Power BI and Smart View.\n\u2022 Implemented Agile Methodology for building an internal application.\n\u2022 Focus on integration overlap and Informatica newer commitment to MDM with the acquisition of Identity Systems.\n\u2022 Coded proprietary packages to analyze and visualize SPCfile data to identify bad spectra and samples to reduce unnecessary procedures and costs.\n\u2022 Programmed a utility in Python that used multiple packages (numpy, scipy, pandas)\n\u2022 Implemented Classification using supervised algorithms like Logistic Regression, Decision trees, Naive Bayes, KNN.\n\u2022 As Architect delivered various complex OLAPdatabases/cubes, scorecards, dashboards and reports.\n\u2022 Updated Python scripts to match training data with our database stored in AWS Cloud Search, so that we would be able to assign each document a response label for further classification.\n\u2022 Used Teradata utilities such as Fast Export, MLOAD for handling various tasks data migration/ETL from OLTP Source Systems to OLAP Target Systems\n\u2022 Data transformation from various resources, data organization, features extraction from raw and stored.\n\u2022 Validated the machine learning classifiers using ROC Curves and Lift Charts.\n\nEnvironment: Unix, Python 3.5.2, MLLib, SAS, regression, logistic regression, Hadoop 2.7.4, NoSQL, Teradata, OLTP, random forest, OLAP, HDFS, ODS, NLTK, SVM, JSON, XML and MapReduce.', u""Data scientist/ R Developer\nOracle Corp - Pleasanton, CA\nFebruary 2014 to July 2015\nDescription: Oracle Corporation is a multinational computer technology corporation, headquartered in Redwood Shores, California. The company specializes primarily in developing and marketing database software and technology, cloud engineered systems and enterprise software products particularly its own brands of database management systems. In 2015 Oracle was the second-largest software maker by revenue, after Microsoft.\n\nResponsibilities:\n\u2022 Designed an Industry standard data Model specific to the company with group insurance offerings, Translated the business requirements into detailed production level using Workflow Diagrams, Sequence Diagrams, Activity Diagrams and Use Case Modeling\n\u2022 Involved in design and development of data warehouse environment, liaison to business users and technical teams gathering requirement specification documents and presenting and identifying data sources, targets and report generation.\n\u2022 Recommend and evaluate marketing approaches based on quality analytics of customer consuming behavior.\n\u2022 Determine customer satisfaction and help enhance customer experience using NLP.\n\u2022 Work on Text Analytics, Naive Bayes, Sentiment analysis, creating word clouds and retrieving data from Twitter and other social networking platforms.\n\u2022 Conceptualized the most-used product module (Research Center) after building a business case for approval, gathering requirements and designing the User Interface\n\u2022 A team member of Analytical Group and assisted in designing and development of statistical models for the end clients. Coordinated with end users for designing and implementation of e-commerce analytics solutions as per project proposals.\n\u2022 Conducted market research for client; developed and designed sampling methodologies, and analyzed the survey data for pricing and availability of clients' products. Investigated product feasibility by performing analyses that include market sizing, competitive analysis and positioning.\n\u2022 Successfully optimized codes in Python to solve a variety of purposes in data mining and machine learning in Python.\n\u2022 Facilitated stakeholder meetings and sprint reviews to drive project completion.\n\u2022 Successfully managed projects using Agile development methodology\n\u2022 Project experience in Data mining, segmentation analysis, business forecasting and association rule mining using Large Data Sets with Machine Learning.\n\u2022 Automated Diagnosis of Blood Loss during Accidents and Applied Machine Learning algorithms to diagnose blood loss from vital signs (ECG, HF, GSR, etc.). Demonstrated performances of 94.6% on par with state-of-the-art models used in industry\n\nEnvironment: R, MATLAB, MongoDB, exploratory analysis, feature engineering, K-Means Clustering, Hierarchical Clustering, Machine Learning), Python, Spark (MLlib, PySpark), Tableau, Micro Strategy, SAS, Tensor Flow, regression, logistic regression, Hadoop 2.7, OLTP, random forest, OLAP, HDFS, ODS, NLTK, SVM, JSON, XML and MapReduce"", u'Data Analyst\nMcKesson - Dublin, CA\nNovember 2012 to January 2014\nDescription: McKesson company is U.S-based multinational corporation that produces medical technology related to wounds and wound healing including therapeutic beds, wound care and tissue regeneration technology.\n\nResponsibilities\n\u2022 Worked with BI team in gathering the report requirements and also Sqoop to export data into HDFS and Hive\n\u2022 Involved in the below phases of Analytics using R, Python and Jupyter notebook.\n\u2022 a. Data collection and treatment: Analysed existing internal data and external data, worked on entry errors, classification errors and defined criteria for missing values\nb. Data Mining: Used cluster analysis for identifying customer segments, Decision trees used for profitable and non-profitable customers, Market Basket Analysis used for customer purchasing behaviour and part/product association.\n\u2022 Developed multiple Map Reduce jobs in Java for data cleaning and preprocessing.\n\u2022 Assisted with data capacity planning and node forecasting.\n\u2022 Installed, Configured and managed Flume Infrastructure\n\u2022 Administrator for Pig, Hive and HBase installing updates patches and upgrades.\n\u2022 Worked closely with the claims processing team to obtain patterns in filing of fraudulent claims.\n\u2022 Worked on performing major upgrade of cluster from CDH3u6 to CDH4.4.0\n\u2022 Developed Map Reduce programs to extract and transform the data sets and results were exported back to RDBMS using Sqoop.\n\u2022 Patterns were observed in fraudulent claims using text mining in R and Hive.\n\u2022 Exported the data required information to RDBMS using Sqoop to make the data available for the claims processing team to assist in processing a claim based on the data.\n\u2022 Developed Map Reduce programs to parse the raw data, populate staging tables and store the refined data in partitioned tables in the EDW.\n\u2022 Created tables in Hive and loaded the structured (resulted from Map Reduce jobs) data\n\u2022 Using HiveQL developed many queries and extracted the required information.\n\u2022 Created Hive queries that helped market analysts spot emerging trends by comparing fresh data with EDW reference tables and historical metrics.\n\u2022 Was responsible for importing the data (mostly log files) from various sources into HDFS using Flume\n\u2022 Enabled speedy reviews and first mover advantages by using Oozie to automate data loading into the Hadoop Distributed File System and PIG to pre-process the data.\n\u2022 Provided design recommendations and thought leadership to sponsors/stakeholders that improved review processes and resolved technical problems.\n\u2022 Managed and reviewed Hadoop log files.\n\u2022 Tested raw data and executed performance scripts.\n\nEnvironment: HDFS, PIG, HIVE, Map Reduce, Linux, HBase, Flume, Sqoop, R, VMware, Eclipse, Cloudera, Python.', u""Python Developer\nIBM - Hyderabad, Telangana\nJanuary 2011 to October 2012\nDescription: IBM India Private Limited is the Indian subsidiary of IBM.IBM has been the multinational with the largest number of employees in India. IBM is very secretive about the geographic distribution of its employees. By most estimates, it has close to third of its 430,000 employees in India, and it likely has more employees there than in the US.\n\nResponsibilities:\n\u2022 Involved in the design, development and testing phases of application using AGILE methodology.\n\u2022 Designed and maintained databases using Python and developed Python based API (Restful Web Service) using Flask, SQLAlchemy and PostgreSQL.\n\u2022 Designed and developed the UI of the website using HTML, XHTML, AJAX, CSS and JavaScript.\n\u2022 Participated in requirement gathering and worked closely with the architect in designing and modeling.\n\u2022 Worked on Restful web services which enforced a stateless client server and support JSON few changes from SOAP to RESTFUL Technology Involved in detailed analysis based on the requirement documents.\n\u2022 Involved in writing SQL queries implementing functions, triggers, cursors, object types, sequences, indexes etc.\n\u2022 Created and managed all of hosted or local repositories through Source Tree's simple interface of GIT client, collaborated with GIT command lines and Stash.\n\u2022 Responsible for setting up Python REST API framework and spring frame work using Django\n\u2022 Develop consumer based features and applications using Python, Django, HTML, behaviour Driven Development (BDD) and pair based programming.\n\u2022 Designed and developed components using Python with Django framework. Implemented code in python to retrieve and manipulate data.\n\u2022 Involved in development of the enterprise social network application using Python, Twisted, and Cassandra.\n\u2022 Used Python and Django creating graphics, XML processing of documents, data exchange and business logic implementation between servers.\n\u2022 worked closely with back-end developer to find ways to push the limits of existing Web technology.\n\u2022 Designed and developed the UI for the website with HTML, XHTML, CSS, Java Script and AJAX\n\u2022 Used AJAX&JSON communication for accessing Restful web services data payload.\n\u2022 Designed dynamic client-side JavaScript codes to build web forms and performed simulations for web application page.\n\u2022 Created and implemented SQL Queries, Stored procedures, Functions, Packages and Triggers in SQL Server.\n\u2022 Successfully implemented Auto Complete/Auto Suggest functionality using JQuery, Ajax, Web Service and JSON.\n\u2022 Identified and added the report parameters and created the reports based on the requirements using SSRS 2008.\n\u2022 Tested and managed the SSIS 2005/2008 and SSIS 2007/8 packages and was responsible for its security.\n\nEnvironment: Python 2.5, Java/J2EE, Django1.0, HTML, CSS Linux, Shell Scripting, Java Script, Ajax, JQuery, JSON, XML, PostgreSQL, Jenkins, ANT, Maven, Subversion, Python"", u""SQL developer\nIntergraph - Hyderabad, Telangana\nMay 2009 to December 2010\nDescription: Intergraph is the leading global provider of engineering and geospatial software that enables customers to build and operate more efficient plants and ships, create intelligent maps, and protect critical infrastructure and millions of people around the world. Intergraph is part of Hexagon.\n\nResponsibilities:\n\u2022 Responsible for the study of SAS Code, SQL Queries, Analysis enhancements and documentation of the system.\n\u2022 Used R, SAS, and SQL to manipulate data, and develop and validate quantitative models.\n\u2022 Brainstorming sessions and propose hypothesis, approaches, and techniques.\n\u2022 Analyzed data collected in stores (JCL jobs, stored-procedures, and queries) and provided reports to the Business team by storing the data in excel/SPSS/SAS file.\n\u2022 Performed Analysis and Interpretation of the reports on various findings.\n\u2022 Responsible for production support Abend Resolution and other production support activities and comparing the seasonal trends based on the data by Excel.\n\u2022 Used advanced Microsoft Excel functions such as pivot tables and VLOOKUPin order to analyze the data.\n\u2022 Successfully implemented migration of client's requirement application from Test/DSS/Model regions to production.\n\u2022 Prepared SQL scripts for ODBC and Teradata servers for analysis and modeling.\n\u2022 Provided complete assistance of the trends of the financial time series data.\n\u2022 Various statistical tests performed for clear understanding to the client.\n\u2022 Implemented procedures for extracting Excel sheet data into the mainframe environment by connecting to the database using SQL.\n\u2022 Complete support to all regions (Test/Model/System/Regression/Production).\n\u2022 Actively involved in Analysis, Development, and Unit testing of the data.\n\nEnvironment: R/R Studio, SQL Enterprise Manager, SAS, Microsoft Excel, Microsoft Access, outlook.""]",[u'Bachelor Of Information Technology in TOOLS AND TECHNOLOGIES'],[u'Informatica Power Center 9.x'],degree_1 : Bachelor Of Information Technology in TOOLS AND TECHNOLOGIES
0,https://resumes.indeed.com/resume/6e7e5ace585f3281,"[u'Data Scientist Intern\nCCC Information Services Inc. - Chicago, IL\nMay 2017 to August 2017\n\u2022 Developed a prediction system which forecasts the future events on production servers using Long Short-Term Memory Recurrent neural networks saving CCC millions of dollars in operations.\n\u2022 Forecasted the reliability of the whole system for the window of next two weeks, based on the historical log data using machine learning algorithms like Neural Networks and Time series.\n\u2022 Extract, clean, transform raw JSON data collected from multiple sources, aggregate them by grouping on certain fields.\n\u2022 Organize, format, validate and perform SQL queries on the extracted JSON data.\n\u2022 Created visualization dashboard by Tableau for exploratory data analysis.\n\u2022 Developed and Validated predictive models after analysing CCC\u2019s 300 WebLogic servers using R, SQL and Python.\n\u2022 Handled large dataset of 4TB. Used R, Python, MySQL, Tableau. Worked on Unix environment.', u'Associate Software engineer\nAccenture - Bengaluru, Karnataka\nAugust 2015 to August 2016\n\u2022 Coordinated with Business Analysts from the client team (Travelers \u2013 based out of Hartford, CT, US) to gather business requirements and frame a Functional Design Document.\n\u2022 Performed advance Excel data analysis (Pivot tables, vlookup) and SQL queries.\n\u2022 Checked for quality of the design code and improved the performance by 18% through modification of code.\n\u2022 Creating Functional test case scenarios and performing risk analysis.\n\u2022 Unit Testing, Coding, Adding/Updating the functionality of existing Web Service (SOAP) of Travelers and enhancing the code to make sure the team remains 100% compliant with the standards set by the client.\n\n\nACADEMIC PROJECTS:\n\nIMDB DATA ANALYSIS Jan\u201917\u2013Apr\u201917\n\u2022 Used Web Scraping to scrape data from the IMDB website using\nBeautifulSoup library.\n\u2022 Data Cleansing and refining using OpenRefine tool.\n\u2022 Applied regression models (Random Forest, Logistic Regression, Decision T\nTree) to predict the IMDB rating. Model comparison was done based on R-\nsquare values. Used R and python.\n\u2022 Predicted optimal release season for a movie using topic modelling\ntechnique called Latent Dirichlet Allocation(LDA).\n\nDOCUMENT CLUSTERING FOR LSA, LDA AND WORD2VECTOR Jan\u201917 \u2013 Apr\u201917\n\u2022 Evaluated 3 different document representations, tf-idf, LSA, LDA (Topic\nModelling) on Enron email, 20 newsgroup and amazon datasets. Analysed\nsemantic spaces computed by LSA/LDA using the most representative\nwords.\n\u2022 Used different clustering techniques such as k-means, Density-Based and\nHierarchical clustering and evaluated the performance using SSE measure.\nUsed R.\n\nEXPEDIA HOTEL RECOMMENDATION (KAGGLE PROJECT) Oct\u201916 \u2013 Dec\u201916\n\u2022 Worked on massive dataset of 38 million data points provided by Expedia.\n\u2022 Implemented various feature reduction techniques such as PCA, LDA to\nscale down the features of data.\n\u2022 Applied various classification algorithms and achieved R2 value of 0.82 with\nrandom forest.\n\u2022 Predicted which hotel group will a user stay out of the 100 different hotel\ngroups given based on various customer data and the search the customer\nis conducting on Expedia. Used Python.']","[u""Master's in Data Science"", u""Bachelor's in Information Science""]","[u'Illinois Institute of Technology Chicago, IL\nAugust 2016 to December 2017', u'Ramaiah Institute of Technology Bengaluru, Karnataka\nAugust 2011 to June 2015']","degree_1 : ""Masters in Data Science"", degree_2 :  ""Bachelors in Information Science"""
0,https://resumes.indeed.com/resume/06ab494edd579e40,"[u'Data Scientist\nFreelance - Atlanta, GA\nOctober 2017 to Present\n\u2022 Explore and examine data from multiple disparate sources assist the client with\ntheir company/project\n\u2022 Perform complex and detailed statistical analysis and modeling with machine\nlearning concepts\n\u2022 Work with datasets using tools such as Python, R, and SQL', u'Help Desk Specialist\nAtlanta International School - Atlanta, GA\nDecember 2016 to Present\nPerform MacBook diagnostics and repairs\n\u2022 Assist in updating and maintaining the school\u2019s inventory of hardware (includes\n180+ MacBooks, 400+ iPads) and software (Windows OS deployment, iOS app\ndeployment, Microsoft Office suite)\n\u2022 Perform onboarding of new users by inserting their information to our\ndatabases (Active Directory, Blackbaud, Veracross)\n\u2022 Assist teachers, students, and parents with their technology issues through a ticketing system']","[u'BS in Computer Science, Mathermatics']","[u'Georgia State University Atlanta, GA\nAugust 2013 to December 2017']","degree_1 : BS in Compter Science, degree_2 :  Mathermatics"
0,https://resumes.indeed.com/resume/ac5e8b807a096ac7,"[u'Data Science & Analytics Lead\nEngage3 - Davis, CA\nAugust 2015 to Present\n\u2022 Data Science & Analytics Lead in organization of 10-15 full-time employees -- we turned company\naround from 2-week runway (2015) to closing Series B funding round (2017)\n\u2022 Designed, developed, and implemented pipeline of in-store data collection, management, analysis, and on-demand reporting; achieved via SQL databases, Python processes and endpoints, and bash shell\nscripts; system supported 50%+ of total 2017 revenue\n\u2022 Led team of 2-4 contractors and employees to scale-up data processing pipeline and reporting\n\u2022 Directly drove 3% of 2017 revenue through analytics offering of reverse-engineering competitor\npricing strategies using 10M+ data points; methods include unsupervised clustering and classification\nalgorithms using Scala and Spark on Amazon EMR\n\u2022 Interfaced directly with clients regularly to outline client needs and requirements; created and presented Tableau and Powerpoint analyses directly to clients\n\u2022 Designed, developed, and deployed realtime processing of 1M+ daily crawl records in Amazon Redshift\nand Amazon RDS Postgres\n\u2022 Deployed and actively maintained 5+ Python Flask and CherryPy endpoints for external and internal\nneeds on Amazon Elastic Beanstalk and company hardware\n\u2022 Created fully-automated QA system to process millions of price records hourly using Oracle PL/SQL', u'Analytics Consultant\nEngage3 - Davis, CA\nJune 2014 to August 2015', u'Data Analytics Intern\nGENERAL ELECTRIC - San Ramon, CA\nNovember 2014 to March 2015\nCreated easy-to-use search flow in parts database using Elasticsearch, Python Flask, and basic\nJavaScript', u'Data Scientist\nEngage3 - Davis, CA\nDecember 2013 to June 2014', u'Aviation Data Analyst\nATAC - Sunnyvale, CA\nJune 2012 to June 2013\nAnalyzed hundreds of thousands of flight data points to identify opportunities for noise and emissions\nreduction in standard departure/arrival routes using Microsoft Excel extensively']","[u'MS IN ANALYTICS in ANALYTICS', u'BS IN MECHANICAL ENGINEERING in MECHANICAL ENGINEERING']","[u'UNIVERSITY OF SAN FRANCISCO\nJanuary 2015', u'UNIVERSITY OF CALIFORNIA Los Angeles, CA\nJanuary 2013']","degree_1 : MS IN ANALYTICS in ANALYTICS, degree_2 :  BS IN MECHANICAL ENGINEERING in MECHANICAL ENGINEERING"
0,https://resumes.indeed.com/resume/de465c648fb57f4a,"[u'Data Scientist\nRancho Biosciences - San Diego, CA\nMarch 2017 to Present\nData Scientist with solid background directly applicable to multiple industries including pharmaceutical companies and healthcare companies.', u'Genetics Data Scientist\nPathway Genomics - San Diego, CA\nMay 2016 to February 2017\n* Lead genomic diagnostic test development\n* Explore and visualize genomic big data to find patterns and build model', u'Assistant Project Scientist\nUNIVERSITY OF CALIFORNIA SAN DIEGO - La Jolla, CA\nSeptember 2007 to May 2016\n* Lead a drug discovery team implementing experimental and computational biological methods.', u'Research Assistant\nUNIVERSITY OF OKLAHOMA HEALTH SCIENCES CENTER - Oklahoma City, OK\nSeptember 2001 to August 2007\n* Solved multiple high-resolution protein structures.']","[u""Master's in Computer Science"", u""Bachelor's in Biotechnology""]","[u'Georgia Institute of Technology Atlanta, GA\nJanuary 2015 to January 2017', u'WUHAN UNIVERSITY Wuhan, China\nJanuary 1997 to January 2001']","degree_1 : ""Masters in Compter Science"", degree_2 :  ""Bachelors in Biotechnology"""
0,https://resumes.indeed.com/resume/500e0b8e640a2837,"[u'Data Scientist\nSynnex Corporation - Fermont, CA\nFebruary 2017 to Present\nResponsibilities:\n\u2022 Utilized Apache Spark with Python to develop and execute Big Data Analytics and Machine learning applications, executed machine Learning use cases under Spark ML and Mllib.\n\u2022 Identified areas of improvement in existing business by unearthing insights by analyzing vast amount of data using machine learning techniques.\n\u2022 Interpret problems and provides solutions to business problems using data analysis, data mining, optimization tools, and machine learning techniques and statistics.\n\u2022 Designed and developed NLP models for sentiment analysis.\n\u2022 Led discussions with users to gather business processes requirements and data requirements to develop a variety of Conceptual, Logical and Physical Data Models. Expert in Business Intelligence and Data Visualization tools: Tableau, Microstrategy.\n\u2022 Worked on machine learning on large size data using Spark and MapReduce.\n\u2022 Let the implementation of new statistical algorithms and operators on Hadoop and SQL platforms and utilized optimizations techniques, linear regressions, K-means clustering, Native Bayes and other approaches.\n\u2022 Developed Spark/Scala, Python for regular expression (regex) project in the Hadoop/Hive environment with Linux/Windows for big data resources.\n\u2022 Data sources are extracted, transformed and loaded to generate CSV data files with Python programming and SQL queries.\n\u2022 Stored and retrieved data from data-warehouses using Amazon Redshift.\n\u2022 Worked on TeradataSQL queries, Teradata Indexes, Utilities such as Mload, Tpump, Fast load and FastExport.\n\u2022 Used Data Warehousing Concepts like Ralph Kimball Methodology, Bill Inmon Methodology, OLAP, OLTP, Star Schema, Snow Flake Schema, Fact Table and Dimension Table.\n\u2022 Refined time-series data and validated mathematical models using analytical tools like R and SPSS to reduce forecasting errors.\n\u2022 Worked on data pre-processing and cleaning the data to perform feature engineering and performed data imputation techniques for the missing values in the dataset using Python.\n\u2022 Created Data Quality Scripts using SQL and Hive to validate successful dasta load and quality of the data. Created various types of data visualizations using Python and Tableau.\n\nEnvironment: Hadoop, Map Reduce, Spark, Spark MLLib, Tableau, SQL, Excel, VBA, SAS, Matlab, AWS, SPSS, Cassandra, Oracle, MongoDB, SQL Server 2012, DB2, T-SQL, PL/SQL, XML, Tableau.', u'Data Scientist\nFirst Atlantic Bank - Jacksonville, FL\nAugust 2014 to January 2017\nDescription: First Atlantic Bank gives subsidizes or advances to individuals with little business prerequisites. Candidates get their advances authorized taking into account their record of loan repayment. The candidate data is kept up in a database alongside the points of interest of the advance for reimbursement. This information is filtered into diverse classifications in light of parameters like kind of record, advance sum, due date. The filtered information is utilized for insights for producing report.\n\nResponsibilities:\n\u2022 Collaborates with cross-functional team in support of business case development and identifying modeling method (s) to provide business solutions. Determines the appropriate statistical and analytical methodologies to solve business problems within specific areas of expertise.\n\u2022 Generating Data Models using Erwin9.6 and developed relational database system and involved in Logical modeling using the Dimensional Modeling techniques such as Star Schema and Snow Flake Schema.\n\u2022 Guide the full lifecycle of a Hadoop solution, including requirements analysis, platform selection, technical architecture design, application design and development, testing, and deployment\n\u2022 Consult on broad areas including data science, spatial econometrics, machine learning, information technology and systems and economic policy with R\n\u2022 Performed Datamapping between source systems to Target systems, logicaldata modeling, created classdiagrams and ERdiagrams and used SQLqueries to filter data\n\u2022 Enabled speedy reviews and first mover advantages by using Oozie to automate data loading into the Hadoop Distributed File System and PIG to pre-process the data.\n\u2022 Used various techniques using R data structures to get the data in right format to be analyzed which is later used by other internal applications to calculate the thresholds.\n\u2022 Maintaining conceptual, logical and physical data models along with corresponding metadata.\n\u2022 Done data migration from an RDBMS to a NoSQL database, and gives the whole picture for data deployed in various data systems.\n\u2022 Developed triggers, stored procedures, functions and packages using cursors and ref cursor concepts associated with the project using PLSQL\n\u2022 Used Meta data tool for importing metadata from repository, new job categories and creating new data elements.\n\nEnvironment: R, Oracle 12c, MS-SQL Server, Hive, NoSQL, PL/SQL, MS- Visio, Informatica, T-SQL, SQL, Crystal Reports 2008, Java, SPSS, SAS, Tableau, Excel, HDFS, PIG, SSRS, SSIS, Metadata.', u'Data Scientist/Data Analyst\nAssurant Specialty Property - Santa Ana, CA\nMay 2013 to July 2014\nDescription: Assurant partners with leaders in mortgage lending, manufactured housing, multifamily housing and other industries to protect client and consumer property.\n\nResponsibilities:\n\u2022 Worked on data cleaning and reshaping, generated segmented subsets using Numpy and Pandas in Python\n\u2022 Wrote and optimized complex SQL queries involving multiple joins and advanced analytical functions to perform data extraction and merging from large volumes of historical data stored in Oracle 11g, validating the ETL processed data in target database\n\u2022 Identified the variables that significantly affect the target\n\u2022 Continuously collected business requirements during the whole project life cycle.\n\u2022 Conducted model optimization and comparison using stepwise function based on AIC value\n\u2022 Applied various machine learning algorithms and statistical modeling like decision tree, logistic regression, Gradient Boosting Machine to build predictive model using scikit-learn package in Python\n\u2022 Developed Python scripts to automate data sampling process. Ensured the data integrity by checking for completeness, duplication, accuracy, and consistency\n\u2022 Generated data analysis reports using Matplotlib, Tableau, successfully delivered and presented the results for C-level decision makers\n\u2022 Generated cost-benefit analysis to quantify the model implementation comparing with the former situation\n\u2022 Worked on model selection based on confusion matrices, minimized the Type II error\n\nEnvironment: Tableau 7, Python 2.6.8, Numpy, Pandas, Matplotlib, Scikit-Learn, MongoDB, Oracle 10g, SQL', u""Data Analyst/Data Modeler\nNestle - IN\nJanuary 2013 to June 2014\nDescription: The Nestle is a Swiss transnational food and drink company. Nestle's products include baby food, medical food, bottled water, breakfast cereals, coffee and tea, confectionery, dairy products, ice cream, frozen food, pet foods, and snacks.\n\nResponsibilities:\n\u2022 Created and maintained Logical and Physicalmodels for the data mart. Created partitions and indexes for the tables in the datamart.\n\u2022 Performed data profiling and analysis applied various data cleansing rules designed data standards and architecture/designed the relational models.\n\u2022 Developed SQLscripts for creating tables, Sequences, Triggers, views and materializedviews\n\u2022 Worked on query optimization and performance tuning using SQL Profiler and performance monitoring.\n\u2022 Developed mappings to load Fact and Dimension tables, SCD Type 1 and SCD Type 2 dimensions and Incremental loading and unit tested the mappings.\n\u2022 Utilized Erwin's forward/reverse engineering tools and target database schema conversion process.\n\u2022 Worked on creating enterprise wide Model EDM for products and services in Teradata Environment based on the data from PDM. Conceived, designed, developed and implemented this model from the scratch.\n\u2022 Involved in extensive DATA validation by writing several complex SQL queries and Involved in back-end testing and worked with data quality issues.\n\u2022 Developed and executed load scripts using Teradata client utilities MULTILOAD, FASTLOAD and BTEQ.\n\u2022 Exporting and importing the data between different platforms such as SAS, MS-Excel.\n\u2022 Generated periodic reports based on the statistical analysis of the data using SQL Server Reporting Services (SSRS)\n\u2022 Write SQLscripts to test the mappings and Developed Traceability Matrix of Business Requirements mapped to Test Scripts to ensure any Change Control in requirements leads to test case update.\n\nEnvironment: DB2, Oracle SQL Developer, PL/SQL, Business Objects, Erwin, MS office suite, Windows XP, TOAD, SQL*PLUS, SQL*LOADER."", u'Data Analyst\nAccenture - Bengaluru, Karnataka\nMay 2010 to December 2012\nDescription: Accenture is a global management consulting and professional services company that provides strategy, consulting, digital, technology and operations services.\n\nResponsibilities:\n\u2022 Designed different type of STARschemas for detailed data marts and plan data marts in the OLAP environment.\n\u2022 Developed and executed load scripts using Teradata client utilities MULTILOAD, FASTLOAD and BTEQ.\n\u2022 Responsible for development and testing of conversion programs for importing Data from text files into map Oracle Database utilizing PERL shell scripts & SQL*Loader.\n\u2022 Used Graphical Entity-Relationship Diagramming to create new database design via easy to use, graphical interface.\n\u2022 Responsible for development and testing of conversion programs for importing Data from text files into map Oracle Database utilizing PERL shell scripts &SQL*Loader.\n\u2022 Worked with the ETL team to document the Transformation Rules for Data Migration from OLTP to Warehouse Environment for reporting purposes.\n\u2022 Created SQLscripts to find dataquality issues and to identify keys, data anomalies, and data validation issues.\n\u2022 Formatting the data sets read into SAS by using Format statement in the data step as well as Proc Format.\n\u2022 Applied Business Objects best practices during development with a strong focus on reusability and better performance.\n\u2022 Developed Tableau visualizations and dashboards using Tableau Desktop.\n\u2022 Used Graphical Entity-Relationship Diagramming to create new database design via easy to use, graphical interface.\n\u2022 Co-ordinate with various business users, stakeholders and SME to get Functional expertise, design and business test scenarios review, UAT participation and validation of financial data.\n\u2022 Write SQL scripts to test the mappings and Developed Traceability Matrix of Business Requirements mapped to Test Scripts to ensure any Change Control in requirements leads to test case update.\n\nEnvironment: Oracle SQL Developer, PL/SQL, Business Objects, TOAD, Tableau, Informatica, MS SQL Server, SQL*PLUS, SQL*LOADER, XML.']","[u""Bachelor's""]",[u''],"degree_1 : ""Bachelors"""
0,https://resumes.indeed.com/resume/7c8ce5eaed3759bb,"[u'Data Scientist\nSykes Enterprises - Tampa, FL\nJanuary 2017 to January 2018\nThis Project AML Transaction Monitoring. Compliance programs depend on accurate and timely information. AML compliance centers on sifting through thousands of transactions and matching them against risk profiles. The result of that process is a focused examination of transactions and identification of suspicious.\n\u2022 Performed data extraction and analysis to develop business process mining model using BupaR.\n\u2022 Built forecast models using Prophet that improved planning and productivity by 25%.\n\u2022 Created Anomaly detection model, and auto answering chatbot to improve task management which was highly recognized by top management.\n\u2022 Developed SLA prediction models to help teams enhance work quality well before deadline.\n\u2022 Improved old models of SPSS into R framework by execution speed and accuracy by 20%.\n\u2022 Developed sophisticated data models to support automated reporting and analytics.\n\u2022 Analyzed & processed complex data sets using advanced query, visualization and analytics tools.\n\u2022 Collaborated with teams to develop and support internal data platform, ongoing analyses of client behavior and business outcomes, deployment of models on R server.\n\u2022 Performed data integrity checks, data cleaning, exploratory analysis and feature engineer\n\u2022 Developed personalized products recommendation with Machine Learning algorithms including collaborative filtering and Gradient Boosting Tree to meet the needs of existing customers and acquire new customers.\n\u2022 Coordinated the execution of A/B tests to measure the effectiveness of personalized recommendation system.\n\u2022 Recommended and evaluated marketing approaches based on quality analytics of customer consuming behavior.\n\u2022 Implemented Dynamic time wrapping for time series classification.\nEnvironment: R, Oracle 12c, Tableau.', u""Data Scientist\nAccenture - Mumbai, Maharashtra\nOctober 2011 to July 2016\nProject 1: Mining and Textile Data Analysis\nThe team of Data analysts focused on providing analytics insights and decision support tools for executives for accurate demand planning and task allocation.\n\u2022 Identified, measured and recommended improvement strategies for KPIs across all business areas.\n\u2022 Assisted in defining, implementing, and utilizing business metrics calculations and methodologies.\n\u2022 Managed a team of Research, Development and Analysis (RDA) professionals for 1 year.\n\u2022 Assisted in demand planning by delivering the accurate forecasts and allocation plans for tickets.\n\u2022 Provided analytical support to underwriting and pricing by preparing and analyzing data to be used in auctorial calculations.\n\u2022 Designed dashboards with Tableau and Meteor JS provided complex reports including summaries, charts, and graphs to interpret findings to team and stakeholders.\n\u2022 Identified process improvements that significantly reduce workloads or improve quality.\n\u2022 Worked for BI & Analytics team to conduct A/B testing, data extraction and exploratory analysis.\n\u2022 Generated dashboards and presented the analysis to researchers explaining insights on the data.\n\u2022 Improved probability predictions model for client to check the prices of raw material the next year.\nEnvironment: Excel 2010, R, MS SQL Server 200.\n\nProject 2: Insurance and Mining\nData Analyst\nThe team of developers and consultant worked across multiple projects to implement OLAP and OLTP systems, data modeling, system migration from old data warehouse and system maintenance.\n\u2022 Built data pipelines from multiple data sources by performing necessary ETL tasks.\n\u2022 Performed Exploratory Data Analysis using R, Apache Spark and text analysis, tri-idf analysis.\n\u2022 Worked on Data Cleaning, features scaling, features engineering.\n\u2022 Handle natural language processing to extract features from text data.\n\u2022 Visualized bigrams networks to investigate individual importance.\n\u2022 Built a forecasting model to predict future sales for anti-diabetes vaccines in global market.\n\u2022 Built multiple time-series models like ARIMA, ARIMAX (Dynamic Regression), TBATS, ETS.\n\u2022 Evaluated model's performance on multiple test metrics such as MAPE, MAE & MASE.\n\u2022 Developed a shiny app to highlight Bayesian analysis and performed visualizations with ggplot2.\nEnvironment: Oracle, SQL.""]","[u""Master's in Management Information Systems in Management Information Systems"", u'Bachelor of Engineering in Engineering']","[u'University of South Florida', u'Mumbai University Mumbai, Maharashtra']","degree_1 : ""Masters in Management Information Systems in Management Information Systems"", degree_2 :  Bachelor of Engineering in Engineering"
0,https://resumes.indeed.com/resume/466e976dc225ce1e,"[u'Data Scientist\nHELMERICH & PAYNE INC - Tulsa, OK\nMay 2017 to December 2017\n\u2022 Built production-ready Machine Learning (Random Forest and Regressions) models using Base SAS, Python for Oil Drilling Dept. and predicted the wear and tear of various parts of the Oil Rigs machinery, which resulted in 26% decrease in cost of production.\n\u2022 Created Tableau dashboards for improving reporting process in real-time data streams and for easy user interactivity. Integrated and visualized the Python Models in Tableau using TabPy Framework.', u'Graduate Research Assistant\nOKLAHOMA STATE UNIVERSITY - Stillwater, OK\nJanuary 2017 to December 2017\n\u2022 Conducted research and developed machine-learning models like Na\xefve Bayes, clustering and classification in Python for Threat (Spam) Detection Analytics Research project.\n\u2022 Consolidated the research project into a report conveying clustering approach combined with SVM classifier can be best modelling technique to detect the spam.\n\u2022 Collaborated with students in class of ""Programming for Data Science"" with R and Python.', u""Data Scientist\nINFOSYS LIMITED - Hyderabad, Telangana\nApril 2014 to July 2016\n\u2022 Led initiative to build statistical models using historical data to predict sales of \u2018Nike' merchandise in several economic markets by taking various external factors into consideration. Focused on analyzing the factors affecting the quantity of sales in United States.\n\u2022 Conducted data preparation, descriptive statistics and model building using SQL Server, Base SAS programming, SAS Enterprise Miner and Python.\n\u2022 Developed customer segmentation algorithm (K-Means Clustering) in Python, which scored sales leads for sales and lead to increased market share when compared to its competitors.\n\u2022 Applied data mining to online marketing campaigns for various categories of merchandise, which brought in savings of $1.6 million over 3 quarters in year 2015.\n\u2022 Created and presented executive dashboards and scorecards to show the trends in the data using Excel, Tableau and created SQL/ETL queries for generating automated daily/weekly reports, creating relevant KPIs and Metrics.""]","[u'Masters in MIS in Data Science', u""Bachelor's in Computer Science""]","[u'Oklahoma State University Stillwater, OK\nDecember 2017', u'JNT University Hyderabad, Telangana\nAugust 2011 to May 2015']","degree_1 : Masters in MIS in Data Science, degree_2 :  ""Bachelors in Compter Science"""
0,https://resumes.indeed.com/resume/bb63d4fd5df56dba,"[u'Senior Cyber-Data Scientist\nPurisolve, LLC - Lanham-Seabrook, MD\nOctober 2017 to Present\nOctober 2017 - Present\n@The Internal Revenue Service (Business UnderReporting Program) Senior Cyber -Data Scientist\n* Supported the BUR case selection process providing analytical\nrecommendations in support of business operations.\n* Provided architectural and schema expertise in the construction\nof fact and dimensional tables for data warehouses.\n* Responsible for the development of the data science/data\narchitectural vision, strategy, technology innovations, and\ndata architecture used for data modeling, analytics, and data\nvisualization.\n* Implemented self-developed data science life-cycle for all data\nthe case processing systems at the IRS.\n* Developed mapping documentation identifying all source data\nsystems, forming and integrating large datasets, and analyzing\ndata collected in support of ongoing analytical projects.\n* Applied latest technologies used in the capture and storage of\ndata including the use of data warehouses for transactional\ndata and data lakes for unstructured data.\n* Automated the report generation process by using SAS, SPSS,\nSQL, and additional scripting languages.\n* Developed the needed data schemas used to load source system\ndata, assess data quality, and develop tuning schema in support\nof analytical requirements by business processes.\n* Perform the analysis, design, development, and testing\nactivities in support of new Splunk indexing and reporting.\n* Implemented procedures used to convert the legacy reporting\nenvironment to the new BUR reporting environment including the\ndevelopment of web and mobile dashboards.\n* Applied machine learning expertise using regression analysis,\ntime series, probabilistic models, supervised classification\nand unsupervised learning activities.\n* Validated extraction, transformation, and load processes used\nin development of large analytical databases.\n* Developed new data models using data architecture tools such as\nErwin & Power Designer implementing conceptual, logical, and\nphysical data models including reengineering these models for\nvarious databases.\n* Defined all legacy data flows by source allowing data to be\nexchanged between various systems ensuring all legacy\napplication data is collected, stored, and retired.\n* Experienced with Big Data concepts and technologies including\nthe development of complex data models.\n* Developed data dictionaries for new database applications\naddressing undocumented business processes. Prepared high\nquality design documentation, including visual diagrams,\ncomplex data models, and data flows.\n* Maintains and updates Database Design Documents (DDDs) for\nassociated business processes.\n* Experienced project management using the Agile SCRUM\ndevelopment processes.\n* Defined, monitored, and developed data standards, data quality\nvalidation, and data management procedures used in the\nintegration of systems throughout the organization.', u'Senior Cyber -Data Scientist\nPurisolve, LLC @IRS Office of Criminal Investigations - Lanham, MD\nOctober 2016 to October 2017\nSenior Cyber -Data Scientist\n* Developed supporting CDM data applications used to\nimplement the CDM Current & Future State in accordance\nwith specifications developed by the IRS, Department of\nthe Treasury, and Department of Homeland Security.\n* Strategic advisor developing data management and\narchitecture solutions in supporting several business\nunits.\n* Prepared CDM information delivery solutions for current &\nfuture state cyber tools to feed various Splunk indexes.\n* Implemented data requirements collections for each cyber\ntool for existing and desired state for each business\nunit.\n* Collaborated with all business processes ensuring business\nprocess cross-functionality and data strategy alignment.\n* Identified and assessed cyber tools data requirements in\nthe preparation of the cyber CDM data warehouse.\n* Evaluated DHS Master CDM identifying the amount of\ncustomization needed to meet agency requirements.\n* Planned Splunk requirements needed to feed the CDM agency\ndashboard using Archer visualization technologies.\n* Assessed data requirements feeds from cyber tools, indexed\nthem in Splunk in order to feed agency dashboard.\n* Evaluated data flow by business unit in support of the\ndevelopment of agency data governance strategy.\n* Provided high-level guidance and hands-on support for data\nmigration and data cleansing projects.\n* Determined IRS FISMA analytics reporting requirements for ""As is State"" and ""Future State"" in support of Homeland\nSecurity, the Department of the Treasury monthly,\nquarterly, and annual reporting requirements.\n* Defined data collection requirements from cyber sources &\nSplunk environments supply IRS specific dashboards.\n* Worked to define all data collected by each cyber tool,\nhow it is stored, and finding ways to automate data feeds.\n* Captured CDM business projects requirements helping to\ndefine CDM ""As is State"", and ""Future State"" including the\ndevelopment of business process flow diagrams for HWAM,\nSWAM, CSM, & VUL management.\n* Reviewed data requirements for CDM data model versions 1\nand 2 as the holder of IRS CDM cyber data.\n* Converted DHS CDM data model\'s Version 1 & 2 into working\nIRS logical data model using a data architecture tool to\nproviding data engineering for several forms of database\nmanagement systems.\n* Assessed sunseting cyber tool data requirements for\ninclusion into CDM Future State cyber environment.\n* Completed architecture assessment and recommend needed\ncustomization of DHS CDM data model.\n* Established knowledge transfer process from DHS CDM data\narchitecture expert to the IRS Cyber team.', u""Senior Strategic Program Manager\nAssociated Veterans, LLC @ the Veterans Administration - Washington, DC\nAugust 2015 to October 2016\nWashington, DC Senior Strategic Program Manager\n* Provided strategic program management direction and data\nassessment for the VRM PMO supporting 27 projects.\n* Supported the VA deputy director on data governance (CDO)\nestablishing overall agency data management strategy.\n* Lead the highly visible White House Contact Data\nIntegration project seeking to develop a single veteran\nsign on.\n* Developed the EA&I development strategy used to transform\nnew product offered by the division of VRM.\n* Worked with VA's Deputy Director developing new agency\napplications in analytics, business intelligence,\nknowledge management, and assessing data flows initiatives\nacross the agency.\n* Developed data warehouse supporting business processes\nincluding data usage, database architecture, and metadata\nand repository creation.\n* Determined data used by all 27 VA business units allowing\nthe integration of data across all business processes.\n* Helped to define future cloud technology needs using AWS\nbased upon line of business operations.\n* Supported the agency CDO, business analysts, SME's, and\ndevelopment teams in the design of the MDM projects.\n* Managed data-driven projects involving project timelines,\nvendor relationships, and budgets to ensure deliverables\nwere met within specified timeframes.\n* Developed ETL strategies supporting analytical project\ndesign & implementation for cyber and business processes.\n* Owner of the data collection, analytics, and governance\nprocess with a focus on data strategy, analytics projects,\ngovernance, knowledge management, data quality, data\nintegrity, in support of enterprise strategies.\n* Proactively championed the value of data as a strategic\nbusiness asset and revenue generator by providing the\nbusiness and strategy teams with ideas and being able to\ntransform those ideas into action.\n* Obtained stakeholder & project SME buy-in across multi-\nfunctional environments and business units.\n* Developed data collection strategies used to manage and\nsharing data across the VA business units.\n* Led the technology assessment in standing up the new data\ncenter while acting as the VRM lead in Dynamics.\n* Provides solutions to improve data access efficiency for\ncurrent and legacy applications within the organization."", u""Business Intelligence Practice Lead\nThink Logically, LLC - Fulton, MD\nJanuary 2002 to August 2015\n* Senior program manager with over 15 years' experience\nmanaging over 30 BI and financial projects.\n* Led the delivery of BI, analytics, and data warehouse\nsolutions raising organizational revenues by 50%.\n* Designed and implemented the information architecture for\npartners and clients of the organization.\n* Creation of information requirements, principles, context\ndiagrams, conceptual, logical, and physical data models.\n* Served as data and information champion creating and\nsustaining data driven technologies, processes, and\npolicies.\n* Implemented data architecture, vision and strategy,\ninnovations, and enterprise architecture services.\n* Provide superior leadership skills with the proven ability\nto lead change and apply leadership models.\n* Facilitated the design the new revenue sources used to\nenable and challenge thinking within the organization.\n* Develop, execute, and modify client IT strategic plan and\nset the overall direction for the organization.\n* Mentored data architects, developers, and analyst of all\nlevels of industry best practices, procedures, and\nconcepts.\n* Collaborated with client IT development staffs to develop\ninternal compliance controls for SOX, PCI, and HIPPA.\n* Led the development and execution of enterprise-wide\ndisaster wide continuity plan for all clients.\n* Designed the professional services resource model and\nincentives necessary to deliver repeatable revenue growth\nand the generation of standard data architecture for the\nfinancial and retail industries.\n* Participated in meetings with technology and BI business\nteams to facilitate the understanding, clarification, and\nimplementation of organizational data requirements.\n* Researched, identified, and acquired several high dollar\ncontracts for the organization.\n* Built several high-performance project teams of BI, CRM,\nERP, KM, and software engineering professionals.\n* Established data governance processes aligning processes\nto organization's business needs."", u""Chief Information Officer\nDirect Web Inc - Mount Laurel, NJ\nJanuary 1998 to January 2002\n* Oversaw an information technology budget of $25 million\nfor several domestic and global locations.\n* Senior data and business intelligence individual\nresponsible for developing new BI applications.\n* Worked extensively with venture capital representative\ndefining new BI revenue generating models.\n* Led a staff of 55 individuals in the disciplines of\ncustomer support, software engineering, and data center.\n* Senior manager tasked with the design and implementation\nof several new e-commerce applications.\n* Provided expertise and consultation on all data driven\ninitiatives for the organization.\n* Optimized IBM's data vendor management and conducted\nvendor performance and risk assessments.\n* Worked directly with the CEO and COO developing new\nbusiness and revenue operations.\n* Adept at leading the development and launching new\ntechnologies such as BI and portals.\n* Integrated call and data center technologies with Voice\nover IP to enhance customer retention.\n* Directed the new practice areas of data analytics and\ncyber security generating new revenue sources.\n* Identified all architectural risk ensuring a mitigation\nstrategy was in place for practice area.""]","[u'Ph.D. in Data Science/Finance', u'Master of Science in Computer Systems Management/Database Architecture/Cyber Security', u'Bachelor of Science in Information Systems/Accounting and Finance']","[u'University of Phoenix School of Advanced Studies\nJune 2018', u'University of Maryland University College', u'University of Maryland University College']","degree_1 : Ph.D. in Data Science/Finance, degree_2 :  Master of Science in Compter Systems Management/Database Architectre/Cyber Secrity, degree_3 :  Bachelor of Science in Information Systems/Acconting and Finance"
0,https://resumes.indeed.com/resume/46de70aa0481bd6e,"[u'Contract Data Scientist\nSakura Sky\nDecember 2016 to Present\nWith another data scientist/programmer, refactored a large VBA/Excel program to Python/NumPy/Xarray. The program runs multiple dynamic programming algorithms in sequence.\nAccuracy of results and speed of execution were critical goals. Our results increased execution\nspeed by a factor of 12\u221225. (Further vectorizations are pending.)\nThe refactoring posed several challenges: the original program was written by several programmers at different times and was known to be buggy. It was driven by an Excel workbook which carried\nout numerous calculations, which also had to be included in the Python program. The current\nmaintainers were not programmers and could only advise us on the DP algorithms.', u""Managing Partner / Data Scientist\nPiscataqua Capital Management, LLC\nJanuary 2012 to January 2016\nPCM was a startup quantitative asset management firm that specialized in providing investment\nsolutions to the institutional marketplace. It was built around a core of three of us from UBS (see\nbelow). Together, we designed and implemented our own quantitative investment processes from scratch. We chose Thomson Reuters QA Studio (QAS) as our market analytics and data delivery\nplatform and off-the-shelf software where possible for We data scientists chose all our own tools for programming PCM's investment process. R was our primary programming tool.\nHighlights of PCM work in which I played a lead or major role:\n\u2022 Created a new equity scoring (alpha prediction) procedure using machine learning methods.\nCreated a new portfolio construction/rebalancing method including built-in risk controls and a portfolio simulation (backtesting) procedure, including portfolio analytics and graphical\ndisplays.\n\u2022 Wrote approximately half our 16,000 lines of R code. Wrote over 5,000 lines of Hedgehog\n(QAS's programming language) code to access and preprocess the data, build factors, and drive\nthe QAS version of backtesting.\n\u2022 As a member of the Board of Managers, I reviewed contracts, budgets, our IT compliance policy,\nour business continuity and disaster recovery plan, etc."", u""Statistician / Programmer / (contract employee)\nUBS Global Asset Management / DSI International Management, Inc\nJanuary 1996 to January 2012\nDSI was a quantitative asset management firm with AUM that grew to $6.6 billion. It started as independent, but after two acquisitions became a unit within UBS Global Asset Management.\nMark Abrahams and I gradually took over and expanded on the DSI work of Professor Leo\nBreiman, originally on the faculty at UCLA and later at UC Berkeley, who consulted with DSI\nfrom its inception until his passing in 2005. Our main projects included:\n\u2022 Design proprietary algorithms for equity scoring (alpha prediction), portfolio construction and portfolio analytics.\n\u2022 Design, code, and maintain a suite of programs to implement the above - code them in C and Python. The code base we wrote during our tenure at DSI/UBS grew to over 65,000 lines of C\nand over 45,000 lines of Python, of which I wrote about half. I created a user-friendly GUI\nfrontend to our programs in wxPython. Until then, the programs were exclusively driven by command scripts and complex parameter files. We also wrote the technical documentation for\nour programs and algorithms (over 300 pages). We translated the core of the scoring portion of our proprietary process into Octave/Matlab (about 2400 lines of Matlab code in total).\n\u2022 Data cleaning, validation, and reformulation.\nInformation Services & Technology, U.C. Berkeley. Manager, Academic UNIX Applications.\nIS&T offers administrative and academic computing support to the entire campus, split between large shared computers and individual PC's. Supervised five full-time application programmers\nand consultants. Managed all applications software on all IS&T UNIX systems: software selection,\ninstallation, and maintenance; a consulting service; short courses; contributions to IS&T's\nnewsletter. Made high level decisions for the Central Campus Systems department in areas that\naffect IS&T UNIX services. Received awards and formal recognitions for my leadership roles.\nPrior Professional Experience includes design, programming, data analysis, and application\ntesting for the BLSS (Berkeley Interactive Statistical System) Project.\n\n14 March 2018""]","[u'B.A. in Mathematics', u'Ph.D. in Statistics', u'M.S. Engineering in Operations Research']","[u'California State University\nFebruary 2018', u'University of California, Berkeley, California Berkeley, CA', u'University of California, Berkeley, California Berkeley, CA']","degree_1 : B.A. in Mathematics, degree_2 :  Ph.D. in Statistics, degree_3 :  M.S. Engineering in Operations Research"
0,https://resumes.indeed.com/resume/11d5808a8db74653,"[u'GIS Analyst Volunteer\nTown of Superior, CO\nFebruary 2018 to Present\nMaking infrastructure maps', u'GIS Technician Volunteer\nUnited States Geologic Survey\nAugust 2017 to November 2017\n\u2022 Worked on a process to enable batch downloads of Landsat data from Google Earth Engine.\n\u2022 Classified burned areas in Alaska using high resolution data.', u""Image Data Scientist II\nNASA Johnson Space Center\nAugust 2016 to August 2017\nAugust 2016 - August 2017\n\u2022 Developed automated feature detection for NASA's International Space Station image\ndatabase, by utilizing image processing, computer vision, and machine learning software to detect lightning, clouds, aurora, and other features.\n\u2022 Created a Quantum GIS plugin and GUI to visualize and analyze geo-referenced lunar spectral\ndata.\n\u2022 Increased safety of NASA's Object Reentry Survival Analysis Tool by developing an automated\nCAD Python program for 3D viewing of the modeled objects."", u'Software Engineer II\nNational Center for Atmospheric Research\nJanuary 2014 to January 2014\nDesigned a data processing pipeline for Mauna Loa Observatory Coronal Multichannel\nPolarimeter Instrument solar maps.', u""Associate Scientist III\nUniversity of Colorado/Cooperative Institute for Research in Environmental Sciences\nJanuary 2001 to January 2014\nCIRES)\nNational Oceanic and Atmospheric Administration, Space Weather Prediction Center\nAssociate Scientist III 2001-2014\n\u2022 Determined metadata and format for solar wind raster output data.\n\u2022 Constructed scripts to automatically retrieve solar maps, perform a geographic\ntransformation, interpolate the raster data, and run a solar wind prediction model.\nAutomatically visualized the results and uploaded the predicted map of solar wind speed to the WSA Space Weather Prediction Center web page.\n\u2022 Designed and visualized maps of the D-Region Absorption values which is used to determine\nHF radio degradation. Wrote scripts to automate map generation and display on the D-RAP\nweb page.\n\u2022 Interpolated raster maps back and forth between the ionospheric and air pressure grids.\n\nAwards:\n\u2022 CIRES Science and Engineering Award for successful delivery of the Phase One Space Weather\nalgorithms for GOES-R, 2010.\n\u2022 CIRES Silver Medal Award for developing and transitioning the Nation's first space weather\nmodel, 2013""]","[u'Certificate in Geographic Information Systems', u'B.S. in Computer Science and Mathematics', u'M.S. in Atmospheric Science']","[u'Front Range Community College Longmont, CO', u'Emory University Atlanta, GA', u'Georgia Institute of Technology Atlanta, GA']","degree_1 : Certificate in Geographic Information Systems, degree_2 :  B.S. in Compter Science and Mathematics, degree_3 :  M.S. in Atmospheric Science"
0,https://resumes.indeed.com/resume/d24cc25271e2b2ee,"[u'Teaching Assistant - Statistics\nUniversity of Texas at Austin - Austin, TX\nSeptember 2017 to Present\n\u2022 Teach statistics and modeling for 400 students (hypothesis test, regression, decision tree)\n\u2022 Coordinate office hour and teach R language.', u'Data Scientist Intern\nSparkCognition - Austin, TX\nMay 2017 to August 2017\n\u2022 Apply deep learning for financial portfolio optimization, increaseing 30% return.\n\u2022 Build AI trading model using RNN recurrent neural network (Python, TensorFlow, LSTM)\n\u2022 Optimize model parameters by Bayesian optimization in parallel computing. (google cloud)\nProject 2: Gas facility failure detection\n\u2022 Analyze 1 million sensor data with 2K features in unsupervised approach.\n\u2022 Perform data imputation, reduce features dimension by Autoencoder, apply density-based clustering\n\u2022 Detect key cluster label that matches 90% with failure events.', u'Research Assistant\nThe University of Texas - Austin, TX\nAugust 2014 to May 2017\n\u2022 Collect nuclear magnetic data for rock, apply moving average to suppress noise. (matlab, python)\n\u2022 Compute loss function using ridge regression, solve inversion problem by gradient descent.\n\u2022 Normalize data, apply clustering methods for fluid typing (k-means, hierarchical, gaussian mixture)\n\u2022 Implement deep learning (CNN) using ImageNet to classify 2000+ rock images with 95% accuracy.', u'Data Scientist Intern\nING Energy Solutions - Austin, TX\nSeptember 2016 to January 2017\n\u2022 Design abnormal detection model for drilling optimization, Achieved 100% accuracy in field tests.\n\u2022 Develop algorithm pipeline for real-time data acquisition, modeling, and visualization.\n\u2022 Build failure risk model using machine learning with physical model for 10K sensor data.']","[u'PhD in Petroleum Engineering', u""Master's in Civil Engineering"", u'BS in Geophysics']","[u'University of Texas at Austin Austin, TX\nAugust 2014 to December 2018', u'Institute of Geophysics\nAugust 2011 to July 2014', u'University of Science and Technology of China\nSeptember 2007 to July 2011']","degree_1 : PhD in Petrolem Engineering, degree_2 :  ""Masters in Civil Engineering"", degree_3 :  BS in Geophysics"
0,https://resumes.indeed.com/resume/277f6e1082540fe4,"[u'Power BI Developer\nKeurig Green Mountain Inc - Burlington, MA\nFebruary 2018 to Present\nResponsibilities:\n\u2022 Leverage skillsets in ETL methodologies and data manipulation requiring complex SQL coding\n\u2022 Organizing complex enterprise data into compelling presentations to satisfy analytical and operational needs of the client\n\u2022 Participate with business analysts to gather requirements; produce technical specifications and deliver ETL processes and reporting solutions according to specifications\n\u2022 Developing Power BI dashboards (Power BI Desktop / Online)\n\u2022 Migrating current SSRS reporting to Power BI dashboards with changes in data sources.\n\u2022 Performing Project-related and Ad-Hoc (Data) Analysis - Including Deep-Dive Analysis', u'DATA INTEGRITY SPECIALIST\nPDT Partners - New York, NY\nJanuary 2018 to February 2018\nResponsibilities:\n\u2022 Worked on the corporate actions data (rights offering, cash dividend, stock dividend) coming from vendors such as Bloomberg, Thomson Reuters and Fidelity Investments.\n\u2022 Checked for the consistency of corporate actions data and communicated with the representatives from Bloomberg, Thomson Reuters and Fidelity for exceptions in the data\n\u2022 Used SQL to query from SQL Server database for finding metrics such as Average Liquidity and Portfolios', u'DATA INTEGRITY SPECIALIST, VentureWell\nI- Corps - Hadley, MA\nFebruary 2017 to December 2017\nResponsibilities:\n\u2022 Cleaned, Matched, Merged and migrated the event registration data of I- Corps program funded by National Science Foundation (NSF) to Salesforce CRM using Python scripts and Data Loader of Salesforce\n\u2022 Migrated and Pre-processed the Intake Survey Data worth of 5 years to Salesforce CRM using R scripts and Data Loader of Salesforce\n\u2022 Built Predictive and Classification models on top of the Survey Data using R to predict the satisfaction of participants about the National Science Foundation (NSF) I-Corps course structure\n\u2022 Created custom Reports and Dashboards in Salesforce, Tableau and Power BI upon ad-hoc requests from\n\u2022 Designed workflow rules, Validation rules, custom objects, fields, records, privacy requirements and set up the page layouts appropriately in Salesforce\n\u2022 Provided end user training and support in finding and using data for organization needs in Salesforce', u'DATA SCIENTIST CO-OP\nJP Morgan Chase & Co - New York, NY\nJanuary 2016 to December 2016\nResponsibilities:\n\u2022 Worked on a research aimed at the case of customers default payments in Taiwan and compared the predictive accuracy of probability of default among three data mining methods.\n\u2022 The data obtained was in .xls for mat with 30000 number of instances and 24 number of attr ibutes\n\u2022 Used Power BI, Excel Pivot and Tableau to visualize, understand and detect the anomalies in the raw data\n\u2022 Performed Cleansing and Pre-processing of raw data using R programming language\n\u2022 Built Statistical models using Logistic Regression, Regression Tress and Artificial Neural Networks for classifying each customer as a default or a non-default\n\u2022 Compared the predictive accuracy of probability of default among the three data mining methods and chose classification tree as the best method with error rate being 17%\n\u2022 Submitted the research to risk management team to help them estimate the risk and the factors the company should consider while issuing the credit cards to people in Taiwan']","[u'Master of Science in Information Systems in Data Sciences and Machine Learning', u'Bachelor of Engineering in Computer Science in Redis clusters and created a connection']","[u'Northeastern University - College of Engineering Boston, MA\nSeptember 2014 to December 2016', u'Amrita University Coimbatore, Tamil Nadu\nAugust 2009 to May 2013']","degree_1 : Master of Science in Information Systems in Data Sciences and Machine Learning, degree_2 :  Bachelor of Engineering in Compter Science in Redis clsters and created a connection"
0,https://resumes.indeed.com/resume/0ba960713eb24484,"[u'Data Scientist\nSchool of Professional Studies, CUNY\nJanuary 2016 to January 2018', u'Stony Brook University, SUNY\nJanuary 2010 to January 2014\nBA, Political Science (GPA: 3.0/4.0) Stony Brook University, SUNY\n\n\u2022 (917) 705 4637\n\n\u2022 michaelgmuller.com Trending Expertise\n\u2022 expediency@live.com Recommender Systems \u2022 Machine Learning Algorithms \u2022 Data Visualization\n\n\u2022 /in/michael-muller-30394097 Experience\n\u2022 parastyle Jan 2016 - Candidate for MS in Data Science School of Professional Studies\nPresent\n\u2022 Currently undergoing accelerated graduate studies at CUNY for\nData Science. Courses include Statistics, Computational Mathe-']",[],[],degree_1 : 
0,https://resumes.indeed.com/resume/2a787e6c48364564,"[u'Data Scientist\nRoyalcaribbean - Miami, FL\nJune 2017 to Present\nResponsibilities:\n\u2022 Massively involved in Data Architect role to review business requirement and compose source to target data mapping documents.\n\u2022 Responsible for the data architecture design delivery, data model development, review, approval and Data warehouse implementation.\n\u2022 Set strategy and oversee design for significant data modeling work, such as Enterprise Logical Models, Conformed Dimensions, and Enterprise Hierarchy.\n\u2022 Analyzed existing Conceptual and Physical data models and altered them using Erwin to support enhancements.\n\u2022 Designed the Logical Data Model with the entities and attributes for each subject areas.\n\u2022 Lead Architectural Design in Big Data, Hadoop projects and provide for a designer that is an idea-driven.\n\u2022 Developed and configured on Informatica MDM hub supports the Master Data Management (MDM), Business Intelligence (BI) and Data Warehousing platforms to meet business needs.\n\u2022 Loaded data into Hive Tables from Hadoop Distributed File System (HDFS) to provide SQL access on Hadoop data\n\u2022 Used Agile Methodology of Data Warehouse development.\n\u2022 Design and implement data ingestion techniques for real time and batch processes for structured and unstructured data sources into Hadoop ecosystems and HDFS clusters.\n\u2022 Designed and developed architecture for data services ecosystem spanning Relational, NoSQL, and Big Data technologies.\n\u2022 Implemented multi-data center and multi-rack Cassandra cluster.\n\u2022 Created HBase tables to load large sets of structured, semi-structured and unstructured data coming from NoSQL and a variety of portfolios.\n\u2022 Involved in data model reviews as data architect with business analysts and business users with explanation of the data model to make sure it is in-line with business requirements.\n\u2022 Created Entity relationships diagrams, data flow diagrams and enforced all referential integrity constraints using Rational Rose\n\u2022 Worked with the ETL team to document the SSIS packages for data extraction to Warehouse environment for reporting purposes.\n\u2022 Developed data Mart for the base data in Star Schema, Snow-Flake Schema involved in developing the data warehouse for the database.\n\u2022 Involved in Data loading using PL\\SQL Scripts and SQL Server Integration Services packages\n\u2022 Established data governance, monitoring of Data Quality and clear documentation for facile implementation.\n\u2022 Involved in the validation of the OLAP, Unit testing and System Testing of the OLAP Report Functionality and data displayed in the reports.\n\u2022 Generated ad-hoc SQL queries using joins, database connections and transformation rules to fetch data from Teradata database.\n\u2022 Created HBase tables to load large sets of structured, semi-structured and unstructured data coming from UNIX, NoSQL and a variety of portfolios.\n\u2022 Worked on Amazon Redshift and AWS and architecting a solution to load data creates data models and run BI on it.\n\u2022 Created UNIX scripts for file transfer and file manipulation\n\u2022 Directed to Create Dashboards based on the business requirement using SSRS/Cognos and helped development team in knowledge about the requirement.\n\u2022 Handled importing of data from various data sources, performed transformations using Hive, Map Reduce, loaded data into HDFS and Extracted the data from Oracle into HDFS using Sqoop\n\u2022 Worked with various Teradata15 tools and utilities like Teradata Viewpoint, Multi Load, ARC, Teradata Administrator, BTEQ and other Teradata Utilities.\n\u2022 Involved in several facets of MDM implementations including Data Profiling, Metadata acquisition and data migration.\n\u2022 Extensively used Aginity Netezza work bench to perform various DML, DDL etc operations on Netezza database.\n\u2022 Created DDL scripts using Erwin and source to target mappings to bring the data from source to the warehouse.\n\u2022 Lead database level tuning and optimization in support of application development teams on an ad-hoc basis.\n\nEnvironment: HDFS, AWS Redshift, MapReduce, Hive 2.3, HBase, MongoDB, Cassandra, Metadata, Netezza, MySQL, Hadoop 3.0, ODS, Oracle 12c, T-SQL, MDM, PL/SQL, Teradata R15, Teradata SQL Assistant 15.0, Flat Files.', u'Data Scientist\nPhilips Healthcare - Baltimore, MD\nMarch 2016 to May 2017\nResponsibilities:\n\u2022 Understand the high level design choices and the defined technical standards for software coding, tools and platforms and ensure adherence to the same\n\u2022 Implemented the NoSQL database HBase and the management of the other tools and process observed running on YARN.\n\u2022 Used Agile Methodology of Data Warehouse development using Kanbanize.\n\u2022 Developed Data Mapping, Data Governance, and Transformation and cleansing rules for the Master Data Management (MDM) Architecture involving OLTP, ODS.\n\u2022 Extensively worked on creating the migration plan to Amazon web services (AWS).\n\u2022 Extracted Mega Data from Amazon Redshift, AWS, and Elastic Search engine using SQL Queries to create reports.\n\u2022 Analyze business requirements and build logical data models that describe all the data and relationships between the data\n\u2022 Designed both 3NF data models for ODS, OLTP systems and dimensional data models using Star and Snow Flake Schemas\n\u2022 Provided suggestion to implement multitasking for existing Hive Architecture in Hadoop also suggested UI customization in Hadoop\n\u2022 Architect and lead significant data initiatives in various data dimensions Master Data, Meta Data, Big Data & Analytics.\n\u2022 Involved in Planning, Defining and Designing database using ER Studio on business requirement and provided documentation.\n\u2022 Translate business and data requirements into logical data models in support of Enterprise Data Models, Operational Data Structures and Analytical systems.\n\u2022 Partner with DBAs to transform logical data models into physical database designs while optimizing the performance and maintainability of the physical database\n\u2022 Work with Data Management to establish governance processes around metadata to ensure an integrated definition of data for enterprise information, and to ensure the accuracy, validity, and reusability of metadata.\n\u2022 Developed Full life cycle of Data Lake, Data Warehouse with Big data technologies like Spark and Hadoop.\n\u2022 Applied all phases of the Software Development Life Cycle, which include requirements definition, analysis, review of design and development, and integration and test of solution into the operational environment\n\u2022 Responsible for full data loads from production to AWS Redshift staging environment.\n\u2022 Developed Map Reduce programs to cleanse the data in HDFS obtained from heterogeneous data sources to make it\n\u2022 Provided optimal design for structured and non-structured data using SQL and NoSQL databases.\n\u2022 Created data schema and architecture of data warehouse for standardized data storage and access\n\u2022 Designed data models with industry standards up to 3NF (OLTP/ODS) and de-normalized (OLAP) data marts with Star & Snow flake schemas.\n\u2022 Generated and DDL (Data Definition Language) scripts using ER Studio and assisted DBA in Physical Implementation of Data Models.\n\u2022 Used data profiling automation to uncover the characteristics of the data and the relationships between data sources before any data-driven.\n\u2022 Develop test scripts for testing sourced data and their validation and transformation when persisting in data stores that are physical representations of the data models\n\u2022 Designed and documented Use Cases, Activity Diagrams, Sequence Diagrams, OOD (Object Oriented Design) using UML and Visio.\n\u2022 Completed enhancement for MDM (Master data management) and suggested the implementation for hybrid MDM (Master Data Management)\n\u2022 Developed and implemented data cleansing, data security, data profiling and data monitoring processes.\n\u2022 Generated ad-hoc reports using Crystal Reports XI.\n\u2022 Designed processes and jobs to source data from Mainframe sources to HDFS staging zone\n\u2022 Integrated data from multiples sources including HDFS to Hive Data warehouse.\n\u2022 Worked very close with Data Architectures and DBA team to implement data model changes in database in all environments.\n\nEnvironment: ER Studio 9.6, Hive 2.3, Hadoop 3.0, MDM, AWS, Redshift, HDFS, Teradata 14, PL/SQL, Informatica 9.0, Oracle 12c, UNIX.', u""Data Scientist\nSteve Madden Ltd - Long Island City, NY\nNovember 2014 to February 2016\nDescription: Steven Madden, Ltd., together with its subsidiaries, designs, sources, markets, and sells fashion-forward name brand and private label footwear for women, men, and children. It offers wholesale footwear under the Steve Madden Women's, Madden Girl, Steve Madden Men's, Steven.\nResponsibilities:\n\n\u2022 Developed logical data models and physical database design and generated database schemas using Erwin 8.5.\n\u2022 Performed data analysis and data profiling using complex SQL on various sources systems including Oracle and MS SQL Server.\n\u2022 Document all data mapping and transformation processes in the Functional Design documents based on the business requirements.\n\u2022 Prepared High Level Logical Data Models using Erwin, and later translated the model into physical model using the Forward Engineering technique.\n\u2022 Generated and DDL (Data Definition Language) scripts using Erwin and assisted DBA in Physical Implementation of data Models.\n\u2022 Translated business requirements into working logical and physical data models for OLTP & OLAP systems.\n\u2022 Generated SQL scripts and implemented the relevant databases with related properties from keys, constraints, indexes & sequences.\n\u2022 Used Reverse Engineering to connect to existing database and developed process methodology for the Reverse Engineering phase of the project.\n\u2022 Developed the batch program in PL/SQL for the OLTP processing and used UNIX Shell scripts to run in corn tab.\n\u2022 Performed extensive data profiling and data analysis for detecting and correcting inaccurate data from the databases and track the data quality.\n\u2022 Provided guidance and solution concepts for multiple projects focused on data governance and master data management.\n\u2022 Created DDL scripts using Erwin and source to target mappings to bring the data from source to the warehouse.\n\u2022 Designed and developed SAS macros, applications and other utilities to expedite SAS Programming activities.\n\u2022 Involved in writing T-SQL working on SSIS, SSRS, SSAS, Data Cleansing, Data Scrubbing and Data Migration.\n\u2022 Analyzed and Gathered requirements from business people and management and business requirement document to prioritize their needs.\n\u2022 Responsible for backing up the data and involved in writing stored procedures and involved in writing ad-hoc queries for the data mining.\n\u2022 Developed and maintained data dictionary to create metadata reports for technical and business purpose.\n\u2022 Create and Monitor workflows using workflow designer and workflow monitor.\n\u2022 Involved in extensive DATA validation by writing several complex SQL queries and Involved in back-end testing and worked with data quality issues.\n\u2022 Used SSRS for generating Reports from Databases and Generated Sub-Reports, Drill down reports, Drill through reports and parameterized reports using SSRS.\n\u2022 Developed PL/SQL scripts to validate and load data into interface tables and Involved in maintaining data integrity between Oracle and SQL databases.\n\u2022 Heavily worked on SQL query optimization also tuning and reviewing the performance metrics of the queries\n\u2022 Performed the Data Mapping, Data design (Data Modeling) to integrate the data across the multiple databases in to EDW.\n\u2022 Collaborated with the Relationship Management and Operations teams to develop and present KPIs to top-tier clients.\n\nEnvironment: Erwin 8.5, Oracle 10g, MS SQL Server 2008, SSRS, OLAP, OLTP, MS Excel, Flat Files, , PL/SQL, OLAP, OLTP , SQL, IBM Cognos, Tableau."", u'Data Scientist\nQuality Systems Inc - Irvine, CA\nMarch 2013 to October 2014\nDescription: Quality Systems, Inc., together with its subsidiaries, develops and markets healthcare information systems in the United States. The company operates through four divisions: QSI Dental, NextGen, Inpatient Solutions, and Practice Solutions.\nResponsibilities:\n\n\u2022 Statistical Modeling with ML to bring Insights in Data under guidance of Principal Data Scientist\n\u2022 Data modeling with Pig, Hive, Impala.\n\u2022 Ingestion with Sqoop, Flume.\n\u2022 Used SVN to commit the Changes into the main EMM application trunk.\n\u2022 Understanding and implementation of text mining concepts, graph processing and semi-structured and unstructured data processing.\n\u2022 Worked with Ajax API calls to communicate with Hadoop through Impala Connection and SQL to render the required data through it. These API calls are similar to Microsoft Cognitive API calls.\n\u2022 Good grip on Cloudera and HDP ecosystem components.\n\u2022 Used Elasticsearch (Big Data) to retrieve data intothe application as required.\n\u2022 Performed Map Reduce Programs those are running on the cluster.\n\u2022 Developed multiple MapReduce jobs in java for data cleaning and preprocessing.\n\u2022 Analyzed the partitioned and bucketed data and compute various metrics for reporting.\n\u2022 Involved in loading data from RDBMS and weblogs into HDFS using Sqoop and Flume.\n\u2022 Worked on loading the data from MySQL to HBase where necessary using Sqoop.\n\u2022 Developed Hive queries for Analysis across different banners.\n\u2022 Extracted data from Twitter using Java and Twitter API. Parsed JSON formatted twitter data and uploaded to the database.\n\u2022 Launching Amazon EC2 Cloud Instances using Amazon Images (Linux/ Ubuntu) and Configuring launched instances with respect to specific applications.\n\u2022 Exported the result set from Hive to MySQL using Sqoop after processing the data.\n\u2022 Analyzed the data by performing Hive queries and running Pig scripts to study customer behavior.\n\u2022 Have hands-on experience working with Sequence files, AVRO, HAR file formats and compression.\n\u2022 Used Hive to partition and bucket data.\n\u2022 Experience in writing MapReduce programs with Java API to cleanse Structured and unstructured data.\n\u2022 Wrote Pig Scripts to perform ETL procedures on the data in HDFS.\n\u2022 Created HBase tables to store various data formats of data coming from different portfolios.\n\u2022 Worked on improving the performance of existing Pig and Hive Queries.\n\nEnvironment: Pig, Hive, Impala, ETL, HBase, AVRO, MapReduce, Java, HDFS, MySQL, Hadoop', u""Data Architect/Data Modeler\nLava International Limited - Noida, Uttar Pradesh\nOctober 2011 to February 2013\nDescription: Lava International Limited is an Indian multi-national company in the mobile handset industry. The company was founded in 2009 as an offshoot of a telecommunication venture. It is headquartered in Noida, India and has overseas operations in Thailand, Nepal, Bangladesh, Sri Lanka, Indonesia, Mexico and the Middle East, Pakistan and Russia.\n\nResponsibilities:\n\u2022 Analyzed data sources and requirements and business rules to perform logical and physical data modeling.\n\u2022 Analyzed and designed best fit logical and physical data models and relational database definitions using DB2. Generated reports of data definitions.\n\u2022 Conducted source data analysis of various data sources and develop source-to-target mappings with business rules.\n\u2022 Involved in Normalization/De-normalization, Normal Form and database design methodology.\n\u2022 Maintained existing ETL procedures, fixed bugs and restored software to production environment.\n\u2022 Developed the code as per the client's requirements using SQL, PL/SQL and Data Warehousing concepts.\n\u2022 Involved in Dimensional modeling (Star Schema) of the Data warehouse and used Erwin to design the business process, dimensions and measured facts.\n\u2022 Worked with Data Warehouse Extract and load developers to design mappings for Data Capture, Staging, Cleansing, Loading, and Auditing.\n\u2022 Developed enterprise data model management process to manage multiple data models developed by different groups\n\u2022 Designed and created Data Marts as part of a data warehouse.\n\u2022 Effectively used triggers and stored procedures necessary to meet specific application's requirements.\n\u2022 Created SQL scripts for database modification and performed multiple data modeling tasks at the same time under tight schedules.\n\u2022 Reviewed new data development and ensured that it is consistent and well integrated with existing structures.\n\u2022 Wrote complex SQL queries for validating the data against different kinds of reports generated by Business Objects XIR2.\n\u2022 Involved in reviewing business requirements and analyzing data sources form Excel/Oracle SQL Server for design, development, testing, and production rollover of reporting and analysis projects.\n\u2022 Document and publish test results, troubleshoot and escalate issues\n\u2022 Worked on SAS and IDQ for Data Analysis.\n\u2022 Using Erwin modeling tool, publishing of a data dictionary, review of the model and dictionary with subject matter experts and generation of data definition language.\n\u2022 Coordinated with DBA in implementing the Database changes and also updating Data Models with changes implemented in development, QA and Production.\n\u2022 Created and execute test scripts, cases, and scenarios that will determine optimal system performance according to specifications.\n\u2022 Worked Extensively with DBA and Reporting team for improving the Report Performance with the Use of appropriate indexes and Partitioning.\n\u2022 Developed Data Mapping, Transformation and Cleansing rules for the Master Data Management Architecture involved OLTP, ODS and OLAP.\n\u2022 Tuned and coded optimization using different techniques like dynamic SQL, dynamic cursors, and tuning SQL queries, writing generic procedures, functions and packages.\n\u2022 Analyzed the data and provide resolution by writing analytical/complex SQL in case of data discrepancies.\n\u2022 Experienced in GUI, Relational Database Management System (RDBMS), designing of OLAP system environment as well as Report Development.\n\u2022 Extensively used SQL, T-SQL and PL/SQL to write stored procedures, functions, packages and triggers.\n\u2022 Analyzed of data report were prepared weekly, biweekly, monthly using MS Excel, SQL & UNIX.\n\nEnvironment: Erwin 7.5, Oracle 10g Application Server, Oracle Developer Suite, PL/SQL, T-SQL, DB2, SQL Plus, Microsoft SQL Server 2005"", u""Data Analyst/Data Modeler\nSpice Digital Limited - Noida, Uttar Pradesh\nMarch 2009 to September 2011\nDescription: Spice Digital Limited, previously known as Cellebrum Technologies, was founded in year 2000. The company provides Telco Solutions, Value Added Services (MVAS), Enterprise Solutions, Financial technology, GSP and Digital transformation products and services. Spice Digital is headquartered at Noida in NCR, India.\n\nResponsibilities:\n\u2022 Normalized and De-normalized the tables and maintaining Referential Integrity by using Triggers and Primary and Foreign Keys.\n\u2022 Conducted and automated the ETL operations to Extract data from multiple data sources, transform inconsistent and missing data to consistent and reliable data, and finally load it into the Multi-dimensional data warehouse\n\u2022 Developed packages using Fast Parse in SSIS to reduce the extraction time in ETL process by 9.5%.\n\u2022 Involved in design and analysis of underlying database schema, altering and creation of the table structure.\n\u2022 Developed Informatica Mappings using heterogeneous sources like flat files and different relational databases, Mapplets, Mappings using Power Center Designer.\n\u2022 Experience with routine DBA activities like Query Optimization, Performance Tuning and Effective SQL Server configuration for better performance and cost reduction. Installed and configured SQL Mail client for SQL 2000.\n\u2022 Responsible for report generation using SQL Server Reporting Services (SSRS) and Crystal Reports based on business requirements.\n\u2022 Created number of jobs, alerts and operators to be paged or emailed in case of failure for SQL 2000.\n\u2022 Created and Configured Data Source & Data Source Views, Dimensions, Cubes, Measures, Partitions, KPI's & MDX Queries using SQL Server 2005 Analysis Services(SSAS).\n\u2022 Experience in Creating Backend validations using Insert/Update and Delete triggers and Created views for generating reports, Indexed Views.\n\u2022 Improved the performance of the SQL server queries using query plan, covering index, indexed views and by rebuilding and reorganizing the indexes.\n\u2022 Configure and manage database maintenance plans for update statistics, database integrity check and backup operations.\n\nEnvironment: SQL Server 2008, SSRS, SSIS, SSAS, Microsoft office, SQL Server Management Studio, Business Intelligence Development Studio, MS Access, Erwin data Modeler""]",[u'Bachelor of Computer Science & Technology in TOOLS AND TECHNOLOGIES'],[u'SGD\nMarch 2011'],degree_1 : Bachelor of Compter Science & Technology in TOOLS AND TECHNOLOGIES
0,https://resumes.indeed.com/resume/7b557bb945862020,"[u""DATA SCIENTIST\nDIGITAL REASONING - Franklin, TN\nAugust 2016 to Present\nProvided analysis to clients in the retail services market to provide actionable\ninsights and solutions based on human behavioral analysis, buying trends, merchandise placement and marketing using big data analytical systems, Cloudera Hadoop, Hortonworks Hadoop and AWS.\n* Perform Data Profiling to learn about user behavior and merge data from multiple data sources.\n* Worked on Clustering and classification of data using machine learning algorithms.\n* Used Tensor Flow machine learning to create sentimental and time series analysis.\n* Implemented big data processing applications to collect, clean and normalize large volumes of open data using Hadoop eco system such as PIG, HIVE, and HBase.\n* Worked with the data engineers and data architects to define custom solutions and analytical needs.\n* Implemented Text mining to transposing words and phrases in unstructured data into numerical values\n* Independently coded new programs and designed Tables to load and test the program effectively for the given POC's using with Big Data/Hadoop.\n* Integrate R into Micro Strategy to expose metrics determined by more sophisticated and detailed models than natively available in the tool.\n* Develop documents and dashboards of predictions in Microstrategy and present it to the Business Intelligence team.\n* Used CloudVision API integrate vision to detection features within applications, including image labeling, face and landmark detection, optical character recognition (OCR), and tagging of explicit content.\n* Developed various QlikViewDataModels by extracting and using the data from various sources files, DB2, Excel, Flat Files and Bigdata.\n* Experience architecting BIG Data solutions for Projects & Proposal using Hadoop, Spark, ELK Stack, Kafka, Tensor flow.\n* Designing and developing various machine learning frameworks using Python, R, and Matlab."", u""DATA SCIENTIST\nDIGITAL REASONING\nAugust 2016 to Present\nProvided analysis to clients in the retail services market to provide actionable\ninsights and solutions based on human behavioral analysis, buying trends, merchandise placement and marketing using big data analytical systems, Cloudera Hadoop, Hortonworks Hadoop and AWS.\nPerform Data Profiling to learn about user behavior and merge data from multiple data sources.\nWorked on Clustering and classification of data using machine learning algorithms.\nUsed Tensor Flow machine learning to create sentimental and time series analysis.\nImplemented big data processing applications to collect, clean and normalize large volumes of open data using Hadoop eco system such as PIG, HIVE, and HBase.\nWorked with the data engineers and data architects to define custom solutions and analytical needs.\nImplemented Text mining to transposing words and phrases in unstructured data into numerical values\nIndependently coded new programs and designed Tables to load and test the program effectively for the given POC's using with Big Data/Hadoop.\nIntegrate R into Micro Strategy to expose metrics determined by more sophisticated and detailed models than natively available in the tool.\nDevelop documents and dashboards of predictions in Microstrategy and present it to the Business Intelligence team.\nUsed CloudVision API integrate vision to detection features within applications, including image labeling, face and landmark detection, optical character recognition (OCR), and tagging of explicit content.\nDeveloped various QlikViewDataModels by extracting and using the data from various sources files, DB2, Excel, Flat Files and Bigdata.\nExperience architecting BIG Data solutions for Projects & Proposal using Hadoop, Spark, ELK Stack, Kafka, Tensor flow.\nDesigning and developing various machine learning frameworks using Python, R, and Matlab."", u'DATA SCIENTIST\nCERIDIAN HCM, INC - Minneapolis, MN\nJanuary 2015 to July 2016\nHealth studies analysis longitudinal data from various sources which has been pooled into a centralized system using Hadoop HDFS data lake. Developed a machine learning based algorithm for prediction of various adverse incidences and progressions of health.\n* Developed MapReduce/Spark Python modules for machine learning & predictive analytics in Hadoop on AWS.\n* Developed LINUX Shell scripts by using NZSQL/NZLOAD utilities to load data from flat files to Netezza database.\n* Demonstrated experience in design and implementation of Statistical models, Predictive models\n* Hands on database design, relational integrity constraints, OLAP, OLTP, Cubes and Normalization (3NF) and De-normalization of database.\n* Developed MapReduce/SparkPython modules for machine learning & predictive analytics in Hadoop on AWS. Implemented a Python-based distributed random forest via Python streaming.\n* Worked with various Teradata15 tools and utilities like Teradata Viewpoint, Multi Load, ARC, Teradata Administrator, BTEQ and other Teradata Utilities.\n* Utilized Spark, Scala, Hadoop, HBase, Kafka, Spark Streaming, MLlib, Python, a broad variety of machine learning methods including classifications, regressions, dimensionally reduction.\n* Utilized domain knowledge and application portfolio knowledge to play a key role in defining the future state\n* Designed and implemented system architecture for Amazon EC2 based cloud-hosted solution for client.\n* Performed Source System Analysis, database design, data modeling for the warehouse layer using MLDM concepts and package layer using Dimensional modeling.\nWorked on customer segmentation / clustering using the following machine learning techniques: naive Bayes, Random Forests, K-means, and KNN.\n* Created ecosystem models (e.g. conceptual, logical, physical, canonical) that are required for supporting services within the enterprise data architecture (conceptual data model for defining the major subject areas used, ecosystem logical model for defining standard business meaning for entities and fields, and an ecosystem canonical model for defining the standard messages and formats to be used in data integration services throughout the ecosystem) data model, metadata solution and data life cycle management in both RDBMS, Big Data environments\n* Used Pandas, NumPy, seaborn, SciPy, Matplotlib, Scikit-learn, NLTK in Python for developing various Tested Complex ETL Mappings and Sessions based on business user requirements and business rules to load data from source flat files and RDBMS tables to target tables.', u'DATA SCIENTIST\nAC NIELSEN - Chicago, IL\nSeptember 2013 to September 2014\nCreated custom algorithms and mapping processes to analyze large volume of data from human buying and usage data. Data of various categories, various projects derived from the data lake created by these disparate sources.\n* Developed applications of Machine Learning, Statistical Analysis and Data Visualizations with challenging data Processing problems in sustainability and biomedical domain.\n* Compiled data from various sources public and private databases to perform complex analysis and data\n* manipulation for actionable results.\n* Designed and developed Natural Language Processing models for sentiment analysis.\n* Worked on Natural Language Processing with NLTK module of python for application development for automated customer response.\n* Used predictive modeling with tools in SAS, SPSS, R, Python.\n* Applied concepts of probability, distribution and statistical inference on given dataset to unearth interesting findings through use of comparison, T-test, F-test, R-squared, P-value etc.\n* Applied linear regression, multiple regression, ordinary least square method, mean-variance, theory of large numbers, logistic regression, dummy variable, residuals, Poisson distribution, Bayes, Naive Bayes, fitting function etc to data with help of Scikit, Scipy, Numpy and Pandas module of Python.\n* Applied clustering algorithms i.e. Hierarchical, K-means with help of Scikit and Scipy.\n* Developed visualizations and dashboards using ggplot, Tableau\n* Worked on development of data warehouse, DataLake and ETL systems using relational and non-relational tools like SQL, No SQL.\n* Built and analyzed datasets using Python, R, SAS, and MatLab (in decreasing order of usage)\n* Applied linear regression in Python and SAS to understand the relationship between different attributes of dataset and causal relationship between them\n* Performs complex pattern recognition of financial time series data and forecast of returns through the ARMA and ARIMA models and exponential smoothening for multivariate time series data\n* Pipelined (ingest/clean/munge/transform) data for feature extraction toward downstream classification.\n* Used Spark on Cloudera Hadoop to perform analytics on data in Hive.\n* Wrote Hive queries for data analysis to meet the business requirements.\n* Expertise in Business Intelligence and data visualization using R and Tableau.', u""DATA SCIENTIST\nOPPENHEIMER FUNDS - New York, NY\nFebruary 2012 to August 2013\nTime Series Analysis for Investment Bank. Predictive analytics for investment trend among portfolio owners as well as market predictions. Used LSTM model applied to rebalancing strategies.\n* Validated the Macro-Economic data (e.g. BlackRock, Moody's etc.) and predictive analysis of world markets using key indicators in Python and machine learning concepts like regression, Boot strap Aggregation and Random Forest.\n* Worked in large scale database environment like Hadoop and MapReduce, with working mechanism of Hadoop clusters, nodes and Hadoop Distributed File System (HDFS)\n* Interfaced with large scale database system through an ETL server for data extraction and preparation.\n* Identified patterns, data quality issues, and opportunities and leveraged insights by communicating opportunities with business partners.\n* Worked with high-level platforms for data processing such as Apache Pig and Hive to consume data and provide custom query results.\n* Worked in product implementation across various stages of the analytical value chain, with focus areas in dashboard visualization, sentiment analysis and building clustering and regression models for business.\n* Identifying, gathering, and analyzing complex, multi-dimensional datasets utilizing a variety of tools.\n* Performed data visualization and developed presentation material utilizing Tableau.\n* Working with clients in defining the key business problems to be solved while developing, maintaining and leveraging key client relationships"", u'DATA ANALYST\nVERIZON WIRELESS - Township of Warren, NJ\nFebruary 2010 to February 2012\nConstructed models to analyze churn, propensity to buy, segmentation, cross-sell/up-sell, etc., and perform advanced analytics using machine learning, and product recommender system to identify optimal products and product groupings for sales offerings to new and existing customers\n* Conducted and interpreted multivariate analyses examples, including regressions with various distributions and duration models.\n* Collaborated with other analysts and key stakeholders to identify underlying trends, both internal and external, impacting current and future enrollment and financial considerations, and incorporate trends into forecast models.\n* Work independently to develop models that address specific business problems related to enrollment management, retention, marketing and class scheduling\n* Analysis using R and Python on data derived from big data Hadoop systems using distributed processing paradigms, stream processing and databases such as SQL and NoSQL.\n* Collaborated with cross-functional teams such as data onboarding, functional requirements group and development team to help map exchange data in to a normalized model.\n* Update, maintain and validate large financial data sets for derivatives using SQL.\n* Provided root cause analysis and preventative measures for any data quality issues that occurred in day to day operations to clients.\n* Use advanced data mining, statistical analysis, machine-learning and visualization techniques to create solutions to challenging real-world problems\n* Work with diverse data sets, identify and develop valuable new sources of data and collaborate with product teams to ensure successful integration\n* Identify and analyze anomalous data (including metadata)']","[u""Bachelor's in Information Technology""]",[u'COLORADO STATE Denver\nJune 2017'],"degree_1 : ""Bachelors in Information Technology"""
0,https://resumes.indeed.com/resume/f2a6b595459d16e7,"[u'Data Scientist\nCredit Suisse (Consultant) - New York, NY\nNovember 2017 to Present\nData Scientist for Credit Suisse (New York, NY): Systemsoft Consultant 11/17 \u2013 Present\nConvince Management during meetings with Director, VP across globe, the need for Open Source tools, present an overview on Machine Learning, diagrams, implement Open Source Machine Learning, Big Data, Python tools.\nPython to connect to SQL Server, Oracle, Impala to import data from RDBMS for building Machine Learning Algorithm. Pentaho for data cleaning, pipe lining, storing data in Hive, Impala, Spark,Tableau team.\nUse Classification, Clustering Algorithms models using Python for Text Analytics for Text Data, grouping Data.\nUse Python Pandas, seaborn, matplotlib, tried Tensorflow, Keras, Text Data Clustering, RNN.', u'Data Scientist\nJefferies LLC - New York, NY\nApril 2017 to October 2017\nConvince Management during meetings with Director, VP the need for Open Source, present an overview on Machine Learning, diagrams, Build & implement Open Source Machine Learning, Big Data, Python, AWS tools.\nUse Classification, Regression, Clustering Algorithms models using Python to solve Demand Forecasting, Text Analytics for Log Data, segregate Fixed Income Data into groups using Clustering. Visualize Client Data, build histograms, barplot using Python Pandas, seaborn, matplotlib. Use Tensor Flow, Keras for Deep Learning - RNN.\nUse Python to connect to SQL Server, Oracle, Sybase to import data from RDBMS for building Machine Learning Algorithm. Use ETL tool Pentaho for data cleaning, pipe lining in Spark, storing data in Hive, PowerBI visualisation.\nPerform Data Cleaning, Data Analysis using Python, AWS S3 and connect S3 to AWS EMR and EC2 to run PySpark, SparkR. Connect AWS S3 with IDE like Jupyter, Zepplin to run PySpark, Python scripts. Use Boto3 Python SDK.\nUse ETL tool Pentaho to migrate data from RDBMS to Cloud services, Create Hive tables, ingest data in Hive tables, PySpark for performing ETL on AWS EMR, store data S3. Use Git, Sourcetree, Bitbucket to store script share.', u'Data Scientist\nBarclays (Consultant) - New York, NY\nAugust 2016 to February 2017\nFor the Bank\u2019s Sales division, use Machine Learning algorithms, Time Series, Text Analytics, Clustering,\nForecast and visualize 30 day Sales Trends of Securities based on Bank\u2019s Historical Sales Data. Decompose Trends, spot & visualize Seasonal & Non Seasonal charts and forecast Sales demand for Sales Team using Time series.\nFor Client Analytics, perform Sentiment Analysis and discover patterns in Client Meeting based on the words spoken and documented in meeting minutes. Classify meetings into High, Medium, Low in terms using Python NLTK, Scikit Learn, Tidyr, Tidytext, WordCloud in R for Text Analytics.\nDiscover similarly situated Clients of the Bank based on their Transaction Amount, LOB, Region, Time using K Means Clustering, Hierarchical Clustering. Classify Client into Diamond, Gold, Silver partner based on their Transaction Amount, LOB, Region and forecast new incoming Client Classification based on features using KNN.\nVisualize Client Data - build histograms, barplot using R ggplot, Python seaborn, matplotlib. Experimented with Python Tensor Flow, Keras for Deep Learning. Use R, Python packages to connect to SQL Server, Oracle, Sybase.']","[u'', u'Certificate', u'MS Computer Science in Computer Science', u'MS in Business in Intelligence', u'BA in Economics in Economics']","[u'Online University of Washington\nJanuary 2017', u'Harvard X and Microsoft\nJanuary 2015', u'Stratford University\nJanuary 2012 to January 2014', u'Stevens Institute of Technology\nJanuary 2006 to January 2008', u'University of Delhi Delhi, Delhi\nJanuary 1992 to January 1996']","degree_1 : , degree_2 :  Certificate, degree_3 :  MS Compter Science in Compter Science, degree_4 :  MS in Bsiness in Intelligence, degree_5 :  BA in Economics in Economics"
0,https://resumes.indeed.com/resume/a8599645ebcd609f,"[u'Teacher of data science\nMethod coding school - Almaty\nSeptember 2017 to Present\nMy students is top 3% in drivendata.org competition', u'Senior data scientist\nPetrel AI: - Almaty\nSeptember 2017 to Present\nChurn prediction model for bank\nClient activation model for bank\nSalary prediction model for bank', u'Data scientist\nMladex\nDecember 2016 to Present\n\u2022 Predicting cost of products\n\u2022 Forecasting an optimal loan\n\u2022 Identifying a fraud with the help of transaction\n\u2022 Speech, text analysis\n\u2022 Searching an optimal segment to give clients offer\nMethod coding school (Teacher of data science)\n\u2022 My students is top 3% in drivendata.org competition\nPetrel AI: (Senior data scientist)\n\u2022 Churn prediction model for bank\n\u2022 Client activation model for bank\n\u2022 Salary prediction model for bank', u'Pharmacoeconomic analyst\nMladex\nJuly 2016 to Present\n\u2022 Classifying Anatomical Therapeutic Chemical indicators and medicines in optimal groups\n\u2022 Fitting business indicators, income and outcome, forecasting the USD to KZT based on barrel of oil\n\u2022 Fitting indicators of company', u'Pharmacoeconomic analyst\nMladex\nJuly 2016 to Present\n\u2022 Writing articles in the journal about pharmacoeconomics market\n\u2022 Making client oriented presentations on POWER BI', u'analyst\nRAAR labs\nJune 2016 to Present\n\u2022 Build recommender system using circular matrix algorithm, collaborative filtering, content-based filtering and hybrid recommender system\n\u2022 Made market research presentations based on various analysis', u'Data Scientist\nKaspi Bank - Almaty\nDecember 2016 to October 2017\nPredicting cost of products\nForecasting an optimal loan\nIdentifying a fraud with the help of transaction\nSpeech, text analysis\nSearching an optimal segment to give clients offer']",[u'in Engineering and Natural Sciences'],[u'Suleyman Demirel University'],degree_1 : in Engineering and Natral Sciences
0,https://resumes.indeed.com/resume/ea0da36ebd78928b,"[u'Data Scientist Intern\nHylink Digital Solutions - Santa Monica, CA\nAugust 2017 to October 2017\n- Built predictive models (Regression) with 85.6% accuracy to help company make decisions on travel reimbursement\n- Implemented SVM, k-NN, Random Forest to forecast the category of staff job level for travel reimbursement\n- Constructed server-side scripting to crawl datasets from multiple RESTful APIs for feature engineering\n- Evaluated data integrity and feasible solution to the current/potential data storage', u'Research Assistant\nUniversity of Southern California - Los Angeles, CA\nAugust 2016 to July 2017\n- Developed a user-based collaborating filtering for movie recommendation, reaching the ratings for ~94.2% accuracy\n- Implemented Locality-sensitive hashing (LSH) to find similar movies according to the Jaccard similarity\n- Visualized 20 million movies with 465 thousand tags to find annual and anomaly patterns to boost accuracy', u'Engineering Intern\nMedicusTek USA - Taipei, TW\nJune 2015 to August 2015\nTaiwan\n- Analyzed HealthCare App datasets via A/B testing for 5 hospitals to save more than 6k clicks/week for nursing stations\n- Created data visualizations via Tableau on mobile applications performance analysis (Android /iOS app)\n- Initiated data cleaning, assessment and analytics to ensure the quality of feature selection process']","[u'Master of Science in Electrical Engineering in Data Science', u'Bachelor of Science in Electrical Engineering']","[u'University of Southern California, Viterbi School of Engineering Los Angeles, CA\nMay 2018', u'Fu Jen Catholic University Taipei, TW\nJune 2014']","degree_1 : Master of Science in Electrical Engineering in Data Science, degree_2 :  Bachelor of Science in Electrical Engineering"
0,https://resumes.indeed.com/resume/084b67ab2f2b81e0,"[u'Data Scientist\nIcon Payment Solutions - Chicago, IL\nJanuary 2013 to January 2017\nHelping business owners with Social Media Marketing, increase traffic to there business by making over 150 calls a day and getting them to want the marketing.', u'Assistant Manager\nNew Vision, LLC - Chicago, IL\nSeptember 2012 to August 2016\nManaged and trained up to 35 employees in phone sales to business owners and set appointments to switch the merchant service account for credit card processing.', u'Sales Manager\nAmerican Community Services - Michigan City, IN\nAugust 1998 to March 2012\nHired, and trained employees for magazine sales door to door, holding morning motivational meetings, paying out bonuses, and incentives for doing a great job.Writing scripts, committals, and rebuttals.Traveled across the U.S running ads for a new hire. Recruiting, teaching,and leading by excample']","[u""Bachelor's in Business Management and Leadership""]","[u'Jackson State University Jackson, MS\nAugust 1993 to June 1997']","degree_1 : ""Bachelors in Bsiness Management and Leadership"""
0,https://resumes.indeed.com/resume/8891e7271ddad6d6,"[u'Data Scientist\nPHOTON INFOTECH - Bohemia, NY\nSeptember 2017 to Present\nDeveloping a personalized recommender engine for the client Natures Bounty Co. The products are recommended to the user based on the user preferences (questionnaire taken by the user when entered the website). The responsibilities involved are exploratory data analysis, modeling of the data. Used Excel for some part of the Data manipulation. Affinity score matrix is built for the products and their corresponding questionnaire. Used classification algorithms such as KNN (K-Nearest Neighbours) and Random Forests algorithms. Build these models in python. This enables user to engage better.', u'Senior Data Scientist\nAPPLE INC - Cupertino, CA\nJanuary 2015 to August 2017\nPlayed a key role in developing and maintaining statistical and machine learning models that mine, analyze and turn Apple data into insights that is helping Apple grow their user base and revenue.\n\nKEY PROJECTS\nPurchase Propensity Modeling\n\u2022 Developed classification machine learning models in python that predicted purchase propensity of customers based on customer attributes such as demographics - education, income, age, geography, historic purchases and other related attributes. Predicting customer propensity helped marketing teams to aggressively pursue prospective customers.\nCustomer Churn\n\u2022 Developed classification models to predict the likelihood of customer churn based on customer attributes like customer size, revenue, type of industry, competitor products and growth rates etc. The models deployed in production environment helped detect churn in advance and aided sales/marketing teams plan for various retention strategies like price discounts, custom licensing plans etc.\nCustomer Life Time Analysis\n\u2022 Projected customer lifetime values based on historic customer usage and churn rates using survival models. Understanding customer lifetime values helped business to establish strategies to selectively attract customers who tend to be more profitable for Apple. It also helped business to establish appropriate marketing strategies based on customer values.\nCustomer Segmentation\n\u2022 Developed 11 customer segments using unsupervised learning techniques like KMeans and Gaussian mixture models. The clusters helped business simplify complex patterns to manageable set of 11 patterns that helped set strategic and tactical objectives pertaining to customer retention, acquisition, spend and loyalty.\nForecast Process Innovations\n\u2022 Improved sales/demand forecast accuracy by 20-25% by implementing advanced forecasting algorithms that were effective in detecting seasonality and trends in the patterns in addition to incorporating exogenous covariates. Increased accuracy helped business plan better with respect to budgeting and sales and operations planning.\nCross Sell and Upsell Opportunities\n\u2022 Implemented market basket algorithms from transactional data, which helped identify products ordered together frequently. Discovering frequent product sets helped unearth Cross sell and Upselling opportunities and led to better pricing, bundling and promotion strategies for sales and marketing teams.', u'Data Scientist\nRETAILMENOT INC - Austin, TX\nDecember 2013 to December 2014\nKEY PROJECTS\nOffer Recommender System\n\u2022 Developed a personalized coupon recommender system using recommender algorithms (collaborative filtering, low rank matrix factorization) in python that recommended best offers to a user based on similar user profiles. The recommendations enabled users to engage better and helped improving the overall user retention rates.\nUser Click Prediction\n\u2022 Developed a lead scoring system by modeling the users based on company size, industry segment, job title or geographic location using supervised learning algorithms. Scoring leads led to increased sales efficiency and effectiveness, increased marketing effectiveness and tighter marketing and sales alignment.\nOther Projects\n\u2022 Designed the Data Warehouse and MDM hub Conceptual, Logical and Physical data models\n\u2022 Used Normalization methods up to 3NF and De-normalization techniques for effective performance in OLTP and OLAP systems. Generated DDL scripts using Forward Engineering techniques to create objects and deploy them into the database.', u""Data Scientist\nBANK OF AMERICA - Charlotte, NC\nOctober 2012 to November 2013\nPlayed key role in developing and deploying Dodd-Frank Act Stress Test models across several bank portfolios. Provided architectural leadership on several high priority initiatives including account prioritization, account prospecting, and opportunity scoring. Drove the creation of comprehensive datasets encompassing user profiles and behaviors, and incorporating a wide variety of signals and data types.\n\nKEY PROJECTS\nTop down Models - Residential Real Estate\n\u2022 Automated the scraping and cleaning of data from various data sources in R and Python. Developed Banks's loss forecasting process using relevant forecasting and regression algorithms in R.\n\u2022 The projected losses under stress conditions helped bank reserve enough funds per DFAST policies.\nCredit Risk Scorecards\n\u2022 Developed several interactive dashboards in Tableau to visualize 8 billion rows (1.2 TB) credit data by designing a scalable data cube structure.\n\u2022 Built credit risk scorecards and marketing response models using SQL and SAS. Evangelized the complex technical analysis into easily digestible reports for top executives in the bank."", u""Data Modeler/Data Analyst\nCUMMINS, INC - Pune, Maharashtra\nJuly 2011 to September 2012\n\u2022 Designed scalable processes to collect, manipulate, present, and analyze large datasets in a production ready environment, using Akamai's big data platform.\n\u2022 Achieved a broad spectrum of end results putting into action the ability to find, and interpret rich data sources, merge data sources together, ensure consistency of data-sets, create visualizations to aid in understanding data, build mathematical models using the data, present and communicate the data insights/findings to specialists and scientists in their team.\n\u2022 Implemented full lifecycle in Data Modeler/Data Analyst, Data warehouses and DataMart's with Star Schemas, Snowflake Schemas, and SCD& Dimensional Modeling Erwin. Performed data mining on data using very complex SQL queries and discovered pattern and used extensive SQL for data profiling/analysis to provide guidance in building the data model."", u""Data Analyst /Data Modeler\nHEWLETT-PACKARD - Bengaluru, Karnataka\nJune 2010 to June 2011\n\u2022 Worked with SME's and other stakeholders to determine the requirements to identify Entities and Attributes to build Conceptual, Logical and Physical data Models.\n\u2022 Used Star Schema methodologies in building and designing the logical data model into Dimensional Models extensively. Developed Star and Snowflake schemas based dimensional model to develop the data warehouse. Designed Context Flow Diagrams, Structure Chart and ER- diagrams.""]",[u'B.S. in Computer Science'],"[u'K L College of Engineering Guntur, Andhra Pradesh\nAugust 2006 to May 2010']",degree_1 : B.S. in Compter Science
0,https://resumes.indeed.com/resume/e2ade4243d14e167,"[u'Data Scientist\nGLEASON WORKS - Rochester, NY\nJanuary 2018 to February 2018\nCurrent) July 2018\n\u2022 Applied data mining techniques in order to perform statistical analysis in Industry 4.0 Data Analytics projects. Built high\nquality Prediction Systems integrated with Gleason Products. Discovered hidden insights in vast amounts of machine\nmanufacturing data and performed Time Series Analysis to help Gleason make smarter decisions and develop better\nproducts & services.', u'Software Engineer\nADVENTIVE INC - Rochester, NY\nMay 2017 to July 2017\nCoded, developed and implemented new software modules for ad serving and ad building components for SaaS product.\nDeveloped Rest API for software modules and prototyped several consumer modules. Performed ETL and designed\ndata warehouse for faster analytics.', u'Software Engineer\nSOFTVAN SOFTWARE COMPANY - Ahmedabad, Gujarat\nJune 2015 to April 2016\nDesigned, coded various Java based software modules using vast amount of consumer data, deployed using Amazon\nWeb Services instances. Developed analytics module to filter out user comments and perform text analysis.']","[u'MS in Information Science and Technology', u'BE in Computer Science and Engineering']","[u'ROCHESTER INSTITUTE OF TECHNOLOGY Rochester, NY\nAugust 2016 to June 2018', u'GUJARAT TECHNOLOGICAL UNIVERSITY Ahmedabad, Gujarat\nJune 2012 to May 2016']","degree_1 : MS in Information Science and Technology, degree_2 :  BE in Compter Science and Engineering"
0,https://resumes.indeed.com/resume/8aef5ef4eee5ffcb,"[u'Data Scientist Intern\nUptake - Chicago, IL\nMay 2017 to August 2017\n\u2022 Assessed applications of data science in the IoT security domain.\n\u2022 Performed exploratory data analysis to find feature interactions, identify attacks on various sensors and equipment.\n\u2022 Built models to classify the events as attack or non-attack with over 97% accuracy. XGboost, clustering, timeseries, PCA', u'Senior Programming Analyst, Barclaycard Acquiring\nBarclays Technology Centre India (BTCI)\nNovember 2012 to July 2016\n\u2022 Assisted in maintaining tier 1 and tier 2 applications which processed payments of nearly 350M GBP on a daily basis and generated statements worth 900M GBP.\n\u2022 Spearheaded in-depth analysis of a statement generation batch which was generating the merchant statements incorrectly and devised a work around by applying data adjustments with SQL that saved Barclaycard the growing reputation damage.\n\u2022 Developed shell scripts that would run SQL queries to gather daily stats and data which reduced manual efforts by around 10% daily.']","[u'Master of Data Science in Data Science', u'B.E. in Electronics Engineering']","[u'Illinois Institute of Technology\nJanuary 2017', u'Vishwakarma Institute of Technology (University of Pune)\nJanuary 2012']","degree_1 : Master of Data Science in Data Science, degree_2 :  B.E. in Electronics Engineering"
0,https://resumes.indeed.com/resume/28e3b7413cfa0fb3,"[u'Data Scientist\nMarch 2015 to Present\n\u2022 Identifying, gathering, and analyzing complex, multi-dimensional datasets utilizing a variety of tools\n\u2022 Working with clients in defining the key business problems to be solved while developing, maintaining and leveraging key client relationships\n\u2022 Maintaining knowledge and understanding of current and emerging trends within the analytics industry\n\u2022 Establish and refine the end to end risk management process by working with product owners, business stake holders, engineering, and fraud operations\n\u2022 Participate in product redesigns and enhancements to know how the changes will be tracked and suggest product direction based on data patterns.\n\u2022 Strong foundation in statistics -Mining and organizing datasets of both structured and unstructured data: preferably large datasets\n\u2022 Knowledge of algorithms, data structures and performance optimization\n\u2022 Experience with applied statistics and/or applied mathematics\n\u2022 Facilitate the data collection sessions. Analyze and document data processes, scenarios, and information flow.\n\u2022 Perform crosswalks of HUD policies to ensure compliance with Executive Orders, National Security Directives, HUD Orders and Manuals and Federal regulations.\n\u2022 Determine data structures and their relations in supporting business objectives and provides useful data in reports.\n\u2022 Experience supporting a federal agency\n\u2022 Promoted enterprise-wide business intelligence by enabling report access in SAS BI Portal and on Tableau Server\n\u2022 Utilizes tools such as Python, Tableau, SAS, etc. to perform complex data analysis', u'Data Scientist\nDMI\nJune 2014 to February 2015\nProject Summary Online Loan Application System to automate loan approval processes.\n\n\u2022 Interprets and documents Agency and component data information needs, including business rules.\n\u2022 Knowledge of data mining and machine learning algorithms, theories, principals and practices.\n\u2022 Experience with data manipulation, analytic tools, and data visualization\n\u2022 Establish and standardize on the end to end modeling process for data scientist, and work with product owner and engineering to define the set of tools and frameworks required to support the end to end modeling process\n\u2022 Facilitate the mapping and auditability of information assets as they flow from upstream systems and interfaces to downstream analytical applications.\n\u2022 Manually review logs & provide documentation guidelines to business process owners and management.\n\u2022 Obtain functional requirements from subject matter experts during group workshops or follow-up interviews.\n\u2022 Analyzing and modeling structured data using advanced statistical methods and implementing algorithms and software needed to perform analyses\n\u2022 Facilitate the data collection sessions. Analyze and document data processes, scenarios, and information flow.\n\u2022 Determine data structures and their relations in supporting business objectives and provides useful data in reports.\n\u2022 Document and describe logical data models, semantic data models and physical data models.\n\u2022 Identify important and interesting questions about large datasets, then translate those questions into concrete analytical tasks.\n\u2022 Analyzed financial data for prospective purchase, asset allocation, fee generation or other cash flow, using typically industry/company standard analytical tools or measures.\n\u2022 Uses and learns a wide variety of tools and languages to achieve results (e.g., R, SAS, Python, SQL).', u'Data Scientist\nXerox\nOctober 2013 to June 2014\n\u2022 Define the requirements and make recommendations on tools and capabilities to further data analytic capabilities.\n\u2022 Perform machine learning, natural language, and statistical analysis methods, such as clustering and classification, collaborative filtering, association rules, sentiment analysis, topic modeling, time-series analysis, multivariate regression analysis, statistical inference, and validation methods.\n\u2022 Perform complex modeling, simulation and analysis of data and processes.\n\u2022 Analyze model performance and communicate model design and performance to business stakeholders, product owners, engineering, and fraud operations.\n\u2022 Conceptualize analysis processes that will use mechanisms such as algorithms, statistical sampling, regression analysis.\n\u2022 Work independently to develop models that address specific business problems related to enrollment management, retention, marketing and class scheduling.\n\u2022 Proficiency in analysis (e.g. R, SAS) packages, programming languages (e.g. Python) as well as the ability to implement, maintain, and troubleshoot big data infrastructure, such as distributed processing paradigms, stream processing and databases such as SQL.', u'Data Scientist\nMagnum IT Solutions, Inc\nJune 2008 to October 2013\n\u2022 Use advanced data mining, statistical analysis, machine-learning and visualization techniques to create solutions to challenging real-world problems.\n\u2022 Work with diverse data sets, identify and develop valuable new sources of data and collaborate with product teams to ensure successful integration.\n\u2022 Identify new fraud patterns through data analysis on varieties of data.\n\u2022 Design, develop and maintain predictive models used for risk management.\n\u2022 Identify and analyze anomalous data (including metadata).\n\u2022 Analyze data using appropriate tool set.\n\u2022 Evaluate research for practical application.\n\u2022 Analyze data using mathematical/statistical methods.\n\u2022 Apply mathematical and computation methods and lines of reasoning.\n\u2022 Works with software engineers, analysts and data modelers to assist in creating advanced technical software routines to facilitate analysis.\n\u2022 Fraud detection & automated ranking content quality.\n\u2022 Created customized reports and processes in SAS and Tableau Desktop.', u'Data Scientist\nKellogg Brown & Root\nAugust 2005 to June 2008\n\u2022 Visualizations and optimization techniques.\n\u2022 Summarize data through various tabular and visual modes.\n\u2022 Conduct and interpret multivariate analyses examples, including regressions with various distributions and duration models.\n\u2022 Collaborate with other analysts and key stakeholders to identify underlying trends, both internal and external, impacting current and future enrollment and financial considerations, and incorporate trends into forecast models.\n\u2022 Establish and standardize on the end to end, modeling process for data scientist, and work with product owner and engineering to define the set of tools and frameworks required to support the end to end modeling process.\n\u2022 Knowledge of data mining and machine learning algorithms, theories, principals and practices.\n\u2022 Experience with data manipulation, analytic tools, and data visualization', u'Data Scientist\nUnited States Army\nMarch 2002 to August 2005\n\u2022 Perform complex modeling, simulation and analysis of data and processes.\n\u2022 Analyze model performance and communicate model design and performance to business stakeholders, product owners, engineering, and fraud operations.\n\u2022 Facilitate the data collection sessions. Analyze and document data processes, scenarios, and information flow.\n\u2022 Determine data structures and their relations in supporting business objectives and provides useful data in reports.\n\u2022 Manually review logs & provide documentation guidelines to business process owners and management.\n\u2022 Obtain functional requirements from subject matter experts during group workshops or follow-up interviews.\n\u2022 Analyzing and modeling structured data using advanced statistical methods and implementing algorithms and software needed to perform analyses.\n\u2022 Facilitate the data collection sessions. Analyze and document data processes, scenarios, and information flow.\n\u2022 Facilitate the data collection sessions. Analyze and document data processes, scenarios, and information flow.', u'Data Scientist\nUniversity of Maryland Eastern Shore\nJanuary 1998 to February 2002\n\u2022 Knowledge of algorithms, data structures and performance optimization.\n\u2022 Experience with applied statistics and/or applied mathematics.\n\u2022 Facilitate the data collection sessions. Analyze and document data processes, scenarios, and information flow.\n\u2022 Analyze data using appropriate tool set.\n\u2022 Evaluate research for practical application.\n\u2022 Analyze data using mathematical/statistical methods.\n\u2022 Apply mathematical and computation methods and lines of reasoning.']",[u'Master of Science'],"[u'University of Maryland University College Adelphi, MD']",degree_1 : Master of Science
0,https://resumes.indeed.com/resume/49bc2c0b46eb4ad2,"[u'Director of Business Analytics\nAmerimark - Cleveland, OH\nJune 2014 to Present\nTeam lead for data science team and enterprise data warehouse.\nManaging data science and EDW roadmap, sprints, and releases.\nDesigned/implemented automated modeling platform to productionalize models inc. monitoring lift and model drift.\nDeveloped ongoing systematic models, e.g. ongoing attrition scoring.\nModeling in R, python, in SQl Server and a hadoop-spark-tensorflow ecosystem; combining machine learning and traditional econometric modeling, and large scale data engineering.\nEDW architecting,fact/dim tabling, BI, strategic analyses.', u'Chief Data Scientist & Explore Product Manager\nSegmint Software - Akron, OH\nApril 2013 to June 2014\nAkron, Ohio (startup - big-data programmatic ad-tech) 4/13 - 6/14\nChief Data Scientist & Explore Product Manager: Designed and delivered initial releases of automated modeling platform: primary focus on event-triggers (EBM), cross-sell and retention in financial-services. Solution abstracted for verticals such as retail.\nC Designed/implemented ensemble learning framework blending machine learning and parametric buyer\nbehavior models, dynamic lead valuation / dynamic pricing.\nC Leading the analytics team spanning ETL/data curation, data model (Pivotal/Greenplum), adaptive models w/ process monitoring.\nHands-on modeling, recursive text-mining of transactional data, in-database (PL/pgSQL)/noSQL; scrum process/agile product development; Established product Roadmap, feature/function design, consultative biz-dev engagements.', u'VP Client Insight, VP Risk, VP Client Strategy\nKey Bank\nMarch 2004 to April 2013\nProforma modeling of risk-based tiered pricing for re-entry to credit card issuance.\nLoss modeling of consumer portfolios - before and thru the financial crisis:\nEstablished a PD/LGD/EAD methodology in advance of the industry, now a standard; Created semi-automated modeling spanning $17B of portfolios- lines/loans, mortgage, auto, HELOC,etc. Responsible for ongoing ALLL, regulatory equity requirements, and handling audits of Fed/OCC, various internal/external audits; Stress testing design/execution.\nMarketing strategic analysis of product design elements and consumer segments, formulating CRM strategies for small business and retail.\n\nABA Graduate School of Banking.', u'Other: Management Consulting, Operations Improvement, Strategic Sourcing/procurement\nAT Kearney, Merck & Company (vaccine and tablet mfg)\nJune 2005 to December 2012\nAT Kearney: top billing consultant in supply chain practice - selling, projectizing, and delivering engagements.\n\nMerck: Global Commodity Manager hardware/software\nProcurement Re-engineering team.\nIndustrial Engineer - Operations Improvement\nsupport material flow/supply chain projects w/ Hepatitis-B and varivax vaccines and table mfg.', u""Data Scientist / marketing scientist\nPROS Revenue Management - Houston, TX\nJune 2001 to February 2004\nR&D: Development of dynamic pricing algorithms and software solutions - extending from the company's core airline industry pricing to new verticals: express packaging, public storage, fuel distillate/fuel wholesale market, and broadcast media.""]","[u'Ph.D. in Marketing Science (modeling)', u'M.S. in Operations Research, operations management (shortest path or A*)', u'B.S.']","[u'London Business School London\nDecember 2003', u'Industrial Engineering Purdue University\nJanuary 1991', u'Industrial & Systems Engineering Ohio University\nJanuary 1989']","degree_1 : Ph.D. in Marketing Science (modeling), degree_2 :  M.S. in Operations Research, degree_3 :  operations management (shortest path or A*), degree_4 :  B.S."
0,https://resumes.indeed.com/resume/58b04f22aa3f349a,"[u'Actuarial Analyst\nMilliman Inc - San Diego, CA\nJuly 2016 to Present\nAssist in actuarial projects such as; client market research, rate benchmarking, Medicare/Medicaid rate filing, reserving analysis (IBNP) and rate reconciliation\n\u25cf Conduct independent research projects using SQL, SAS, Python and R, such as create databases comparing Medicare allowed rates, allowed versus billing rates, payment rate trend analysis and Monte Carlo Simulations\n\u25cf Create detailed spreadsheets of client data, workflow processes and graphical analysis in Excel\n\u25cf Maintain file architecture; eliminate folder redundancy, evaluate checklists and update project memorandum\n\u25cf Automate creation of client ready charts and attachments with Excel and VBA\n\u25cf Conduct peer review of high-risk work and edit client facing documents\n\u25cf Brainstorm project implementation and new research projects\n\u25cf Study and pass actuarial examinations', u'Data Scientist\nNeoway Business Solutions - Florianopolis, BR\nOctober 2015 to April 2016\nData preparation and analysis conducted in R, Python and SQL\n\u25cf Performed supervised and unsupervised ML techniques to solve real-world big-data problems such as high-dimensional clustering, regression analysis, decision-tree and random forest algorithms\n\u25cf Conducted results analysis for accuracy/model fitness and graphical analysis for client presentations\n\u25cf Researched optimal techniques and procedures to fit client needs and data limitations']",[u'Bachelor of Science in Cognitive Science'],"[u'University of California San Diego, CA\nJanuary 2014']",degree_1 : Bachelor of Science in Cognitive Science
0,https://resumes.indeed.com/resume/8b38f061398389a9,"[u'Data Scientist\nFord Motor - Dearborn, MI\nFebruary 2017 to Present\nDescription:\nThe Ford Motor Company (commonly referred to simply as ""Ford"") is an American multinational automaker headquartered in Dearborn, Michigan. The company sells automobiles and commercial vehicles under the Ford brand and most luxury cars under the Lincoln brand. Ford also owns Brazilian SUV manufacturer, Troller, and Australian performance car manufacturer FPV.\n\nResponsibilities:\n\u2022 Developed a Predictive analytics based Online Advertising Pricing Model to maximize client\'s net revenues\n\u2022 Predicted accurate Revenue per Click estimates with confidence intervals for millions of Advertisements in the client\'s portfolio\n\u2022 Deployed an Ensemble of machine learning models including Neural Networks, Random Forest & Gradient Boosting Method\n\u2022 Built a Fraud traffic detection system to flag potential bot sessions that cause inflated billings to the client\'s customers\n\u2022 Apply data visualization, descriptive and summary statistical techniques for discovery and timely insights\n\u2022 Extract data from client database, check the data sanity and load for data processing\n\u2022 Develop SQL scripts to access client database for data analysis\n\u2022 Understand the co relation between the dependent variable and independent variables and select best variables\n\u2022 Applies data aggregation, descriptive analysis and data presentation techniques and tools to communicate complex findings and recommendations to influence others to take action\n\u2022 Create independent variables from the historical data and run statistical models to predict the value of the search keywords\n\u2022 Validate the predictions and submit the keyword bids\n\u2022 Responsible to migrate the ETL part of the code base from statistical programming software R to SQL, new code reduced the run time from 1 day to 2 hours\n\u2022 Responsible for handling the database access (locking) and error flagging system\n\u2022 Responsible for deploying the code for client accounts across Europe\n\u2022 Developed Python code for Semantics Analysis of the keywords\n\u2022 Revive dead keywords as part of Search Engine Optimization\n\u2022 Worked on Sentiment analysis to analyze the importance of a keyword to revive it back into the system; Automate the process of data extraction, transformation and processing using soft coded Linux shell scripts\n\u2022 Work on Linux for data quality checks varying from quick to complex data checks\n\u2022 Develop predictive models in Neural Networks and Gradient Boosting methods to be run in statistical software R\n\u2022 Provide period-over-period analysis and comparison of data extracts and trends.', u'Data Scientist\nMcKesson Corp - New York, NY\nDecember 2015 to January 2017\nDescription:\nMcKesson is based in the United States and distributes health care systems, medical supplies and pharmaceutical products. Additionally, McKesson provides extensive network infrastructure for the health care industry; also, it was an early adopter of technologies like bar-code scanning for distribution, pharmacy robotics, and RFID tags.\n\nResponsibilities:\n\u2022 Worked in Various stages of Data warehousing life cycle of development database logical and physical design, ETL process, performance tuning on Windows NT.\n\u2022 Created Mapping Documents, ETL technical specifications and various documents related to Data Migration.\n\u2022 Provided support to Data Architect and Data Modeler in designing and implementing Databases for MDM using ERWIN Data Modeler Tool and MS Access\n\u2022 Involved in the Database Designing (Relational and Dimensional models) using Erwin.\n\u2022 Performed data analysis and data profiling using complex SQL on various source systems including Oracle and Netezza.\n\u2022 Involved in requirement gathering and database design and implementation of star-schema, dimensional data warehouse using Erwin.\n\u2022 Managed, updated and manipulated report orientation and structures with the use of PivotTables and V-Lookups.\n\u2022 Tested several business reports developed using Business Objects including dashboard, drill-down, summarized, master-detail & Pivot reports.\n\u2022 Implemented very advanced and complex queries in DDL and DML involving Joins, Sub-queries. Alter Table etc.\n\u2022 Active involvement in Optimization Process through database capacity planning, creation of materialized views, partitions, tables, views, indexes as tuned and optimized SQL statements.\n\u2022 Prepared complex T-SQL queries, views and stored procedures to load data into staging area.\n\u2022 Created T-SQL stored procedures to improve the performance of aggregate queries dynamically.\n\u2022 Assisted in logical and physical Modeling for OLAP and OLTP systems.\n\u2022 Extensively worked on the ETL mapping, source data analysis, source to target mapping and documentation of OLAP reports requirements.\n\u2022 Experience in Data Visualization including producing tables, graphs, listings using various procedures and tools like Tableau.\n\u2022 Worked with Tableau Report Writers to test, validate data integrity of reports.\n\u2022 Support for the development, pre-production, and the production databases.', u""Data Analyst\nFleetCor Technologies Inc - Norcross, GA\nFebruary 2014 to November 2015\nDescription: FleetCor Technologies, Inc. provides specialized payment products and services to Dataes, commercial fleets, oil companies, petroleum marketers, and government entities in North America, Europe, South Africa, and Asia.\nResponsibilities:\n\n\u2022 Compiled data submitted by States and prepared summary reports based on them to monitor overall AYUSH program implementation\n\u2022 Utilized SPSS and MS Excel to perform data analysis and generate reports\n\u2022 Worked on promotion of AYUSH& other programs through media. Designed informational materials, worked as part of organization team, and coordinated with media & publishing division of GoI\n\u2022 Worked in the organizing committee of AROGYA Melas & Multi-media campaigns that was organized all over India. These focused on showcasing the AYUSH program & it's strengths as well as implementation. Designed information materials for the campaigns and prepared TORs for conducting the same\n\u2022 Undertook other administrative duties as assigned.\n\u2022 Support business users across Customer Operations by applying knowledge in ERP: Order Management, Service Contract, Install Base, and TCA (Customer Master)\n\u2022 Analyze moderately complex business transactions to perform recommended data adjustments\n\u2022 Interpret issue trends to create a quality checklist to the global support team\n\u2022 Manage cases and provide resolution meeting established SLA (service level agreement)\n\u2022 Provide Quarter-end support for Sales and Order Processing.\n\u2022 Assist with identifying potential process improvements and potential long term fixes.\n\u2022 Handling customer issues related to orders on chats.\n\u2022 Support on website issues.\n\u2022 Login support.\n\u2022 Resolving problem to place hassle free orders and tracking them.\n\u2022 Providing end support to your team members as floor support.\n\u2022 Sharing email updates if any and downloading the update to the team."", u""Data Analyst\nFirst Indian Corporation Private Limited - Hyderabad, Telangana\nFebruary 2011 to January 2014\nDescription: First Indian Corporation Private Limited, a member of The First American family of companies, provides specialized offshore transaction services, technology services, and analytics to the mortgage industry.\n\nResponsibilities:\n\u2022 Activating lease contracts to generate invoices for customer payments in a system, Pyramid.\n\u2022 Assigning work to the team members and handling in-house workflow system.\n\u2022 Handling review calls with front-end customers on process updates.\n\u2022 Being involved in the quality check process for peers\n\u2022 Work closely with the deal desk team and other business partners like corporate systems ops and finance, to ensure accurate quotes and contracts are quoted and booked Requirement:\n\u2022 Involved with various customers for the quick resolution of pending deals or orders; preparing operational metrics for the process.\n\u2022 Maintaining relationship with clients to achieve quality service norms by resolving their service related critical issues.\n\u2022 Handling escalation cases in SFDC (Vm Star Application of Vm ware) with quality.\n\u2022 Preparing control reports for the team and self are accurate.\n\u2022 Looking after US& Domestic customers and following TAT.\n\u2022 Maintaining daily trackers of the team in processing orders.\n\u2022 Providing updates on University's closure and the action to be taken by CSR to understand the delivery time to reach customer.\n\u2022 Identify root cause analysis of all complaints and standardize processes aspect wise & recommend process improvements. Conduct or coordinate gaps in internal business process are verified and fixed. Identify critical processes, schedule and audit processes/practices for compliance and performance, generate report, follow-up for closing of gaps\n\u2022 Supporting backend processes and ensuring that the orders are dispatched in warehouse and finally to end customer.\n\u2022 Ensuring SLA and TAT for all the process, which includes Singapore and US customers as well.""]","[u""Bachelor's""]",[u''],"degree_1 : ""Bachelors"""
0,https://resumes.indeed.com/resume/c9cfb8dd0a68a337,"[u'Jr. Data Scientist\nJacksonville Electric Authority\nMarch 2018 to Present\nWorking on prediction models to evaluate fair pay rates for utility services. Also, Responsible for developing innovative projects to save costs by developing machine learning models on Hadoop Data Lakes which includes meter data, customer demographics.\nCosts are reduced by correctly predicting outages and optimizing servicing trips done by personnel. TECH: SAS, R, ML', u'Developer, Data Scientist\nDept. of Nephrology\nJanuary 2017 to December 2017\nDeveloped appropriate data cleaning algorithms and machine learning models to predict post surgery complications.Responsible for presenting project progress in weekly meetings to surgeons using visualization tools. TECH: Python, Scikit, Numpy, Pandas', u""Manager\nUniversity of Florida - Gainesville, FL\nOctober 2016 to December 2016\nGainesville, FL October-December 2016\n\u25cf Designed a blind hiring application, to help remove bias like gender, race and sex from an applicant's resume. The design\nenhanced communication process within the hiring team to smoothen the process. Prototype highly appreciated by the Hiring\nManager. TECH: InVision, Balsamiq"", u'Full Stack Developer - Summer Intern\nIndian Institute of Technology - Delhi, Delhi\nMay 2015 to August 2015\nDesigned and developed a prototype web-app which supports the various services of a university portal like Salary Manager,\nCourse Assignments etc. Excelled at taking user requirements and programming functions to accommodate them, and build user\n\nauthentication and access control features. TECH: Python (Django Framework), HTML, CSS, Javascript\n\nPROJECTS:']","[u'Master of Science in Computer Science', u'Senior Certificate', u'Bachelor of Technology in Computer Science and Engineering']","[u'University of Florida Gainesville, FL\nDecember 2017', u'University of Florida Gainesville, FL\nMay 2016', u'Jaypee University of Information Technology Solan, Himachal Pradesh\nMay 2016']","degree_1 : Master of Science in Compter Science, degree_2 :  Senior Certificate, degree_3 :  Bachelor of Technology in Compter Science and Engineering"
0,https://resumes.indeed.com/resume/1bd11a2a13dff0e9,"[u'Data Scientist\nSantander Bank - Holmdel, NJ\nJune 2017 to Present\nSantander Bank is based in Boston and its principal market is the northeastern United States. The Bank offers financial services and products including retail banking, mortgages, corporate banking, cash management, credit card, capital markets, trust and wealth management, and insurance.\n\nThe project was to build predictive models for the identification/detection of fraudulent transactions by applying machine learning methods, principle component analysis, and logistic regression on large dataset.\n\nResponsibilities:\n\u2022 Participated in all phases of data acquisition, data cleaning, developing models, validation, and visualization to deliver data science solutions.\n\u2022 Worked on fraud detection analysis on payments transactions using the history of transactions with supervised learning methods.\n\u2022 Collected data in Hadoop and retrieved the data required for building models using Hive.\n\u2022 Developed Spark Python modules for machine learning & predictive analytics in Hadoop.\n\u2022 Used Pandas, Numpy, Seaborn, Matplotlib, Scikit-learn in Python for developing various machine learning models and utilized algorithms such as Decision Trees, Logistic regression, Gradient Boosting, SVM and KNN.\n\u2022 Used cross-validation to test the models with different batches of data to optimize the models and prevent overfitting.\n\u2022 Used PCA and other feature engineering techniques for high dimensional datasets while maintaining the variance of most important features.\n\u2022 Created Transformation Pipelines for preprocessing large amount of data with methods such as imputing, scaling, selecting, etc.\n\u2022 Ensemble methods were used to increase the accuracy of the training model with different Bagging and Boosting methods.\n\nEnvironment:\nHadoop 2.x, HDFS, Hive, Pig Latin, Python 3.x (Numpy, Pandas, Scikit-learn, Matplotlib), Jupyter, GitHub, Linux', u""Data Analyst/Data Scientist\nSCIO Health Analytics - Hartford, CT\nApril 2015 to May 2017\nSCIO Health Analytics provides analytics solutions and services that turns data into actionable insights for health care providers in the United States and globally. Services also include medical and pharmacy claims auditing, inpatient data pursuits, care gaps closure, and commercial analytics.\n\nI was part of the team that worked with Subrogation claims of Healthcare Providers such as Humana. The objective was to load data, analyze, and provide monthly reports for the predictions on a claim's potential of a third-party recovery. Tableau and SSRS were used to build claim and recovery reports.\n\nResponsibilities:\n\u2022 Assembled a Predictive Modelling module by using supervised learning for Subrogation Claim Prediction to identify which claims would be classified as having Subrogation potential.\n\u2022 Implemented models such as Logistic Regression and Na\xefve Bayes, in Python using scikit-learn, to predict the claim potential outcome.\n\u2022 Dimensionality Reduction techniques applied to refine the attribute lists and feature selection applied to rank selected features to generate accurate results.\n\u2022 Gathered requirements and business rules from business users to implement Predictive Modelling.\n\u2022 Designed and developed ETL packages using SSIS to create Data Warehouses from different tables and file sources like Flat and Excel files, with different methods in SSIS such as derived columns, aggregations, Merge joins, count, conditional split and more to transform the data.\n\u2022 Designed reporting solutions for different stakeholders from mock-up till deployment in different areas such as Potential Subrogation claims, Monthly Revenue from Subrogation & Transactions.\n\u2022 Performed data visualization and designed dashboards with Tableau, and provided complex reports, including charts, summaries, and graphs to interpret the findings for Adjustors to view various claim information.\n\u2022 Optimized queries in T-SQL by removing unnecessary columns and redundant data, normalized tables, established joins and indices; developed complex SQL queries, stored procedures, views, functions and reports that meet customer requirements.\n\nEnvironment:\nPython 3.x (Scikit-learn, Matplotlib), Jupyter, SQL Server 2012, MS SQL Server Management Studio, MS BI Suite (SSIS/SSRS), T-SQL, Visual Studio BIDS, Tableau"", u'SQL BI Developer\nADP - Chennai, Tamil Nadu\nFebruary 2012 to March 2015\nADP is a leading provider of human resources management software and services worldwide.\n\nThis project was done for an internal business unit (ADP France) to comply with French statutory requirements for employee training. Goal was to develop a web-based SQL application built upon a baseline HRMS application to generate/support the Report development for training plans, budget preparation, cost tracking and 2483 reporting.\n\nResponsibilities:\n\u2022 Collected requirements from business users, and designed report models to meet business requirements.\n\u2022 Directed and managed meetings with clients, tracked document changes and ensured sign-off from clients.\n\u2022 Maintained and developed complex SQL queries, stored procedures, views, functions and reports that meet customer requirements using Microsoft SQL Server 2008 R2.\n\u2022 Created Views and Table-valued Functions, Common Table Expression (CTE), joins, complex subqueries to provide the reporting solutions.\n\u2022 Optimized the performance of queries with modification in T-SQL queries, removed the unnecessary columns and redundant data, normalized tables, established joins and created index.\n\u2022 Created, managed, and delivered interactive web-based reports to support daily operations.\n\u2022 Validated reports and resolve issues in a timely manner.\n\u2022 Developed and implemented several types of Reports (Training Reports, Schedules, Costs Summary Reports and Annual 2483 Report) by using features of SSRS such as sub-reports, drill down reports, summary reports and parameterized reports.\n\u2022 Designed and developed new reports and maintained existing reports for the Human Resource Management System Dashboards using Tableau, Qlikview and Microsoft Excel to support the business strategy and management.\n\u2022 Identified process improvements that significantly reduce workloads or improve quality.\n\nEnvironment:\nSQL Server 2008 R2, MS SQL Server Management Studio, SSRS, T-SQL, Visual Studio BIDS, Tableau, Qlikview']",[u'Master of Science in Business & Information Systems in Business & Information Systems'],"[u'New Jersey Institute of Technology Newark, NJ']",degree_1 : Master of Science in Bsiness & Information Systems in Bsiness & Information Systems
0,https://resumes.indeed.com/resume/ce5fa591757d73c1,"[u""Data Scientist\nDanske Bank - Tampa, FL\nJanuary 2017 to Present\nResponsibilities:\n\u2022 Participated in all phases of data mining, data collection, data cleaning, developing models, validation, and visualization and performed Gapanalysis.\n\u2022 Used pandas, numpy, seaborn, scipy, matplotlib, scikit-learn, NLTK in Python for developing various machine learning algorithms.\n\u2022 Provide dataarchitecture support to enterprisedatamanagement efforts, such as the development of the enterprise data model and master and reference data, as well as support to projects, such as the development of physicaldatamodels, data warehouses and datamarts.\n\u2022 Worked on ApacheSpark with Python to develop and execute BigDataAnalytics and Machine learning applications, executed machine learning use cases under SparkML and MLLib.\n\u2022 Handled importing data from various data sources, performed transformations using Hive, MapReduce, and loaded data into HDFS\n\u2022 Developed MapReduce/Spark Python modules for machine learning & predictive analytics in Hadoop.\n\u2022 Utilized machine learning algorithms such as linear regression, multivariate regression, Naive Bayes, Random Forests, K-means, & KNN for dataanalysis\n\u2022 Designed both 3NF data models for ODS, OLTP systems and dimensional data models using Star and SnowflakeSchemas.\n\u2022 Implemented a Python-based distributed random forest via Pythonstreaming.\n\u2022 Worked with Datagovernance, Dataquality, datalineage, Dataarchitect to design various models and processes.\n\u2022 Developed Analytical systems, data structures, gather and manipulate data, using statistical techniques.\n\u2022 Used Python and R for programming for improvement of model. Upgrade the entire models for improvement of the product\n\u2022 Experienced in managing and reviewing Hadoop log files\n\u2022 Loading data into the Hadoop distributed file system (HDFS) with the help of Kafka\n\u2022 Independently coded new programs and designed Tables to load and test the program effectively for the given POC's using with Big Data/Hadoop.\n\u2022 Designed data models and data flow diagrams using Erwin and MSVisio.\n\u2022 Developed script using hadoop hdfs command to download the file from sftp box and put into hdfs path\n\u2022 Assists business with casual inferences & observations with finding patterns, relationships in data\n\u2022 Used the AgileScrum methodology to build the different phases of Softwaredevelopmentlifecycle.\n\u2022 Application of various machine learning algorithms and statistical modeling like Decision Trees, Random Forest, Regression Models, neural networks, SVM, clustering to identify Volume using scikit-learn package\n\u2022 Contribute to the detailed statistical analysis of our Inclusion and Collaboration data, to identify actionable insights, and building (predictive) models.\n\u2022 Evaluate the performance of various algorithms/models/strategies based on the real world data sets.\n\u2022 Used PySpark Streaming to divide streaming data into batches for batch processing and Spark-SQL/Streaming for faster testing.\n\u2022 Worked with statistical methodologies such as: logit models, generalized linear models, categorical dataanalyses, ANOVA and regression\n\u2022 Used Tableau and designed various charts and tables for data analysis and creating various analytical Dashboards to showcase the data to managers.\n\u2022 Create and statistically analyze large data sets of internal and external data\n\u2022 Applied statistical modeling techniques on the queried datasets such as regression analysis and curve fitting techniques\nEnvironment: Python3.3.2, Jupyter Notebook, SQL, Oracle,MapReduce, R studio, SPSS, Hadoop (HDFS), Hive, Tableau, MLLib, regression, Spark, regression, logistic regression, random forest, neural networks, SVM (Support Vector Machine)"", u""Data Scientist\nStefanini IT Solutions - Dearborn, MI\nSeptember 2015 to November 2016\n\u2022 Created a high-level industry standard, generalized data model to convert it into logical and physical model at later stages of the project using Erwin and Visio\n\u2022 Worked on designing, Constructing, Testing and Deploying the Cognos Framework Manager Models, Packages, Reports and Power Play Models.\n\u2022 Implemented the Dashboard reports with the various Cognos Dimensional Functions.\n\u2022 Enforced referential integrity in the OLTP data model for consistent relationship between tables and efficient database design.\n\u2022 Created logical data model from the conceptual model and it's conversion into the physical database design using Erwin.\n\u2022 Tuned and optimization of Teradata Queries Created the ETL data mapping documents between source systems and the target data warehouse.\n\u2022 Performed Data mapping using Business Rules and data transformation logic for ETL purposes.\n\u2022 Developed and maintained data dictionary to create metadata reports for technical and business purpose.\n\u2022 Designed the ER diagrams, logical model (relationship, cardinality, attributes, and, candidate keys) and physical database (capacity planning, object creation and aggregation strategies) as per business requirements.\n\u2022 Worked on building up a SQL query for extraction of data from the warehouse for a reporting schedule\n\u2022 Facilitated JAR/JAD sessions to determine data definitions and business rules governing the data and facilitated review sessions with subject matter experts (SMEs) on the logical and physical data models.\n\u2022 Worked with statistical methodologies such as: logit models, generalized linear models, categorical data analyses, ANOVA and regression\n\u2022 Applied data naming standards, created the data dictionary and documented data model translation decisions and maintained DW metadata.\n\u2022 Worked on enterprise logical data modeling project (in third normal form) to gather data requirements for OLTP enhancements.\n\u2022 Created Hive tables, loaded data and wrote Hive queries that run within the map.\n\u2022 Involved in extensive Data validation by writing several complex SQL queries and Involved in back-end testing and worked with data quality issues.\n\u2022 Used Erwin and Visio to create 3NF and dimensional data models and published to the business users and ETL / BI teams.\n\u2022 Used R studio to apply Machine Learning to find correct model for business using Linear regression, Logistic regression, SVM, KNN, K -Mean algorithms for decision making.\n\u2022 Data Visualization using TABLEAU, ggplot2 package in Python.\n\u2022 Text Mining Packages under Python Programming to understand the frequent words for each categorical rating.\n\u2022 Worked in importing and cleansing of data from various sources like Teradata, Oracle, flat files, SQL Server with high volume data.\n\nEnvironment: Erwin 9.3, 9.5, Tableau 9.3, 10, R Studio, Tableau, Python 3.0, MS Visio, Oracle 11g, Toad, Oracle Designer, SQL Server 2008 R2, Oracle SQL developer 11g, SQL, PL/SQL, Flat Files, Teradata, MS Excel 2007, PL/SQL, Business Objects, ETL Tools Informatica9.1, SSIS"", u'Data Modeler\nLevel one Banking Financial - Farmington Hills, MI\nJanuary 2014 to August 2015\nResponsibilities:\n\u2022 Data modeling process was created and developed on the samples of data to test for the new data base system\n\u2022 Constructed the conceptual model & the logical model for the data modelling using Access\n\u2022 SQL quires are conducted for the verification of the conceptual model that built using the Access tool\n\u2022 Involved in Data mapping specifications to create and execute detailed system testplans.\n\u2022 Built a system model-based design for the big book business with the help of a Simulink for the work flow operations.\n\u2022 Used Erwin and Visio to create 3NF and dimensional data models\n\u2022 Developed mathematical/statistical modeling techniques to synthesize multiple scientific sources ofinformation and applied data management\n\u2022 Work independently to develop models that address specific business problems related to enrollmentmanagement, retention, marketing and class scheduling\n\u2022 Perform complex modeling, simulation and analysis of data and processes\n\u2022 Communicated reports and presentations for explaining the data, methodology, trends and analysis, to abroad group of clients with varying backgrounds\n\u2022 Skilled in System Analysis, E-R/Dimensional Data Modeling, Database Design and implementing RDBMS specific features.\n\nEnvironment: SQL, Simio, Simio 8, Erwin 9.3, Tableau 8, 10, MS Access', u'Data Analyst / Data Modeler\nData Care - IN\nFebruary 2012 to December 2013\nResponsibilities:\n\u2022 Performed data analysis and profiling of source data to better understand the sources.\n\u2022 Analyzed the source data coming from different sources (SQL Server, Oracle, Sales force and from flat files like Access and Excel) and working with business users and developers to develop the Model\n\u2022 Designed and Developed Oracle PL/SQL Procedures and UNIX Shell Scripts for Data Import/Export and Data Conversions.\n\u2022 Skilled in developing Entity-Relationship diagrams, modeling Transactional Databases and Data Warehouse.\n\u2022 Created 3NF business area data modeling with de-normalized physical implementation data and information requirements analysis using Erwin tool.\n\u2022 Expertise in data base programming SQL, MS Access, Oracle, DB2, Database tuning and Query optimization.\n\u2022 Extracted data from the source database using Proc SQL. Created SAS/Views from tables in Oracle database.\n\u2022 Used ODS to generate results in Excel, PDF, and HTML formats. Involved in handling the ad-hoc reporting requirements. Extensively used Data _NULL_ to create XML files from SAS datasets.\n\u2022 Involved in extensive data analysis on the Teradata and Oracle systems querying and writing in SQL.\n\u2022 Created tables, views, sequences, indexes, constraints and generated SQL scripts for implementing physical data model.\n\u2022 Developed SAS datasets from Excel files, XML files, Oracle Server, TD SQL and DB2 database.\n\u2022 Conducted several Physical Data Model training sessions with the ETL Developers.\n\u2022 Conducted complex ad-hoc programming and analysis including statistical programming and analysis.\n\u2022 Perform Data validation and data quality work.\n\u2022 Strong data collection, analysis and interpretation skills\n\u2022 Strong SQL querying skills\n\u2022 Strong written and verbal communication skills\n\u2022 Proven ability to work independently and within a team structure\n\u2022 Proficiency in Microsoft Office Suite\n\nEnvironment: SAS 9.1.3, Base SAS (MACROS, Proc SQL, ODS), SAS/ACCESS, SAS/STAT, SAS/GRAPH, SAS Enterprise Guide 3.0, SAS Enterprise Miner,SQL, PL/SQL, UNIX,MS-Windows XP, MS-Office.']","[u""Bachelor's""]",[u''],"degree_1 : ""Bachelors"""
0,https://resumes.indeed.com/resume/2f684f3051e302b8,"[u'Data Scientist Intern\nMicron Technologies\nMay 2017 to December 2017\n\u2022 Image Analytics: Classified images of surface at various temperatures to help quality engineers achieve a better profiling of thermal characteristics using an ensemble of machine learning models on quantitative and vision analytical features.\n\u2022 Causal Inference: Leveraged causal inference and machine learning to derive relation between internal parameters and customer test failures to help engineers in identifying root causes and insights for the enhancement of the product yield.\n\u2022 Failure Analytics: Reinforced sampling by developing a semi-supervised machine learning model utilizing large unlabeled big data and small labeled data to predict score for devices which significantly improved quality team analysis cost and time.\n\u2022 Forecasting: Collaborated in researching and structuring a new quality control methodology for forecasting accuracy of predicted internal score to help senior management gain useful key trends and insight of the internal performance.\n\u2022 Analytical Dashboard: Developed R shiny application to reduce time for worldwide manufacturing and quality assessment team to perform various hypothesis testing and advanced statistical analysis based on different distribution assumptions.', u'Data Scientist and AI Intern (Co-op)\nShoptaki - New York, NY\nJanuary 2017 to May 2017\n\u2022 Distributed Neural Network: Designed and implemented an algorithm for distributed neural network architecture based on weight sharing to enable AI (artificial intelligence) power on blockchain.\n\u2022 Fraud Detection: Implemented online machine learning pipeline for detection and reporting of fraud transactions in a blockchain ledger based on demographic and transaction features of users to reduce load using python and PySpark.', u'System Engineer/Developer (DevOps Web Services)\nBank of New York Mellon Technology - Pune, Maharashtra\nJuly 2014 to July 2016\nSystem Health Analysis: Developed web servers health monitoring scripts using python for status and load analysis using system and web logs and automated creation of cloud load balancer to enhance the reliability and productivity by 12.5%.', u'Project Intern\nPersistent System Ltd - Pune, Maharashtra\nJune 2013 to May 2014\nHadoop Cluster Service: Open-sourced Amazon Elastic MapReduce web service to create a software as a service on top of Eucalyptus cloud to spin-up instances and auto-configure Hadoop to run MapReduce analysis job using Java, shell, and Chef.']","[u'Master of Science in Data Science', u'Bachelor of Engineering in Computer Engineering']","[u'Indiana University Bloomington\nAugust 2016 to May 2018', u'University of Pune Pune, Maharashtra\nAugust 2010 to May 2014']","degree_1 : Master of Science in Data Science, degree_2 :  Bachelor of Engineering in Compter Engineering"
