,resume,work_exp,edu,univ
0,https://resumes.indeed.com/resume/36002491f85ba23a,"[u'Data Analyst\nHappiest Minds Technologies - Bengaluru, Karnataka\nApril 2015 to November 2017\nDevelop analytics solutions for multinational clients using technologies such as, Python, R, and\nTableau\n\u2022 Lead all calls and meetings to gather requirements, and assist on SOW creation\n\u2022 Data visualization for a US IT infrastructure company, creating dashboards for company website\nusing Python Plotly and Tableau.\n\u2022 Increase company sales by providing complete analytics solution for infrastructure clients using\nPython, and R.\n\u2022 Created footfall prediction for medical companies using Python scikit-learn\n\u2022 Provide social media analytics solutions, including analysis of Audience Data, Engagement Data\nSentiment Data Analysis using Python\n\u2022 Led all meetings and conference calls for product demos, product pre-releases, and enhancement\nrequests', u'Project Engineer\nWipro Technologies - Kochi, Kerala\nJuly 2011 to June 2014\nDeveloping and maintaining different modules of multifunction peripheral software for a\nJapanese multinational conglomerate.\n\u2022 Oversee the project for development of scan applications for MFP using C++\n\u2022 Automate texting editing software using Python\n\u2022 Oversee the development of Microsoft office Add-in for Scan using C#']","[u'', u'Bachelor of Technology in Computer Science']","[u'Praxis Business School Kolkata, West Bengal\nJanuary 2014 to January 2015', u'Model Engineering College Kochi, Kerala\nJanuary 2007 to January 2011']"
0,https://resumes.indeed.com/resume/3a6b373bb8ef33b3,"[u'Data Analyst\nAnnik Inc - Bellevue, WA\nJune 2017 to March 2018\n\u2022 Created a website using C# for Program Managers to view different events to plan product launches effectively\n\u2022 Automated the process to extract work item data from Visual Studio Team Service into MS SQL using C# for reporting\n\u2022 Created Power BI dashboards for a Fortune 100 company effective issue tracking across the system and strategic overview\n\u2022 Collaborated with multiple teams to ensure data flow across systems for smooth product launches\n\u2022 Conducted gap analysis, identified and provided tactical recommendations for 6 software tools of a Fortune 100 company across 8 parameters to drive executive level strategic overview\n\u2022 Built a case study on the framework and methodology implemented, to be used by business development managers for potential\nclients and improve organizational growth\n\u2022 Analyzed Partner Network data to understand their satisfaction with the Partner Program using Python\n\u2022 Document business requirements for continuous improvement process using automation to reduce product launch by 5 days\n\u2022 Delivered weekly project status review to stakeholders focused on deliverables, issues and risks\n\u2022 Oversaw project lifecycle execution from planning, execution, risk valuation\n\u2022 Designed a business model for business growth and expansion\n\u2022 Participated in Project Management Bootcamp to gain knowledge in project management', u""Senior Systems Engineer\nInfosys Ltd - Pune, Maharashtra\nMay 2013 to August 2016\n\u2022 Built custom reporting solution to extract data from multiple DB2 instances into concise report, reducing analysis time for the business from 3 hours to 30 mins using SQL and object - oriented programming\n\u2022 Led a team of 3 resources for system analysis, development and QA support for application development\n\u2022 Initiated and mentored team members on Member Enrolment Application for knowledge transfer\n\u2022 Optimized SQL queries using Performance Analysis Tool, Strobe, to increase performance by 20%\n\u2022 Created project estimates based on code changes and resources allocation\n\u2022 Earned the 'Best Debutant' Award for outstanding performance for project delivery in Q2 2014""]","[u'MS in Information Management', u'Bachelor of Engineering in Engineering']","[u'University of Washington Seattle, WA\nSeptember 2016 to June 2018', u'University of Mumbai Mumbai, Maharashtra\nAugust 2008 to June 2012']"
0,https://resumes.indeed.com/resume/4d2bd193cc520b58,"[u""Software Engineer Intern\nOpenClinica LLC - Waltham, MA\nJanuary 2014 to April 2014\n\u2022 Investigated and reported products's defects. Designed test plans and executed automation test scripts using\nSelenium IDE to accelerate testing turn-around time by 60%.\n\u2022 Initiated and created application security vulnerabilities test based on source code review and Burp Suite in addition to company's routine functionality test.\n\u2022 Developed a Session ID Hacker app to illustrate the risk and remediation strategy of product's security\nvulnerabilities to the Dev team."", u""Data Analyst Intern\nMonotype Imaging Inc - Woburn, MA\nJanuary 2012 to January 2013\n\u2022 Designed SQL queries in Oracle database to generate revenue reports based on sales and financial team's requests.\n\u2022 Maintained and updated database daily, provided technical help to sales team and financial analysts on database.\n\nProject\nUser Information System (JAVA Web Application)\n\u2022 Tools: JAVA, SpringMVC, Spring, MyBatis, JSP, MySQL, JUnit, HTML\n\u2022 A user information management system. Functions includes user login/logout/register, create/read/update/delete\nuser information, and search user information.\n\u2022 Spring MVC used for request routing and handling. MyBaits used as data persistence layer. JUnit used for unit\ntesting framework. MySQL for database.\n\nSession ID Hacker (Web Security) related with experience in OpenClinica\n\u2022 Tools: Cross-Site Scripting(XSS), javascript, Node.js, Burp Suite Scanner.\n\u2022 A server-side hacking application built with Node.js. Functions include stealing authenticated users's session ID and automatically changing users' password and saving new passwords to hacker's server.""]","[u'MS in Computer Information Systems in Computer Information Systems', u'MS in Finance in Finance', u'BS in Biology Science in Biology Science']","[u'Bentley University in Waltham\nJanuary 2013 to January 2015', u'Bentley University in Waltham\nJanuary 2010 to January 2012', u'Capital Normal University in Beijing Beijing, CN\nJanuary 2005 to January 2009']"
0,https://resumes.indeed.com/resume/8831f04940c30361,"[u'Lead Data Warehouse Engineer\nBirst Analytics Platform - San Francisco, CA\nJanuary 2007 to Present\nSan Francisco, CA Microsoft SQL Server\nJanuary 2007 - PRESENT 2008-2014\nLead Data Warehouse Engineer Microsoft Analysis']",[u'in Business Management'],"[u'Interamerican University of Puerto Rico Rio Piedras, PR\nJune 2001 to May 2003']"
0,https://resumes.indeed.com/resume/7aef5b5b9dd78e4d,"[u'GM Financials\nSeptember 2017 to Present\n\u2022 Conducting Fraud Detection in regular time intervals using Predictive Analytics and Behavioral Analytics.\n\u2022 Managed diverse data sets and ensures they effectively advance risk identification, issue management and control effectiveness.\n\u2022 Identification and collection of data for Key Risk Indicators (KRIs) to detect Fraud.\n\u2022 Analyzed data using various statistical PROCEDURES like PROC SUMMARY, PROC MEANS, PROC FREQ, PROC UNIVARIATE, PROC REG and PROC ANOVA for modelling.\n\u2022 Used Enterprise Miner for taking data and converting them into required SAS datasets. Also used the SAS Forecast Studio for appropriate analysis of data for different Forecasting models like Time Series Forecasting.\n\u2022 Used SAS Data Step logics to Sort, Merge, Stack, Update and Interleave datasets for producing required analysis of data and used SAS Enterprise Guide to produce required Reports.\n\u2022 Extensively used PROC SQL and SELECT sub-queries to generate various reports by connecting to the Oracle DB.\n\u2022 Building, publishing customized interactive reports and dashboards, report scheduling using Tableau server.\n\u2022 Created Tableau Dashboards with interactive views, trends and with user level security for managers.', u'Data Analyst\nWipro Technologies\nFebruary 2014 to December 2015\n\u2022 Involved in Credit Risk Assessment Model to calculate risk factor for individual clients based on hierarchy\n\u2022 Performed competitor and customer analysis, risk and pricing analysis and forecasted results for credit card holders on demographical basis\n\u2022 Used transactional and customer records, external data, publicly- available information, and other information to identify suspicious or unusual activity.\n\u2022 Responsible for Statistical Data Model for different customers and credit card holders.\n\u2022 Proposed a set of action plans to mitigate the risks identified with the sourcing decision.\n\u2022 Used SAS Base programming as well as SAS Enterprise Guide 4.0 to produce various reports, charts and graphs.\n\u2022 Analyzed data using various statistical PROCEDURES like PROC SUMMARY, PROC MEANS, PROC FREQ, PROC UNIVARIATE, PROC REG and PROC ANOVA for modeling.\n\u2022 Used SAS/Macro facility to create macros for statistical analysis, reporting results and data extraction\n\u2022 Generated HTML, Listings, EXCEL and RTF reports for presenting findings of various statistical PROCEDURES using PROCEDURES like PROC REPORT, PROC PRINT, PROC FREQ and SAS ODS', u'Project Engineer\nWipro Technologies\nJanuary 2013 to February 2014\nTo ease maintenance of legacy documents and current documents we implemented Oracle ECM (Enterprise Content Management) integrating with Oracle EBS. As a team of 5, my role was to validate the legacy documents uploaded in SharePoint and help to migrate the documents to ECM through SOAP (Webservice).\n\u2022 Integrated Oracle EBS with ECM (Enterprise Content Management) using PL/SQL, SQL, and SOAP (Webservice).\n\u2022 Designed and implemented package and stored procedures using SQL and PL/SQL to validate the existing/new files in the system.\n\u2022 Developed Stored Procedures, Functions, Packages and SQL Scripts using PL/SQL.\n\u2022 Generated reports using Global Variables, Expressions, and Functions in Oracle Reports tool.\n\u2022 Loaded the data into database tables using SQL*loader from a text and excel files.\n\u2022 Developed data model, SQL Queries, SQL Query tuning process, and Schemas.\n\u2022 Extensive experience in developing complex Stored Procedures, Functions, Triggers, Views, Cursors, Indexes, Joins and Subqueries with T-SQL.']","[u'M.S. in Information Technology & Management', u'B.E. in Electrical and Electronics Engineering']","[u'The University of Texas at Dallas Dallas, TX\nAugust 2017', u'Sathyabama University Chennai, Tamil Nadu\nApril 2013']"
0,https://resumes.indeed.com/resume/891156a6b5c253cb,"[u'Data Engineer\nSimplePart - Atlanta, GA\nJune 2015 to Present\nMonitoring our production environment and ensuring data movement between the various systems are running smooth and efficient. Overseeing the data warehouse and ensuring all processes are completing successfully and producing accurate results. Working with our in-house digital marketing agency (SEO/SEM) teams to maintain and enhance processes that generate various data feeds.\n\n* Creating and maintaining data pipelines between manufacturers and other 3rd party systems utilizing Python and SQL Server.\n\n* Consuming web services with Python.\n\n* Creating and maintaining new data warehouse processes and tables to enhance our reporting system.', u'Software Engineer\nAmplify Nation - Atlanta, GA\nJune 2013 to May 2015\nMarketing technology startup building a multichannel marketing automation software solution. I primarily worked as a backend engineer using Python as our core language, Django as our web framework and PostgreSQL as our primary database. We also utilized MongoDB for some logging of marketing activities.\n\n* Created the automation processes to retrieve data from a 3rd party vendor, perform data hygiene, validate and then insert/update the records into the marketing database.\n\n* Created an API endpoint for the other services of the application to communicate with the database for various marketing activities.\n\n* Created processes that extracted the data and triggered the automated marketing jobs.', u'Business Analyst\nKids II - Atlanta, GA\nJanuary 2012 to May 2013\nProvide all departments with database solutions to improve and streamline processes by applying technical skills, development skills and business management skills. Creating data warehouses, developing critical reports and automating manual processes to improve productivity and reduce cost. I primarily work with Microsoft Access/VBA, SQL Server, Oracle PL/SQL and Excel/PowerPivot.\n\n* Working with sales and forecasting to build SSAS (Tabular) model to help analyze forecast accuracy.\n\n* Integrate our forecasting software (Logility) into our ERP software program.\n\n* Providing various departments with ad-hoc reports and data analysis.', u'SQL Developer\ndDirect - Atlanta, GA\nJuly 2009 to December 2012\nCreating, enhancing and maintaining SQL Server solutions for our largest clients. This includes working with TSQL to build stored procedures, building and maintaining SSIS processes, performing data analysis and ad-hoc reporting.\n\n* Perform response analysis to determine ROI on marketing activities.\n\n* Data mining activities to rank customer to allow for better selections for campaigns.\n\n* Performing location-based data visualization of customer/store location.\n\nNOTE: I also did some work here Oct. 2014 to May 2015.', u'Data Analyst\nAHC Media LLC - Atlanta, GA\nSeptember 2002 to June 2009\nProvide the marketing department with all database needs to complete successful marketing campaigns. Work with the circulation department to provide and implement programs and databases to streamline and automate various processes within the department. I worked with Microsoft SQL Server, Microsoft Access/VBA, Microsoft Visual FoxPro and Microsoft Office as well as several third party tools to accomplish this.\n\n* Developed databases and utilize these databases to process direct mail, email and PURL campaigns. This includes data import/hygiene, merge/purges, custom programming, custom reports and both frontend and backend analysis.\n\n* Worked with the circulation department on a variety of database projects to include but not limited to the following; Renewal database, Sponsored Education database, Pick Ticket database/program, Credit Letter database, Invoicing database and Inventory database.\n\n* Worked as the SQL Server Database Administrator (DBA).']","[u'Intensive Program in Introduction to Data Analysis', u'Two-year Technical Diploma in Computer Science']","[u'Codecademy Atlanta, GA\nDecember 2017 to March 2018', u'Pinellas Technical Education Center-Clearwater Clearwater, FL\nSeptember 1990 to October 1992']"
0,https://resumes.indeed.com/resume/36de548780234f4d,"[u'Data Center Engineer\nKCG Holdings, Inc\nJanuary 2007 to March 2017\nProvide technical assistance to computer users (750+ employees). Answer questions and resolve computer problems. Provide assistance concerning the use of computer hardware and software, including printing, installation, word processing, electronic mail, and operating systems. Design, develop, deploy and support data center infrastructure. Manage and plan the server deployments based on power, cooling and rack space. Coordinate projects. Work collaboratively with other engineers and product owners to bring new features and services into production. Analyze, test, troubleshoot, and evaluate infrastructure, including Cat6, 10G multimode, single mode, and new cabinets. Liaise with 3rd party suppliers to support data center, determining hotspots, completing thermal imaging, and improving equipment dispersion.\n\n\u25cf IT Infrastructure: Planned the infrastructure. Programmed and arranged swing environment to accommodate 400 servers and networking equipment. Designed, built, maintained, and supervised new data center and trading floor with 200 desks.\n\u25cf Project Management: Integral participant in cross-functional troubleshooting of complex systems, software, applications and programs; deliver recommendations for solutions and improvement. Member of project teams identifying and meeting capacity requirements and weight specifications.\n\u25cf Operations Management: Accept/Test and extend Data communication circuits within the data centers to WAN equipment.Perform OTDR tests and troubleshooting on companies Fiber DWDM infrastructure. Maintained the inventory of servers and parts and coordinate with various technology departments for major upgrades, deployments, server switches, and firewalls. Troubleshooted 200+ WAN circuits, installed major server deployments and server cabling. Configured Cisco switches and ports.\n\u25cf Third Party Management: Liaised with vendors to troubleshoot servers and storage units and replace hardware in them when needed including hard drives, DIMMs, and system boards. Supervised vendors and coordinated engineering teams to complete the project of relocating data center.\n\u25cf Systems Administration: Administer WAN acceptance, T-1, ISDN, DS-3, OC-3, DWDM and Ethernet circuits. Managed cabling (Fiber copper, coax), spare network equipment, and server equipment.', u'KCG Holdings, Inc - Purchase, NY\nJanuary 1995 to January 2017\nAmerican global financial services firm, engaging in market trading, high-frequency trading, electronic execution, and institutional sales and trading.', u'Trade Floor Support Analyst\nKCG Holdings, Inc\nJanuary 1995 to January 2007\nBuilt all aspects of local infrastructure for 250+ members of staff. Acted as IT Purchase SME, advising on $2M-worth equipment for data center and trading floor site.\n\u25cf Systems Management: Configured and maintained PBX (Mitel &Tadiran).\n\nPrevious experience includes Technical Specialist role with MCI Telecommunications,\nField Engineer position with A C & E as well as Customer Service Representative role with US SPRINT.\n\nTechnology Proficiencies']","[u'Certificate', u'in Business Administration']","[u'INSTITUTE OF AUDIO RESEARCH New York, NY\nJanuary 2012', u'WESTCHESTER COMMUNITY COLLEGE Valhalla, NY']"
0,https://resumes.indeed.com/resume/1a03c6d8ccc59f34,"[u'Web Analyst\nMcMahon Group - New York, NY\nJanuary 2016 to February 2018\n\u2022 Manage web ads & tags on a daily basis so that campaigns reach their monthly goals\n\u2022 Creating new reports for medical advertising clients with detailed campaign metrics\n\u2022 Analyzing key metrics including ad impressions, pageviews, ad clicks, eNewsletter open rates, ad\nviewability, invalid/non-human traffic\n\u2022 Creating eNewsletter campaigns & templates', u'Marketing & Data Engineer\nParchem-Fine & Specialty Chemicals - New Rochelle, NY\nJanuary 2013 to January 2016\nManaging data on a B2B website with complex products along with two rapidly-growing B2C e-\nCommerce websites\n\u2022 Improve SEO strategy in order to maintain & increase position as an authoritative top-ranking site in the industry\n\u2022 Analyzing PPC marketing campaign keyword performance\n\u2022 Finding ways to increase traffic and overall brand recognition']","[u'', u'', u'Bachelor of Arts in Economics']","[u'SUNY Purchase\nJanuary 2016', u'Western Connecticut State University Danbury, CT\nMarch 2013', u'University at Albany New York, NY\nMay 2012']"
0,https://resumes.indeed.com/resume/d8d796b1157e19c4,"[u'Mechanical Enginner; Data Engineer\nHighly motivated to do good quality project work as follows:\n\u27a2 Demonstrated a solution to the problem of missing data by understanding the behavioral\nmodeling of the available data and implementing the predictive modeling on missing data of\nimportant section.\n\u27a2 Proven ability to gain insights from data and influence the missing data results during my Data\nScience study.\n\u27a2 Identified and suggested future work to improve results in my Data Science study.\n\u27a2 Strong data handling skills which include importing data from files of different formats,\nmanipulating the data, concatenation of multiple files in an axis, obtaining values of a column,\ndeleting a column, extracting any value of a column, calculating the number of counts, extracting\nthe title/codebook and other related functions.\n\u27a2 Exploited Design of Experiment concepts using ANOVA and data visualization techniques to\ndetermine change in demands of the customer during their visit to a 2-wheeler dealership in a\nperiod of 2 years.\n\u27a2 We exercised Machine Learning concepts, algorithms and strong analytical skills with the ability\nto collect, organize, analyze and predict with certain probability whether visiting customers will\npurchase vehicle during their visit to a 2-wheeler dealership. The code is available on my GitHub\naccount ""https://github.com/karthiksur93/Machine_Learning_Application"".(Python)\n\u27a2 Exhibited strong communication and partnership skills in all the projects involving team\nmembers.\n\u27a2 Developed a Product-Safety Engineering Plan for WANHAO 3D printer.\no This involved working on Intended-Use Statement, Performance Specifications,\nEngineering Design, Engineering Analysis(PHL, Haddon Matrix, FMEA, FTA and\ndesigned warning labels), Testing Plan, Safety Materials, Post-Sale Monitoring and\nSupplemental Materials for WANHAO 3D.\n\u27a2 Analyzed components related and essential to the functioning of an Airbag System in a 2005\nMazda MX-5.\no This involved analysis using MX-5 Severity Categories, Risk Assessment Matrix and\nperforming FMEA.\n\u27a2 Proposed abatements to give the student an option of standing and/or sitting in a workstation\nwhich consists of a table/desk, an adjustable chair and a computer system.\n\u27a2 Designed a simulation in Eye Model.\no Blender and Human-Haptic interface using CHAI 3D for Telemanipulated Head Mounted\nRobotic Eye Surgery project.\n\u27a2 Customized Giraff Robot for better User Interaction using Arduino IDE and ROS at \u04e6rebro\nUniversity.\n\u27a2 Strong theoretical knowledge in Industrial Engineering, Operations Research, Tero Technology\nand Manufacturing Technology.\n\u27a2 Submitted a report to my Divisional Manager-Production on Axle assembly layout modification\nand quality enhancement which focuses on error correction and ergonomics enhancement.\n\nINPLANT EXPERIENCE\n\nSuccessfully completed two in-plant training and had an industrial training exposure for 6 months during\nmy Undergraduate Studies. During the training, I earned knowledge on the manufacture of industrial\ncomponents, working of various sections of Industry and working of teamed effort of the workers in\ndifferent sections of an Industry.\n\nINDUSTRIAL PROJECT WORK\n\nCOMPANY: ASHOK LEYLAND\nPROJECT: AXLE ASSEMBLY LAYOUT MODIFICATION AND QUALITY ENHANCEMENT.']","[u'MS in Mechanical Engineering', u'B.Tech in Mechatronics']","[u'University of Utah Salt Lake City, UT\nJanuary 2015 to January 2017', u'SASTRA University Thanjavur, Tamil Nadu\nJanuary 2011 to January 2015']"
0,https://resumes.indeed.com/resume/88217349adf36faf,"[u'Lead Data Analyst\nNectar Omni channel Marketing - Dallas, TX\nJanuary 2017 to Present\n\u2022 Responsible for building descriptive statistical models to provide insights on data\n\u2022 Building predictive models to assist marketing campaigns to increase revenue\n\u2022 Responsible for designing, building and maintaining data warehouse for reporting and analytics\n\u2022 Designing and building reports and dash boards in tableau for retail clients\n\u2022 Generate reports on campaign performance and automate the process', u'Data Engineer\nNectar Omni channel Marketing - Dallas, TX\nJune 2016 to Present\n\u2022 ETL of large unstructured data from various source to MongoDB/MySQL using java scripts/python\n\u2022 Writing scripts for transforming client data into required data format for efficient storage and retrieval\n\u2022 Developing data models for personalized recommendations system to recommend products, brands, food and movies', u'Data Integration & Analysis Intern\nNectar Omni channel Marketing - Dallas, TX\nJanuary 2016 to June 2016\nDesigning and implementing statistical models to build recommendation system using RFM, collaborative filtering\nand market basket analysis\n\u2022 Assisting the marketing team by providing reports and data insights for effective marketing campaigns', u'System Engineer\nTATA Consultancy Services - Bengaluru, Karnataka\nOctober 2011 to December 2014\n\u2022 Worked with clients as a techno-functional expert to analyze and implement User Stories in agile methodology\n\u2022 Worked as a lead QA for testing Enterprise data storage systems while coordinating between onsite and offshore teams']","[u'Master of Science in Information Technology and management', u'Bachelor of Engineering in Electronics and Communication Engineering']","[u'The University of Texas at Dallas (UTD) Richardson, TX\nMay 2016', u'Visvesvaraya Technological University (VTU)\nJune 2011']"
0,https://resumes.indeed.com/resume/799fb4ca2d90ab44,"[u""Data Engineer\nBank of America - San Francisco, CA\nSeptember 2016 to November 2017\nResponsibilities:\n\u2022 Created SQL scripts for production models and used to monitor the model performance every month.\n\u2022 Worked on databases including Netezza, Teradata, and HBase.\n\u2022 Worked with Data Modelers, ETL staff, Business System Analysts in Functional requirements reviews.\n\u2022 Was responsible for migrating data from Netezza to Hadoop using SQOOP.\n\u2022 Involved in creation of Internal and External tables in Hive and later accessing this tables using Hive-Spark integration for faster results.\n\u2022 Extensively worked with Teradata utilities like Fast Export, Fast Load, to export and load data to/from different source systems including flat files.\n\u2022 Involved in Using various software tools like Tableau, Toad, SVN, SAS enterprise guide, Teradata SQL assistant, Teradata studio express, WinSCP.\n\u2022 Had Involved in creating tables and debugging SAS datasets through Proc SQL.\n\u2022 Involved in creating PD, EAD, LGD models for Credit cards, Auto loans, Mortgage loans.\n\u2022 Involved in Migrating code from a legacy system to a new System.\n\u2022 For Every Quarter was responsible for generating Ongoing Monitoring Reports through Tableau.\n\u2022 Experienced in Spark context(SC), Data frame, datasets and RDD's\n\u2022 Developed Spark applications using Scala to interact with the Teradata database to analyze the credit card accounts.\n\u2022 Used SVN to share our Tableau reports and work-related documents.\n\u2022 Created work tables, global temporary tables, volatile tables as part of developing the SQL script/code in Netezza and Teradata databases.\n\u2022 Involved in giving KT sessions to the newly joined and made improvements/fixes to the existing code.\n\u2022 Worked and created with Sqoop jobs with full refresh and incremental load to populate Hive External tables.\n\u2022 Developed the Teradata stored procedures and scripts based on the technical design documents.\n\u2022 Used Spark SQL to process a large amount of data.\n\u2022 After migrating the data, involved in Testing the data between Netezza and HDFS.\n\u2022 Experience with analytical manipulation and interpretation of large SAS data sets.\n\u2022 Developed Hive scripts to perform aggregation operations on the loaded data.\n\u2022 We used spark SQL for better performances and faster results to compare the data between Netezza and Hadoop.\n\u2022 Whenever the Credit cards, Mortgage, Auto loans performance is low then we used to go upstream the database finds the root cause of the issue.\n\u2022 Developed dashboards in Tableau Desktop and published them on to Tableau Server, which allowed end users to understand the reports.\n\u2022 Define, design and develop complex/nested SQL queries for Tableau dashboards.\n\u2022 Used calculated fields to show detailed trend analysis in Tableau.\n\u2022 Involved in data analysis as part of monitoring model's performance.\n\u2022 Used various flat files/Servers to generate graphs in Tableau and Scheduled the reports and sent email reports using Tableau Server.\n\nEnvironment:\nHadoop, Teradata, Netezza, SAS, Tableau Desktop, Tableau Online, Toad, SQL, Proc SQL, Fast Export, Excel, Fast Load, Impala, Sqoop, Scala, Spark, Hive, Teradata SQL assistant, Cloudera."", u""Programmer Analyst\nCognizant Technology Solutions(PRNewswire) - Hyderabad, Telangana\nMarch 2013 to December 2014\nResponsibilities:\n\u2022 Creation of Hive tables, loading the structured data into tables and writing Hive Queries to further analyze the data\n\u2022 Hands-on experience with MapR Hadoop platform to implement Bigdata solutions using Hive and Pig.\n\u2022 Developed Spark scripts by using Scala Shell commands, as per the requirement.\n\u2022 Involved in creating partitions for hive tables which also includes Multilevel Partitioning.\n\u2022 Using Flume, we used to get the data from client sources and used PIG scripts to clean the data.\n\u2022 Validated final data sets by comparing RDBMS source systems and writing SQL, Hive queries.\n\u2022 Involved in creating HBase tables for querying minimum columns for a huge table.\n\u2022 Also gained a hands-on experience on using Scala in the backend.\n\u2022 Involved in creating Data Frames and Performed several aggregation logics using Spark.\n\u2022 Experienced in developing Spark applications using Scala on different data formats like a Text file, CSV file.\n\u2022 Involved in writing PIG scripts to transform the data and save them into hive tables.\n\u2022 Involved in exploring new components of Hadoop like Spark, Oozie, HBase and Kafka for the development of a project.\n\u2022 Learned to schedule the Sqoop jobs using Oozie to get the incremental data from the traditional database.\n\u2022 Developed Hive queries to analyze the sales pattern and customer satisfaction index over the data present in various relational database tables.\n\u2022 Developed PIG Latin scripts to extract the data from the web server output files and used to store files into HDFS.\n\u2022 Created temp tables and used flat files to load the data into Spark RDD's and used the data for modeling calculations.\n\u2022 Worked on tuning the performance of HIVE and PIG queries.\n\u2022 Worked on importing and exporting data from Oracle to HDFS using Sqoop.\n\u2022 Involved in Data Ingestion techniques to move data from various sources into HDFS.\n\u2022 Created hive staging tables Managed Tables and permanent tables External Table.\n\u2022 Involved in requirements gathering and Responsible for helping teammates with the Hive and Hadoop architecture.\n\nEnvironment:\nHadoop, HDFS, Map Reduce, Hive, Pig, HBase, Spark, Scala, Cloudera, Oracle, SQL, SQOOP, Flume, Oozie, JSON."", u""Programmer Analyst\nCognizant Technology solutions(The Travelers Companies) - Chennai, Tamil Nadu\nJuly 2011 to February 2013\nResponsibilities:\n\u2022 Involvement in all the stages of (SDLC) like requirements specifications, review, test documentation, application testing, and defect reporting.\n\u2022 Involved in all the Test cases design and Test execution whenever the new build is deployed.\n\u2022 Worked on IE Developer tools to debug given HTML.\n\u2022 Used Travelers Product Designer to fix any bugs or any issues identified during the testing period.\n\u2022 Written test cases for Unit testing using JUnit on eclipse.\n\u2022 Involved in Configuration Testing, Manual Testing, Unit Testing, integration testing, and Smoke testing to ensure web applications are Defect-free.\n\u2022 Actively involved in Configuring new UI drop down box, radio buttons and text boxes as per requirements.\n\u2022 Involved in complete development of Agile Development Methodology and tested the application in each iteration.\n\u2022 Using Selenium-IDE and Selenium-web driver we used to automate test cases.\n\u2022 Created regression test cases on web applications for future references to make there won't be any code break when a new build is deployed.\n\u2022 Whenever new build is integrated to web applications we used to check and verify using Selenium.\n\u2022 Perform browser compatibility testing of an application under various cross browsers (Safari, Firefox, IE, Chrome) using HTML IDs and XPATH in Selenium Web Driver.\n\u2022 Experienced in scripting Tests and automate them with Selenium IDE / Selenium Web Driver.\n\u2022 We used to run various XML files in SOAP UI tool, this XML file data would be prefilled the insurance policy automatically.\n\u2022 Creation and execution of automated software Test plans, Test cases and Test scripts using Selenium (Java).\n\u2022 We used to test Umbrella, Master Pac, Master Pac plus polices accounts using SQL queries to evaluate the MySQL database captures correct values.\n\u2022 Getting walkthrough on new requirements from my onsite leads.\n\u2022 Preparing weekly and daily status reports for onsite managers.\n\u2022 Used to Find the defects in system testing/System Integration Testing and we used HP ALM QC to raise the defects.\n\u2022 Prepared high and low-level design documents for the business modules for future references and updates.\n\nENVIRONMENT:\nJava, XML, Excel, Test case design, HTML, Java Script, HP quality center. Selenium IDE/web driver, Firebug, Fire path, Eclipse, SOAP UI, Product Designer, SQL, MySQL, MS access, Account manager.""]","[u'Masters of Science in Computer Science', u'Bachelors of Technology in Electronics and Communication Engineering']","[u'Silicon Valley University San Jose, CA\nJanuary 2015 to May 2016', u'Vellore Institute of Technology University Vellore, Tamil Nadu\nJuly 2007 to May 2011']"
0,https://resumes.indeed.com/resume/c4ae4a8a6078116a,"[u""Data Visualization Intern\nElectric Power Group\nJune 2016 to August 2016\n- Interned with Electric Power Group, a synchrophasor solutions company, during my junior year.\n- Worked on improving the data visualization software for one of their best-selling products, the Phasor RTDMS (Real Time Dynamics Monitoring System).\n- Focused on generating contour plots/heat maps of real-time electric power grid data from phasor measurement units (PMU) across the company's database.\n- Used Microsoft Visual Studio (C# and XML) to render smooth contour plots, accurately visualize the user data, and integrate it with EPG's existing software modules."", u'Software Engineer Intern\nMicrosoft\nMay 2014 to July 2014\n- Interned with Microsoft Mobile Labs during my freshman year.\n- Given the task of innovating and executing a fully functional game for the Windows PC and Windows Mobile platforms.\n- Successfully created ""Space Escape"", a 2-D adventure game that was conceptualized and designed using Unity and MonoDevelop.\n- Worked on back-end programming (C#), as well as graphic design and character/sprite animations (Photoshop and Unity).']","[u""Bachelor's""]",[u'University of Massachusetts-Amherst']
0,https://resumes.indeed.com/resume/c7992fbd04b68882,"[u'Data Entry Specialist\nDirect To You\nMay 2013 to May 2014\n\u2022 Transferred large amounts of data from documents containing details of farmers from various parts of Karnataka(India) into MS Excel\n\u2022 Input crops and seasonal produce details for every district to the company database\n\u2022 Analyzed data in the system for errors and discrepancies\n\u2022 Updated database with latest changes in details of agriculturists and their farm products', u""System Engineer\nTata Consultancy Services - IN\nJuly 2010 to July 2012\n\u2022 Reduced time and effort by about 20% by building an easy-to-use tool to operate background processes\n\u2022 Cut down response time of software from 10 secs to 2 secs by making the application's background layout to be displayed first before the actual content\n\u2022 Increased the rating of front end interface from 3 to 4.5 by adding user-understandable graphical icons with tag names instead of plain text""]",[u'Bachelor of Technology in Information Technology'],"[u'SASTRA University Thanjavur, Tamil Nadu\nMay 2010']"
0,https://resumes.indeed.com/resume/7c25e26b49d698af,"[u'Manager & Master Data Engineer\nCAPITAL ONE - Plano, TX\nJanuary 2014 to Present\nHired for implementing automated monitoring solutions to identify performance variances, before customer impact, for IT infrastructure health, application health, log file ingestion, visualization, predictive analytics and event notification.\n\u2022 Successfully implemented the first open source monitoring tool, Zabbix, completely in AWS; integrated it with Splunk, Influx DB, Grafana, and with HP OM, HPSM & MIR3 for alerting and ticketing. Set up Zabbix infrastructure in AWS-West for resiliency. Switching to open source tool provided annual cost savings of $300k.\n\u2022 Served as a single point of contact for all Business Units for collecting the requirements and providing monitoring solutions to fit their needs. Improved the demand for new monitoring tools (Splunk, Bluestripe, Zabbix) with savvy marketing. Also, improved timely fulfillment of all monitoring requests with an efficient and agile onboarding process, and onboarded 530 applications within 6 months, reducing the mean time to detect issues by 23%.\n\u2022 Ingested data into the Elastic stack & InfluxDB for the largest streaming Enterprise-APIs customer activity audit data (130k events/min). Analyzed & validated the data, and created more than 40 Grafana & Kibana dashboards for the Digital Wallet and Mobile Customer Account Servicing application teams, to reduce their mean time to detect issues by 10%. Tested the performance for Elastic stack and InfluxDB with high volume data streaming.\n\u2022 Acquisition integration: Personally migrated 1,400 (out of 4k) ING DIRECT Windows and Unix server monitors from Ionix to HP Sitescope; and 60 (out of 200) HSBC Unix server monitors and 40 Oracle Database monitors from BMC PATROL to Sitescope. This allowed retiring Ionix and PATROL software, hardware and support; saving $500k in annual costs.\n\u2022 Setup monitoring, using HP Sitescope, for 1, 200 Dev. and QA servers, for software developers in all business units. Created dashboards for 72 applications in HP BSM and VMWare Alive, and sent the alerts to both the development and server support teams. This improved the server uptime by 15%, thus improving productivity for over 1,000 developers.\n\u2022 Coordinated and successfully migrated the notification system from AlarmPoint (Data Center) to MIR3 (Cloud) within 3 months, saving $100k annually in support costs.\n\u2022 Managed the HP offshore team. Trained them on Zabbix and Splunk monitoring tools for 24x7 support to optimize the usage of the HP contract, and to free-up internal resources from 24x7 support.', u'Director, IT Infrastructure\nMAXIM INTEGRATED - Dallas, TX\nJanuary 2007 to January 2014\nResponsible for IT infrastructure program management; strategic planning; project proposals; project roadmaps; and for providing IT infrastructure and support services to the business unit design organizations and production testing operations (manufacturing) at 36 sites globally. Served as an IT business partner for all business units.\n\u2022 Managed a global team of 25 IT professionals and all aspects of engineering computing needs, including client workstations, compute server grid, high performance storage, license servers, data management servers and data centers at 36 sites globally by automating the infrastructure management tasks.\n\u2022 Directed a program (multiple projects) to migrate design engineering from Linux workstations to server based flexible computing. Provided 52% improvement in engineering simulation time and a 33% reduction in data centers, providing significant savings in data center operating costs and improvement in engineering productivity.\n\u2022 Managed projects to integrate 13 acquisitions into the company design flow to reduce support costs. Disintegrated three divestitures, closed seven design sites and moved two to three sites / data centers every year, with minimum downtime, for significant savings in yearly lease costs.\n\u2022 Directed a program to standardize and consolidate NIS and DNS domains and automated Linux configuration management at 36 sites globally for significant savings in support costs. Migrated the sites from NIS to LDAP. Virtualized IT infrastructure servers to maximize data center space for engineering servers.\n\u2022 Directed a program to replace disk storage at 36 sites globally with high availability storage clusters to improve performance, reliability and uptime. Setup disk space quota for design projects, and monitoring, to improve engineering productivity. For the production testing sites, the new storage provided 150% improvement in test program download times to the test equipment and testing results write times to the disk.\n\u2022 Oversaw project to standardize and consolidate license servers from 24 sites to two sites. Provided $1M per year in cost savings and better access to the license pool. Migrated servers from HP-UX and Solaris to high-availability Linux virtual machines on VMWare, with failover between the two sites for disaster recovery. Standardized license installs, reporting and monitoring.\n\u2022 Directed a project to standardize and refresh the global compute grid (cloud computing). This provided an average of 350% improvement in engineering simulation time and 40% more simulation jobs for the same license count, providing considerable savings in license costs and engineering productivity.\n\u2022 Oversaw program to set up proxy servers (local cache) and Riverbed compression & network acceleration appliances for design data management software at 24 global sites. Cut design download time from hours to seconds, thus improving engineering productivity.\n\u2022 Directed a project to standardize software configuration management tool, for all of the business units, to wanDISCO Subversion, with multi-site replication to 8 global sites, to facilitate collaboration among the software development teams. Setup external facing Subversion and GIT servers to collaborate with customers.\n\u2022 Oversaw project to setup secure engineering design at one site in United States and one in Europe, to meet the European common criteria security guidelines for European government business.']","[u'M.B.A. in Global Management', u'M.S. in Computer Science', u'M.S. in Electrical Engineering', u'B.S. in Electrical Engineering']","[u'University of Phoenix Mesa, AZ', u'Arizona State University Tempe, AZ', u'New Jersey Institute of Technology Newark, NJ', u'University of Bombay Mumbai, Maharashtra']"
0,https://resumes.indeed.com/resume/3aba0248d78052d9,"[u'Data Engineer\nEricsson Global Service Center - Dalian, CN\nNovember 2013 to April 2014\nSkills: MS/VBA/JavaScript/SQL\nRoles & Responsibilities:\n\u25cf Preparing project design and data configuration of new BSC sites and BSC roaming for\nSweden projects.\n\u25cf Performing software package preparation and upgrade for Ericsson products.\n\u25cf Planning and coordinating activities with Project Manager about Project process,\nacceptance test and handover to customer, and giving feedback to R&D.', u'Data Engineer\nEricsson Communications Co. LTD - Beijing, CN\nMarch 2009 to November 2013\nSkills: MS/VBA/JavaScript/SQL\nRoles & Responsibilities:\n\u25cf Operate and maintain Ericsson Mobile Switching Equipment.\n\u25cf Preparing GSM Project Design, IP planning for Ericsson products.\n\u25cf Collecting and integrating information from separate data sources.\n\u25cf Analyzing and extracting Data to develop Data Scripts for equipment.\n\u25cf Planning the implementation of the Product Configuration / Integration work.\n\u25cf Preparing Data Configuration for the new sites Configuration, as well as Upgrade or\nTransformation for Current Network sites.\n\u25cf Participating Projects in Sites Configuration or CutOver, Core Network Transformation, such as MIP, VOIP Projects.\n\u25cf Planning and coordinating activities with customer CMCC and CUCC about Project\nprocess, acceptance test and handover to customer, and giving feedback to R&D.']","[u'Master of Information Technology in Information Technology', u'Bachelor of Engineering in Information Engineering']","[u'State University of New Jersey\nSeptember 2016 to May 2017', u'Henan University of Science & Technology\nSeptember 2004 to June 2008']"
0,https://resumes.indeed.com/resume/a1642e5c2958aaa4,"[u""Data Analyst Intern\nYinzCam, Inc\nJune 2017 to December 2017\n\u2022 Analyzed and visualized data insights to YinzCam's various clients which include NBA, NFL and NHL teams\n\u2022 Developed predictive models to forecast the number of views and clicks on matchdays and non-matchdays\n\u2022 Used HiveQL, MySQL and AWS CLI to query the data from the organization's various data sources\n\u2022 Collaborated in designing the database for IPTV project for the Mercedes Benz Stadium\n\u2022 Worked with the CEO, System Architect and assisted the developers to improve the data analytics platform\n\u2022 Built data analytics reports using Excel, R and Tableau for the NFL summit leading to two new clients (teams)"", u""Data Analyst Intern\nYinzcam, Inc.\nJune 2017 to December 2017\n\u2022 Conducted a thorough audit of over 70 of YinzCam's apps to locate the discrepancies in data reporting\n\u2022 Built reports for individual leagues (NFL, NHL, etc.) listing the bugs and potential upgrades for data reporting\n\u2022 Suggested ways to improve the quality of data reporting by introducing new metrics that could be tracked"", u'Software Engineer\nAbhyudaya Multimedia Ltd\nMay 2014 to June 2014\n\u2022 Analyzed customer purchase data for the website shopitdaily.com using Excel and Tableau\n\u2022 Engineered data visualizations models including heat maps, trend lines, pivot tables to analyze product sales\n\u2022 Developed 85% of use cases, wireframes and class diagrams for successful implementation of three projects\n\u2022 Pitched business proposal and framed a business requirement document to land two projects for the firm']","[u'Master of Science in Information Sciences', u'Bachelor of Engineering in Information Technology']","[u'The University of Texas at Dallas Dallas, TX\nMay 2018', u'Rajiv Gandhi Technical University\nMay 2016']"
0,https://resumes.indeed.com/resume/acf0dbd30127e512,"[u""Data Engineer Intern\nEverlasting Wardrobe Crop - New York, NY\nSeptember 2017 to Present\n\u2022 Work on full-stack of website which includes users' information and different feature of clothes using Node.js.\n\u2022 Utilize collaborative filtering and SVD model to recommend based on machine learning.\n\u2022Build a RPC server for recommend operation by Django. Parameters and results were transmitted in JSON.\n\n\u2022 Improving the algorithm that can use more modular to increase the accuracy of the recommendation.\n\nACADEMIC PROJECT EXPERIENCE""]","[u'Master of Science in Electrical and Computer Engineering in Database, web', u'in Opto-electrical Information Engineering']","[u'New York University, Tandon School of Engineering, Brooklyn\nJanuary 2016 to December 2017', u'Beijing Institute of Technology(BIT) Beijing, CN\nJune 2015']"
0,https://resumes.indeed.com/resume/a7930ef05b15639a,"[u'Data Center Engineer\nHewlett Packard Enterprise - Houston, TX\nSeptember 2015 to July 2016\n\u2022 Managed 2 Next Generation Tier 3 Data Centers with 125,000 sq. ft. of combined whitespace, eliminating analytical and data inconsistencies.\n\u2022 Provided on-going support for power and cooling for Data Centers\u2019 5,000+ servers, resulting in a reduction of down time.\n\u2022 Coordinated on-site personnel for critical monthly, quarterly, and annual maintenance; ensuring proper procedure and safety precautions maintaining a 100% injury free work environment and no lost Data Center uptime for customers.\n\u2022 Improved of Houston Business Continuity Plans and MOPs, increasing site accuracy and efficiency.\n\u2022 Presented Data Center Tours to high-priority customers, positively influencing cooperate sales.', u'Community Advisor\nTexas Tech University Student Housing - Lubbock, TX\nJanuary 2010 to May 2014\n\u2022 Received Jerry Callumn Award for Outstanding Achievement and Excellence in Student Housing.\n\u2022 Supervised and mentored 57 engineering students per year in a learning community, nurturing student success.\n\u2022 Provided avant-garde Customer Service, improving student satisfaction.']","[u'Masters of Science in Electrical Engineering', u'Bachelor of Science in Electrical Engineering']","[u'Texas Tech University Lubbock, TX\nMay 2014', u'Texas Tech University Lubbock, TX\nMay 2012']"
0,https://resumes.indeed.com/resume/656955bbc1d66d82,"[u'Data Analyst\nDELTA AIR LINES INC - Atlanta, GA\nJune 2017 to Present\n\u2022Construct utility tools for efficient data processing facilitating automation to deliver improved metrics analysis.\n\u2022Leverage Business Intelligence tools(Tableau) for instant analysis on Big Data to perform real time analytics.\n\u2022 Execute SQL queries on daily basis to achieve efficient data extraction and analysis.', u'Research Assistant\nUNIVERSITY OF TEXAS ARLINGTON - Arlington, TX\nOctober 2015 to May 2017\n\u2022 Provided remarkable assistance in research projects by performing data collection, preprocessing and analysis.\n\u2022 Managed & mentored students in data science related academic projects.', u'Applications Engineer\nQUANTEL ELECTRONICS\nDecember 2013 to July 2015\nIndia\n\u2022Preprocessed complex data, gained insights for better customer experience.\n\u2022Delivered successful knowledge transitions and presentations to clients post project deployment.', u""Business Data Analyst\nTECHNOPHILIA SYSTEMS\nDecember 2011 to November 2013\nIndia\n\u2022 Effectuated ETL process in Informatica to migrate data records from standard to customized ORACLE database.\n\u2022 Handled various types of data: flat file, csv file. Achieved pre-processing activities and Coded data migration.\n\u2022 Responsible for coming up with new business process resulted in increased sales by 100.%\n\nPROJECTS\nRECOMMENDER SYSTEM FOR DELTA SKYMILES CUSTOMERS - MACHINE LEARNING PYTHON\nAccomplished efficient marketing through collaborative filtering and recommended add-on services to customers,\nhence delivered personalized airline experience based on customer's priorities and interests.\nEVALUATE AIRCRAFT PERFORMANCE USING APACHE HADOOP- HDFS MAP REDUCE UNIX\nAnalyzed influential factors(internal/external) that affect aircraft performance, therefore computed vital features of aircraft components to evaluate overall performance of the aircraft.\nTEXT WEB ANALYTICS TO PREDICT SENTIMENT SCORES- PYTHON NLTK TABLEAU\nPerformed sentiment analysis on customer feedback data based on positive & negative sentiments. Visualized\nresults demographically and created interactive dashboards for faster regional analysis of customer experience.\nEVALUATION OF WEBSITE SEARCH RELEVANCE SCORES- LINEAR REGRESSION PANDAS MATPLOTLIB\nBuilt ensemble regressor & classifier algorithm to compute relevance scores of search terms based on customers\nsearch results, enhancing customer experience by providing the closest match in accordance to relevance scores.\nMULTIVARIATE TIME SERIES ANALYSIS OF EXCHANGE RATES- ARDL MACRO ECONOMICS SAS R\nBuilt multivariate- ARDL & VAR time series models and studied the impact of changes in exchange rate on export\ngrowth rates. Analyzed exchange rate impact on developed and developing countries export rates.""]","[u'in Data Analytics', u'Masters in Information Systems & Economics', u'BE in Electronics & Communication engineering']","[u'College of Business Arlington, TX\nMay 2017', u'UNIVERSITY OF TEXAS ARLINGTON Arlington, TX', u'ANNA UNIVERSITY']"
0,https://resumes.indeed.com/resume/e07a55c3aee84db2,"[u'Software Engineer\nShopperTrak - Chicago, IL\nSeptember 2017 to Present\n> Developed data driven applications using Qlik Sense to visualize retail related metrics such as Traffic, Conversion, and STAR rates\n> Utilized NPrinting alongside Qlik Sense for the generation and distribution of data visualization reports to major retailers\n> Implemented Python scrips for the extraction, cleaning, and management of data from a MongoDB\n> Resolved JavaScript based bugs in a mashup between Qlik Sense and an inhouse developed software\n> Designed and implemented a tracking algorithm between the pairing between two individuals and tracked their movements based on a grid layout of a floor plan.', u'Data Analyst\nPersonal - Glenview, IL\nJanuary 2014 to May 2017\nDATABASE FOR APPLICATION OF SABERMETRIC PRICIPLES (PERSONAL)\n+ Created and managed a relational database to analyze baseball player performance and outcome probabilities using SQL and Java\n+ Mined for baseball player data and developed a predictive algorithm through regressions and classification models with the help of RapidMiner and Excel\n+ Observed and identified correlational relationships to statistically model and predict player performance\n+Designed and executed a random forest and naive bayes model using Rapidminer\n+Analyzed variance through a PCA analysis in order to isolate independent variables for training and example sets\n+Achieved an f-measure of 80% for cross validation and 75% for real life data using random forests with a predictive value of 70% of baseball player performance']",[u'BACHELOR OF SCIENCE in PHYSICS'],"[u'LOYOLA UNIVERSITY CHICAGO Chicago, IL\nAugust 2013 to May 2017']"
0,https://resumes.indeed.com/resume/3df9f8b8aa8224a3,"[u""Analyst and Operator\nCHINA COMMERCE AGRICULTURAL PRODUCT EXCHANGE Co., Ltd - Dalian, CN\nJune 2016 to August 2016\n\u2022 Analyzed company's marketing intelligence and consumer behavior data and used market segmentation method to conduct a precise positioning on target customers;\n\u2022 Launched effective and cost-saving advertising strategy, which promoted brand image and expanded brand influence\n\u2022 Proposed optimized siting plan based on data collection and analysis on local business area's population density, consumption structure, business environment maturity, and transportation condition\n\u2022 Maintained and updated product database on e-commerce website"", u""Data Acquisition Engineer\nNANJING HUGEDATA NETWORK TECHNOLOGY Co., Ltd - NANJING, CN\nJanuary 2016 to February 2016\n\u2022 Used data acquisition tool to acquire internet data to support analytic for clients' business strategy\n\u2022 Completed adaptability configuration of acquisition module and implemented data acquisition from designated target websites\n\u2022 Conducted data cleaning on the data acquired and wrote processed data back to database\n\u2022 Ensured the timeliness and integrity of the data acquisition from target websites on a scheduled basis""]","[u'Master of Science in Business Analytics in Business', u'Bachelor of Management in Information Systems']","[u'Carlson School of Management\nJune 2018', u'SOUTHWEST JIAOTONG UNIVERSITY Chengdu, CN\nJune 2017']"
0,https://resumes.indeed.com/resume/512b999751e3f30c,"[u'Senior Data Center Engineer\nDOJ(SAVA)\nJanuary 2017 to Present\n\u2022 Managed Rack Power Manager and PowerIQ to monitor power in the data center\n\u2022 Configured Raritan, APC, and Liebert PDUs and provided test results to government executives\n\u2022 Conducted testing of data center PDUs and reported issues to vendors and executives\n\u2022 Work with executives, project managers, stakeholders, and vendors\n\u2022 Provides power reading and space capacity\n\u2022 Create diagram for different network projects and data center layouts in VISIO\n\u2022 Research DCIM tools such as Nlyte and Sunbird DCIM\n\u2022 Managed and collaborated with Tier 2 and Microsoft team on Microsoft Surface Hub deployment with DEA HQ and US Marshalls\n\u2022 Assist in tracking and inventories of circuits and PDUs\n\u2022 Assists in installing new equipment in data center\n\u2022 Provided advise on data center to management', u'Data Center Manager - Data Center Engineer\nNIAID/NIH(CSRA)\nOctober 2015 to January 2017\n\u2022 Helped defined role for Data Center Engineer\n\u2022 Deployed Data Center Operation, Data Center Expert, Data Center Optimization or DCIM as part of the requirement from Federal CIO or Office of Budget and Management. Helped solved data center inventories and Power Usage Efficiency.\n\u2022 Collaborated and met with team leads, executives, departments and vendors on different projects\n\u2022 Managed multiple data center and space storage organization for IT equipment\n\u2022 Maintained relationship with multiple vendor and procured contracts for regular data center cleaning\n\u2022 Deployed and surplus physical servers and other devices meeting Federal mandated requirement\n\u2022 Trained new hire\n\u2022 Assisted in racking new servers, storage devices, and network equipment in the data center\n\u2022 Escorted vendors in the data centers to install or repair equipment\n\u2022 Work with NIH Property Office to track inventories of servers, networking equipment and storage devices that cost more than $5000\n\u2022 Maintain inventories on a DCIM tool\n\u2022 Supported day to day data center operation and conducted a daily morning walk thru and reported any abnormalities to engineers\n\u2022 Monitor PDUs and provided report to management\n\u2022 Monitor cooling and reported any abnormalities to facility group\n\u2022 Worked closely with facility group for any issues or maintenance activities and relay all maintenance activities to the engineering team\n\u2022 Provided documentation for the role of the Data Center Engineer', u'LAN Engineer - Data Center Engineer\nRCN\nJune 2012 to October 2015\n\u2022 Managed Local Area Network and data center\n\u2022 Racked physical server, storage devices, and network equipment\n\u2022 Decommission equipment in the data center\n\u2022 Maintain data center supplies and inventories\n\u2022 Assisted in inventory of data center equipment\n\u2022 Ran fiber and cat cables from network equipment, blades, brocades, storage devices or servers inside the data center\n\u2022 Assisted and escorted vendors in troubleshooting or installing equipment in the data center\n\u2022 Maintain and managed data center space and reported any abnormalities to other engineers\n\u2022 Supported Tier I and Tier II Engineers. Trained junior technical staff. Worked closely with Tier III Engineers in different projects.\n\u2022 Deployed physical servers and created virtual machines in HyperV and VMware\n\u2022 Upgraded Cisco devices OS and hardened configurations during PCI compliance\n\u2022 Created user accounts and permission in Active Directory and Microsoft Exchange\n\u2022 Worked with SCCM and created company standard image on client machines\n\u2022 Supported remote data centers as needed in coordinating installs and upgrade', u'LAN Administrator - Data Center Engineer\nOWT\nFebruary 2005 to November 2012\n\u2022 Supported Data Center operations on a day to day basis\n\u2022 Ran cable in the data center\n\u2022 Monitor cooling and heating in the data center\n\u2022 Assisted in procuring an APC power back up in the data center\n\u2022 Rack equipment and maintain space in data center\n\u2022 Administered servers and supported remote offices\n\u2022 Developed social media strategy to a Military discount travel website and promoted affiliates which generated revenues\n\u2022 Maintained website and developed script for travel engine\n\u2022 Managed deployment of computer to remote offices\n\u2022 Managed antivirus server and monitored devices for any cybersecurity threats.\n\nVOLUNTEER & MISC\n\u2022 Vice President for Home Owners Associations working on community projects\n\u2022 Provided media support to a non-profit organization\n\u2022 Assisted Small Business setup web presence and cloud environment, from email to server hosting']","[u'Technical in Amazon Web Services', u'in Architecture']","[u'Montgomery College', u'University of New Mexico']"
0,https://resumes.indeed.com/resume/1585e2e8855cae6d,"[u'Software Engineer\nAvum\nJanuary 2017 to Present\nMentor junior developers who are completing an application in react/redux by doing 1 on 1 code reviews.\n\u25cf Implement manage assignments, customers, deliverables, departments, projects, users, and timesheet sections.\n\u25cf Refactor the Spring controllers, services, service impls, and Hibernate data models to serve JSON.', u'Software Engineering Instructor\nHack Reactor\nJanuary 2017 to January 2017\nMentor pre-course students by conducting weekly 1 on 1 code reviews, and reviewing github repos.\n\u25cf Supported help desk by resolving issues with MySql, Node, Bcrypt, MongoDB, AngularJS, React, and Algorithms.\n\u25cf Proctor technical interviews by having a heavy understanding of Test Driven Development, and Algorithms.', u'Data Analyst\nCydcor\nJanuary 2014 to January 2014\nUsed Pivot Tables, Macros, V-Lookups, H-Lookups in Microsoft Excel to deliver weekly reports to vital persons.\n\u25cf Ownership of Direct TV, Bell Canada, and Wal-Mart company accounts in support of all of North America.\n\u25cf Performed ad-hoc research which saved the company a combined total of $150,000 in less than 1 month.']","[u'M.B.A. in Marketing', u'B.A. in Social Science']","[u'California Lutheran University\nJanuary 2013', u'California Lutheran University\nJanuary 2011']"
0,https://resumes.indeed.com/resume/68ea0cbc0dc75d6e,"[u'Data Scientist\nDeep-R - San Francisco, CA\nJune 2017 to September 2017\n\u25cf Developed web scraping tool using Python, Scrapy and Selenium to crawl 100+ government and private web pages to collect commodity and stock data from commercial and agricultural industries.\n\u25cf Cleaned 20 years\u2019 worth of yearly and monthly data (4,000+ data points) with Python and Pandas to analyze and visualize in Plotly graphs designed to predict monthly oil demand with 90% accuracy.\n\u25cf Researched 10 potential machine learning models such as linear regression and ARIMA for forecasting oil demand.', u'Associate Engineer\nCity of Palo Alto - Palo Alto, CA\nJanuary 2016 to December 2016\n\u25cf Documented over 600 utility mapping jobs in GIS database with data verification from field engineers.', u'Design Engineer\nBKF Engineers - Redwood City, CA\nMarch 2015 to January 2016\n\u25cf Documented over 600 utility mapping jobs in GIS database with data verification from field engineers.', u'Project Engineer\nGEI Consultants - Sacramento, CA\nMay 2012 to September 2013\n\u25cf Crafted Java UI for more than 100 users to input, output, and update one of 1000 rainfall gauge stations for historical documentation of California rainfall.\n\u25cf Generated dynamic plots for Java UI and dashboard to showcase intensity-duration frequency curve using logarithmic regression prediction for rainfall return period.\n\u25cf Translated 10 rainfall models for imputing and validating 50GB of data from R to Java using JRI library for historical and newly uploaded rainfall.\n\u25cf Extracted 10GB of LIDAR data in Java Jzy3D to create 3D visualizations of terrain contours with 10 meter moving average to estimate levee height.', u'Data Analyst\nGEI Consultants - Sacramento, CA\nOctober 2010 to April 2012\n\u25cf Extracted 5000+ files of daily and hourly rainfall data using VBA from 10+ excel formats for rainfall Oracle database.\n\u25cf Programmed data cleaning on extracted rainfall data in VBA to document and impute faulty data with 99% accuracy.\n\u25cf Programmed data transfer, upload, and download for 50GB of clean rainfall data using MySQL hosted on Oracle DB.\n\u25cf Created data pipeline flowchart to document end-to-end processes of extraction, transformation, and uploading data.']","[u'Masters of Science in Civil Engineering', u'Bachelors of Science in Civil Engineering']","[u'University of California - Davis Davis, CA\nSeptember 2009 to September 2010', u'University of California - Davis Davis, CA\nSeptember 2004 to December 2008']"
0,https://resumes.indeed.com/resume/01c16a8d0609c292,"[u'Chief Engineer\nAble Engineering Services - RR Donnelley - Offices / Data Center - Warrenville, IL\nOctober 2016 to Present\nOperate and maintain a 175,000 square foot office building including a 1,600 square foot Data Center. Forecast capital budgets for deteriorating building systems and compile monthly reports for client review. Reduced fire alarm, sprinkler system, and mechanical preventative maintenance vendor cost 50% while rendering same services. Cut mechanical vendor cost on service calls by doing work in house. Manage, schedule, and direct all other vendor services that are necessary to the property.\n\nEaton 65 kVA UPS system supported by VRLA batteries\n200 kW Diesel CAT generator with 390 gallon day tank for fire life safety\n125 kW Natural gas Cummins generator supporting critical load\nFour 80,000 cfm Trane modular climate changer air handlers\nFour Lochinvar 2,000,000 btu hot water boilers\nTwo air cooled 350 Ton Carrier Chillers 134A, 30% glycol loop, 50 HP Armstrong pumps\nJohnson Controls Metasys BAS and DDC VAV controls\nLiebert dry coolers supporting Intermediate distribution frame closets\nTwo Liebert 20 ton 407C CRAC units supporting Data Center\nWatt Stopper Lighting system\nTwo make up air units both gas fired, one 407C 30 ton unit\nEST-3 Fire alarm system including a dry system for the raised floor', u""Data Center Engineer\nJones Lang LaSalle, BMO Harris Bank Data Center, Naperville\nOctober 2013 to November 2016\nOperate and maintain a 329,770 square foot office building including a 7,000 square foot Tier II Data Center. Manage multiple vendors daily, as well as assist in three separate budgets ranging from $125K to $2.5 million, capital projects, and the JLL engineering audit. Reduced budget by $100K within first year between HVAC services and professional engineering of domestic water systems. Currently implementing a building fall protection plan for roof top maintenance.\n\n\u2022 Two Emerson 500 kVA UPS systems supported by 126 NXL wet cell batteries\n\u2022 Two 1825 kW Diesel CAT Generators with 8,000 gallon underground storage tanks\n\u2022 One 300 kW Diesel CAT Generator with 500 gallon day tank for fire life safety\n\u2022 Eight York 105 ton 410a, gas fired heat, six stage roof top units\n\u2022 Two York 155 ton 410a, air cooled chillers supporting Data Center\n\u2022 Eight 30 ton Liebert CRAH units\n\u2022 Six Liebert 5 ton CRAC units with redundant Carrier split system R-407C units\n\u2022 Avtron permanent load bank 1500 kVA\n\u2022 Asco Paralleling Switchgear\n\u2022 Automated Logic Control BAS and DDC Controls\n\u2022 Yaskawa and Danfoss variable frequency drives\n\u2022 380 Nailor fan powered boxes and 30 VAV's\n\u2022 Lutron lighting control with exterior perimeter light harvesting system\n\u2022 VESDA, Notifier, glycol, smoke beam, and preaction fire life safety systems\n\u2022 Four Bradford White hot water heaters\n\u2022 Five Story 830 space parking structure"", u'Mobile Engineer\nColliers International\nOctober 2012 to October 2013\nOperated and maintained over 350 properties company wide.\n\n\u2022 Scheduled and managed contractors for building repairs and maintenance\n\u2022 Created purchase orders, filed payroll, and reported building deficiencies to Property Managers\n\u2022 Retrofitted office building lighting to T5 and T8 fixtures as well as repair of warehouse and parking lot lighting\n\u2022 Maintenance and repair of various roof top units up to 80 tons including Carrier, York, Lennox, Trane, etc.\n\u2022 Installation and repair of gas fired high efficiency unit heaters such as Reznor, Sterling, etc.\n\u2022 Tracer Summit BAS\n\u2022 Trane air cooled 50 ton chiller, R22\n\u2022 Troubleshooting and maintenance of F.A.C.P., life safety, and sprinkler systems', u""Lead Engineer\nHilton Chicago O'Hare\nJuly 2005 to October 2012\nAssisted in the planning and execution of over $30 million of capital projects including complete renovation of every guest, meeting, and ballroom of the hotel, replacement of the hotels domestic water heating system, installation of a new life safety system, and modernization of the hotel elevators."", u""Stationary Engineer Apprentice\nHilton Chicago O'Hare\nAugust 2003 to July 2005\nOperated and maintained 860 guest rooms in a 591,969 square foot building with 77 meeting rooms totaling 33,000 square feet.\n\n\u2022 Managed and delegated work between five Apprentices\n\u2022 Two Kewanee low pressure boilers\n\u2022 Chilled water cooling system from central plant on O'Hare Airport property\n\u2022 Hot water heating system from central plant on O'Hare Airport property\n\u2022 15 air handling units ranging from 29,500 to 33,800 cfm\n\u2022 Barber-Colman pneumatic systems\n\u2022 2 Quincy compressors\n\u2022 Toshiba variable frequency drives\n\u2022 Perform chemical tests and maintenance on a 16,800 gallon pool/1,700 gallon hot tub\n\u2022 Perform building maintenance such as changing filters on air units, sweeping air pits, greasing motors, changing fan belts, and room maintenance\n\u2022 Repair kitchen equipment such as ovens, ranges, fryers, coolers, freezers, steamers, pot washers, dishwashers, and multiple walk in coolers and freezers""]",[u'Associates in Stationary Engineering'],[u'Triton College\nSeptember 2011']
0,https://resumes.indeed.com/resume/54aea845842ab771,"[u'Database Engineer\nS&P GLOBAL - New York, NY\nJune 2016 to Present\n\u2022 Based on business requirements, designed data model and created database components like tables, views, stored procedures, SSIS packages and jobs to ETL daily pricing feeds on SQL Server with TSQL\n\u2022 Monitored procedures and jobs on the server, fixed failed jobs and tuned performance of slow stored procedures and frequent running queries\n\u2022 Developed Data Pipeline Service with C# and SQL Server to migrate data from database to customer-facing platform\n\u2022 Created unit test cases with tSQLt and C# to validate correctness of procedures and applications and data accuracy\n\u2022 Supported on development environment, resolved issues like user access, server slowness, procedure blocking etc.\nUsed Delphix to snapshot production backup and refresh development server regularly.', u""Data Management Assistant\nUNIVERSITY OF NORTH CAROLINA, Department of Psychiatry - Chapel Hill, NC\nNovember 2015 to May 2016\n\u2022 Gathered survey and source data structure's requirements, designed data model for back-end database system\n\u2022 Used MS Access and VBA to implement back-end calculation and front-end dashboard for data entry\n\u2022 Created online questionnaires for co-operation of different institutes and collected survey data with Qualtrics"", u""Database Engineer Intern\nS&P CAPITAL IQ, Data Science Team - New York, NY\nJune 2015 to August 2015\n\u2022 Used Aginity Workbench to connect to AWS and ran bash script to copy datasets from S3 to Redshift\n\u2022 Constructed python program to automatically validate and parse office plug-in products' usage data\n\u2022 Employed clustering and association rule to discover patterns in user behavior and identify user groups in usage data""]","[u'Master of Science in Information Science in Information Science', u'Bachelor of Management in Information Management and System in Management']","[u'University of North Carolina at Chapel Hill Chapel Hill, NC\nMay 2016', u'Peking University Beijing, CN\nJuly 2014']"
0,https://resumes.indeed.com/resume/1ace03263ea378d2,"[u'Data Intern\nKGISL\nAugust 2015 to October 2015\nIntern | KGISL (KG Information Systems Private Limited) Aug 15 -Oct 15\nBuilt the database containing demographics of information about the customers and devised the metrics for the analysis, scoring and quality assessment of the customer data Employed statistical techniques like maximum likelihood estimation, multiple data imputation to impute missing data\nApplied segmentation techniques such as K- means clustering, Support vector machine(SVM) & Latent Class Analysis(LCA) to group individual customer profiles into different \u201cmarket segments\u201d to identify important information about key customers.', u'Project Engineer\nWipro Technologies - Chennai, Tamil Nadu\nJanuary 2014 to July 2015\nProject Engineer | Wipro Technologies | Jan 14 \u2013 July 15\nThis project is a Business to Business platform which involved Testing, Implementation and Technical Support for all member banks in Asia Pacific region.\n\u2022 Lead a team of four efficiently in the Automation Development using VB Macros which increased the productivity significantly and decreased the manual processes by 22% with tighter integration thus reducing the overall efforts remarkably.\n\u2022 Standardized content from disparate databases, enabling support staff to quickly respond to customer requests\n\u2022 Tested storage strategies and designed disaster-recovery plan for member bank\u2019s operational database, delivering solution that guaranteed recovery performance and high availability']","[u""Master's""]",[u'']
0,https://resumes.indeed.com/resume/bdf8a4d8f7955c95,"[u'Data Scientist Co-op\nDoor of Clubs - Boston, MA\nMay 2017 to Present\n\u2022 Built supervised predictive model in R and Python to predict categories of new registered club based on historical data.\n\u2022 Performed data cleaning and wrangling and conducted data exploratory analysis on word count of company\napplications to recommend the length of their applications subject and description.\n\u2022 Analysed and visualized data of student profiles through MySQL and R to provide useful information and insights to management of Door of Clubs.\n\u2022 Implemented analytics functions on data to predict which are best majors and skills of students for companies.\n\u2022 Developed algorithm to anticipate which criteria used by companies have highest success ratio for students.\n\u2022 Recommended management which time of day, day of week and month of the year is best suitable for companies to post applications for students on Door of Clubs platform.', u'Associate Engineer\nQlogic Private Limited - Pune, Maharashtra\nAugust 2013 to December 2015\n\u2022 Conducted functional testing of device drivers and Qlogic management application as a part of product release.\n\u2022 Prepared, executed test case plans for features of Qlogic products. Recommended features for product improvement.\n\u2022 Performed Sanity, black box and regression tests for network and storage features of Qlogic adapters.\n\u2022 Improved testing efficiency up to 20% by implementing software testing methodology.\n\u2022 Analyzed critical and blocking issues to ensure best quality of product and interacted with customers to solve them.']","[u'Master of Science in Engineering Management', u'Bachelor of Engineering in Electronics']","[u'Northeastern University Boston, MA\nDecember 2017', u'Shivaji University Kolhapur, Maharashtra\nJune 2012']"
0,https://resumes.indeed.com/resume/dea043c2e4f581f7,"[u'Data Center Engineer\nJPMorgan chase - New York, NY\nJanuary 2005 to January 2016\n\u2022 Administer UNIX, NT, and Windows, LAN/WAN protocols, and backup more than 50 domestic and international sites.\n\u2022 Administer the setup and configuration of HP software server software as well as network activity and NT domains development.\n\u2022 Assume accountability in handling the business as usual (BAU) work within the space.\n\u2022 Maintain active engagement with all vendors to strategize and deploy all projects in the datacenter.\n\u2022 Rank and stack servers and all equipment coming into the data center on a daily basis.\n\u2022 Rack and stack network servers\n\u2022 Running copper and fiber cables\n\u2022 Troubleshooting and diagnostics of server hardware/connectivity\n\u2022 Commission and Decommission of Hardware\n\u2022 Familiarity with Patch Panels\n\u2022 Vendor Management\n\u2022 Rack & Stack server and IT hardware in corporate Data Center.\n\u2022 Responsible for server implementation, support and decommissioning within a secure raised-floor environment.\n\u2022 Install support hardware for all devices to be installed into cabinets on raised floor.\nKey Highlights:\n\u2713 Played a vital part in building one of the largest datacenters of the company as well as in working on special projects globally.\n\u2713 Rendered hands-on management to all company projects entering the datacenter which supports the investment bank.\n\u2713 Drove efforts in building and overseeing the electronic trading platform for the Investment Banking Division.', u'System Analyst\nIBM - New York, NY\nJanuary 2003 to January 2005\n* Managed various protocols and servers including Sun Sparc Stations, Ultra Sparcs 5/10/20 servers, Sun 1000/2000/4000 series servers as well as user accounts, external disk storage arrays, and Sun Enterprise 3500 proxy server.\n* Offered technical support for the Chase.com Internet site which include the UNIX/NT server infrastructure, as well as related maintenance and site rebuild projects.']","[u'Certification in Liberal Arts', u'in Chase Manhattan Bank', u'']","[u'New York City College New York, NY', u'Long Island University New York, NY', u'Albert Merrill School New York, NY']"
0,https://resumes.indeed.com/resume/2c084462325f4989,"[u""Data Science Analyst\nRomp N' Roll - Wethersfield, CT\nSeptember 2017 to December 2017\nR, Excel, Tableau\n\u2022 Recommended the best pricing model by performing A/B testing for each franchise location by analyzing the historical sales data\n\u2022 Segmented the customers and built a survival model to decrease attrition through churn prevention\n\u2022 Text mined customer reviews to generate insights to understand member attrition\n\u2022 Created a tracking framework to see variances and trends in Sales and customer traffic across different locations"", u'Systems Engineer (Data Analyst)\nInfosys - Chennai, Tamil Nadu\nJune 2013 to August 2015\nMicroStrategy 9.X, MS SSIS, SSAS, IBM Cognos, Netezza\n\u2022 Created and executed contracts, cost, and risk reporting & analysis of vendor management systems\n\u2022 Translated simpler ad-hoc reports and translated them to dynamic dashboards; improved the user experience\n\u2022 Automated the daily vendor reports for business team using SQL and MicroStrategy\n\u2022 Built and implemented intelligent cubes that decreased the reporting time over 50%\n\u2022 Performed vendor segmentation to identify and analyze the high risk and high spending vendor profiles\n\u2022 Delivered artifacts for technology platforms by collaborating with stakeholders; documented and analyzed the technical and functional requirements\n\u2022 Designed and created proof of concept and prototypes for various project proposals to help acquire new\nprojects from prospective clients']","[u'Master in Business Analytics and Project Management', u'Bachelor of Engineering in Computer Science']","[u'University of Connecticut School of Business Hartford, CT\nMarch 1995 to March 2018', u'Anna University Chennai, Tamil Nadu\nJanuary 2010 to May 2012']"
0,https://resumes.indeed.com/resume/a5f7f7abb4d94708,"[u'Data Mining Engineer\nPing An Technology Co., Ltd.\nSeptember 2017 to December 2017\n\u2022 Segmented words from sentences of Chinese business reports crawling online using jieba module in python\n\u2022 Identified involved subjects and obtain one of the 13 relationships between them using regular expression\n\u2022 Established relation-inference-model based on 10,000 artificially annotated data to predict subject relationships', u'Data Analyst\nFang Duo Duo Network Technology Co., Ltd.\nAugust 2015 to December 2015\n\u2022 Extracted online advertisement data using SQL and create models to improve the performance of the launch of advertisement\n\u2022 Created weekly and monthly analysis reports utilizing advance excel and business intelligence tools']","[u'M.S., Business in Analytics', u'M.S. in Teaching Chinese to Speakers of Other Languages', u'B.S. in Chinese Linguistics & Literature']","[u'The University of Texas at Dallas Dallas, TX\nJanuary 2016 to May 2018', u'Shanghai Jiao Tong University Shanghai, CN\nJune 2014', u'Shanghai Jiao Tong University Shanghai, CN\nJune 2012']"
0,https://resumes.indeed.com/resume/e1a47eac9bacf8c8,"[u'Data Engineer\nETL PARTNERS GROUP\nOctober 2002 to Present\n\u2022 OBIEE / OBIA / HRA / DAC Installation / Integration / Development / Security / Technical Support using Informatica Powercenter & Metadata Manager Integration/Development/Technical Support of various Operating Systems, Databases, & Languages using Informatica in a Agile Environment. Rational Concert.\n\u2022 Architected Informatica ETL XML ERP Benefit focus System for 834 Membership Enrollment for E-business group. Java transformations. Access Mappings.\n\u2022 Developed Informatica mappings to/from Oracle 11 Operational Data Store and Sql Server 2005, C++, Java, Performance tuning for low latency.\n\u2022 Operations/Support Win 2003/sql server 2000/Informatica 6.xx-8.xx for Energy Trading / Credit Risk Algorithms Data warehousing for Major Oil Company\n\u2022 Architected Informatica Installation, Configuration, Supporting in Algorithm Design for High Frequency / Scalability / Availability for low latency applications.\n\u2022 Sun Solaris Oracle 9/10g RAC Environment\n\u2022 Informatica Development of Master Data Portal from Siebel and Global Data Conversion, C++, Performance Tuning for low latency. Access Data Mappings.\n\u2022 Architected Installation /Configuration / Administration of Informatica Powercenter on Solaris, NT, Win2k on Oracle PowerConnect SAP\n\u2022 Informatica Development of various clinical trial data for multiple data marts\n\u2022 Data integration / Migration / Warehousing from Oracle Clinical & other Legacy systems into Oracle Data Marts\n\u2022 Informatica Metadata Administration and Data Mart Security\n\u2022 Brio validations, building reports and assisted with Business Specifications and Rules. Web development Reports', u'Informatica Data Warehouse Consultant\nYurcor\nJanuary 2000 to October 2002\n\u2022 Installation/Configuration/Administration of Informatica Powercenter/Powermart on HP-UX, Solaris, NT, Win2k on Oracle 8/9\n\u2022 Development of various data warehouses such as Transportation centralized data warehouse, Income/Revenue/Expense Enterprise Data Warehouse, Credit Collections Data Warehouse combined w/ PL/SQL\n\u2022 Data integration/migration/warehousing from Informix, SAP & other Legacy systems into a Oracle/Informix Staging Data Warehouse which fed into the Centralized Data Warehouse on Oracle / Teradata / Informix\n\u2022 Extraction from IBM and IMS mainframes sourcing fixed width, delimited, line sequential files in binary and EBCDIC formats into an Enterprise Data Warehouse\n\u2022 Modified COamBOL to include wrappers to import copybooks. Transformed using occurs clauses.\n\u2022 Modified SAP ABAP code for extraction, C++, VC++, VB custom code\n\u2022 Informatica metadata administration and Performance Tuning of Informatica data warehouse loads', u'Informatica Data Warehouse Consultant\nSan Diego, CA\nSeptember 1998 to January 2000\n\u2022 Design/Developoment/Administration/Installation/Configuration/Performance Tuning of Informatica Powermart\n\u2022 Development/Administration of Powermart Server on Solaris with Oracle 7/8\n\u2022 Data migration/conversion into a MRPII packaged application on HP-UX/ Oracle feeding a Centralized Data Warehouse using combined w/ pl/sql, C++, VC++\n\u2022 Extractions using fixed width ASCII data as line sequential from Unisys and IBM mainframes', u'Informatica Consultant\nInformatica Corporation\nJune 1997 to September 1998\n\u2022 Informatica Architecture and metadata exchange with Business Objects, Brio, Microstrategy\n\u2022 Installation/Configuration/Performance Tuning of Powermart/Powercenter at numerous client sites throughout the country.\n\u2022 Development/Administration of Powermart/Powercenter for Data Warehousing, financial, investment, and other applications using SAP, Oracle Financials, Peoplesoft, Oracle, Sybase, MS Sql Server, Informix, DB2, mainframes\n\u2022 Informatica extractions from IBM, DB2, MVS mainframes using Occurs clauses and Cobol wrappers around copybooks/copylibs loading into Red Brick data warehouse via flat files', u""Technical Associate\nEDS - San Jose, CA\nMay 1995 to June 1997\nComputer Manufacturer - Materials Management & Manufacturing Applications\n\u2022 Performed views, joins and queries of tables for sales and financial applications on an ERP package.\n\u2022 Development of Crystal Reports running on a NT server with Oracle database running on HP-UX.\n\nHealthcare / Pharmaceuticals - Warehouse Management System\n\u2022 Applications Development on a Digital UNIX platform with Sybase\n\u2022 Performed maintenance of database tables, data dictionary, and SQL programming on a Materials Management packaged application.\n\u2022 Customer enhancements written in C / C ++\n\u2022 Proposed, planned, designed, and implemented migration from a centralized printing operation to a distributed operation. Saved customer $20,000 in maintenance.\n\u2022 Development/Administration of Web pages for an Intranet.Installation/Configuration of Netscape's Web Server.\n\nComputer Services - Financial/Sales applications\n\u2022 Identified bugs and suggested improvements on a UNIX platform running an Astea client/server packaged application.\n\u2022 Processed backlog of financial transactions with the customer.""]",[u'B.S. in Computer/Electrical Engineering'],"[u'University of Southern California Los Angeles, CA']"
0,https://resumes.indeed.com/resume/b146f382f1c02406,"[u""Senior Data Center Engineer\nEXPEDIA INC., Apex systems - Chandler, AZ\nJanuary 2016 to Present\nData Center Infrastructure Engineer\nLead all design and development of Expedia's Data Centers, both nationally and internationally.\n\u2022 Significantly mitigated unplanned outages and increased bookings by building a stable environment.\n\u2022 Oversee capacity planning for all lines of business from inception to completion to determine if growth and what size budget is required.\n\u2022 Supervise team of data center technicians while planning, proposing, and managing complex Data Center projects, including resource and scope management as well as budgeting and scheduling.\n\u2022 Deliver specification information to support IT environments, including UPS power, HVAC, racks, structured cabling, and physical room layout.\n\u2022 Steer capacity analysis and requirements for floor space based on IT equipment specifications and building facilities requirements.\n\u2022 Partner with manufacturers to support comprehensive Data Center build outs, creating cost effective solutions to support each component.\n\u2022 Plan and design all electrical requirements within the Data Centers, collaborating with electrical contractors for installations.\n\u2022 Lead planning and design of fiber and copper needs for all Expedia Data Centers as well as documentation for structured cabling and equipment installations.\n\u2022 Record IT infrastructure requirements to ensure implementation of standards related to power, cooling of structured cabling, and room design.\n\u2022 Interface with operations and numerous IT management teams on projects.\n\u2022 Evaluate and recommend hardware technologies to enhance IT facility support as well as vendor selection.\n\u2022 Assess and advise regarding prospective hardware to support standard components to be utilized in Data Center build support.\n\u2022 Constructed project plans for various projects, including scope, cost, schedule, and contractual deliverables.\n\u2022 Facilitated techniques behind project planning, tracking, change control, and risk management.\n\u2022 Key contributor to weekly meetings with stakeholders, clients, and teams, executing relocation of equipment and users.""]",[],[]
0,https://resumes.indeed.com/resume/4bc1a0933585f8fe,"[u'ML Engineer\nBerkeley DeepDrive\nPresent\n\u2022 Implementing our lab\'s new deep learning architecture ""Deep Layer Aggregation"" as the feature extractor for Faster-RCNN\n\u2022 Conducted experiments between object-detection models(faster-rcnn, ssd, yolo) to observe the effects of different\ndomains(weather, time, location) on self-driving vision systems. Results coming in upcoming authored CVPR paper.', u""Towards Data Science Publication \u21a6 Writer\nJune 2017 to August 2017\nCurrent\n\u2022 A top writer on medium in the category of Artificial Intelligence. Writing about my journey learning data science, and published a popular article about object detection on TDS, medium's biggest data science publication\nSalesforce \u21a6 Software Engineer Intern Summer 2017\n\u2022 Implemented new user interface for Data.com with industry-standard security features and code coverage\n\u2022 Thoroughly tested components using continuous integration and sandbox environments to ensure stability"", u""Data Engineer\nBerkeley Lab for Transportation\nJune 2016 to August 2016\n\u2022 Dealt with millions of traffic data entries using modular MySQL tables and PANDAS data library for high performance\n\u2022 Built the database of a smart traffic assistant by scraping real-time traffic data from various public transportation API's\n\nENGINEERING PROJECTS\nDeep Layer Aggregation for Faster-RCNN\nGoal: Implement an architecture to boost performance of Faster-RCNN - FasterRCNN & DLA medium posts\n\u2022 Modified Pytorch codebase of Deep Layer Aggregation to work with popular implementation of Faster-RCNN\n\u2022 Improved Faster-RCNN mAP performance on PASCAL 2007 test set by 1 point\n\nStructured Data: Deep Learning vs Tree Methods\nGoal: Explore Deep Learning vs Tree Methods on Structured Data - Structured Data medium post\n\u2022 Used embedding matrices to encode categorical variables for deep learning\n\u2022 Comparison with popular tree methods like XGBoost and Random Forests\n\u2022 Ultimately achieved a top 5 score on past kaggle Housing Prices competition\n\nRandom Forest Interpreter\nGoal: Learn deployment of ML apps and provide a useful ML service\n\u2022 Used Django & Scikit learn to deploy Random Forest as a service for non-technical people\n\u2022 Implemented services to interpret Random Forests such as feature importance, individual tree scores, and visualizations.\n\nPearl Jam Multiplayer Game\nRealtime Mobile/Desktop game to play with your friends - https://github.com/mikeliao97/pearlJamGame\n\u2022 Implemented real time game logic using Socket.io to create fast response times between server and client\n\u2022 Architected a scalable architecture by using Nginx and Docker Compose to route client requests to multiple game servers\n\u2022 Developed concurrently a mobile application React Native to bring the game experience to mobile user""]",[u'B.S in Computer Science'],"[u'University of California Berkeley, CA\nMarch 2019 to June 2019']"
0,https://resumes.indeed.com/resume/591a9936b01316bb,"[u""Engineer & Data Analyst\nO'Brien & Gere - Chicago, IL\nJune 2016 to June 2017\n- Team Leadership: Supervised 5 people and directed cross-functional teams of more than 30 experts to accomplish all project goals in tight schedules; optimized workflows and reduced labor costs by 17%.\n- Financial Analysis: Constructed a generic landfill probability valuation model for multiple power plants to allocate capital and reduce short and long term operational risks and costs for clients, increased the return on investment by 1%.\n- Data Management and Simulation: Managed internal database and designed and implemented novel simulation techniques (Monte Carlo simulation, power analysis, and moving window analysis) on 100,000 data points to assess the environmental impact of contaminants; improved the efficiency of the data group by 40%.\n- Strategy Implementation: Developed a comprehensive strategic action plan for client based on 40 years of historical site investigation data and supplemented with on-going site remediation sampling events; implemented the entire action plan and oversaw the construction; removed the contamination and increased the property value by $1.1M."", u""Engineer\nO'Brien & Gere - Milwaukee, WI\nJanuary 2014 to June 2016\n- Process Improvement: Created the first company-wide automated and streamlined data management and project reporting approach to reduce operation time from 3 hours to 5 minutes, resulting in securing 2 new business opportunities.\n- Team Leadership: Initiated and led education and training programs of air monitoring process for 18 employees with limited budget; ultimately delivered 100% satisfactory rate; mentored and supervised 3 technicians to facilitate field responsibilities and professional development; increased work efficiency by 25% and maintained zero accidents within 2 years.\n- Problem Solving: Adopted statistical resampling logistics to solve the problem of small sample size in field sample collection; reduced operation cost by half and optimized labor resources.\n- Data Analysis: Designed analytical and numerical flow-and-transport approach for more than 10 projects to capture groundwater and soil contaminants migration trends; predicted future migrations and recommended action."", u'Marketing Specialist\nBCEG ENVIRONMENTAL REMEDIATION - Beijing, CN\nJuly 2010 to August 2011\n- Business Development: Analyzed the market and monitored external business environment (policies, regulations and pollution incidents) to identify target client companies with potential remediation needs ahead of competition; established 4 new business opportunities within one year; introduced $30M revenue; helped break into Southern China market.\n- Client Relationship Management: Developed a service survey system to collect and organize feedback from clients on performance improvement; increased repeat client rate from 62% to 77%.\n- Cost Efficiency: Led a team of 3 people to perform business cost efficiency analysis of contractor procurement and budget control of remediation projects ranging from $50K to $8M.']","[u'Master of Business Administration', u'Master of Science in Environmental Engineering', u'BS in Environmental Engineering']","[u'GEORGETOWN UNIVERSITY, McDonough School of Business Washington, DC\nJuly 2017 to May 2019', u'NORTHWESTERN UNIVERSITY, School of Engineering & Applied Science Evanston, IL\nSeptember 2011 to June 2013', u'SOUTH CHINA UNIVERSITY OF TECHNOLOGY Guangzhou, CN\nSeptember 2006 to July 2010']"
0,https://resumes.indeed.com/resume/0f14c78f85f80a4a,"[u'Data Analyst\nSysware Inc - Fremont, CA\nDecember 2016 to Present\nUnderstand information and create scripts to collect, manipulate and transform data using\nPython/SQL/Hive\n\u25cf Design and build ways to link data together from disparate sources\n\u25cf Create meaningful reporting base on requirements\n\u25cf Interpret data, analyze result using statistical techniques\n\u25cf Assist in building predictive models using machine learning algorithms', u'Data Analyst\ni-Tech Inc - Fremont, CA\nNovember 2015 to November 2016\nArchitect the data pipelines to connect various data sources\n\u25cf Develop Extract, Transform and Load (ETL) process and maintain using Python/SQL\n\u25cf Assist to build streaming services for real time reporting and data movement\n\u25cf Tune and improve the performance of relational data warehouse\n\u25cf Support team with development of analytical dashboard', u'Data Engineer\nWuhan Medicine Company - Wuhan, CN\nAugust 2010 to August 2012\nInterpret data, analyze results using statistical techniques\n\u25cf Identify, analyze, and interpret trends/patterns in complex data sets using Python, R and SQL\n\u25cf Filter and clean data and review reports/performance to locate and correct code problems\n\u25cf Work closely with management to prioritize business and information needs\n\u25cf Locate and define new process improvement opportunities', u'Software Engineer\nApollo Computer Inc - Wuhan, CN\nJuly 2008 to July 2010\nPerform documentation and maintenance of architectures, requirements, algorithms, inter- faces and designs for software systems.\n\u25cf Develop and maintain code and integrate software components into fully functional soft- ware system using Java/C/C++ GUI\n\u25cf Assist with test procedures and documenting test results to ensure software system requi- rements are met.\n\u25cf Troubleshoot basic software issues.']","[u'Master of Computer Science in Computer Science', u'Bachelor of Computer Science in Computer Science']","[u'Northwestern Polytechnic University', u'Huazhong University of Science and Technology']"
0,https://resumes.indeed.com/resume/da3737a81ea601d3,"[u'Data Engineer\nJanuary 2016 to March 2018', u'Development Team Lead\nAugust 2014 to July 2016', u'Senior Software Engineer\nMarch 2013 to August 2014', u'Software Engineer\nSeptember 2010 to March 2013']","[u""Master's in Computer Science""]",[u'September 2000 to June 2005']
0,https://resumes.indeed.com/resume/0d428fafbf3e0ab2,"[u'Sr. Data Center Engineer\nEdgecast Networks - Playa Vista, CA\nJuly 2017 to Present\nResponsible for the maintenance of 15+ edge nodes located all over the world\n\xb7 Manage a team of 3 technicians to ensure that reporting metrics are met and provide technical guidance when needed\n\xb7 Led a project in Germany where 10 racks of data center hardware were decommissioned and transferred to a different site; existing equipment was replaced and retired\n\xb7 Interprets project requirements and complete Bill of Material based on data center specifications\n\xb7 Identify areas for improved efficiency within the Data Center Operations team and process implementation', u'Data Center Engineer\nEdgecast Networks - Playa Vista, CA\nMarch 2015 to July 2016\nLed the physical deployment of 12+ points of presences in North America, South America, Europe, Asia, Australia and Africa \u2013 ensured that the maintenance states and locations were updated in our data base\n\xb7 Implemented a port mapping document for better traceability of connections and visibility of available ports\n\xb7 Provide detailed work instructions to Remote Hands to upkeep hardware globally; adapted to working with different time zones and communicating through language and cultural barriers Worked with vendors to expedite RMAs and replace faulty hardware and components in a timely manner\n\xb7 Worked with vendors to expedite RMAs and replace faulty hardware and components in a timely manner', u'Data Center Technician\nEdgecast Networks - Santa Monica, CA\nMarch 2014 to March 2015\nLed the physical deployment of 8+ points of presences in North America, South America, Europe, Asia, Australia and Africa \u2013 ensured that the maintenance states and locations were updated in our data base\n\xb7 Implemented a port mapping document for better traceability of connections and visibility of available ports\n\xb7 Provide detailed work instructions to Remote Hands to upkeep hardware globally; adapted to working with different time zones and communicating through language and cultural barriers Worked with vendors to expedite RMAs and replace faulty hardware and components in a timely manner\n\xb7 Worked with vendors to expedite RMAs and replace faulty hardware and components in a timely manner', u'DATA CENTER TECHNICIAN\nTelephone & Data Systems - Des Moines, IA\nNovember 2012 to September 2013\nProvide Remote Hands Support to Customers.\n\u2022 Maintain 100% Site Uptime.\n\u2022 Structured/Data Cabling.\n\u2022 Performed Day to Day Operations.\n\u2022 Provide Site Access to Customers.\n\u2022 Server Hardware RMA.\n\u2022 Provide Site Security.\n\u2022 Monitor the Data Center Via Andover Software.\n\u2022 Server O.S Installation for customers.', u""DATE CENTER ENGINEER\nMYSPACE INC\nSeptember 2006 to May 2009\nProgramed Existing and New Servers.\n\u2022 Installed New Servers For Website Upgrades On A Weekly Basis.\n\u2022 Server Reboots Hands On, And Via Integrated Lights Out.\n\u2022 Located, Serviced, Upgraded and Maintained Over 5,000 Servers.\n\u2022 Oversaw and Managed New Deployment's.\n\u2022 Installed Several Blade Enclosure's For Hulu.com.\n\u2022 Imaged Servers Through Altiris and an In-House Built Imaging Software.\n\u2022 Server Hardware RMA.\n\u2022 Maintain 100% Server Uptime Through Four States And Eleven Data Centers.\n\u2022 Traveled To Out Of State Data Centers For New Deployments.\n\u2022 Structured Cabling.\n\u2022 On Call Responsibilities.""]",[],[]
0,https://resumes.indeed.com/resume/fdeb59e43ee9fdb3,"[u'Data Science Engineer\nCompanionLabs - Akron, OH\nFebruary 2017 to Present\n-Built and Manage Data Pipeline that processes roughly 250,000 incoming files per day.\n--The Data Pipeline consists of the following tools: Streamsets, Apache Spark, Python, Amazon S3, Amazon Athena, PostgreSQL, Amazon EC2 and Amazon EC2 Container Service (Docker)\n\n-Built Amazon S3 based data warehouse that utilizes Parquet files stored on S3 and queried using Amazon Athena (Serverless data warehouse)\n\n-Designed and Developed Ad Grading System that provides statistically significant grades to help customers easily understand which of their Facebook Ads are performing most effectively. Tools used: Python, PySpark, Pandas, SparkSQL, Multiple-Criteria Decision Analysis\n\n-Built Predictive Budget Recommendations System, which provides Marketers with recommendations to maximize their marketing budget, while accounting for diminishing returns. Tools used: Python, PySpark, Pandas, scikit-learn\n\n-Created Tableau Workbooks and Dashboards to better understand and assistant customers in understanding their performance using our product.', u""Senior Marketing Data Scientist\nSolarCity (Acquired by Tesla) - Cleveland, OH\nApril 2015 to February 2017\nSolarCity was acquired by Tesla in Nov 2016, and is now a part of Tesla Energy.\n\n-Managed analysts and served as project lead on key projects.\n\nProjects:\n\n-A Key Member on team building internal Marketing Data Management Platform (DMP)\n--Connecting numerous Data Points across consumer journey such as:\n----Billions of Website Display Ad Impressions\n----Millions of Website Page Views\n----Social Interactions\n----Press Release & News Interactions\n----Website Conversions\n----Sales Funnel Conversion Steps\n----CRM Data\n----Search\n\n-Facebook Atlas API integration\n--Ingest Impression, Click and Display Ad Metadata using REST API and Python.\n\n-Facebook Lead Form API integration\n--Integration with Facebook REST API using Python and SQL Server. Leads from Facebook are then added into Salesforce.\n\n-Built Direct Mail Data Warehouse for reporting and analysis.\n\n-Terrestrial Radio and TV Campaign Lift Analysis\n--Using A/B tests, analyze the effect of Terrestrial Radio and TV campaigns on business generation in metro areas. Utilized Regression analyses to create baselines to identify the lift created by Offline Advertising.\n\n-Online Video A/B test: Measure the effectiveness of Online video channels (YouTube, AOL, etc), and how they influence consumer's propensity to convert.\n\n-Marketing Channel Attribution Models\n--Last Touch Attribution (View + Click) between numerous Display Advertising Vendors (10+)\n--Multi-touch Attribution - Implemented Linear and Time Decay Models integrating numerous marketing channels.\n\nTools Used: Amazon Redshift, Python, R, SSIS, SQL Server, Tableau, REST APIs, Facebook Graph API, Salesforce"", u'Data Scientist / Data Engineer\nKnotice (acquired by IgnitionOne) - Akron, OH\nApril 2010 to April 2015\nKnotice was acquired by IgnitionOne in 2014.\n\n-Conducted Exploratory Data Analyses on client data to uncover customer centric marketing opportunities.\n\n-Digital Marketing Response Modeling using R, MADlib and SQL. I created digital marketing engagement models to predict customer response in different marketing channels. These models utilized Logistic Regression.\n\nPredicted the next product category an individual customer would be interested in purchasing. This utilized Neural Networks for prediction. I wrote R code that translated the Neural Network model scoring into SQL code so that all scoring could be completed within the database.\n\n-Used Hadoop, Hive and Impala on Amazon Elastic MapReduce (EMR). I built a Display Advertising Data Warehouse using Hive to process new data and Impala to allow for quick access and aggregation of data.\n\n-Building Data Exploration Dashboards using Tableau.\n\n-Built ETL to load large database tables and large scale queries with Greenplum Database and Amazon Redshift.\n\n-Utilized many of the MS SQL Server BI tools:\n--MS SQL Server 2008,2012.\n--Microsoft SQL Server Integration Services 2008,2012.\n--Reporting Services 2008 R2 using Report Builder 3.0.\n\n-Developed ETL processes to load and process gigabyte sized files on a daily basis.\n\n-Query Optimization for SQL Server. Created new database indexes, and used query join hints when necessary.\n\n-Fuzzy Matching for data reconciliation. Performed fuzzy matching on Street Addresses to consolidate user profiles.\n\n-Complex T-SQL and PL/pgSQL to handle large data processing.', u'Business Intelligence Developer\nProgressive Insurance - Cleveland, OH\nJuly 2007 to April 2010\nUsing the following technologies to create dashboards, cubes (OLAP), and reports for analysts and technicians:\nMS SQL Server 2000, 2005 (Database).\nMS Integration Services 2005 (ETL).\nMS Analysis Services 2005 (OLAP).\nMS Reporting Services 2005 (Reporting).']","[u'Certificate in Data Science', u""Master's in Computing and Information Systems"", u'Bachelor of Business Administration in Computer Information Systems']","[u'University of Washington\nDecember 2004 to January 2014', u'Youngstown State University Youngstown, OH\nAugust 2006 to December 2007', u'Kent State University at Kent Kent, OH\nAugust 2001 to April 2005']"
0,https://resumes.indeed.com/resume/50afc3a0fdb6f7fc,"[u'Staff Consultant\nSWC Technology Partners - Oak Brook, IL\nNovember 2016 to November 2017\n* Generation of reports and dashboards in PowerBI, some which required the use of custom visualizations to enhance presentation\n* Refactoring complex SQL queries to add data elements for consumption by external applications\n* Creating new dimensions for slicing data utilized in Tabular and Multidimensional cubes\n* Development on pre-existing ASP.net web forms to add/enhance functionality\n* Utilization of BIML to generate pipelines and datasets for an Azure Data Factory project\n* Deployment and management of an Azure Data Factory project with over 2700 dataset, 27 pipelines and 5 linked services\n* Building of a Master Data Services model, entities and views\n* Development of SSRS reporting solutions', u'Data Engineer\nZirMed - Chicago, IL\nFebruary 2015 to November 2016\n* Creation of C# based windows forms that allow users to interact with SQL Server databases and SSIS jobs to run complex queries and produce reports in Microsoft Excel\n* Use of SQL Profiler to investigate execution plans and enhance performance of queries\n* Modification and creation of stored procedures, views, tables, functions and SQL Server Agent jobs\n* Development of PowerShell scripts that automate tasks across multiple servers\n* Debugging, re-constructing and re-deployment of previously created SSIS applications\n* Development of reporting in Microsoft Excel and HTML formats via SSIS custom scripting tasks\n* Implementation of Change Data Capture in SQL Server in order to track changes in database tables for weekly reporting\n* Tuning of SQL queries to ensure efficiency and improve readability and maintenance\n* Analysis of healthcare data to troubleshoot and resolve issues encountered by a team of certified medical record auditors\n* Collaboration with highly knowledgeable healthcare professionals to produce critical data gathering queries and reports\n* Cross team collaboration to identify, understand and resolve critical and non-critical bugs and product enhancements\n* Documenting of processes and steps taking to resolve issues\n* Transfer of documentation and knowledge for the completion of tasks, operation of applications and troubleshooting of issues', u'Data Developer\nApartments.com - Chicago, IL\nNovember 2011 to February 2015\n* Created an ASP.net web application for accessing SQL data used to manage daily inbound feed details\n* Constructed new orchestrations, maps, pipelines, send/receive ports for the reception and storage or data via FTP and web services using Microsoft BizTalk platform\n* Reorganized legacy BizTalk maps for enhanced readability and increase the ease of future maintenance\n* Wrote custom XSLT for the transformation of XML data into a standardized format\n* Analyzed and sourced data from SQL Server for daily migration into Salesforce using SSIS, APEX Data Load, SOQL, Salesforce Workbench and CozyRoc\n* Developed SSIS package that utilized a framework to migrate data from files into SQL Server and from SQL Server to outbound files\n* Maintained and modified legacy jobs used to create outbound feed files with DataStage\n* Established production migration instructions to aide in the deployment of job to multiple environments\n* Built Talend jobs to extract data from Microsoft SQL Server 2008 transform the data and insert it into a MySQL database\n* Utilized NuGet packages for encoding and decoding of data in SSIS\n* Assisted in testing to assure the quality of processes developed by self and others\n* Working collaboratively with other teams to resolve production issues and resolve consumer complaints\n* Effectively participated in agile methodology for software development through both Scrum and Kanban using multiple tools\n* Participated in work supplemented Toastmasters International program to improve upon communication']","[u'Masters of Science in Mathematics Education', u'Bachelors of Science in Computer Science']","[u'Brooklyn College Brooklyn, NY\nJune 2007', u'Morehouse College Atlanta, GA\nMay 2005']"
0,https://resumes.indeed.com/resume/4a9eabe9211437f5,"[u'Data Analytics Researcher\nIBRC, Kelley School of Business\nFebruary 2017 to Present\n(Tools and Technologies: R, Excel, Python)\n\u2022 Collect and cleanse demographic data from multiple sources.\n\u2022 Forecasted population of Indiana by 5-year age groups and sex using components of demographic change.\n\u2022 Used univariate ARIMA models to forecast age-specific mortality and fertility rates.\n\u2022 Performed characteristic based clustering on employment time series of US counties.', u'Data Science Intern\nIBM\nMay 2017 to August 2017\n(Tools and Technologies: Java, Python, JavaScript, NoSQL, HTML, CSS, IBM Watson)\n\u2022 Built a Chabot prototype using IBM Watson Conversation service to automate the hiring process.\n\u2022 Used IBM Watson Natural Language Understanding to extract key concepts and topics from the chat.\n\u2022 Integrated the chatbot with IBM Cloudant service to store chat history as a NoSQL JSON document.', u'Software Engineer\nNucleus Software Private LTD - Noida, Uttar Pradesh\nJuly 2013 to August 2016\n(Tools And Technologies: Java, SQL, HTML, JIRA, SVN, Selenium, HP Load Runner.)\n\u2022 Maintained SQL scripts and complex queries for detailed analysis and extraction.\n\u2022 Developed and implemented automated test scripts using Selenium Webdriver Java bindings.\n\u2022 Engaged in constant communication with stakeholders for gathering and understanding business requirements.']",[u'Masters in Data Science in Data Science'],[u'Indiana University at Bloomington\nAugust 2016 to December 2017']
0,https://resumes.indeed.com/resume/843285560faadf2a,"[u'Data Center Technician/Engineer\nTELX/DIGITAL REALTY - Clifton, NJ\nOctober 2014 to Present\n-Develops installation standards and project tracking/management documents.\n-Responds effectively, verbally and in writing, to sensitive issues, complex inquiries or complaints.\n-Interacts effectively with managers, clients, customers and the general public.\n-Verifies access level via the customer access list.\n-Unlock and lock customer cages and cabinets.\n-Responds to TST in accordance with standard operating procedures.\n-Performs collocation room inspections as required.\n-Assist carrier representatives to the P.O.E. and Telco spine as needed.\n-Installs network media cross-connects as needed.\n-Occasionally installs networking gear.\n-Perform fiber testing, rack and stack, and troubleshooting of network gear.', u""Security Officer\nALLIEDBARTON SECURITY SERVICES - Lyndhurst, NJ\nJuly 2013 to October 2014\n\u2022 Provided customer service in the NJR3 loading dock office by e-mailing clients to inform them when their package arrived.\n\u2022 Created TRT's for each package that arrived at the facility to be transferred.\n\u2022 Filed packing slips regarding internal Telx packages.\n\u2022 Answered phone calls and directed clients in the proper direction.\n\u2022 Monitored who came in and out from the building through the loading dock.\n\u2022 Responsible to be sure that everyone in the building was checked in.""]",[u'A.S. in IT Management'],"[u'BERKELEY COLLEGE Paramus, NJ\nApril 2018']"
0,https://resumes.indeed.com/resume/df2ff4aad162cbe6,"[u'Associate Software Engineer/ Data Analyst\nAccenture - IN\nAugust 2014 to August 2016\nAreas: SQL, Machine learning, Pattern recognition, Data Mining\n\u2022 Exhibited expert knowledge in statistical Pattern recognition techniques, clustering algorithms.\n\u2022 Performed data analysis, analyzing the performance of different clustering algorithms.\n\u2022 Implemented statistical modeling techniques like linear regression, logistic regression, decision tree\n\u2022 Projects performed on neural networks and analyzed the performance.\n\nDATA SCIENCE PROJECTS:\nUS airline sentimental analysis\nObjective: To identify customer mood based on tweets about airlines\n\u2022 Created rich data repository using twitter API to collect tweets about different US airlines.\n\u2022 Performed data cleansing and data transformation using R.\n\u2022 Developed complex algorithm to identify customer attitude towards airlines using Na\xefve Bayes, Clustering and K- Nearest neighbors.\n\nPredicting the probability of a customer having flu\nObjective: To predict whether the student has affected with flu\n\u2022 Analyzed train data for 2000 students, performed variable exploration, imputation and transformation.\n\u2022 Implemented statistical modeling techniques like decision tree and logistic regression.\n\u2022 Assessed and compared the implemented models which yielded 75% accuracy rate for logistic regression.\n\nIncome Classification\nObjective: To determine whether each individual has income above or below a certain threshold, given his description\n\u2022 Analyzed train data and performed variable exploration, normalization on numerical data and one hot encoding on categorical data.\n\u2022 Implemented statistical modeling techniques like decision tree and logistic regression, Na\xefve predictor, Supply vector machines(SVM), neural networks\n\u2022 Assessed and compared the implemented models which yielded 85% accuracy rate for SVM.\n\nSMS Classification as spam or ham\nObjective: To classify whether the given text is Ham or spam\n\u2022 Analyzed train data of 6000 messages and created features from raw data using Python to classify the data\n\u2022 Implemented statistical modeling techniques like decision tree and logistic regression, Na\xefve predictor, Supply vector machines(SVM), random forest\n\u2022 Assessed and compared the implemented models which yielded 96% accuracy rate for Random forest.\n\nEXTRA CIRRUCULAR:\n\u2022 President of COCA (Club of Canvas and Arts) in Wright State University.\n\u2022 Suggested new initiatives at project level which resulted in ultimate client satisfaction.\n\u2022 Active member of ""GPTW"" events team, and organized various events in Accenture, India.\n\u2022 Manager as the coordinator in the fest conducted in GNITS, JNTU, responsible for overseeing the operations and functionality.\n\u2022 Leaded the Blue team(related to orphanages) in ""Nations Army"" an NGO(non-governmental organization which helps in conducting events or senior citizens and kids in orphanages.']","[u'Bachelor of Science in Information Technology', u'Master of Science in Computer Science']","[u'JNTU\nJanuary 2014', u'Wright State University']"
0,https://resumes.indeed.com/resume/1c67182180dc913b,[u'Data Engineer\nSanghvi Foods Private Limited\nJuly 2016 to December 2016\nConverted business requirements into technical specifications particularly around dataflow and database\nmanagement systems\n\u2022 Performed data validation of 5 years of demand planning and transportation data using MySQL\n\u2022 Assisted business user in reporting of demand planning data using R and JavaScript'],"[u'in Social Media for Music Lovers', u'Master of Science in Information Systems', u'Bachelor of Engineering in Computer Science']","[u'Northeastern University Boston, MA\nJanuary 2018 to Present', u'Northeastern University Boston, MA\nSeptember 2017 to May 2019', u'Acropolis Institute of Technology and Science\nMay 2012 to May 2016']"
0,https://resumes.indeed.com/resume/c23b96584e89c910,"[u""Data Analyst Intern\nReliance Jio - Dallas, TX\nJanuary 2018 to Present\n\u2022 Analyzed 10,000 records generated by the telecom towers to calculate its efficiency on JCP access\n\u2022 Based upon the tower's efficiency manipulate bandwidth usage"", u'DB Analyst/SW Engineer\nTata Consultancy Services - Bengaluru, Karnataka\nDecember 2015 to July 2016\n\u2022 Created smart forms and maintained a restaurant repository using ABAP tools\n\u2022 Enhanced client conversion by 25% by segregating customers using SQL based on their issues\n\u2022 Maintained SQL scripts to create and populate tables in the data warehouse for daily reporting across departments\n\u2022 Establishes an on-going process to maintain data quality by creating a weekly program evaluations and visualization reports using Macros']","[u'M.S. in Information Technology and Management', u'B.E. in Computer Engineering']","[u'The University of Texas at Dallas Dallas, TX\nMay 2018', u'Mumbai University\nMay 2015']"
0,https://resumes.indeed.com/resume/efd9c4afd42cfb84,"[u'Integration Engineer\nWageWorks - Irving, TX\nJune 2017 to Present\n\u2022 Implemented a large healthcare data migration project for hundreds of clients.\n\u2022 Extensively worked in Microsoft SQL Server building stored procedures and complex queries\ninvolving joins, pivots, views, and functions.\n\u2022 Generated weekly eligibility report files for thousands of subscribers with a broad range of formats and designs.\n\u2022 Developed Python scripts for parsing data and automating tasks to increase personal efficiency.\n\u2022 Fostered relationships with clients and carriers and served as a liaison for technical and non- technical representatives.', u'Data Manager\nGrand Tex Auto Parts - Stafford, TX\nNovember 2015 to May 2017\nCreated SQL databases and documentation systems to store all business related data including\nsales, inventory, and customer information.\n\u2022 Built dashboards in Power BI to aid in business and logistics decisions.\n\u2022 Designed and implemented efficient processes for all warehouse activities, doubling production in under a year.\n\u2022 Oversaw entire process of online distribution, from receiving inventory to customer delivery.\n\u2022 Solved all IT problems associated with a startup environment and developed new systems from the ground up.']",[u'BS in Geophysics'],"[u'Texas A&M University College Station, TX\nMay 2015']"
0,https://resumes.indeed.com/resume/86ea47dec92a5939,"[u'Senior Data Engineer\nSTAQ - Baltimore, MD\nJanuary 2017 to Present\nBaltimore, MD Data Architecture\nExperience:\n2017 to present\nArchitect and develop large scale data ingestion and reporting frameworks, AWS Services\nsupporting the data and analytics engine of a software as service platform. Report Development\nData Visualization\n\u2022 Designed data ingestion frameworks using Nifi, HDFS, and HAWQ to Project Management\ningest data from over 400 data integrations into optimized Parquet\nLanguages\ntables, serving 1000s of reports per day across 30 TB of data.\nMastery:\n\u2022 Develop data migration processes using Sqoop to move data from two\nSQL\nRedshift clusters and six RDS instances to a 16 node HAWQ Hadoop\nPython\ncluster.\n\u2022 Developed automated data validation and delta identification framework Experience:\nusing Python to ensure data integrity throughout data and system Ruby\nmigration. Scala\n\u2022 Develop code to dynamically generate, coordinate, and execute SQL to Javascript\nsupport user driven data loading, aggregation, and reporting using HAWQ JSON\ndatabases. Axon\n\u2022 Architect data replication and backup processes across Hadoop clusters to provide active and passive environments to provide platform failover. Technologies', u""Senior Data Engineer / WebbMason Analytics\nMastery - Baltimore, MD\nJanuary 2017 to January 2017\nto 2017 SQL Server/PostgreSQL\nArchitect and implement big data analytics platforms, with a strong Git/Subversion\nemphasis on AWS, supporting enterprise analytics that provide actionable\nPandas\ninformation.\nSqlAlchemy\nJira\n\u2022 Develop data ingestion and migrations frameworks to move clients' data\nExperience: from traditional data architectures to Hadoop/Big Data distributed\nHadoop Stack\narchitectures, using tools like AWS Data Migration Service, Sqoop,\nPython, and Alteryx. Maintain and extend these migrations. NoSQL\n\u2022 Develop Spark programs to perform logical transformations and analysis Spark of billions of records of data across 40 source systems, providing AWS EMR\nvalidation for source system ingestion.\nAWS Glue\n\u2022 Design and iteratively develop an integrated semantic layer implemented AWS Data Pipeline in Impala to support hundreds of enterprise reports across multiple AWS RDS\nbusiness units.\nAWS Lambda\n\u2022 Design and implement data visualizations and dashboards using Tableau and AWS Quick Sight. AWS Redshift\n\u2022 Designed, implemented, and maintain a report framework implemented AWS Athena\nusing Python, AWS Data Pipeline, Lamdba, and RDS services for Ab Initio\nautomated report generation and delivery of over 200 reports per day, Alteryx\nsupporting internal business operations."", u'Data Engineer\nAtSite, Inc - Washington, DC\nJanuary 2014 to January 2017\nDesigned and implemented end-to-end data solutions supporting insights and actions of 500 people across 800 facilities providing facility systems\noptimization.\n\n\u2022 Design and develop data integration and acquisition solutions for building\nsensor data for 800 facilities with over 40 million records per day using\nPython and SQL, allowing for operational monitoring and analysis of real-time building information using relational and NoSQL data stores.\n\u2022 Develop data warehousing for analytical application and reporting\nservices.\n\u2022 Manage, extend, and maintain data models as new data sources emerge and requirements evolve.\n\u2022 Develop algorithms to identify faults and patterns in building systems to provide energy efficiency and savings, resulting in millions of dollars of\nidentified savings.\n\u2022 Design and develop semantic and access data layers serving over 300\nreports per day via a web platform and business intelligence tools.\n\u2022 Develop data validation and anomaly identification routines for master,\nreference, and transactional data.\n\u2022 Work under agile development process using tools like Jira and Trello to support iterative development to manage, extend, and maintain data\nsystems and integrations.', u'Senior Consultant\nBooz Allen Hamiltion - McLean, VA\nJanuary 2012 to January 2014\nDesigned and developed data integration strategies, logical data models,\ncommon vocabularies, metadata management methods, queries, and reports for the implementation of an Enterprise Resource Planning system.\n\n\u2022 Worked side by side with clients and subject matter experts to integrate over 20 data systems into one Enterprise Resource Planning system.\n\u2022 Conducted data profiling using SQL and varying databases, such as\nOracle and MYSQL.\n\u2022 Developed data standards used to organize and define business and functional requirements, including integrated common vocabularies.\n\u2022 Developed metadata management strategies and logical data models.\n\u2022 Performed conceptual analysis and reporting on data elements, business\nterms, business processes, and business rules to identify redundancy,\nquantify relevance, and satisfy business logic.', u'Data Analyst\nRevelytix, Inc - Baltimore, MD\nJanuary 2009 to January 2012\nDeveloped and implemented graph based and big data solutions to solve\nclient data integration and analysis requirements.\n\u2022 Developed data integration solutions for varying data types and forms,\nleveraging both relational and semantic technologies.\n\u2022 Implemented and administered Hadoop clusters for data analysis and software testing.\n\u2022 Designed metadata repositories for use in data integration to provide\nlineage from source to target system.\n\u2022 Designed and performed software test plans and developed standard and ad hoc testing reports.\n\u2022 Developed and implemented queries, materialized views, reports,\nontologies, and relational to RDF mappings.']",[u'BA in advanced logics'],[u'University of Maryland\nJanuary 2004 to January 2008']
0,https://resumes.indeed.com/resume/cecc5a1680b3e524,"[u""Mud Engineer\nDecember 2012 to June 2015\nThis company is active in drilling fluids service established to succeed MI SWACO after ending of it's it's contract in Sudan my roles as mud engineer are\n\u2022 Provides onsite services under general supervision by testing, measuring and supervising the operation of fluid pumping and mixing.\n\u2022 Maintaining fluid properties on rig sites by accurately testing the properties of the fluid.\n\u2022 Provides constant support to aid operations in providing clients the most efficient,\nenvironmentally safe drilling fluids that are available.\n\u2022 Maintains the inventory at customer Well sites.\n\u2022 Accountable for creating Mud reports, ordering products for any treatments, and keeping field\nrelationships."", u""Data Engineer\nChina National Logging Corporation\nMay 2010 to September 2012\nThis company is specialized in Mud Logging services my scopes of job are\n\u2022 Operating and maintaining advanced real time data acquisition, analysis, monitoring and display\nsystem.\n\u2022 Executes the maintenance and calibration of sensors and gas equipment\n\u2022 Keep contact with both of geological and drilling supervisor.\n\u2022 Is aware of, and respect all safety regulations and procedures, as specified by the Client, or relevant\nsafety authorities.\n\nJul 2006-May 2010 Mud Logger (CNLC) I was responsible for \u2022 Clean up and prepare samples before examination Hydrocarbon.\n\u2022 Perform various chemical tests on samples and examine samples under a microscope when necessary. Check for hydrocarbon fluorescence. Record all test results.\n\u2022 check drilling lag time, geological parameters, and report all questionable samples and data.\n\u2022 Ensures the design of an accurate and comprehensive Master Log according to the client's format.\n\u2022 Keep all logging equipment and instruments clean and in proper working order\n\u2022 Make sure all scheduled equipment maintenance takes place as necessary, and communicate all\nequipment issues to the necessary personnel\n\u2022 Prepare both of daily geological report and final well report.\n\u2022 Is aware of, and respect all safety regulations and procedures, as specified by the Client, or relevant\nsafety authorities.\nNov 2005-Jul 2006 Sample Catcher (CNLC) I was responsible for\n\u2022 Collect drilling samples from various depths during the ongoing drilling operations.\n\u2022 Assists in core recovery and core description when required.\n\u2022 Updates the End of Well Report section concerning the sample collection, dispatch and lithological\ndescription.\n\u2022 Is aware of, and respect all safety regulations and procedures, as specified by the Client, or relevant\nsafety authorities."", u'Geologist\nGeological Research Authority of Sudan I\nOctober 2004 to October 2005\ninvolve in\n\u2022 Revised geological Maps.\n\u2022 Outline mineralogical out crop of some exploration sites.']","[u'B.SC in Geology and Chemistry', u'CERTIFICATE', u'']","[u'Khartoum University\nJune 1999 to October 2003', u'Khartoum University', u'UNICCO Institution']"
0,https://resumes.indeed.com/resume/e61c3493c63222f5,"[u'Director / Data Engineer\nInvestorPlace Media - Rockville, MD\nFebruary 2002 to Present\nLed multi-year project to create a core data warehouse application that drives all aspects of business development\n\n\u2022 Worked with marketing and finance to understand existing process, reports and systems and future needs and defined critical business architecture\n\u2022 Worked with marketers to define business processes, KPIs, formulas and data flow policies\n\u2022 Collect and aggregate user data touch points from various sources (web logs, email marketing, CRM, GA, Facebook,\nSEO vendors, Omniture, Unica etc) into one simple platform\n\u2022 Spearheaded design, development, implementation and maintenance of data warehouse and data marts. DW\nconcepts such as star & snowflake schema, slowly changing dimensions, summary tables applied\n\u2022 ETL development with SAP BusinessObjects DI, SSIS, DTS, C#, and Java\n\u2022 Designed and developed BusinessObjects (BOBJ) Universe\n\u2022 Built reports in BOBJ WebI, Crystal reports, SSRS, PowerPivot and exposure to Tableau\n\u2022 Built C# components for SSIS, CLR stored procedures and SQL functions\n\u2022 Maintained partitioned tables, views, and stored procedures.\n\u2022 Performed query optimization to improve production reports with data analysis\n\u2022 Design and developed customer lifetime value analysis (LTV) data mart in Spark/Scala/Zeppelin. Heavily used\nwindow functions and Scala functions\n\u2022 Installed and maintained Spark standalone cluster in Azure VM\n\u2022 Performed Spark task optimization to improve job performance\n\u2022 Implemented Hadoop cluster in HDInsight (hosted in Azure) before replacing it with Azure Data Lake and USQL\n\u2022 Involved in two large OLTP database migrations\n\u2022 Prepared data mapping specification documentation for OLTP data migration\n\u2022 Designed and developed ETL code in staging SQL Azure database for data migration\n\u2022 Designed and developed ETL functions in Scala/Spark for OLTP data migration\n\u2022 Built email preference data mart based on specification. Developed workflow logic and policies in .NET WF/C#\n\u2022 Designed and developed bi-directional Rest API for e-letter signups, e-letter preferences and confirmed opt-in process in C#, SQL Azure\n\u2022 Built LOB apps in C# with DevExpress controls within VS2015 and SQL Server\n\u2022 Develop Web Application in ASP.NET with Telerik Controls and SQL Server\n\u2022 Implemented replication from publisher to subscriber in SQL Server 2000\n\u2022 Built Rest API that can handle large blob post request and persist in Azure Data Lake\n\u2022 Performed user management, database backup and restore, instance creation\n\u2022 Dealt with various data warehouse changes and issues.\n\u2022 Maintained Think Subscription application and database. Built logics to consolidate subscriber accounts, addresses and payment accounts in C# via Soap API\n\u2022 Built an e-commerce content catalog archive application using .net and MongoDb\n\u2022 Built rest API to track matric threshold. Easily accessible from pipeline jobs', u""Software Engineer II\nDigex - Beltsville, MD\nJanuary 2001 to January 2002\nDeveloped and managed IT group's software development and database application administration. Determined software\nrequirements and designed business logic, security, and application solutions. Compiled workflows and requirements from cross-functional teams into a single database application that allowed tier one operations engineers to build and manage\ncustomer virtual network and security firewalls.\n\n\u2022 Developed and implemented a customized logical network and equipment discovery/configuration system.\n\u2022 Derived logical database design in ERwin. Used Visio and custom topology tools to discover networks. Performed ETL\ntask in Perl/Unix scripting\n\u2022 Designed and developed application in ASP.NET, C#, and Sql Server\n\u2022 Develop heavily used JavaScript interfaces that implemented business logic\n\u2022 Lead Oracle DBA team to support PeopleSoft and Siebel applications. Maintained Oracle database replication, stored\nprocedure, views and functions."", u""Developer/Oracle DBA\nPermitsNOW.com - Rockville, MD\nJanuary 2000 to January 2001\nHands-on development and oversaw technology project development for dot com startup that enabled request and approval of construction permits\n\n\u2022 Led full-scale redesign and re-launch of the company's web portal that provided the foundation architecture for business\noperations. Designed and implemented a quick, efficient way to build forms based on analysis of existing municipal\nforms\n\u2022 Built and maintained functions to store & query thousands of blobs in Java, PL/SQL and Oracle8\n\nPrevious role: Oracle DBA/Developer at Phillips International. Maintained Database, Performance tuning, RMAN backups,\nPerl and Unix scripting. Application Developer and Informix Database Administrator for Acterna / Telecommunication\nTechniques Corporation. engineered a 360 sales application that allowed a comprehensive view of account information and facilitated cross-sells, up-sells, account reviews, and telemarketing note previews. VB6, Lotus Notes, Power Builder""]","[u'', u'B.S. in Information Systems']","[u'University training', u'University of Maryland, University College']"
0,https://resumes.indeed.com/resume/488090ee6e30e7d7,"[u'Data Analyst\nAT&T - Los Angeles, CA\nNovember 2017 to Present\n\u2022 Develop, disseminate and present reports to internal stakeholders on pricing topics to include: brand, customer and product segments, competitive activity, distribution channel performance and market share\n\u2022 Measure and monitor KPIs for benchmarking of key revenue initiatives and projects through generating dashboards in Power BI by leveraging transaction, customer and website level data\n\u2022 Helped analytical team in Providing analytical, data support and customer churn prediction using python.\n\u2022 Preparing data for analytics through queries in Teradata, Hive and Vertica.', u""Senior Software Engineer\nCapgemini India - Mumbai, Maharashtra\nJuly 2013 to July 2016\n\u2022 Working with over 3 Million data records using (Python/R): facts, figures, and number crunching to find meaningful conclusions\n\u2022 Clustered 240k+ customer data using K-means algorithm to identify potential customers for email marketing\n\u2022 Used statistical techniques for hypothesis testing to validate data and interpretations\n\u2022 Communicating with clients to gather their requirements and presenting them with meaningful results\n\u2022 Presented findings (failure of the client to fulfill compliances to complete the On-boarding process) and data to client to improve strategies (compliance's which are time consuming and trivial) and operations for sales. Hence reducing the on-boarding process time by 26%, making the business process completion faster.\n\u2022 Consolidated Data using Python to perform data scrubbing to eliminate redundancies and outliers, enhancing the data integrity by 7%\n\u2022 Improved demand forecasting that reduced backorders to retail partners by 19%.\n\u2022 Queried data using SQL and cleaned it using python for reporting purpose.\n\u2022 Predicted revenue data by creating automated and interactive dashboards and visualizations using Tableau to identify key metrics"", u'Data Analyst\nLoad Infotech - Jaipur, Rajasthan\nJune 2012 to June 2013\n\u2022 Analyzed 14 GB data using Python, SQL and MS Excel to identify and monitor fraud pattern in the e-commerce transactional history data\n\u2022 Identified trends and opportunities to improve policies, processes and procedures that estimated in 30% revenue growth\n\u2022 Performed analysis and mining business data to identify patterns and correlations among the various data points.\n\u2022 Performed statistical analysis of business data.\n\u2022 Responsible for analyzing marketing/merchandising data to look for patterns, topics and improvement opportunities.\n\u2022 Manipulating and transforming various datasets into the data required for analysis.\n\u2022 Provide data-driven insights to optimize marketing campaigns, monitor campaign performance and build predictive models that improve campaign efficiencies\n\u2022 Translated business user concepts and ideas into technical business solution development and developed business process\n\u2022 Designed and create data reports that helped business executives in their decision making using Qlikview.', u'Data Intern\nAUR - Jaipur, Rajasthan\nJanuary 2011 to May 2012\nAmity University Rajasthan is one of the biggest university of India. Having 100000+ students in all locations in India, while having 130+ courses. Our team responsibility was to maintain a database for all student records and making easy for admins to fetch records and email updates and results of students electronically to their parents\n\u2022 Worked with different teams to gain insights about the data concepts behind their business.\n\u2022 Research, update, and validate data underlying spreadsheet production; strategically fill gaps.\n\u2022 Create pivot tables and modify spreadsheets for individual campus\n\u2022 Wrote simple and advanced SQL queries for extracting data and created dashboard and stories for senior managers.\n\u2022 Ensured best practices are applied and integrity of data is maintained through security, documentation, and change management\n\nAcademic Project Experience (Florida International University)']","[u'M.S. in Information Systems', u'B.Tech. in E&C']","[u'Florida International University Miami, FL\nOctober 2016 to October 2017', u'Amity University Rajasthan Jaipur, Rajasthan\nAugust 2008 to August 2012']"
0,https://resumes.indeed.com/resume/a0cb586af6984ba0,"[u""Logistics Engineer\nIntegra Technologies LLC\nMarch 2017 to Present\nChampion Lean engineering goals. Includes being an aid in developing, implementing and maintaining a Lean culture within the organization. Actively coach the organization for maintaining a Lean environment.\n\u25c6 Ensures the performance, condition, and reliability of all plumbing, mechanical and electrical equipment to ensure efficient operation of all equipment.\n\u25c6 Develops, evaluates, and maintains Preventative Maintenance Program. Develops, implements, and maintains written maintenance policies and procedures. Assures that outside services are properly completed/supervised in accordance with contracts/work orders.\n\u25c6 Provides fiscal management for unit operations to ensure proper utilization of financial resources. Prepares and plans unit's operating and capital budget, and forecasts needs of the unit.\n\u25c6 Schedules required major equipment purchases, remodeling, and special projects, as assigned.\n\u25c6 Provide Leadership and Support to Continuous Improvement initiatives at the plant level including work studies to analyze work systems and develop standards, allowances and/or improvements to existing systems, providing input for the development of functional layouts that minimize space, material transit distance and provide flexibility for expansion, and utilize Simulation software to analyze and/or improve work systems.\n\u25c6 Assess ergonomic factors in work systems and provide solutions/design factors to minimize RMI (Repetitive Motion Injuries) and accommodate the functional range of workers.\n\u25c6 Utilize Integra Lean Standards in everyday assignments and analysis including: Line Balance, PFEP, Value Stream Maps, Engineering Studies, and Resource Requirement Sheets\n\u25c6 Provide input for the development of Process FMEA's and Control Plans for existing and potential work systems.\n\u25c6 Champion Lean Awareness as necessary STD Work, 5s, TPM, JIT, and Mistake Proofing.\n\u25c6 Conform to the processes and requirements of the synchronized management system."", u'Industrial Data Analyst\nLVR Enterprises\nMay 2011 to June 2015\nCollects data and performs statistical analysis\n\u25c6 Maps and documents processes Recommends and implements process improvement\n\u25c6 Applies company methodologies and tools to design transportation operations\n\u25c6 Establishes performance measures\n\u25c6 Assists in communications with internal and external customers to understand business requirements\n\u25c6 Supports business development and helps create efficient designs and solutions processes\n\u25c6 Determines efficient utilization of resources by analyzing driver and equipment utilization charts\n\u25c6 Assists the Operations Managers to implement efficient and competitive solutions\n\u25c6 Applies various Lean Six Sigma (LSS) tools']","[u""Bachelor's Degree in Industrial Engineering"", u'Master of Science in Information in systems']","[u'Gitam University Visakhapatnam, Andhra Pradesh\nApril 2015', u'Stratford University Falls Church, VA']"
0,https://resumes.indeed.com/resume/3c1a85a2a853e729,"[u'Application Support Engineer\nGazprombank - \u041c\u043e\u0441\u043a\u0432\u0430\nJune 2009 to June 2016\n* Provided technical support, reproduce customer bug scenarios, and develop solutions for the customer and their environment.\n* Shared knowledge and experience with customers and team members through documentation, knowledge base authoring, mentoring, and consulting.\n* Implemented quality controlling tools for daily generated financial and information reports.\n* Set up the process of corporate technical support using Mantis bug-tracking system.\n* Provided functional, load and failure back-end modules testing using Unix and PowerShell scripts and database stored procedures.\n* Monitored database performance by analyzing information provided by various monitoring tools to ensure both efficient and effective operation.\n* Acted as a consultant for operational, development and analytical teams and B2B-customers to resolve technical issues and improve services reliability.', u'Data exchange coordinator\nGazprombank - \u041c\u043e\u0441\u043a\u0432\u0430\nJune 2008 to June 2009\n* Implemented card processing back-end migration modules. Outcome: data center processing increased from 0.5 M to 2 M financial transactions daily.\n* Developed and maintained automated disputes resolution modules with PL/SQL, Unix-shell, and Visual Basic. It allowed delegating simplified operations to customer support specialists.\n* Established card processing control and monitoring system (batch data processing workflow) with the use of Bash and Perl scripts plus PL/SQL function/procedures.']",[u'BS'],[u'State University of Management \u041c\u043e\u0441\u043a\u0432\u0430\nJanuary 1994 to January 1999']
0,https://resumes.indeed.com/resume/2f2e3c2672b4165d,"[u""Senior Telecom Engineer\nSamsung Electronics America - Worldlink, Inc. - Frisco, TX\nDecember 2011 to Present\nCarriers - Verizon, Sprint, Metro PCS\n\n\nProject : 4G LTE Small Cell Solutions for Verizon (January ,2016 - Present)\n\n\nActively engage with Verizon teams, Service Performance Engineers and end customer in remote troubleshooting of 4G Network extender - efemto deployment and call processing issues.\n\u2022 troubleshoot LTE call processing issues setting up S1AP, call traces and Femtocell deployment issues\n\u2022 Recommend IP and Firewall settings on the end customer's network (router, switches and firewalls) to bring Network extender in service.\n\u2022 Analyze eccb/rrc and pdcb debug logs from the efemto for troubleshooting low throughput, X2/S1 (HO/HI) issues.\n\u2022 Collect and analyze packet capture from the Femto Gateway for signaling between efemto (Virtual e node B)and the MME\n\u2022 Analyze KPI reports and look for areas of improvement; focusing on worst offenders on the network\n\u2022 Proactively perform health checks on Oracle servers in the network\n\u2022 Assist FOA team with testing of different modules of the efemto and sfemto, which includes collecting debug logs, performing call tests under varying network conditions using multiple VoLTE capable hand sets and QXDM.\n\n\n\n\nProject : 3G and 4G National Deployment for Sprint (December ,2011 - January ,2016)\n\n\u2022Proactively look for areas to improve LTE network performance and to meet network KPIs.\n\u2022Responsible for Load balancing parameter changes during Special Event Monitoring to avoid congestion and to improve capacity of the\nNetwork.\n\u2022Troubleshoot; restore service on single/multiple, sites/sectors in the commercial network for 1X, EVDO and LTE in multiple markets.\n\u2022 Troubleshooting worst offenders for Accessibility, Retainabilty, Mobility and Integrity of the Network on daily basis.\n\u2022 Provide initial analysis on call drops, access failures, blocking and HO issues on a site/network and recommend changes on 1x voice, EVDO and LTE parameters, as needed\n\u2022 Implementation of new features, tuning LTE radio network parameters and perform Multi-Layer (carrier) management\n\u2022 Create batch files for market wide network parameter changes and implement neighbor lists.\n\u2022 Scrub neighbor lists for missing neighbors- SHO, HHO, IVHHO, eCSFB and eHRPD\n\u2022 Implement market wide neighbor lists on the commercial network.\n\u2022 Perform market wide 1X Voice, EVDO and LTE parameter, firmware and software version upgrades, audits and fix discrepancies that lead to network impact.\n\u2022 Assess channel element and backhaul utilization trends for capacity issues on sites\n\u2022 Determine IP connectivity, backhaul utilization on the network using unix on Cisco IP Aggregator, Cisco Cell Site Routers and Dragonwave Microwave sites\n\u2022 Coordinate troubleshooting sessions and work with Customers, RF Engineers, Tier II /Tier III System Engineers and Field Operations.\n\u2022 Collect call traces for troubleshooting sessions from call/resource control processors to check for call failure reasons.\n\u2022 Load balancing parameter changes during Special Event Monitoring to avoid congestion and to improve capacity of the network.\n\u2022 Correlate Core, RAN and IPA outages to KPI degradation for BSC.\n\u2022 Commissioned and remotely supported integration of enBs, BTS and DAS in Sprint and Metro PCS markets.\n\u2022 Provide remote technical support during migration to new multi-modal base station (3G) which includes but is not limited to Acceptance Tests and Call Tests on the site, isolating and troubleshooting equipment/network specific issues"", u""Associate RF Engineer\nFusion Solutions, Inc\nSeptember 2011 to December 2011\nCarrier - AT & T\nClient - Global Wireless Solutions /AT & T Network)\n\n\u2022 Perform data analysis using Mapinfo and in house tools for site estimation for AT&T 's benchmarking project\n\u2022 Identify site locations in the market, based on individual market analysis (bands and channels, market PN plan as well as RF parameters, using Mapinfo.\n\u2022 Deliver site estimation reports to AT&T including number of sites, percentage of each band used in each technology and channels used in the market during the test drive.\n\u2022 Worked efficiently under highly competitive atmosphere with tight deadlines."", u""RF Data Collector\nQuadGen Wireless Solutions\nMarch 2011 to September 2011\nClient: Alcatel Lucent)\n\u2022 Perform Drive Test Investigation for AT&T's UMTS network using Alcatel Lucent's Agilent E6474A, thereby including network and cluster performance drives in Pennsylvania, Washington DC, Virginia, Atlantic City, Michigan and Indiana.\n\u2022 Conduct Baseline, Green cluster drives; Single Site Verifications; IFHO, IRAT drives.\n\u2022 Collect UE Physical measurements (SC, RSCP and Ec/No) for all the cells in the active set.\n\u2022 Check if the site is transmitting the desired UARFCN, and verify the biasing of all sectors.\n\u2022 Analyze incoming L3 messages on Agilent tool - monitoring measurements, active/monitoring/detected sets.\n\u2022 Co ordinate with Lucent Engineers, report overshooting sites, unbiased sites, probability of missing neighbor, UE sensitivity issues and terrain issues during drive.\n\u2022 Check for drops and maintain a timeline of PS, CS drops files.\n\u2022 Troubleshoot hardware as needed.\n\u2022 Maintain functionality, store and insure inventory of all equipments. Perform data transfers using various methods.""]","[u'M.S in Electrical Engineering', u'B.E in Electronics and Communication']","[u'University of North Texas Denton, TX\nDecember 2010', u'Osmania University Hyderabad, Andhra Pradesh\nMay 2008']"
0,https://resumes.indeed.com/resume/93e1a297e3cd974e,"[u'GMS Tutor\nGeneral Math Studies\nJuly 2017 to Present\n\u2022 Tutored students in the subject of mathematics and solved their queries in-class or in the laboratory.\n\u2022 Assisted professors in grading as well as distribution of tests.\n\u2022 Gathered data from students and set them in an order for future references.', u'Software Engineer/Data Analyst\nCross-Tab Marketing Services\nMay 2014 to March 2016\n\u2022 Filtered Data, reviewed computer reports and performance indicators to locate and correct data and code problems.\n\u2022 Worked on and implemented requests by clients on data analysis using in-house and commercial software.\n\u2022 Acquired data from primary or secondary data sources and maintained databases/data systems.', u""INTERNSHIP\nBhabha Atomic Research Center\nJune 2012 to July 2012\nInternship on 'Visualization of complex exponential function using stereo display' in reactor control division, BARC (Bhabha Atomic Research Center). 3D mathematical programming was used to achieve this result.""]","[u'Master of Science in Computer Networks', u'Bachelor of Engineering in Electronics and Tele-Communications']","[u'San Diego State University San Diego, CA\nMay 2018', u'University of Mumbai\nAugust 2013']"
0,https://resumes.indeed.com/resume/b369e7641c51b9a5,"[u'Data Sceinetist\nAmerican Family Insurance - Chicago, IL\nFebruary 2016 to Present\n\u2022 Build highly scalable deep learning models for Image classification, object detection and segmentation.\n\u2022 Reproduce and validate state-of-art Deep Learning models from papers and publications.\n\u2022 Handle large image datasets, build classification models using machine learning / Deep learning algorithms.\n\u2022 Optimizing machine learning and Deep learning algorithms.\n\u2022 Generate performance metrics to evaluate different classifier models.\n\u2022 Performs Embedded Software testing, Regression and Functional testing for developed applications.\n\u2022 Implemented using Lua, Torch, Python, Caffe, Matlab in Linux environment.', u'Research Assistant\nUniversity of Texas at Tyler - Tyler, TX\nSeptember 2013 to Present\nI am currently doing my thesis project under the guidance of Dr. Melvin Robinson. The project is ""License Plate recognition using Deep learning techniques ""\n\n\u2022 Build a convolution neural network for recognition of information from the license plate.\n\u2022 Developed a hybrid segmentation technique to improve the accuracy.\n\u2022 Achieved competitive results compared to conventional learning techniques.\n\u2022 Trained using supervised learning on GTX 980M graphic card.\n\u2022 Implemented using caffe, OpenCV, Matlab, c/c++, python.\n\nSkills Used\nC/C++, Matlab, OpenCV, Python, Numpy, Caffe', u'Systems Engineer\nTATA Consultancy Services Ltd - Hyderabad, Andhra Pradesh\nJuly 2011 to December 2013\nResponsibilities:\n\u2022 Handle the test targets from the scrum in discussion with the project lead and completing work within the stipulated time.\n\u2022 Part of Agile software development team and was involved in on-time product release.\n\u2022 Performed System Verification Testing (Quality Analysis) and Cluster Administration for a project called Flexi Platform (Client: Nokia Siemens Networks) on Release Basis.\n\u2022 Involved in Embedded Software Testing, Firmware testing, Regression Testing, Functional Testing, Pronto testing, automating test cases, manual testing, Hardware support and Troubleshooting Hardware issues.\n\nKey Practices in project\nAgile Methodology, Embedded Software Testing, Regression Testing , Functional Testing , Pronto testing, Test planning, Automating test cases, Manual testing.\n\nSkills used\nLinux environment, C++, Unix shell scripting, Quality Center.', u'Internship\nElectronics Corporation of India - Hyderabad, Andhra Pradesh\nMay 2010 to June 2010\nResponsibilities\nUnderwent an intern program in may 2010 at \u201cElectronics Corporation of India\u201d, Hyderabad in the field of Embedded Systems and done a project on \u201c Cell Phone Operated Land Rover\u201d.\n\n\nSkills Used\nC++, Keil complier, P89v51RD2, Proteus software.']","[u'Masters in Electrical Engineering', u'Bachelors of Technology in Electronics and Communication Engineering']","[u'University of Texas at Tyler Tyler, TX\nJanuary 2014 to January 2015', u'GITAM University Visakhapatnam, Andhra Pradesh\nJanuary 2011']"
0,https://resumes.indeed.com/resume/97187f48c764e3f6,"[u'Data Analyst\nRoostify Inc. - San Francisco, CA\nJune 2017 to December 2017\n\u2022 Collaborated with a team of product managers to create new leads and opportunities to be implemented in Q1 2018.\n\u2022 Improved data quality for data mining from various Real estate public records, Salesforce, and other database repositories using Python scripting language and Excel based on Roostify\u2019s specifications.\n\u2022 Implemented Dynamic Regression forecasting model with an accuracy of 79% for product\u2019s sales for Q4 2017.\n\u2022 Provided analytical solutions using PowerBI which was successfully implemented during business expansion.', u'Software Engineer\nBhabha Atomic Research Centre - Mumbai, Maharashtra\nJuly 2014 to December 2015\n\uf0a7 Created an encrypted SSL Certificate for BARC to protect communication channel of their internal networks. This helped secure internal servers with an efficiency of over 95%\n\uf0a7 Maintained databases by providing access to different groups of users in the company using PostgreSQL.']","[u'Master of Science in Management Information Systems in Data science', u'Bachelor of Engineering in Information Science & Engineering']","[u'California State University\nJanuary 2016 to January 2018', u'New Horizon College of Engineering, Visvesvaraya Technological University\nJune 2015']"
0,https://resumes.indeed.com/resume/6ddf0fb8742bea57,"[u""Data Analyst/ Software Engineer\nUtah State University - Logan, UT\nJanuary 2017 to August 2017\n\u2022 Worked as a Research Assistant with Dr. Colby Tofel-Grehl in collaboration with NSA in creating an App called 'STITCH'.\n\u2022 Performed data analysis which needed cleaning of humungous data received from schools all over US, used machine learning techniques like Na\xefve Bayes,\nRandom Forest and Decision Tree Classification to accurately show where students lack their knowledge and how to increase their interest.\n\u2022 Created an Android App which had all basic information about electronic components and various projects using it. This App is deployed in Google Play Store\nwhich is accessible to all students and teachers."", u""Data Scientist\nCapgemini India Pvt. Ltd - Bengaluru, Karnataka\nAugust 2015 to July 2016\nIndia\n\u2022 Worked with a team size of 60 members for AT&T client where we followed Hadoop, Spark, Hive and Spark MLLib technologies for database development.\n\u2022 Worked in Hadoop Management System that handles creation of clusters, live activity of clusters, number of resources needed for technologies like Spark and Hive.\n\u2022 Handled huge chunks of incoming data by using Spark Streaming and discretizing it into RDD's.\n\u2022 For better and faster response, collaborated Spark with HIVE giving us a massive performance boost.\n\u2022 To provide meaningful insights to the client, we used Spark MLLib and Tableau for providing reliable options out of all available services provided by AT&T.""]","[u""Master's in Computer Science in Computer Science"", u'in Computer Engineering', u'']","[u'Utah State University\nAugust 2016 to Present', u'Mumbai University\nJanuary 2011 to January 2015', u'Education Centre']"
0,https://resumes.indeed.com/resume/492c88bdd9e12f40,"[u""Automation Test Engineer\nDIRECTV/AT&T\nJanuary 2017 to January 2018\nResponsible for testing one of the largest consumer products for streaming television (ATT&T's Set Top Box and AT&T's mobile application).\n\u2022 Performed automation test scripts for RESTful services, Set-Top Box components (MEF, Informer, Cirrus) and Android applications to ensure the product is bug friendly to customers.\n\u2022 Analyzed business requirements and software specifications to create automated test cases.\n\u2022 Documented all automation test scripts in QMetry to ensure the automation test scripts can be run in the CICD pipeline.\n\u2022 Performed feature and regression testing using Appium.\n\u2022 Documented software bugs using Jira bug tracking system.\n\u2022 Collaborated with a scrum team of 7 developers and 2 testers to successfully complete stories within a 2-week sprint."", u""Test Lead\nApril 2015 to January 2017\nTest lead for multiple new features to improve customer's experience with the DirecTV 's Set-Top Box and Genie Go live stream for android and iOS.\n\u2022 Reviewed specifications and requirements documents to create manual feature and regression test cases.\n\u2022 Created test plan documents to describe the test scopes, approaches, resources and schedules for each test cycle.\n\u2022 Managed internal and offshore team's feature, regression, smoke, and stress test results through Jira.\n\u2022 Attended sign-off meetings with integration managers, developers and STB leads to get the final approval for features to be released to production."", u'Data Analyst\nSPAWAR System Center\nAugust 2013 to July 2014\nSupported latest Type 1 Encryption device called HAIPE (High Assurance Internet Protocol Encryption) to allow our clients to have secure communications in the presence of the third party.\n\u2022 Designed, updated and reconfigured HAIPE test software that follows with the HAIPE specifications and requirements.']",[u'Bachelor of Science in Computer Science'],[u'San Diego State University\nDecember 2013']
0,https://resumes.indeed.com/resume/ec20b070bd6c1508,"[u'Big Data Development Engineer, full-time\nThe 55th China Electronics Technology Group Institute\nFebruary 2017 to August 2017\n\u2022 Worked on the Cloud Computing R&D Department\n\u2022 Used Java, Linux Shell, Hadoop, SQL and Hive to develop back-end of several systems\n\u2022 Completed a job analysis project using WebCrawler and text-clustering algorithm\n\u2022 Gained experience in data collecting, cleaning, analyzing and visualizing\n\u2022 Designed national cloud computing competition test for higher vocational colleges', u'Performance Tuning of Distributed Computing on SPARK\nDecember 2014 to May 2015\n\u2022 Aimed to derive a generalized scheduling scheme to minimize the completion time of a task\n\u2022 Practice in implementing simulations on SPARK']","[u'M.S. in Statistics in Statistics', u'Bachelor of Science in Statistics in computational mathematics', u'in Statistics']","[u'University of Washington Seattle, WA\nSeptember 2017 to Present', u'Nanjing University Nanjing, CN\nJanuary 2012 to January 2016', u'University of California Davis, CA\nSeptember 2015 to December 2015']"
0,https://resumes.indeed.com/resume/4ffa820db9fe10de,"[u'Data Visualization Researcher\nNortheastern University - Boston, MA\nJanuary 2018 to Present\nDescription \u2022 Perform initial data wrangling operations to clean data from different sources\n\u2022 Developing interactive visualizations and plotting them into user-friendly reports for Northeastern community to enable evidence-driven decision making and planning\n\u2022 Developing an application using Tableau and R-shiny to generate reports for the qualtrics survey file and response file\n\nProjects', u'Goa, India - Senior Engineer\nVedanta Limited\nJune 2016 to June 2017\nDescription \u2022 Managed maintenance, documentation, asset optimization (AO) and spare Part Management of the plant\n\u2022 Analyzed the spare parts requirement for every essential machinery and made a single order sufficient for the year,\nreducing maintenance cost by 60% ($0.437 million to $0.283 million)']","[u'Masters in Data Analytics Engineering', u'B.Tech in Mechanical Engineering']","[u'Northeastern University Boston, MA\nMay 2019', u'Indian Institute of Technology Bhubaneshwar, Orissa\nJanuary 2012 to January 2016']"
0,https://resumes.indeed.com/resume/c90a6d6368d55a4b,"[u'Data Engineer\nVerizon, HQ\nJanuary 2011 to Present\nJoined as a Regional Engineer responsible for the Data Performance of the wireless Network. Transitioned to a HQ role responsible for the Southeast US.\n\u25cf Created National Tool for data integrity within transport systems\n\u25cf Tier 2 Operational Support for Cisco and Nokia markets including a wide range of routing/switching platforms\n\u25cf MOP / CIQ validation and Engineering contact for technicians and Vendors\n\u25cf Training and knowledge base creation for improved MTTR (Mean Time to Repair)', u'Systems Technician\nVerizon, HQ - Charlotte, NC\nJanuary 1995 to January 2011\nStarted as a field technician and progressed to Senior position in wireless switching offices in Raleigh, NC, and Charlotte, NC.\n\u25cf System Administrator for distribution level equipment in multiple States. Cisco, Juniper, and Nokia exposure\n\u25cf Responsible for multiple projects including site decommissioning and site upgrades in a wide range of scope\n\u25cf Launched advanced technologies including 3G and 4G wireless infrastructure']","[u'MBA in MIS', u'Bachelor of Science']","[u'Bellevue University Omaha, NE', u'Bellevue University Omaha, NE']"
0,https://resumes.indeed.com/resume/b22ce5c7b649b1ea,"[u'Data Engineer II\nWellCare Health Plans - Tampa, FL\nJanuary 2017 to Present\n\u2022 Lead premium team with 4 members, provided and supported Tableau and Cognos for Financial team\n\u2022 Provided production support for python & Hive based iEngine which is processing 12 types of file\n\u2022 Customized ETL processes to load data from different source systems such as Oracle and Flat Files\ninto target systems i.e. Oracle, SQL Server and Flat files\n\u2022 Extensively worked on Informatica Power Center - Source Analyzer, Data warehousing designer, Mapping Designer, Mapplet & Transformations to import source and target definitions into the repository and to build mappings\n\u2022 Created mappings in Informatica to load the data from various sources into Data Warehouse, using different transformations like Joiner, Aggregator, Update Strategy, Rank, Router, Lookup, Sequence Generator, Filter, Sorter, Source Qualifier', u'Data Engineer I\nWellCare Health Plans - Tampa, FL\nJanuary 2015 to January 2017\n\u2022 Used Session parameters, Mapping variable/parameters and created Parameter files for imparting flexible runs of workflows based on changing variable values\n\u2022 Involved in creating Hive table, loading with data, writing hive queries to test the test cases\n\u2022 Received training of Agile Development and participated Testing Projects & Quality Assurance practices\n\u2022 Obtained knowledge of industry lifecycle development methodologies (SDLC)\n\u2022 Strengthened Object-Oriented Design (OOD) skills', u""Software Developer Intern\nPERTH LEADERSHIP INSTITUTE - Gainesville, FL\nJanuary 2014 to January 2015\n\u2022 Redesign website function process for better user experience, increasing user engagements, brand promotion and meeting the changing business requirements\n\u2022 Financial Simulation System Documentation includes User Specification, Internal Specification, Database Specification, Program Specification\n\u2022 Assessed based on intense analysis and study of web analytics report, generated from the website's data gathered""]","[u'Master of Science in Computer Information Systems in ISOM', u'Bachelor of Electrical Engineering in Teaching Assistant', u'']","[u'UNIVERSITY OF FLORIDA Gainesville, FL\nMay 2015', u'YANSHAN UNIVERSITY QINHUANGDAO, CN\nJuly 2010', u'Hough Graduate School of Business']"
0,https://resumes.indeed.com/resume/6c4fa72f1fa947ef,[],[],[]
0,https://resumes.indeed.com/resume/54a5d2f4dd654dde,"[u'Software Engineer\nWells Fargo (Diversant)\nMay 2017 to February 2018\n\u2022 Wrote Java programs that automated applications (web, native, and hybrid) using Selenium, Appium, Perfecto, and Applitools\n\u2022 Wrote a JavaScript program that sent requests to the JIRA API and parsed the response JSON object for data\n\u2022 Wrote a Python program to verify that CSV files had been properly obfuscated\n\u2022 Performed API testing using Postman and internal frameworks\n\u2022 Planned and tracked issues using JIRA\n\u2022 Configured Jenkins to execute test suites and perform error checks\n\u2022 Reported defects to improve quality of Wells Fargo applications', u'Data Analyst\nSilver Spring Networks\nJanuary 2013 to May 2014\n\u2022 Manipulated databases using SQL\n\u2022 Worked in a team of 2 to process data files for electric meters, relays, and access points', u'Data Analyst\nCatapultWorks\nMay 2012 to November 2012\n\u2022 Used SQL to query data from a database containing hundreds of thousands of records and compiled reports for clients\n\u2022 Performed data scrubbing using Excel, Mail Manager, Apex Data Loader, and Map Designer']","[u'B.S. in Computer Science', u'B.A. in Business Economics']","[u'University of California Irvine, CA\nSeptember 2014 to September 2016', u'University of California Santa Barbara, CA\nSeptember 2009 to June 2011']"
0,https://resumes.indeed.com/resume/1293e0a9c8ffead3,"[u""Data Engineer-Hadoop\nWells Fargo - San Leandro, CA\nJune 2017 to Present\nData Engineer-Hadoop, Wells Fargo, San Leandro, CA June 2017 - To-date\n\u2022 Developed spark applications in Python(PySpark) on distributed environment\n\u2022 Worked extensively with Sqoop for importing and exporting the data from HDFS to Relational Database systems and vice-versa\n\u2022 Worked in developing Pig scripts to create the relationship between the data present in the Hadoop cluster\n\u2022 Experience in analyzing data using Hive, HBase and custom Map Reduce program\n\u2022 Extending HIVE and PIG core functionality by using custom User Defined Function's (UDF), User Defined Table-Generating Functions (UDTF) and User Defined Aggregating Functions (UDAF) using python\n\u2022 Worked on Oozie Workflow Engine in running workflow jobs with actions that run Hadoop Map/Reduce and Pig jobs\n\u2022 Created Hive tables, partitions and loaded the data to analyze using Hive QL queries\n\u2022 Worked on Sequence files, Map side joins, Bucketing, Static and Dynamic Partitioning for Hive performance enhancement and storage improvement\n\u2022 Good understanding of ETL tools and how they can be applied in a Big Data environment\n\u2022 Worked on Spark SQL and Spark Streaming\n\u2022 Good knowledge in writing spark application using Python and Scala\n\u2022 Implemented Spark SQL to access hive tables into Spark for faster processing of data\n\u2022 Worked on Spark streaming using Apache Kafka for real time data processing\n\u2022 Experience in creating Kafka producer and Kafka consumer for Spark streaming\n\u2022 Used Hive to do transformations, joins, filter and some pre-aggregations after storing the data to HDFS."", u'Hadoop Developer\nCognizant Technology Solutions - IN\nSeptember 2014 to May 2017\n\u2022 Worked on writing transformer/mapping Map Reduce pipelines using Python and MRJob/Streaming API\n\u2022 Involved in creating Hive Tables, loading with data and wrote Hive queries, which will invoke and run Map Reduce jobs in the backend\n\u2022 Involved in loading data into HBase using HBase Shell, HBase Client API, Pig and Sqoop\n\u2022 Designed and implemented Incremental Imports into Hive tables\n\u2022 Involved in collecting, aggregating and moving data from servers to HDFS using Apache Flume\n\u2022 Written Hive jobs to parse the logs and structure them in tabular format to facilitate effective querying on the log data\n\u2022 Involved in creating Hive tables, loading with data and writing hive queries that will run internally in map reduce way\n\u2022 Migrated ETL jobs to Pig scripts do Transformations, even joins and some pre-aggregations before storing the data onto HDFS\n\u2022 Implemented workflows using Apache Oozie framework to automate tasks\n\u2022 Load data from various data sources into HDFS using Kafka\n\u2022 Sqoop jobs, PIG and Hive scripts were created for data ingestion from relational databases to compare with historical data\n\u2022 Utilized Storm for processing large volume of datasets\n\u2022 Used Pig as ETL tool to do transformations, event joins, filter and some pre-aggregations\n\u2022 Extensively worked with Cloudera Distribution Hadoop and HortonWorks Data Platform\n\u2022 Involved in story-driven agile development methodology and actively participated in daily scrum meetings', u'Data Analyst\nSumTotal Systems - IN\nDecember 2013 to September 2014\n\u2022 Gathered the business requirements from the Business Improvement Team and Subject Matter Experts\n\u2022 Responsible for loading unstructured data into Hadoop File System (HDFS)\n\u2022 Involved in managing and reviewing Hadoop log files\n\u2022 Importing and exporting data into HDFS and Hive using Sqoop\n\u2022 Supported Map Reduce Programs those are running on the cluster\n\u2022 Imported data using Sqoop to load data from RDBMS to HDFS on regular basis\n\u2022 Developed Scripts and Batch Job to schedule various Hadoop Program\n\u2022 Developed PIG-Latin script to generate report\n\u2022 Created jobs to load data from HBase into Data warehouse\n\u2022 Wrote Hive queries for data analysis to meet the business requirements', u'Senior System Engineer\nInfosys Limited\nNovember 2009 to December 2013\n\u2022 Designed and developed data driven Web Forms using ASP.NET, ADO.NET, HTML, JavaScript and CSS technologies\n\u2022 Worked on upgrading the application using AngularJS, HTML, CSS, JavaScript and JQUERY\n\u2022 Implemented code according to coding standards and Created AngularJS Controller, which isolate scopes perform operations\n\u2022 Developed custom directives and Services in AngularJS\n\u2022 Worked on AngularJS, used its two-way data binding to achieve the feedback functionality from the user\n\u2022 Wrote SPA (Single Page Applications) using RESTful web services plus AJAX and AngularJS\n\u2022 Involved in designing and development of REST services\n\u2022 Developed the Master Pages and applied that Master Pages to all Content Pages using ASP.NET\n\u2022 Created User Controls and Custom Controls to enable reusability and used rich server controls to design ASP.NET pages\n\u2022 Used AJAX controls to minimize server round trips to enhance customer experience and to improve application performance\n\u2022 Created MVC controller models and views according to the requirement of client\n\u2022 Created Stored Procedures, Views, Triggers, and Complex T-SQL queries in SQL Server\n\u2022 Testing the code with the production support team\n\u2022 Involved in maintenance and enhancements of an application using Microsoft .NET Framework 4.0 C# .NET, ASP.NET, LINQ, WCF, Web API, AJAX, JavaScript and WEB SERVICES\n\u2022 Performed query optimization and performance tuning for complex SQL queries\n\u2022 Used Team Foundation Server (TFS) for version controlling and assisted in documentation and creating Help files\n\u2022 Coordinated the build/migration of releases to test and production environment']",[u'Bachelors of Engineering in Electronics and Communication in Certifications & Appreciations'],"[u'Dr. MGR University Chennai, Tamil Nadu\nJanuary 2005 to January 2009']"
0,https://resumes.indeed.com/resume/13d9bdb3eab74dd3,"[u'Data Analyst Intern\nAgility Technologies LLC\nJune 2017 to August 2017\n\u2022 Performed Statistical analysis, Statistical modeling, data analysis, data aggregating, data modeling, data mining, data cleaning.\n\n\u2022 Experience with a variety of file formats (XML, JSON, unstructured data, etc.).\n\n\u2022 Derived useful insights from websites, blogs by web scraping data with BeautifulSoup, and analyzed data using NumPy, Pandas, scikit_Learn, Matplotlib python modules.\n\n\u2022 Carried out data Pre-processing, Feature Scaling, Feature extraction, Feature Engineering and built models using predictive modeling techniques like Decision Tree, Random Forest, Naive bayes.\n\n\u2022 Testing the accuracy of model using the test dataset and Visualized results using Tableau.', u'Big Data Developer/Big Data Engineer\nTata Consultancy Services\nJune 2015 to July 2016\nEmployer Name: Tata Consultancy Services (JUNE 2015-JULY2016), Job Title: Big Data Developer/Big Data Engineer']",[u'Master of Science in Information Technology'],"[u'University of North Carolina at Charlotte Charlotte, NC\nAugust 2016 to December 2017']"
0,https://resumes.indeed.com/resume/4dbd93ed8a51bbc6,"[u'Big Data developer\nCGI - Fairfax, VA\nAugust 2016 to December 2017\nProject Type: The project involved developing application services with agility, transformation, and cost efficiencies for their esteemed Red Roof Inn hotel client.\nResponsibilities:\n\u2022 Gathered information and requirements from the users, then documented in BRD, FSD.\n\u2022 Using Kafka to build pipelines from different sources to HDFS.\n\u2022 Written java Map-Reduce programs in AWS EMR to get the semi & un-structured data to structured data and to incorporate all the business transformations.\n\u2022 Developing the process to move the output of map-reduce data to Mark Logic and HBase for analytics\n\u2022 Used Informatica tool for cleaning, enhancing, and protecting the data.\n\u2022 Performed Hive Queries to analyze the data in HDFS and to identify issues.\n\u2022 Worked on shell scripting to automate jobs.\n\u2022 Involved in building Hadoop Cluster.\n\u2022 Configured Hive MetaStore to use Oracle/MySQL database for establishing multiple connections.\n\u2022 Experience in retrieving data from MySQL and Oracle databases into HDFS using Sqoop and ingesting them into HBase for data processing.\n\u2022 Responsible for deriving the new requirements based on business data driven method for ETL applications.\n\u2022 Created Search server using Solr, indexed the files and then queried using HTTP GET calls\n\u2022 Used Oozie & Airflow to schedule automatic workflows in Hadoop Ecosystem.\n\u2022 Writing web scraping programs using python, java\n\u2022 Using PySpark SQL and Streaming for querying and analyzing real time data.\n\u2022 Writing Spark Programs in Python, Scala.\n\u2022 Building PySpark Models by using Machine Learning Algorithms.\nEnvironment: HDFS, MapReduce, Hive, Kafka, Spark 2.1.0, Java, AWS, Python, Scala, MySQL, HBase, Oracle, Sqoop,\nMark Logic, Informatica', u""Software Engineer\nCognisun Technologies\nSeptember 2013 to August 2015\nProject Type: The team is responsible for developing the underlying framework, infrastructure, as well as the algorithms.\nResponsibilities:\n\u2022 Obtain requirements from business SME's, documentation of system requirements, create data models, reporting specifications and test plans/cases.\n\u2022 Develop the custom map-reduce programs to get the semi-structured data to structured data and to incorporate all the business transformations.\n\u2022 Developing the process to move the output of map-reduce data to Mark Logic and HBase for analytics.\n\u2022 Used Informatica tool for cleaning, enhancing, and protecting the data.\n\u2022 Design the new modules to support the market share project\n\u2022 Develop the market share project\n\u2022 Modify the existing process as part of change request or fixing the identified issues.\n\u2022 Participating the re-processing in case of any major changes.\n\u2022 Participated and implemented the security to anonymous the sensitive customer data.\n\u2022 Building data pipelines using Kafka and loading the data into HDFS.\n\u2022 Creating the HBase tables and design HDFS data models to optimize the store.\n\u2022 Data enrichment project data by integrating with registrations or other telemetry data.\n\u2022 Building Spark Applications using Spark SQL, Streaming libraries.\n\u2022 Writing Scala and Python programs to build Spark Models.\n\u2022 Developing web scraping (python) programs to pull the data from retail stores.\n\u2022 Processing the scrapped data to perform the sentiment analysis of a printer and cartridge.\nEnvironment: Hadoop, Hive, Kafka, Spark 1.6.0, Spark SQL, Spark Streaming, Java, Python, Scala, MySQL,\nHBase, Oracle, Informatica.""]","[u'M.S in Computer Engineering', u'B.S in Electronics Engineering']","[u'San Jose State University', u'KL University']"
0,https://resumes.indeed.com/resume/20dfa2a22d4218a2,"[u""Data Scientist\nFixional, INC - Chapel Hill, NC\nSeptember 2017 to October 2017\n\u2022 Fixional Story Recommender: Building a recommender system for Fixional's massive online library of literary fiction pieces consisting of over 500,000 stories. Scraping over a 1000 different sources of content to collect over 100,000 stories for the learner. Also developing an\neffective market segmentation strategy using Google Analytics."", u'Software Engineer\nCognizant Technology Solutions\nDecember 2012 to May 2015\nI have a 29 month experience as a full stack Software Engineer working with C#.NET, and using Oracle for DB and Javascript for frontend development.']","[u'Master of Computer Science in Computer Science', u""Bachelor's in Computer Science""]","[u'North Carolina State University Raleigh, NC\nAugust 2016 to May 2018', u'West Bengal University of Technology Kolkata, West Bengal\nAugust 2008 to May 2012']"
0,https://resumes.indeed.com/resume/e053d4d4321e8100,"[u'Data / Visual Analytics Internship\nACT inc\nJune 2015 to Present\nIntegrate R and Tableau to preprocess raw data and used the transformed data to build predictive models and derive insights from the data.', u""Data Analyst Internship\nHobsons - Cincinnati, OH\nJune 2014 to December 2014\nCollaborated with product managers to understand business objectives and KPI's to provide an advanced and more robust higher education CRM to more than 600 universities in USA\n\u2022 Created interactive Dashboard combining multiple reports using Pentaho BA Server and MDX queries, where each\nclient's usage of different metrics of the Hobsons application is monitored and compared with other like clients and rated accordingly.\n\u2022 The sales team of Hobsons uses this dashboard during Client contract renewals and based on the ratings of the\nClients Usage they were able to explain and make the client use the product to their maximum advantage"", u'Systems Engineer\nRetail Domain- Infosys Limited\nMarch 2011 to July 2013\nDevelopment Team:\n\u2022 One of six dedicated systems engineers developing Avon e-commerce sites across Asia Pacific and Latin America\n\u2022 Gathered and documented business requirements from Avon\'s local IT project managers in each country\n\u2022 Developed and launched ""Apply to be a Representative"" module for Asia pacific and Latin America markets, through which new customers can fill the online application instead of sending an application through mail\n\u2022 Developed and launched ""Hot leads"" for the Zonal managers and ""Online Appointments"" for the sales Leaders of\nAvon with J2EE programming on Spring Framework platform, so they can recruit and monitor the representatives\nApplication Maintenance Team/Customer Support\n\u2022 One of 11 core team members in maintenance of 64 e-commerce websites across APAC, LATAM and CEE markets.\n\u2022 Modified existing software to fix performance issues like tuning the SQL queries, which decreased the data\nretrieval time from the database by 60 seconds\n\u2022 Analyzed customer sales of more than 10 million Avon reps to propose the best market / trendsetter markets\n\nGRADUATE PROJECTS:\nPredictive analysis on potential customers using SAS EM:\n\u2022 Preprocessed and analyzed a dataset of more than 5000 records and built a predictive model using SAS enterprise\nminer, to predict the customer behavior and segment customers based on most positive receptiveness to a new\nproduct that the bank is going to launch. Concluded with a model of 94% efficiency that customers with high\nsavings balance are more likely to respond positively for a newly launched bank product or scheme.\nAWARDS/RECOGNITIONS']","[u'Masters in Information Technology and Management', u'Bachelor of Technology in Information Technology']","[u'University of Texas at Dallas Dallas, TX\nAugust 2015', u'Madras Institute of Technology, Anna University Chennai, Tamil Nadu\nAugust 2006 to May 2010']"
0,https://resumes.indeed.com/resume/8324a1d2a8d6fb4d,"[u""Data Engineer\nThe Honest Company - Los Angeles, CA\nJune 2016 to Present\nMajor Project 1: Tracking Master for Order Management System\n* This project is a cloud-based, distributed data infrastructure. It serves to near-real-time detect line-level order leaks throughout any order\u2019s whole life cycle. Also it is the foundation data pipeline to populate the company\u2019s data warehouse with data about every order from the company\u2019s wholesale and retail chains\n* I use Spark as the main computational engine for data processing. This includes getting data from different sources like Web Logging Api, PostgreSQL, MySQL, S3 and SFTP; filtering and parsing JSON structures of big log files; performing deduplication and aggregation on huge datasets; writing processed data on to databases like Postgres and datawarehouse like Redshift\n* I code in Scala, SQL, Python, and Shell Script, deploy the pipelines/applications with technologies/tools of AWS Elastic MapReduce, S3, EC2, Elastic Load Balancing, and relevant open source packages/libraries\n* I collaborate with data users, product manager, project manager, DevOps and team members to collect and interpret the business requirements, develop and implement SQL logic, and deploy the code onto production\n* The project gives the company the ability of near-real-time knowing every order status and detecting the order leaks at the line level, in the sense that which line of the order leaks, when and where it leaks, and why it leaks (for both E-Commerce platform and wholesale/retail chains)\n\nMajor Project 2: Job Master for Data Infrastructure/Pipelines/Applications\n* This project is using Airflow as the framework for programmatically managing and monitoring the many data infrastructures/pipelines/applications, including Java, Python, Shell, and Spark applications, running on the tech stacks of the Data Science and Engineering Team\n* I design the project to fit in the current tech stacks used on the team. I deploy the cluster on AWS using distributed mode, which include one Airflow master node and three Airflow worker nodes. I use Celery as the cluster\u2019s distributed task queue backed by RabbitMQ, and use MySQL as the clusters metastore backed by AWS RDS\n* I code in Python to develop sensors, operators, and DAGs, and I implement the Event-Driven idea by developing various sensors to control data tasks. Based on S3Hook I developed S3 File Size Sensor, which checks S3 file landing events and the file size to coordinate the downstream jobs. Also I incorporate EMR Step Operator/Sensor to kick off and coordinate Spark applications running on EMR clusters\n* The project serves the team by not only taking over job scheduling from Crontabs and displaying job status on a user-friendly Web UI, but also coordinating various computational resources/tasks based on the sensors developed and deployed, and sending notifications/alerts triggered by events\n\nMajor Project 3: ETL for Web Browsing Events on Honest.com\n* The project is a pipeline for ingesting and ETL-ing the large-scale data of web browsing events, which are generated on the company's e-commerce platform (honest.com & honestbeauty.com) and are hitting the company\u2019s Web Logging Api\n* I develop the agent for transporting the logs, which make through the Web Logging Api, onto S3 bucket folders partitioned by date, and deploy the Web Logging Api with the agent behind AWS Elastic Load Balancing\n* I code in Scala to develop Spark applications and deploy them on EMR. The Spark applications filter and parse the raw logs, which are JSON, into single events represented by structured format of Spark DataFrame, then land the processed data on to Redshift\n* I develop SQL logic in Redshift for aggregating the data of browsing events, and based on that creating single type events table to feed Data Science for further data analysis and data mining\n\nMajor Project 4: Data Integration for payment transactions\n* The project is an integration of the Api of the third-party payment service provider with the current data stacks at the company, and to build a dashboard of the payment transactions of all the orders\n* I work with the accounting team, security team, DevOps, product manager and project manager on my team to design the integration to make sure Payment Card Industry Data Security Standard compliance (PCI compliance) is strictly followed and implemented\n* I code in Java to build the integration. I hit the Api of the payment service provider, extract the data returned from the Api, and transform them to meet the design requirements, then load the data on to Redshift and the business intelligence tool - Domo, from where I build the dashboard for the accounting team to further aggregate and analyze"", u'Software Engineer Intern\nAudienceScience Inc - Bellevue, WA\nJuly 2015 to March 2016\n* Coded in Java to implement a tool for monitoring Kafka status, e.g. producers, consumers, offsets & replica\ndistributions\n* Coded to enable the offset dual-commit for Secor, a zero-data-loss log persistence service between Kafka\nand S3\n* Developed Ansible Playbooks for deploying application, and create Dockerfiles for building Docker\ncontainer']","[u'Master of Science in Information Management', u'Certificate of Advanced Study in Data Science', u'Doctor of Medicine in Medicine']","[u'Syracuse University Syracuse, NY\nAugust 2014 to May 2016', u'Syracuse University Syracuse, NY\nAugust 2014 to May 2016', u'Second Military Medical University Shanghai, CN\nAugust 1997 to May 2009']"
0,https://resumes.indeed.com/resume/25b8c73ea9937332,"[u'Database Developer\nASCAP\nJanuary 2015 to December 2015', u'Data Analyst\nSantander Holdings\nAugust 2015 to November 2015\nUSA', u'Associate Data Analyst\nConnolly\nJune 2013 to January 2015', u'Consultant\nCollabera Inc, Formerly Global Consultants Inc - Middletown, NJ\nNovember 1998 to June 2000', u'Software Engineer\nSatyam Computer Services - Hyderabad, Telangana\nMay 1996 to May 1998\nIndia.\nWorked onshore for a couple of months, at client site, NCR El Segundo, CA, before moving to NJ.']","[u'Master of Computer Applications in Computer Applications', u'Bachelor of Mathematics in Mathematics']","[u'University of Pune Pune, Maharashtra', u'Osmania University Hyderabad, Telangana']"
0,https://resumes.indeed.com/resume/91371138f4b6c6b4,"[u'Sr. Systems Engineer\nIan, Evan & Alexander Corporation - Tampa, FL\nSeptember 2016 to Present\nSupport of classified and unclassified networks and related systems, workstations and associated software along with stand-alone systems, laptops, and various peripheral devices and systems with analysis, design and engineering support for expanding and/or upgrading capabilities/equipment\n\u2022 Maintain and administer LANs on a continual basis\n\u2022 Provide account management services\n\u2022 Provide assistance in audit collection and updating of technical documentation packages\n\u2022 Provide support for IT software:\n\u2022 Evaluate current operational software for enhancements\n\u2022 Install and maintain software upgrades/modifications\n\u2022 Install/test/maintain new software procedures by IT systems\n\u2022 Conduct tests of all new releases and modifications and complete/update all documentation affected by required changes\n\u2022 Provide written test results, and implementation plan, routed through appropriate approvals and user notifications\n\u2022 Ensure upgrade/modifications of IT software are stable and backup is maintained\n\u2022 Provide system development services for software development and enhancements:\n\u2022 Prepare plans, analyze requirements\n\u2022 Document specifications, design system configurations, write application code, conduct requirements and integrate tests and write documentation\n\u2022 Train personnel to provide effective and efficient technical solutions\n\u2022 Security Software Mitigation as required\n\u2022 Software development COTS packages\n\u2022 Technical engineering and programmatic support through requirements definition, design, identifications, implementation, equipment selection recommendations, installation, integration, test and acceptance\n\u2022 Provide hardware/network documentation generated as a result of or used in the performance of tasking\n\u2022 Perform computer/network hardware troubleshooting and, when appropriate and economically feasible, perform computer/network hardware repairs\n\u2022 Technical advice and engineering support across IT systems\n\u2022 Control and manage IT connections in accordance with ISSO and other Government guidelines\n\u2022 Coordinate the resolution of problems\n\u2022 Control, manage and trouble-shoot IT systems and conduct logical operating system fault monitoring\n\u2022 Manage formulation of equipment security requirements,\n\u2022 Coordination of physical security, verification of access control, documentation of audit trails\n\u2022 Backup/restore IT systems and perform reconfiguration, diagnostics/remedial action and ensure end-to-end physical connectivity\n\u2022 Provide technical support in areas that supplement design stage activities\n\u2022 Information and design reports on specialized software (i.e., languages, database management software (DBMS), applications, etc.)\n\u2022 Analysis and evaluation of existing off-the-shelf application software packages\n\u2022 Review and evaluation of management, planning, security, audit and other products\n\u2022 Attendance at design sessions and evaluation and modification of previously prepared design stage documents\n\u2022 Provide management of current physical configurations of IT systems\n\u2022 Maintain IT systems operations\n\u2022 Monitor applications/system software and hardware operations\n\u2022 Conduct routine high priority systems problem identification and high priority corrective actions\n\u2022 Operational support shall be provided in accordance with the guidelines of the configuration management program\n\u2022 Interface with the NIPRNET, SIPRNET, BICES, JWICS and Defense Messaging System', u""Systems Engineer/Data Collections Engineer\nGeneral Dynamics -IT Systems - National Geospatial-Intelligence Agency (NGA) - Fort Belvoir, VA\nMay 2011 to September 2016\nMay 2011 - September 2016)\n\n\u2022 Ability to work under pressure and with very little supervision.\n\u2022 Outstanding written and oral communication skills.\n\u2022 Administers user accounts/profiles and network access for Windows 2003 and 2008 Servers.\n\u2022 Experience troubleshooting and configuration management through Group Policy Objects (GPO) and permissions.\n\u2022 Extensive knowledge of Microsoft Windows 2003, Active Directory, DNS, DHCP, DFS, File and Print Services, and GPO validation and implementation.\n\u2022 Setup and configured windows servers and workstations for R&D network\n\u2022 Planned, test, installed and integrated new and upgrade versions of operating systems on organization computer systems and third party software components and subsystems.\n\u2022 Experience with SCCM , Windows XP, and Windows 7.\n\u2022 Experience with PowerShell scripting and automation tools.\n\u2022 Successful completion of Computer Equipment Replacement Program (CERP) involving installing over 1200 Dell 980 computers and over 100 networked Dell and HP printers. Completing the project with no downtime.\n\u2022 VOIP installs and utilization of Cisco Call Manager and Unity Systems\n\u2022 Provide second Level Desktop Support for troubleshooting applications developed by the customer.\n\u2022 Contributes in resolving network configuration and network problems.\n\u2022 Conduct setup of user accounts in Active directory, including password resets, group memberships or modifications\n\u2022 Managed numerous upgrades from Windows XP to Windows 7, configuring, maintaining security builds for 600+ classified and 600+ unclassified workstations.\n\u2022 Acquisition, installation, configuration and management of Server applications including Symantec Antivirus 2011 Server and Client.\n\u2022 Responsible for Windows server and the operating systems software and its successful integration with hardware and software application across the organization.\n\u2022 Supervised a team of 9 Technical Support Representatives in a successful Computer Equipment Replacement Program (CERP) involving installing over 1200 Dell 980 computers and over 100 networked Dell and HP printers. Completing the project with no downtime.\n\u2022 Service Level Agreement management and reporting through data collection in Seibel ticketing system.\n\u2022 Maintains knowledge of applicable service level agreement modifications.\n\u2022 Conduct and record Quality Call Backs to ensure that customers are satisfied with the service that they received.\n\u2022 Responsible as Queue Manager for distributing Siebel help desk tickets and help define policies and procedures. Ensuring all service requests are handled according to service level agreement (SLA) and exceeding the 95% resolved Siebel ticket successfully minimized user downtime.\n\u2022 Provides advice on potential process improvement for general ticket and queue management.\n\u2022 Experience implementing and administering PKI/CAC-based Active Directory environment.\n\u2022 Responsible as the Local PKI Subject Matter Expert (SME) to ensure 100% PKI compliance for over 500 users beating the client's deadline by 90 days.\n\u2022 PKI Trusted agent as such trained coworkers in policy and procedures leading them to become Trusted Agents to ensure delivery of high quality service for PKI distribution to include User enrollment and retrieval of PKI, requesting PKI revocation and troubleshooting PKI issues.\n\u2022 Ensured 100% PKI compliance for over 500 users exceeding customer's goals by 3 months.\n\u2022 Develop and implement test plans and procedures, coordinate and conduct product installations\n\u2022 Performs system security administration functions, including creating and managing customer accounts Security knowledge and understanding of industry standards and requirements\n\u2022 Remediate multiple Servers and Workstations bringing them into compliance with DISA standards both existing equipment and new equipment. Utilizing patch management and Group Policy.\n\u2022 Installation, configuration and DISA remediation of Window s7 Professional and XP workstations in mixed network environment.\n\u2022 Proactively monitor machines and servers to ensure efficiency.\n\u2022 Perform backups and installation of patches.\n\u2022 Trouble shooting Video Teleconferencing, VTC (including Tandberg's).\n\u2022 Support Audio Visual infrastructure which includes: Tandberg Secure Video-Conferencing, 10 conference rooms, Satellite TV infrastructure, and other AV hardware.\n\u2022 Ability to evaluate critical systems, prioritize work, and determine solutions."", u""Systems Engineer\nScience Application International Corporation - Herndon, VA\nMay 2006 to March 2011\nHerndon, VA.\nSystems Engineer May 2006 to March 2011\n\n\u2022 Develop Authorization to Operate (ATO) policy for COTS/GOTS materials and hardware in compliance with DOD 8570 standards.\n\u2022 Perform yearly reinstatement of equipment exercised in a SCIF environment.\n\u2022 Develop functional research and development systems requirements for government customers.\n\u2022 Perform quality assurance review for sensitive programs.\n\u2022 Responsible for systems architecture of M&S, operational networks, and standalone systems\n\u2022 Provide planning, design, and implementation of new and improved business and systems requirements for multiple on-site and off-site customer organizations.\n\u2022 VTC scheduling through JWICS and VTC physical support of Tandbergs, Polycons, as well as larger auditoriums.\n\u2022 Provide services within the Hard Target Research and Analysis Center (HTRAC), a division of the Defense Threat Reduction Agency (DTRA) and collaborate efforts with The Defense Intelligence Agency (DIA).\n\u2022 Experience gained working with the division's senior government leads and directors combined with four (4) years of military experience with joint cognizance of multiple agencies.\n\nDeployment\n\u2022 Conduct site survey, cleanup and pre-migration readiness reviews with the site/state prior to deployment.\n\u2022 Utilizing Visio to diagram server architecture layout and wiring diagrams.\n\u2022 Determined spacing, power and cooling requirements.\n\u2022 Server installation and associating cabling\n\u2022 Labeling fiber and Patching of servers.\n\u2022 Transported Classified IT equipment to various sites.\n\nTechnology Summary\nSystems: Windows 7/XP/2000, Windows server 2003/2008, Cisco VOIP phones 7960/7940/7912/7910.\nHardware:\nDell (workstations and servers), HP (workstations and servers), HP and Dell printers, UNIX Sun Microsystems, Video Conferencing, Projectors, desktop VTC, Windows Server 2003/2008, Image MASSter Disk Duplicator, Smart Card reader, NVidia Graphics Cards, EIZO 3D Display,\n\nSoftware:\nSymantec Antivirus Server/Client, Symantec Ghost Cast Server, Numerous Geospatial and imagery apps, Seibel, MS Project, MS Visio, MS Office, Microsoft WSUS, Cisco Call Manager, Cisco Unity Call Manager, System Management Server (SMS), Systems Center Configuration Manager (SCCM), ActiveClient, 90-Meters Software.""]","[u'', u'Certificate in Network Communication', u'']","[u'Northern Virginia Community College Herndon, VA', u'Community College of the Air Force Luke AFB, AZ', u'Cisco Academy']"
0,https://resumes.indeed.com/resume/f1b54bce9cacc530,"[u'Stocker\nHy-vee\nMay 2017 to Present', u'Electrical Engineer\nRana Technologies\nMay 2016 to May 2017\n\u2022 Solar Installation\n\u2022 Generator 5KW and 45KW Installation and service\n\u2022 Electrical issues Troubleshooting in Aerostat and Tower\n\u2022 Electrical path troubleshooting in both aerostat and Tower.\n\u2022 Troubleshoot of Gondola, A3 box, A2 box.\n\u2022 Complete installation of Tower and Aerostat.\n\u2022 Installation of switch, outlets.\n\u2022 Design of wiring in Autocad.', u'Data Entry, DCA\nDutch Committee for Afghanistan\nJanuary 2013 to February 2014']",[u'B.S in Electrical Engineering'],[u'Kabul University\nJanuary 2016']
0,https://resumes.indeed.com/resume/376bd5e17cef0e99,"[u'IT Specialist\nMakita Corporation\nOctober 2015 to Present\n\u2022 AS400 Administration\n\u2022 Daily, Weekly, Monthly and Quarterly system backups.\nOS\nAS/400\n\u2022 Creation and maintenance of queries. Using Query400.\nWindows 7/8/10\n\u2022 Creation and maintenance of queries using SQL in web environment Mac OS X\n\u2022 Retrieve files and reports Windows Server\n\u2022 Maintained file system on AS400 2003/2008/R2/2013\nLinux\n\u2022 Project Management\n\u2022 Created Security Surveillance plan in emergency situation.\n\u2022 Researched and upgraded security system to enterprise level with 80 cameras.\n\u2022 Installed wireless network throughout warehouse\n\u2022 Trained users in new warehouse automation program', u'Helpdesk Engineer\nAltuscio Networks\nFebruary 2014 to October 2015\n\u2022 Work with Sr. level engineers as a team to resolve complex or emergency issues such as work stoppage.\n\u2022 Issue tickets and determine priority of issue\n\u2022 Setup new users on their first day of work PC, Mobile and Desk phone\n\u2022 Work with vendors as a liaison to expedite issues\n\u2022 Active Directory, Exchange and 365 User creation and support\n\u2022 Create VPN and port forwarding or remote users\n\u2022 Train new employees and monitor their behavior with clients for remote support\n\u2022 Troubleshoot and resolve PC, server and network issues in a timely manner\n\u2022 Able to resolve most issues without escalation\n\u2022 Excellent soft skills. Speak well on the phone.\n\u2022 Participated in an on-call rotation', u'Help Desk Engineer\nDynasis - Alpharetta, GA\nDecember 2012 to February 2014\nTroubleshoot and resolve PC, server and network issues in a timely manner\n\u2022 Excellent soft skills. Speak well on the phone.\n\u2022 Participated in an on-call rotation', u'Data Entry Clerk\nAman and Peters - Jacksonville, NC\nFebruary 2011 to August 2012\nCleared out over 50 years of tax returns and other tax related paperwork\n\u2022 Answered phones\n\u2022 Greeted clients\n\u2022 Entered tax related data into the file server']",[u'Associates of Applied Science in design and development'],"[u'Coastal Carolina Community College Jacksonville, NC\nAugust 2010 to May 2012']"
0,https://resumes.indeed.com/resume/fd8d77d51176dc0e,"[u'Data Scientist\nSoothsayer Analytics - Livonia, MI\nJuly 2017 to Present\no Kids Read Now (Non-profit Organization): Developed predictive analytics models for the utility of summer reading program on the reading score of 100,000 Pre-K to fourth-grade kids of the schools in Ohio state.\no Data cleaning, pre-processing, imputation, transformation, scaling, feature engineering, data aggregation, merge data-frames, descriptive statistics, data visualization, regular expressions, report client with weekly Tableau dashboards.\no Decision trees, Random Forest, linear & logistic regression, KNN models to classify quartiles & predict scores.\no Achieved accuracy, precision, recall in the range of 75-80 % on average for the validated models.\no Implemented agile methodology, project management, and Burndown charts to execute the scrum units for the sprint.\no The program showed improvement in the reading score of 75.7% students from Fall 2016 to Fall 2017.\no Steelcase: Analyzed human mobility from embedded-analytics IoT sensors, accuracy stats prediction.\no Markov Chains, Data visualization, Tableau dashboards, proof of concept for the IoT sensors.\no An average accuracy of 73.8% for spatially accurate human prediction in a room was estimated.\no Kaggle: Demand forecasting, inventory mgmt. using RNN & LSTM for a food processing company - TensorFlow, Keras\no Genetic Algorithms for Job shop scheduling optimization - Google Or-tools, Constraint Programming\no Reinforcement learning for Alpha Zero AI - Keras, Python.', u'Graduate Engineer\nVoltas Limited - Mumbai, Maharashtra\nJuly 2014 to July 2015\nBusiness Development: cost estimations, Supply Chain & Procurement: supplier mapping, HVAC Design: Auto-CAD, MS Project\no Derived KPIs like Fill Rate and Perfect Order, derived business development metrics for in a cross-functional team.\no SQL reports for mapping suppliers, supplier segmentation with Kraljic matrix, improved supply chain lead times by 19%.\no Estimated costs with MS Excel (Pivot tables/V-Lookups/VBA/Macros) & analyzed the Critical to Quality (CTQ) outputs.\no Lean Six Sigma DMAIC tool to streamline logistics, tracking production spreadsheet with project milestone dates.', u'Process Analyst Intern\nReliance Infrastructure Limited - Mumbai, Maharashtra\nDecember 2012 to January 2013\no Kaizen, lean manufacturing principles, 5S, six sigma methodologies for process optimization.']","[u'M.S. in Industrial Engineering', u'B.E. in Mechanical Engineering']","[u'Arizona State University\nAugust 2015 to May 2017', u'University of Mumbai Mumbai, Maharashtra\nAugust 2010 to June 2014']"
0,https://resumes.indeed.com/resume/406497da21770259,"[u""Big Data Engineer Intern\nMedeAnalytics - Dallas, TX\nMay 2017 to Present\n\u2022 Developed data pipelines using Hadoop, SPARK & MSBI to generate dashboards for Healthcare Payer clients to identify where improvement initiatives are needed most and determine their revenue impact.\n\u2022 Monitored daily, weekly and monthly data loads for 50+ Healthcare Payer clients and fix issues generated in ETL.\n\u2022 Developed Scala scripts for dynamically parsing, validating and converting input text files into ORC in HDFS.\n\u2022 Improved query performance to 10% by implementing MS-SQL Performance tuning & optimization techniques.\n\u2022 Reduced DB storage utilization by 40% by migrating from traditional SQL backup restore to NetApp's Snap Manager (SMSQL) for snapshot backup and recovery.\n\u2022 Automated test cases to validate files by using PowerShell scripts and reduced QA effort.\n\u2022 Built real-time Clicks dashboards to gain insights about client adoption and engagement details using Spark streaming tools like Apache Flume, Kafka with REST API as source.\n\u2022 Designed POC to process logs for aggregation and analysis, create dashboards by using Elastic search& SPLUNK."", u'Systems Engineer\nInfosys Ltd - Hyderabad, Andhra Pradesh\nAugust 2014 to June 2016\nBusiness Intelligence Developer\n\u2022 Worked in complete migration project which involved migrating ETL Packages from Sagent (Japan based BI tool) to SSIS.\n\u2022 Optimized the existing ETL\u2019s built on SSIS by using C# in script tasks to increase the data flow rate by 30%.\n\u2022 Worked in One Plan BI project for Microsoft client. Responsibilities included modifying code on SQL server Databases based on changes in business logic, performance tuning and code deployments.\n\u2022 Designed SSIS packages to pull data from 40+ upstream sources. Implemented delta pulls to reduce run time of packages.\n\u2022 Increased query performance, by optimizing the performance of various SQL scripts, stored procedures and triggers by identifying slow running queries using SQL Profiler.\n\u2022 Developed Multi-Dimensional cubes on SSAS and generated insightful and automated dashboards in Power BI to solve analytical needs and established right metrics for the key decision making purpose.\n\u2022 Deployed database using PowerShell scripts and C#, reducing tasks from 4 hours to 30 minutes and post-deployment errors.\n\u2022 Migrated legacy system built using MSBI tools to MS Azure Data Lake using Hive scripting and Azure Data factory.\n\u2022 Built Azure Resource Manager(ARM) templates to deploy, manage &monitor all Azure resources into single group. \n\u2022 Designed and developed automated test cases for ETL packages from On Premise to Cloud and ensured successful migration for more than 2TB of data.\n\u2022 Actively participated in talent development initiatives across the business unit and trained 20 associates in MSBI.', u'Intern\nNanomindz - Visakapathnam, India\nJuly 2013 to December 2013\nWorked on Electrical Networking Mapping and Automation project to reduce Aggregate Technical & Commercial Losses in power supply, reduce power thefts up to a minimum level of 15% and increase the reliability of power supply using ArcGIS, AutoCAD.']","[u'MS in Business Analytics', u""Master's""]","[u'University of Texas at Dallas Dallas, TX\nAugust 2016 to May 2018', u'']"
0,https://resumes.indeed.com/resume/a890eda8cf970304,"[u'Associate Data Scientist\nHelloFresh - New York, NY\nSeptember 2017 to Present\nDevelop and improve time-series forecasting and analysis algorithms to\nguide KPI improvement across teams. Design and implement association\nrule learning algorithms to improve supply chain processes and reduce\ninefficiencies. Created the first live U.S. database (Amazon RDS for\nPostgreSQL) and automated all necessary pipelines for datasets analysis and\non demand data refresh tasks. Develop and deploy python and R applications\nfor data driven decision making.', u'Data Analyst\nHallcon @ Google - Mountain View, CA\nFebruary 2017 to March 2018\nManage the planning and development of design procedures for metric\nreports. Create and deploy predictive models, audit large data sets requiring\npreprocessing and advice upper management with live KPI dashboard and\npresentations. Work with engineering teams to integrate live analytics to\nGPS tracking technology. Develop and maintain databases and improve\nefficiency of data collection systems. (Management requested I stay on to work\nremotely on code maintenance)', u'Software Engineer\nViolin Memory - Santa Clara, CA\nJuly 2016 to December 2016\nTest Engineer assigned to the ARIA firmware team in charge of testing and\nfixing future update and releases for the FSP 7300 and FSP 7700 platforms.\nImplemented CI systems and unit tests, collected failure data and analyzed it\nfor insights and feature improvements. Maintained Openstack Cinder driver', u'Data Analyst Intern\nSavepa S.R.L.\nJune 2014 to July 2016\nAnalyzed sales data under the supervision of the Senior Data Analyst.\nCreated prediction models for client ordering patterns to maximize\ninventory and transportation efficiency. Cleaned and processed data for\nfurther analysis. Helped drive down costs through use of combinatorial\noptimization']","[u'M.S. in Applied Statistics', u'B.S. in Management Science', u'A.A. in Political Science', u'A.A. in Sociology']","[u'University of Delaware Newark, DE\nJanuary 2020', u'University of California San Diego, CA\nJune 2016', u'Foothill College\nJune 2014', u'Foothill College\nJune 2014']"
0,https://resumes.indeed.com/resume/09f748c85474a8a5,"[u""Data Migration Engineer\nBond International Software, Inc\nJune 2016 to June 2017\nImplemented client conveyed specifications by deploying customized ETL solutions\n\u25cb Built and altered SQL scripts to conform to the client's business needs and internal software architecture\n\u25cb Verified and delivered data from staging environments to both internal testing databases and external production databases through SSMS\n\u25cb Reinforced data integrity through close collaboration with the implementation team"", u'Software Support Analyst\nBond International Software, Inc\nJune 2015 to June 2016\nProvided consultation for clients ranging from data issues to industry best practices\n\u25cf Diagnosed defects and delegated to the development team to resolve bugs\n\u25cf Executed DDL and DML SQL commands in live production environments to alter data and resolve client-facing errors']",[u'Bachelor of Business Administration in Computer Information Systems'],"[u'Georgia State University, J. Mack Robinson College of Business Atlanta, GA\nMay 2015']"
0,https://resumes.indeed.com/resume/6e832141a12f8a8d,"[u""Data Engineer III\nChoice Hotels International - Scottsdale, AZ\nJune 2012 to Present\nApart of a team tasked with developing new real time Data Analytics Platform (DAP) using\nCloudera Distribution Hadoop (CHD). This system is being designed to help consolidate all of Choice Hotels data warehouse and analytical databases\n\u2022 Developed ETLs using Spark and Hive to load external S3a/HDFS tables to be later\nconsumed by reporting applications such as Tableau and Business Objects\n\u2022 Developed and implemented in house data lineage application using Java and Neo4j Graph\ndatabase to help our developers track and maintain table and data dependencies for Choice's\nhundreds of ETLs\n\nBusiness Intelligence Developer II\n\u2022 Developed ETLs using SSIS/DMExpress to populate data marts in the company's\nVertica/SQL Server analytical databases to be used by downstream reporting tools and business analytics\n\u2022 Worked on writing and improving SQL code to run against Informix data warehouse\nsystems and Informix OLTP databases\n\u2022 Lead major projects to migrate and upgrade legacy reporting applications that house and distribute hundreds of production reports without any downtime or affected users\n\u2022 Used Java/.NET to develop reporting programs that required custom functionality that could\nnot be satisfied by current enterprise applications\n\u2022 Maintained legacy reporting programs developed in Informix 4gl and Java at the same time\nworking to help re-develop the reports using more current reporting tools and systems\n\nBusiness Intelligence Developer I\n\u2022 Developed and maintained SAP Business Objects Universes/ Metadata Layers used by business analytic power users\n\u2022 System Administrator of reporting applications SAP Business Objects and Tableau\n\u2022 Acted as technical analyst to help bridge the gap between what the business wants/needs and the requirements for the IT teams doing the development""]",[u'Bachelor of Science in Computer Information Systems'],[u'Arizona State University - W. P. Carey School of Business\nMay 2012']
0,https://resumes.indeed.com/resume/fc0220d8c6f0f2c2,"[u'Big Data Engineer\nSTD - California\n\u2022 5 years in AL-Baian high school of teaching Computer and Mathematics.\n\n\u2022Two years of work in the Engineers Brothers Construction Company (Supervisor acts).\n\n\u2022 One-year teaching scientific programming in the Noor center of the information and Internet.\n\n\u2022 Open-term experience in driving and maintenance of electricity and auto mechanics.\n\n\u2022 Two years of work in the refinery of crude oil (civil) in Kirkuk.\n\nProfessional computer & IT skills:\n\u2022 ICDL(Windows, Office, Internet, IT).\n\u2022 Networks.\n\u2022 Programming languages:\nMATLAB, mysql, \u0421++']","[u'Certificate', u'Bachelor in software engineering in software engineering', u""Master's in software""]","[u'University of Kirkuk\nJanuary 2009', u'University of Kirkuk', u'Saint Petersburg College Saint Petersburg, FL']"
0,https://resumes.indeed.com/resume/c561ade8f88965c0,"[u'Data + BI Consultant\nVERITAS TECHNOLOGIES LLC - Mountain View, CA\nDecember 2015 to Present\nMountain View, CA Dec 2015 - Till date\nDomain: Channel Partner, Bookings, Pricing, Services\nRole: Data + BI Consultant\n\nResponsibilities:\n\u2022 Collaborate with business areas and data owners to successfully implement and maintain an enterprise level business analytics and data warehousing solution.\n\u2022 Participated in Post implementation reviews for project with all Stakeholders.\n\u2022 Design the data model and implement the business logic to present to customer using OBIEE RPD.\n\u2022 Created Session variable, Repository variable in RPD and used the same for role based security and Dynamic calculation in OBIEE answers.\n\u2022 Responsible for Data Modeling. Created Logical and Physical models for staging, transition and Production Warehouses.\n\u2022 Implemented Dimensional Hierarchies, Aggregate navigation, Fragmentation content, and other features.\n\u2022 Configuring iBOTs to deliver analytics content based on schedule to obtain weekly reports and sent notification Alerts.\n\u2022 Managed Security privileges for each Subject area and Dashboards according to user requirements.\n\u2022 Reduced the number of reports using Column Selectors and View Selectors in OBIEE.\n\u2022 Designed and developed dashboards using Tableau Desktop for Business executives.\n\u2022 Used Calculated Fields, Action filters, URL Actions, Parameters in tableau as per business requirement.\n\u2022 Created drill down reports yearly to weekly reports in Tableau Desktop.\n\u2022 Created multiple KPI reports in Tableau Desktop based on achievements as per requirement.\n\u2022 Created reports using multiple data sources by implementing data blending in Tableau Desktop and created different reports like bars and pie charts, line charts, drill down and other formats in generating the reports.\nEnvironment: OBIEE, Tableau, ODI, Oracle, Windows, UNIX.', u'IT Engineer\nCISCO SYSTEMS - Bengaluru, Karnataka\nDecember 2007 to December 2015\nResponsibilities:\n\u2022 Collaborate with business areas and data owners to successfully implement and maintain an enterprise level business analytics and data warehousing solution.\n\u2022 Participated in Post implementation reviews for project with all Stakeholders.\n\u2022 Designed and developed of various dashboards using Tableau Desktop for Business Leaders.\n\u2022 Created drill down reports yearly to weekly reports in Tableau Desktop.\n\u2022 Created multiple KPI reports in Tableau Desktop based on achievements as per requirement.\n\u2022 Implementation experience and good knowledge of SAP HANA.\n\u2022 Consuming SAP HANA views in business layer and have proven experience with Tableau reporting.\n\u2022 Created reports using multiple data sources by implementing data blending in Tableau Desktop and created different reports like bars and pie charts, line charts, drill down and other formats in generating the reports.\n\u2022 Used Calculated Fields, Action filters, URL Actions, Parameters in tableau as per business requirement.\n\u2022 Worked on Scheduling data extracts in Tableau Server.\n\u2022 Configured Tableau Server to use Active Directory authentication so that user when sign in to the server, their user name and password is verified through Active Directory.\n\u2022 Implemented Performance Steps to improve Tableau Server Performance.\n\u2022 Used JavaScript API to embed Tableau Visualizations into the web applications.\n\u2022 Involved in troubleshooting the issues in Tableau Server.\n\u2022 Handling admin activities like User Management, Scheduling the Extracts.\n\u2022 Design the data model and implement the business logic to present to customer using OBIEE RPD.\n\u2022 Reduced the number of reports using Column Selectors and View Selectors in OBIEE.\n\u2022 Utilized session variables, repository variables and initialization blocks in the repository building and modification procedures in OBIEE.\n\u2022 Implemented Dimensional Hierarchies, Aggregate navigation, Fragmentation content, and other features.\n\u2022 Configuring iBOTs to deliver analytics content based on schedule to obtain weekly reports and sent notification Alerts.\n\u2022 Managed Security privileges for each Subject area and Dashboards according to user requirements.\n\u2022 Interacted with the Business users to understand the user requirements, issues, layout, and look and feel of the application to be developed.\n\u2022 End to End testing during the development and also for version upgrade.\n\u2022 Troubleshoot Production issues, identify root cause and migrate fixes. Develop reporting standards and best practices to ensure data standardization and consistency.\n\u2022 Participated in LDM sessions to make sure the objects meet the business requirements.\n\u2022 Perform troubleshoot on all ETL processes and resolve issues effectively.\n\u2022 Requirements Gathering and Business Analysis. Project coordination, End User meetings\n\u2022 Responsible for designing, developing, and testing of the ETL (Extract, Transformation and Load) strategy to populate the data from various source systems (Flat files, Oracle) feeds using Informatica.\n\u2022 Responsible for creating complex mappings according to business requirements, which can are scheduled thru Dollar Universe.\nEnvironment: Tableau, OBIEE, Informatica, Teradata, Oracle, UNIX, Dollar U, PVCS, Unica, Kintana.', u'Software Engineer\nGE (Health Care) iGATE GLOBAL SOLUTIONS - Bengaluru, Karnataka\nDecember 2005 to December 2007\nResponsibilities:\n\u2022 Evolving Strategies for Extraction, Transformation, Conditioning and Loading of data from various Heterogeneous Data Sources into Data Warehouses using Informatica Power Centre.\n\u2022 Developing Informatica Mappings / Mapplets and Tasks using various Transformations for ETL of data from Multiple Sources to Data Warehouse.\n\u2022 Involved in designing of ETL specs, stage tables and ETL strategy.\n\u2022 Involved in analyzing and importing source tables from the respective databases and flat files.\n\u2022 Creating Mappings, Sessions, workflows for extracting the data from various source systems like relational databases, flat files and load data in to the data warehouse.\n\u2022 Involved in the Unit testing to validate the target data as well the mappings and workflows to make sure they are following the standards.\n\u2022 Also worked with QA team during test cycles, take ownership of the bugs raised by them and resolve the same to give the deliverables back.\n\u2022 Created the job chains according to the enhancements and run the chain using Cronacle.\n\u2022 Involved in preparing the BTEQ scripts.\n\u2022 Prepared the MD120, the migration document to guide the migration team to migrate the enhanced objects from Dev to QA and QA to Prod as well.\n\u2022 Prepared the Teradata Performance Document which includes Collect Stats, Explain plan and Validation Scripts.\n\u2022 Taking ownership of the migration requests and giving Co-ordination to DBA and BI Migration teams.\n\u2022 Contributed in Impact Analysis during the enhancements in case of common dimensions.\n\u2022 Worked with functional and technical leads, business groups to identify, analyze, and document business requirements\nEnvironment: Informatica Power Centre, Teradata, Oracle, Redwood Cronacle, UNIX & Windows']",[u'Master of Science in Information Technology in Information Technology'],[u'Bharathidasan University']
0,https://resumes.indeed.com/resume/59f7eb8d103003b8,"[u'Project Data Engineer\nXylem Inc. - Columbia, MD\nAugust 2013 to Present\nWon Innovations Award in 2017 by spearheading a team in the development of an industry-first monetized risk prioritization model through application of quantitative methods, actuarial analysis, Monte Carlo simulations, Weibull analysis, Bayesian statistics and business intelligence tools\n\nWon Innovations Award in 2016 by co-developing an asset management model using system dynamics\n\nFacilitated and pioneered the company\u2019s first asset degradation modelling program through successful scripting of machine learning (ML) algorithms like random forest, neural networks and decision trees\n\nDesigned and supervised the implementation of a life data analysis and reliability model for utilities using signal processing, data analytics and ETL using statistics and mathematical modeling software like R and Python\n\nGuided the capital investment planning (CIP) and engineering consultancy of an asset management program in Missouri serving 1.3 million people through quantitative risk assessment and asset reliability\n\nDirected the project management of a program involving consumption analysis monitoring using SQL-based database, ESRI based GIS, EPANET based modelling software, and various data visualization tools like Microsoft PowerBI and Tableau for a utility serving over 700,000 people\n\nMaximized business operational processes through automated enhancements and self-service adaptation', u'Application Engineer\nSiemens\nJune 2011 to August 2013\nFormalized 50 project proposals($28M), and evaluated proposal content for full compliance to request for proposals (RFP), request for quotations (RFQ) and requests for information (RFI) with guidance and coordination of project managers, commercial business leads, and other technical personnel\n\nOversaw 25 project bids ($7.5M) through process design, data analysis and detailed engineering calculations\n\nReduced project bid expenses between 10-20% through cost-saving techniques and smart engineering\n\nAchieved 100% on-time bid submission through effective time management and project scheduling', u'Unit Manager\nManila Water Company\nJuly 2010 to May 2011\nWon awards in 2Q and 4Q 2010 for implementing Excel tools for efficient and increased productivity\n\nFostered a team in managing day-to-day operations (est. $700,000 per year) including key accounts\n\nInvestigated 3,000 to 5,000 customer sales accounts for time series, monthly sales and collection forecasting\n\nAchieved organic growth of 3% and reduced the sales revenue losses to 5.5%\n\nEarned 100% on-time customer resolution in the customer relationship management (CRM)']","[u""Master's in Applied Mathematics""]","[u'Johns Hopkins University Baltimore, MD\nMay 2016 to December 2018']"
0,https://resumes.indeed.com/resume/faafc8e4aaaae4d0,[u'Data Analyst/Field Engineer\nJuly 2010 to Present\nDATA ANALYST/FIELD ENGINEER\n\u25c6 Run MAT-Lab scripts to analyze data dealing with signal processing.\n\u25c6 Troubleshoot field systems to optimize system performance and minimize downtime.\n\u25c6 Test new hardware modifications and software releases for functionality and accuracy on systems.\n\u25c6 On site data collections (real time)\n\u25c6 Electrical/Electronic Diagnostics with onsite repair of equipment\n\nCAD/MECHANICAL/MACHINIST\nCAD:\n\u2666 Computer & Drafting (solid works)\n\u2666 Multiple CAD design Simulation and Analysis\nMechanical/Machinist:\n\u2666 Fabrication\n\u2666 Mechanic\n\u2666 Welding / Soldering'],[],[]
0,https://resumes.indeed.com/resume/17ac7a5194222c3a,"[u'Aerospace Systems Engineer\nBelcan Engineering LLC - Windsor, CT\nMarch 2017 to Present\nPratt & Whitney Diagnostics, Prognostic, and Health Management Team\no Developed Data Quality Analysis tool in Matlab to automate monitoring of large data sets\no Wrote data analytics code currently monitoring GTF (Geared Turbo Fan) fuel system health\no Successfully conducted Root Cause Analysis to understand data quality issues\no Coordinated and worked with specialty engineering teams to solve data quality issues\no Performed data mining and analysis of full flight cycle jet engine data in Matlab and Python', u'Project Engineer\nPioneer Aerospace Corporation - South Windsor, CT\nJune 2013 to March 2017\no Test engineer on multiple NASA space projects\no Performed various types of testing in support of engineering projects including:\n\u25cf Low altitude drop testing\n\u25cf Wind tunnel testing in the National Full-Scale Aerodynamics Complex (NFAC) at NASA Ames\n\u25cf Parachute mortar ejection testing\no Test engineering responsibilities included:\n\u25cf Determined appropriate data acquisition system and setup for tests\n\u25cf Reduced and analyzed test data\n\u25cf Performed video analysis of parachute tests to determine parachute performance\n\u25cf Wrote test reports to be provided to customer\no Conducted trajectory simulations of parachute systems and aircraft\n\u25cf Simulations to determine drop test setup to meet parachute load and performance requirements\n\u25cf Post-test analysis of test data for trajectory reconstruction and validation\no Developed and validated 3DOF air vehicle trajectory model in Simulink', u'Data Entry Plug\nDelta Air Lines - Minneapolis, MN\nSeptember 2011 to August 2012\nMinneapolis, MN\nWorkscope Engineering Co-Op September 2011-August 2012\no Wrote Workscopes detailing maintenance to be accomplished on jet engines\no Designed a Data Entry Plug converter and tester program using LabVIEW\no Created Engineering Repair Authorizations to create new repairs']","[u'Masters of Science in Mechanical Engineering in Mechanical Engineering', u'Bachelor of Aerospace Engineering and Mechanics in Aerospace Engineering and Mechanics']","[u'University of Connecticut Storrs, CT\nSeptember 2018', u'University of Minnesota Minneapolis, MN\nMay 2013']"
0,https://resumes.indeed.com/resume/f936155c6f6720db,"[u'Big Data Engineer\nInfosys Limited - Phoenix, AZ\nMarch 2016 to Present\nResponsibilities\n\u2022 Responsible for building scalable distributed data solution s using Big Data technologies like Apache Hadoop, Spark, Drools, Scala and Java\n\u2022 Write complex Hive queries to extract data from heterogeneous sources (Data Lake)\n\u2022 Processing Parquet files from Data Lake using Apache Spark and Scala\n\u2022 Writing the Business Rules in Drools rule engine and Integrate them with Apache Spark an d Scala\nsolutions\n\u2022 Create data pipeline to implement business solutions and generate different files th at can fed to down Stream systems and NoSQL database\n\u2022 Develop Spark/Scala jobs to create and load the files into NoSQL DB Apache HBASE to consume by the Real time applications\n\u2022 Integrate all the components using Oozie work flow scheduler\n\u2022 Building Automated QA process using Shell Scripting, Java and Hive to identify Errors and requirement gaps, thus create high quality applications\n\u2022 Actively participating in daily Scrum meetings and provide status of the deliverables to Scrum\nMaster and stake holders, periodically attend retrospective meetings', u'Big Data Engineer\nInfosys Limited - Phoenix, AZ\nJanuary 2015 to March 2016\nIndia\n\nResponsibilities\n\u2022 Responsible for building scalable distributed data solutions using Big Data technologies like Apache Hadoop, Map Reduce, Drools, Java, Shell Scripting\n\u2022 Write complex Hive queries to extract data from heterogeneous sources (Data Lake) and persist\nthe data into HDFS\n\u2022 Writing the Business Rules in Drools rule engine and Integrate them with Apache Hadoop and Map\nReduce solutions\n\u2022 Create data pipeline to implement business solutions and generate different files th at can fed to down Stream systems and NoSQL database\n\u2022 Develop Map Reduce jobs to create and load the files into NoSQL DB Apache HBASE toconsu me\nby the Real time applications\n\u2022 Integrate all the components using Oozie work flow scheduler\n\u2022 Building Automated QA process using Shell Scripting, Java and Hive to identify Errors and requirement gaps, thus create high quality applications\n\u2022 Actively participating in daily Scrum meetings and provide status of the deliverables to Scrum\nMaster and stake holders, periodically attend retrospective meetings', u'Hadoop - ETL Developer\nHCL Technologies - Chennai, Tamil Nadu\nMay 2013 to January 2015\nOrganization HCL Technologies, India\n\nResponsibilities\n\u2022 Responsible for migrating existing application data from Oracle database to Apache Hadoop\ndistribution and provide ETL solutions\n\u2022 Migrate the data from RDBMS to Hive tables using Sqoop\n\u2022 Migrating existing business logic from Data Warehousing application to complex Hive queries\n\u2022 Created Hive UDFs as per the business requirements\n\u2022 Achieved very good difference in performance after migrating to Hadoop Distribution\n\u2022 Construct the Oozie work flows to integrate the components to run the applications on scheduler\n\u2022 Involved in Unit, Integration and Regression testing\nProject #4', u'Data Warehouse/ETL Developer\nCognizant Technology Solutions - Chennai, Tamil Nadu\nJuly 2009 to May 2013\nOrganization Cognizant Technology Solutions, India\n\nResponsibilities\n\u2022 Responsible for building Data Warehousing applications using ETL tool SAP BODS/BODI\n\u2022 Convert the business requirements into technical solutions and implement them\n\u2022 Create Workflow and Data flows which implement the business logic\n\u2022 Involved in Unit, System and Integration testing.']",[u'Bachelor of Science in Electronics in Electronics'],[u'PSG College of Arts and Science\nJanuary 2008']
0,https://resumes.indeed.com/resume/52d46849ce327005,"[u'Software Engineer (Data Consultant)\nInfosys - Seattle, WA\nJanuary 2017 to Present\n\u2022 Developed custom spark receiver to read the streaming data from HTTP endpoint. The current application built on top of this receiver process 6 million events per hour during peak time.\n\u2022 Developed batch processing pipeline to process google analytic session data using python and airflow. Daily data ingestion of rate 20 GB compressed.\n\u2022 Developed a multi threaded utility to copy files from source to data lake. This generic utility enabled multiple teams to copy data in scalable fashion.\n\u2022 Designed and developed scalable data quality framework using spark to run on cloud to process 60 GB data per day. It has significantly reduced the manual effort to identify quality issue.\n\u2022 Designed and developed business rules processing framework using spark to run on cloud to process data roughly around 100 GB to apply business rule. Automated rules processing has helped business to quickly arrive insights with less effort.\n\u2022 Designed and developed python based library to ingest data from S3, sftp sources to ADLS. This generic library made easy to integrate with Apache airflow for Data Pipeline orchestration.\n\u2022 Worked on developing boundary less data lake framework to automate data processing on cloud.', u'Senior Consultant (Data)\nInfosys - Chennai, Tamil Nadu\nJanuary 2014 to January 2017\n\u2022 Designed and developed real time ingestion framework to ingest data from kafka to Hbase.\n\u2022 Designed and developed generic framework to process data using Spark sql without writing the spark code. This framework is completely schema driven which abstracts user from creating data frames.\n\u2022 Developed a machine learning framework using H2O sparkling water for POC to run supervised (regression) and un supervised (Kmeans) models.', u'Senior Project Engineer\nWipro Technologies - Chennai, Tamil Nadu\nJanuary 2008 to January 2014\n\u2022 Developed HDFS file crawler application using Map reduce to find PII data in HDFS files.\n\u2022 Developed Hive queries for ETL jobs in HDFS.\n\u2022 Have worked in various data warehouse projects using legacy ETL tools and databases.\n\u2022 Worked on data warehouse testing projects to validate data.']",[u'MS in Software Engineering'],"[u'Birla Institute of Technology and Science Pilani, Rajasthan\nJanuary 2012']"
0,https://resumes.indeed.com/resume/ad18446963c66fb8,"[u'Data Specialist\nVerizon\nApril 2017 to Present', u'Data/System Analyst\nFault Isolation\nSeptember 2011 to August 2015\n\u2022 Responsible for extracting network surveillance and customer datasets across the organization to support business initiatives\n\u2022 Analyzed network drops Metrics data according to Business KPI\n\u2022 Worked on Technician Dispatch Reduction which created huge savings to Verizon\n\u2022 Built large datasets from log files\n\u2022 Experience studying and mining data sets in partnership with IT, cross functional teams, derive actionable insights and drive change.\n\u2022 Visualized data drops in different network layers and brought into light, The Insights of network drops.\n\u2022 Scrutinized and track customer behavior to identify trends and unmet needs.\n\u2022 Developed statistical models to forecast inventory and procurement cycles.\n\u2022 Provided in-depth analytical insights by developing or analyzing data models, assess the impacts, and partner with cross functional teams to safeguard organizational impacts.\n\u2022 Experience with researching issues related Customer and Organizational impacts, perform root cause analysis, assess risk, and work across the teams to mitigate negative impacts to customers and organizational goals.\n\u2022 Lead efforts to drive teamwork by engaging and partnering on strategic objectives.\n\u2022 Assisted in developing internal tools for data analysis.\n\u2022 Performed Advanced Analytics and showed Trend lines\n\u2022 Data analysis duties involve turning large volumes of data into actionable insight for the business by:\n\u2022 Experience asking relevant and meaningful business questions.\n\u2022 Collected the best data that is relevant for the decision from multiple large volume data repositories.\n\u2022 Reviewed and analyzed the data in context to the business and how it changes.\n\u2022 Cleaned and transformed the data to prepare for data analysis models.\n\u2022 Analyzed the data, creating data visualizations, and studied visualizations to answer the business question.\n\u2022 Communicated results to stakeholders using applicable metrics, visuals, actionable insights, recommendations and decisions.\n\u2022 Researched Networks - Service, Network elements, capabilities and diagnostics\n\u2022 Created framework, using data inferences and automation, to develop reproducible data driven recommendations optimized stated business objectives.\n\u2022 Created Advanced visualizations in Tableau, ad-hoc dashboards, traditional BI analytics\n\u2022 Published weekly reports that display network component metrics such as percentage utilization, migration plans, quantity of customers\n\u2022 Performed Advanced Analytics and showed Trend lines\n\u2022 Wrote Advanced SQL Queries to Integrate data from different sources of various Network Layers\n\u2022 Assisted in the creation and support of internal analytics databases to ensure actionable outcomes for analysis and reporting.', u'Project Engineer\nWipro Technologies\nAugust 2009 to August 2011\nProject: EBX-PLATFORM\n\u25e6 Coding in C++ to implement use cases in Service Layer of MFP Application from scratch\n\u25e6 Development of Ramdisk application to support various features such as user-authentication, platform component update, digital signature verification, file-system integrity check\n\u25e6 Upgradation of MFP and system boards with latest released platform (Kernel image, boot loader and root file system)\nProject: NSN-FlexiPlatform(MiddleWare)\n\u25e6 Development of userStory in C++\n\u25e6 Development of SNMP Mediator component which converts SNMP traps to alarms.\n\u25e6 UserStory development of IF-MIB, Pronto Correction\n\u25e6 Patching of windriver net-snmp package with open-source netsnmp Package.\n\u25e6 Part of TestCase Automation framework.\n\u25e6 Part of Agile Methodology-Sprint Planning, DSM, Sprint Retro.']","[u'Bachelor of Science in Computer Science and Engineering', u'Master of Science in Computer Science']","[u'Sri Venkateswara University\nSeptember 2015 to March 2017', u'Santa Clara University Santa Clara, CA']"
0,https://resumes.indeed.com/resume/625b70bcbd8f2d40,"[u'Sr. Software Engineer\nCerner Corporation - Bangalore, Karnataka\nDecember 2010 to May 2012\nResponsibilities\nDeveloped a C# based pharmaceutical solution to provide supply chains management\nDesigned and developed a new COM inter-ops to for existing C++ components\nTook responsibility of black box testing for my product to meet project deadlines\nOrganized and handled daily scrum meetings, and code reviews\n\nAccomplishments\nRecognition for my consistently good development and troubleshooting capabilties\n\n\n\nSkills Used\nC# DOT NET, XML, UML', u'Software Engineer\nABB - Bangalore, Karnataka\nJanuary 2008 to January 2010\nResponsibilities\nEnhanced Ether Net IP protocol suite, used for industrial automation\nAnalyzed business requirements, design, coding, testing and delivery management\nDesigned detailed test methodology and plans\n\nAccomplishments\nWas an individual contributor during the complete cycle of product enhancement and handled it effectively\n\n\n\nSkills Used\nC# DOT NET, XML, UML', u'NTT Data\nSr software engineer - Bangalore, Karnataka\nJanuary 2006 to January 2008\nResponsibilities\nDesigned and developed Pharmacy Info Management applications at McKesson\nAnalyzed business requirements, design, coding, testing and delivery management\nSuccessfully handled more than 2 releases of product independently\nExcelled delivery management\u2019s Quality initiative\n\nAccomplishments\nQuick learning capability aided in meeting timely deadlines with efficiency\n\n\n\nSkills Used\nC# DOT NET, XML, UML, SQL']","[u'in Complete Web Developer', u""Bachelor's""]","[u'udemy Kenmore, WA\nAugust 2017 to September 2017', u'K S Institute of Technology']"
0,https://resumes.indeed.com/resume/2f2044f1a2cb6585,"[u'Data Engineer\nHealth Catalyst - Cottonwood Heights, UT\nJuly 2015 to Present\n\u2022 Created data warehouses for Healthcare from disparate data sources using T-SQL and ETL processes\n\u2022 Collaborated directly with clients on their data requests and essential reports, in either engineering the data, or creating another solution\n\u2022 Designed end-to-end solutions from raw data to visualization\n\u2022 Lead discussions, internal and external, to visual best practices and use cases for visual tools (Tableau, Qlikview, D3, Shiny)\n\u2022 Consulted clients and internal personnel on Visualization best practices\n\u2022 Converted Qlikview visualizations to work fluidly in Tableau', u'Business Intelligence Developer\nAncestry.com - Provo, UT\nJuly 2013 to July 2015\n\u2022 Applied basic data mining algorithms to Big Data (Hadoop, hive) providing basis for business decisions\n\u2022 Created a lightweight Python script that ran every 10 minutes which automated part of my job: refreshing Tableau extracts based on data warehouse refreshes\n\u2022 Migrated key reports from both a datawarehouse change (SQL to Paraccel) and Visualization change (Excel and Microstrategy to Tableau)\n\u2022 Built a Microstrategy mobile app, from Star to Snowflake Schema, using sql views and objects/reports within Microstrategy\n\u2022 Provided support and administration to 1400 users of Tableau desktop/server\n\u2022 Maintained reports in Microstrategy, or Tableau with a Javascript wrapper to create a reporting portal\n\u2022 Created dynamic visual reports for in house and external use', u'Database Analyst\nAncestry.com - Provo, UT\nNovember 2011 to July 2013\n\u2022 Performed manual ETL, by converting varying data types (flat files, access, excel etc.) into a 3 table system to be used in tools\n\u2022 Devised different cursors and stored procedures within SQL server to conserve time and resources']","[u""Master's in Information Systems"", u'Bachelor of Science in Economics']","[u'University of Utah Salt Lake City, UT\nAugust 2015 to August 2016', u'Brigham Young University Provo, UT\nJanuary 2008 to January 2012']"
0,https://resumes.indeed.com/resume/307d4b6fca91d3a4,"[u'Construction Data Analyst & Engineer\nTidewater Inc - Elkridge, MD\nJune 2017 to August 2017\n\u2022 Managed construction schedule and preventive maintenance data using Primavera P6, Python, and Computerized\nMaintenance Management System (CMMS)\n\u2022 Evaluated project statement of work in preconstruction phase and assessed project viability by analyzing project\nvalue, location, timeframe, resources, risk, etc.', u'Superintendent\nKAPAA Design Build Co., Ltd - Bangkok, TH\nOctober 2015 to August 2016\n\u2022 Facilitated the collaboration amongst military commanders, researchers, and subcontractors for the renovation\nproject of national medical lab\n\u2022 Planned for demolition and renovation activities in the restricted zone to minimize the noise, dust, and vibration\nimpacts on nearby hospital operations\n\u2022 Supervised the replacement of an old air handling unit while maintaining temperature stability of the laboratory', u'Site Engineer\nPre-Built Public Co., Ltd\nJune 2014 to September 2015\n\u2022 Controlled quality of internal precast wall and external steel frame fa\xe7ade installation\n\u2022 Supervised lobby hall interior finishing to meet the deadline before operating permit appointment\n\u2022 Checked unit defect correction before handing over to the inhabitants of a 48-story condominium\n\u2022 Prepared RFIs, weekly and monthly reports for owner, consultants, and head office']","[u'Master of Science in Civil Engineering', u'Bachelor of Engineering in Civil Engineering']","[u'Carnegie Mellon University Pittsburgh, PA\nDecember 2017', u'Sirindhorn International Institute of Technology (SIIT), Thammasat University Bangkok, TH\nMarch 2014']"
0,https://resumes.indeed.com/resume/bb0678e6854c8a8a,"[u'PRINCIPAL Engineer\nMstar Semiconductor - \u53f0\u5317\u5e02\nAugust 2017 to November 2017\nString Kernel repesentation+PCA+DNN.\nLSTM, DNN, BPTT/rProp/ADAM/ResBProp.\nVoice control in TV, which works as Amazon Echo.', u'Data Scientist\nLegoly - San Jose, CA\nDecember 2016 to March 2017\n1.data analysis and intelligence extraction of {Magento EC product catalog, public product review and shopping logs}.\n2.AI selling brain behind chatbot']",[],[]
0,https://resumes.indeed.com/resume/78260c17a1fb668c,"[u'Data Scientist/Machine Learning Engineer\nHewlett Packard Enterprise - Madison, WI\nDecember 2016 to Present\nDevelop data driven solutions in Smart cloud services with emphasis on cost reduction, service quality under current trend of automation and data exchange in manufacturing technologies. Leverage Cyber physical systems, cloud Computing and IOT.', u""Data Scientist\nMckesson Specialty Health - The Woodlands, TX\nSeptember 2015 to November 2016\nDescription: As part of Mckesson Health Care Services, a collection advanced diagnosis systems development for different brain disorders and various tumors using fMRI and other scanning systems. Incorporated with realized continuous learning leveraging Machine learning and Artificial Intelligence.\n\nResponsibilities:\n\u2022 Follow data descriptions and gain required information on the medical conditions and classifications within the context.\n\u2022 Performed data analysis, visualization, feature extraction, feature selection, feature engineering using python pandas, Apache Spark etc.\n\u2022 Used Pyspark data frame to read heavy loads of text, csv and image data from S3 and Cassandra.\n\u2022 Applied Spark RDD's transformations and actions on raw data\n\u2022 Derive feature importance using techniques Random Forest (RF) classification and boosting.\n\u2022 Implemented Two Dimensional Fast Fourier Transformations on raw input data from fMRI.\n\u2022 Implemented Deep convolutional neural networks (CNNs) in modeling cortical representation and organization for spatial and visual processing with computer vision\n\u2022 Created Scikit-learn based learning models for POC's on sample dataset.\n\u2022 Applied statistical modeling like decision trees, regression models, and SVM.\n\u2022 Utilized Convolution Neural Networks to implement a machine learning image recognition component. Implemented Backpropagation in generating accurate predictions\n\u2022 Performed Information Extraction using NLP algorithms coupled with Deep Learning (ANN and CNN), Keras and TensorFlow.\n\u2022 Implemented Apache Spark to speedup Convolutional neural networks modeling.\n\u2022 Analyzed sentimental data and detected patterns in customer usage data sets.\n\u2022 Avoid overfitting by following standard practices such as keeping the number of independent parameters less than the data points avoidable in the model.\n\u2022 Loaded data from Hadoop and made it available for modelling in Keras.\n\u2022 Prepared multi-class classification data for modeling using one hot encoding.\n\u2022 Used Keras neural network models with Apache Spark.\n\u2022 Enhanced model performance by calibrating parameters, researching and improving optimization and weights initialization methods\n\u2022 Used Pyspark dataframe to read data from HDFS and S3.\n\nEnvironment: Python, Keras, TensorFlow, Scala, Apache Spark, Jupyter Notes, Anaconda, SciPy, Scikit-Learn, Numpy, pandas, AWS, HDFS, S3, Git, GitHub, REST, NVIDIA, CUDA, tmux, Linux."", u""Data Analyst/Data Scientist\nTema Business Systems, India - IN\nApril 2013 to August 2015\nDescription: Empowering grocery store operations and maintenance using advanced analytics and predictive modeling. Support email companies and store organization using machine learning modeling and statistical analysis in order to best understand and serve customers.\n\nResponsibilities:\n\u2022 Responsible for performing statistics and machine learning techniques classification/regression. Design and develop advanced R/Python modules to process and prepare datasets for modeling.\n\u2022 Analyzed large datasets to answer business questions by generating reports and outcome.\n\u2022 Worked in a team of programmers and data analysts to develop insightful deliverables that support data driven marketing strategies.\n\u2022 Following to best practices for project support and documentation.\n\u2022 Actively participated in meetings, presented findings and hypotheses\n\u2022 Understanding the business problem, build the hypothesis and validate the same using the data analysis and machine learning models.\n\u2022 Managing the Reporting/Dash boarding for the Key metrics of the business.\n\u2022 Collect and manage datasets using panda's data frames, queried RDBMS using python ORMs and session bindings along with data collected from Excel, Spreadsheets and API end points\n\u2022 Performed data visualizations with matplotlib's pyplot and seaborn. Applied transformations on data samples using numPy and sciPy.\n\u2022 Performed descriptive and infernal statistical analysis\n\u2022 Cleaning data with missing value treatments and removing outliers.\n\u2022 Implemented time series analysis by generating fixed frequency dates and time spans, convert time series frequencies.\n\u2022 Executed primary statistical analysis including linear regression, hypothesis testing, kmeans, SVMs random forest and many more.\n\u2022 Visualize and present machine learning hypotheses modeled on proprietary datasets.\n\u2022 Created visualizations and grouped various graph types using matlpltlib and seaboarn.\n\u2022 Determine relationship between random variables using covariance/correlation and plot it with seaborn's heatmaps.\nEnvironment: Python, Ipython Notebook, Scipy, Scikit-Learn, Numpy, pandas, MySQL, Git, REST,\nAWS, Flask, Linux.""]",[u'Master of Science in Information System Technologies'],[u'August 2015 to January 2017']
0,https://resumes.indeed.com/resume/7e1ec8d8e338520e,"[u""Data Center Engineer\nGreen Shoot Group - Raleigh, NC\nNovember 2015 to Present\nServed as professional service resource for VMware, Veeam, NetApp, and Citrix products for customers\nMade recommendations and developed plans to update and protect customer environments\nHelped maintain and improve Green Shoot's cloud environment\nActed as a support resource and would often engage with the customer on-site\nPerformed NetApp Data ONTAP upgrades"", u'Support Engineer\nSirius Computer Solutions - Raleigh, NC\nAugust 2014 to November 2015\nProvided tier 2 support for enterprise customers\nEmployed technologies from Cisco, EMC, VMware, and Citrix\nServed as a primary VMware resource during Severity 1 level issues\nWorked with customers to resolve VDI issues in View and Xen environments\nEmployed outside vendors when applicable']",[u'Bachelor of Science in Information Computer Technology'],"[u'East Carolina University Greenville, NC\nJanuary 2014']"
0,https://resumes.indeed.com/resume/b759ad911b54a0b4,"[u'Data Engineer\nSilicoinformatics, LLC - Cleveland, OH\nFebruary 2014 to September 2016\nDesign and develop data mining solutions for operational and strategic problems for clients in various\nindustries. Collaborate with technology team to support the development of analytical models and techniques to improve decision making for the clients. Big Data developing with Cloudera Hadoop and\nDatabase management with Oracle SQL, MS-SQL and MySQL. Web service projects with application of J2EE, Spring, JSF, JSP, Struts, Restful, Hibernate, etc', u'Statistical Consultant\nThe University of Akron - Akron, OH\nJanuary 2012 to May 2013\nPerform data cleaning, data manipulation, data analysis with comprehensive statistical techniques with statistical software R, SAS, SPSS, MS-Excel and MS-Access. Present data analysis report to clients from different areas including engineering, ecology, biology, social work, education, linguistics,\ncommunication, nutrition and psychology on their research projects', u'Teaching Assistant\nThe University of Akron - Akron, OH\nAugust 2011 to May 2013\nProvide lectures and instructions in statistics labs, tutoring services and homework grading to promote\nteaching undergraduate students in statistics courses']","[u'MS in Computer Science in Computer Science', u'MS in Statistics in Statistics']","[u'International Technological University San Jose, CA\nDecember 2018', u'The University of Akron Akron, OH\nDecember 2013']"
0,https://resumes.indeed.com/resume/f52a29867253ef52,"[u'LOGGING SUPERVISOR\nHALLIBURTON\nFebruary 2015 to November 2017\n\u2022 Analyze and ensure superior results to fulfill all company requirements\n\u2022 Prepare project updates within required timeframe and delegate individual tasks to all crew members\n\u2022 Ensure to stay in compliance with applicable laws, company policies and procedures\n\u2022 Develop plans and supervise various departmental meetings, join in on many growth based campaigns\n\u2022 Troubleshoot issues with existing company installed systems such as automations and machines\n\u2022 Check quality of assemblers and Technicians work and respond to inquiries during assembly\n\u2022 Efficiently followed up with crew to ensure proper safety education and training was used\n\u2022 Categorized and delivered important employee documents to maintain organization\n\u2022 Power up electrical systems and configure hardware per application specifications', u'Surface Data Engineer\nHALLIBURTON\nSeptember 2011 to January 2015\n\u2022 Observe drilling well parameters while multi-tasking real time operations\n\u2022 Trouble-shoot equipment while testing various items for job satisfactory\n\u2022 Test and monitor safety equipment according to well standards and procedures\n\u2022 Troubleshoot instrumentation systems and advance gas detection systems\n\u2022 Maintained service quality to ensure the best work possible for the job']",[u'BACHELOR OF SCIENCE in Electronic Engineering Technology'],[u'NORTHWESTERN STATE UNIVERSITY\nMay 2011']
0,https://resumes.indeed.com/resume/e3378e836eba1567,"[u'Data Analyst Intern\nUSF Advancement Office - Tampa, FL\nJanuary 2018 to Present\n\u2022 Extract, clean and analyse data to identify trends and patterns about various donations made to university\n\u2022 Apply Machine Learning Algorithms to identify key elements that are significant in major giving/donations from historical data', u'System Engineer\nTata Consultancy Services - Delhi, Delhi\nMarch 2015 to July 2017\nCollaborated with Business Analysts and other stakeholders of the application to gather requirements and performing\nfeasibility studies to provide most efficient technical solutions and then created the Application Design Document\n\u2022 Worked as ETL developer, created robust designs using tools like IBM Datastage, UNIX, Attachmate Reflection\n\u2022 Implemented new mapping rules using ETL tools for existing application configuration and enhanced it with features\nrequested by the client\n\u2022 Assisted project manager in maintaining project schedules, issue/decision logs, costing, meeting summaries and status\nreports to ensure effective, ongoing communications across all team members\n\u2022 Created reports in Excel and presentations using MS PPT and presented it to CIO PM and PMs of other teams on weekly calls to apprise them of developments by the team\n\u2022 Performed rigorous testing of the deliverables before they were sent for QA and UAT phase\n\u2022 Worked closely with QA and UAT team to fix any bugs and errors they reported\n\u2022 Provided warranty support to client when the application went live and assisted in case there were any issues']","[u'Master of Science in Business Analytics and Information Systems in Business Analytics and Information Systems', u'Bachelor of Technology in Electronics and Communication Engineering']","[u'University of South Florida Tampa, FL\nDecember 2018', u'Guru Gobind Singh Indraprastha University Delhi, Delhi\nJuly 2014']"
0,https://resumes.indeed.com/resume/b47c71b6e67599b3,"[u'Technology Summer Analyst\nBarclays - New York, NY\nJune 2017 to August 2017\nBuilt the internal data lake search engine and implemented reporting dashboards using\nConfluence to track key business metrics and supported project plan\n\u2022 Utilized A/B testing, content analysis and Data Warehouse and BI solutions with Engineering and Product teams to optimize user experience for Barclays Live application\n\u2022 Used Java, Python to develop electronic trading systems reducing latency in an SDLC\nenvironment and improved user experiences', u'Google Online Marketing Challenge\nJanuary 2017 to January 2017\nGoogle Adwords, Google Analytics\n\u2022 Ranked 7th Globally for the Social Impacts Category, Ranked among the top 5 teams in\nAmericas region\n\nTop 5% Kaggle Data Science Bowl Lung Cancer Detection using Machine Learning\nPython, AWS, TensorFlow, Git\n\u2022 Cleaned 1 TB datasets and built deep learning model based on Convolutional Neural Network for prediction\nPublication: Predicting Salary to Get Data Science Job in Top Companies', u'Data Engineer\nAmazon - Beijing, CN\nJanuary 2016 to June 2016\nProduced regular reports and analysis, wrote systems queries to gather data, performed data\nmining and analysis using tools including Oracle, RedShift to identify trends and develop\nforecasts in a Unix/Linux environment\n\u2022 Worked with product owners and downstream consumers to define stories for Agile development and application running time was improved by 20%\n\u2022 Optimized database queries in SQL for predictive analysis by monitoring existent metrics,\nanalyzing data and partnering with internal teams to suggest recommendations', u'Data Analyst\nPfizer - Beijing, CN\nJuly 2015 to September 2015\n\u2022 Applied Excel, Tableau to analyze over 200 sets of monthly sales data providing risk assessment and mitigation to support product managers in improving marketing and sales e\u21b5ectiveness\n\u2022 Maintained and presented client-approved databases that managed vendor relationships using\nSQL, R\nSkills\nProgramming Languages: Java, Python, SQL, SAS, R, VBA, JavaScript\nData Science: Pandas, NumPy, scikit-learn, TensorFlow, NLTK, Scrapy\nDatabases: Oracle, MS SQL Server, MySQL, MongoDB, Cassandra\nTools: MS Excel, Hadoop, MapReduce, Git, Tableau, AWS\nProjects']","[u'MS in Data Science and Business Analytics', u'BS in Management Information Systems', u'in Engineering']","[u'Stevens Institute of Technology Hoboken, NJ\nMay 2018', u'Beijing Forestry University Beijing, CN\nMay 2016', u'NYC Data Science Academy\nJanuary 2016']"
0,https://resumes.indeed.com/resume/32df691cf63d9227,"[u'Lead Data Engineer\nSLK America Inc - Minneapolis, MN\nFebruary 2016 to Present\nApr 2017 onwards \u201cData Governance \u2013 Data Quality Project\u201d for TCF Bank, Plymouth MN\nRole: Lead Data Engineer\n\nA product development for TCF Bank on Big Data platform to identify the data anomalies in their major source systems and to build a Risk Data Store to place risk and business-related data for reporting, modeling, and analysis. Through ETL process we develop automated, timely and consistent extracts of key data. This will source Quality Data for different regulatory reports like CECL (Current Expected Credit Loss), ILDR and FairLending. This is a strategic Big Data initiative in TCF Bank with Agile methodology.\n\n\nMar 2016 to Mar 2017 \u201cData Governance \u2013 Data Quality Project\u201d for M&T Bank, Buffalo NY\nRole: Lead Data Engineer\n\nM&T Bank offers a wide array of financial services for private, retail, commercial and investment banking. Liquidity Coverage ratio (LCR) is designed to ensure that financial institutions have the necessary assets on hand to ride out short-term liquidity disruptions. Data Warehousing was built using Teradata FSLDM in M&T and data from various source systems were loaded in to FSDM using Informatica Power Center.', u'Assistant Consultant\nTata Consultancy Services (TCS) - New York, NY\nJuly 2008 to February 2016\nNov 2014 \u2013 Jan 2016 \u201cCorSo IDS Reporting\u201d for Swiss Re, Kansas\nRole: Lead Data Analyst\n\nCorSo desires an integrated solution that supports insurance capabilities inclusive of local regulatory compliance while allowing easy and efficient interfacing with other Swiss Re systems and industry solutions. To meet the business needs in North America, the CorSo IDS Reporting was established in Nov, 2014.\n\nMar 2013 \u2013 Nov 2014 \u201cClient Market Underwriting for Property and Casualty (CM/UW-P&C)\u201d for Swiss Re, Kansas\nRole: Data Analyst\n\nAug 2011 \u2013 Mar 2013 \u201cProfit & Loss Database and Reporting (PLD/R)\u201d for Swiss Re, New York City NY\nRole: Tech Lead \u2013 Onsite-Offshore Co-ordinator.\n\n\nJul 2008 \u2013 Aug 2011 \u201cFinancial Accounting Hub (FAH)\u201d for Swiss Re, Bangalore India\nRole: Tech Lead', u'Software Engineer\nM Result Services Pvt Ltd - Bangalore Urban, Karnataka\nApril 2003 to June 2008\nApr 2005 \u2013 Jun 2008 \u201cBusiness Intelligence System\u201d for Etihad Airways, Bangalore India\nRole: Software Developer\n\nOverall vision of this Business Intelligence System is to create a single Operational Data Store (ODS) with all revenue, expenses, and other financial data that is required to support Financial Reporting. Integrated Profitability Reporting is also being implemented with data sources from the ODS. BIS also supports to analyze day-to-day operations, risks, procurement opportunity assessment, supplier performance, compliance of contracts and Purchase Order process by creating different reports.']","[u'Master', u'Bachelor in BSc Electronics']","[u'Bharathiar University Coimbatore, Tamil Nadu\nJanuary 2000 to January 2003', u'University of Calicut Calicut, Kerala\nJanuary 1996 to January 1999']"
0,https://resumes.indeed.com/resume/449656881e67ba48,"[u'Graduate Research Assistant\nUniversity of Illinois\nFebruary 2018 to Present\nAnalysis of healthcare data to draw valuable insights for research at the school of Information Sciences. Advisor: Jana\nDeisner', u'Data Engineer\nJayabheri NEXA - Hyderabad, Telangana\nDecember 2016 to July 2017\nHyderabad, India\n\u25cf Programmed python scripts that extracted inventory data from CSV format to MySQL for analysis\n\u25cf Developed SQL statements and built reporting dashboards using Tableau for various business entities\n\u25cf Gathered and analyzed information that provided recommendations on reducing inventory spend, led to reduction of costs by 12%\n\u25cf Built and designed prediction models using machine learning techniques for further data analysis', u""Data Analyst (Intern)\nAlekhya Homes - Hyderabad, Telangana\nDecember 2015 to February 2016\nHyderabad, India\n\u25cf Gathered past sales data and provided sales volume analysis based on size, price and location\n\u25cf Developed ETL process using Pentaho to transform data into queryable form in SQL, for reporting purposes\n\u25cf Using past sales data, predicted futures sales using regression models in R and base SAS\nPROJECTS\nExploratory Data Analysis of United States Pollution Rates\n\u25cf Gathered pollution rates data from year 2000 from Kaggle and conducted exploratory analysis using R\n\u25cf Created a RMD file that explores variables, structures, patterns oddities, and underlying relationships\n\u25cf Visualized various trends and plots using Tableau\nSan Francisco Housing Prices: Web Application using Python Flask\n\u25cf Developed web application for visualizing San Francisco housing prices, developed various charts for visualization by number of rooms, county, square feet, etc\n\u25cf Built the web app using Python Flask, MySQL for data persistence, and data visualization using matplotlib\n\u25cf Deployed web app in AWS, using AWS Free Tier and EC2 instances\nImage Classification Model using Convolutional Neural Network\n\u25cf Built a CNN using TensorFlow and keras libraries, the model classifies images of a given dataset\n\u25cf Built convolutional layers, developed max pooling layer for downsampling and performed flattening\n\u25cf Full connection formed by connecting layers to the artificial neural network and train the model using the training data\n\u25cf The model learns with an accuracy of 92.43%\nAnalysis of Bank's customer churn using Artificial Neural Network\n\u25cf Preprocess the dataset by OneHotEncoding, splitting the dataset and feature scaling of the data\n\u25cf Built the input layer followed with four hidden layers connected to the output layer and run stochastic gradient descent\n\u25cf Training data is fit to ANN, the test set results and accuracy are predicted using confusion matrix\n\u25cf Activation Functions: Sigmoid, Rectifier (ReLu), Epochs: 64""]","[u'M.S in Data Science & Analytics', u'Bachelor in Engineering']","[u'University of Illinois Urbana-Champaign Urbana-Champaign, IL\nAugust 2017 to Present', u'Amrita University\nMay 2017']"
0,https://resumes.indeed.com/resume/402a4ad652422c41,"[u""IT Security Engineer\nCOPART - Dallas, TX\nSeptember 2015 to February 2018\nLed ISO/IEC, SSAE-18/\u2022SOC 2 Type I, and PCI-DSS audits by understanding information technology objectives,\ninformation structure, policies, processes, and internal controls; identified risk areas by preparing audit scope and objectives and preparing audit programs\n\u2022 Coordinated and assisted monthly to quarterly testing initiatives of internal and external controls to identify and document for risk analysis of non-compliant findings; developed audit metrics to document and coordinate\nremediation efforts to Infrastructure Teams\n\u2022 Developed hands-on experience to support operational services such as domain support, certificate management and collected weekly to monthly metrics for key performance index reporting\n\u2022 Configured enterprise data loss prevention solution of incoming and outgoing e-mail to improve e-mail security\n\u2022 Developed a comprehensive security awareness training as an onboarding initiative; developed and initiated\nweekly to monthly simulated phishing campaigns to improve end user security awareness\n\nAccomplishment\nRecognized by the CTO and VP of Infrastructure of achieving critical audits and assessments that contributed the company's brand and marketability."", u'Data Clerk\nELECTRONIC TRANSACTION CONSULTANTS - Richardson, TX\nJune 2013 to September 2015\nProcessed over 100 license plate data through a quality assurance and billing system\n\u25cf Trained multiple new hires one-on-one to operate the Image Review billing system inclusive to evaluate and verify\nlicense plates\n\nAccomplishment\nConsistently ranked on the Top 25 performer list']","[u'Master of Information Systems Management in IT Security', u'Bachelor of Science in Business Administration']","[u'KELLER GRADUATE SCHOOL OF MANAGEMENT Dallas, TX\nSeptember 2012 to January 2016', u'UNIVERSITY OF TEXAS AT DALLAS Richardson, TX\nAugust 2006 to May 2010']"
0,https://resumes.indeed.com/resume/567c0f0e6e4d0429,"[u'Principal Data Engineer\nVerizon Wireless - Charlotte, NC\nJanuary 2014 to January 2017\n- Rendered subject matter expertise to the Data Transport Group in designing EBH network and collaborated with other network teams in providing support to projects and initiatives.\n- Delivered technical support to network deployments, encompassing Cloud-RAN (C-RAN), small cells, dark fiber, and SpiderCoud.\n- Supervised vendors and internal functions to guarantee on-time and top-quality delivery of service to customers.\n- Organized and facilitated functional training for Internet Protocol (IP) and data, contract management, failure domain, cell site routers (CSRs), microwave, and fiber design.\nCareer Highlights:\n\u2713 Built the Data Transport Group tasked to support Carolinas and Tennessee markets by centralizing IP and Ethernet backhaul into one engineering team and eliminating reaction to support the build plan and drive the region to a more advanced planning mode that resulted to the following:\n\u25e6 Advance detection of router availability and need for additional hardware;\n\u25e6 Pre-assignment of IP subnets and early processing of subnet requests, thus preventing delay in fulfilling tasks;\n\u25e6 Maintenance of standard and correct quality of service (QoS) statements by setting up a central router configuration tool; and \u25e6 Seamless sharing of accurate information by positioning the team as the focal point of contact with network vendors, such as Cisco and Nokia.\n\u2713 Managed the seamless coordination of technical network redesign projects for venues and college campus including Bank of America Stadium, Clemson University, and University of Carolina that led to excellent customer experience and increase in redundancy, thus keeping the network up and running despite one or two failures took place.\n\u2713 Played an integral role as a member of the inaugural innovation class in the Verizon Southeast territory with focused on Agile and Scrum practices.', u""Senior Data Engineer\nVerizon Wireless - Charlotte, NC\nJanuary 2013 to January 2014\n- Ensured the full functionality of data distribution capacity for the projected growth.\n- Took charge of guaranteeing on-time implementation of customer input questioner (CIQ) and method of procedure (MOP) to surpass quality metrics, save cost, and minimize customer impact.\n- Managed long term evolution (LTE) deployments efforts in support of other business functions, including development and review of key process indicator (KPI) to identify if the capacity were met.\n- Functioned as primary engineer, in charge of supporting the Switch Engineering Team in fulfilling data-related capacity adds, switch, adjunct, and evolution data optimized (EVDO) deployments.\n- Coordinated trainings and provided briefings on installation and DC power connectivity practices to guarantee team's knowledge of current installation standards.\n- Performed technical and project management leadership to fellow engineer with regard to design, acquisition, and implementation.\n- Managed project meetings concerning current status of pending and ongoing projects.\nCareer Highlights:\n\u2713 Contributed key inputs behind the successful completion of Alltel Network Integration with Verizon Wireless Network for five mobile switching center (MSC) locations.\n\u2713 Generated a project timeline tool that aided in identifying required processes, timelines, and equipment for each group per distribution locations; as well as helped in preventing missing items toward the successful completion of tasks at hand."", u'Data Engineer\nVerizon Wireless - Charlotte, NC\nJanuary 2010 to January 2012\n- Made major contribution to the execution of data distribution BAU project including first on air (FOA), integration support, RE-IPs, planning, and budgeting.\n- Assumed accountability in meeting targeted network milestones and turn-up dates by managing vendors and providing input for capital and expense budgets.\n- Led spending and receiving in compliance with Sarbanes-Oxley Act.', u'Equipment Engineer\nVerizon Wireless - Charlotte, NC\nJanuary 2007 to January 2009\n- Conducted network design, project administration, and equipment implementation for Charleston, Charlotte, Colombia, and Greenville mobile telephone switching office (MTSO).\n- Took part in budgeting and overall planning in alignment with network growth.\n- Performed key tasks, including creating PO, processing payments, verifying payment execution with Financial Team while providing all other necessary documentation required by Verizon Wireless internal processes.\nCareer Highlight:\n\u2713 Received Employee of the Quarter Award for excellent work performance and outstanding efforts in ensuring customer expectation in 2008.\n\u2713 Controlled and allocated $60M budget for several switch upgrades within various mobile switching centers, which involved monthly forecasting, maintaining expenses within budget constraints, collaborating with vendors, and assuming project management role.\n\nEarlier Positions Held:\nVerizon Wireless - Various Locations\nSwitch Technician Greensboro, NC\nEquipment Engineer, Switch Annapolis Junction, MD\n\nBell Atlantic Mobile - Various Locations\nField Engineer, Switch Chantilly, VA; Rochester, NY\nField Engineer, Cells Chantilly, VA\n\nLoral Quintron - Various Locations\nMaintenance Technician II Chantilly, VA\nProduction Engineer, B-52 Weapons System Trainer Chantilly, VA\nField Engineer, 3A/B Flight Simulator NAS Willow Grove, PA\n\nBethlehem Steel - Bethlehem, PA\nElectrical Engineer, Electrical Mechanical Maintenance Department']","[u'Bachelor of Science in Computer Information Systems', u'in Mathematics', u'in Electrical Engineering', u'Associate of Applied Science in Electronics Engineering']","[u'Strayer University Charlotte, NC', u'George Mason University Fairfax, VA', u'Drexel University Philadelphia, PA', u'Montgomery Community College Blue Bell, PA']"
0,https://resumes.indeed.com/resume/b732e361ad6fda8b,"[u""Network Engineer\nPoint 72 - Stamford, CT\nOctober 2017 to Present\n\u2022 Migration of end of life Cisco CUCM from 8.6 to 11.5 and ongoing support\n\u2022 Migration of Cisco Unity and ongoing support\n\u2022 Migration of end of life BT ITS turret to Web based Cloud 9 application and ongoing support\n\u2022 Migration of Speakerbus hoot and hollar network to Web based Cloud 9 application and ongoing support\n\u2022 Extensive documentation maintenance of the firm's policies, practices, policies, technical documents and vendor contact lists"", u""Engineer\nPepsiCo - Purchase, NY\nFebruary 2016 to October 2017\n\u2022 Team member of migrations and implementations of Cisco to Avaya\n\u2022 Onsite support of PepsiCo's VoIP and LAN/WAN network\n\u2022 Contact Center and Vector programming for World Headquarters Call Center\n\u2022 Vector programming for various on call departments and after hours notification changes\n\u2022 Provide daily support of Avaya Aura IP Platform. Modular Messaging and Call Manager system\n\u2022 Provide daily support of Cisco CUCM and Unity\n\u2022 Moves adds and changes, LAN/WAN monitoring\n\u2022 Provide daily support of Desktop moves, adds and changes\n\u2022 Provide System and Network documentation"", u'Engineer\nWells Fargo - New York, NY\nNovember 2014 to February 2016\n\u2022 Migrations to CUBE, UCCX, SIP trunking and Gateways\n\u2022 Provided onsite support of various offices within the New York City.\n\u2022 Accomplished the migration from Avaya TDM to Cisco CUCM and Unity VoIP systems\n\u2022 Trading room support of Siemens ip turrets, Cisco CUCM and Unity consisting of system programming, moves adds and changes.\n\u2022 Support of a 2,000 station environment over four clusters of Cisco 7900 series phones consisting of programming, moves, add and changes, including Unity voicemail, CUBE and SBC.\n\u2022 Set up and support of Cisco Telepresence, Jabber, Video and audio conferencing\n\u2022 Administrative duties include Project Management, system documentation and technical drawing maintenance.\n\nEMPLOYMENT HISTORY (continued):', u""Manager, Voice and Data Services\nTullett Prebon Holdings - Jersey City, NJ\nJanuary 1990 to March 2014\n\u2022 Technical duties include the installation and maintenance of Avaya and Nortel PBX's, IPC trading systems as well as Cisco and Speakerbus ip phones and turrets, encompassing six North American offices, including our Disaster Recovery location in Piscataway, NJ. Additional responsibilities include short and long term network design and implementation, infrastructure management and documentation\n\u2022 Managed the operation of all Data Centers, provisioning of electrical, cabling and equipment housing requirements for multiple internal departments, router and server monitoring, performing periodic generator and UPS testing and management of the electrical requirements.\n\u2022 Project Management of all IT related projects required to keep pace with changing market conditions and/or personnel, including major broker and administrative personnel moves, adds & changes while utilizing internal staff and external vendors. Achieved multiple positive results for managing many voice and data circuit cutovers for cost saving measures and relocations, utilizing my vendor relationships as well as providing final documentation to production.\n\u2022 Served as a main point of contact during major outages as well as providing hands on support to quickly restore services, utilizing various vendor contacts, ensuring reliability and performance of company systems. My strong vendor relationships minimize down times considerably\n\u2022 Managed staff of technicians and administrative personnel to resolve everyday problems quickly and efficiently, in-house and in the field.\n\u2022 Accomplished migrations from Centrex to Avaya and Avaya to Cisco.\n\u2022 Administrative duties include cost/benefit analysis, budget preparation, inventory control, employee evaluation reports, technical report writing, training and guidance, ongoing technology evaluations as well as assisting the billing department, maintaining and providing circuit documentation for the purpose of contract negotiations.""]",[u'Bachelor of Business Administration in Business Administration'],"[u'Iona College New Rochelle, NY\nJanuary 1976 to January 1980']"
0,https://resumes.indeed.com/resume/737335f8e02b3ea6,"[u'Data/Planning Officer\nNigeria aviation handling company(nahcoaviance)\nJanuary 2012 to January 2016', u'Maintenance Engineer\nNigeria aviation handling company (nahcoaviance)\nJanuary 2008 to January 2012', u'Field Engineer\nFederal Airport Authority of Nigeria\nSeptember 2006 to September 2007']","[u'B. Eng', u'Ordinary National Diploma (OND)']","[u'Nnamdi Azikiwe University Anambra State Nigeria', u'Federal Polytechnic Nekede Imo State, Nigeria']"
0,https://resumes.indeed.com/resume/372f092580c3478d,"[u'Data Analyst\nFujitsu - Richardson, TX\nMay 2017 to October 2017\nPricing analysis for new product to improve price points.\nVisualization and reporting to aid in quick business decision making.\nEngineered the analysis framework from scratch thus helping improve the pre sales team with better data analysis\nWorked with product owners to improve product offering', u""Test Engineer\nInfosys Ltd - Pune, Maharashtra\nMay 2014 to June 2016\nSpearheaded testing of custom invoice application on SAP platform with 100% defect identification\n\u25cf Validated business reports and business objects in SAP BI/BW environment handling data from 3 different sources, facilitating 10+ critical business applications\n\u25cf Developed sales orders to cover a span of retail buying scenarios enhancing customer experience\n\u25cf Designed and enhanced test cases based on functional & technical documents, increasing reusability by 30%\n\u25cf Recognized among top 10% performers and awarded the 'Tech Domain Champion' based on high quarterly achievements and client appreciations (US based)""]","[u'M.S. in Business Analytics', u'Bachelors of Technology in Electronics and Communication Engineering']","[u'The University of Texas at Dallas Dallas, TX\nMay 2018', u'Indian Institute of Information Technology\nMay 2014']"
0,https://resumes.indeed.com/resume/ab1c939a322073b8,"[u'Sr. Data Engineer\nTMT Client\nSeptember 2016 to Present\nSep 2016 - till now)\nExtracting data files in order and transform the data as per the business requirement and loading the data in to the data lake and to further create an analytics data warehouse on Azure and further rollup wise DataMart to support reporting on top of this for various business requirements. This project involved a variety of finance and Sales related data sources in the form of XML and Flat file.\nTools: UNIX Shell Scripting. Java, Talend for Big Data, Hadoop, Hive, Azure\nRole: Sr. Data Engineer\nResponsibilities:\n\u2022 Architecting the ELT data pipelines for handling continuous processing of data files of high size data.\n\u2022 Worked on release & Go-live of the project.\n\u2022 Worked production issues and fixing the defects on Talend.\n\u2022 Working on re-deployment of code fixes to talend and scripts to hdfs.\n\u2022 Task automation on stats collection of the batch load of EDL pipeline using shell script and hive.\n\u2022 Working on manual data reprocessing for data catch up, missing files, & for data issues.\n\u2022 Identifying performance issues and bottlenecks for Talend jobs and hive hqls.\n\u2022 Working on Talend administration, creating projects, job scheduling etc.\n\u2022 Extensively used Talend BigData components like thdfsexist, tHiveCreateTable, tHiveRow, thdfsinput,\nthdfsoutput,tHiveload, thdfscopy, thdfsdelete, thdfslist, thdfsconnection, thiveconnection, tHiveClose,\ntHdfsget, tHdfsPut, tHdfsproperties, tHdfscompare, tHdfsrename, tHbaselnput, tHbaseOutput, tSqoopExport,tSqooplmport, tSqooplmportAllTables, tSqoopMerge.\n\u2022 Used various Talend components like tFilterRow, tMap, tJoin, tPreJob, tPostJob, tFileList, tSplitRow,\ntAddCRCRow, tJava, tAggregateRow, tDie, tWarn, tLogRow, etc\n\u2022 Performing some admin activities such as JobServer Configuration, Command Line, Project Creation,\nAssigning user Access and Job Scheduling etc.', u'Consultant\nDeloitte Consulting Pvt. Ltd\nAugust 2013 to Present', u""Sr. Data Engineer\nBuilding Recommendation Engine (Retail Client)\nMay 2016 to August 2016\nMay 2016 - Aug 2016)\nProducts recommendation engine is set up for making the personalized recommendations for the customers. It predicts what products the customers are looking to buy based on their previous buying history, browsing data, wish-lists, demographic information and income group. Content based filtering and Collaborative filtering are implemented then to generate the product recommendations to the customer.\nTools: Python, Spark, MLLib, Item-based and User-based recommendation algorithms, AWS S3, EMR\nRole: Sr. Data Engineer\nResponsibilities:\n\u2022 Refining the problem usecase and laying the solution architecture.\n\u2022 Building design and development of products recommendation engine.\n\u2022 Creating the data ingestion and data processing pipeline and storing the data on Hadoop cluster.\n\u2022 Analyzing the data and filtering the data by multiple approaches such as collaborative filtering and Content based filtering and clustering.\n\u2022 Building and training models for products recommendations using user's profile data, Buying history & current time of the year and popularity using spark MLLib library.\n\u2022 developing the spark code using python for testing the model and finding the right model and then delivering the recommendations.\n\u2022 Running the solution and monitoring the job."", u""Data Engineer\nFMCG Client\nJanuary 2016 to April 2016\nJan 2016 - Apr 2016)\nThis application integrates hive queries in HQL for analyzing Weblog data and obtaining different conclusions based on pre-defined conditions. As the outcome of the project, this analysis is going to identify which location on the web where the customers are visiting and what other actions they are performing on the website and their navigating behavior of webpage visitors in terms of order of pages visited, sequence of mouse click for each visitors and similar analytical tasks.\nTools: HDFS, Kafka, Spark, Spark Streaming, Parquet, Apache Avro, Map Reduce, Pig, Hive, Hue\nRole: Data Engineer\nResponsibilities:\n\u2022 Writing UNIX shell scripts to move data into HDFS from NFS.\n\u2022 Examined the logfiles and defined the job flows.\n\u2022 Developed Custom Partition Map Reduce program based on business logic.\n\u2022 UDF's were developed using Mapreduce java for collecting, analyzing and reporting aggregate data.\n\u2022 Developed Pig Script to eliminate duplicates and sort the records.\n\u2022 Created Hive External tables to store pig processed partitioned output and developed hive queries for analysis of data."", u'Data Engineer\nFMCG Client\nJuly 2015 to December 2015\nJul 2015 - Dec 2015)\nEstablishing and performing sentiment analysis on social media data and integrating the same with the transaction data from SAP HANA at Qlikview with very informative visualizations. It was achieved utilizing Hadoop framework and its eco-system tools as part of solution implementation and extracting Twitter and Facebook data for processing Sentiment Analysis results. Currently, this exercise was performed for Nestle and its brands like Cerelac, Milo, Nescafe, KitKat, Maggi etc. This solution implementation helps, increase in sales and customer satisfaction.\nTools: Python, Flume, Hdfs, Hive, Qlikview, SAP HANA, Lexicon-based Sentiment Analysis\nRole: Data Engineer\nResponsibilities:\n\u2022 Configured flume listener to retrieve data from twitter and stream into HDFS specified path.\n\u2022 Written python code using Graph api to fetch data from Facebook and writing it to HDFS as json data format.\n\u2022 Involve in creating the PIG scripts to process the HDFS data and calculate sentiment.\n\u2022 To stage and query the json data, Created Hive managed, External tables, Views to store the processed data and integrate them to reporting tool Qlikview.\n\u2022 Performing the Sentiment Analysis. We can create views in Hive that can be used in the sentiment calculations to get the polarity by joining already existing table having a list of positive and negative words.\n\u2022 Connecting SAP HANA with Qlikview Data source to pull Sales data.\n\u2022 Joining Hadoop Hive (Sentiment) & SAP HANA (Sales, Promotions etc.) in Qlikview to make Dashboards and Story about use cases', u""Data Engineer\nSearch Engine\nJanuary 2015 to June 2015\nusing Solr and Hadoop (Asset Management Client) (Jan 2015 - Jun 2015)\nThis project is about building a basic song search engine that takes user-defined query and lists down the songs that match the qualifying category. The data unstructured and was available in multiple HDF5 files. The project involved loading the data to Hdfs and converting it to CSV and implementing a querying engine using Hive and impala. The data was also loaded in Hbase and built search and recommendation engine on top of that.\nTools: Hdfs, Flume, Hive, Hbase and Solr.\nRole: Data Engineer\nResponsibilities:\n\u2022 Configured flume listener to retrieve data files and stream into HDFS specified path.\n\u2022 Writing a PIG/Hive scripts to load data to hive depending upon the file types.\n\u2022 Import MySQL data into solr using DataImportHander.\n\u2022 Configuring Solr on HDFS\n\u2022 Worked on SOLR and spearheading the index and setting up text based search related development work.\n\u2022 Creating collections in Solr for indexing data.\n\u2022 Created facets for the asset management back-office system for accounting and investments and every day's trade files.\n\u2022 Implementing powerful auto-suggest feature and configuring the high availability multi-core servers using replication, request handlers, analyzers and tokenizers.\n\u2022 Performance tuning for search operation implemented using faceted search and autocomplete functionalities.\n\u2022 Experience using multi-core solr server for indexing and un-indexing.\n\u2022 Grouping results using functions, queries, field values.\n\u2022 Used tokenizers for special characters and wildcards."", u""Data Engineer\nPerformance Analytics and Accounting\nJune 2013 to December 2014\nsystem (Asset Management Client) (Jun 2013 - Dec 2014)\nExtracting data from various Accounting systems for asset management and Trading Company to transform the data as per the business requirement and generate reports for the business partners and users. This project involved a variety of accounting and investment related data sources in the form of XML, Flat file and tables. In this Project, we have variety of Source data format such as Flat files, Relational data and XML. The role in the project also involved adhoc business users' requirements pertaining to VBA, UNIX and Informatica and other technology stuffs.\nTools: Informatica, Unix Shell scripting, SQL/Oracle, MS excel, XML and Tivoli workload scheduler.\nRole: Data Engineer\nResponsibilities:\n\u2022 Worked on development of requirements, code design and build.\n\u2022 Developing Mapping, session and workflow in Informatica to generate various extracts based on the business requirements.\n\u2022 Testing the ETL in development and performing UAT and getting the release sign-off from the business.\n\u2022 Performance tuning Informatica ETL, SQL queries and Shell Scripts.\n\u2022 Writing instructions to add new ETL jobs in TWS and making various changes in Job scheduling in TWS and submitting adhoc jobs and perform the troubleshooting.\n\u2022 Preparing and updating application service manual and unit test case and migrating the code or the files from one environment to the other.\n\u2022 Resolving production issues and fixing the defects.\n\u2022 Monitoring the jobs to fix the failures and analyzing the task statistics for performance optimization.\n\u2022 Performing System integration testing, deployment and providing production support during warranty.\n\u2022 Maintaining and tracking everyday development status of the team. Making and maintaining Review checklist and Deployment checklist.\n\u2022 Worked on XML parser transformation and Java Transformation to trace the Xpath of a given element in the XML."", u'Software Engineer\nAccenture Services Pvt. Ltd\nJune 2011 to August 2013', u""Sr. ETL Engineer\nlarge Insurance Client\nApril 2012 to May 2013\nApr 2012 - May 2013)\nExtracting data from various Accounting systems for asset management and Trading Company to transform the data as per the business requirement and generate reports for the business partners and users. This project involved a variety of accounting and investment related data sources in the form of XML, Flat file and tables. In this project, we have variety of Source data format such as Flat files, Relational data and XML. The role in the project also involved adhoc business users' requirements pertaining to VBA, UNIX and Informatica and other technology stuffs.\nTools: Informatica, Unix Shell scripting, SQL/Oracle, MS excel, XML and Tivoli workload scheduler.\nRole: Sr. ETL Engineer\nResponsibilities:\n\u2022 Working with the Team on development requirements, priorities and working on scope and determining estimates.\n\u2022 Developing Mapping, session and workflow in Informatica for new tables and modification in the rule for old tables in DW/DM as per the requirements.\n\u2022 Testing the ETL in development and UAT environments also.\n\u2022 Writing complex SQL queries, performance tuning and designing the ETL while loading DWH.\n\u2022 Writing UNIX scripts to add new ETL jobs or perform some task.\n\u2022 Making the Design document and unit test case and migrating the code or the files from one environment to the other.\n\u2022 Analyzing, Debugging and fixing the production defects/issues that arouse during the data warehouse loading.\n\u2022 Monitoring the jobs to fix the failures and analyzing the task statistics for performance optimization.\n\u2022 Requirement Understanding and preparing technical design (Both HLD and LLD) for ETL development and report generation. Making and maintaining the project documents for deliverables.\n\u2022 Making mapping sheets from BTRD and in some case validating the mapping sheets with business rules.\n\u2022 Performing System integration testing, deployment and providing production support during warranty.\n\u2022 Maintaining and tracking everyday development status of the team. Making and maintaining Review checklist and Deployment checklist.\n\u2022 Worked on XML parser transformation in Informatica to pull transaction data to populate staging tables."", u'ETL Engineer\nHealth Insurance Services\nAugust 2011 to March 2012\nprovider) (Aug 2011 - Mar 2012)\nThis project involved Informatica for sourcing data from Teradata EDW and generating various extracts out of that. The other module of this project involved Data conversion where in the source data was coming from mainframe system in XML format and was required to be loaded in the data mart.\nTools: Informatica, Unix Shell scripting, SQL/Teradata, Tivoli workload scheduler.\nRole: ETL Engineer\nResponsibilities:\n\u2022 Worked on Requirement understanding and refinement and preparation of technical requirement and perform the impact analysis in the existing design.\n\u2022 Worked High level design of various business reports such as claims report, pharmacy reports and Medical eligibility reports etc.\n\u2022 Worked on the low-level design and development of Mapping, session and workflow in Informatica in order to generate various extracts based on the business requirements.\n\u2022 Writing complex queries in Teradata and work on the performance tuning of them.\n\u2022 Working on various utilities of Teradata and using them in Informatica ETL.\n\u2022 Perform the entire documentation of the project from the Technical design document till final user manual for production support team.\n\u2022 Worked on another Data Migration project in the same industry involving Pentaho Data Integration tool - Spoon as ETL tool, XML from mainframes as source and Oracle database as Target.\n\u2022 Worked building ETLs in PDI involving Transformation, steps and Jobs.\n\u2022 Worked PLSQL stored procedures to for loading the XML in to the Oracle table and the pulling the XML from Oracle column to load various other staging tables in Oracle.\n\u2022 Utilized the macros to generating more than 50 stored procedures for loading various staging tables, thereby reducing the time and saving a lot of it.']","[u'B.Tech in Computer Science Engineering', u'Certification of Appreciation']","[u'K.I.I.T UNIVERSITY\nMay 2011', u'KIIT UNIVERSITY']"
0,https://resumes.indeed.com/resume/7ce24194d3769c6b,"[u'Senior Software Engineer\nLockheed Martin E.O - Grand Prairie, TX\nJanuary 2008 to Present\n\u2022 Designed and developed forward-thinking systems that meet user needs and improve productivity.\n\u2022 Performed all testing and troubleshooting methods and documented resolutions in the system.\n\u2022 Worked closely with other departmental peers to develop high availability solutions for mission-critical applications.\n\u2022 Worked closely with customers to efficiently resolve issues.\n\u2022 Reviewed code and corrected errors.\n\u2022 Worked with project managers, developers, quality assurance and customers to resolve technical issues.\n\u2022 Estimated work hours and tracked progress using Agile/SCRUM methodology.\n\u2022 Updated and fixed existing software and system applications.\n\u2022 Collaborated with team members to create applications system analysis based upon client requirements.\n\u2022 Improved system performance by making proactive adjustments and resolving bugs.\n\u2022 Created proofs of concept for innovative new solutions.\n\u2022 Devised tracking software with modules for inventory monitoring, customer relationship management, staff administration\nand the generation of reports.', u""Data Warehouse Analyst\nLockheed Martin E.O - Grand Prairie, TX\nJuly 2006 to January 2008\n\u2022 Developed and implemented complex Internet and Intranet applications.\n\u2022 Provided continued maintenance and development of bug fixes and patch sets for existing web applications.\n\u2022 Ensured network, system and data availability and integrity through preventative maintenance and upgrades.\n\u2022 Implemented company policies, technical procedures, and standards for preserving the integrity and security of data,\nreports, and access.\n\u2022 Designed strategic plan for component development practices to support future projects.\n\u2022 Designed Intranet portal serving as department's main website for latest local and enterprise news and happenings.\n\u2022 Aligned office departments and increased inter-department communication and data sharing."", u""Quality Software Engineer\nLockheed Martin E.O - Camden, AR\nAugust 2004 to July 2006\n\u2022 Performed all testing and troubleshooting methods and documented resolutions in the system.\n\u2022 Worked closely with other departmental peers to develop high availability solutions for mission-critical applications.\n\u2022 Worked closely with customers to efficiently resolve issues.\n\u2022 Collaborated with team members to create application's system analysis based on client requirements.""]","[u'Masters of Science in Information Systems', u'Bachelor of Science in Computer Science']","[u'DePaul University Chicago, IL\nJanuary 2008', u'University of Arkansas Fayetteville, AR\nJanuary 2004']"
0,https://resumes.indeed.com/resume/96decea3f6039cd8,"[u'Quality Engineer\nDrive DeVilbiss Healthcare - Port Washington, NY\nOctober 2014 to Present\n\u2022 Perform root cause analysis on product failure and present potential solutions in accordance with ISO 13485 quality management system (i.e.: nerve stimulation units, pulse oximeter, blood pressure monitors, nebulizers, and power scooters)\n\u2022 Develop test protocols for new prototypes and analyze\n\u2022 Help trouble shoot with customers as to why their devices are no longer working and provide them with options of replacing', u'Data Analyst\nSouth Shore Neurologic Associates - Patchogue, NY\nJune 2012 to August 2013\n\u2022 Helped analyze data in GAIT program for patients with MS as well as different patients with neurological problems\n\u2022 Helped eliminate any inconclusive data for further research and conclusions to be made\n\u2022 Analyzed the progress of the experimental drug on various patients']","[u'High School Diploma', u'Bachelors of Engineering in Biomedical Engineering in Biomedical Engineering']","[u'Sayville High School West Sayville, NY\nMay 2010', u'Stevens Institute of Technology Hoboken, NJ']"
0,https://resumes.indeed.com/resume/7689aa586429afab,"[u""Data Specialist\nIBM India (P) Ltd - Bengaluru, Karnataka\nSeptember 2014 to December 2016\n\u2022 Worked on projects for risk capture and Global Loan Management System for a leading Australian Bank- Australia And New Zealand Bank as ETL lead-handling a team of 5 members.\n\u2022 Extract and process structured/unstructured data from legacy systems, traditional data warehouses and complex data sources. This includes JSON data, XML data, copybooks, hexadecimal data and LoanIQ data.\n\u2022 Direct client interaction for requirement gathering, problem solving, metric and KPI identification, database creation and building operational dashboards to monitor real time functioning of loan recovery processes.\n\u2022 Conducted multiple training sessions on behalf of IBM's learning and knowledge department for training new hires on Database and reporting skills."", u'Senior Systems Engineer\nINFOSYS LIMITED - Pune, Maharashtra\nSeptember 2010 to September 2014\n\u2022 Worked as an ETL and reporting developer and tester, for development, enhancement and support projects for clients Neptune Orient Lines (NOL), Reckitt Benckiser and Nike (US).\n\u2022 Designing ETL framework to fetch large datasets from legacy systems (mainframe system) and storing in databases. This helped tracking of the shipment and estimating the delivery time by analyzing current location, route and mode of shipment using SAP Business Objects reports.\n\u2022 Identifying metrics and KPIs and their performance to plan and forecast for upcoming fiscal year and represented them using dashboards. Monitoring customer behavior and product safety.\n\u2022 Complete business understanding of sales database of Nike and providing support Global database of Nike, interacted with customers across the globe. Resolved issues related to data availability and sanity.\n\u2022 Worked extensively on SDLC including SIT and UAT, ETL and reporting (visualization) processes followed in the project.']","[u'Master of Science in Business Intelligence and Analytics', u'Bachelor of Engineering in Computer Science']","[u""Saint Joseph's University Philadelphia, PA\nMay 2018"", u'Mumbai University Mumbai, Maharashtra\nJune 2010']"
0,https://resumes.indeed.com/resume/fd7b374f6fc1c89a,"[u'Senior Data Analyst\nHealth Current - Phoenix, AZ\nJanuary 2017 to Present\n\u2022 Model ad hoc and recurrent reports for executive board to represent monthly progress metrics using R, PowerBI & Tableau\n\u2022 Cross-functional collaboration with different teams to develop data standardization and modelling process, for improving\nquality of product services provided for 300+ participants\n\u2022 Lead and coordinate report request process for internal and external client requests, to efficiently track and automate report\nprocess\n\u2022 Developed an automated error monitoring and tracking system mining errors from multiple server databases in the network\n\u2022 Build BI scoreboard and dashboards with operation stats and forecast trends mining real time data of over 7 MM members to detect implementation progress, process data variations and recommend best practice methods to clients\n\u2022 In charge of the alert summary dashboard for clients including Fortune 10 participants providing insights on their patient\npopulation to support their care management process', u'Data Analyst\nHealth Current - Phoenix, AZ\nJune 2016 to December 2016\nDesigned dynamic and interactive dashboards using R Shiny, integrated disparate datasets and facilitated cross-functional\ndepartments to track progress and define opportunities for business process improvement\n\u2022 Transform and map real-time data from over 2500 Arizona providers and over 1.2MM beneficiaries to develop cost and utilization measures and process improvement models to drive value-based healthcare system\n\u2022 Integrated data and work with vendors to automate transition of internal process to CRM to improve customer engagement', u'Project Intern\nAvnet, Inc - Chandler, AZ\nJanuary 2016 to May 2016\nEffectively communicate with leadership and present forecasted demand-delay relationship and proposed optimize\nintegration model to reduce variation between actual and estimated shipment time', u'Consulting Engineer\nKalkitech - Bengaluru, Karnataka\nAugust 2011 to October 2014\nBangalore, India\n\u2022 Integrated data from SQL databases to design reports with the existing fault alert mechanism to recommend corrective and preventive actions and maintain reliability of nation-wide grid\n\u2022 Involved in the project planning and implementation phase, including technical proposal creation, vendor management and customer relationship management\n\u2022 Influenced 70% increase in department revenue from clients upon successfully implementing accelerated data\ntransferability and process automation\n\u2022 Extracted real time raw data from multiple datasets and designed actionable reports for analysing consumer usage patterns for over 240 cities representing approximately 230 million consumers\n\u2022 Developed insightful ad hoc reports and dashboard for clients to ensure proactive monitoring and timely fault detection\n\u2022 Presented project progress with weekly reports and moderated cross-functional discussion to coordinate project activities\n\u2022 Provided training for over 20 site engineers on data collection process, and supervised their project activities\n\nAditi R Datta Page2 aditirdatta@gmail.com']","[u'Master of Science in Business Analytics', u'Bachelor of Technology in Electrical and Electronics Engineering']","[u'W.P. Carey School of Business, Arizona State University\nMay 2016', u'University of Calicut Calicut, Kerala\nJune 2011']"
0,https://resumes.indeed.com/resume/cb595e6d4b78c9a7,"[u'Associate Engineer\nTeknor Apex - Pawtucket, RI\nDecember 2016 to Present\nCreating and revamping P&ID\u2019s with AutoCAD. Designing equipment for a new 3 million dollar vinyl line', u'Regulatory Data Coordinator\nTeknor Apex - Pawtucket, RI\nSeptember 2016 to Present\nData entry for SDS\u2019s, creating NAFTA\u2019s and overseeing inventories.']","[u""Master's of Science in Chemical Engineering in Chemical Engineering"", u'Bachelor of Science in Chemical Engineering']","[u'UMass Lowell Lowell, MA\nAugust 2017 to May 2019', u'University of Massachusetts Lowell Lowell, MA\nJanuary 2012 to January 2016']"
0,https://resumes.indeed.com/resume/d6b7d4738bbf76ad,"[u'Student Data Analyst\nAugust 2017 to December 2017\nLenovo, Raleigh-Durham\n\n\u2022 We build a predictive analytics model to predict the optimal next service actions for the machines based on the history of the machine, cost and repair time.\n\u2022 Developed a stochastic model for machines to identify the movements of machines from one state to another based on repair attempts.\n\u2022 Recommended optimal policies for the company.', u'Industrial Engineer Trainee/ Jr. Engineer\nCoca Cola Company\nJune 2015 to July 2016\nWorked with engineering team to increase efficiency of the supply chain strategies using\noptimization tools.\n\u2022 Assigned to Production Operations under Manufacturing Department where I supervised\nand helped in maintenance of production lines.\n\u2022 Participated in the review of inventory levels and warehouse capacity and worked to maintain proper inventory levels in accordance with minimal logistics cost.']","[u'M.S in INDUSTRIAL AND SYSTEMS ENGINEERING in INDUSTRIAL AND SYSTEMS ENGINEERING', u'B. TECH in MECHANICAL ENGINEERING in MECHANICAL ENGINEERING']","[u'North Carolina State University\nAugust 2017 to May 2019', u'VIT University\nJuly 2011 to May 2015']"
0,https://resumes.indeed.com/resume/8e8b2cf36c13efdc,"[u""DATA CENTER ENGINEER\nUS Colo LLC\nJune 2008 to Present\nE \u2022 Install, configure and maintain server-class hardware\nS \u2022 Install, configure and maintain network infrastructure hardware (switches, routers)\nP \u2022 Perform hardware upgrades and planned replacements of failed components\nO\n\u2022 Work in teams to deploy new data center infrastructure. Install new servers, design rack layout and\nN power allocation\nS\n\u2022 Extend Cross Connections like T-1's, E-1's, DS-3's, Fibers, Internet, FastE Lines, and Phonelines\nI\nB \u2022 Use some Test Equipments like Network Tools, Telecom Tools\nI \u2022 Provide hand-on response to emergencies as directed by other members of the NOC\nL \u2022 Independently handling minor customer emergencies\nI \u2022 Work with 3rd party service providers\nT \u2022 Monitor all DC resources (power, cooling and space) and keep data center facility tidy\nI \u2022 Learning about new and evolving hardware technologies, network/system administration technologies\nE and techniques, and anything else that may relate to expertise in the field of IT.\nS\n25 General Roxas St, Barangay San Jose,"", u'TECHNICAL SUPPORT\nZenshin System Corporation - Manila\nSeptember 2007 to March 2008\nE\nS \u2022 Perform server reboots and consoling\nP \u2022 Assemble Computer/Server\nO\nN \u2022 Perform server hardware check\nS \u2022 Replace broken server components\nI\nB \u2022 Rack equipment and keep data center facility tidy\nI \u2022 Inventory components as they arrive\nL\n\u2022 Troubleshoot, and fix broken servers MS/Linux OS related issues on server\nI\nT\nI\nE\nS']","[u'', u'B.S. in Computer Engineering']","[u'Learnet Academy Los Angeles, CA\nJanuary 2009 to January 2010', u'Don Bosco Technical College\nJanuary 2001 to January 2003']"
0,https://resumes.indeed.com/resume/b89f0224bebe9505,"[u""Data Engineer\nCompas Technology - San Francisco, CA\nJuly 2016 to January 2018\nCompas Technology is a CRM ATS Recruiting Software company which provide staffing agencies a complete recruiting and sales process.\n\n\u2022 Act as one of the primary data migration team members to bring in clients' data into Compas system.\n\u2022 Restore/ Import clients' database backups (that would be in various formats) into Compas platform.\n\u2022 Work with data team in analyzing data and producing meaningful insight and relate to a data warehouse model.\n\u2022 Prepare data quality and data consistency report from clients' database and provide data cleansing recommendations based on data warehouse business rules.\n\u2022 Apply reverse engineering to clients' workflow system and offer them best practices that will help to grow their businesses.\n\u2022 Create and execute complex SQL queries.\n\u2022 Perform data validation and data cleansing using SQL queries and other tools such as Data Quality Services (DQS).\n\u2022 Create/ Execute ETL processes to move data from source to target system, parse data in various formats such as json file, binary file, etc.\n\u2022 Prepare mapping report that identify relationship between fields in source and target systems.\n\u2022 Perform basic query performance tuning.\n\u2022 Optimizing/ Creating Stored Procedures.\n\u2022 Prepare numerous workflow document reports for future reference."", u'Database Engineer\nCodemandu Software\nMay 2011 to May 2013\nCodemandu Software is an offshore counterpart of OpenI Inc. in San Francisco. It specializes in Business Intelligence and Data Analysis and provides end-to-end BI solutions including Dashboard, OLAP cube design, Complex ETL Jobs, Reporting, Dimensional Modeling and Predictive Models.\n\u2022 Maintain data-warehouse with daily and monthly data feeds.\n\u2022 Import various file formats into data-warehouse.\n\u2022 Automate bash script for automatic ETL execution.\n\u2022 Develop, Design and automate ETL as per business requirement using Pentaho Data Integration tool.\n\u2022 Maintain database instances.\n\u2022 Execute SQL queries for data validation.\n\u2022 Develop logics for upgrading tasks.\n\u2022 Data cleansing and data de-duping process using tools such as Melissa Data- Contact Zone and Matchup Box.']","[u'Master of Science in Computer Science', u'Bachelors in Computer Engineering']","[u'California State University East Bay Hayward, CA\nJanuary 2016', u'Tribhuvan University Kathmandu, NP\nJanuary 2011']"
0,https://resumes.indeed.com/resume/865b9bf1f5ce59ee,"[u""Senior Big Data Engineer\nIo-Tahoe (Startup)\nSeptember 2017 to Present\nWorking on design and implementation of highly concurrent, scalable, distributed, and resilient applications to process 200 TB+ data including computation of BitSet and HyperLogLog using Akka, Kafka, Spark, Hbase.\n\u25cf Designed and implemented analytics framework to process 10+TB Client's data using Kafka, Spark streaming Scala API, Redis, and Hbase.\n\u25cf Led re-design of the core framework to scale out Io -Tahoe applications data processes using Spark and HBase, which is able to compute BitSet, HyperLogLog, and other statistics on table columns in parallel fashion; load those metrics into Hbase as storage which will provide fast data retrieval for further computations\n\nTechnologies Used: Scala, Java 8, Python, NLP, Pandas, NumPy, Redis, Spring, Spring Boot, Spark Streaming, Akka, Kafka, Hbase, Spark 2, AWS EC2, S3, Redshift, EMR."", u'Big Data Engineer (Consultant)\nJPMorgan Chase & Co\nFebruary 2017 to September 2017\nDesigned and implemented data ingestion Spark streaming framework from various data source like REST API, Kafka using Spark Streaming Scala API and Kafka.\n\u25cf Developed Cyber Analytics Workbench framework, which protects JPMChase clients from phishing attempts. Designed and implemented real-time Spark streaming applications integrated with Kafka to handle large volume and velocity data streams in a scalable, reliable and fault tolerant manner; implemented Kafka offset management and application monitoring framework to monitor data inflow using Scala, Python, Spark streaming, and Kafka.\n\u25cf Developed backup and restore application to transfer data from various data sources to private Cloud using Java Concurrency framework, AWS S3 API, and Hadoop HDFS API.\n\u25cf Developed Data Quality (DQ) framework to ensure data validity and consistency for consumption by downstream applications using Spark Scala API.\n\u25cf Developed a string similarity spark application for matching company names from multiple source using edit distance algorithms and Spark Scala API; tuned the spark jobs for optimal efficiency by increasing parallelism and reducing shuffles; implemented a spelling check engine using Python NLP to automatically correct misspelled company names.\n\nTechnologies Used: Scala, Java 8, Python, Hive, Impala, HBase, NLP, Pandas, NumPy, Flask, Unix shell, Apache Spark 2, Spark Streaming, Kafka, Cloudera CDH, Docker.', u'Lead Big Data Engineer (Consultant)\nNBC Universal\nAugust 2015 to February 2017\nDesigned and implemented data ingestion Spark streaming framework to pull data from multiple data sources (Facebook, Youtube, Instagram, and Twitter) using REST API, Spark streaming, PySpark, and Python.\n\u25cf Designed and implemented big data ingestion pipelines to ingest multi PB data from various data source using Kafka, Spark streaming including data quality checks, transformation, and stored as efficient storage formats like parquet.\n\u25cf Performing data wrangling on Multi-Terabyte datasets from various data sources for a variety of downstream purposes such as analytics using PySpark.\n\u25cf Built a POC sentiment analyzer engine using NLP (Natural Language Processing) and Python.\n\u25cf Developed an internal search engine to look up news transcripts for NBC news shows, by creating a python parser to convert those transcripts to JSON documents and loading into Elasticsearch; implemented automated data ingestion process for indexing new transcripts as they become available.\n\nTechnologies Used: Scala, Python, Java, PySpark, Hive, Pig, NLP, Pandas, NumPy, Unix shell, Apache Spark, Kafka, AWS EC2, S3, Redshift, EMR , Elasticsearch,', u'Lead Big Data Engineer (Consultant)\nADP\nOctober 2014 to August 2015\nDesigned and implemented Big Data analytics platform for handling data ingestion, compression, transformation and analysis of 30+ TB payroll and HR data. Implement automation, traceability, and transparency for every step of the process to build trust in data and streamline data science efforts using Python, Java, Hadoop streaming, Spark, Spark SQL, Scala, Hive, and Pig.\n\u25cf Designed highly efficient data model for optimizing large-scale queries utilizing Hive complex data types and Parquet file format.\n\u25cf Performed data validation and transformation using Python and Hadoop streaming.\n\u25cf Developed highly efficient Pig Java UDFs utilizing advanced concept like Algebraic and Accumulator interface to populate ADP Benchmarks cube metrics.\n\u25cf In order to improve performance, scalability, and memory usage of processing large volume of payroll data, adopt Spark and Spark SQL to build ADP Benchmarks cubes and populate cube annual and quarter metrics using Scala.\n\u25cf Perform Big Data analysis using Scala, Spark, Spark SQL, Hive, Mlib, Machine Learning algorithms.\n\nTechnologies Used: Scala, Python, Java, Pig, Hive, Unix shell, Apache Spark, Spark SQL, PySpark, CDH 5.3, Red Hat Enterprise Linux.', u'Big Data Engineer/Data Architect (Consultant)\nLightPath Cablevision\nApril 2014 to October 2014\nDesigned and developed an automatic data processing pipeline, including data ingestion, compression, transformation, loading data into Hive database to process CDR (call detail records) using Unix Shell, Hadoop streaming, Python, Java, Hive, Pig, and UDFs.\n\u25cf Performed data profiling and transformation on the raw data using Pig, Python, and Java\n\u25cf Designed efficient data model for loading transformed data into Hive database.\n\u25cf Created analytics reports using Hive.\n\u25cf Lead architecture and design of data processing, warehousing and analytics initiatives.\n\u25cf Analyzed the business requirements and data, designed, developed and implemented highly effectively, highly scalable ETL processes for a fast, scalable data warehouses.\n\u25cf Developed various Oracle SQL scripts, PL/SQL packages, procedures, functions, and java code for data\n\u25cf Extraction, transformation, and data load.\n\u25cf Performed SQL Query and Database tuning for high BI reporting performance.\n\nTechnologies Used: Python, Java, Hive, Unix shell, Hadoop, ETL(Talend), Oracle SQL, PL/SQL, Oracle 11.2g, Red Hat Enterprise Linux', u'Big Data Engineer/ETL Architect\nEmerging Health, Montefiore Information Technology-CLG\nFebruary 2013 to April 2014\nBuilt Big Data analytical framework for processing healthcare data for medical research using Python, Java, Hadoop, Hive and Pig. Integrated R scripts with MapReduce jobs.\n\u25cf Perform data transformation using Pig, Python, and Java\n\u25cf Designed efficient data model for loading transformed data into Hive database.\n\u25cf Created analytics reports using Hive.\n\u25cf Designed, developed, and implemented conceptual, logical and physical data models for highly scalable and high performance relational database systems.\n\u25cf Designed, developed, and implemented highly scalable and efficient ETL flows using SSIS.\n\u25cf Extensive experience in tuning SQL query and multi-terabyte databases for high performance; Utilized new features of SQL Server 2012, such as Columnstore indexes, table partitioning to increase performance in data warehousing.\n\u25cf Developed various T-SQL Scripts, Stored Procedures, and UDFs, C# code for data extraction, transformations and data load.\n\u25cf Performed SSIS tuning for efficiency ETL and utilized best practices SSIS design pattern to improve ETL performance and scalability.\n\nTechnologies Used: Python, Java, Hadoop Pig and Hive, Unix shell, T-SQL, Oracle SQL, PL/SQL, Oracle11g, SSIS/SSRS/SSAS 2012/2008R2, Microsoft SQL Server 2012/20008R2, Visual C#.NET, Sybase15.5, CentOS', u'Big Data Engineer\nMount Sinai Medical Center - New York, NY\nMarch 2001 to February 2013\nDepartment of Information Technology', u'Senior BI/Database Developer/DBA\nMount Sinai Medical Center\nJanuary 2006 to January 2012\nDesigned, developed, and implemented SSIS framework and templates for ETL efficiency.\n\u25cf Designed, developed, and implemented robust, efficient ETL for processing of healthcare data from various sources into Data Warehouse, ODS, Data Marts using SSIS.\n\u25cf Led design, development, implementation of SSRS reports and development and management of the Microsoft BI reporting environment.\n\u25cf Extensive experience in SQL programming, stored procedure creation and optimization as well as tuning and maintenance of highly available and highly transactional databases.\n\u25cf Designed, developed, and implemented conceptual, logical and physical data models for analytics reporting with scalability in mind.\n\u25cf Developed various SQL scripts, Stored Procedures, UDFs, views, triggers, and tables for data extraction, transformations and data loads.\n\u25cf Extensive experience in tuning SQL query and Multi-Terabyte database for high performance.\n\nTechnologies Used: T-SQL, SSIS/SSAS/SSRS 2012/2008R2, Windows PowerShell, Visual C#.NET, ASP.NET, SQL CLR, XML, Microsoft SQL Server 2000/2005/2008R2/2012, Crystal Report', u""Software Engineer\nDepartment of Radiation Safety and Neuroscience P.E.T.Lab\nJanuary 2001 to January 2006\nDeveloped Radioisotope Inventory Web application using Java, JSP, and Oracle; designed a user-friendly Web interface using HTML and JavaScript.\n\u25cf Performed the logical and physical data model design to support the development of Web Based Radioisotope Inventory Control System.\n\u25cf Performed SQL, database tuning and optimization to increase the performance of application and data retrieval.\n\u25cf Developed ETL processes and implemented Perl and SQL*Loader scripts to load data from legacy Informix database to Oracle database\n\u25cf Implemented reporting solutions using Crystal Report\n\u25cf Designed and developed software using C, java, and Perl to facilitate manipulation of brain images created with PET (Position Emission Tomography) and MRI (Magnetic Resonance Imaging) modalities for laboratory's research in brain aging and development.\n\u25cf Performed administration duties of SunOS/Solaris UNIX systems that supported PET and MRI modalities; Developed automation shell scripts for system and data management.\n\nTechnologies Used: Java, JavaScript, C, JSP, Perl, HTML, CSS, SQL, PL/SQL, Oracle 9i/10g, Crystal Report, Unix shell, Red Hat Linux, SunOS/Solaris""]","[u'Master of Arts in Computer Science', u'Bachelor of Science in Medicine']","[u'Queens College of the City University of New York', u'FuJian Medical College FuZhou, CN']"
0,https://resumes.indeed.com/resume/dd524a6632879fe1,"[u'Data Analyst\nCF Advance Corp - Industry, CA\nOctober 2015 to Present\nProjects/Assignments: Internal business system and DWH system\nPlatform and tools\uff1a MySQL v5.7, MS SQL Server 2012, Tableau 10.4,\nTalend Open Studio for DI V6.4.1, PHP v7.1, Java\nSummary:\nSupport company to optimize internal processing by performing data normalization, data modeling and dimensional data analysis on inventory, packaging standard and historical sales data. Mastered the front and back end business knowledge of e-Commerce industry.\n\u25c6 Applied data normalization on business data, conducted data modeling for internal system and dimensional data modeling for ETL jobs in internal DWH.\n\u25c6 Developed views and stored procedures for internal system, did some of further data analysis in Excel using functions and pivot table.\n\u25c6 Helped to develop internal systems for operational purpose, provide queries on internal data such as products details, inventory, packaging standard, shipment, online orders and warranty/return data, helped our company to query referential data and operate more efficiently and smoothly.\n\u25c6 In order to get data-based perspective to help business decision making, building DWH/ETL jobs performing data extraction, profiling, cleansing, de-duplication, transformation and loading, archiving and reporting, to collect sales measure data based on categories, time and customer location dimensions, monitor sales trends and defects, improve purchasing process by increasing fast moving items and decreasing defective parts, and to develop report views to represent analysis results.\nKey Responsibilities:\n\u25c6 Identify Use Cases from business requirements and write Use Case Specifications.\n\u25c6 Create DB logical and physical design documents for internal system.\n\u25c6 Create dimensional data modeling documents for ETL jobs.\n\u25c6 Prepare data mapping documents and programing ETL jobs.\n\u25c6 Participating in the Peer review on design documents, code and Unit test cases\n\u25c6 Participating in the project report meetings and provide feedback to department manager for further decisions.\n\u25c6 Facilitating weekly and monthly status review meeting with the manager.\n\u25c6 Monitoring the performance of data processing, work out optimization plan.\n\u25c6 Creating internal training materials for business users of internal system.\n\u25c6 Ensuring that all deliverables will be done with high quality on time by following best practices/processes/standards and providing weekly status to manager.', u""Senior Data Engineer\nIBM Japan Services Company Ltd\nMarch 2013 to July 2015\nProjects/Assignments: LCR Data Mart project, NSFR Data Mart project\nPlatform and tools\uff1a UNIX Shell Scripting, Oracle 11g, Informatica PowerCenter V9.1, IBM InfoSphere DataStage V8.5 / V8.7\nSummary:\n\u25c6 Developed and sustained ETL mappings/jobs and Business Intelligence (BI) features for bank companies (Banking) in Japan. Ensured LCR and NSFR reports were generated accurately by conducting job monitoring and troubleshooting.\n\u25c6 The projects' goal were to build LCR(Liquidity Coverage Ratio) and NSFR (Net Stable Funding Ratio) data mart, and provide monthly LCR and NSPR reports. DWH system collects all transaction data from domestic, oversea and marketing departments, and transfer transaction data into monthly reports. Various transaction data were imported into source DB, then according to business specifications, data in source DB was transformed into standard format and was set financial product flags, then standardized financial records would be imported into Financial database, and aggregated by financial categories to get Available Amount. Cash flow data was transferred as well, and was imported and transformed in similar way to get Required Amount. Finally, LCR and NSFR reports was generated based on Available and Required Amount of stable funding.\nKey Responsibilities:\n\u25c6 Did Impact Analysis and revised High Level and Low Level design documents base on the Business Requirements and provided proper estimates for the Data Warehousing project.\n\u25c6 Designed and developed ETL mappings/jobs on Informatica or DataStage to achieve the desired functionality, initiated peer reviews.\n\u25c6 Prepared test cases and test data for Unit testing.\n\u25c6 Monitored scheduled jobs on Informatica PowerCenter and IBM DataStage Workflow monitor.\n\u25c6 Resolved technical issues of project and do trouble shooting on the incidents and failures happened in production environment.\n\u25c6 In the event of load failure/system unavailability, handled communication with business user and prepared recovery plan.\n\u25c6 Ensuring all the Performance Optimization techniques were incorporated while developing the Mappings. Improved the performance related to long running jobs.\n\u25c6 Participating in the release management, deployment checklist review and defect management of each release.\n\u25c6 Mentored new members to understand system, created extensive documentation on the design, development, implementation, loads and process flow of the data processing."", u""Senior Data Engineer\nIBM China Global Delivery Center - Dalian, CN\nJuly 2006 to March 2013\nProjects/Assignments: DWH system AMS (Pharmaceutical), EDWH Fitch & Bloomberg\nEnvironment and tools\uff1aUNIX K Shell Scripting, Oracle 11g, PL/SQL, Cognos PowerPlay, Business Objects\nSummary:\n\u25c6 Led DWH team in AMS project (Pharmaceutical industry), accomplished service transition and supervised service operations such as trouble-shooting, database management, project support, etc. Successfully supported business users' work based on my comprehensive understanding on business flow and system structure of DWH, and made efficient communications with business users, cross teams and project teams.\n\u25c6 The EDWH Fitch & Bloomberg project was to add new features on data marts which proceed Fitch and Bloomberg financial reports daily and monthly in client's EDWH system.\n\nKey Responsibilities:\n\u25c6 Created plan and allocated resources for Knowledge Transfer, led team accomplished service transition according to ITIL best practices and standards.\n\u25c6 Developed work plan and supervised service operations include project support, trouble shooting, DB management etc.\n\u25c6 Did Functional and Technical support, resolve technical issues, conduct performance tuning for long running queries in PL/SQL scripts.\n\u25c6 Smoothly communicated with users (Business department), client (IT department), vender companies, offshore team, Japan IT operation team, India IT operation team and Data Center team.\n\u25c6 Manage daily operational calls and client issues.\n\u25c6 Developed and maintained KPI reports using Excel Pivot Table and reported to Project Manager and stake holders, assured all KPI were under control and fulfill requirements in SLA.\n\u25c6 Participating in the project report meetings and provide feedback to manager for further decisions.\n\u25c6 Prepare high and low level design documents before starting the development phase.\n\u25c6 Participating in the Peer review related to Design, Code and Unit test cases and monitoring the performance.\n\u25c6 Experienced the release management, deployment checklist review and defect management of each release."", u'Senior Software Engineer\nHUAWEI Technology - Shenzhen, CN\nJune 2004 to July 2006\nProjects/Assignments: PPSV8.1Dx~ PPSV8.4Dx, GFEP V2.5\nEnvironment and tools\uff1a UNIX/Solaris/SUSE Linux, Unix C/C++, ACE C++, K shell, Informix\nSummary:\n\u25c6 PPS (Pre-Paid Service) was a telecommunication system designed to operate Pre-paid Service. There were 4 modules: SER module provide main feature/functionalities, SCPser module control user license and lifecycle, SMPser module count user bills and generated service statistic data files. SMAP module present service statistic data onto c-s client. I was in SMPser team, collected business requirements, design and developed bill calculation functionalities.\n\u25c6 GFEP (General Front End Processor) system is used to convert protocols between platforms, to provide data exchange service between the service control point (SCP) and the external platform (EP) such as bank interface, telephone pay interface, PINT server interface and short message service center (SMSC) interface. As a key team member, designed and developed part of functionalities of proceeding inquiry requests in LDAP format with workload balancing.\n\nKey Responsibilities:\n\u25c6 Creating business requirement documents.\n\u25c6 Prepare high and low level design documents, and test documents before the coding phase.\n\u25c6 Participating in the Peer review related to Design, Code and Unit test cases and monitoring the performance.\n\u25c6 Ensuring that all deliverables will be done with high quality on time by following all the best practices/processes/standards and providing weekly status to Team Leader.']",[u'Bachelor of Science in Computer Science'],"[u'Hebei University of Economics and Business Shijiazhuang, CN\nJanuary 2000 to January 2004']"
0,https://resumes.indeed.com/resume/ee673fd2f33611ba,"[u""Expert Data Architect\nNike World Headquarters\nAugust 2013 to Present\nLead the data architecture/data modeling team in a large software project implementing a Product Lifecycle Management system (PLM) in support of Nike's Product Creation organization.\nDevelop and maintain logical and conceptual data models, enabling clear communication between business and technical users.\nDevelop standards - modeling, document models, JSON message schemas.\nSpecialties: Materials, Bill of Materials, Color, Costing, Pricing.\nResponsibilities: Logical data models, conceptual data models, term alignment. Collaborate with business users and development teams in the development of data design solutions.\nTechnology and Development Environment:\nMethodology: SAFe (Scaled Agile Framework), Version One\nLogical Data Models: CA ERwin; Conceptual Data Models: Visio; Databases: Oracle, MongoDB\nData/Database Design Highlights:\nLogical design and physical implementation of Line Planning in MongoDB\nLogical modeling of Footwear Materials, Color, Bill of Materials"", u""Data Architect\nFiserv\nJuly 2011 to August 2013\nPrimary Role: Data Architect, Enterprise Architecture Team\nLed all aspects of database design and development for the Corillian on-line banking product. Corillian is used by some of the largest banks in the US and around the world.\nDesign database solutions; establish standards; mentor database developers; participate in the Enterprise Architecture team as the sole data architect.\nRole 2: Manager, Database Development, Product Development Team\nManaged three database developers.\nRole 3: Manager, Patterns and Practices, Enterprise Architecture Team\nManaged three Solution Architects in Corillian's Professional Services organization.\nLed change efforts around architecture process for Professional Services Solution Architects.\nTechnology and Development Environment:\nSQL Server 2008 R2 and 2012 in a Windows/.NET environment, SSMS, ER/Studio\nDeployment environment: both on-premises and SaaS offerings\nMethodology: FDD, continuous integration, introducing agile/scrum"", u'Data Architect\nWebMD Health Services\nOctober 2007 to July 2011\nData Architect, Data Warehouse Team\nAs a relatively new member of the DW/BI leadership team, was responsible for data warehouse physical architecture and source data profiling. Researched data issues in source systems and recommended solutions to both the DW team and the source development teams.\nProvided database leadership across all technology teams for both OLTP and DW, including consulting with product teams, developing standards and best practices, and providing training.\nData Architect (OLTP), Product Development Teams\nResponsible for all aspects of OLTP architecture, design, coding, standards, and scalability.\nProvided database leadership across all technology teams. Created ""Data Architecture Community of Practice"", an all-volunteer cross-team database group that is responsible for all database changes.\nTechnology and Development Environment:\nMulti-site SQL Server 2005/2008 in a Windows/.NET environment with merge replication, Teradata 13.0/13.10, ER/Studio\nDeployment environment: SaaS\nMethodology: Agile, continuous integration', u'Staff Software Engineer (Data Architect)\nFiserv - Hillsboro, OR\nJuly 1999 to October 2007\nResponsibilities:\nDesign database solutions; program stored procedures and unit tests.\nReview and update use cases.\nAct as primary liaison between Product Management and Engineering development group for project.\nTechnology and Development Environment:\nSQL Server 2005 in a Windows/.NET 2.0 environment\nMicrosoft SQL Server (6.5, 7.0, 2000, 2005), T-SQL, ER/Studio, ERwin\nMethodology: Agile, continuous integration\nOther projects and assignments:\nDatabase designer on multiple projects through complete SDLC.\nWorked on Corporate Banking, \u201cPlatform\u201d (application server), Bill Payments/Bill Presentment, and CRM teams.\nDeveloped and documented a set of best practices for database development.\nManaged a development team (CRM) for one year.\nWrote requirements for new releases of existing products.\nData/Database Design Highlights:\nPayments processing, international wires, authentication/authorization']",[u'Bachelor of Science in Computer Science'],"[u'Portland State University Portland, OR\nAugust 1996']"
0,https://resumes.indeed.com/resume/a3b2c46cb2a98940,"[u'Database Administrator\nMcCracken & Gillen, LLC\nAugust 2017 to Present\nHired by McCracken & Gillen Intellectual Property law firm to create and manage efficient databases for clients and document intensive litigation.\n\u2022 Created uniform database systems for litigation teams to quickly filter through voluminous documents and files during courtroom proceedings.\n\u2022 Established a standardized, custom form design in Microsoft Access to manage and organize documents responsive to discovery requests.\n\u2022 Designed effective databases for documents using both Microsoft Excel, and Access.\n\u2022 Enabled database hyperlink fields to be accessible via local files or network drives.\n\u2022 Built and maintained systems for digital use rather than physical copies.', u""Process Engineer & Data Analyst\nRotation Dynamics Corporation\nJune 2016 to August 2016\nWorked directly under the CFO and completed several independent studies for finance and engineering. Hired to provide the finance department with updated information and statistics to input in the new company-wide accounting software program.\n\u2022 Worked with software designers and financial officers to identify and calculate data inputs to be coded into the company's new accounting software.\n\u2022 Responsible for validating existing metrics and identifying more accurate figures through my own methodology and procedure.\n\u2022 Observed the manufacturing process and assigned a monetary value to each stage, comparing it against Rotadyne's estimates to ensure they were effectively quoting their products and desired profit margins.\n\u2022 Created a custom formula to more precisely estimate material consumption on a per unit basis.\n\u2022 Identified and calculated deadweight losses in the manufacturing process.\n\u2022 Provided chief officers with critical manufacturing statistics such as most profitable material and size distribution of manufactured goods.\n\u2022 Completed time & motion studies for products in all four phases of manufacturing: stock prep, product manufacturing, grinding, and buffing.\n\u2022 Identified inefficiencies in the manufacturing process and discussed practical solutions with head of engineering department and CFO.\n\u2022 Developed and helped implement a more effective means of communication between the finance department and manufacturing managers.\n\u2022 Completed a full and detailed process mapping of the manufacturing plant.\n\nRelevant Coursework\n\u2022 Building Design \u2022 Safety- Injury Prevention\n\u2022 Environmental Control & HVAC Systems \u2022 Wiring, Motors and Control Systems\n\u2022 Project Management- (Microsoft Project)\n\u2022 Accounting\n\u2022 Business Management & Organizational Behavior\n\n\u2022 Introduction to AutoCAD\n\u2022 Advance Project Management\n\u2022 Materials & Construction Systems""]",[u'Bachelor in Technical Systems Management'],"[u'University of Illinois Urbana, IL']"
0,https://resumes.indeed.com/resume/3cfe93bc5217e93e,"[u'Software Engineer\nHappy LEAF - Multi Platform App for Nissan LEAF OBD Data Logging\nMarch 2016 to Present\nEmber based front end, running on Cordova. Dynamic Widgets for user customizable data dashboard. Real time OBD data\nfrom vehicle, customizable graphs over time.\n\u25cf App used worldwide on all first generation LEAF models.\n\n\n(253) 592-9341\ncareer@brandonmartin.life BLM\n5390 SE Ravenridge CT, Port Orchard, WA 98367', u'Software Engineer\nBlufish LLC\nDecember 2012 to Present\nFull stack web based technology architect, created custom solutions for conference planning.\n\u25cf Aided in the organization of 5 annual conferences utilizing our technologies, 400+ attendees per year. Hosted Makerspace.', u'Data Integration Analyst\nPeninsula School District\nJuly 2011 to Present\nPreparation, deployment and management of Linux, PC and MacOS systems and software across over 3,000 devices.\n\u25cf Utilized ticketing software to handle thousands of user tickets with highly praised customer service.\n\u25cf Close collaboration in a small team, strong leadership role.\n\u25cf Overcame complicated issues in stressful situations.\n\u25cf Provided creative open source solutions to organization wide issues.\n\u25cf In house developed software served hundreds of thousands requests per month. Millions of web sessions served.\n\u25cf Currently using advanced techniques: Docker swarm orchestration, AWS EFS shared volumes, and advanced load\nbalancing.\n\u25cf Job roles not in line with my passion for programming and inventing, looking to satisfy that passion at your organization.', u'Startup Founder\nPondre LLC\nSeptember 2015 to January 2017\nApp based in store menu utilizing tablets.\n\u25cf Developed full stack cloud software infrastructure - SQL, NodeJS, AngularJS, Swift app.']","[u'Computer Science degree in Physics', u'']","[u'Olympic College Bremerton, WA\nSeptember 2013 to January 2015', u'Henderson Bay High School Gig Harbor, WA\nJanuary 2013']"
0,https://resumes.indeed.com/resume/ca91b39f55d322e0,"[u'data Testing\nACT - Iowa City, IA\nMarch 2018 to Present\nTesting student records.', u'Software Test Engineer']","[u'in engineering', u'B.Tech in Computer Science & Engineering', u'in Education', u'']","[u'JNTu Hyderabad, Telangana\nJanuary 2016', u'Biju Pattanaik University Of Technology ORISSA, IN\nJanuary 2009', u'Science College Brahmapur, Orissa\nJanuary 2005', u'Board/ University']"
0,https://resumes.indeed.com/resume/9ce63b60646f0b13,"[u'Data Engineer Intern\nFebruary 2018 to Present\n\u2022 Contributed to the design and coding of air fleet database by creating several PostgreSQL objects\n\u2022 Streamlined and optimized ETL processes to extract, transform and integrate aviation data from disparate sources\n\u2022 Performed project management activities using JIRA and Confluence', u""Data Analyst, Capstone Project\nAugust 2017 to December 2017\nThe objective of the Capstone Project - Advanced Analytics & Project Management, is to identify an optimal pricing strategy (consistent across all franchise locations) for a leading Kids' Gym company to maximize revenue and customer acquisition/retention\n\u2022 Employed a customer-centric approach to derive an optimal and consistent pricing structure across locations\n\u2022 Identified Key performance indicators (KPIs) to enable business track and monitor performance\n\u2022 Recommended marketing strategies and targeted promotions based on customer segmentation to enhance business\n\u2022 Analyzed non-member to member transition patterns and identified key statistics to highlight actionable recommendations"", u'Programmer Analyst\nData Engineering - Chennai, Tamil Nadu\nAugust 2013 to September 2016\n\u2022 Built 3-layered Business Intelligence Platform using ETL for medical devices client to deliver a scalable Enterprise Data Warehouse (EDW) solution and accomplish reporting/BI needs\n\u2022 Extracted data from SAP ECC6 system to Oracle database by leveraging Informatica Business Content Integration framework\n\u2022 Automated and scheduled data extraction from source system to target Analytical Data Store layer through intermediate Staging, Normalized and Detailed Data Store layers by executing UNIX wrapper scripts and CRON jobs respectively\n\u2022 Replaced complex Look-Up, Joiner Transformations with Source Qualifier Override, yielding 40% reduction in data load time\n\u2022 Optimized performance of business reports/visualizations by building materialized views, yielding 90%-time savings\n\u2022 Incorporated Platinum Code Reviewer (In-house code review tool) in ETL build; reducing manual review effort by 70%\nAward: Best Performer - Quarterly Award (Q3 2015) Certification: Cognizant Certified Professional - Informatica Practitioner']","[u'in Business', u'in technology']","[u'University of Connecticut School of Business Hartford, CT\nMay 2018', u'Anna University Chennai, Tamil Nadu\nMay 2013']"
0,https://resumes.indeed.com/resume/d57096677932469b,"[u""Data Scientist\nwellsfargo Des Moines,Lowa\nMarch 2017 to Present\nDescription: Wells Fargo is forever linked with the image of a six-horse stagecoach thundering across the American West, loaded with gold. The full history, over more than 160 years, is rich in detail with great events in America's history. From the Gold Rush to the early 20th Century, through prosperity, depression and war, Wells Fargo earned a reputation of trust due to its attention and loyalty to customers.\n\nResponsibilities:\n\u27a2 Extracted data from HDFS and prepared data for exploratory analysis using data munging\n\u27a2 Built models using Statistical techniques like Bayesian HMM and Machine Learning classification models like XGBoost, SVM, and Random Forest.\n\u27a2 Participated in all phases of data mining, data cleaning, data collection, developing models, validation and visualization and performed Gap analysis.\n\u27a2 A highly immersive Data Science program involving Data Manipulation Visualization, Web Scraping, Machine Learning, Python programming, SQL, GIT, Mongo DB, Hadoop.\n\u27a2 Setup storage and data analysis tools in AWS cloud computing infrastructure.\n\u27a2 Installed and used Caffe Deep Learning Framework\n\u27a2 Worked on different data formats such as JSON, XML and performed machine learning algorithms in Python.\n\u27a2 Worked as Data Architects and IT Architects to understand the movement of data and its storage and ER Studio 9.7\n\u27a2 Used pandas, numpy, seaborn, matplotlib, scikit-learn, scipy, NLTK in Python for developing various machine learning algorithms.\n\u27a2 Data Manipulation and Aggregation from different source using Nexus, Business Objects, Toad, Power BI and Smart View.\n\u27a2 Implemented Agile Methodology for building an internal application.\n\u27a2 Focus on integration overlap and Informatica newer commitment to MDM with the acquisition of Identity Systems.\n\u27a2 Coded proprietary packages to analyze and visualize SPCfile data to identify bad spectra and samples to reduce unnecessary procedures and costs.\n\u27a2 Programmed a utility in Python that used multiple packages (numpy, scipy, pandas)\n\u27a2 Implemented Classification using supervised algorithms like Logistic Regression, Decision trees, Naive Bayes, KNN.\n\u27a2 As Architect delivered various complex OLAP databases/cubes, scorecards, dashboards and reports.\n\u27a2 Updated Python scripts to match training data with our database stored in AWS Cloud Search, so that we would be able to assign each document a response label for further classification.\n\u27a2 Used Teradata utilities such as Fast Export, MLOAD for handling various tasks data migration/ETL from OLTP Source Systems to OLAP Target Systems\n\u27a2 Data transformation from various resources, data organization, features extraction from raw and stored.\n\u27a2 Validated the machine learning classifiers using ROC Curves and Lift Charts.\nEnvironment: Unix, Python 3.5.2, MLLib, SAS, regression, logistic regression, Hadoop 2.7.4, NoSQL, Teradata, OLTP, random forest, OLAP, HDFS, ODS, NLTK, SVM, JSON, XML and MapReduce."", u""Data Scientist\nEllucian - Malvern, PA\nOctober 2015 to February 2017\nDescription: Ellucian's technology solutions are designed for the modern student specifically to meet the needs of higher education. Our software and services help students, staff, and faculty achieve their goals.\n\nResponsibilities:\n\u27a2 Utilized Spark, Scala, Hadoop, HQL, VQL, oozie, pySpark, Data Lake, TensorFlow, HBase, Cassandra, Redshift, MongoDB, Kafka, Kinesis, Spark Streaming, Edward, CUDA, MLLib, AWS, Python, a broad variety of machine learning methods including classifications, regressions, dimensionally reduction etc.\n\u27a2 Utilized the engine to increase user lifetime by 45% and triple user conversations for target categories.\n\u27a2 Application of various machine learning algorithms and statistical modeling like decision trees, text analytics, natural language processing (NLP), supervised and unsupervised, regression models, social network analysis, neural networks, deep learning, SVM, clustering to identify Volume using scikit-learn package in python, Matlab.\n\u27a2 Worked onanalyzing data from Google Analytics, AdWords and Facebook etc.\n\u27a2 Evaluated models using Cross Validation, Log loss function, ROC curves and used AUC for feature selection and elastic technologies like ElasticSearch, Kibana.\n\u27a2 Performed Data Profiling to learn about behavior with various features such as traffic pattern, location, Date and Time etc.\n\u27a2 Categorized comments into positive and negative clusters from different social networking sites using Sentiment Analysis and Text Analytics\n\u27a2 Performed Multinomial Logistic Regression, Decision Tree, Random forest, SVM to classify package is going to deliver on time for the new route.\n\u27a2 Performed data analysis by using Hive to retrieve the data from Hadoop cluster, Sql to retrieve datafrom Oracle database and used ETL for data transformation.\n\u27a2 Performed Data Cleaning, features scaling, features engineering using pandas and numpy packages in python.\n\u27a2 Exploring DAG's, their dependencies and logs using Air Flow pipelines for automation\n\u27a2 Performed data cleaning and feature selection using MLlib package in PySpark and working with deep learning frameworks such as Caffe, Neon.\n\u27a2 Developed Spark/Scala,R Python for regular expression (regex) project in the Hadoop/Hive environment with Linux/Windows for big data resources.\n\u27a2 Used clustering technique K-Means to identify outliers and to classify unlabeled data.\n\u27a2 Tracking operations using sensors until certain criteria is met using Air Flow technology.\n\u27a2 Responsible for different Data mapping activities from Source systems to Teradata using utilities like TPump, FEXP,BTEQ, MLOAD, FLOAD etc\n\u27a2 Analyze traffic patterns by calculating autocorrelation with different time lags.\n\u27a2 Ensured that the model has low False Positive Rate and Text classification and sentiment analysis for unstructured and semi-structured data.\n\u27a2 Addressed over fitting by implementing of the algorithm regularization methods like L1 and L2.\n\u27a2 Used Principal Component Analysis in feature engineering to analyze high dimensional data.\n\u27a2 Used MLlib, Spark's Machine learning library to build and evaluate different models.\n\u27a2 Implemented rule based expertise system from the results of exploratory analysis and information gathered from the people from different departments.\n\u27a2 Created and designed reports that will use gathered metrics to infer and draw logical conclusions of past and future behavior.\n\u27a2 Developed MapReduce pipeline for feature extraction using Hive and Pig.\n\u27a2 Created Data Quality Scripts using SQL and Hive to validate successful data load and quality of the data. Created various types of data visualizations using Python and Tableau.\n\u27a2 Communicated the results with operations team for taking best decisions.\n\u27a2 Collected data needs and requirements by Interacting with the other departments.\n\nEnvironment: Python 2.x, CDH5, HDFS, Hadoop 2.3, Hive, Impala, AWS, Linux, Spark, Tableau Desktop, SQL Server 2014, Microsoft Excel, Matlab, Spark SQL, Pyspark."", u""Data Scientist\nWalgreens - Deerfield, IL\nDecember 2014 to September 2015\nDescription: The Walgreens Companyis an American company that operatesas the second-largest pharmacy store chain in the United States. It specializes in filling prescriptions, health and wellness products, health information, and photo services\n\nResponsibilities:\n\u27a2 Involved in Design, Development and Support phases of Software Development Life Cycle (SDLC)\n\u27a2 Performed data ETL by collecting, exporting, merging and massaging data from multiple sources and platforms including SSIS (SQL Server Integration Services) in SQL Server.\n\u27a2 Worked with cross-functional teams (including data engineer team) to extract data and rapidly execute from MongoDB through MongDB connector for Hadoop.\n\u27a2 Create Hive Internal tables with appropriate partitioning and bucketing and write complex queries for data Analysis. Configured Oozie work flows to automate data flow.\n\u27a2 Performed data cleaning and feature selection using MLlib package in PySpark.\n\u27a2 Performed partitional clustering into 100 by k-means clustering using Scikit-learn package in Python where similar hotels for a search are grouped together.\n\u27a2 Used Python to perform ANOVA test to analyze the differences among hotel clusters.\n\u27a2 Implemented application of various machine learning algorithms and statistical modeling like Decision Tree, Text Analytics, Sentiment Analysis, Naive Bayes, Logistic Regression and Linear Regression using Python to determine the accuracy rate of each model.\n\u27a2 Determined the most accurately prediction model based on the accuracy rate.\n\u27a2 Used text-mining process of reviews to determine customers' concentrations.\n\u27a2 Delivered analysis support to hotel recommendation and providing an online A/B test.\n\u27a2 Designed Tableau Bar Graphs, Scatter Plots, and Geographical maps to create detailed level summary reports and dashboards.\n\u27a2 Developed hybrid model to improve the accuracy rate.\n\u27a2 Delivered the results to operation team for better decisions and feedbacks.\n\nEnvironment: Python, SAS, Tableau, MongoDB, Hadoop, SQL Server, SDLC, ETL, SSIS, Recommendation Systems, Machine Learning Algorithms, Text-mining Process, A/B test"", u""Data analyst\nTransamerica - Plano, TX\nNovember 2013 to November 2014\nDescription: Transamerica is the US-based brand of Aegon, a Dutch financial services firm. Aegon is one of the world's leading providers of life insurance, pensions and asset management and is helping approximately 30 million customers globally to achieve a lifetime of financial security.\n\nResponsibilities:\n\u27a2 Worked with BI team in gathering the report requirements and also Sqoop to export data into HDFS and Hive\n\u27a2 Involved in the below phases of Analytics using R, Python and Jupyter notebook.\n\u27a2 a. Data collection and treatment: Analysed existing internal data and external data, worked on entry errors, classification errors and defined criteria for missing values\nb. Data Mining: Used cluster analysis for identifying customer segments, Decision trees used for profitable and non-profitable customers, Market Basket Analysis used for customer purchasing behaviour and part/product association.\n\u27a2 Developed multiple Map Reduce jobs in Java for data cleaning and preprocessing.\n\u27a2 Assisted with data capacity planning and node forecasting.\n\u27a2 Installed, Configured and managed Flume Infrastructure.\n\u27a2 Administrator for Pig, Hive and HBase installing updates patches and upgrades.\n\u27a2 Worked closely with the claims processing team to obtain patterns in filing of fraudulent claims.\n\u27a2 Worked on performing major upgrade of cluster from CDH3u6 to CDH4.4.0\n\u27a2 Developed Map Reduce programs to extract and transform the data sets and results were exported back to RDBMS using Sqoop.\n\u27a2 Patterns were observed in fraudulent claims using text mining in R and Hive.\n\u27a2 Exported the data required information to RDBMS using Sqoop to make the data available for the claims processing team to assist in processing a claim based on the data.\n\u27a2 Developed Map Reduce programs to parse the raw data, populate staging tables and store the refined data in partitioned tables in the EDW.\n\u27a2 Created tables in Hive and loaded the structured (resulted from Map Reduce jobs) data\n\u27a2 Using HiveQL developed many queries and extracted the required information.\n\u27a2 Created Hive queries that helped market analysts spot emerging trends by comparing fresh data with EDW reference tables and historical metrics.\n\u27a2 Was responsible for importing the data (mostly log files) from various sources into HDFS using Flume\n\u27a2 Enabled speedy reviews and first mover advantages by using Oozie to automate data loading into the Hadoop Distributed File System and PIG to pre-process the data.\n\u27a2 Provided design recommendations and thought leadership to sponsors/stakeholders that improved review processes and resolved technical problems.\n\u27a2 Managed and reviewed Hadoop log files.\n\u27a2 Tested raw data and executed performance scripts.\n\nEnvironment: HDFS, PIG, HIVE, Map Reduce, Linux, HBase, Flume, Sqoop, R, VMware, Eclipse, Cloudera and Python."", u'Data analyst\nHitachi Consulting - Pune, Maharashtra\nFebruary 2011 to October 2013\nDescription: Hitachi corporate statement, ""Inspire the Next"" expresses Hitachi\'s determination to breathe new life into the next era. With its social innovation businesses, Hitachi strives to become the ""Best Solutions Partner"" and help create a comfortable and abundant society.\n\nResponsibilities:\n\u27a2 Collaborating with business and technology teams.\n\u27a2 Data Analysis-Data collection, data transformation and data loading the data using different ETL systems like SSIS and Informatica.\n\u27a2 Performed source to target mapping as part of data migration from JD Edwards system to Agile PDM system.\n\u27a2 Data Migration testing and implementation activities using SSIS and SSRS tools of Microsoft SQL Server 2008.\n\u27a2 Responsible for accuracy of the data collected and stored in the corporate support system.\n\u27a2 Performed data review, evaluate, design, implement and maintain company database.\n\u27a2 Involved in construction of data flow diagrams and documentation of the processes.\n\u27a2 Interacted with end users for requirements study and analysis by JAD (Joint Application Development).\n\u27a2 Performed gap analysis between the present data warehouse to the future data warehouse being developed and identified data gaps and data quality issues and suggested potential solutions.\n\u27a2 Participated in system and use case modeling like activity and use case diagrams.\n\u27a2 Analyzed user requirements & worked with data modelers to identify entities and relationship for data modeling.\n\u27a2 Actively participated in the design of data model like conceptual, logical models using Erwin. Used Exception handling application block for checking errors/exceptions across the website.\n\u27a2 Developed Report Component, so that it retrieves the data by executing Stored Procedures throw Data Access component.\n\nEnvironment: Windows, Oracle, MS Excel, SSIS, Informatica, GAP Analysis, ERWIN', u'Associate Software Engineer\nEchidna Software pvt ltd - Bengaluru, Karnataka\nJuly 2009 to January 2011\nDescription: Echidna began when a small group of ecommerce leaders knew there had to be a better way to do ecommerce. So they branched off and created a new kind of agency one that combines amazing UX, enterprise-level technology implementation, and value-added marketing and analytics services.\n\nResponsibilities:\n\u27a2 Assisted in development and testing of various interior installations and instrumentation.\n\u27a2 Documented the technical specification for the reports and tested the generated reports.\n\u27a2 Gathered user requirements and created the business requirements documents.\n\u27a2 Used the technical document to design tables.\n\u27a2 Prepared user manual and technical support manuals.\n\u27a2 Prepared test plans for various modules.\n\u27a2 Created and managed Databases.\n\u27a2 Optimized the SQL queries for improved performance.\n\u27a2 Created Database triggers to maintain the audit data in the tables.\n\nEnvironment: Oracle 9i, SQL* Loader, PL/SQL, SQL.']","[u""Bachelor's""]",[u'']
0,https://resumes.indeed.com/resume/0c47de5601135ba8,"[u'Lead Building Engineer\nCBRE-Cleveland Beachwood CCAC\nJune 2015 to Present\n1-1/2 million square feet\n\n\u2022 Manage Engineering Operation/staff both daily and long term/6 man staff direct reports\n\u2022 Manage and Maintain Emergency backup equipment UPS systems, Emergency Generators, Automatic\nTransfer Switches, and all related critical environment equipment.\n\u2022 Correspondence with Property Management, group managers, vendors, staff, etc.\n\u2022 Maintain Five office buildings for client including 24 hour call centers\n\u2022 Oversee Maintain and upkeep of office space, mechanical equipment and control systems for multiple\nData Center, environment. This included building automation controls, HVAC, lighting controls, and the electrical monitoring system.\n\u2022 Supervised multiple vendors on various projects, participated in all phases of project work from planning to implementing.\n\u2022 EOP, MOP, SOP creation, updates, entries and posting\n\u2022 Site specific training/new development/requirement training requirements\n\u2022 Document development and training\n\u2022 Employee reviews, entries and ongoing conversations, mid-year and year end\n\u2022 Service request review\n\u2022 Quarterly building inspections reports completed and reviewed\n\u2022 Energy Star updates\n\u2022 Compliance updates and preparation (MSDS, Audit Volumes, Documentation, etc.)', u'Chief Engineer Charles Schwab / OptionsXpress Data Center\nJLL - Richfield, OH\nSeptember 2010 to August 2014\n\u2022 Manage Engineering Operation/staff both daily and long term\n\u2022 Manage and Maintain Emergency backup equipment UPS systems, Emergency Generators, Automatic\nTransfer Switches, and all related critical environment equipment.\n\u2022 Correspondence with Property Management, group managers, vendors, staff, etc.\n\u2022 Oversee Maintain and upkeep of office space, mechanical equipment and control systems for multiple\nData Center, environment. This included building automation controls, HVAC, lighting controls, and the electrical monitoring system.\n\u2022 Supervised multiple vendors on various projects, participated in all phases of project work from planning to implementing.\n\u2022 EOP, MOP, SOP creation, updates, entries and posting\n\u2022 Site specific training/new development/requirement training requirements\n\u2022 Document development and training\n\u2022 Manage Capex and Opex over $2 million dollars\n\u2022 Employee reviews, entries and ongoing conversations, mid-year and year end\n\u2022 Service request review\n\u2022 Permit renewals and submission to proper agencies\n\u2022 Quarterly building inspections reports completed and reviewed\n\u2022 Energy Star updates\n\u2022 Compliance updates and preparation (MSDS, Audit Volumes, Documentation, etc.)\n\u2022 Off site management Options Express data center facilities side of the house\n\u2022 Serving Over 600 Charles Schwab clients\n\u2022 Manage Options Express Data Center in Chicago maintaining all critical environment equipment.', u""Data Center - Plant Engineer II\nProgressive Insurance - Mayfield, OH\nJanuary 2000 to January 2010\n\u2022 Commissioned and startup of Data Center\n\u2022 Ensure all Cleveland-area Progressive central plants operate efficiently\n\u2022 Operate and maintain all Cleveland chiller plants and equipment including data center's mechanical and electrical systems\n\u2022 Maintained and up keep of mechanical equipment and control systems for data center, environment.\nThis included: building automation controls, lighting controls, and the electrical monitoring system.\n\u2022 Supervised multiple vendors on various projects, participated in all phases of project work from planning to implementing\n\u2022 Responsible for operations, repair, installation and maintenance of HVAC, Central Plant equipment in compliance with OSHA, EPA and ASHRAE guidelines\n\u2022 Wrote Sequence of operation procedures for central plant switchgear, emergency generator operations\n\u2022 Responsible for testing Caterpillar Generation system, Russell Electric switchgear\n\u2022 Collected and stored data for reporting purposes monthly electrical and fuel usage reports along with mission critical and uptime reports\n\u2022 Knowledge of MGE, Liebert UPS systems\n\u2022 Maintained all raised floor HVAC applications Liebert Air boxes both chiller water and compressor\n\u2022 Knowledge of raised floor practices including change control and demand\n\u2022 Knowledge of MGE Power distribution units PDU 'S, RPP'S Static switches\n\u2022 Preventive maintenance of Fire system and life safety""]",[],[]
0,https://resumes.indeed.com/resume/23e101d1a1669cab,"[u'Manager\nBEI TIAN SOFTWARE TECHNOLOGY CO., LTD - NANJING\nApril 2016 to May 2017\nResponsible for the design and implementation of enterprise product\narchitecture\nResponsible for resolving technical problems\nResponsible for the management of the development team', u'DATA ENGINEER/PROJECT MANAGER\nBANK OF COMMUNICATIONS, SOFTWARE DEVELOPMENT CENTER, SHANGHAI, CHINA (Global 500) - SHANGHAI, CN\nFebruary 2006 to July 2015\nResponsible for the design and implementation of the enterprise data\nwarehouse\nAs public group leader, organized global enterprise core system development\nResponsible for training all Chinese technicians of the enterprise']","[u'in IMBA', u'MBA']","[u'University of Missouri-St. Louis St. Louis, MO\nAugust 2017 to May 2018', u'Nanjing University\nSeptember 2016 to May 2018']"
0,https://resumes.indeed.com/resume/0809c78f70abae48,"[u'Senior Software Engineer\nCapgemini America Inc - Columbia, SC\nMarch 2017 to Present\n\u2022 Efficiently performed root cause analysis and code fixes for production issues and platform upgrade defects\n\u2022 Wrote advanced SQL queries to assist data repairs as well as to find relevant policies for analysis and QA\n\u2022 Leveraged Splunk to monitor and track down the issues\n\u2022 Skills used: MSSQL Server, Stored Procedures, XML, C#, Splunk, Duck Creek Billing, Duck Creek Author, TFS', u'Software Engineer\nTCube Solutions Inc - Columbia, SC\nJuly 2016 to February 2017\n\u2022 Performed root cause analysis and fixed several issues caused by platform upgrade and in production\n\u2022 Deployed, designed, tested and efficiently managed Reinsurance project\n\u2022 Functional level expertise in Duck Creek Billing System and earned master-level certification in Duck Creek Author\n\u2022 Skills used: Microsoft SQL Server, Stored Procedures, XML, Duck Creek Author, TFS', u'Data Analytics Intern\nSymantec Corporation World HQ - Mountain View, CA\nJune 2014 to December 2014\n\u2022 Extracted actionable insight on the performance of Symantec Products and Campaigns to help business grow\n\u2022 Wrote scripts that automate scraping and storing of real time Facebook and Twitter data to cloud server\n\u2022 Analyzed and graphically visualized the data to obtain meaningful statistics\n\u2022 Skills used: Python, Apache Hadoop, Hive, Impala, GraphLab, NodeXL, Gephi, Linux\n\nPROJECTS\nPredicting Autism over Large-Scale Child Dataset, SJSU\n\u2022 Built text-based classifier model to predict if a patient has Autism or not\n\u2022 Used Machine Learning Na\xefve Bayes algorithm to develop a finely tuned binary classifier model\n\u2022 Learning performed over symptom stories (text) obtained from VAERS\n\u2022 Training, tweaking and optimizing was done on the classifier using pre-defined set of training dataset\n\u2022 Skills used: Apache Spark, Weka, Scala, Amazon Web Services -AWS S3, AWS EC2, AWS EMR\nBig Data Analytics using Hadoop on Yelp Dataset, SJSU\n\u2022 Analyzed Yelp data set to help business personnel gain insight into the locations where they can open their business\n\u2022 Created statistics in support of interesting events found in the data set\n\u2022 Skills used: Apache Hadoop, Hive, GoGrid Cloud Environment, Putty']","[u'M.S. in Computer Science', u'B.E. in Computer Science']","[u'San Jose State University San Jose, CA\nJanuary 2016', u'Rajiv Gandhi Technical University Indore, Madhya Pradesh\nJune 2012']"
0,https://resumes.indeed.com/resume/14b00eb9b0eee462,"[u'Sofware Engineer\nTCS - Willmington, DE\nSeptember 2017 to Present\n6 weeks Trained in JAVA, SQL, HTML and JSP Cincinnati OHIO', u'Data Analyst\nQuEST Global Engineering - Windsor, CT\nSeptember 2016 to Present\n\u2022Create formulas in excel for the suppliers. Understand and modify excel macros.\n\u2022Importing, cleaning, transforming, and modeling data with the purpose of understanding and making conclusions.\n\u2022Presenting data in charts, graphs, tables, PowerPivot tables for suppliers.', u'Data Analyst\nHorizon Blue Cross - Newark, Nj\nJuly 2017 to September 2017', u'Billing Specialist\nLos Ninos services INC - New York, NY\nJune 2014 to October 2014']","[u""Bachelor's in Computer Science"", u'Associate in Business Administration']","[u'Kean University Union, NJ\nJanuary 2011 to January 2016', u'Kean University Union, NJ\nJanuary 2013']"
0,https://resumes.indeed.com/resume/ad838a00e189a88b,"[u'Data Science Intern\nOpen Data Nation - Washington, DC\nSeptember 2017 to Present\nFound traffic counts data and crash data for Chicago on internet and cleaned raw data;\nPreprocessed Dataset, including geolocating data, reprojecting, and joining other data;\nDebugged the team\u2019s python code, so that the script can deal with more situations;\nUsed machine learning models to predict traffic counts and crash probability for city Chicago;\nBuilt Data Pipelines with Data Factory in Azure Cloud Platform to predict probability of accidents\non road automatically with trained models for city Chicago', u'Assistant Product Engineer Internship\nNANJING ERICSSON PANDA COMMUNICATIONS COMPANY Ltd. (ENC) - Nanjing\nJuly 2015 to August 2015\nAssisted the product engineer in performing product yield analysis with pivot tables;\nDeveloped Microsoft Excel tools to help manage group members\u2019 performance by Visual Basic for\nApplications']","[u'Master of Science in Data Sciences', u'Bachelor of Science in Geographic Information System']","[u'THE GEORGE WASHINGTON UNIVERSITY Washington, DC\nAugust 2016 to May 2018', u'NANJING TECH UNIVERSITY Nanjing, CN\nJune 2016']"
0,https://resumes.indeed.com/resume/512af02fcf26a89d,"[u'Data Scientist/Analytics intern\nRexel\nApril 2017 to Present\nData Science/Machine learning\n\u2022 Developed recommender engines to improve the core product flow and E-Business\n\u2022 Developed substitution model to help customers when product goes out of stock/unavailable\n\u2022 Developed anomaly detection model to detect unusual patterns and presented it using Power BI\n\u2022 Developed churn model to predict customer churn with responsible features\n\u2022 Built NLP model on live chat data to understand customer problems/requirement\n\u2022 Created business rules and KPIs through demand forecasting and anomaly detection analysis\n\u2022 Developed a desktop tool to automate the E-commerce scorecard reporting on monthly basis\n\u2022 Created dashboards presenting the customer life cycle segmentation using Power BI\n\u2022 Managed implementation, tagging, QA and Production for google analytics\n\u2022 Managed A/B testing and experimentation across web-shops, setting up tests and measuring results\n\u2022 Analyzed and optimized website for customer ease of navigate and increased time on site', u'Systems Engineer-Data Analyst\nTata Consultancy Services\nJuly 2013 to January 2016\n\u2022 Responsible for data collection and generate reports on a weekly, monthly and quarterly basis\n\u2022 Developed pipelines to analyze large datasets with SQL\n\u2022 Assisted and developed internal tools for data retrieval and analysis from the web\n\u2022 Transformed the sales data using ETL tool and build facts of it to answer any business questions\n\u2022 Utilized Tableau in analyzing/creating visualizations to identify trends and relationships betwe en data\n\u2022 Presented findings and data to team to improve strategies and operations\n\u2022 Organized inter module knowledge transfer sessions for effective productivity of teammates\n\u2022 Designed presentations, summary reports, year-end reports and trackers']","[u'M.S., Information Technology in Management', u'B.S in Civil Engineering']","[u'The University of Texas at Dallas Dallas, TX\nDecember 2017', u'Pondicherry Engineering College Puducherry, Puducherry\nMay 2013']"
0,https://resumes.indeed.com/resume/449fd2d334c043cb,"[u'Data Analyst\nSamkit Diamonds Exports Pvt. Ltd - Mumbai, Maharashtra\nJuly 2015 to June 2016\n\u2022 Developed 7 reports/3 interactive dashboards providing insights in decision making which increased revenue by 15%\n\u2022 Collaborated with business stakeholders to understand requirements and provide visualization with data insights which can be used by recommendation processes\n\u2022 Designed and implemented operational and financial reports using SQL with 100% success in data reconciliation\n\u2022 Analyzed similar ad hoc queries accordingly created GUIs with filters for real time reporting using MS Access\n\u2022 Automated sales tracking by designing and developing sales metric report to track business performance', u'Senior Support Engineer\nSAR Microsystems - Mumbai, Maharashtra\nMarch 2014 to May 2015\n\u2022 Enhanced existing SQL process by 30% through query optimization and index improvisation\n\u2022 Developed SSIS packages to ingest data from disparate sources with appropriate business integrity and logic\n\u2022 Built data marts (Employee data mart, Product data mart) by transforming information in needed format using MS Access\n\u2022 Gathered Business requirements from clients and documented functional specifications for the application', u'Support Engineer\nJanuary 2013 to February 2014\n\u2022 Partnered with the development team to develop a membership management application using Java and MySQL\n\u2022 Designed and implemented an automatic Email/SMS notification system for membership renewal, events, and festivals\n\u2022 Prepared test plan, documented test specifications and executed SIT and UAT with 99% for multi-tiered systems']","[u'M.S., Management in Information Systems', u'B.E. in Electronics and Telecommunication']","[u'The University of Texas at Dallas Dallas, TX\nMay 2018', u'The University of Mumbai/K J Somaiya College of Engineering\nMay 2012']"
0,https://resumes.indeed.com/resume/dddd2802b263c54c,"[u'Senior Consultant\nFEDEX\nApril 2016 to Present\nWork Location: NY, US\n\nHardware Handling:\nCisco Routers: 2800, 3800, 3900, ASR 1000, ASR 7000\nCisco Switches: 2900, 3500, 3700, 6500, Nexus5K, Nexus 2K, Nexus 7K, VSS\nArista Switches: 7148\nCisco ACS: 1121\nFirewalls: ASA, Palo Alto\nProxy: Bluecoat Threat Pulse\nWireless: Cisco Prime, WLC\nTools: NetFlow Analyzer, Solar Winds, Zenoss, Remedy\n\nJob Responsibilities:\n\u2022 Responsible for configuration, maintenance, and troubleshooting of dynamic routing protocols: BGP, OSPF & EIGRP.\n\u2022 Implemented VSS Virtual Switching System in the campus and involved in WAN redesign project where DMVPN is configured as backup with MPLS as primary.\n\u2022 Providing Daily network support for national wide area network consisting of MPLS-VPN, MP-BGP, IPVPN, DMVPN.\n\u2022 Involved in Design and proposal for new project implementations related to network.\n\u2022 Providing Support with Firewall Administration, Rule Analysis, Rule Modification on Cisco ASA 5500 Single and Multimode Contexts.\n\u2022 Troubleshooting proxy issues and carry out changes on Blue Coat Threat Pulse.\n\u2022 Creation of Object-groups, NAT statements in Cisco ASA as per the user/project requirement.\n\u2022 Troubleshooting Routing, switching issues for all critical cases reported.\n\u2022 Configuring new sites with appropriate routing protocols post gathering all pre-requisites form project teams.\n\u2022 Troubleshooting VRF, VRF-Lite, MPLS, MP-BGP issues for all critical cases reported.\n\u2022 Configuring iBGP. EBGP and tweaking parameters according to the requirements provided.\n\u2022 Configuring (import/export) the BGP communities RT, RD based on the customer requirement.\n\u2022 Configuring OSPF, BGP using route maps to influence inbound and outbound routing.\n\u2022 Hardening Cisco devices and ensure a secure network in compliance to meet all regulations.\n\u2022 Play an active role in many projects which include ACS Roll-Out, ISP Data/voice circuit upgrades, IOS upgrades of switches, routers, and firewalls and ensure compliance of business policies.', u'Senior Network Engineer/Data Centre Manager\nWIPRO TECHNOLOGIES\nFebruary 2007 to March 2016', u""WIPRO TECHNOLOGIES\nSeptember 2010 to March 2015\nWork Location: United Kingdom\n\nHardware Worked:\nCisco Routers: 800, 2800, 3800, Nexus7K\nCisco Switches: 2900, 3500,3700,6000,6500\nCisco ACS: 1121\nFirewalls: CheckPoint R 77, ASA\nTools: NetFlow Analyzer, Nagio's, Wireshark, MS-Visio, BMC Remedy\n\nJob Responsibilities:\n\u2022 Configure the Router/Switch as per the projects requirement.\n\u2022 Implemented Nexus7009 in client network as part of project Data Centre Migration.\n\u2022 Involved in Design and proposal for new project implementations related to network.\n\u2022 Worked on Cisco 6509 Layer 3 Switches, Routers 800,2800, 2900 routers.\n\u2022 Configuring and Troubleshooting OSPF, BGP and RIPv2 Routing Protocols.\n\u2022 Designed and built an entire network for the new branch offices: VPN Access, internet connectivity, and LAN setup.\n\u2022 Configuring and Troubleshooting L2 and L3 Ether channels on Cisco 3550, 3560.\n\u2022 Configured site-to-site VPN with Managed Router Service (MRS) for customer.\n\u2022 Implemented Cisco Secure Access Control Server (ACS-5.2) with TACACS+.\n\u2022 Data Center Management which includes Power and Rack space management to ensure power distribution with failover redundancy.\n\u2022 Manage projects involving system installation, PDU maintenances, infrastructure upgrades and monitor overall health of Data center, Comms/Server cabinets.\n\u2022 Involved in design and renovation of client's data center. Managed and implemented the entire DC BCP/DR procedure.\n\u2022 Monitor the LAN/WAN Utilizations and traffic optimization issues. Implemented Solarwinds for the client for centralized management of all network devices.\n\u2022 Optical connections between devices using LC, SC, SFC and GBICs.\n\u2022 Interact with ISP, Vendors for WAN/Device related issues.\n\u2022 IOS/Firmware upgrade for Routers and switches whenever required.\n\u2022 Responsible to manage firewall rule sets in Checkpoint firewalls with appropriate change approvals. Perform installs, configure and troubleshooting on stateful inspection firewalls.\n\u2022 Configuring the IPSEC VPN (site to site) on Cisco ASA and identify, resolve firewall VPN connectivity issues.\n\u2022 Managing corporate Checkpoint Firewall implementing security rules and mitigating network attacks."", u""Network Engineer\nWIPRO TECHNOLOGIES\nAugust 2009 to August 2010\nWork Location: Chennai, India\n\nHardware Worked:\nCisco Routers: 2800, 3800, 7206 VXR.\nCisco Switches: 2900, 3500,3700,6000,6500\n\nTools: Secure CRT, Men&Mice, BMC Patrol, Cisco WCS, MS Visio, BMC SDE\n\nJob Responsibilities:\n\u2022 Provide the remote support service for LAN, WAN of customer.\n\u2022 Handling the entire customer's Data Center. Configuration management and Support.\n\u2022 Remote Administration of Routers and Switches, WAN circuit repair and troubleshoot.\n\u2022 Link related other than site down, CRC errors, Link flaps and latency issues.\n\u2022 Verifying the configuration for (BGP/EIGRP/OSPF Sub-interfaces etc.)\n\u2022 Dealt with Slow response problems. Any addition/deletion made to configuration.\n\u2022 Troubleshooting of Cisco Catalyst Switches and Routers, which includes ACL Management, VLAN Configuration.\n\u2022 Configured access ports, trunk ports. Verifying the configuration for switches (VLAN, STP). Add/Remove ports from respective VLAN's as per RFC's.\n\u2022 Handled Wireless related issues like adding MAC addresses to wireless controllers, checking the WLAN controller's status, checking the active profiles.\n\u2022 Responsible for providing Network diagram using Visio, excel and Word."", u'IP Data Engineer in NOC\nWIPRO TECHNOLOGIES - Bengaluru, Karnataka\nJune 2008 to July 2009\nWork Location: Bangalore, India\n\nHardware Product: Cisco\nOperating system: Win 2k\nHardware Handling: WYSE terminal\nTools: Netcool, Remedy, puTTy, live link, CMDB, BT Wholesale, BT Broad band.\n\nJob Responsibilities:\n\u2022 FLT Team will raise proactive tickets for IP Connect, IP Native services. As a part of L1, L2 diagnostics we use to localize the fault.\n\u2022 Analyzing scripts checking the Interface status and backup status.\n\u2022 Monitor the link status, and clear counters.\n\u2022 Provide Level 1& Level 2 support for troubleshooting of Network related problems.\n\u2022 Raising fault ticket with Vendors and follow-up.\n\u2022 Used Remedy Service Management tool for incident and solution management.\n\u2022 Troubleshooting WAN link and coordinating with ISP.\n\u2022 Diagnostic the fault & troubleshoot the fault and escalate the issue whenever required within stipulated time.']","[u""Master's""]",[u'']
0,https://resumes.indeed.com/resume/893f6f3d492d7119,"[u""Data Modeler/Data Warehouse Engineer\n21st Century Fox\nMarch 2016 to Present\nRoles & Responsibilities:\n\u2022 Point-person for integrating, consolidating, and data modeling for the new Fox Network Group's main system into the Enterprise Data Warehouse.\n\u2022 Design, build, and maintain data abstraction/virtualization layer to facilitate data blending across disparate data sources using Cisco Composite System.\n\u2022 Gather requirements, build data models, and provide data sources for BI Reporting Teams.\n\u2022 Gather requirements and deliver complex and efficient database procedures (Tables, SQL Stored Procedures, and Views)\n\u2022 Support ETL Team for critical data movements and created source to target mapping documents.\n\u2022 Responsible for Cloud Migration(AWS) of the existing projects using Informatica.\n\u2022 Involved in optimizing queries for best performance. Created partitioned tables and Indexes.\n\u2022 Derived the physical model from the database and further created the subject area models using Power designer and ERWIN.\n\u2022 Created API's to access the data virtualization layer and worked with the APIGEE team for Integration and security.\n\u2022 Analyzed existing Conceptual and Physical data models and altered them using Erwin to support enhancements.\n\u2022 Build and maintain Dashboards using Qliksense and Palantir\n\u2022 Worked with DBA's to create a best-fit physical data model from the logical data model.\n\u2022 Created a Normalized logical model for the business to understand the relationships and at the deeper level of granularity.\n\u2022 Exhaustively collected business and technical Metadata and maintained naming standards.\n\u2022 Reverse engineering of the existing reports to identify the key data elements and data entities required to design the data warehouse.\n\u2022 Used Model Mart of ERWIN for effective model management of sharing, dividing and reusing model information and design for productivity improvement.\n\u2022 Developed Star and Snowflake schemas based dimensional model to develop the data warehouse.\n\u2022 Worked on slowly changing dimension tables and hierarchies in dimensions.\n\u2022 Monitor the load and requests on the virtual layer using Unix server and schedule the ETL jobs for DW using SSIS.\n\u2022 Expertise in creating data lineage and provide access to the business users by creating API's\n\u2022 Create and manage LDAP users and their access to the resources in the virtualization layer."", u'Calisto Corp\nJanuary 2015 to February 2016\nData Modeler\n\n\u2022 Created conceptual, logical and physical data models and updated according to the updates in the business requirements.\n\u2022 Manage the Metadata for the Subject Area models for the Data Warehouse environment. Highly involved in maintaining the Data Dictionary for all the models across Enterprise.\n\u2022 Worked with ETL Team to load sequential files (flat files, text files, CSV files) in data stage.\n\u2022 Involved in writing complex queries as part of data analysis and producing fact tables.\n\u2022 Managed multiple OLAP and ETL projects for various needs.\n\u2022 Responsible for the replication of the tables into the Operational Data Store(ODS)\n\u2022 Debugging the SQL-Statements and stored procedures for various business scenarios.\n\u2022 Developed advanced SQL queries to extract, manipulate, and/or calculate information to fulfill data and reporting requirements including identifying the tables and columns from which data is extracted.\n\u2022 Executed the scripts that invoked SQL loader to load data into tables.\n\u2022 Performed extensive Data Validation, Data Verification against Data Warehouse.\n\u2022 Implemented slowly changing dimensions Type2 and Type3 for accessing history of reference data changes.\n\u2022 Worked with the database administrators (DBAs) to finalize the physical properties of the tables.\n\u2022 Extensively made use of Compressor, Table Spaces, Indexes, Sequences, Materialized Views, Procedures and Packages in Data Models.\n\u2022 Tested and validated the BO reports by running similar SQL queries against the source system(s).\n\u2022 Tested several Informatica Mappings to validate the business conditions.\n\u2022 Responsible for Data Mapping, Data Governance, Transformation and cleansing rules for the Master Data Management Architecture involving OLTP, ODS and OLAP.\n\u2022 Testing the source and target databases for conformance to specifications.\n\u2022 Developed and executed various manual testing scenarios and exceptionally documented the process to perform functional testing of the application.\n\u2022 Design and execute the test cases on the application as per company standards and tracked the defects using HP Quality Center 9.2.', u'Data Modeler/ Analyst\nMaveric systems - IN\nJanuary 2012 to August 2014\nRoles & Responsibilities:\n\u2022 Worked on creating a custom data warehouse for Financials (General Ledger, Account Receivables & Account Payables).\n\u2022 Conducted Joint Application Design (JAD) sessions with business users to gather data warehouse requirements.\n\u2022 Worked on Data Modeling Standardization process across enterprise and created multiple documents\n\u2022 Modeled new tables and added them to the existing data model using Erwin as part of data modeling.\n\u2022 Used forward engineering to create a Physical Data Model with DDL that best suits the requirements from the Logical Data Model.\n\u2022 Implemented Referential Integrity using primary key and foreign key relationships.\n\u2022 Worked with Database Administrators, Business Analysts and Content Developers to conduct design reviews and validate the developed models.\n\u2022 Worked on specifications given by the Data Governance team and Data Quality team that required managing the master data from all the business units and ensuring data quality across the enterprise\n\u2022 Exhaustively collected business and technical metadata and maintained naming standards.\n\u2022 Analyze the OLTP Source Systems and Operational Data Store and research the tables/entities required for the project. Designing the measures, dimensions and facts matrix document for the ease while designing.\n\u2022 Create the data results through techniques and tools such as basic SQL queries, data mining, and multidimensional analysis.\n\u2022 Participated in performance management and tuning for stored procedures, tables and database servers.\n\u2022 Facilitated in developing testing procedures, test cases and User Acceptance Testing (UAT)\n\u2022 Integrated the work tasks with relevant teams for smooth transition from testing to implementation phase.\n\u2022 Developed necessary support documentation for end users, operations and testing teams.\n\u2022 Worked closely with OBIEE reporting users to provide/design the Ad-hoc reports.\n\u2022 Tested and validated the reports by running similar SQL queries against the source system(s).\n\u2022 Interacted with end users to identify key dimensions and measures that were relevant quantitative.']",[],[]
0,https://resumes.indeed.com/resume/cb77dcfe0d2526fc,"[u'Bid Data Engineer\nDatascichaos.com - Washington, DC\nSeptember 2017 to March 2018\n\u2022 Designed and developed high-performance data processing platform based on Apache Kafka and Apache Zookeeper.\n\u2022 Imported, transformed and set in the right JSON format the recordings collected from a data acquisition system using Python.\n\u2022 Created MongoDB shared cluster for storage.\n\u2022 Used Apache Kafka for log/event analysis.\n\u2022 Hands on experience working with Elasticserach, Kibana, Elastic queries and other Kafka connectors.\n\u2022 Implemented bash script to automate system deployment with Docker, Nginx on AWS EC2/ECS.', u'Ruby on Rails Developer\nDatascichaos.com - Washington, DC\nMay 2016 to September 2017\n\u2022 Developed consumer-based features for online education application using Ruby on Rails, JavaScript, JQuery, HTML, CSS and Bootstrap.\n\u2022 Hands on experience in database modeling and database migration with ActiveRecord.\n\u2022 Implemented user registration and admin system using MVC architecture in Ruby on Rails.\n\u2022 Designed RESTful API for the application and implemented with REST URL Helpers in Rails.\n\u2022 Developed shopping cart application and assure payment security and reliability with Stripe API.\n\u2022 Built course recommend system for signed users based on their interests.\n\u2022 Implement Behavior Driven Development using RSpec and Capybara.']","[u'Master of Science in Computer Science in Computer Science', u'Bachelor of Science in Physics in Fujian']","[u'The George Washington University Washington, DC\nJanuary 2016 to January 2018', u'Xiamen University\nSeptember 2011 to June 2015']"
0,https://resumes.indeed.com/resume/adf0bf5a02692dfd,"[u""Data Analyst\nCiti Group Inc - Buffalo, NY\nNovember 2017 to Present\n\u2022 Maintained integrity of Citi's Market Risk data used for risk limit reporting, VaR, Stress VaR calculations by implementing daily data quality scorecards and performing root cause analysis in case of data quality issues for Citi's various trading businesses.\n\u2022 Automated the daily reconciliation processes using VBA and enhanced SQL queries and macros for an improved data quality accuracy.\n\u2022 Maintained close working relationship with Market Risk Management, different Business Units and control groups by providing necessary information for audit, threshold reviews, stress testing and ad-hoc queries.\n\u2022 Worked with key partners to perform and deliver data tracing reviews between the various systems and data mappings to improve reporting on a weekly basis."", u'Business Intelligence Analyst\nBuffalo, NY\nDecember 2016 to January 2017\n\u2022 Performed a comparative study to evaluate the capabilities of a relatively new Business Intelligence tool, Pinpoint, and Tableau using health care insurance data related to charges and payments.\n\u2022 Translated the real world business requirements already developed in Pinpoint into reports and data visualizations in Tableau to help hospital staff improve revenue and receivable cycle management, delay reduction and trend/pattern mining.\n\u2022 Created reports and dashboards to optimize scheduling efficiency by projecting FTE needs by department, seasonality, function etc. and to evaluate efficiency/productivity by employee, equipment, department, etc.', u'Senior Systems Engineer, Data Analyst\nInfosys Limited\nJuly 2013 to July 2016\n\u2022 Developed solutions for Budgeting and Forecasting, integrated it with the other ancillary applications, and acted as the SPOC for a FTSE 100 client, while working in an agile scrum environment.\n\u2022 Designed and delivered reports based on important Trade Promotion Management related KPIs that helped business in planning and forecasting each promotion more accurately and efficiently.\n\u2022 Steered a team of 6 comprising of BI and CRM consultants, monitored the project progress, coordinated with clients, global team members and other stakeholders to prioritize the issues and ensure timely and accurate resolutions on daily basis.\n\u2022 Elicited, gathered and documented the design changes, developed crucial functional specification, business flow diagrams and high-level design documents. Provided knowledge transfer and mentored to the junior team members and end users.\n\u2022 Assisted business financial planners analyze the root cause for data inconsistencies during reconciliation process using MS Excel features like pivot tables and v-lookups.\n\u2022 Streamlined the month end reporting process by removing the dependency on the offshore team and enabling the business to complete their analysis and reconciliations 15 hours in advance.\n\u2022 Extensively worked on data modelling and complex transformations using SQL for data extraction from Finance & Sales data sources.']","[u'MS in Management Information Systems in Management Information Systems', u'B.Tech in Electronics in Electronics & Communication']","[u'SUNY, UNIVERSITY AT BUFFALO\nJune 2017', u'KERALA UNIVERSITY\nJune 2012']"
0,https://resumes.indeed.com/resume/5845301591038a1d,"[u""Senior software Engineer/Data Engineer\nBank of America - Pennington, NJ\nJanuary 2017 to Present\nRefactor the architecture of MongoDB to make the environment highly available and fault tolerant by a. adding multiple secondary nodes to the cluster in different data centers.\nb. adding delayed secondary to recover from accidental deletion of data\n\u2022 Lead the end-to-end implementation MongoDB upgrade project in order to ensure more stability in the environment.\n\u2022 Design and automate the deployment of the following in cloud platform.\n\u2022 NoSQL Database (MongoDB, Elasticsearch) cluster for the corporate cloud platform.\n\u2022 Version control repository (Bit bucket Datacenter) for the corporate cloud platform\n\u2022 Hadoop cluster to archive Elasticseach indexes.\n\u2022 Migrate and upgrade jFrog artifactory to ensure compliance with the standards at the Bank.\n\u2022 Design and Architect Machine Learning capabilities for the corporate cloud platform, which will be used to forecast the capacity.\n\u2022 Develop Dashboard on the system's metric and generate alert and take preventive action to avoid unplanned downtime. Reduced more than 95% percent unplanned downtime.\n\u2022 Automate the Deployment of new features to the cloud platform. Reduced the 60-70% manual deployment by automation"", u'Data Engineer/Analyst\nCisco Systems - Research Triangle Park, NC\nOctober 2015 to December 2016\n\u2022 Built a metrics gathering, storing and reporting framework for the Management team to consume and make data driven decision to plan the workload on the developers of different teams.\n\u2022 Automated the building and configuration (using ConcourseCI) of private repository for different kinds of artifactories such Yum, Docker etc. in Jfrog Artifactory.\n\u2022 Achieved 60% reduction of manual testing by developing automated functional tests to verify the functionality of Kafka, RabbitMQ, and Streamsets.\n\u2022 Achieved 95 % reduction on unplanned downtime by developing an automated predictive alert system to ensure on time preventive maintenance of servers.\n\u2022 Delivered a predictive model by using random forest classification model to predict possibility of denial of service attack. Improved the accuracy to 95% by using grid search technique', u'Data Scientist Intern\nLear Corporations - Southfield, MI\nMay 2015 to August 2015\n\u2022 Led an initiative to deliver an analytical data model to help improve the product development cycle, integration, data transparency and better collaboration tools to all the functional teams engaged and analyze the impact of system change to downstream systems.\n\u2022 Created a linear regression model with 95% accuracy to predict the demand for a given month to plan for the required inventory level.', u'Data Analyst\nMichigan State University - East Lansing, MI\nJanuary 2015 to May 2015\n\u2022 Achieved more than 95% accuracy on finding the appropriate criteria to hire a celebrity for product endorsements. This was achieved by extracting the celebrity posts and tweets from social media websites and training a classification model.\n\u2022 Attained 94% accuracy on a liner regression model by using residual plot analysis, box-cox transformation. The model predicted the expected popularity of a celebrity in the long run.\n\u2022 Recommended the business the combinations of products to cross-sale to increase (expected 15%) the average order value by using association analysis (apriori).', u'Data Engineer/Data Analyst\nJohnson and Johnson\nJuly 2013 to December 2014\n\u2022 Delivered Global Business Intelligence Framework that provides 360-degree view of the performance of the company across the Globe. Analyzed the average selling price(s) of top 20 products across US and EMEA to set the optimum prices for those products in those regions.\n\n\u2022 Transformed data into visually appealing stories for 500+ users using data visualization tools to facilitate business decision making. The reports and dashboards reduced manual workload.\n\n\u2022 Led a project to deliver more than 200 Data Integration (ETL) processes to build the Data Mart/Data Warehouse by extracting, transforming and loading the data into Data Mart/Data warehouse from different source systems.', u'Data Engineer/Data Analyst\nFebruary 2009 to July 2013\n\u2022 Built 150 + Data Integration processes to extract data from source systems and developed reports to display different key performance indicators that help business to improve operational efficiency.\n\u2022 Re-engineered and optimized existing Informatica Data Integration processes (100+) to reduce manual workload. Optimized the query performance by removing row-chaining and gathering statistics.\n\u2022 Reduced more than 30% of manual workload by delivering an automated process to refresh more than 100 cubes and publish those cubes to web portal for consumption.', u""Data Engineer/Data Analyst\nIBM Cognos\nAugust 2005 to February 2009\n\u2022 Developed and Designed an Enterprise Data Warehouse, which reduced the dependency of the business on the IT to run complex queries to generate reports.\n\u2022 Migrated existing legacy reporting solution to the IBM Cognos to ensure efficient and consistent reporting across the organization i.e. single version of truth.\n\u2022 Automated the execution of more than 200+ ETL processes by writing UNIX shell\n\nADDITIONAL PROJECTS\nKAGGLE COMPETITONS\nKernel Link: https://www.kaggle.com/iamchanchal/kernels\n\u2022 Data Science for Good: Kiva Crowdfunding: The objective is to pair Kiva's data with additional data sources to estimate the welfare level of borrowers in specific regions, based on shared economic and demographic characteristics.\n\u2022 House Prices: Advanced Regression Techniques: The aim of this project is to accurately predict the price of homes.\n\u2022 Predicting Red Hat Business Value: The aim of this project is to classify customer potential by analyzing the behavior of individual customer.""]","[u'Master of Science in Business Analytics', u'Bachelor of Technology in Computer Science and Engineering']","[u'Michigan State University East Lansing, MI\nJanuary 2015 to December 2015', u'Kalyani Government Engineering College Kalyani, West Bengal\nJuly 2001 to July 2005']"
0,https://resumes.indeed.com/resume/da468f039f93c268,"[u""Data Scientist Intern\nSoothsayer Analytics - Dearborn, MI\nJanuary 2018 to Present\nApply statistical modeling and Natural Language Processing methods in Python to understand consumer shopping habits,\nand predict product demand from online text conversation datasets\n\u2022 Categorize all these text conversations into business-relevant topics using Latent Dirichlet Allocation\n\u2022 Use lexicon based approach to extract sentiment and people's emotion in text"", u'Research Assistant\nWearable and Signal Processing Lab, University of Michigan - Dearborn, MI\nMay 2017 to December 2017\nCollected, cleansed and provided modeling and analyses of structured and unstructured data sets using MATLAB,\ndesigned Deep Learning convolutional neural networks with different convolutional, pooling and dropout architectures\n\u2022 Examined batch normalization procedures for model regularization to avoid overfitting, delivered research papers', u'Software Engineer\nProhaktiv Inc - Pune, Maharashtra\nAugust 2014 to December 2016\n\u2022 Migrated application from .NET to Laravel 5.2 backend, and Angular Js Frontend, created RESTful API\n\u2022 Integrated application with Google Analytics and Google AdWords to download the analytics data\n\u2022 Built App that communicates with RESTful services, integrated Cordova into an existing web application\n\u2022 Interacted with clients and business subject matter experts for gathering requirements for the project\n\u2022 Wrote custom modules in Drupal using Drupal APIs such as Node API Hooks, and Forms API Hooks']","[u'Master of Science in Computer and Information Science in Advanced Artificial Intelligence', u'in Technology']","[u'The University of Michigan Dearborn, MI\nJanuary 2017 to Present', u'Uttar Pradesh Technical University Ghaziabad, Uttar Pradesh\nAugust 2009 to June 2013']"
0,https://resumes.indeed.com/resume/7b5a5e66956057cf,"[u'Senior Data Engineer\nGE Healthcare - Bengaluru, Karnataka\nOctober 2016 to December 2016\n\u2022 Streamlined the ETL processing of medical devices\u2019 log data by developing UDFs / modules in Java and batch processing the data on Hadoop using Sqoop and Hive', u'Software Engineer\nGE Healthcare - Bangalore, India\nJuly 2014 to October 2016\n\u2022 Built RESTful APIs using Spring Boot and UI using React JS & Redux framework; Wrote Junit test cases using Mockito framework\n\u2022 Developed a notification system to track medical risks and appointments of patients using ActiveMQ; Integrated SMS gateways\n\u2022 Designed a rule-engine based on disease protocols configured in JSON to notify care-providers, the patients with medical risks\n\u2022 Implemented offline app. capability using Service Workers to record patient information even in areas with intermittent network\n\u2022 Refactored code to fix critical issues using SonarQube and used Jenkins for CI & CD of application on AWS EC2 instance']","[u'Master of Computer Science in Computing, Databases', u'Bachelor of Engineering in Computer Science & Engineering']","[u'New York University New York, NY\nJanuary 2019', u'Manipal Institute of Technology\nMay 2014']"
0,https://resumes.indeed.com/resume/ef5a0edd9b2e1406,"[u'Data Analytics Intern\nMatrixCare - Bloomington, MN\nMay 2017 to August 2017\n\u2022 Automated the setup of the Data Science Lab on Azure Cloud using Azure ARM templating.\n\u2022 Performed Exploratory Data Analysis (EDA) to understand the data available and detect any outliers present.\n\u2022 Identified, extracted and combined various data points to predict if a patient will fall down based on fall events in the past.\n\u2022 Extracted most valuable predictors, and ran and evaluated different ML models using ROC curves.\n\u2022 Highest accuracy of 80.01% and F-Measure of 78.27% was identified for Random Forest algorithm.', u""Software Engineer\nCerner Healthcare Solutions Pvt. Ltd - Bengaluru, Karnataka\nJanuary 2014 to May 2016\nTook ownership of full stack development of multiple modules, including integrating AboRh (Cerner's blood bank transfusion\nsystem) with Bedrock, a core product used to configure the Cerner Millennium database.\n\u2022 Reduced the response time of Bedrock from 1.5 minutes to 20 seconds by rewriting the CCL (Cerner Command Language, SQL\nwrapper) join queries and pushing down filters.\n\u2022 Mentored new recruits on the product and the programming framework.""]",[u'Masters in Computer Science'],"[u'Arizona State University Tempe, AZ\nAugust 2016 to May 2018']"
0,https://resumes.indeed.com/resume/b78b88b4da7875c3,"[u'Treasurer\nWhitepath Fab Tech - Canton, GA\nJanuary 2017 to Present\nMaintaining the following for the church: tithes, bills, missionary offerings.', u""Manufacturing Engineer\nWhitepath Fab Tech - Ellijay, GA\nApril 2015 to Present\nQuoting, establishing, and maintaining manufacturing goods for the facility to mass produce. Cost analysis and process\nimprovement work, thus far saving the company $80,000 a year.\n\u2022 Microsoft Excel Projects, with VBA include:\no Creating a program to scan, count, sort, and print shipping information using the program Bartender, where previously workers were manually counting and inputting all information, improving efficiency by 90%.\no Creating a standardized quote template that can also retrieve and dump information into the company database.\no Creating a wire tracking program showing where any wire is or should be within the facility.\no Creating a paperless wire instruction program, allowing production laborers to search and open job plans.\n\u2022 Bartender (Bartender is label creation/editing software) Projects include:\no Switching labels connected to a manual excel spreadsheet database to the company's database - MOLE,\nautomatically updating any information needed on labels.\no Reducing the amount of label options by 75% by removing, updating, and standardizing labels.\no Minimizing time fixing labels by switching the method of how production co-workers print labels."", u'Data Analyst/Engineer Intern\nDept. of Water Resources, Gwinnett County\nMay 2014 to August 2014\nImplementing and improving the maintenance planning and scheduling program for two water production plants and three water reclamation maintenance plants. Work with planning dept. to create reusable job plans in Maximo CMMS, to\nallow a Deming cycle of learning and start 100% weekly scheduling to improve productivity. Optimizing preventive\nmaintenance (PM) plans by reviewing past actuals versus planned estimates. Largest savings was worth over $126,000\nper year reducing 4,200 annual PM labor hours at the F. Wayne Hill plant.', u'Math Lab Tutor\nKennesaw State University - Kennesaw, GA\nAugust 2010 to July 2011\nTutoring university students in: Algebra, Pre-calculus, Calculus 1-3, Linear Algebra, Differential Equations.']","[u'B.S. in Industrial Engineering in conc. Supply Chain', u'', u'in International Studies Highlights']","[u'Georgia Institute of Technology Atlanta, GA\nJanuary 2009 to January 2013', u'Kennesaw State University Kennesaw, GA\nAugust 2009 to July 2011', u'presenting research on excessive mobile phone usage to Tokyo University of Marine & Science Tech Kyoto, JP\nDecember 2010']"
0,https://resumes.indeed.com/resume/d0ce454bedfd9a01,"[u'Data Engineer\nDun & Bradstreet - Waltham, MA\nMay 2016 to Present\nDevelop and deploy Python automation scripts enhancing data auditing and reporting abilities using\nasynchronous multiprocessing resulting in drastically reduced runtimes by 90%\n\u2022 Redesign and convert outdated scripts and procedures to PEP8 standard in Python\n\u2022 Deployed code to a remote server using fabric\n\u2022 Maintain and update current and legacy code using local Bitbucket repositories\n\u2022 Fully automated the daily data ETL of the Verification Services team\n\u2022 Create stored procedures in MySQL to improve data handling and ETL transactions\n\u2022 Provide data validation and reporting for customer service and sales teams', u'Project Manager/ Web Developer\nOpen Data Science Conference - Cambridge, MA\nFebruary 2016 to May 2016\n\u2022 Worked directly under the founder and CEO providing project budget, delivery, and problem resolution\n\u2022 Managed 5 people on multiple projects using agile methodology\n\u2022 Translated customer needs over to our development and design teams\n\u2022 Prioritized weekly sprints and manage progress through daily scrum\n\u2022 Analyzed product data to better understand customer needs and product features\n\u2022 Launched the production of the Open Data Science Conference consisting of over one hundred speakers,\ntwenty workshops, and over three thousand attendees', u'Software Support Engineer\nPredictive Index - Wellesley, MA\nJune 2015 to December 2015\n\u2022 Lead Administrator of all tools and products used by clients, partners, and internal production.\n\u2022 Programmed and maintained data and queries in SQL.\n\u2022 Documented and analyze customer issues for the engineering team\n\u2022 Implemented a 24 hour turnaround time for all ticket requests\n\u2022 Created plots, charts, and graphs as per the requirements from team members\n\u2022 Assisted in our client churn analysis project, resulting in a 13% client retention increase\n\u2022 Mentored and trained other technical support personnel']",[u'B.Sc. in Computer Information Systems'],"[u'Bryant University Smithfield, RI\nJanuary 2011 to January 2015']"
0,https://resumes.indeed.com/resume/9cafd35cd323e7a7,"[u'Data Analytics Engineer\nGE Power - Atlanta, GA\nAugust 2015 to Present\n\u2022 Used Keras and TensorFlow to create and train neural networks to act as surrogate models for thermal performance simulation.\n\u2022 Created and automated a Python-based data pipeline which queries and processes timeseries gas turbine performance data in monthly batches and stores the results in a local MS SQL database. Also created a Tableau dashboard which is linked to this database that has been viewed over 1,000 times in 4 months.\n\u2022 Led a team in creating and maintaining a portal to the gas turbine performance organization\u2019s Tableau dashboards. This effort involved networking throughout the organization to determining important metrics to report to executives and building the necessary data pipelines and dashboards.\n\u2022 Converted a business-critical, legacy system written in a proprietary language to Python. The new code is about 10% faster than the legacy system and it can be run on a desktop in addition to the required ability to run in the organization\u2019s analytics infrastructure.\n\u2022 Built the Python interface to our thermal performance simulation DLL which is now used across the organization.\n\u2022 Create tools to make other people\u2019s jobs easier \u2013 This includes making tools which integrate into coworkers\u2019 Excel spreadsheets to improve efficiency and creating a unix-based virtual environment with Spark and other development tools preinstalled to overcome issues with running Spark on Windows.\n\u2022 Utilize agile software development principles as a member of a SCRUM team and practice test driven development and paired programming during development cycles in order to ensure robust operation and team familiarity with the code.\n\u2022 Often give technical briefings to large groups and share progress reports with executives.', u'Teaching Assistant\nUGA College - Athens, GA\nJanuary 2014 to August 2015\n\u2022 Teaching Assistant (TA) for senior-level Feedback Controls (spring \u201814) and Linear Systems (fall \u201814) classes and TA for an introductory mechanical engineering course with about 150 students (spring \u201915). Graded assignments and held weekly office hours for these classes. Also gave a fill-in lecture for Feedback Controls.']","[u'Master of Science in Optical Engineering', u'Bachelor of Science in Physics']","[u'University of Georgia Athens, GA\nMay 2015', u'University of Georgia Athens, GA\nDecember 2012']"
0,https://resumes.indeed.com/resume/95848caaacdebc1d,"[u""Data Analytics Engineer\nRaytheon - Orlando, FL\nAugust 2017 to Present\nAs a data engineer, I support Raytheon's Warfighter FOCUS, where I provide analytics for the Sustainable Range Program (SRP). My two primary focus areas are sustainment\ncosts and lifecycle analysis of Army training devices worldwide. In addition to this, I\nassist other platforms as needed. My other responsibilities are listed below.\n\no Data preprocessing and visualization\no Data scrubbing automation\no Ad hoc request fulfillment\no Data quality analysis"", u""Technology Consulting Sr. Analyst\nAccenture - Sacramento, CA\nOctober 2013 to July 2016\nAs a senior data analyst, I supported Covered CA's marketing decisions during open\nenrollment, which led to 1.4 million Californians with health insurance as part of the ACA. I provided clear and concise data through the first year, where we surpassed the\nState's goal of 1.2 million enrollments and achieved self-sustainability. By the second\nopen enrollment period, I produced ad hoc reports to aid renewal efforts, resulting in a\n92% retention rate and 500K new enrollments. My other responsibilities are listed below.\n\no Developed extracts using PL/SQL\no Reconciled data discrepancies\no Designed and tested queries\no Analyzed data integrity\n\nTECHNICAL/NONTECHNICAL SKILLS:\n\u2022 PL/SQL, T-SQL, Python, R, Statistics, Data Mining, Machine Learning, Scikit-Learn,\nSQL Developer, SQL Server, Access, Hadoop, Spark, MapReduce, AWS, D3.js, Tableau""]","[u'MS in Data Science in Data Science', u'BS in Mechanical Engineering in Mechanical Engineering']","[u""Saint Mary's College Notre Dame, IN\nAugust 2018"", u'University of Puerto Rico Mayag\xfcez, PR']"
0,https://resumes.indeed.com/resume/8b2081087b0987ac,"[u'Data Science Intern\nEventbrite - San Francisco, CA\nOctober 2017 to Present\nDesigned a machine learning model to predict sister event categories and recommend new events to customers. Python,\nPandas, Scikit-Learn, Machine Learning, SQL, Collaborative Filtering\n\u2022 Using an unstructured dataset, modeled the product usage behavior of buyers and defined the customer engagement metrics.\nThis helps the eventbrite marketplace to target customer retention. Python, Pandas, Presto, SQL', u'Data Engineer\nLogMeIn - Bengaluru, Karnataka\nSeptember 2016 to June 2017\nLead developer for a system that autogenerates daily analytics of product feature usage, which helps in reporting product\nhealth. Developed the complete workflow from event parsing to ETLs to Tableau dashboard. Java, Hadoop MapReduce, Pig,\nHive, Redshift, RDS, Tableau\n\u2022 Responsible for migration of data marts from legacy Oracle systems to AWS. Reduced cost by 50% and improved performance\nby 30% compared to traditional ETLs using RDBMS. Java, Pig, Hadoop, Hive, Postgresql, Oozie', u""Data Engineer\nCitrix R&D - Bengaluru, Karnataka\nJuly 2014 to August 2016\n\u2022 Designed and developed a cross-sell recommendation engine using customer's provisioning and usage data. This idea started as a hackathon project and won the 1st position globally. Python, R, Supervised Learning, Tableau\n\u2022 Built a machine learning model to forecast the chances of churning of a customer, along with finding top factors that influence\ncustomer retention. This project was again conceived as a hackathon idea, which one 1st position in India level, and, went on to be actively used across all BU teams today. R, HTML5, Shiny""]","[u'Masters of Science in Analytics in Analytics', u'Bachelor of Engineering in Computer Science in Computer Science']","[u'University of San Francisco San Francisco, CA\nJuly 2017 to June 2018', u'PES Institute of Technology Bengaluru, Karnataka\nSeptember 2010 to May 2014']"
0,https://resumes.indeed.com/resume/12db6420e1e71ce2,"[u'Data Engineer\nElder Pharmaceuticals Ltd - Mumbai, Maharashtra\nJuly 2012 to July 2015\n\u2022 Developed and maintained software solutions using SQL to address business requirements\n\u2022 Prepared monthly SQL reports by understanding the requirements received from business stakeholders\n\u2022 Interfaced with Engineers, and Analytics team to understand and aid in the implementation of database requirements and analyze the performance', u'Management Trainee\nFinance & MIS\nJuly 2010 to June 2012\n\u2022 Controlled and monitored on-going project schedule and expenses\n\u2022 Analyzed planned financial outcomes vs the actual financial outcomes to identify the project variance']","[u'Master of Science in Information Systems and Technology', u'Bachelor of Engineering in Computer Science']","[u'California Lutheran University Thousand Oaks, CA\nAugust 2015 to May 2017', u'Mumbai University Mumbai, Maharashtra\nAugust 2006 to May 2010']"
0,https://resumes.indeed.com/resume/2f3293aff8508466,"[u""Research & Development Engineer\nBristol-Myers Squibb Company - Lawrenceville, NJ\nSeptember 2016 to April 2017\n\u2022 Reduced energy costs of scientific operations by implementing advanced laboratory assets into new research facilities\n\u2022 Negotiated with research teams and reviewed suggested changes for laboratories to achieve Sustainability 2020 Goals\n\u2022 Directed the Green Laboratories Certification Program to enroll scientists through workspace evaluations and initiatives\n\u2022 Presented best practices to Discovery, Pharmaceutical Development, Clinical & Translational Research, and GMS Biologics\n\u2022 Aided Jacobs Engineering Group to design Lawrenceville's new Discovery Research facility with respect to cost metrics\n\u2022 Launched the Group Responsible for Ecological Engagement Network to deliver company-wide sustainability practices\n\u2022 Participated in the BMS Workplace Concierge for Princeton Pike to provide occupants with multiple support channels"", u""Research Investigator\nBristol-Myers Squibb Company - Lawrenceville, NJ\nDecember 2016 to March 2017\nDiscovery Immuno-Oncology Stretch Assignment December 2016 - March 2017\n\u2022 Administered explicit forms of research to design and execute scientific experiments in Discovery and Medicinal Chemistry\n\u2022 Evaluated phosphorylation methods to stop the disruption between two domain proteins affiliated with genetic diseases\n\u2022 Applied STD-NMR fragment screening for protein preparation to transfer magnetization from proteins to small molecules\n\u2022 Investigated a nucleophilic reaction between a carboxylic acid and a polar amine forming a homochiral intermediate amide\n\u2022 Tested a small molecule's separation based on polarity through High Performance Liquid Chromatography operations\n\u2022 Synthesized a bacterial endotoxins test for a large molecule oncology product to support Global Manufacturing & Supply\n\u2022 Assessed results of each solution with a fixed number of endotoxins to measure the product's turbidity with respect to time"", u'Software Engineer\nBristol-Myers Squibb Company - Nassau, BS\nSeptember 2015 to March 2016\n\u2022 Managed iOS mobile applications for medications including Coumadin, Eliquis, Erbitux, Opdivo, Orencia, Sprycel, and Yervoy\n\u2022 Developed web interfaces spanning the Research & Development, Global Business Operations, and Commercial departments\n\u2022 Evaluated trends of new technologies and software to ensure devices deliver solutions through each mobile application\n\u2022 Advanced application user experiences by adjusting codes using JavaScript, HTML & CSS, and Objective-C languages\n\u2022 Executed each software and hardware configurations project with a System Development Life Cycle process\n\u2022 Hosted meetings with Accenture to test prototypes of BMS 360, inSite, and SharePoint webpage interfaces', u""Data Analyst\nBristol-Myers Squibb Company - Hopewell, NJ\nSeptember 2014 to March 2015\n\u2022 Rotated throughout projects in Information Management, Data Analytics, Finance, and Network Engineering teams\n\u2022 Supervised network trends and monitor systems to manage capacity across Bristol-Myers Squibb's wide area network\n\u2022 Conducted global network performance services with software from British Telecom and Cisco to monitor data us age\n\u2022 Constructed a system to validate data sources and invoices between Bristol-Myers Squibb and British Telecom\n\u2022 Upgraded the company's Microsoft Outlook On-Premise 2007 to Office 365 in a collaborative work environment\n\u2022 Coordinated time signals for each end user's email account to be migrated to a cloud-based storage system\n\u2022 Analyzed simulations of connections between Microsoft Office 365 and the Bristol-Myers Squibb server""]","[u'Bachelor of Science in Chemical Engineering & Biological Engineering in Chemical Engineering & Biological Engineering', u'in Business Administration']","[u'Drexel University, College of Engineering Philadelphia, PA\nJune 2018', u'LeBow College of Business']"
0,https://resumes.indeed.com/resume/c954f53dd1005c20,"[u'Test Data Management Engineer\nBMW of North America\nNovember 2017 to Present\nResponsibilities:\n\u2022 Worked with Fast Data Masker to Mask the sensitive data\n\u2022 Worked with different sources like XML, EDI, Excel, CSV and DB2 tables\n\u2022 Involved in refresh data into non production environments like integration and testing\n\u2022 Implemented the data reservation feature through the TDM portal.\n\u2022 Involved in the requirement gathering for the synthetic data generation use cases.\n\u2022 Generated the synthetic data by using the GT-Data maker\n\u2022 Used the Javelin flows to refresh the data into testing environment\n\u2022 Used the Agile Requirement Designer flows to create the connection between portal and the data maker\n\u2022 Created the TDM tiles in CATDM portal\nEnvironment: CA-Grid Tools, HPALM, AQT, WinScp, JIRA', u""Test Data Management Engineer\nState of Tennessee\nJanuary 2017 to October 2017\nResponsibilities:\n\u2022 Worked on Implementing TDM framework for State of Tennessee\n\u2022 Implemented Various features of CATDM(Data Masking, Synthetic Data Generation, Test Matching, Sub-Setting )\n\u2022 Worked on Data bases like Oracle, Sql Server, DB2\n\u2022 Worked on Different file formats like Flat files, XML, EDI, HL7 and VSAM\n\u2022 Implemented Data reservation feature to TDM Portal\n\u2022 Implemented Test Matching feature for various use cases\n\u2022 Used CA GT-Subset tool for Sub-setting the data from Production\n\u2022 Implemented Synthetic Data Generation for Various Sub Systems by using CA Test Data Manager version 4.1/4.2\n\u2022 Implemented Data masking feature of CA TDM by Masking PHI/PII data by using Fast Data Masker\n\u2022 Actively Participated in Requirement Gathering Meeting with SME's\n\u2022 Participated in Various Interviews/Meetings with test Leads, Developers and Testers\n\u2022 Worked on Data reservation in Portal\nEnvironment: SQL developer, Sql Server, DB2, Toad,\nTools: CA Test Data Manager version4.2/4.1, Fast Data Masker, GT Subset, Javelin"", u'TDM Engineer\nSchimatic Technologies Pvt Ltd\nApril 2014 to March 2015\nResponsibilities:\n\u2022 Involved in the Data Profiling to identify the sensitive data from the production\n\u2022 Crating the masking rules and the definition files to mask the sensitive data\n\u2022 Creating the profile connections in the Fats Data Masker (FDM) and GT Data Maker\n\u2022 Worked with DBA team to identify the test data to in Data Sub setting\n\u2022 Identifying the XPATHS in the XML Masking and also involved in EDI file masking\n\u2022 Involved in the synthetic data generation and worked with Data Maker\n\u2022 Validating the masked data against the source data for tables and files\nTools: Grid Tools, Fast Data Masker, Javelin, ARD', u'QA Analyst\nSchimatic Technologies Pvt Ltd\nMay 2012 to March 2014\nResponsibilities:\n\n\u2022 Responsible for creating complete test cases, test plans, test data, and reporting status ensuring accurate coverage of requirements and business processes.\n\u2022 Experience in SDLC and Agile methodologies\n\u2022 Analyzing requirements and creating and executing test cases\n\u2022 Performed Integration, End-to-End, system testing.\n\u2022 Involving in Functional Testing & Regression Testing\n\u2022 Involving in writing complex SQL queries to verify data from Source to Target\n\u2022 Test Case Execution and Adhoc testing\n\u2022 Performed data validation testing writing SQL queries.\n\u2022 Used Quality Center for creating and documenting Test Plans and Test Cases and register the expected results.\n\u2022 Preparing documentation for some of the recurring defects and resolutions and business comments for those defects.\n\nEnvironment: SQL, HPQC, Teradata, HPALM']","[u""Master's""]",[u'']
0,https://resumes.indeed.com/resume/cf5f074a536a53bf,"[u'Machine Learning Engineer\nBorehole Seismic - Houston, TX\nSeptember 2017 to Present\n\u2022 Retrieved borehole data from every stage, clean and visualize the data in the form of line charts for every stage to indicate the events, surface treating pressure, B/U STP, Slurry rate, Blender Prop and BH Prop according to the client requirements and Image processing the graphs visualized from the drilling data using Python.\n\u2022 Developed and maintaining the desktop based picking tool using JAVA which will be used by pickers to pick the readings of the drilling data graphically.\n\u2022 Successfully optimized the compilation time of each event from 50 seconds to 4 seconds.\nLanguages and Tools Used: Python, JAVA, MATLAB, Jupyter, Visual Studio', u'Data Analyst\nUlysses Commodities - Houston, TX\nJuly 2017 to September 2017\n\u2022 Retrieved the weekly petroleum data in real time and clean the data using python following the test-driven development (TDD).\n\u2022 Created a database using Oracle DB for the petroleum data using SQL.\n\u2022 Created a visualization using d3.js and created an app on the website to view data graphically based on user selection.\nLanguages and Tools Used: Python, JavaScript, SQL, Oracle DB, Apache, PHP', u'Data Analyst\nUniversity of Houston - Houston, TX\nAugust 2015 to May 2017\n\u2022 Organized, updated and maintained large database of undergraduate student details enrolled in CASA for online examination.\n\u2022 Retrieved the data from the classroom database using python and normalizing the data.\n\u2022 Created and updated weekly report with more than 3000 entries and produced visualizations using Spotfire.\nLanguages and Tools Used: Python, SQL Server, Spotfire', u'Data Analyst\nSiree Infratech Pvt. Ltd - Hyderabad, Telangana\nMay 2013 to May 2015\n\u2022 Closely worked with Agile SDLC team involved throughout the agile methodology focusing on Scrum in combination with waterfall.\n\u2022 Successfully interpreted data to draw conclusions for managerial action and used statistical techniques for hypothesis testing to validate data and interpretations.\n\u2022 Presented findings and data to team to improve strategies and operations.\n\u2022 Proposed solutions to improve system efficiencies and reduce total expense by over 25%, saving over $76,000 annually.\nLanguages and Tools Used: Python, SQL, C/C++']","[u'M.S. in Computer and Systems Engineering', u'B.S. in Electronics and Communication Engineering']","[u'University of Houston\nAugust 2015 to May 2017', u'Jawaharlal Nehru Technological University\nAugust 2011 to June 2015']"
0,https://resumes.indeed.com/resume/9dd4ac54f1477b28,"[u'Senior Data Analyst/ Data Analysis Engineer\nIM Flash Technologies, LLC\nJanuary 2007 to Present\nIM Flash Technology (IMFT), Lehi UT 2007 - current\nSenior Data Analysis Engineer, Process Integration Group\n\u2022 Won two highly respected company Impact Awards.\n\u2022 Analyzed large volumes of complex data for 5+ problem level 1\nprojects. In one of the company\u2019s new technology Problem Level 1\nprojects, identified root cause signals through extensive data mining.\nIt brought in over $20million savings to the Company.\n\u2022 Led data analysis team to build predictive models to provide the\nforecasting of top 10 Pareto projects and collaborated with Metrology\nand operation departments to monitor and control critical model\npredictors.\n\u2022 Recognized as data analysis expert. Guided coworkers using a\nsystematic approach to analyze data and solve complex problems\nthat involve multiple variables.\n\u2022 As a DOE coach, designed advanced process experiments for\nprocess improvement, provided regression analysis report (fit model)\non DOE data, interpreted the analysis results, made recommendation\nand coordinated implementation.\n\nData Analysis Engineer/Integration Engineer, Process Integration Group\n\u2022 Led a multi-departments team for optimizing repairs. The team\nrealized a 3% yield improvement and a reduction of 40%+ in repairs.\nIt achieved a profit gain of $5 million for the Company.\n\u2022 Brought data mining techniques Decision Tree/Random Forest\ntraining to Data Analysis team.\n\u2022 Provided directions/theoretical explanations to coworkers on statistics\nrelated problems. Supervised and mentored eight engineers for data\nanalysis skills.\n\u2022 Owned parametric trends analysis and generated weekly trends PPT\npresentation.\n\u2022 Created professional quality engineering reports.', u'Research Data Analyst\nNew Mexico State University - Las Cruces, NM\nJanuary 2002 to January 2007\nResearch data analyst in the University Statistics Center\n\u2022 Involved in the development of predictive statistical models to\nevaluate irrigation districts and to design conservation management\nstrategies.\n\u2022 Executed statistical analysis plans including data organization, data\nanalyses and SAS coding.\n\u2022 Provided interpretations of statistical results to the clients.\n\nResearch Assistant and Teaching Assistant in Electrical Engineering Department\n\u2022 Generated Matlab codes to simulate communication system and\nverify improved communication technique. Resulted in a publishable\npaper.\n\u2022 Assisted in teaching electronics and digital signal process courses.']","[u'Master in Statistics', u'Master in Electrical Engineering']","[u'New Mexico State University-Main Campus', u'New Mexico State University-Main Campus']"
0,https://resumes.indeed.com/resume/833f01f95bc77168,"[u'Data Science Intern\nRapportboost.AI\nMay 2017 to August 2017\n\u2022 Successfully implemented code to filter unwanted data from a huge dataset using pandas data-frame\n\u2022 Generated script to replicate an existing database using SQLAlchemy Flask-migrate to create a new sandbox environment\n\u2022 Tools used: PyCharm, jupyter iPython notebook, SQLAlchemy', u'Software Engineer\nPersistent Systems Ltd - Pune, Maharashtra\nNovember 2014 to June 2016\nProjects- Testmate, Onboarding\n\u2022 Designed the User Interface using HTML, CSS, JavaScript, Bootstrap, and AJAX, wrote REST APIs, created tables, wrote stored procedures and implemented token-based authorization for security purposes. Used ASP.Net (C# and MVC) at the server-side\n\u2022 Successfully provided live support through backend by executing Stored Procedures and queries with very limited guidance\n\u2022 Followed Agile methodology - SCRUM']","[u'Master of Science in Software Engineering', u'Bachelor of Engineering in Computer in Management']","[u'Arizona State University Tempe, AZ\nMay 2018', u'University of Pune Pune, Maharashtra\nJuly 2014']"
0,https://resumes.indeed.com/resume/5ffd8526d6e2b021,"[u""Data Scientist /Software Engineer\nActive Network, LLC - Dallas, TX\nJune 2015 to February 2018\nDeveloped a python Asset_Prediction_API classification engine, predicted topics from unstructured text uti- lizing NLP techniques (TFIDF etc) and ML algorithms (SVM, LR, NN etc), reduced the workload of market team\nby 80% and saved 200+ thousand dollars each year.\n\u2022 Built YMCA retention model (a Generalized Linear model implemented by R) ranking the customer's\nprobability to retain membership. Increased customers' membership retention rate considerably.\n\u2022 Developed unified-profile-api (an entity resolution and record linkage solution). Deployed a python-based bot- tle-gunicorn-nginx web framework on linux server to get users' RFM scores in JSON format.\n\u2022 Hands on experience on Deep Learning frameworks (TensorFlow, Keras etc).\n\u2022 Performed daily data cleaning, dimension reduction, classification/regression/clustering tasks.\n\u2022 Finished various SQL stored procedures, performed statistical analysis and present to C-level leaders.""]","[u'Master of Science in Computer Science', u'Bachelor of Science in Software Engineering in Software Engineering', u'certificate']","[u'The University of Texas at Dallas Richardson, TX\nMay 2015', u'University of Electronic Science and Technology of China Chengdu, CN\nJuly 2012', u'Andrew Ng Stanford University\nJanuary 2012']"
0,https://resumes.indeed.com/resume/8fc63bca7d0af444,"[u'Field Engineer\nCrosspoint Engineering - Westwood, MA\nJanuary 2017 to October 2017\n\u25cf Analyzed customer P&IDs with engineering department to determine most efficient and economical system components in WFI sterile water system.\n\u25cf Performed calibration, certification, and maintenance of various electronic instruments in manufacturing and laboratory environments.\n\u25cf Created calibration certificates to ensure measured parameters were accurate and traceable to the National Institute of Standards and Technology (NIST).\n\u25cf Created test procedures for reference of engineers and technicians.\n\u25cf Proficient with HART instrumentation protocols\n\u25cf Gained knowledge of industrial environments and common problems regarding maintenance of instruments.', u'Project Engineer\nCyient, Inc - East Hartford, CT\nJanuary 2015 to January 2017\n\u25cf Managed parts, assemblies, bills of materials, engineering changes, workflows, and technical documents using product data management software Teamcenter.\n\u25cf Used NX/Unigraphics to examine, manage, and measure CAD models.\n\u25cf Manipulated bill of materials for military and commercial jet engines; updated changes and part information.\n\u25cf Used standardized revision control protocols to store and organize assemblies within structure hierarchy.\n\u25cf Read technical drawings to obtain mechanical properties and production information.\n\u25cf Trained in international import/export protocols of classified data.\n\u25cf Created advanced functions, tables, and charts in Microsoft Excel to manage large datasets.\n\u25cf Acted as liaison between Pratt & Whitney and Cyient offshore engineering department.', u'Machine Operator\nNeurocare, Inc - Franklin, MA\nJanuary 2012 to January 2014', u'Data Entry Assistant\nNeurocare, Inc - Newton, MA\nJanuary 2010 to January 2014']",[u'Bachelor of Science in Mechanical Engineering'],"[u'University of Massachusetts Amherst Amherst, MA\nJanuary 2015']"
0,https://resumes.indeed.com/resume/4b1609b4cb901058,"[u'Data Link Analyst - C2P Engineer\nFuture Technologies\nApril 2016 to Present\n- 40 hrs/wk\n\u2022 AEGIS link computer simulation- configuration, testing, operation and troubleshooting of the MLST3 Data Link Simulator. Configuring and operation of C2P/CDLMS. Rebuilding and installing new loads on to hard drives.\n\u2022 Aviation engineering support- AEGIS Combat systems testing including setup, regression testing and air control, IFF testing, engagements. Gathering and formulating solutions from data and failed testing results.\n\u2022 Testing tools- Tech Rep procedures, Dynamic Object Oriented Requirements Management System (DOORS).\n\u2022 Customer service- Providing reports and results to project leads and owners. Scheduling support for milestone testing and daily operations.\n\u2022 Analyst- Recording of data through C2P and MLST3. Data is sorted and broken down into priority categories. TORs and CPCRs are written and assigned to create solutions for problems.']","[u'Bachelors in Engineering in Engineering', u'Certificate', u'Diploma']","[u'Camden County College\nJanuary 2016 to Present', u'Air Traffic Control ""A"" school\nOctober 2009 to April 2010', u'Upper Darby High School\nSeptember 2005 to June 2009']"
0,https://resumes.indeed.com/resume/945e6c35f75193a9,"[u'Data Specialist- ETL Informatica\nIBM - New Jersey\nJuly 2017 to Present\nConcur Feed\nModify the Teradata views and Informatica code to supply two new values for all\nusers in the Concur feed for Johnson & Johnson, USA.\n\nRoles and Responsibilities:\nModified existing mappings/workflows for enhancements of new business requirements.\nWrote UNIX shell Scripts for FTP of files from remote server and backup to MBOX\nInvolved in Performance tuning at source, target, mappings, sessions, and system levels.\nPrepared Technical Design Specification document outlining the refinement and explanation / configuration of all the component in the project.\n\nHead-Count\nModify/Add logic for new HR related elements in Informatica code as well as configure MDM tables for Johnson & Johnson, USA.\n\nRoles and Responsibilities:\nExtensively developed various mappings to perform ETL of source data into the OLAP/data warehouse.\nWorked with SQL, PL/SQL procedures and functions, stored procedures and packages within the mappings.\nTuned Informatica mappings and sessions for optimum performance.\nAssisted the other ETL developers in solving complex scenarios and coordinated with source systems owners with day-to-day ETL progress monitoring and delivered the product working in an agile environment.', u""Associate System Engineer\nIBM - Kolkata, West Bengal\nMay 2014 to June 2017\nBeyond\nSupport the entire Janssen IT organization's effort to select and migrate applications\nto the optimal operating environments including On Premises Cloud (OPCx) and\nVirtual Private Cloud (VPCx) using Amazon Web Services (AWS) for Johnson &\nJohnson, USA.\n\nRoles and Responsibilities:\nSupport and educate the Business owners to get their applications migrated to optimal cloud environment .\nFollowed the industry practice of TIME (Tolerate/Invest/Migrate/Eliminate) model to rationalize the IT application landscape in order to move towards the Next Generation Infrastructure (NGI) in phased manner using defined T- schedule.\n\nWebtech\nEnhance custom Java based application and provide steady state support to critical\nresearch and development related applications for Johnson & Johnson, USA.\n\nRoles and Responsibilities:\nWorked on front end JSP, CSS, JavaScript and xml to make the application compatible with new browser IE11.\nWorked with the ITS team to migrate the application\u2019s database and server to SDDC (Cloud) and also configure the application for the same.\nWorked on supporting, maintaining and enhancing the application. Handling high priority incidents and tickets independently.""]",[u'Bachelor of Technology in Computer Science'],"[u""Jayoti Vidyapeeth Women's University Jaipur, Rajasthan\nSeptember 2009 to June 2013""]"
0,https://resumes.indeed.com/resume/4a52ff63cd53222f,"[u'Network Engineer\nBusiness Communications Inc\nNovember 2016 to Present', u'Data Network Specialist\nU.S. Marine Corps\nFebruary 2013 to October 2016']","[u""Bachelor's in Tactical Communications""]",[u'Marine Corps Communication Electronics School\nPresent']
0,https://resumes.indeed.com/resume/954053410948e64f,"[u'Data Warehouse Engineer\nHouse of Beauty Inc - Philadelphia, PA\nOctober 2016 to January 2018\n\u2022 Assisted in data analysis, star schema data modeling and design specific to data warehousing and business intelligence environment.\n\u2022 Led efforts to analyze data for source/target mappings, created T-SQL scripts for data processing and automation.\n\u2022 Led database administration and database performance tuning efforts to provide scalability and accessibility in a timely fashion, provide 24/7 availability of data, and solve end-user reporting and accessibility problems.\n\u2022 Managed Packaging IT Security group in validating users for database access privileges, Intranet access for application and databases directly responsible for.', u""Software Developer\nGT-Trans Ukraine - Kiev, UA\nJuly 2012 to October 2015\n\u2022 Developed customer interfaces for transport, sales and account departments.\n\u2022 Provided JavaScript programming for upgrade of Terrasoft CRM system.\n\u2022 Client liaison to determine mapping of the firm's workflow procedures to the software environment.\n\u2022 Designed and implemented specialized user interfaces.\n\u2022 Developed custom reports.\n\u2022 Designed and implemented of a database system in SQL Server.\n\u2022 Responsible for Client side support and data validation of ongoing Terrasoft CRM implementation.\n\u2022 Trained users to work in Terrasoft CRM system.\n\u2022 Implemented triggers and stored procedures in database.\n\u2022 Debugged and maintained Terrasoft CRM System.""]","[u""Master's in Computer Information Systems and related areas""]","[u'European University Kiev, UA\nJune 2012']"
0,https://resumes.indeed.com/resume/a422f1edde303c73,"[u'Data Center Systems Administrator\nCharter Communications\nSeptember 2017 to Present\nInstall and build new servers with Linux(Red Hat, Oracle) or Windows systems using Software provisioning tools over SSH or other Out of Band technology\n\u25cf Manage and distribute IP addresses and Vlans within Data Center switch ranges.\n\u25cf Take part in project management calls to keep project on targeted timeline\n\u25cf Work with teammates in multiple St Louis locations and throughout Charter/Time Warner global footprint to keep learning new methods and mentoring other processes.\n\u25cf Use tools such as mRemote, RoyalTS to remote login to (and save information of commonly used devices) multiple devices at a time as necessary in order to multi-task effectively.\n\u25cf Create MOPs and SOPs when working with Critical Systems\n\u25cf Rack and stack new devices or remove decommissioned devices and cabling.\n\u25cf Managing inventory and keeping a clean warehouse to find equipment effectively.\n\u25cf Manage Remote lab and and serve as liason to teams using equipment within.', u'Implementation Engineer\nThomson Reuters\nNovember 2016 to September 2017\nBuild new servers with Linux (Red Hat, Oracle) or Windows systems using Software provisioning tools over SSH or other Out of Band technology\n\u25cf Test all functioning alarms are being generated to L1/2 Teams prior to go-live\n\u25cf Add application packages to build as designed by Architecture\n\u25cf Take part in project management calls to keep project on targeted timeline\n\u25cf Work with teammates in St Louis and Bangalore to keep learning new methods and mentoring other processes.\n\u25cf Use tools such as mRemote, RoyalTS to remote login to (and save information of commonly used devices) multiple devices at a time as necessary in order to multi-task effectively.', u'System Engineer\nJune 2015 to November 2016\nMonitor personal and group Emails in communication with internal and external customers\n\u25cf Create, Escalate and/or Resolve Trouble tickets depending on complexity of Alarms (HP Service Manager, Vantive, Remedy)\n\u25cf Use monitoring tools to review alerts on remote nodes worldwide (IBM Tivoli, HP Openview)\n\u25cf Log into remote nodes via RDP or SSH to resolve errors in OS (Windows or Unix/Solaris)\n\u25cf Familiar with Windows Server 2003, 2008, 2012, Solaris 9,10, Unix, some Linux releases and Ubuntu.\n\u25cf Serve as 1st Level Support for OS teams, Application teams, Storage and Database teams.\n\u25cf Serve as a Senior technician on Major Incident calls needing immediate resolution\n\u25cf Performing Occupational Acceptance testing prior to devices being placed in Production.', u'Data Center Technician\nNovember 2013 to June 2015\nIdentify new migrations, solutions and changes that are to be implemented for site.\n\u25cf Initiate and attend project meetings with technical and non-technical customers and facilitate needs to ensure migration or other solution is completed with as few complications as possible\n\u25e6 Work with Project Managers, Portfolio Managers/Directors, Networking and Systems teams and have a good rapport with their members.\n\u25cf Work closely with Critical Services to ensure space and power requirements can be met prior to assigning locations to devices.\n\u25e6 Work with PDU, UPS, and HVAC systems to ensure no loads will be overloaded.\n\u25cf Lead a 4 person team of Planning and implementation and assign tasks to be followed up on\n\u25cf Set teams to perform installations and decommissions that can be completed in a timely manner\n\u25cf Perform installations and decommissions as needed by team members.\n\u25cf Perform receipting and receiving duties as needed.\n\u25cf Serve as high-level technical resource for complex troubleshooting.\n\u25cf Work with Telco and Telco equipment for customer circuits.\n\u25cf Perform cabling through IDF and BIX blocks, use network switching technologies to connect devices.', u'Data Center Technician\nSavvis, a CenturyLink Company\nApril 2011 to November 2013\nWork directly with the customer and/or Savvis support staff to identify and troubleshoot as necessary to isolate the root cause of problems\n\u25cf Support and communicate customer billable requests through Vantive.\n\u25cf Manage and prepare incoming and outgoing customer tape backups.\n\u25cf Responsible for the installation and troubleshooting of all copper and fiber customer cross\nconnects.\n\u25cf Plan, prioritize and implement installation and decommission projects.\n\u25cf Maintain documentation of network systems infrastructure, cross connects and circuit audits.\n\u25cf Assist customers with afterhours shipping and receiving.\n\u25cf Perform and execute installation projects that would include the racking and stacking of Savvis\nand/or customer equipment.\n\u25cf Monitor and control access to the Data Center to prevent unauthorized access.']",[u'Associates of Arts Degree in General Studies in General Studies'],[u'Southwestern Illinois College\nJanuary 2003 to May 2006']
0,https://resumes.indeed.com/resume/00b70679561cc1e3,"[u'Software Engineer (Assistant Vice President)\nBank of America - New York, NY\nMarch 2014 to Present\n\u2022 Building applications in Python and Java with focus on architecture and efficiency. Helping improve performance 2 to 10 times faster using optimizations.\n\u2022 Helping automate code release process including regression analysis. Revising several 1-hour worth of activities into button clicks in a revamped user interface.\n\u2022 Building applications interfacing with databases, web services, live market data.', u'Software Developer\nThink Tech Labs - Ann Arbor, MI\nJanuary 2013 to December 2013\n\u2022 Developed Full Stack to build an application leveraging Salesforce cloud platform, using MVC design in an Agile startup environment. Provided analytics and graphics options to represent data.\n\u2022 Developed Apex Classes/Triggers using an object-oriented database - SOQL, AngularJS, jQuery. Designed responsive and dynamic web pages with a more streamlined front end.\n\u2022 Mentored two software engineering interns and completed 6 projects end to end using Scrum methodology. Conducted and led numerous planning meetings and design reviews.', u'Data Science Engineer\nInfosys Limited - Chennai, Tamil Nadu\nJune 2010 to July 2011\n\u2022 Analyzed large data sets, generated reports and visualization to make business decisions\n\u2022 Performed ETL (Extract, Transform and Load), transforms, filters, improved test coverage to 96%\n\u2022 Used Business Intelligence tools such as Informatica, MS BI (SSIS, SSAS, SSRS), Java and Perl\n\nMACHINE LEARNING PROJECTS:\n\u2022 Built anomaly detection model for detecting credit card fraud (Dataset) with F-Score of 0.88\n\u2022 Image classification (Deep Learning): Convoluted Neural Network(TensorFlow)-0.68 accuracy score\n\u2022 Home Office 3D: Built SVM Classifier for 3D Scene from Cornell-RGBD-Dataset with 72% accuracy']","[u'Machine Learning Engineer Nanodegree', u'Masters in Computer Science', u'Bachelors in Electrical Engineering']","[u'Udacity San Jose, CA\nMarch 2017 to September 2017', u'University of Michigan Ann Arbor, MI\nDecember 2013', u'Anna University Chennai, Tamil Nadu\nMay 2010']"
0,https://resumes.indeed.com/resume/e636085b870dc063,"[u'Data Architect\nMultiplan - Rockville, MD\nOctober 2017 to Present\n\u2022 Developed Enterprise Data Architecture strategy to support futuristic needs of business\n\u2022 Contributed to design, implementation and support of major Operation Data Store, data warehouse/data mart and business intelligence projects and initiatives\n\u2022 Explored functional areas to revise data model and Conducted fit gap analysis with subject matter experts which resulted in a robust data model\n\u2022 Introduced Data quality assurance checks as routine part of incremental data loads to verify that ODS and warehouse data meets the optimum quality standards', u'Data Architect, CHIA\nEnterprise Data Modeler - Boston, MA\nMarch 2014 to July 2017\n\u2022 Design and Maintenance of Data Model &Governance- complete metadata, database change logs, Sub models and associated support / maintenance procedures using the Erwin\n\u2022 Developed BRD, TRD, Use-Case Diagrams, Activity Diagrams, Sequence Diagrams\n\u2022 Responsible for Data Loading and Delivery, Quality Assurance Duty/Responsibility\n\u2022 Created ad hoc reporting to turn over results for power users using SSRS and Tableau services\n\u2022 Worked on a POC to develop Azure & AWS cloud services for easy data migration services\n\u2022 Documented user manuals, program documentation, training manuals, and operational procedures', u'Data Architect/Data Modeler\nEnterprise Data Modeler\nSeptember 2013 to March 2014\n\u2022 Designed enterprise Model, financial area models based for incoming data from people soft financials and people soft HR systems after data profiling using Toad for oracle platform\n\u2022 Created detailed Data dictionary, technical- design specifications document and STT document\n\u2022 Worked with DBAs to recreate a best fit physical data model and life cycle changes', u'Research Assistant Data Engineer\nCAMT - Rolla, MO\nAugust 2011 to August 2013\n\u2022 Worked with Boeing design teams on research and development of AI systems\n\u2022 Design and development of Robotic fused filament fabrication system\n\u2022 Conducted JAD sessions with design teams to drive consensus & the attainment of project goals', u'Engineer\nTransvahan Technologies - Bengaluru, Karnataka\nJanuary 2011 to May 2011\n\u2022 Performed Data analysis on Business Data and Reporting on the National logistic data\n\u2022 Trained power users in utilizing information through dashboards']","[u'Masters in Engineering in Missouri S &T', u'in Education', u'']","[u'John Hopkins Univ\nJanuary 2013', u'Univ. of Wharton', u'MIT']"
0,https://resumes.indeed.com/resume/7b6bf8f40c468d98,"[u'Owner/Contractor\nVandewater Communications - Northridge, CA\nJune 1996 to Present\nDBA Vandewater Communications California State License # 724094\nResponsible for the sales, bids, contracts, installation, maintenance and invoicing of all jobs. Installation and maintenance of telecommunications and networking systems for both commercial/residential properties. Extensive cabling and custom server room design and construction.', u'Data/Voice Engineer\nImpulse Internet Services - Goleta, CA\nJune 2011 to March 2012\nTelecom/Data, Adtran router/switch configuration, testing, and installation. Verizon/Frontier badged Central Office Colocation builds and terminations/x-connects. T1/PRI/DS3 and EoC/ADSL extensions and testing.']","[u'in Cisco router/switch installation and maintenance', u'in Telecommunications']","[u'Pearson VUE Los Angeles, CA\nMay 2012 to May 2012', u'Western Technical College Van Nuys, CA\nFebruary 1985 to February 1986']"
0,https://resumes.indeed.com/resume/38bc585b13adf76f,"[u""Data Center Engineer\nCerner - Kansas City, MO\nJanuary 2014 to Present\nResponsible for maintaining the hardware and software within Cerner's data centers.\nCoordinate with associates to bring downed servers back online and providing proactive repairs.\nDecommission and perform reclaims of obsolete servers that are no longer needed Coordinate and monitor vendors in support of maintaining Cerner's data center. Provide and improve necessary new documentation and changing processes to improve workflow and response time. Configuring blade enclosures and ensuring hardware reliability. Perform scheduled and reactive server upgrades.\n\nSelected projects\n\n\u25cf Facilitated in upgrading a multitude of enclosure onboard administrators to enable support for HPE Gen 9 servers with no interruption in service to the clients\n\u25cf Implemented procedures to test support spares and ensure servers are fully updated and reliable when needed\n\u25cf Communicated with vendors and client teams to replace failing backplanes in several enclosures which required shutting down 16 servers per enclosure with zero impact to the clients"", u'Unit Test Analyst\nCerner - Kansas City, MO\nJanuary 2012 to January 2014\nWorked to ensure client requested data is consistent in the database for various Cerner solutions. Used and provided feedback for Excel and Excel scripts. Trained and supervised new associates to perform tasks successfully. Presented process improvement feedback to further improve efficiency and clarify documentation.\n\nSelected projects\n\n\u25cf Part of a team that worked exclusively with a high priority, valued client to provide real time database updates\n\u25cf Maintained a client site to assist technicians, nurses and doctors with transitioning to Cerner solutions', u'Level 1 Engineer\nISPN - Lenexa, KS\nJanuary 2011 to January 2012\nGuided customers through installation and troubleshooting of their computer and company specific hardware and diagnostics. Aided customers with account setup and management including remote diagnostics on their equipment. Provided over the phone technical support for multiple internet service providers. Assisted new employees and provided constructive feedback to help improve customer satisfaction.']","[u'Bachelor of Science in Information System Security in Information System Security', u'Associates in Computer Networking Systems']","[u'ITT Technical Institute-Kansas City Kansas City, MO\nAugust 2010 to June 2012', u'ITT Technical Institute Kansas City, MO\nJanuary 2008 to January 2010']"
0,https://resumes.indeed.com/resume/5f65f6a2118c3d51,"[u""System Performance / Data Collection Engineer\nGlobal Wireless Solutions\nJanuary 2015 to January 2018\n\u2022 Conducted drive testing and analyze drive test data results for new site integration and LTE system optimization.\n\u2022 Evaluated and improve areas of poor VOLTE and LTE coverage by identifying and analyzing service problems.\n\u2022 3G, 4G LTE, GSM/ GPRS/CDMA2000/EVDO/ EDGE/ UMTS/ LTE/ VOLTE Optimization.\n\u2022 Monitored of KPI's, throughput (Upload & Download speed), call drop rate, handover successful rate as requested by the RF performance team.\n\u2022 Performed CTTP and making E911 test calls on different sectors/PNs (alpha, beta & gamma) on corresponding Azimuth confirming the LAT/LONG.\n\u2022 Analyzed drive test data on XCAL for network analysis, site verification, problem identifications and solutions and generate a report on drop calls, coverage intensity.\n\u2022 Strong troubleshooting skills in configuring of 3G, CDMA, LTE phones, to the right frequencies 800MHZ, 1900MHZ for a better coverage using QPST software.\n\u2022 Handled optimization of new sites and handles problems like call drop, call fail interference, handover failure.\n\u2022 Worked directly with RF Engineering Team for optimization of network and to analyze the data collected for EVDO Upload, Download speed and ping time testing.\n\u2022 Handled customer complaints and related issues.\n\u2022 Performed Clockwise and Counter-Clockwise drive to verify successful handovers between sectors within a site as well as sanity drive to verify handovers among many sites.\n\u2022 Conducted nationwide drive testing in major cities and highways using Microsoft Street and Trips.\n\u2022 Investigated drop calls and other network failures and resolved all issues."", u'Data Collection Engineer\nGlobal Wireless Solutions - Dulles, VA\nDecember 2013 to January 2015\n\u2022 Set up of equipment, Laptops, GSM, CDMA phones, hub, GPS and antennas.\n\u2022 Drive test using TEMS Investigation software to collecting voice, data on GSM, CDMA, and LTE. Test file are uploaded back to Optimization team (QA) through FTP\n\u2022 Performed Benchmarking testing on different vendors such as AT&T, VERIZON, T-MOBILE, and SPRINT, US CELLULAR, METRO PCS, CRICKET, CLEAR\n\u2022 Good understanding of system design/ optimization techniques in LTE/ VOLTE for Ericsson.\n\u2022 Good understanding and knowledge of VOLTE/ LTE/GSM/HSDPA cells infrastructure, RF planning and RF propagation.', u""E911 Data Collection Engineer\nGlobal Wireless Solutions\nMarch 2009 to December 2013\n\u2022 Performed measurements and drive tests to verify or dispute and resolve any network issue.\n\u2022 Established Network performance using drive test to ensure no drop call within coverage.\n\u2022 Analyzed and describe underlined problems related to design, UE and system.\n\u2022 Identified and improve radio network problems using statistics, recordings and events.\n\u2022 Established subscriber's behavior and perceptions.\n\u2022 Ensured that traffic growth can be handled by network.\n\u2022 Monitored daily, weekly and monthly drive test data report.\n\u2022 Handled customer complaints related issues performing a sanity drive and verifying the coverage in the area.\n\u2022 Strong troubleshooting skills in Rebooting and Updating of CDMA, LTE phones 2G, 3G and 4G technologies.\n\u2022 Resolved technical issues with navigation and data collection software.""]",[u'Bachelor of sciences in RF Planning and Optimization'],"[u'California state University San Marcos, CA\nJanuary 1996']"
0,https://resumes.indeed.com/resume/673d523fdd4e6135,"[u'Customer and Data Support Specialist\nHindu Center of Virginia - Richmond, VA\nMay 2017 to Present\nRichmond, VA\nProject Summary: Provide customer support for end user applications used to purchase services and publish reports to internal Leadership teams containing service and revenue data.\nRole: Customer and Data Support Specialist May 2017 - Present\nResponsibilities:\n\u2022 Interface with customers to understand their needs\n\u2022 Document, triage, and resolve challenges experienced by customers while using one or more end user applications, escalating as needed\n\u2022 Maintain and manage customer account data across various CRM platforms and issue monthly invoices to customers for services received\n\u2022 Perform weekly scrub of PII data not required for accounting and auditing purposes\n\u2022 Publish monthly reports containing data on services rendered, frequently occurring issues, and revenue', u'Software Engineer\nTech Mahindra - Pune, Maharashtra\nMay 2012 to February 2014\nPune, India\nProject Summary: Develop/maintain a Java Web Application that generates service orders based on requests entered into the system from installations across USA for AT&T.\nRole: Software Engineer May 2012 - February 2014\nResponsibilities:\n\u2022 Performed tasks spanning the full SDLC, from requirements gathering and analysis through design, development, testing, and deployment\n\u2022 Developed application architecture, use cases, and flowcharts and built prototypes for customer demos\n\u2022 Developed comprehensive design documents detailing application specifications\n\u2022 Developed front end user interface for websites using JSP\n\u2022 Supported backend development using Java and XML while interfacing with Oracle databases\n\u2022 Developed Junit test cases for unit testing and supported integration and regression testing efforts\n\u2022 Implemented struts validation framework for error handling\n\u2022 Debugged and resolved issues by reviewing logs on Unix servers']",[u'Bachelors of Technology in Information Technology'],"[u'Priyadarshini College of Engineering Nellore, Andhra Pradesh\nJanuary 2012']"
0,https://resumes.indeed.com/resume/db26e96d432f3732,"[u'Managing Consultant\nExusia, Inc - Jersey City, NJ\nNovember 2015 to Present\nBoca Raton, FL ; Charlotte, NC\nJob Duties:\n\u2022 Create parsers to extract information from web service calls.\n\u2022 Design and architect database systems to handle batch requests.\n\u2022 Design and develop systems to implement Data Acquisition framework.\n\u2022 Design systems to create KPI reports and dashboards.\n\u2022 Setup and administration of Abinitio servers\n\u2022 Develop Abinitio Express>It Templates\n\u2022 Develop frameworks to consume and execute data from vendors.\n\nTechnology: Ab initio ETL, Abinitio Express>It, Abinitio Control Center, UNIX, My SQL, Python, C++', u'ETL Technical Lead\nITI Data Inc\nOctober 2012 to October 2015\nLocation: Jersey City, USA\nJob Duties:\n\u2022 Implementation of CDC architecture to capture data from various vendors.\n\u2022 Development of reusable common utilities to compare data changes between code releases.\n\u2022 Used web service, XML, Excel components.\n\u2022 Converted legacy CRM application written in .Net and Java to Abinitio and salesforce (Cloud DB).\n\u2022 Tuned Abinitio graphs for performance.\n\u2022 Development of utilities and solutions to track job performance and system resource usage.\n\u2022 Generated various kinds of KPI reports.\n\u2022 Development of utilities to scrap data from web.\n\nTechnology: Ab initio ETL, TalenD, Python, Hadoop, Spark, UNIX, Oracle DBMS, Autosys Scheduling, Salesforce CRM.', u'Sr. Data Engineer\nHewlett Packard Co\nApril 2011 to September 2012\nLocation: Newyork, USA\nJob Duties:\n\u2022 Identification and troubleshooting code and design issues on developed applications. Creation of Conduct>it plans to execute Ab initio graphs.\n\u2022 Development of scripts for executing Informatica workflows.\n\u2022 Fine tune the existing graphs to improve the overall performance and reduce the execution time.\n\u2022 Automating the process of creating parameter sets.\n\u2022 Setting up of Operational console environment\n\u2022 Scheduling and migration of existing Informatica jobs.\n\nTechnology: Ab initio ETL, Abinitio Conduct>It, Abinitio Control Center, UNIX Programming, Oracle DBMS, Autosys Scheduling.', u'Data Engineer\nHewlett Packard Co - Chennai, Tamil Nadu\nJuly 2007 to April 2011\nIndia\nJob Duties:\n\u2022 Monitoring of abinitio ETL graphs during production, co-ordinating and maintaining of onsite projects.\n\u2022 Analysing data integration requirement, creating designs for implementing Ab initio graphs.\n\u2022 Trained new employees on Ab Initio concepts and UNIX with live hands.\n\nTechnology: Ab initio ETL, UNIX Programming, MS SQL, Oracle DBMS, Autosys Scheduling']","[u""Bachelor's""]",[u'']
0,https://resumes.indeed.com/resume/a846577be1058d24,"[u'Data Scientist\nSoutheast Toyota Finance (Florida)\nMay 2017 to Present\n\u2022 Building forecasting model for the inbound calls and staffing optimization at operational center. (Time Series)\n\u2022 Built Propensity to pay model for recovery - improved collection by 10%. Integrated the three years of credit bureau data\n(Equifax) and automated the ETL process using macro and shell script. (Classification: Logistic regression, Decision trees)\n\u2022 Rebuilt Retail/Lease Early stage collection score model - New model performance showed 30% lift.\n\u2022 Market Basket Analysis for Auto-Insurance products - Grouped the products in the design to increase the cross-selling.', u'Project Member\nComcast Sponsored (OSU)\nJanuary 2017 to May 2017\nIdentified the homogeneous segments of customer engagement that helped Comcast business to upsell and cross-sell by customizing services and product positioning. (Analysis- Clustering, Market Basket Analysis)', u'Data Engineer\nITI DATA\nApril 2015 to July 2016\nDeveloped LRR data integration system for CITI group on BASEL-II accord instructions. Improved risk management calculation\nefficiency by 30%. (Technology Used: Informatica, Oracle PL/SQL, Unix shell Scripting and scheduler)', u'Software Engineer\nL&T Infotech\nJuly 2012 to February 2015\nDeveloped the Profit & Loss processing system for Citi finance group. Application served better economic calculation and traceability of trade processing. (Technology used- Ab Initio, Oracle PL/SQL, Unix Shell Scripting, Autosys scheduler )']","[u'Master of Science in Business Analytics in Business Analytics', u'']","[u'Oklahoma State University (OSU)\nMay 2018', u'National Institute of Technology (NITRR)\nMay 2012 to January 2017']"
0,https://resumes.indeed.com/resume/beba5ef41ab78074,"[u'Data Analyst\nInternational Brake Industries - Lima, OH\nJanuary 2017 to Present\n\u2022 Identify trends in large volumes of data and present results in a clear and understandable manner.\n\u2022 Create and maintain numerous databases designed to track critical customer service and inventory management KPIs.\n\u2022 Create and maintain year over year volume tracker to identify opportunities for gained business with existing customers.\n\u2022 Implement and maintain best practices inventory management and procurement techniques to reduce cost and inefficiencies.\n\u2022 Perform excess inventory analysis to gain visibility to slow moving and obsolete components.', u'Operations Intern\nN.K.S. Distributors - New Castle, DE\nJune 2016 to August 2016\n\u2022 Analyzed sales and inventory reports to identify operational inefficiencies.\n\u2022 Collaborated with V.P. of Operations to determine the main causes of beverage case breakage and expired products.\n\u2022 Designed an excel function that converts product expiration dates from Julian format to a standard format automatically.', u'Electrical Engineer\nGrob Systems, Inc - Bluffton, OH\nMay 2014 to August 2015\n\u2022 Designed electrical hardware circuitry for industrial automation, CNC and assembly machines for Ford and General Motors plants.\n\u2022 Implemented a program in Excel that calculated circuit breaker sizes for new machines. The use of this program decreased design calculation time by 20%.\n\u2022 Created a process for cross referencing past schematics with new and existing machines. This process decreased design inconsistencies by 10%.']","[u'Master of Business Administration in AACSB Accredited', u'Bachelor of Science in Electrical Engineering']","[u'University of Delaware Newark, DE\nJanuary 2017', u'Ohio Northern University Ada, OH\nMay 2014']"
0,https://resumes.indeed.com/resume/4634d3e041126844,"[u'Data Analyst Intern\nEnergizer Holdings, Inc - St. Louis, MO\nAugust 2017 to December 2017\n\u2022 Collected and cleaned transactional data from worldwide branches for the Global Forwarding project\n\u2022 Extracted and presented data insights to internal customers with SQL, R, and various visualization tools\n\u2022 Produced forecasts for surge demands in hurricane seasons with multivariate linear regression and communicated results to the management and cross-functional teams\n\u2022 Defined and executed analytic goals of a package redesign project, made final recommendations based on data analyses, as well as feedbacks from sales, inventory management, and customer service teams.\n\u2022 Leveraged various Business Intelligence tools to enhance the data analytic capacity of the team', u""Supplier Chain Engineer\nDoosan Infracore, Incheon - KR\nJuly 2008 to June 2015\nSupply Chain Analysis\n\u2022 Maintained supplier scorecard and performance metrics on a regular basis and created reports with\nPowerPoint or Tableau\n\u2022 Conducted demand forecasting by analyzing trends and seasonalities.\n\u2022 Analyzed costs by product cateogries, led efforts in operations optimization, achieved significant reduction in cost of labor, material and equipments.\n\u2022 Reviewed monthly transaction data of suppliers, dealt with backlogs and payment drafts in ERP system\n\u2022 Worked closely with engineering and sales team, providing reliable real-time updates on supplier\nperformance, in form of both spreadsheet reports and data visualizations\nProject Management\n\u2022 Participated in Lean Six Sigma projects as a part of 30-person team for improving quality, reduced field\nquality failure by 40% and increased suppliers' capacity by 23%\n\u2022 Organized meetings with 20+ stakeholders before the productions of new models, and managed the biweekly releasements of commodity requirements for suppliers\n\u2022 Managed supplier APQP, supervised operations and processes to guarantee new products launched on schedule and were within the tolerance of defects""]","[u'M.S. in Industrial Engineering', u'BEng in Automative Mechanics']","[u'Northeastern University Boston, MA\nMay 2018', u'Ludong University\nJanuary 2004 to January 2008']"
0,https://resumes.indeed.com/resume/f8596b5914747ad0,"[u'Data Processor\nSouthern Traffic Services - Gulf Breeze, FL\nMay 2013 to January 2016\nEffectively gathered and organized data following certain specifications; Input and analyzed data using several computer programs; Operate computers and other communications equipment; Responsible for identifying job problems and ensuring job orders are accurately completed according to schedule\n\n10800 Gable Run Drive\nKnoxville, TN 37931\n850-712-1739\np17smith@yahoo.com', u'Premise Technician\nAT&T - Gulf Breeze, FL\nMay 2010 to April 2013\nPremtech\nInstalled and troubleshot phone, internet, and television services; Connect additional items including wireless laptops, wireless printers, and surround sound equipment; Performed quality checks after each installation to confirm that all work was completed accurately; Interacted with customers on a daily basis and ensured customer satisfaction prior to closing out each task; Informed customers about our products and how to operate equipment and ensured customers ability to operate equipment appropriately; Upgraded customers internet or television services while on site', u'Engineer - Construction\nMediacom - Gulf Breeze, FL\nAugust 2000 to February 2009\nMet with managers, builders, and contractors to gather an information on development of new condos, hotels, and subdivisions; Worked with both commercial and residential accounts; Developed and presented competitive proposals for contracts; Managed and supervised contractors for each project; Provided weekly update reports for each project and introduced methods to improve the operation of new and existing cable systems; Implementation of labor estimate to create schematic for installation and activation of utility; Entry of purchase orders into Oracle after receiving authorization and necessary paperwork; Submitted dig permits and/or pole permits before any construction began\nTechnician\nRepaired hard lines, service lines, and underground/aerial cable; Resolved technical issues with business and residential customers; Interacted with customers on a daily basis for problem solving and educational purposes; Responded to customer complaints and provided fast troubleshooting, maintenance, upgrades, and repairs on-site; Read general system layouts from blueprints; Gained experience as a maintenance technician and CLI technician\nInstaller\nInstalled cable and/or internet service which includes basic, digital, and HD equipment in both new and existing homes; Sold optional packages to customers at time of install and created additional cash flow; Interacted with customers at premises to insure customer satisfaction and to upgrade services where possible\n\n10800 Gable Run Drive\nKnoxville, TN 37931\n850-712-1739\np17smith@yahoo.com\n\nLocator\nLocated underground cable for building and landscaping construction sites; Assessed damaged equipment and cable lines; Prepared and delivered bills to companies responsible for damaged cable lines; Acted as a liaison between Mediacom and customers regarding billing, customer satisfaction, and quality assurance\n\nAwards and Certificates:\nNCTI Class - Fiber Testing and Maintenance\nNCTI Class - Troubleshooting Advanced Services\nNCTI Class - Computers and Broadband Modems\nNCTI Class - Broadband Digital Installer\nNCTI Class - Customer Service for Techs\nNCTI Class - Installer']","[u'', u'']","[u'Jefferson Davis Community College Brewton, AL\nAugust 1998 to June 2000', u'Gulf Breeze High School Gulf Breeze, FL\nAugust 1995 to May 1998']"
0,https://resumes.indeed.com/resume/b5cee5a0e6c5db3a,"[u'Associate Data Engineer\nMicron Technologies - Manassas, VA\nMay 2007 to Present\n\u2022 Monitor and analyze electrical data trends to ensure product quality improvements\n\u2022 Efficient in Statistical Analytic.\n\u2022 Maintain and create visual graphics using Tableau, JMP, and Yield3 programs.\n\u2022 Creating automation to daily tasks to free up valuable resources.\n\u2022 Maintain data base that provide trends and patterns of product stability companywide for quick response to product issues.\n\u2022 Maintain data base for Cost of Poor Quality, Calculate revenue loss based on poor quality of product. Over $100 million worth of product evaluated quarterly.\n\u2022 Maintain and coordinate company wide product BPM\u2019s\n\u2022 Use data analysis techniques to find root cause for issues\n\u2022 Organizes and participates in weekly electrical data trend monitoring meeting\n\u2022 Established and maintain effective relationships with customers to provide feedback on ongoing issues and resolutions.', u'Data Technician\nMicron Technologies - Manassas, VA\nMay 2002 to April 2007\n\u2022 Interpreted data, analyze results using statistical techniques and provide ongoing reports\n\u2022 Acquired data from primary or secondary data sources and maintain databases/data systems\n\u2022 Identified, analyzed, and interpreted trends or patterns in complex data sets\n\u2022 Located and defines new process improvement opportunities']",[u'Associate of Science in Computer Information & Science'],"[u'ECPI University Manassas, VA\nJuly 2015']"
0,https://resumes.indeed.com/resume/0298d56d4a59b6f9,"[u'Data Analyst\nMphasis Limited - Bengaluru, Karnataka\nSeptember 2013 to July 2015\n\u2022 Interpret data, analyze results using statistical techniques and provide ongoing reports\n\u2022 Develop and implement data collection systems and other strategies that optimize statistical efficiency and data quality\n\u2022 Acquire data from primary or secondary data sources and maintain databases/data systems\n\u2022 Identify, analyze, and interpret trends or patterns in complex data sets\n\u2022 Filter and clean the data, and also review reports and performance indicators to locate and correct code\nproblems\n\u2022 Work closely with management to prioritize business and information needs', u""Software Engineer\nJuly 2011 to September 2013\n\u2022 Worked in the domain of Mainframes, Java and Software Testing. I also worked on Automation Testing, where I was responsible for automating the manual test cases using QTP/UFT\n\u2022 Created Automation test scripts using QTP (UFT) and prepared release specific test plans for End to End\ntesting\n\u2022 Developed, executed and documented highly complex test plans and test cases for business-related\napplication running on multi-tiered platforms\n\u2022 Performed test management activities like test planning, test execution, defect management and reporting\n\u2022 Earned the coveted Mphasis Summit Award in Feb'13 for my dedication, ingenuity and hard work""]","[u'Master of Science in Computer Science', u'Bachelor of Engineering in (B.Engg.) majoring']","[u'Rochester Institute of Technology\nDecember 2017', u'Visvesvaraya Technological University']"
0,https://resumes.indeed.com/resume/1312b1f294e9b17f,"[u'Data Scientist Intern\nTapGenes - Chicago, IL\nJune 2017 to December 2017\nDeveloped health assessment data models using genealogical family trees and health information data to identify health risks,\nenabling preventive action and minimized healthcare cost\n* Designed and implemented them as RESTful web services through Swagger framework using JSON and MS Azure\n* Accelerated integration and delivery by creating user stories, developer specifications, test profiles and scripts', u'Systems Engineer\nInfosys Limited - Pune, Maharashtra\nJune 2014 to June 2016\nIndia\n* Extracted large volumes of data (~1M rows) from various RDBMSes to centralize the transactional datastore as part of a Scrum team\n* Transformed and ingested the data into MarkLogic NoSQL database through Apache Storm framework using Java and XML\n* Developed code, optimized stored procedures, deployed scripts, performed testing and managed releases under DevOps practices\n* Eliminated the need of building consumer specific solutions for accessing, storing and normalizing financial banking data\n* Streamlined existing framework by including metadata in missing records report, accelerating root-cause analysis and reconciliation']","[u'Master of Science in Management Information Systems', u'Bachelor of Technology in Information Technology']","[u'University of Illinois at Chicago (UIC) Chicago, IL\nAugust 2016 to December 2017', u'Jaypee Institute of Information Technology Noida, Uttar Pradesh\nJuly 2010 to June 2014']"
0,https://resumes.indeed.com/resume/9945b7de56a5c63d,"[u""Data Analyst/Tableau Developer\nPaypal - SanJose, California, US\nSeptember 2016 to Present\nTeam Size: 16\nPlatform/OS Used: Windows/Unix\nLanguages/tool/Database: UNIX, Teradata, Hadoop (HIVE), JIRA, Rally, Confluence\nDescription:\nData Finance Transformation team deals with financial transactions data in PayPal. This Project is focusing on migrating existing Finance data marts in Teradata to Hadoop Platform. It includes redesigning whole Legacy modules to a newer platform in Hadoop for derivation of Transaction Funding Costs for ACH, CC transactions, Loss derivations TLD modules, NEST Case management, Watch etc.\nResponsibility:\n\n\u2022 Cross analyze the massive past transaction data and millions customer accounts to identify potential fraudulent accounts and activities.\n\u2022 Analyze complex and non-standard data from 3 credit bureaus for consumer and small business\nlenders and implement credit risk models using Tableau.\n\u2022 Leading cross-functional teams and coordinating on multiple projects.\n\u2022 Performed data mining, analysis and support various BI group to develop analytical and operational reporting.\n\u2022 Created Tableau Dashboards to track processing fees earned from various sources (Finacle, Symbols), offline/online fees and accruals; generated reports for the senior management\n\u2022 Monitor progress of the project and facilitate the management on impact, issues and risks.\n\u2022 Analyzed losses from Credit card & mortgages by using bank's internal models and SQL database for different cohorts\n\u2022 Creating ad-hoc data analysis to the users as when required.\n\u2022 Used a variety of reporting tools like Tableau and Spreadsheets to assess program performance and customer behaviour\n\nProject #2"", u'Data Analyst\nEbay - Seattle, WA\nApril 2014 to August 2016\nTeam Size: 8\nPlatform/OS Used: Windows/Unix\nLanguages/tool/Database: Oracle, Teradata, JIRA, service now, Rally\nDescription:\nThe shipping Data solution and services team manages all aspects of Shipping data for eBay, including Label Printing, Tracking, Ship Cost Management, Returns, and Global Shipping program. Shipping DSS team provide a wide range of shipping solutions to support seller\'s varying needs to optimize the cost of shipping and increase the predictability of shipping time estimates. Shipment DSS is single source of truth for all eBay marketplace users. Data sets are available on all big data platforms in eBay and are designed to be easily accessible by Targeting engine.\nResponsibility:\n\n\u2022 Performed in-depth analysis for Ebayplus and PUDO programs which includes understanding the business needs and translating to analytic requests, extracting data from a variety of sources like Teradata, oracle, manipulate and analyze using quantitative, statistical, and visualization using tools like Tableau, Excel and SQL assistance.\n\n\u2022 Built, published customized interactive reports and Tableau dashboards, report scheduling using Tableau server. Created action filters, parameters and calculated sets for preparing dashboards and worksheets in Tableau.\n\n\u2022 Met with various groups, including business owners, SMEs (subject matter experts) and marketing team, for requirements gathering in definition Stage\n\u2022 Built end to end reporting layer in Tableau Dashboard for analytical, operational, real- time and adhoc reporting environment.\n\u2022 Defined historical and incremental refresh frequency in tableau for the data refresh\n\u2022 Developed Tableau workbooks from multiple data sources using data blending and interactive views, trends and drill downs.\n\u2022 Responsible for creating /delivering monthly reports to business users.\n\u2022 Provided guidance and recommendations on process improvements and ""best practices.\n\u2022 Worked with the project manager to estimate best/worst case scenarios, track progress with weekly estimates of remaining work to do, conducting informal meetings ad hoc and as needed.\n\u2022 Wrote adhoc queries for business needs.\n\u2022 Adapted agile scrum methodology.\n\nProject #3', u'Data Analyst\nEbay - Chennai, Tamil Nadu\nApril 2014 to August 2016\nLocation: Chennai, India\nPlatform/OS Used: Windows/Unix\nLanguages/tool/Database: UNIX, Teradata\nDescription:. The Project data is sourced from various applications like, customer information, accounting information, marketing information, zip code information, business info and sales information, which are running on mainframe with Teradata database. This Project provides consistent, reliable and timely customer-centric data that is used for multi-million dollar decisions.\nResponsibility:\n\n\u2022 Perform daily data queries and prepare reports on the daily, weekly, monthly and quarterly basis.\n\u2022 Used advanced Excel functions to generate spreadsheets, pivot tables and vlookup tables.\n\u2022 Heavy SQL queries including complex joins, nested queries, stored procedures, common table expressions (CTEs) etc. were being used for Integration and Management of data.\n\u2022 Data migration tasks were accomplished via SQL Server Integration Services (SSIS). Featuring tool for data Extraction, Transformation, and Loading (ETL).\n\u2022 Reviewed and validated business requirements from stakeholders and steering committee members and transform the business requirements to technical design and transformation logics.\n\nProject #4', u'Data Analyst\nCognizant Technologies system - Chennai, Tamil Nadu\nApril 2009 to July 2010\nTeam Size: 10\nLocation: Chennai, India\nDescription:\nCDNA (Customer 360) is a part of the multimillion branding initiative called GBC (Global Brand Campaign) by Centrica. The GBC initiative is being carried out in US and United Kingdom to start with and later on will be implemented across the Globe. By this initiative Centrica along with its partners promotes its brand via Digital, Display, Video, Electronic and Social media etc The ETL process captures the raw impression data, spend data, click data and other metrics on based on user and geographical regions and mediums and store those onto Teradata and MySQL database\nResponsibility:\n\n\u2022 Build and maintain comprehensive dashboards and metrics to enable real-time business decisions\n\u2022 Compiled, analyzed, and evaluated data on various targets and actual\n\n\u2022 Worked closely with Operations and application teams for queries and programs. Used advanced Microsoft Excel to create pivot tables, used VLOOKUP, and other Excel functions.\n\u2022 Worked on increasing Net Sales and AUM based on the insights\n\u2022 Provided the intelligence to measure and drive initiatives to increase revenue per customer and decreased churning of portfolios.\n\u2022 Developed new reports and delegated tasks to team members\n\u2022 Analyzed the Investments Focus report every month for each branch and discussed the same with the Branch Managers and Investment counselors.\n\u2022 Extracted, interpreted and analyzed data to identify key metrics and transform raw data into meaningful, actionable information.\n\u2022 Designed the conceptual and logical data warehouse architecture and implemented on Oracle and DB2 database.\n\n\u2022 Implemented data relationship, data control and referential integrity constraints in Oracle 9i and enforced\nbusiness logic transformations on the data warehouse.\n\u2022 Created and implemented materialized views by fragmenting the data warehouse for various line of business.\n\u2022 Identify the Target Class and analyze the interesting knowledge in the target class based on various aspects such as region, income range, loan period, pay frequency, returns, etc.\n\u2022 Created data marts for Credit risk line of business and implemented Data Cubes for reporting using MS BI.\n\u2022 Identify the target datasets and analyze the interesting knowledge on the datasets based on various variables such as region, income range, loan period, pay frequency, returns, credit line, etc.\nProject #5', u""Software Engineer\nAlliance Data\nJune 2007 to March 2009\nLocation: Chennai, India\nTeam Size: 12\nEmployer Cognizant Technologies system\nPlatform/OS Used: Windows/Unix\nLanguages/tool/Database: UNIX, Teradata, Datastage\n\nDescription:\nThe task of the project was to build a central repository of historic data warehouse that could support various reporting and information needs. Data was spread out in diverse sources in databases located in different regions. The requirement demanded an integrated, unified data pool that was truly\nsubject-oriented, time-variant and Nonvolatile. Dimensional modeling using the star schema architecture was implemented to achieve the task.\nResponsibility:\n\n\u2022 Involved in meetings with SME's, Data Architect and Data modelers in getting the Business Requirements.\n\u2022 Developed the Enterprise Data warehouse in Agile methodology.\n\u2022 Identified and understood the business function activities, entities, data attributes, metadata and documented detailed design specifications for new system.\n\u2022 Analyzed the reverse engineered Enterprise Originations (EO) physical data model to understand the relationships between already existing tables.\n\u2022 Worked with the DBAs to create a best fit physical data model from the logical data model and worked with BI team to ensure all the requirements are met for reporting.\n\u2022 Closely interacted with developers to understand application functionality and navigational flow and keep them updated about Business strategies.""]","[u""Bachelor's""]",[u'']
0,https://resumes.indeed.com/resume/b4ee15df644c2405,"[u'Data Engineer / Data Scientist\nThis is Girish Sukhwani (Data Engineer / Scientist). Currently, I am located in Morristown, NJ. My current project is going to end in this week.Actively I am looking job all over the USA(Open to relocate). Please find my attached updated resume and let me know if you need any additional information\n\nI am really good in Hadoop ecosystem tools, Spark, Scala, Machine Learning, Tenser flow, Python, and Java.\n\n\n\n\nBest regards\nGirish']","[u""Bachelor's""]",[u'']
0,https://resumes.indeed.com/resume/b4f1f58e45d7e81f,"[u'Data Analyst\nThe Richards Group - Dallas, TX\nNovember 2012 to June 2017\nResponsible for multi-faceted development for reporting, database, ETL and API calls. Planned data integration process by developing common definitions of sourced data; designed common keys in physical data structure; establishing data integration specifications; examining data applications; examined data models and data warehouse schema; determined best-fit data interchange methods; assessing middleware tools for data integration, transformation, and routing. Partnered with Sr. DBA, with the planning, design and support of the data warehouse (snowflake, slow changing, type 2). Worked with data from Google Analytics, Doubleclick, Mediamind (Sizmek) and Salesource (ExactTarget). Created API calls using C#/.NET. Managed code in GitHub. Quickly picked up on Unix, Python and Powershell to manage and support existing batch processes.\n\u2022 ETL Development \u2013 Re-wrote 95% of the existing ETL processes to support dynamic execution and reduce manual support by 75%. Created data mappings and transformations in ETL using SSIS to import and export data from flat files into SQL Server databases.\n\u2022 T-SQL \u2013 Created complex T-SQL commands to access data from a 25 TB data ware house (star schema) that imports 60-300 million records daily for 100+ clients.\n\u2022 Technical Documentation \u2013 Created target mapping documents to support data design and mapping from the vendor. Created and maintained technical documentations for ETL development, API development and database development. Created and maintained quick reference guides (QRG) for production support.\n\u2022 Cross Functional Collaboration \u2013 Collaborated between internal clients on the media planning, digital analytics and advanced analytics team to translate technical terms to non-technical team members and to help understand data integration and reporting needs. Received the highest number of nominations received from external colleagues, resulting in me being the recipient of the 2015 year-end employee recognition. Received the highest number of nominations from external colleagues again in 2016.\n\u2022 Data Analysis \u2013 Studied data to identify possible anomalies in data collection and reporting.\n\u2022 Report and Dashboard Development \u2013 Created stored procedure driven visualizations for SSRS and Tableau dashboards. Designed SSRS reports based on requirements.', u'Software Engineer/Sr. Applications Software Engineer\nRapp Collins Worldwide - Irving, TX\nJanuary 2007 to January 2012\nSr. Applications Support Engineer\nPioneered and led a newly formed team to spearhead Production Support. Responsible for on-call support for Retail Marketing, Data and Business Intelligence teams. Created and maintained technical documents. Monitored and provided root-cause analysis to identify and resolve failures to ensure SLA\u2019s were met. Mentored and provided 1st level support for onboarding new developers and production support team members. Generated dynamic Powershell functions and scripts from within T-SQL. Managed hundreds of ETL jobs across a dozen production servers.\n\u2022 T-SQL \u2013 Created complex queries using dynamic SQL to support automation in a MS SQL Server environment.\n\u2022 Technical Documentation \u2013 Oversaw the creation of quick reference guides (QRG) and production support resolutions for all production jobs. Created support documentation for automation and internal system improvements made to increase departmental efficiency.\n\u2022 Cross Functional Collaboration \u2013 Collaborated with QA teams and developers to resolve system defects. Led escalation of high severity production support issues by facilitating active bridge between multiple teams and network operations.\n\nSoftware Engineer\nParticipated in all phases of full life-cycle development / maintenance of systems, including requirements analysis, applications design, implementation, testing, deployment and documentation. Created .NET batch programs for client projects to access a SQL Server database. Trouble-shot and fixed problems through customer requests. Managed work items in Microsoft Team Foundation Server. Worked within an Agile Scrum environment.\n\u2022 ETL Development \u2013 Designed and developed ETL jobs to support desktop, web applications, batch processes and reports in a Microsoft SQL Server, .NET and Cognos environment. Used SSIS to import and export data into data marts and OLTP tables.\n\u2022 T-SQL \u2013 Created tables, stored procedures, functions and views using T-SQL in a Microsoft SQL Server environment. \u2022 Technical Documentation \u2013 Develop flowcharts, layouts and documentation of batch processes and database modeling.\n\u2022 Cross Functional Collaboration \u2013 Became sole on call 24x7 support for one year before rotating duties with other team members.', u'Software Developer\nCitigroup, Inc. / Cards Division - Irving, TX\nJanuary 2003 to January 2006\nManaged QA VMWare workstations in Cards Tech lab, Responsible for reviewing, analyzing, and evaluating current business systems, processes, workflow and overall user needs to formulate new tools to meet overall business strategies. Mentor and onboard new tech analyst. Created tables, stored procedures, functions and views using PL/SQL in an Oracle environment.\n\u2022 Technical Documentation \u2013 Created three system and process improvement documentations every year. Created documentation for subject matter expert (SME) areas that was used to cross-train other members of the team.\n\u2022 Cross Functional Collaboration \u2013 Collaborated with call center teams during desktop implementations and upgrades to ensure quality assurance. Received signed President George W. Bush Volunteer Service Award from Robert B. Willumstad, President & COO of Citigroup Inc. and other chief-level officers for VOE initiatives, service and collaboration in the community.']","[u'Master of Science in Information Systems Management', u'Bachelor of Science in Computer Information Systems']","[u'Keller Graduate School of Management Irving, TX\nJanuary 2006', u'DeVry University Irving, TX\nJanuary 2003']"
0,https://resumes.indeed.com/resume/a79e4055e750b07e,"[u'Data Engineer\nSantander Consumer USA - Dallas, TX\nJanuary 2015 to Present\nSantander Consumer USA is financial service firm in Auto Lending, Lease and Personal Lending.\n\nAccount Expansion Project: Initiated to accommodate accounts greater than 7 digits in existing\nsystems.\nParallel Data Warehouse (APS): Automate the process to Load data to APS from Data\nWarehouse on daily basis.\n\nAdverse-Action Project: Build the ETL process to load and incoming file, process the data and export the file to send it to third party vendor. The third party vendor is responsible to print letter and mail it to customers who apply for Auto Load. Also, automated the process.\n\nWork Day Project: Build and automate the ETL process to load a file generated from workday which has an employee data and load it to Type 2 table in Data Warehouse.\n\nRally Project: Download an XML file from Rally Website, convert it to a csv file using\nPowerShell Script and load data into Staging area. Process the data from Staging area using\nStored Procedure to load it into Data Warehouse and maintained the Historical Load.', u'SQL Developer\nLumbini Bikas Bank - Kathmandu, NP\nAugust 2013 to November 2015\nis a strong capital commercial bank in Nepal, it contributes to different sectors of economy of the nation from small industries to big\nprojects like hydropower, cement factory etc. The bank is equally contributing into the priority and the deprived sectors of the society as a partner for development of the Nepalese economy.\n\nSQL Developer\nResponsibilities:\n\u2022 Plan, design, and implement database objects, such as stored procedures and views.\n\u2022 Build and maintain SQL scripts, indexes, and complex queries for data analysis and extraction.\n\u2022 Provide database SQL support for business applications using MS Server SQL.\n\u2022 Designs and produces in-house reports.\n\u2022 Prepare daily, monthly and quarterly reports using SQL.\n\u2022 Provide timely, effective support, technical assistance, and training to end users.\n\u2022 Translated business requirements into technical requirements and develop application\n\u2022 Identified and implemented automation process enhancements for banking solution.\n\nEnvironment: Windows Server 2003, MS-SQL Server 2008R2/2012, Windows Batch script.']","[u""Bachelor's""]",[u'']
0,https://resumes.indeed.com/resume/8851d31e0f2cc5e3,"[u""Data Center Admin\nNASDAQ\nJanuary 2017 to Present\n\u2022 Troubleshot network related issues by logging into Nexxus switches and navigating through CISCO's CLI to obtain information about the host.\n\u2022 Racked and cabled networking devices and servers for colo customers and NASDAQ.\n\u2022 Configured management IP on HP, DELL, and PENGUIN servers.\n\u2022 Worked closely with the network operations team when deploying and configuring new hosts on the network.\n\u2022 Resolved break/fix tickets (power supply swap, fan swap, hard drive swap, solar flare card install, etc)\n\u2022 Assisted in planning and deploying new cabinets with sufficient infrastructure for NASDAQ's trading environments.\n\u2022 Solved hardware issues on Dell PowerEdge servers by logging into Linux hosts and navigating through the MegaCli application software to identify hard drive issues,"", u'Data Center Operator\nCANON - New York, NY\nFebruary 2016 to October 2016\n\u2022 Coordinated with the support team to record and resolve enterprise system and application issues.\n\u2022 Maintained off-site tape storage rotation.\n\u2022 Monitored, executed, and modified batch job schedules\n\u2022 Worked with the data center team to deploy new data center infrastructure\n\u2022 Monitored routers and switches to ensure connectivity with other sites.', u'Support Engineer Intern\nRQ Partners Inc - Forest Hills, NY\nSeptember 2015 to October 2015\n\u2022 Managed and troubleshot Windows desktops.\n\u2022 Created tickets for problems associated with user applications.\n\u2022 Provisioned and deployed virtual machines using VMware.\n\u2022 Fixed and upgraded server hardware.', u""Volunteer\nElmont Community Center - New York, NY\nMay 2015 to August 2015\n\u2022 Resolved hardware and software related desktop issues.\n\u2022 Built and configured servers and workstations.\n\u2022 Provisioned and deployed virtual machines.\n\u2022 Learned VMware technology and architecture.\n\u2022 Installed and configured VMware's hypervisor on servers.""]",[u'in Psychology'],"[u'Hunter College New York, NY\nDecember 2015']"
0,https://resumes.indeed.com/resume/2011c5040765565e,"[u'Data Engineer\nWorldWide Finance, Microsoft (Wicresoft)\nSeptember 2015 to Present\nDatabases\nWorldWide Finance, Microsoft (Wicresoft)\nSQL Server, Oracle, AWS Redshift, MySQL, Hive\nWorking on building forecasts, budgeting and prediction\nmodels for Microsoft finance group Reporting Tools\nCreated Finance reports which drives Product road-map\nCreating KPI metrics and reports which are used by finance\nMicrosoft SSRS, Visio, Python notebook, Power BI, OLAP Cubes\ncontrollers\nGathering inputs from various controllers to create quarterly Programming\nforecast reports, yearly budgeting models and product wide\nallocation models.\nC#, PL/SQL, T-SQL, MDX, DAX, HQL, Shell Scripting, VBA, MS\nAutomated the report generation workflow\nExcel\nTuning existing ETL and reporting models\nBuilding prototype on various enhancements. ETL Tools']",[u'Bachelor of Technology in mining'],[u'Kalinga Institute of Industrial Technology\nJanuary 2004 to January 2008']
0,https://resumes.indeed.com/resume/d19ba9e2ae5f6020,"[u'Data Specialist\nBank Associates Merchant Services - Brooklyn, NY\nDecember 2015 to March 2017\n\u2022 Used Microsoft Excel to organize data\n\u2022 Created template that expedited work output by a factor of 50%\n\u2022 Assisted with the creation of business proposals', u'Data Entry Clerk\nXequipped Group - Brooklyn, NY\nApril 2014 to June 2014\n\u2022 Created a content generator that mass uploads inventory to company website\n\u2022 Used HTML and CSS to improve the user interface of the website\n\u2022 Used to regularly upload to company blog and social media platforms', u'Engineer/Cashier/Counter Clerk\nHi-Tek Wireless - Brooklyn, NY\nJune 2012 to October 2012\n\u2022 Diagnosed cellular device issues\n\u2022 Dismantled and repaired physically damaged phones\n\u2022 Assisted customers with software issues']",[u'Bachelors of Business Administration in Business'],"[u'New York City College of Technology Brooklyn, NY\nAugust 2017 to May 2021']"
0,https://resumes.indeed.com/resume/533e201e2d43f04f,"[u'Data Analyst Intern\nEntertainment Interactive LLC\nOctober 2017 to Present\n\u2022 Designed pattern analysis and predictive analytics over performance data collected on players.\n\u2022 Modelled data visualization with Tableau, Power BI or R-programming on the generated analysis.', u""Senior Systems Engineer (Client\nInfosys Limited\nOctober 2013 to June 2017\nWorked as a Senior ETL and Reporting developer as well as played the role of the owner of the clients Global Customer and Marketing Data warehouse operations while collaborating and coordinating with downstream teams.\n\u2022 Led the change team proactively from traditional into agile methodology software development method for client's Global marketing database and campaign management\n\u2022 Managed the entire development and maintenance of the client's customer master database for report generation as a single point of contact\n\u2022 Created and supported the dashboards for the client's marketing campaign data-warehouse operations using Tableau\n\u2022 Spearheaded the complete Software Development Lifecycle (SDLC) of a customer database management system in the enterprise data warehouse projects of LexisNexis 3 times\n\u2022 Strengthened and stabilized the problematic batch operations on the client's customer database improving the performance from 60% to 95%\n\u2022 Provided support and developed ETL technical operations to cleanse data in warehouse to improve efficiency in data management for marketing users\n\u2022 Oversaw the post-release Key Performance indicator (KPI) measurements for three projects and reviewed the reports\n\u2022 Successfully directed the team during consecutive projects with 0 - 3 medium post-production issues and patches\n\u2022 Diagnosed and reengineered a specific alternative to a sort stage thereby increased the productivity by 30%\n\u2022 Conducted several knowledge transfer sessions, project presentations and team building events between the development teams, support teams, and end users\n\u2022 Mentored a group of four amateur engineers during project development phase""]","[u""Master's in Information Systems in Information Systems"", u""Bachelor's in Electronics and Communication Engineering in Electronics and Communication Engineering""]","[u'Georgia State University Atlanta, GA\nJuly 2018', u'Anna University\nMay 2013']"
0,https://resumes.indeed.com/resume/27feccea90ddeb8a,"[u'Data Scientist\nCognizant Technology Solutions\nJanuary 2013 to Present\nDeveloped data models, algorithms to implement machine learning solutions as a member of data scientist/data engineer team.\n\u2756 Ingestion of data from different source system via Sqooping/FTPS and perform data enrichment, exploration and enrichment.\n\u2756 Involved in creating/designing Hive tables, and loading and analyzing data using hive queries\n\u2756 Implemented Partitioning, Dynamic Partitions, Buckets in HIVE.\n\u2756 Developed GBM classification model for DNA mapping of more than 475 labels with 95% accuracy.\n\u2756 Chatbot for Document Search Engine using NLP, Tf-idf and web scrapping for keyword searching using cosine similarity in urls and webpage content.\n\u2756 Crawled 6,000 url documents and 2,000 pdf documents from their html links with Beautiful Soup and Goose library.\n\u2756 Applied PCA algorithm to decrease the 2000 dimensions dataset to 120 Dimension with only 0.3% decrease in accuracy.\n\u2756 Exploratory Analysis and feature engineering to best fit the model using Python.\n\u2756 Find best model and parameters using SK-learn Grid Search CV, Cross validation and Metrics.\n\u2756 Applied Ensemble using XGBOOST algorithm to identify the important features and used it to improve the accuracy of the prediction with Random Forest algorithm.\n\u2756 Initiated and Implemented deep-learning works with LSTM for sentiment analysis.\n\u2756 Multivariate analysis to build an algorithm for forecasting of next quarter variance using time series analysis and RNN with an accuracy of 86.2%.\n\u2756 Created framework to build and deploy Jars on cluster built using SBT assembly for different Machine learning modules such as Training, Prediction, Transformation.\n\u2756 Implementation of POCs/POVs with Spark Data Frames and SQL, and Spark Machine Learning (by using objects like Transformer and Estimator for Pipelines, Evaluator, Cross Validator).\n\u2756 Responsible to deliver data driven end-to-end solution, i.e. ingesting raw data, transformation, model training and generating prediction using Cloudera big data ecosystem (Sqoop, Spark, Hue, Hive, Impala).\n\u2756 Expertise in SK-learn, Tensor flow, Keras and H2O libraries.\n\u2756 Implemented data analytics projects and dashboard development on Tableau.', u'ETL Architect\nTata Consultancy Services\nJanuary 2012 to January 2012\nExtensively worked on Informatica, PL/SQL, Linux shell scripting as an ETL architect.\n\u2756 Develop various Oracle PLSQL package, procedure and Unix shell script as per business requirement.\n\u2756 Created the Data Warehouse Data Model to store atomic level data and the summarized data.\n\u2756 Performed SQL performance tuning & optimized queries using Explain plan.\n\u2756 Designed complex mapping architecture for critical applications with data enrichment, mapping design and flow of data.\n\u2756 Architectural design, development & production support of financial applications for Back office IT operation.\n\u2756 Worked on to implement the partitioning and sub partitioning methodologies to store and process high volume data in optimized fashion.\n\u2756 Responsible in gathering requirements from users and designing Use cases, Technical Design and Implementation of end solutions for customers.', u'Software Engineer\nBirlasoft\nJanuary 2009 to January 2011\nDeveloped mapping to extract, transform and load data using Informatica Power center.\n\u2756 RPD, report and dashboard development in OBIEE.']","[u'Master of Technology in Software Engineering', u'Bachelor of Technology in Electronics & Comm', u'Diploma in Mechanical Engineering', u'in Technology', u'']","[u'Birla Institute of Technology', u'Kurushetra University', u'Board of Technical Education', u'Stanford', u'Informatica Power center']"
0,https://resumes.indeed.com/resume/9cd7ecd8df9efe03,"[u'Building Manager\nMeadows and Ohly LLC - Marietta, GA\nFebruary 2018 to Present\nI manage the maintenance, engineering, and operations for Multiple (Class-A) Medical Office Buildings located on the WellStar Kennestone campus.\n\n* Manage vendors, routine maintenance, construction projects, and other\noperations of facilities.\n* Respond to tenants\u2019 service requests and special needs; keep them\ninformed of progress.\n* Ensure strong relationships are developed and maintained between\ntenants and all staff members.\n* Coordinate all necessary inspections and oversee corrective action in\norder to maintain regulatory compliance.\n* Schedule preventive maintenance, testing and inspections of all systems\nand equipment.\n* Conduct routine walk through of the property and look for housekeeping\nand other maintenance issues.\n* Communicate with Portfolio Manager and assist with the budget and\nplans for daily and future operations.\n* Order and maintain routine office and maintenance supplies.\n* Maintain files, manuals, and routine correspondence in the onsite\nmanagement office.', u'Lead Operating Engineer\nJones Lang LaSalle - Lithia Springs, GA\nAugust 2017 to January 2018\nAmazon Fulfillment Center\n\n* Insured that work performed was accomplished efficiently with a\nminimum amount of disruption and inconvenience.\n* Supervised and implemented the preventative maintenance program.\n* Scheduled preventative maintenance with a minimum disruption of\nbuilding services, performing and/or delegated preventative\nmaintenance tasks to the appropriately qualified maintenance staff\nmember, ordered parts and equipment required for repair, maintenance\nand installation of new equipment.\n* Provided training and supervision aimed at expanding the capabilities of\nthe operations staff.\n* Demonstrated the proper use and care of tools and instruments, giving\nhands on instruction in basic maintenance, safety, and trouble shooting\nprocedures, recommended relevant outside engineering courses for\nenrollment and instilling an overall level of professionalism in manner and\nappearance.\n* Complied with departmental policy for the safe storage, usage, and\ndisposal of hazardous materials.\n* Maintained a clean and safe workplace.\n* Planned and managed budget, allowing for appropriate control over, and\nutilization of institutional resources.\n* Recommended and estimated facilities repairs and improvements for\ninclusion in the annual budget.\n* Insured the availability of an adequate operating inventory of tools and\nsupplies.\n* Prepared and submitted purchase order requests, developing sources\nfor stock materials and performing periodic checks for supplies.', u'Operating Engineer\nCushman & Wakefield - Cartersville, GA\nSeptember 2013 to August 2017\nI managed the operations, engineering, and maintenance for a pair of (Class-A) Medical Office Buildings located on the Cartersville Medical campus.\n\n* Operated Building Management Systems (Trane Tracer & Siemens Apogee).\n* Scheduled the preventative maintenance of HVAC, Electrical, and Mechanical equipment.\n* Responded to work requests that are generated by the Tenants.\n* Monitored usage and consumption of utilities.\n* Assigned work requests to appropriate contractors or vendors.\n* Supervised major repairs and construction projects.\n* Authorized work orders, and submits invoices for payment.\n* Maintained records and logs of services and repairs.\n* Reviewed annual budgets, contracts, and submits proposals for capital projects.\n* Arranged Fire and Life-Safety tests and inspections.\n* Recorded, copied, and itemized purchases from expense accounts.\n* Submitted expense and reimbursement reports for outside purchases.\n* Worked on a flex schedule and available on call.\n* Reported directly to the Senior Property Manager.\n* Acted as the Chief Engineer for the Georgia region.\n* Prepared written incident reports for Risk Management.\n* Traveled as required for meetings and training.', u'Stationary Engineer (Data Center)\nLincoln Harris CSG - Atlanta, GA\nSeptember 2012 to September 2013\nSunTrust Data Center\n\n* Monitored critical electrical systems. (UPS, PDU, STS, ATS, Generators)\n* Serviced HVAC equipment. (Dry Coolers, Glycol Applications, Chilled and\nCondenser Water Loop Distributions, CRAC Units, In-row High Density\nSystems)\n* Scheduled preventative maintenance with minimum disruption of critical utility delivery. (100% Up-time)\n* Performed and/or delegated preventative maintenance to qualified service providers.\n* Ordered parts and equipment required for repairs, maintenance, installation, and inventory.\n* Inspected and bypassed FM-200 Fire Suppression System during service.', u'Building Engineer\nLincoln Harris CSG - Johns Creek, GA\nMarch 2009 to September 2012\nI was the Building Engineer for a (Class-A) Medical Office Building located on the Emory Johns Creek Hospital campus.\n\n* Performed the monthly preventative maintenance on the HVAC equipment.\n* Operated the Building Management System. (Trane Tracer Summit)\n* Scheduled Contractors for maintenance and repairs as needed.\n* Responded to work requests from the Tenants.\n* Scheduled the testing for Fire and Life-Safety systems.\n* Worked alone in this building on a flex schedule.\n* Reported directly to the Senior Property Manager.\n* Acted as the Chief Engineer for the North Georgia region.\n* Assisted the Property Manager with annual budgeting and capital projects.', u'Apprentice Engineer\nHines Interests LP - Atlanta, GA\nJanuary 2008 to February 2009\nOne Atlantic Center\n\nI was an Apprentice Engineer for this one million square foot (Class-A) high-rise building located in Atlanta, GA\n\n* Introduced to the concept of building engineering, facility management, and basic maintenance.\n* Trained to perform preventative maintenance on HVAC and Mechanical Equipment.\n* Educated on Electrical Systems and Generator Testing for commercial buildings.\n* Learned how to test Fire/Life-Safety Systems and Fire Pumps.\n* Instructed on how to read building drawings and schematics.']",[u''],"[u'The Medix College, Technical Training Center, in Smyrna']"
0,https://resumes.indeed.com/resume/bb7b1e89afbe42ef,"[u'Big Data/Software Engineer\nComcast - Philadelphia, PA\nMay 2016 to Present\n* Create Data Lake by extracting data from real time customer Set-top box action into HDFS. Implement data ETL with Map-Reduce in Spark SQL. Build load balancer with HAproxy.\n* Build Data pipeline upon AWS EC2. Manage distributed cluster as AWS admin role.\n* Compose distribution system based on Apache Spark framework. Implement Spark Streaming application with Scala for customer request streaming management and modeling.\n* Use YARN as resource manager. Utilize Kafka on distributed streaming system and Zookeeper for configuration synchronization. Monitor production exceptions with Splunk.\n* Implement back-end server using core JAVA as producer of Spark platform.\n* Design Cassandra DB schemas to store customer information. Implement DB driver template.', u'Big Data Engineer\nGoldman Sachs - Jersey City, NJ\nSeptember 2015 to April 2016\n* Developed Enterprise Data Anomaly Detection application.\n* Designed distribution system for TB-scale enterprise data processing. Utilized RabbitMQ on distributed streaming system.\n* Built Machine Learning Pipeline and ETL processing based on Spark distributed platform. Applied Spark MLlib and Random Forest algorithm with Scala to detect anomaly data.\n* Managed and scheduled Spark Jobs on Hadoop cluster.\n* Developed back-end algorithm simulation service using SK-learn framework and Python.\n* Designed Mongo DB 3.x schemas for anomaly data storage. Implemented DB driver template using JAVA. Applied Agile development for entire project.', u'Data/Software Engineer\nComcast - Philadelphia, PA\nJanuary 2015 to July 2015\n* Developed a user TV watching statistical application based on one million customers.\n* Created Data Warehouse by extracting data from customer watching history. Implemented ETL processing with Kettle.\n* Implemented statistical service with JAVA. Applied Spring framework for Web service.\n* Optimized Asynchronous application in Spring framework to process high concurrency request. Managed system performance & capacity according to user watching habit.\n* Designed Oracle DB schemas. Implemented DB driver using Hibernate framework.']","[u'Master in Software Engineering in Software Engineering', u'Master in Software Engineering in Software Engineering', u'Bachelor in Electronic Information Technology in Electronic Information Technology']","[u'The University of Texas at Arlington Arlington, TX', u'Beijing University of Posts and Telecommunications', u'Central South University']"
0,https://resumes.indeed.com/resume/e9db4a2c8cb6bb0d,"[u'Data Scientist\nMontefiore Medical Center\nMay 2015 to Present\nUSA\n\u2022 Presently working with Montefiore Medical Center to investigate and build the quantitative models using R.\n\u2022 Design, scope, and drive implementation to test hypothesis of data-driven models and programs to improve the efficiency in identifying the high risk patients.\n\u2022 Worked on building a model by choosing selective behavioral, demographic and transactional attributes of patients from the enterprise data warehouse. Using statistical models, such as, RFM Analysis, Logistic Regression and Decision Trees, to obtain potential customers, who would be receptive to new Health Plan.\n\u2022 Developed efficient pig and hive scripts with joins on datasets using various techniques in a Big Data environment.\n\u2022 Analyzed large data sets to develop custom models and algorithms to drive business solutions.\n\u2022 Created predictive models using structured and unstructured data available within various departments.\n\u2022 Loaded unstructured data in HIVE and analyzed the data for reporting purpose.\n\u2022 Applied advanced analytical methods to improve decision-making.\n\u2022 Analyzed diverse sources of data, and implemented various machine learning algorithms to realize actionable results.\n\u2022 Solved complex decision-making problems using machine learning, statistical analysis, and mathematical optimization\n\u2022 Assessed various machine learning frameworks and packages.\n\u2022 Building the Data Models and working with Operation team to deploy it in Production environment.\n\u2022 Collaborated effectively with other data sciences teams to align on direction and leverage existing tools and knowledge\n\u2022 Assessed the data quality using R scripts and provided the meaningful insights.\n\u2022 Worked on the data analytics project, that provides data driven solutions to improve the cross-selling strategies implemented by the client. The goal is to identify dependent variables that would influence an existing member in deciding to purchase additional Health Plan benefits.\n\u2022 Developed models and algorithms to learn the recommended strategies.\n\u2022 Worked on both Supervised and Unsupervised Learning Techniques including Neural Network.', u'IT Analyst\nUNM IT\nJanuary 2014 to May 2015\nUSA\n\u2022 Worked closely with key business partners to understand business and operations-related problems, and then applied analytical methods to solve them.\n\u2022 Formulated the development strategy and developed the project plan using MS Project.\n\u2022 Assisted the Project Manager in Project Planning and Work Breakdown Structure\n\u2022 Translated business requirements into system and functional requirements.\n\u2022 Analyzed the Patient data for forecasting the impressions of Care Management intervention.\n\u2022 Worked in an Agile environment using JIRA and coordinating with other team members to follow the best practices.\n\u2022 Developed Use Cases, Activity, Sequence Diagrams and system design and database structure.\n\u2022 Designed and created the management and operation reports using SSIS tool.', u'Sr. System Engineer\nIBM Corporation\nJune 2008 to January 2014\nUK & India\n\u2022 Experience in interacting with business users and executives to identify their needs, gathering requirements and authoring Business Requirement Documents (BRD), Use Case Diagrams, Activity Diagrams and Sequence Diagrams using UML modeling.\n\u2022 Coordinated with other technology teams to extract, transform, and load data from a wide variety of data sources using SQL and other ETL solutions\n\u2022 Wrote complex SQL queries PL/SQL function, procedure, packages, cursor and triggers to retrieve the data from sources system and to count and validate the data and data set.\n\u2022 Gathered the Business Requirements from the Stakeholders and created the functional requirement documents.\n\u2022 Created to Source to Target Mapping documents in an ETL project using Informatica Power Center.\n\u2022 Finalized the project scope transferred over to IBM.\n\u2022 Monitored and assessed the Knowledge Transfer, performed gap analysis, and mitigated risk.\n\u2022 Executed the outsourcing strategy of a software testing project from UK to India.']","[u'MBA in Project Management', u'in Electronics and Computer Engineering']","[u'University of New Mexico Albuquerque, NM\nDecember 2015', u'Rajiv Gandhi Technical University\nMay 2008']"
0,https://resumes.indeed.com/resume/a392cc8a0cdf635d,"[u""SUPERVISOR\nPLANET FITNESS\nMay 2016 to Present\n\u2022 Recruited and developed a high performing staff consisting of management, member service representative, trainers and custodians\n\u2022 Effectively administer and schedule personnel training for over 50 employees\n\u2022 Composed staff development plan offering retraining opportunities to encourage growth and maximize profitability\n\u2022 Slashed payroll while ensuring the continuation and enhancements of services\n\u2022 Surpassed expected sales goals which increased overall EFT\n\u2022 Executed all marketing promotions efforts by making sure all personnel are knowledgeable of marketing efforts\n\u2022 Tremendously cut inventory loss\n\u2022 Lead by example and maintained consistent accountability for direct reports (bi-weekly, weekly, monthly) ensuring adherence to the company's values and goals"", u'DATA ENTRY ENGINEER\nUNITED PARCEL SERVICE\nSeptember 2015 to May 2016\n\u2022 Administered packages for courier shipment\n\u2022 Reviewed invoices\n\u2022 Processed international shipments utilizing web based programs; ensuring specific customer requirements are followed']","[u'BACHELOR OF SCIENCE in BUSINESS MANAGEMENT', u'']","[u'VIRGINIA STATE UNIVERSITY\nDecember 2016', u'Reginald F. Lewis College of Business\nJanuary 2015 to January 2016']"
0,https://resumes.indeed.com/resume/89ee81b5825595f0,"[u'Data Scientist\nCon Edison - New York, NY\nOctober 2017 to Present\nThis project is Building of Public Assembly (BOPA) - is as any school, hospital, nursing home, institution licensed by New York State for the care of children*, or any factory which normally employs 75 or more persons or any other building with a nominal capacity of 75 or more persons to which the public is regularly admitted. Structures which are used solely as office buildings or residential apartments and normally have no other utilization more than the 75-person limit are excluded from this definition. E achieved to get 80% of accuracy.\n\nResponsibilities:\n\u2022 Applied Web scrapping, Web-crawling, Data-Extraction to gather more features and Removed bad data by data cleansing, database management\n\u2022 Wrote Geo Coding (NYC Building CODE API) and collected the building code and latitude and latitude.\n\u2022 Applied clustering algorithms on building code data to study the underlying data patterns.\n\u2022 Implemented MLib functions for training and building fully loaded classifiers models using Spark streaming, Spark SQL and Machine Learning APIs.\n\u2022 Performed data troubleshooting, performance tuning and data analysis as a part of day-to-day client support.\n\u2022 Developed Geo Matching Algorithm with standard radius to identity the matching building codes\n\u2022 Applied SQL for Data Aggregation, queries and created database in SQL Server.\n\n\u2022 Developed Clustering algorithms to categorizing building codes with identify exceptions.\n\n\u2022 Applied linear regression in Spark to understand the relationship between different attributes of dataset and causal relationship between them.\n\u2022 Applied Logistic Regression, Support vector, Light GBM and XGBOOST and selected the Light GBM with high accuracy on building categories.\n\u2022 Designed and developed some of the modules of the system in Azure Platform using Spark Cluster.\n\u2022 Achieved creating automation process on Azure with Hadoop platform\n\u2022 Implemented Spark Scripts using Spark ML Libraries and Spark SQL to access hive tables into spark for faster processing of data.\nEnvironment: Python, PYSPARK, Azure, PyCharm, MS EXCEL, Spark SQL, Microsoft Office, Tableau 8.3, MapReduce, ML/DL algorithms.', u""Data Scientist\nPacific Control Inc\nApril 2017 to October 2017\nPacific Controls is at the forefront of platform, data infrastructure and service delivery innovation for enterprise and managed solutions where convergence of networked computing, large scale data management with real time machine. Pacific control building an application. Mainly working on collecting every information in all sectors of data like GEO, Historic, present time line data so on. This application mainly on Analyzing market and consumer understanding to meet the specific business needs in a wide range of industries, such as foodservice, building cleaning and maintenance, education, hospitality, and care homes. results to deliver the prediction analysis and real time analysis using SPARK.\n\nResponsibilities:\n\u2022 Multivariate business and resource forecasting using machine learning algorithms.\n\u2022 Applied linear regression, multiple regression, ordinary least square method, mean-variance, theory of large numbers, dummy variable, residuals, Poisson distribution, fitting function and many more to data with help of Spark MLIB libraries.\n\u2022 Applied clustering algorithms on market data to study the underlying data patterns. Methodologies used were logistic regression through Spark, for projecting market rate.\n\u2022 Implemented MLib functions for training and building fully loaded classifiers models using Spark streaming, Spark SQL and Machine Learning APIs.\n\u2022 Written Spark programs to model data for extraction, transformation and aggregation from multiple file formats including XML, JSON, CSV& other compressed file formats.\n\u2022 Developed Spark scripts, UDF's using both Data frames/SQL and RDD/MapReduce in Spark for Data Aggregation, queries and writing data back into RDBMS through Sqoop.\n\u2022 Applied linear regression in Spark to understand the relationship between different attributes of dataset and causal relationship between them.\n\u2022 Used Spark SQL to process the huge amount of structured data and Implemented SPARK RDD transformations, actions to migrate Map reduce algorithms.\n\u2022 Implemented Spark SQL to access hive tables into Spark for faster processing of data.\n\u2022 Designed and developed some of the complex modules of the system in AWS using Spark Cluster.\n\u2022 Create, modify and execute DDL and tables to load data into Hive and AWS Redshift tables. Data Validation, Quality check in RedShift using Python.\n\u2022 Implemented Spark Scripts using Spark ML Libraries and Spark SQL to access hive tables into spark for faster processing of data.\n\n\u2022 Developed Clustering algorithms and Support Vector Machines that improved Customer segmentation and Market Expansion.\n\nEnvironment: SPARK, SCALA, AWS, Jupyter notebook, MS EXCEL, Spark SQL, Microsoft Office, Tableau 8.3, MapReduce, ML/DL algorithms."", u""Data Scientist\nTUFT'S MEDICAL CENTER - Boston, MA\nSeptember 2016 to April 2017\nThe project was to build a recommendation system that accurately suggest customer health symptoms and habit based on the historical data, data collected from four areas within healthcare, claims and cost data, pharmaceutical and research and development (R&D) data, clinical data (collected from electronic medical records (EHRs)), and patient behavior and sentiment data. The results were used by the higher authorities to create appropriate strategies. Further, developed BI reports that provided predictive analytics, future prediction of healthcare.\nResponsibilities:\n\u2022 Extract health care claims, provider, and enrollment data from the Colorado All Payer Claims Database (CO APCD) in support of the triple aim for Better health, better quality, and lower costs.\n\u2022 Analyze customer, behavior data, symptoms data, transaction data and campaign data to identify trends and patterns of data in different visualization techniques like Seaborn library in PYTHON.\n\u2022 Designed and developed some of the complex modules of the system using AWS, S3.\n\u2022 Extraction of large amounts of data for analysis and reporting. Responsible for documentation of all analysis as well as data discrepancies in both Spark and Python.\n\u2022 Working closes with other analysts to reconcile all issues related to data production, data extraction and delivery in order to ensure the integrity of the data and the reporting that it is used for.\n\u2022 Determined the missing data, outlier and invalid data and applied appropriate data management techniques.\n\u2022 Worked on data manipulation and raw marketing data of different formats from multiple sources and prepared the data for Sentiment analysis of all the customer medical issue data using packages like NLTK (Natural Language Processing with Python / Analyzing Text with the Natural Language Toolkit).\n\u2022 Wrote script in python to predict number of people getting effect of some diseases, by collecting set of predicted (symptoms) data from all medical sectors and evaluated with outcome data and Make the aware of people using Machine Learning Module like logistic regression.\n\u2022 Wrote simple and advanced SQL queries and scripts to create standard reports for senior managers.\n\u2022 Designed custom reports, charts, tables and dashboards using Power BI and for marketing and operations teams for their decision-making process.\n\u2022 Create focused reports and dashboards on content/channel performance, lead generation and conversion rates.\n\u2022 Assisted in writing a wide range of documents including work plans, monthly, quarterly and annual progress reports, and provider/grantee guidance materials and manual.\n\nEnvironment: Jupyter Notebook, PYTHON, AWS, Sentiment analysis, My SQL, CRM, Tableau, MS Access, Power BI."", u'Data Analytics\nUNIVERSITY OF MASSACHUSETTS - Boston, MA\nAugust 2015 to August 2016\nWorded on projects were to detect the future(new) Ham or scam emails and identify and Search famous Restorents with set of reviews by given address and radius using both spark and python in AWS. During this project, we have encountered whether the new email is ham or span with historical data by trained model. we analyzed and developed a text mining mechanism to identify suspects with high percentage of word repeated in Wikipedia. My responsibilities were to perform text mining in python and parse text documents to compute the TF*ID by using HashingTF and IDF libraries in Spark. More project details in GitHub (GitHub: https://github.com/abhilashpatel22/ )\n\nResponsibilities:\n\u2022 Gathering business requirements and collected all set of required data from different sectors and we clean the data.\n\u2022 Developed two generic categories like Spam and ham classifier using Python.\n\u2022 17,000 emails were processed and categorized into ham and spam classifier in AWS Cloud Service.\n\u2022 Loaded our training data into a pandas DataFrame of ham and spam emails. Implemented sklearn with family na\xefve Bayes to train a module with ham and spam classifier using MultinomialNB.\n\u2022 Naive Bayes Methods in Machine Learning techniques to identify and to predict the New email is whether it is ham or scam.\n\u2022 Identified entities and emotions in a sentence and use these to determine if the entity is being viewed positively or negatively using NLTK Package.\n\u2022 Identified set of restaurants in given radius with longitude and latitude and Extract review snippets.\n\u2022 Gathered set of Wikipedia documents and Worked on analysis in python to analyze the TF-IDF rate of each words in Rating in each restaurant.\n\u2022 Compiled set of 30 reviews (yelp data) each of ten neighborhood restaurants and Achieved to Identify best restaurants around any place. Implemented in both Python and Spark.\n\nEnvironment: Python, Spark, Machine learning modules, Apache Spark, Jupyter notebook, AWS.', u""Data Analyst/Analytics Engineer\nCOVANCE\nJune 2013 to August 2015\nCovance is a contract research organization (CRO) providing drug development and animal testing service and it is a largest companies of its kind in the world. This was a research project to predict the occupancy in an office room using data gathered from Several departments like D-test, H-test, travel and expense(T&E), Investment, HRMS and CRM. Analysis will help in predicting over all expenses in all department in Medical and organization on each test. Mainly, we predict number of people used to clime the expenses. I have worked with various business Areas like Enrollment, Claims, Finance, Providers, and Benefits Admin.\n\nResponsibilities:\n\u2022 Lead a team offshore to work on client requests, Manage regular deliverables /weekly reports. between client's requirement and offshore team. I also mentor offshore team /help them with different data\nanalytics approach to solve Business problems.\n\u2022 Designed use case using UML and managed the entire functional requirements life cycle of credit card transaction.\n\u2022 Developed Python programs for manipulating the data reading from various Oracle data sources and consolidate them as single CSV File, and update the content in the database tables.\n\u2022 Designed easy to follow visualizations using Tableau software and published dashboards on web and desktop platforms.\n\u2022 Created data mapping document to map customer data from different systems to Enterprise customer data mart.\n\u2022 Exploratory data analysis using python to deep dive into internal and external data to diagnose areas of improvement to increase efficiency. Used Jupyter notebook to write script in python.\n\u2022 Lead the company's machine learning and statistical modeling effort including building predictive models and generate data products to support customer segmentation, product recommendation and allocation planning; prototyping and experimenting Machine learning ML algorithms and integrating into production system for different business needs.\n\u2022 Utilized Booted Logistic regression, Decision tree and Regression Machine Learning models to develop.\n\u2022 Involved in start to end process of Hadoop project Initiation, planning, executing, monitoring and controlling.\n\u2022 Involved in running Hadoop jobs for processing millions of records of raw data and creating Hive tables, loading and analyzing data using hive queries..\n\u2022 Automated all the jobs, for pulling data from upstream server to load data into Hive tables, using Oozie workflows.\n\u2022 Worked on importing data from HDFS to MYSQL database and vice-versa using SQOOP.\n\u2022 Handled importing of data from various data sources, performed transformations using Hive, MapReduce, loaded data into HDFS and Extracted the data from MySQL into HDFS in Microsoft Azure platform.\n\u2022 Provide first-line problem resolution or escalation, as well as technical assistance. Analyze end user needs and developing customer oriented solutions which interface with existing applications.\n\nEnvironment: Python, Jupyter Notebook, MS EXCEL, SQL, Microsoft Office, Tableau 8.3, Hadoop, Hive, MapReduce, Machine learning algorithms, Microsoft Azure."", u'Data Analyst/Analytics Engineer\nAmex - New York, NY\nOctober 2011 to June 2013\nThe American Express Company, also known as Amex, is an American multinational financial services corporation headquartered in Three World Financial Center in New York City. The project provides advanced functional and application modules as well as complex system maintenance of the travel expenses and billing applications to ensure compliance with industry standards and regulations. Also, provides support for data exchanges with business partners and successful integration of partner systems. Primary job function was to Interprets results using a variety of techniques, ranging from simple data aggregation via statistical analysis to complex data mining using Machine learning modules.\n\nResponsibilities:\n\u2022 Wrote structured application/interface code from specifications conforming to established methodology and standards. Provide first-line problem resolution or escalation, as well as technical assistance.\n\u2022 Analyze end user needs and developing customer oriented solutions which interface with existing applications. Designed and analyzed the SQL Server database and involved in gathering the user requirements.\n\u2022 Wrote simple and advanced SQL queries and scripts to create standard reports for senior managers.\n\u2022 Imported data from SQL Server and Excel to SAS datasets. Performed data research forecasting models into production by using Base SAS, SAS Macros, PROC SQL and many other SAS tools.\n\u2022 Designed several ETL jobs using SAS DI Studio to port the ODS data into data marts with tasks involving configuring metadata libraries in SCM.\n\u2022 Improved financial status by analyzing results, monitoring variances, identifying trends, recommending actions to management with different time and place using time Series Analysis.\n\u2022 Imported the customer data into Python using Pandas libraries and performed various data analysis - found patterns in data which helped in key decisions.\n\u2022 Maintain data integrity across modules (i.e. marketing, Sales, Contact) in HRMS/CRM Database using Machine learning modules like Regression, Logistics and cluster Modules.\n\u2022 Provide data analysis, finding Root/Cause data integrity problems in HRMS, Report generation. Ensured all data has a complete and accurate definition\n\u2022 Implemented Python libraries such as Numpy, Matplotlib, Pandas, SKlearn and used them to create dashboards and visualizations, with using IDE - Spyder/ Jupyter notebook.\n\u2022 Determined the cost of operations by establishing standard costs, collecting operational data and by using research analytics and data modeling techniques.\n\u2022 Guided cost analysis process by establishing and enforcing policies and procedures, providing trends and forecasts, explaining processes and techniques, recommending actions by Regression, Logistic and cluster modules.\n\u2022 Created audit reports involving charts and graphs using Excel, Power BI dashboards, to illustrate revenue comparison for subprime lending.\n\u2022 Responsible for data aggregation, data pre-processing, missing value imputation and descriptive and inferential analysis.\n\u2022 Worked with the QA team to prepare test strategy documents which include testing overview approach, strategy, role, responsibilities and complete scope of testing.\n\nEnvironment: python, Jupyter notebook, SQL, Microsoft Office, MS Power BI, MS Dynamics CRM, SQL Server Reporting Services (SSRS), Microsoft Excel, Spyder.', u'Data Analyst AS Intern\nBSNL - Bengaluru, Karnataka\nAugust 2010 to October 2011\nBSNL Mobile is an Indian mobile network operator, operated by the public enterprise BSNL. BSNL Mobile has a pan-India presence with presence in all the 21 telecom circles in India. I worked as Intern, analyzing and building variety of business reports to support critical business decisions. My role is to collect data that are crucial for business operations from appropriate sources, analyze, manipulate and present them to senior management in the form of Reports and Dashboards.\n\nResponsibilities:\n\u2022 Worked with different teams to gain insights about the data concepts behind their business.\n\u2022 Research, update, and validate data underlying spreadsheet production; strategically fill gaps.\n\u2022 Create pivot tables and modify spreadsheets to achieve analytical goals.\n\u2022 Wrote simple and advanced SQL queries for extracting data and created dashboard and stories for senior managers.\n\u2022 Delivered file in various file formatting system (ex. Excel file, Tab delimited text, Comma separated text, Pipe delimited text etc.)\n\u2022 Analyzed data using complex SQL queries, across various databases.\n\u2022 Verify entered account data by reviewing, correcting, deleting, or reentering data, combining data form both systems when account information is incomplete, purging files to eliminate duplication of data.\n\u2022 Performed data analysis primarily Identifying data sets, source data, source meta data, data definitions and data formats.\n\u2022 Used R Programing language to simple and complicate data analysis. Mainly, implement a wide variety of statistical and graphical techniques, including linear and nonlinear modeling, classical statistical tests, simple time-series analysis using IDE- R studio.\n\u2022 Created required SQL queries to be integrated with database to meet the requirement for generating the user defined formulary reports.\n\u2022 Assisted and support to application development team members as required for back end development while getting interaction between application and database (MYSQL).\n\u2022 Prepares source data for computer entry by compiling and sorting information; establishing entry priorities to set up and complete build of materials.\n\u2022 Ensured best practices are applied and integrity of data is maintained through security, documentation, and change management\nEnvironment: R, R-studio, SQL, Microsoft Office, Microsoft Excel, MS Access, Statistical techniques, MYSQL.', u'Web Developer/ Academic Projects\nSiddaganga Institute of Technology - Tumkur, Karnataka\nJune 2009 to August 2010\nWorked on Project to developed Event Organization website using PYTHON and front end development technologies using HTML CSS, Bootstrap and JavaScript. Used MYSQL for backend database. Meanwhile, help the team with technology issues including writing SQL statements for data interaction. Eventually, Worked with different team for technology suggestion, manly with HTML/CSS templates and any Development issues.\n\n\u2022 Build the web site mainly for research department of mammals and data management. (This website has been placed in my GitHub drive)\n\u2022 Developed the UI Screens using HTML5, Bootstrap, JavaScript and CSS3 for complex page layouts while adhering to code standards.\n\u2022 Perform design, architect, implement, and maintain real estate investment and financial management web applications.\n\u2022 Set up, configure, and maintain DNS for web application domain name.\n\u2022 Provided HTML 5 validations, Java script validations and Java validations on the controller side.\n\u2022 Created and deployed new features to sustain and amend existing applications.\n\u2022 Developed dynamic e-mails using JavaScript, and hand coding of HTML, XHTML, and CSS.\n\u2022 Implemented the all social sites and twitter connection with review of each mammals.\n\u2022 Implemented the MYSQL database to interact with application and database through SQL queries.\n\u2022 Played an active role in testing the application by writing test cases for different scenarios.\n\u2022 Created use cases, class diagrams, activity diagrams and collaboration diagrams.\n\u2022 Involved in System documentation, object documentation and User documentation.\n\nEnvironment: PYTHON, MYSQL, SQL, Microsoft Office, Microsoft Excel, MS Access, HTML5, Bootstrap, JavaScript and CSS3.', u'Data Scientist\nUnited States']","[u'MASTER OF SCIENCE in Computer Science', u'BACHELOR in ENGINEERING', u""Bachelor's""]","[u'UNIVERSITY OF MASSACHUSETTS Boston, MA', u'SIDDAGANGA INSTITUTE OF TECHNOLOGY', u'']"
0,https://resumes.indeed.com/resume/fdabc8f0b92c5264,"[u'Software Engineer\nMarine Data and Information Center - Tianjin, CN\nJuly 2010 to March 2015\n\u2022 Developed Back Propagation (BP) Neural Network algorithms in C++ to retrieve water depth from satellite images\n\u2022 Developed metadata management software in C# to facilitate XML metadata auto-capture from database\n\u2022 Implemented metadata web service for metadata search and query using J2EE and RESTful API\n\u2022 Designed geodatabase schema employing Unified Modeling Language and created geodatabase using CASE tools\n\u2022 Built geospatial database in Oracle by extracting, transforming and loading (ETL) 100GB heterogeneous datasets']","[u'M.S. in Computer Science', u'in M.E. Geographic Information Science and Remote Sensing']","[u'University of California Irvine Irvine, CA\nSeptember 2017 to December 2018', u'Wuhan University\nSeptember 2004 to June 2010']"
0,https://resumes.indeed.com/resume/3ccd801ac647b1e7,"[u'Data Analyst\nCD Distribution - US\nSeptember 2017 to Present\n\u2022 Analyzed inventory activities in the QuickBooks to prepare various reports and recommendations to improve efficiency and effectiveness of the business\n\u2022 Designed interactive dashboards for customer insights by automated reporting to executives and published real-time data visualizations using Power BI\n\u2022 Implemented Bitrix24 to streamline the customer relationship management within the organization to make the process flow organized', u'Application Developer Intern\nATOS - US\nMay 2016 to December 2016\n\u2022 Developed an Android application using Android studio which does various operations like VM ordering, Service Provisioning, Cloud Infrastructure to our clients\n\u2022 Tested, manipulated, and analyzed large sets of data from SQL server as well as various business intelligence tools using complex SQL queries using joins, grouping, aggregation, nested and subqueries\n\u2022 Performed User acceptance testing, developed test cases for end users and generated reports from SQL Lite DB which is mounted within the Android app for internal auditing', u'System Engineer\nInfosys Limited\nAugust 2011 to July 2015\n\u2022 Coded 1000+ requirements from clients by utilizing Java, SQL and UNIX for Core Banking (FINACLE)\n\u2022 Developed appropriate reports and conducted technical support on each release by evaluated 400+ test plans in various platforms and supported more than 100 production servers including Linux, AIX for ETL\n\u2022 Mentored a sub team of 5 members to coordinate towards an efficient design of the service and administered the environments like SIT, UAT, Production and testing environments']","[u'M.S., Information Technology in Management', u'B.S. in Electronics and Communication Engineering']","[u'The University of Texas at Dallas Dallas, TX\nMay 2017', u'Anna University Chennai, Tamil Nadu\nMay 2011']"
0,https://resumes.indeed.com/resume/8d44de10797ba63f,"[u""Data Engineer\nWalmart - Bentonville, AR\nDecember 2016 to Present\nResponsibilities:\n\n\u2022 Experience in working with different Hadoop distributions like CDH and Hortonworks.\n\u2022 Day to day responsibilities includes solving developer issues, deployments moving code from one environment to other environment, providing access to new users and providing instant solutions to reduce the impact and documenting the same and preventing future issues.\n\u2022 Experienced with the Spark improving the performance and optimization of the existing algorithms Hadoop in using Spark Context, Spark-SQL, Data Frame, Pair RDD's, Spark YARN.\n\u2022 Experienced with Hadoop ecosystems such as Hive, HBase, Sqoop, Kafka, Oozie etc.\n\u2022 Creating end to end Spark applications using Scala to perform various data cleansing, validation, transformation and summarization activities according to the requirement.\n\u2022 Experienced on adding/installation of new components and removal of them through Ambari.\n\u2022 Monitoring systems and services through Ambari dashboard to make the clusters available for the business.\n\u2022 Experience With installing and configuring Distributed Messaging System like Kafka.\n\u2022 Performance tuning the Spark jobs by changing the configuration properties and using broadcast variables.\n\u2022 Experienced in Setting up the project and volume setups for the new projects.\n\u2022 Experience in configuring the Storm in loading the data from MYSQL to HBASE using jms.\n\u2022 Architecture design and implementation of deployment, configuration management, backup, and disaster recovery systems and procedures.\n\u2022 Strong Data warehousing experience specialized in Teradata, ETL Concepts, Development and Testing. Extraction, Transformation and Loading (ETL) data from various sources into Data Warehouses using Teradata load utilities.\n\u2022 Extensive experience in Developing Informatica complex mappings/Mapplets using various Transformations for Extraction, Transformation and Loading of data from multiple sources of data warehouses and creating workflows with work lets & tasks and scheduling using workflow manager.\n\u2022 Good exposure to Teradata Manager, FASTLOAD, MULTILOAD, TSET, TPUMP, SQL for workload management.\n\u2022 Expertise in using Teradata SQL Assistant, Teradata Manager and data load/export utilities like BTEQ, Fast Load, Multi Load, Fast Export and exposure to Tpump on UNIX environment.\n\u2022 Have a strong experience in Teradata development and index's (PI, SI, PARTITION, JOIN INDEX) etc.\n\u2022 Changing the configurations based on the requirements of the users for the better performance of the jobs.\n\u2022 Worked Spark on Treadmill to deploy a cluster from scratch under couple of minutes.\n\u2022 Developed Map Reduce programs to cleanse the data in HDFS obtained from heterogeneous data sources to make it suitable for ingestion into Hive schema for analysis.\n\u2022 Implemented complex Map Reduce programs to perform joins on the Map side using distributed cache.\n\u2022 Setup flume for different sources to bring the log messages from outside to Hadoop HDFS.\n\u2022 Developed a data pipeline using Spark and Hive to ingest, transform and analyzing data.\n\u2022 Supporting and building the Data Science team projects on to Hadoop\n\u2022 Worked on a POC on Spark and Scala parallel processing. Real streaming the data using Spark with Kafka.\n\u2022 Setting up MySQL master and slave replications and helping business applications to maintain their data in MySQL Servers.\n\u2022 Monitored multiple clusters environments using AMBRI Alerts, Metrics and Nagios.\n\u2022 Exported the analyzed data to the relational databases using SQOOP for visualization and to generate reports for the BI team.\nEnvironment: Hadoop, Spark, HDFS, Hive, Pig, HBase, Big Data, Oozie, Sqoop, Kafka, Flume, Zookeeper, MapReduce, Cassandra, Scala, Linux, NoSQL, MySQL Workbench, Java, Eclipse, Oracle 10g, SQL."", u'Data Engineer\nUnited Health Group - Plymouth, MN\nAugust 2015 to November 2016\nDescription:\nAs a Big Data implementation developer within a team of engineers. We are responsible for developing and implementing programs. We are building a brand new, data-intensive software product that is focused on bringing the healthcare data together.\nResponsibilities:\n\u2022 Installed, Configured and Maintained the Hadoop cluster for application development and Hadoop ecosystem components like Hive, Pig, HBase, Zookeeper and Sqoop.\n\u2022 In depth understanding of Hadoop Architecture and various components such as HDFS, Name Node, Data Node, Resource Manager, Node Manager and YARN / Map Reduce programming paradigm.\n\u2022 Monitoring Hadoop Cluster through Cloudera Manager and Implementing alerts based on Error messages. Providing reports to management on Cluster Usage Metrics and Charge Back customers on their Usage.\n\u2022 Extensively worked on commissioning and decommissioning of cluster nodes, replacing failed disks, file system integrity checks and maintaining cluster data replication.\n\u2022 Experience in assigning number of mappers and reducers to Map reduce cluster.\n\u2022 Setting up HDFS Quotas to enforce the fair share of computing resources.\n\u2022 Experience in Configuring and maintaining YARN Schedulers (Fair, and Capacity).\n\u2022 Wrote the shell scripts to monitor the health check of Hadoop daemon services and respond accordingly to any warning or failure conditions.\n\u2022 Responsible for maintaining different Testing/QA Environments and erection of the PROD Environment in AWS.\n\u2022 Utilize Puppet for configuration management of hosted Instances within AWS. Configuring and Networking of Virtual Private Cloud (VPC). Utilize S3 bucket and Glacier for storage and backup on AWS.\n\u2022 Administered databases using RDS, MySQL and DynamoDB in AWS and executed the DML and DDL scripts.\n\u2022 Utilized AWS CloudWatch to monitor the performance environment instances for operational and performance metrics during load testing.\n\u2022 Experience in setting up HBase cluster which includes master and region server configuration, High availability configuration, performance tuning and administration.\n\u2022 Created user accounts and given users the access to the Hadoop cluster.\n\u2022 Involved in loading data from UNIX file system to HDFS.\n\u2022 Worked on ETL process and handled importing data from various data sources, performed transformations.\n\u2022 Coordinate with QA team during testing phase.\n\u2022 Provide application support to production support team.\n\nEnvironment: Cloudera, HDFS, Hive, Sqoop, AWS, EMR, Zookeeper and HBase, Linux Java, HDFS Map Reduce, Pig Hive HBase Flume Sqoop, Shell Scripting. Zookeeper, Spark SQL, Java, Unix and Java.', u'Data Engineer\nFarmers Insurance Group - Woodland Hills, CA\nMarch 2014 to July 2015\nDescription:\nThis project was developed to observe keyword competition among different companies that advertise online. We processed structured and unstructured data using Hadoop ecosystem components. This data is then analysed to see how competitors can make use of similar keywords to make their ads visible.\nResponsibilities:\n\u2022 Experience in working with Flume to load the log data from multiple sources directly into HDFS.\n\u2022 Used Flume to collect, aggregate, and store the web log data from different sources like web servers and pushed to HDFS.\n\u2022 Implemented a distributed messaging queue to integrate with Cassandra using Apache Kafka and Zookeeper.\n\u2022 Takes care about performance and security across all the Restful API.\n\u2022 Implemented data ingestion and handling clusters in real time processing using Apache Storm and Kafka.\n\u2022 Experience with Core Distributed computing and Data Mining Library using Apache Spark.\n\u2022 Used Hive to process data and Batch data filtering. Used Spark for any other value centric data filtering.\n\u2022 Worked extensively with Flume for importing data from various webservers to HDFS.\n\u2022 Worked on Large-scale Hadoop YARN cluster for distributed data processing and analysis using Sqoop, Pig, Hive, Impala and NoSQL databases. Develop Hadoop data processes using Hive and/or Impala.\n\u2022 Zookeeper, and Accumulate stack, aiding in the development of specialized indexes for performant queries on big data implementations.\n\u2022 Responsible for building scalable distributed data solutions using Datastax Cassandra.\n\u2022 Those WIFI data through EMS/JMS get stored in Hadoop ecosystem and through Oryx, Spark.\nEnvironment: Hadoop, HDFS, Hive, Pig, HBase, Big Data, Oozie, Sqoop, Zookeeper, MapReduce, Cassandra, Scala, Linux, NoSQL, MySQL Workbench, Java, Eclipse, Oracle 10g, SQL.', u""Software Engineer\nCapital IQ - IN\nFebruary 2011 to November 2013\nDescription:\nCapital IQ is a leading provider of data and analytics for financial professionals. Involved in developing an internal product for Business Information Developers to maintain Listed Company's Stock records. This system collects Stock information from several documents and uploads to a centralized Capital IQ Information Repository and system also does the content analysis to map the incoming content from other systems into a standard XML format.\nResponsibilities:\n\u2022 Created the UI tool - using Java, XML, DHTML, and JavaScript.\n\u2022 Wrote stored procedures using PL/SQL for data retrieval from different tables.\n\u2022 Worked extensively on bug fixes on the server side and made cosmetic changes on the UI side.\n\u2022 Part of performance tuning team and implemented caching mechanism and other changes.\n\u2022 Recreated the system architecture diagram and created numerous new class and sequence diagrams.\n\u2022 Designed and developed UI using HTML, JSP and Struts where users have all the items listed for auctions.\n\u2022 Developed Authentication and Authorization modules where authorized persons can only access the inventory related operations.\n\u2022 Developed Controller Servlets, Action and Form objects for process of interacting with Oracle database and retrieving dynamic data.\n\u2022 Responsible for coding SQL Statements and Stored procedures for back end communication using JDBC.\n\u2022 Developed the Login screen so that only authorized and authenticated administrators can only access the application.\n\u2022 Developed various activities like transaction history, search products that enable users to understand the system efficiently.\n\u2022 Involved in preparing the Documentation of the project to understand the system efficiently.\nEnvironments: JDK1.2, JavaScript, HTML, DHTML, XML, Struts, JSP, Servlet, JNDI, J2EE, Tomcat, Rational Rose, Oracle.""]",[u'Bachelors of Technology in computer science'],[u'JNTU']
0,https://resumes.indeed.com/resume/9339ac8c82fc6eca,"[u'Data Science Engineer Intern\nWit.ai - Arlington, VA\nJune 2017 to September 2017\nPhone2Action Arlington, Virginia\nDesign and implementation of the Facebook Messenger Bot for Phone2Action. Integrated with Wit.ai to handle end-to-end user conversation along with python nltk library for Natural Language Processing tasks.\nDeveloped APIs to handle the different kinds of campaigns using Node.js and Laravel.\nTech Stack: Node.js, Express.js, Heroku, python NLTK library, Wit.ai, Laravel PHP framework, Mongo DB', u""Software Development Intern\nFlipkart India Pvt. Ltd - Bengaluru, Karnataka\nJanuary 2014 to June 2014\nBangalore, India\nAndroid development for Flipkart EBooks app and Slash N app (both available in Google PlayStore today).\nWorked on text to speech converter and the Recommendation System for the eBooks app using Java in back-end.\nTech Stack: Java, Android\n\nProjects\nYelp Recommendation System\nPredicted the categories of the restaurant from reviews and tips data from the Yelp Dataset using\nMultinomial Naive Bayes algorithm. Observed, analyzed and modeled the user behavior to predict the restaurant that user would likely visit using Matrix factorization recommender.\nTech Stack: python, scikit-learn, NLTK library\nFind My Doctor\nDesigned and built a highly scalable, reliable medical appointment scheduling app using Google App\nEngine, JavaScript and the JAX-RS framework.\neKart\nDeveloped an eCommerce website using the Ruby on Rails framework capable of handling large-scale\ntraffic and users on the website.\nFood Image Tag Resolution\nPredicted the tag of an image containing a food item by first predicting it's cuisine using a 2 layered\nattribute-based classification algorithm. Further used the cuisine information to improve the accuracy of the predicted labels. Tech Stack: python, scikit-learn, MATLAB\nParallelization of Prim's Algorithm\nParallelized minimum spanning tree algorithm, making efficient usage of multi-core processors using\nPthreads, openMP and MPI.""]","[u'Master of Information Systems in Information Systems', u'Bachelor of Engineering in Computer Science']","[u'Drexel University Philadelphia, PA\nSeptember 2016 to June 2018', u'P.E.S. Institute of Technology Bengaluru, Karnataka\nAugust 2010 to May 2014']"
0,https://resumes.indeed.com/resume/6908569fdd4bebe8,"[u'Principle Data Systems Engineer / Consultant\nCornerstone Data Systems, INC\nNovember 1994 to Present\nClient/Project:\nFidelity Investments\nEmployer: Cornerstone Data Systems Inc.\nTitle: Principle Data Systems Engineer / Consultant\nDescription: Mr. Peach was directly responsible for the development of a large Data Warehousing System.\n\nMr. Peach leveraged his over 4 years of concentrated, hands-on experience specifying, costing, installing, configuring, maintaining, and resolving issues with various software and hardware including Oracle SuperCluster, Exadata, and DataGuard technology to configure systems in several locations. This included sizing analysis, throughput analysis, and other engineering functions. Mr. Peach provided vision and implementation of improved hardware solution for Fidelity by leveraging the ToGAF life cycle framework which included planning, documenting, developing, installing, configuring, maintaining, supporting, and optimizing all network software and communication links. This required weekly status reports, and periodic design reviews.\n\nMr. Peach ensured the stable operation of the in-house computer software systems and network connections IAW SLAs. This was accomplished by maintaining patch levels, creating network redundancy, and leveraging Oracle Enterprise Manager Cloud Control (OEM), Oracle Virtual Manager (OVMM), and Enterprise Management Operations Center (EMOC) to maintain the systems, as well as Exachk and other maintenance tools.\n\nMr. Peach specified, designed and installed an Oracle ZFS Appliance based storage and backup system \u2013 including the Bill of Material (BOM), and creating the service agreements with Oracle. This involved deploying configuring, and optimizing multiple ZFS Appliances to be used for primary backup of very large (70 TB+) databases for test & development landscapes. Mr. Peach applied advanced technical knowledge of application networking, including load balancing applications, and ensuring fault tolerance by using Clustering, Disk Mirroring, Redundant Switches and Data Paths as well as using Data Guard at Fidelity.\n\nMr. Peach completed Performance Tuning for this deployment to ensure optimal configuration. He implemented and documented recommendations and improvements, while designing and implementing testing scenarios.\n\nIn addition, Mr. Peach designed and produced a functioning OEM 12C Cloud management system to provide: Platform as a Service (PaaS), Database as a Service (DBaaS), Life Cycle Management, and Role Based Access Control. This deployment used ZFS and OEM Snap / Clone technology to build Snap Clone databases. The deployment required a thorough analysis Business Models, hardware and software resources to build Test Masters, create Testing Scenarios, and developing SNAP/Clones that ultimately reduced development times and costs.\n\nAs the Subject Matter Expert, Mr. Peach worked with The CFO, CIO, and Oracle to set up Service Level Agreements; and provided end user training to Fidelity employees, including labs and documentation.\n\n\nClient/Project: Experian\nEmployer: Cornerstone Data Systems Inc.\nTitle: Principle Data Systems Engineer / Consultant\nDescription: In this role, Mr. Peach provided consulting and support to upgrade Oracle Exadata systems.\n\nThis included installation, configuration, general maintenance, upgrades, and new installs of servers and applications of multiple ZFS Storage Appliances. In addition, his role required setup and validation of a functional RMAN-based backup system.\n\n\nClient/Project: ALTESS - Elastic Cloud Control\nEmployer: Cornerstone Data Systems Inc.\nTitle: Principle Data Systems Engineer / Consultant\nDescription: Mr. Peach worked alongside Oracle to build the first Elastic Cloud for a large Enterprise Resource Planning application in the military, specifically for U.S. Army. This system consisted of the virtualized Oracle Exalogic and Exadata appliances. Mr. Peach\u2019s role brought about a unique opportunity that was directly involved with implementing a system that introduced new technologies including the first virtualized Exalogic appliances, a technology that not publicly available until July of 2012. This required the use of a Hypervisor, Enterprise Manager Ops Center (EMOC) Enterprise Manager Cloud Control (12C), Cisco and Infiniband Switches, Exadata Storage Cells, and a ZFS storage appliance. The result was the physical hosts serving up virtual machines and data as-a-service to cloud users. Mr. Peach\u2019s role was responsible for drastically reducing provisioning time required to field new services from three days to seven minutes. Mr. Peach managed this system which acquired data from over 12 separate datacenters which consisted of over 176 VMs and 56 databases. These systems were consolidated (migrated) into the Oracle Exadata based PeopleSoft environment.\n\nIn addition, Mr. Peach coordinated with Oracle to develop the first STIG and hardening procedures for Exa-Machines, IAW AR25-2 for vulnerability management in alignment with CERT tasking, advisories, and operating system Security Technical Implementation Guides (STIGs).\n\nRemedy was integrated into an ITIL service paradigm with which Mr. Peach would analyze and resolve end user software program and connectivity issues at ALTESS. Additionally, Mr. Peach provided design documents and end-user training as part of the life cycle of this system development. The procedures he implemented are still in use today.\n\n\nClient/Project: Joint Forces Command / AFRL - Prototype C4ISR system\nEmployer: Cornerstone Data Systems Inc.\nTitle: Principle Data Systems Engineer / Consultant\nDescription: As a Principle Data Systems Engineer, Mr. Peach provided recommendations on technology direction to align with JFCOM business vision. He developed and demonstrated a Service Oriented ground based command and control system (C4ISR) system. This system integrated data from various disparate sources, at multiple security Levels. It cross-correlated Geo-Spatial, Multimedia, and thematic data from a wide variety of sources (e.g. SatCom and RADAR - Air and ground based). Mr. Peach assembled and led an Interdisciplinary team that integrated Data, Business Rules, Networking, Security, and Hardware requirements.\n\nThis system used SPARC / SOLARIS enterprise class compute nodes, and Oracle DB nodes in a trusted Multi Level Secure (MLS) environment. NetApps storage appliances were used for data.\n\nBusiness processes were expressed with BPEL workflow management, and identity was managed with a Federated Identity Server.\n\n\nClient/Project: Boeing - Battle Management Command and Control\nEmployer: Cornerstone Data Systems Inc.\nTitle: Principle Data Systems Engineer / Consultant / SME\nDescription: Mr. Peach served as the Systems Engineer for an airborne Command and Control (C4ISR) Service Oriented computing architecture for Boeing. Serving as the architect, he worked with the Air force and Sr. Boeing management to provide a business vision for a Multi Level Secure (MLS) Data System designed to go into a Boeing 787 airframe. The system was constructed around Solaris with trusted extensions, and Trusted Oracle, with NetApp storage, and Infiniband provided connections to SatCom, RADAR, and other high-speed data sources.\n\nThe Zachman life cycle framework which included budgeting, planning, developing, installing, configuring, and optimizing all network software and communication links in support of a holistic business model. This included weekly status reports, design reviews, rigorous change control, and various levels of testing.\n\nMr. Peach was a principle engineer and Subject Matter Expert (SME) for the Data Systems part of the proposal, where he provided cost and pricing estimates, solicited and evaluated vendor proposals, and tracked actual costs during the program. He provided guidance to project team, in the areas of mission computing, data governance, network communications, and program management.\n\nIn this effort Mr. Peach was able to apply decades of relevant technical and managerial experience to apply a historical perspective through modern technical advances and best practices.\n\nWhile working on the proposal for the next phase, Mr. Peach\u2019s part of the WBS received Blues. Unfortunately, the project was defunded before the next phase was awarded.']","[u'Master of Science in Computer Engineering', u'Bachelor of Science in Computer Engineering & Computer Science']","[u'Cal State Long Beach\nSeptember 1991 to April 1994', u'Cal State Long Beach\nSeptember 1986 to June 1991']"
0,https://resumes.indeed.com/resume/5f95e9c419539d9c,"[u""Network & Security Engineer - Data Center Services\nTrimax IT Infrastructure and Services Ltd - Mumbai, Maharashtra\nJanuary 2015 to August 2016\nDuties and Responsibilities Handled:\n* Troubleshooting and maintenance of Juniper and Cisco network and firewall devices.\n* Creating policy based route and traffic shaping with route-maps and policy-maps.\n* Creating, modifying, decommissioning vrf's, creating inter-vrf routing in L3 devices.\n* Gateway Load Balancing configuration using HSRP, VRRP, GLBP.\n* Coordinating with ISPs for bandwidth monitoring and blocking malicious IP addresses.\n* Firewall configuration which includes NAT, ACLs and Port Filtering as per customer requirement.\n* Creation of IPsec and SSL VPNs on Juniper and Cisco Firewalls.\n* Network traffic monitoring using applications like Cacti, Netflow Analyzer.\n* Outage analysis and log monitoring for drafting RCA/RFO in case of network outage.\n* Maintaining documentation of network security policies and procedures.\n\nMajor Projects Undertaken:\n* Team lead for Cyberoam UTM device installation project undertaken by the company across 500+ branches of a Major\nNationalized Bank in India by coordinating with site engineers and remotely monitoring the status and configuration of each\ndevice.\n* Member of team which implemented VSS project on all backbone switches in the data center.\n* Member of team responsible for design and implementation of core network and security infrastructure for a public sector\ncompany hosted in the data center which included Routers, Switches, Firewalls, Load Balancers, IPS etc."", u'Network Engineer\nHT Tech Solutions - Mumbai, Maharashtra\nSeptember 2013 to January 2015\nDuties and Responsibilities Handled:\n* New router and switch provisioning with basic configuration and system updates.\n* Troubleshooting network connectivity issues in Routers, Switches and Firewalls.\n* Basic configuration on Cisco ASA (5510) along with ACL rules and NAT.']","[u'MS in Cybersecurity & Privacy', u'Bachelor of Engineering in Electronics & Telecommunication']","[u'New Jersey Institute of Technology Newark, NJ\nDecember 2017', u'University of Mumbai Mumbai, Maharashtra\nMay 2013']"
0,https://resumes.indeed.com/resume/9d7da8ce5c9521f1,"[u""SVP, Sr Tech Manager, Data Engineer/Data Specialist\nBank of America - Agoura Hills, CA\nJune 2011 to Present\nContributed in creation of target state data architecture roadmap, including data governance and data quality\nstandards, with the CDO organization.\n* Responsible for building data analytics platform in the cloud (AWS) for enterprise document management domain in a fast-paced environment using agile/waterfall methodology.\n* Developed relationships with business stakeholders and acted as a liaison to proactively understand their business,\nregulatory and compliance requirements based on priority and present technology solutions in layman's terms.\n* Collaborated with technology partners for the delivery of data analytics/provisioning system from start to finish.\n* Identified key enterprise data sources and built batch/real-time data ingestion framework using latest big data\nprocessing toolset - Kafka, Spark Kinesis, IBM Data Stage, IBM Message Broker.\n* Designed several physical OLTP relational and OLAP dimensional data models for ODS/DataMart/Data Warehouses.\n* Built machine learning/statistical models (regression, classification, clustering) using Python libraries and present\nfindings to drive strategic decisions using reports and slides to executive management.\n* Reviewed data ingestion and retrieval SQL scripts for performance optimization and tuning.\n* Provisioned data to multiple data consumers for operational reporting, real-time dashboard, web services, and ETL\nbatch extracts\n* Created an enterprise wide end-2-end monthly statement and letter reconciliation framework to help identify\nmissing statements/letters using data originated from each touch point, which is then ingested into a data store for data analysis and monitoring.\n* As part of the digitization strategy for mortgage applications, implemented a DocuSign eSignature backend database\nin Oracle, leading to 15% reduction in paper signing ceremony in 12 months.\n* Responsible for recruiting, training a team of engineers, analyst and QA resources, and their allocation/utilization.\n* Closely work with service delivery lead, scrum masters, product owner, and process owners to prioritize projects,\nfeatures and stories.\n* Worked on import/export scripts to migrate data from Oracle to AWS Aurora."", u'VP, Data Engineer/Data Analyst\nBank of America - Agoura Hills, CA\nMarch 2009 to June 2011\n* Worked with business stakeholders to understand metrics regarding customer complaints across multiple lines of business.\n* Implemented DataMart and reporting solution for a complaint tracking system using Oracle DB, OBIEE, and SSIS.\n* Analyzed and processed complex data sets using advance querying, visualization and analytic tools.\n* Built an ETL nightly batch process using IBM data stage to provision mortgage customer data to the enterprise\ncustomer repository.\n* Built a customer matching logic to consolidate customer data from bank, credit cards, mortgage business units.\n* Collaborated to create a web service platform to present a consolidated 360-degree view of the customer.\n* Supervised a team of 6 Systems/Data Analyst, and responsible for their allocation, utilization and performance.', u'VP, Systems Analyst/Data Analyst\nCountrywide Financial - Agoura Hills, CA\nOctober 2002 to March 2009\nServed as a lead Systems Analyst for implementing Single Sign-on and Strong Mutual Authentication capability\nacross multiple websites (Bank, Insurance, Home Loans and Portal) using Entrust ePass technology.\n* Led many initiatives and projects for my.countrywide.com such as Search module, SSO integration, 360-degree\nAccounts overview page redesign, user security profile pages.\n* Created a customer matching logic using customer profile data to consolidate customer information from bank,\ninsurance and mortgage business units.\n* Created a customer house-holding logic to come up a house-holding key across all customers.', u'Data Engineer/Systems Analyst\nSprint PCS - Overland Park, KS\nJune 2001 to September 2002\nResponsible working with the business stakeholders to understand the business needs and requirements for a CRM\nsystem.\n* Responsible for loading 14 million national accounts in an Oracle database hosted on a Unix platform using SQL\nLoader scripts.\n* Created design document and traceability matrix to match the requirements document to fulfill all business\nrequirements.\n* Created Perl and Korn Shell script to combine 600 different datafiles into one datafile.\n* Involved with capacity planning, infrastructure and architecture setup.\n* Involved in the loading process that included Pre-Processing and scrubbing of data Accounts data, Employee data\nincluding Positions, Divisions, Titles, Contacts data and Leads data into an Oracle backend.\n* Stored all the Industry SIC Codes from Dun & Bradstreet to display the line of Business for all the accounts.\n* Used Unix Korn shell to consolidate all the SQL scripts and automating the loading and updating of accounts data\ninto EIM tables\n* Provided support in loading the Assignment Manager Tables to insert assignment rules and their values using PL/SQL']",[u'Master of Science in Computer Science in Computer Science'],"[u'University of Missouri Kansas City Kansas City, MO\nJanuary 2001']"
0,https://resumes.indeed.com/resume/e6900008588e5300,"[u'Software Engineer Intern (Data)\nStrikedeck - Sunnyvale, CA\nDecember 2017 to Present\n\u2022 Gathering customer requirements in the business calls and preparing requirement documents.\n\u2022 Working on customer related data sources as AWS S3 files, Salesforce, Redshift table and building data\npipelines using Talend ETL tool, Python and SQL scripts to populate the target tables in warehouse in Redshift database.\n\u2022 Analyzing the data using Python Pandas and NumPy, reconciling and testing with the source data.\nCreated visually impactful dashboards in Excel and Tableau for data reporting by using Pivot tables and VLOOKUP. Extracted, interpreted and analyzed data to identify key metrics and transform raw data into\nmeaningful, actionable information.', u'Assistant Software Engineer (Business Intelligence)\nTata Consultancy Services - Bengaluru, Karnataka\nDecember 2013 to July 2016\nIndia (client Thomson Reuters)\n\u2022 Interacted with the customers to analyze and understand the requirements on various financial products.\n\u2022 Designed and built ETL pipelines using SQL/PL SQL to populate data models in EDW for business analysis and reporting.\n\u2022 Generated various reports such as Daily/Monthly/Quarterly metrics of Product details, Order Details, Order\nBilling details, Customer Details, Account Details.\n\u2022 Built and tested Tableau reports accessing Oracle database and SQL server database.']",[u'Master of Science in Software Engineering in Software Engineering'],"[u'International Technological University San Jose, CA\nJanuary 2017 to April 2018']"
0,https://resumes.indeed.com/resume/19f684a1b52ea0ba,"[u'Data Science Engineer\nLIMRA - UCONN - Hartford, CT\nJanuary 2017 to May 2017\nHelped LIMRA to identify and analyze potential surrenders/churn in variable annuity policies for top financial institutions in the US using\nR, SQL, SAS Enterprise Miner and SAS JMP\n\u2022 Sampling, Data Extraction & Transformations, Feature Selection, Dimensionality reduction were popular data wrangling tasks performed\non 2.3 million policies and 94 attributes as a part of data preparation\n\u2022 Trained and Validated various statistical models like logistic regression, decision trees, and random forests in SAS Enterprise Miner\n\u2022 Finalized a rational predictive model using random forests (with 87% accuracy) to provide a plausible explanation of annuity surrenders\nand recommended insights with Tableau dashboards to reduce the frequency of surrenders', u'Data Engineer\nBiCiCo (NGO) - Hartford, CT\nFebruary 2016 to August 2016\n\u2022 Designed Process Flow and Implemented Database Systems for Membership Enrolment in MySQL\n\u2022 Automated the tracking of membership enrollment using Python scripts, and ETL jobs\n\u2022 Generating Automated reports on periodic basis for management to monitor member enrolment traffic and membership renewals', u'Business Intelligence Engineer II\nVirtusa Consulting Services - Bangalore, Karnataka\nSeptember 2013 to December 2015\nConsulted with Solution Designers, Business owners, and stakeholders to capture accurate business requirements and design solutions\nunder agile SDLC framework for a 40TB DW/BI project with one of the telecom giants\n\u2022 Led and trained on-shore team of 18 on the concepts of Enterprise Data Warehouse and eTOM (Business Process Framework)\n\u2022 Created OWB maps for ETL framework to build Enterprise Data Warehouse from multiple source systems including HDFS and deployed\nPL/SQL procedures and packages to populate data marts\n\u2022 Performed data analysis in SQL and built ad hoc KPI reports from data marts using OBIEE and Tableau for business analysis\n\u2022 Brought down project cost by 26% by Implementing Development and Operations (DevOps) process\n\u2022 Diminished proactive incidents by over 60% in the span of two months by automating the ETL BAU, Cron jobs using Unix scripts']",[u'in Data Management'],"[u'University of Connecticut School of Business Hartford, CT\nMay 2017']"
0,https://resumes.indeed.com/resume/e691fb75b99a7d4c,"[u'Data integration developer\nThe Childrens Place - Secaucus, NJ\nDecember 2016 to Present', u""Data Specialist\nIBM India - Middletown, NJ\nDecember 2010 to December 2016\nCore Technologies: Datastage 7.1, 7.5, 8.7; Cognos 10; SPSS; IBM BigInsights; Oracle 9i, 10g, 11g; UNIX; Sun OS; Linux Red Hat; Shell Scripting; Perl; Pro C.\nFocus on data warehousing within telecommunications, working with largest US telecom provider handling multiple projects to gather data from large data clusters. Work extensively in data warehousing, data integration, data migration projects and analytics projects. Coordinate development activities with offshore teams to best meet critical deadlines under intense conditions. Strive to ensure positive and productive environments through openness, flexibility, positive attitude, prompt reporting, and timely issue escalation.\nDesigned and developed multiple OSS, DSS OLAP's, and data integration projects with extensive usage of Datastage, Oracle, and shell scripts. Worked closely with senior architects to design and model multiple data marts, and BI systems.\nSynchronize with other interfaces/teams on defect resolution; support offshore/onsite teams by undertaking project responsibilities and prioritizing work requirements. Collaborate with other application managers, ETE PM, and test managers to resolve issues; provide potential issue solutions for UAT/pre-deployment concerns.\nPlan projects including milestones such as HLD Baseline, development, quality reviews, and ST/JST/IST/UAT support. Oversee the quality aspects of deliverables; maintain responsibility for performance tuning, SQL query enhancements, and code augmentations to improve performance statistics.\nOrient, mentor, and train new team members. Define roles and responsibilities for developers, leads, and Arch Team members to ensure optimum job performance. Counsel and train poor performers to assist in improving performance standards.\nKey Achievements:\n* Received the Best of IBM Award for 2010; awarded the Excellence and Eminence recognition for 2011; granted the Top Contributor Award for 2012.\n* Helped to create the BAO Accelerator, an integration concept for project tracking using the ETL tool, Datastage. This resulted in the achievement of code automation during multiple phases in the project.\n* Created a Datastage Code Review tool built using shell scripts for code reviews. This helped reduce the number of issues arising during the project life cycle, and to sequence-map job names. This proposal was conceptualized to enhance best practices in Datastage, and eventually deployed across all business groups.\n* Expertise in migrating the ETL servers /Jobs and scripts from Unix based servers to Linux based servers by performing reverse engineering of existing code. Participated in the end to End Architecture discussions.\n* Worked on a transformation project which included in migrating more than 40 TB data into new data center, this project included migration of Data stage jobs from server to parallel, 7.x /8.x jobs to 11.3, along with the movement of Oracle data bases to Oracle 12 c Rac environment\n* Organize funding, schedules, and project delivery with the assistant project manager; create quality review teams in ETL to guarantee quality code delivery. Play an integral part in competency audits, DRI, and other quality improvement programs.\n* Lead the team in weekly technology and domain skills training.\n* Participate with developers on review calls to support them in identifying gaps in requirements and installations when required. Continually review work produced by the team to ensure quality, and that all objectives have been met.\n* Designed a data lineage tool from ETL jobs to capture lifecycle data including data origins and tracking, thereby assisting in analyzing how information is used, and maintaining key information that would otherwise have been lost. This tool was created using Metadata Work Bench, XMetal, and Datastage.\n* Created a sentiment analysis for studying the positive and negative sentiments of a product with the usage of SPSS. Proposed design was implemented with by using web crawling methodologies of SPSS\n* Excellent Experience in Designing, Developing, Documenting, and Testing of ETL jobs in Server and Parallel jobs using Data Stage to populate tables in BI/DW systems. Proficient in developing strategies for Extraction, Transformation, and Loading (ETL) mechanism.\n* Expert in designing Parallel jobs using various stages such as join, merge, lookup, remove duplicates, filter, dataset, lookup file set, complex flat file, modify, aggregator, XML and unstructured data processing. Expert in designing Server jobs using various types of stages like Sequential file, ODBC, Hashed file, Aggregator, Transformer, Sort, Link Partitioner and Link Collector.\n* Experienced in the integration of various data sources (DB2-UDB, Oracle, Teradata, XML ) into data staging areas. Expert in working with Data Stage Manager, Designer, Administrator, and Director, and in analyzing the data generated by the business process, defining the granularity, source to target mapping of the data elements, creating Indexes and Aggregate tables for the data warehouse design, and development.\n* Excellent knowledge of studying the data dependencies using metadata stored in the repository and preparing batches for the existing sessions to facilitate scheduling of multiple sessions. Proven track record in troubleshooting Datastage jobs and addressing production issues like performance tuning and enhancement.\n* Expertise in UNIX shell scripts using K-shell for the automation of processes and scheduling the Datastage jobs using wrappers. levels understanding and translating Functional Requirements Documents to Detailed Design Documents.\n* Thorough understanding of ITUP (Information Technology Unified Process) methodology and usage of client specific repositories for knowledge management. Implemented the mapping logics and designed data flows."", u'Software Engineer\nInfinite Computer Solutions - Bangalore, Karnataka\nJuly 2009 to December 2010\nCore Technologies: Infosphere Datastage 7.x, 8.x, and 11.3; Infosphere Fast Track Reporting; Oracle 9i, and 11g databases; SPSS; SunOS UNIX; Red Hat Linux; Shell Scripting; Perl; Python\nInteracted daily with clients and organized multiple teams including system testing, DBA, Enterprise Data Bus Team, and other cross-application teams. Directed teams in the resolution of technical issues by coordinating with other interfacing applications when required. Assembled existing process and flows and enhanced them by incorporating new requirements into existing ones, leaving the process itself, undisturbed.\nDesigned, developed, tested, and documented each step of the process, and wrote documents and unit test cases for critical projects. Coordinated daily with clients to analyze work impacts on the design, development, and support function from testing until deployment.']",[u'Bachelor of Engineering in Computer Science and Engineering'],[u'Tamil Nadu College of Engineering']
0,https://resumes.indeed.com/resume/4e05f605beb88ae1,[u'Database Engineer & Data analyst\nTelecommunication Company of Esfahan\nAugust 1999 to May 2016\nTCE)'],[u'B.Sc. in Computer Engineering'],[u'Isfahan University of Technology\nJanuary 1998']
0,https://resumes.indeed.com/resume/2252c39a9c12c0cd,"[u""Data Analyst\nSYNTEL LTD\nSeptember 2016 to August 2017\n* Functioned with customer to implement data analysis, document deficiencies in data, and generate mapping, and validation reports on client data while using Tableau and SQL.\n* Administered data architecture and modeling activities for data warehouses; analyzed BI data for integration into enterprise conceptual model, formulated new tables and integrated data into existing tables\n* Applied MS-Excel to analyze data and perform daily inventory tracking, RGY status. Gathered requirements to document existing business process and enhancement requests for business system.\n* Based on the daily tracking and RGY's created dashboards in Tableau and Trend analysis using data analysis tool\npack in Excel."", u'Software Engineer\nSYNTEL LTD\nSeptember 2015 to August 2016\n* Played a key role in monitoring system, analyzing trends, and crating, improving, and maintaining support activities of Mainframe systems\n* Eliminated the printing discrepancies by testing of reports on HP Extream Dialogue application to enhance the performance of batch processes.']","[u'MASTER OF SCIENCE in ANALYTICS', u'BACHELOR OF ENGINEERING in ELECTRONICS & TELECOMMUNICATIONS']","[u'Northeastern University Boston, MA\nApril 2019', u'University of Pune, RMD Sinhgad School of Engineering Pune, Maharashtra\nJanuary 2015']"
0,https://resumes.indeed.com/resume/9964806e8f30fa94,"[u'Sr. Data Analyst/Engineer\nAllstate Insurance - Chicago, IL\nJanuary 2017 to Present\n\u2022 Worked with Business Analyst to understand the user requirements, layout, and look of the interactive dashboard to be developed in tableau.\n\u2022 Involved in Manipulating, cleansing & processing data using Excel, Access and SQL and responsible for loading, extracting and validation of client data.\n\u2022 Used Python programs for data manipulation, automation process of generating reports of multiple data sources or dashboards\n\u2022 Knowledge in design and implementation of the Data Warehouse life cycle and entity- relationship/multidimensional modeling star schema, snowflake schema\n\u2022 Involved extensively in creating Tableau Extracts, Tableau Worksheet, Actions, Tableau\nFunctions, Tableau Connectors (Live and Extract) including drill down and drill up capabilities and Dashboard color coding, formatting and report operations (sorting, filtering, Top-N\nAnalysis, hierarchies).\n\u2022 Data blending of patient information from different sources and for research using Tableau and Python.\n\u2022 Used Boto3 to integrate Python application with various databased AWS Redshift, Teradata and S3.\n\u2022 Utilized various Python frameworks and libraries Pandas, Numpy and spicy for analyzing data\nfrom data sources AWS Redshift and Teradata and data manipulation\n\u2022 Developed Python programs and batch scripts on various environments windows, Linux and Unix for automation of ETL processes to AWS Redshift\n\u2022 Managing the Metadata associated with the ETL processes used to populate the Data\nWarehouse.\n\u2022 Created sheet selector to accommodate multiple chart types (Pie, Bar, Line etc) in a single\ndashboard by using parameters.\n\u2022 Working on generating various dashboards in Tableau Server using different data sources such as Teradata, Oracle, Microsoft SQL Server and Microsoft Analysis Services.\n\u2022 Published Workbooks by creating user filters so that only appropriate teams can view it.\n\u2022 Worked on SAS Visual Analytics & SAS Web Report Studio for data presentation and reporting.\n\u2022 Extensively used SAS/Macros to parameterize the reports so that the user could choose the summary and sub-setting variables to be used from the web application.\n\u2022 Created Teradata External loader connections such as MLoad, Upsert, Update, and Fastload\nwhile loading data into the target tables in Teradata Database.\n\u2022 Resolved the data related issues such as: assessing data quality, testing dashboards, evaluating\nexisting data sources.\n\u2022 Created DDL scripts for implementing Data Modeling changes, reviewed SQL queries and involved in Database Design and implementing RDBMS specific features.\n\u2022 Expertise in implementing Data modeling, Erwin, Dimensional Modeling, Datamarts, OLAP,\nFACT & Dimensions tables, Physical & Logical data modeling and Oracle Designer.\n\u2022 Created data mapping documents mapping Logical Data Elements to Physical Data Elements\nand Source Data Elements to Destination Data Elements.\n\u2022 Involved in all phases of SDLC and participated in daily scrum meetings with cross teams\n\nEnvironment: Tableau Server 9.3, Tableau Desktop 9.3, Matillion, AWS Redshift, Teradata,\nPython, Oracle 11g, SQL, PostgreSQL, Linux, Teradata SQL Assistant, EC2, S3, Microsoft SQL\nServer, Windows, Unix', u'Data Engineer\nScotia Bank - Toronto, ON\nDecember 2014 to December 2016\nResponsibilities:\n\u2022 Worked with the analysis teams and management teams and supported them based on their\nrequirements.\n\u2022 Involved in extraction, transformation and loading of data directly from different source\nsystems (flat files/Excel/Oracle/SQL/Teradata) using SAS/SQL, SAS/macros.\n\u2022 Generated PL/SQL scripts for data manipulation, validation and materialized views for remote\ninstances.\n\u2022 Created and modified several database objects such as Tables, Views, Indexes, Constraints,\nStored procedures, Packages, Functions and Triggers using SQL and PL/SQL.\n\u2022 Created large datasets by combining individual datasets using various inner and outer joins in SAS/SQL and dataset sorting and merging techniques using SAS/Base.\n\u2022 Developed live reports in a drill down mode to facilitate usability and enhance user interaction\n\u2022 Extensively worked on Shell scripts for running SAS programs in batch mode on UNIX.\n\u2022 Wrote Python scripts to parse XML documents and load the data in database.\n\u2022 Used Python to extract weekly information from XML files.\n\u2022 Developed Python scripts to clean the raw data\n\u2022 Used Boto3 to integrate Python applications with Amazon web services\n\u2022 worked on AWS CLI to aggregate clean files in Amazon S3 and also on Amazon EC2 Clusters to deploy files into Buckets.\n\u2022 Used AWS CLI with IAM roles to load data to Redshift cluster\n\u2022 Validated regulatory finance data and created automated adjustments using advanced SAS\nMacros, PROC SQL, UNIX (Korn Shell) and various reporting procedures.\n\u2022 Involved in generating dual-axis bar chart, Pie chart and Bubble chart with multiple measures\nand data blending in case of merging different sources.\n\u2022 Developed dashboards in Tableau Desktop and published them on to Tableau Server which allowed end users to understand the data on the fly with the usage of quick filters for on demand\nneeded information.\n\u2022 Created Dashboards style of reports using Qlikview components like Listbox Slider, Buttons,\nCharts and Bookmarks.\n\nEnvironment: Windows 8, UNIX, SAS v9.4, SAS/EG, Python, Boto3, Netbeans, SAS/Macro,\nSAS/SQL, SAS/Graph, MS SQL Server, Oracle, Teradata, MS-Excel, AWS Redshift, AWS S3, AWS\nEC2 ,Tableau 8.2, Qlikview 9.0 SR2.', u'Data Analyst\nStandard Chartered Scope International - Chennai, Tamil Nadu\nAugust 2011 to February 2013\nUtilized SQL to develop stored procedures, views to create result sets to meet varying reporting\nrequirements.\n\u2022 Used Visual Studio report builder to design report of varying complexity and maintain system\ndesign documents.\n\u2022 Involved in Data analysis, reporting using Tableau and SSRS.\n\u2022 Built and supported the transformation of various data inputs into the companies Data\nWarehouse.\n\u2022 Analyzed and designed solutions, created technical integration and interacted with QA team on their tasks related to the imports and extracts.\n\u2022 Written SQL Scripts and PL/SQL Scripts to extract data from Database to meet business\nrequirements and for Testing Purposes.\n\u2022 Field mapping work involved establishing relationships between the databases Tables, filter\ncriteria, formulas etc., needed for the reports.\n\u2022 Involved in performance tuning of queries by working intensively over indexes.\n\u2022 Processed data received from vendors and loading them into database on a weekly basis. The\nextracted data had to be checked for integrity.\n\u2022 Extracted data from different flat files, MS Excel, MS Access and transformed the data based\non user requirement using Informatica Power Center and loaded data into target, by scheduling the sessions.\n\u2022 Created different source definitions to extract data from flat files and relational tables for Data\nmart.\n\u2022 Documented data cleansing and data profiling and tested cleansed data for integrity and uniqueness.\n\u2022 Designing the ETL process using Informatica to populate the Data Mart using the flat files to Oracle database\n\u2022 Designed and developed Insurance, Claims and Financial Reports Using SSRS.\n\u2022 Coordinated between the Business users and development team in resolving issues.\n\u2022 Performed testing and QA role: Developed Test Plan, Test Scenarios and wrote SQL plus Test\nScripts for execution on converted data to ensure correct ETL data transformations and controls.\n\nEnvironment: Flat files, MS Excel Files, MS Access, SSRS, Oracle 9i/10g, Power Designer, MS SQL\nServer 2005/2000, PL/SQL, Teradata V2R5, Tableau, Perl,\nUNIX scripting, Windows NT, SQL, Business Objects']","[u'Masters in Information Systems in Information Systems', u'']","[u'Ryerson University', u'JNTU Hyderabad, Telangana']"
0,https://resumes.indeed.com/resume/2b7b1fd8dc10c3d6,"[u""Machine Learning Engineer\nTeradata - Raleigh, NC\nJuly 2017 to Present\nDescription: Teradata is the world's leading provider of business analytics solutions, data and analytics solutions, and hybrid cloud products and services.\n\nResponsibilities:\n\u2022 Implement machine learning algorithms for document recommendation in enterprise taking advantage of several data sources available in enterprise.\n\u2022 Used R, SQL to create Statistical algorithms involving Multivariate Regression, Linear Regression, Logistic Regression, PCA, Random forest models, Decision trees, Support Vector Machine for estimating the risks.\n\u2022 Created a recommendation system using k-means clustering, NLP and Flask to generate list for potential users and worked on NLP algorithm consists of TF-IDF and LSI on the user reviews.\n\u2022 Developed OLAP cubes for the branding analysis and developed OLAP and Excel Reports for SEC reporting.\n\u2022 Managed data operations team and collaborated with data warehouse developers to meet business user needs, promote data security, and maintain data integrity.\n\u2022 Data Collection, Features creation, Model Building (Linear Regression, SVM, Logistic Regression, Decision Tree, Random Forest, GBM), Evaluation Metrics, Model Serving - R, Scikit-learn, Spark SQL, Spark ML, Flask, Redshift, AWS S3\n\u2022 Experience on Cassandra node tool to manage Cassandra cluster.\n\u2022 Involved in the process of designing Cassandra Architecture.\n\u2022 Installed, Configured, Tested Datastax Enterprise Cassandra multi-node cluster which has 4 Datacenters and 5 nodes each.\n\u2022 Worked on Real Time as well as Batch Data and have build lambda architecture to process the data using Kafka, Spark Streaming, Spark Core and Spark SQL\n\u2022 Designed and provisioned the platform architecture to execute Hadoop and machine learning use cases under Cloud infrastructure, AWS, EMR, and S3.\n\u2022 Involved in creating Data Lake by extracting customer's Big Data from various data sources into Hadoop HDFS. This included data from Excel, Flat Files, Oracle, SQL Server, MongoDb, Cassandra, HBase, Teradata, Netezza and also log data from servers\n\u2022 Developed Python code for data analysis (also using NumPy and SciPy), Curve-fitting.\n\u2022 Performed extensive Data Validation, Data Verification against Data Warehouse and performed debugging of the SQL-Statements and stored procedures for business scenarios.\n\u2022 Used Spark Data frames, Spark-SQL, Spark MLLib extensively and developing and designing POC's using Scala, Spark SQL and MLlib libraries.\n\u2022 Created and reviewed Informatica mapping documents too with business and data governance rules.\n\u2022 Created a recommendation system using k-means clustering, NLP and Flask to generate vehicles list for potential users and worked on NLP algorithm consists of TF-IDF and LSI on the user reviews.\n\u2022 Worked on predictive and what-if analysis using R from HDFS and successfully loaded files to HDFS from Teradata, and loaded from HDFS to HIVE.\n\u2022 Designed the schema, configured and deployed AWS Redshift for optimal storage and fast retrieval of data.\n\u2022 Developed ETL mappings, testing, correction and enhancement and resolved data integrity issues and coordinated multiple OLAP and ETL projects for various data lineage and reconciliation.\n\u2022 Analyzed data and predicted end customer behaviors and product performance by applying machine learning algorithms using Spark MLlib.\n\u2022 Involved in moving the complete website infrastructure to Amazon Web Service\n\u2022 Prediction of Function and Industry based on Job Text Analytics using R, Scikit-learn, NLTK, TF-IDF, Bayesian Classifier and Gensim\n\u2022 Performed transformations of data using Spark and Hive according to business requirements for generating various analytical datasets.\n\u2022 Design, Develop ETL process and create UNIX shell scripts to execute Teradata SQL, BTEQ, jobs.\n\u2022 Analyzed the bug reports in BO reports by running similar SQL queries against the source system (s) to perform root-cause analysis.\n\u2022 NLTK, Stanford NLP, RAKE to preprocess the data, entity extraction and keyword extraction.\n\u2022 Used concepts of Data Modeling Star Schema/Snowflake modeling, FACT & Dimensions tables and Logical & Physical data modeling.\n\u2022 Translated cell formulas for business users in Excel into VBA code to design, analyze, and deploy programs for their ad-hoc needs.\n\u2022 Created dimension and fact tables in Redshift, ETL to get data from different sources and insert into Redshift, Tableau for reporting using Redshift as data source\n\u2022 Coding using Teradata Analytical functions, BTEQ SQL of TERADATA, write UNIX scripts to validate, format and execute the SQLs on UNIX environment.\n\u2022 Worked on analyzing the data statistically and also prepared statistical reports SAS tool.\n\u2022 Created mapreduce running over HDFS for data mining and analysis using R and Loading & Storage data to Pig Script and R for MapReduce operations.\n\u2022 Created various types of data visualizations using R, and Tableau.\n\u2022 Created numerous dashboards in tableau desktop based on the data collected from zonal and compass, while blending data from MS-excel and CSV files, with MS SQL server databases.\n\u2022 Developed SPSS Macro, which reduced time of programming syntax and increased the productivity for whole data processing steps.\n\u2022 Participated in big data architecture for both batch and real-time analytics and mapped data using scoring system over large data on HDFS\n\nEnvironment:Horton works - Hadoop Map Reduce, Pyspark, Spark, R, Spark MLLib, Tableau, Informatica, SQL, Excel, VBA, BO, CSV, Erwin, SAS, AWS Redshift, Scala Nlp, Cassandra, Oracle, MongoDB, Cognos,SQL Server 2012, Teradata, DB2, SPSS, T-SQL, PL/SQL, Flat Files, XML, and Tableau."", u""Data Scientist\nEquinox Fitness - New York, NY\nApril 2016 to June 2017\nDescription:Equinox Fitness, an American luxury fitness company, operates a separate fitness brands like Equinox, Pure Yoga, Blink fitness and SoulCycle.\n\nResponsibilities:\n\u2022 Utilized ApacheSpark with Python to develop and execute Big Data Analytics and Machine learning applications, executed machine Learning use cases under Spark ML and Mllib.\n\u2022 Solutions architect for transforming business problems into BigData and DataScience solutions and define Big Data strategy and Roap map.\n\u2022 Identified areas of improvement in existing business by unearthing insights by analyzing vast amount of data using machine learning techniques. TensorFlow, Scala, Spark, MLLib, Python and other tools and languages needed.\n\u2022 Create and validate machine learning models with AzureMachineLearning\n\u2022 Designing a machine learning pipeline using MicrosoftAzureMachineLearning to predict and prescribe and Implemented a machine learning scenario for a given data problem\n\u2022 Used Scala for coding the components in Play and Akka.\n\u2022 Worked on different Machine learning models like Logistic Regression, Multilayer perceptron classifier, K-means clustering by creating Scala-SBT packaging and run it in Spark-shell (Scala) and Auto-encoder model with using R programming.\n\u2022 Worked on setting up and configuring AWS's EMR Clusters and Used Amazon IAM to grant fine-grained access to AWS resources to users\n\u2022 Created detailed AWS Security Groups, which behaved as virtual firewalls that controlled the traffic allowed to reach one or more AWSEC2 instances\n\u2022 Wrote scripts and indexing strategy for a migration to Redshift from Postgres9.2 and MySQL databases.\n\u2022 Wrote Kinesis agents to pipe data from streaming app into S3.\n\u2022 Good Knowledge in Azure clouds ervices, Azure storage, Azure active directory, Azure Service Bus. Create and manage AzureAD tenants, and configure application integration with AzureAD. Integrate on-premises WindowsAD with AzureAD Integrating on-premises identity with Azure Active Directory.\n\u2022 Working knowledge of Azure Fabric, Microservices, IoT&Docker containers in Azure. Azure infrastructure management & PaaS Solution Architect - (Azure AD, Licenses, Office365, DR on cloud using AzureRecoveryVault, AzureWebRoles, WorkerRoles, SQLAzure, AzureStorage).\n\u2022 Utilized Spark, Scala, Hadoop, HBase, Cassandra, MongoDB, Kafka, Spark Streaming, MLLib, Python, a broad variety of machine learning methods including classifications, regressions, dimensionally reduction etc. and Utilized the engine to increase user lifetime by 45% and triple user conversations for target categories.\n\u2022 Designed and developed NLP models for sentiment analysis.\n\u2022 Led discussions with users to gather business processes requirements and data requirements to develop a variety of Conceptual, Logical and Physical Data Models. Expert in Business Intelligence and Data Visualization tools: Tableau, Microstrategy.\n\u2022 Performed Multinomial Logistic Regression, Random forest, Decision Tree, SVM to classify package is going to deliver on time for the new route and Performed data analysis by using Hive to retrieve the data from Hadoop cluster, Sql to retrieve data from Oracle database.\n\u2022 Worked on machine learning on large size data using Spark and MapReduce.\n\u2022 Let the implementation of new statistical algorithms and operators on Hadoop and SQL platforms and utilized optimizations techniques, linear regressions, K-means clustering, Native Bayes and other approaches.\n\u2022 Developed Spark/Scala,Python for regular expression (regex) project in the Hadoop/Hive environment with Linux/Windows for big data resources.\n\u2022 Developed Data Mapping, Data Governance, Transformation and Cleansing rules for the Master Data Management Architecture involving OLTP, ODS and OLAP.\n\u2022 Data sources are extracted, transformed and loaded to generate CSV data files with Python programming and SQL queries.\n\u2022 Stored and retrieved data from data-warehouses using Amazon Redshift.\n\u2022 Worked on TeradataSQL queries, Teradata Indexes, Utilities such as Mload, Tpump, Fast load and FastExport.\n\u2022 Application of various machine learning algorithms and statistical modeling like decision trees, regression models, neural networks, SVM, clustering to identify Volume using scikit-learn package in python, Matlab.\n\u2022 Used Data Warehousing Concepts like Ralph Kimball Methodology, Bill Inmon Methodology, OLAP, OLTP, Star Schema, Snow Flake Schema, Fact Table and Dimension Table.\n\u2022 Refined time-series data and validated mathematical models using analytical tools like R and SPSS to reduce forecasting errors.\n\u2022 Worked on data pre-processing and cleaning the data to perform feature engineering and performed data imputation techniques for the missing values in the dataset using Python.\n\u2022 Created Data Quality Scripts using SQL and Hive to validate successful data load and quality of the data. Created various types of data visualizations using Python and Tableau.\n\nEnvironment: Python, Azure, ER Studio, Hadoop, Map Reduce, EC2, S3, Pyspark, Spark, Spark MLLib, Tableau, Informatica, SQL, Excel, VBA, BO, CSV, Netezza, SAS, Matlab, AWS, Scala Nlp, SPSS, Cassandra, Oracle, Amazon Redshift, MongoDB, SQL Server 2012, Teradata, DB2, T-SQL, PL/SQL, Flat Files, XML, Tableau."", u""Data Scientist/Data Architect\nFarm Bureaus - Columbus, OH\nDecember 2014 to March 2016\nDescription:Over the last 85 years, Nationwide has grown from a small mutual auto insurer owned by Policyholders to one of the largest insurance and financial services companies in the world. Early growth came\nfrom working together with Farm Bureaus that sponsored the company. Nine Farm Bureaus continue to promote Nationwide.\n\nResponsibilities:\n\u2022 Used R, SQL to create Statistical algorithms involving Multivariate Regression, Linear Regression, Logistic Regression, PCA, Random forest models, Decision trees, Support Vector Machine for estimating the risks.\n\u2022 Managed data operations team and collaborated with data warehouse developers to meet business user needs, promote data security, and maintain data integrity.\n\u2022 Used R and python for Exploratory DataAnalysis, A/Btesting, Anova test and Hypothesis test to compare and identify the effectiveness of Creative Campaigns.\n\u2022 Designed and provisioned the platform architecture to execute Hadoop and machine learning use cases under Cloud infrastructure, AWS, EMR, and S3.\n\u2022 Involved in designing and deploying a multiple applications utilizing almost all of the AWS stack (Including EC2, Route53, S3, RDS, DynamoDB, SNS, SQS, IAM) focusing on high-availability, fault tolerance, and auto-scaling in AWSCloudformation.\n\u2022 Experience in Performance Tuning and Query Optimization in AWSRedshift and ConfiguredAWSIdentityAccessManagement (IAM) Group and users for improved login authentication.\n\u2022 Used ETL Tools for masking and cleaning data andmined data from various sources.\n\u2022 Involved in creating Data Lake by extracting customer's Big Data from various data sources into Hadoop HDFS. This included data from Excel, Flat Files, Oracle, SQL Server, MongoDb, Cassandra, HBase, Teradata, Netezza and also log data from servers\n\u2022 Developed Python code for data analysis (also using NumPy and SciPy), Curve-fitting.\n\u2022 Performed extensive Data Validation, Data Verification against Data Warehouse and performed debugging of the SQL-Statements and stored procedures for business scenarios.\n\u2022 Used Spark Data frames, Spark-SQL, Spark MLLib extensively and developing and designing POC's using Scala, Spark SQL and MLlib libraries.\n\u2022 Worked on predictive and what-if analysis using R from HDFS and successfully loaded files to HDFS from Teradata, and loaded from HDFS to HIVE.\n\u2022 Developed ETL mappings, testing, correction and enhancement and resolved data integrity issues and coordinated multiple OLAP and ETL projects for various data lineage and reconciliation.\n\u2022 Optimized and tuning the Redshift environment, enabling queries to perform up to 100 xs faster for Tableau and SASVisualAnalytics.\n\u2022 Design, Develop ETL process and create UNIX shell scripts to execute Teradata SQL, BTEQ, jobs.\n\u2022 Analyzed the bug reports in BO reports by running similar SQL queries against the source system (s) to perform root-cause analysis.\n\u2022 NLTK, Stanford NLP, RAKE to preprocess the data, entity extraction and keyword extraction.\n\u2022 Used concepts of Data Modeling Star Schema/Snowflake modeling, FACT & Dimensions tables and Logical & Physical data modeling.\n\u2022 Translated cell formulas for business users in Excel into VBA code to design, analyze, and deploy programs for their ad-hoc needs.\n\u2022 Coding using Teradata Analytical functions, BTEQ SQL of TERADATA, write UNIX scripts to validate, format and execute the SQLs on UNIX environment.\n\u2022 Created MapReduce running over HDFS for data mining and analysis using R and Loading & Storage data to Pig Script and R for MapReduce operations.\n\u2022 Created numerous dashboards in tableau desktop based on the data collected from zonal and compass, while blending data from MS-excel and CSV files, with MS SQL server databases.\n\nEnvironment:Horton works - Hadoop Map Reduce, Pyspark, Spark, R, Spark MLLib, Tableau, Informatica, SQL, Excel, VBA, BO, CSV, Erwin, SAS, AWS Redshift, Scala Nlp, Cassandra, Oracle, MongoDB, Cognos,SQL Server 2012, Teradata, DB2, SPSS, T-SQL, PL/SQL, Flat Files, XML, and Tableau."", u""Data Architect/ Data Modeler\nAmeresco Inc - Framingham, MA\nApril 2013 to November 2014\nDescription:Ameresco, Inc. provides energy efficiency solutions for facilities in North America. The company engages in the design, engineering, development, and installation of projects that reduce the energy, and operations and maintenance costs of its customers' facilities.\nResponsibilities:\n\u2022 Understand and analyze business data requirements and architect an accurate, extensible, flexible and logical data model and Defining and implementing conceptual, logical, and physical data modeling concepts.\n\u2022 Defining Data Sources and data models, documenting actual data flows, data exchanges, and systems interconnections and interfaces. Ensuring these is aligned with the enterprise data model.\n\u2022 Design and build world class high-volume real-time data ingestion frameworks and automate various data sources into Bigdata technologies like Hadoop etc.\n\u2022 Performed Data mapping between source systems to Target systems, logical data modeling, created class diagrams and ER diagrams and used SQL queries to filter data.\n\u2022 Developed MapReduce programs to parse the raw data, populate staging tables and store the refined data in partitioned tables in the EDW.\n\u2022 Designed different type of STAR schemas using ERWIN with various Dimensions like time, services, customers and FACT tables.\n\u2022 Analyze database infrastructure to insure compliance with customer security standards, database performance considerations, and reverse engineering of existing database environments.\n\u2022 Used Hive and created Hive tables and involved in dataloading and writing Hive UDFs.\n\u2022 Creation of BTEQ, Fast export, MultiLoad, TPump, Fast load scripts for extracting data from various production systems.\n\u2022 Creation of database objects like tables, views, Materialized views, procedures, packages using Oracle tools like PL/SQL, SQL* Plus, SQL*Loader and Handled Exceptions.\n\u2022 Worked on Hive for exposing data for further analysis and for generating transforming files from different analytical formats to text files.\n\u2022 Extensively used Erwin for developing data model using star schema methodologies.\n\u2022 Worked on importing and exporting data from Oracle and DB2 into HDFS using Sqoop\n\u2022 Created, optimized, reviewed and executed Teradata SQL test queries to validate transformation rules used in source to target mappings/source views, and to verify data in target tables.\n\u2022 Used Pig as ETL tool to do transformations, event joins and some pre-aggregations before storing the data onto HDFS.\n\u2022 Provided ad-hoc queries and data metrics to the Business Users using Hive, Pig.\n\nEnvironment: ERWIN 9.x, Informatica Power Mart (Source Analyzer, Data warehousing designer, Mapping Designer, Transformations), MS SQL Server, Oracle, SQL, Hive, Map Reduce, PIG, Sqoop, HDFS, Hadoop, Teradata, Netezza, PL/SQL, Informatica, SSIS, SSRS."", u""Data Modeler/Data Analyst\nKotak Mahindra Bank - Mumbai, Maharashtra\nNovember 2011 to March 2013\nDescription:Kotak Mahindra Bank is an Indian private sector bank headquartered in Mumbai, Maharashtra, India. In February 2003, Reserve Bank of India (RBI) gave the licence to Kotak Mahindra Finance Ltd., the group's flagship company, to carry on banking business.\n\nResponsibilities:\n\u2022 Analyzed data sources and requirements and business rules to perform logical and physicaldatamodeling.\n\u2022 Analyzed and designed best fit logical and physicaldatamodels and relational database definitions using DB2. Generated reports of data definitions.\n\u2022 Involved in Normalization/De-normalization, Normal Form and database design methodology.\n\u2022 Maintained existing ETL procedures, fixed bugs and restored software to production environment.\n\u2022 Developed the code as per the client's requirements using SQL, PL/SQL and Data Warehousing concepts.\n\u2022 Involved in Dimensional modeling (Star Schema) of the Data warehouse and used Erwin to design the business process, dimensions and measured facts.\n\u2022 Worked with Data Warehouse Extract and load developers to design mappings for Data Capture, Staging, Cleansing, Loading, and Auditing.\n\u2022 Developed enterprise data model management process to manage multiple data models developed by different groups\n\u2022 Designed and created Data Marts as part of a data warehouse.\n\u2022 Wrote complex SQL queries for validating the data against different kinds of reports generated by Business Objects XIR2.\n\u2022 Using Erwin modeling tool, publishing of a data dictionary, review of the model and dictionary with subject matter experts and generation of data definition language.\n\u2022 Coordinated with DBA in implementing the Database changes and also updating DataModels with changes implemented in development, QA and Production.Worked Extensively with DBA and Reportingteam for improving the ReportPerformance with the Use of appropriate indexes and Partitioning.\n\u2022 Developed Data Mapping, Transformation and Cleansing rules for the Master Data Management Architecture involved OLTP, ODS and OLAP.\n\u2022 Tuned and coded optimization using different techniques like dynamic SQL,dynamic cursors, and tuningSQL queries, writing generic procedures, functions and packages.\n\u2022 Experienced in GUI, Relational Database Management System (RDBMS), designing of OLAP system environment as well as Report Development.\n\u2022 Extensively used SQL, T-SQL and PL/SQL to write stored procedures,functions, packages and triggers.\n\u2022 Analyzed of data report were prepared weekly, biweekly, monthly using MS Excel, SQL & UNIX.\n\nEnvironment: ER Studio, Informatica Power Center 8.1/9.1, Power Connect/ Power exchange, Oracle 11g, Mainframes,DB2 MS SQL Server 2008, SQL,PL/SQL, XML, Windows NT 4.0, Tableau, Workday, SPSS, SAS, Business Objects, XML, Tableau, Unix Shell Scripting, Teradata, Netezza, Aginity."", u""Data Analyst\nJSW Steel Ltd - Mumbai, Maharashtra\nJanuary 2009 to October 2011\nDescription: JSW Steel Ltd. The flagship company of over $11 billion JSW Group, JSW Steel is one of India's leading integrated steel manufacturers with a capacity of 18 MTPA.\n\nResponsibilities:\n\u2022 Understood and articulated business requirements from user interviews and then convert requirements into technical specifications. Effectively communicated with the SMEs to gather the requirements.\n\u2022 Worked on Regression in performing Safety Stock and Inventory Analysis using R and performed data visualizations using Tableau and R.\n\u2022 Used SQL to retrieve data from the Oracle database for data analysis and visualization and performed Inventory Analysis with Statistical and Data Visualization Tools.\n\u2022 Followed the RUP based methods using Rational Rose to create Use Cases, Activity Diagrams / State Chart Diagrams, Sequence Diagrams.\n\u2022 Designed different type of STAR schemas for detailed data marts and plan data marts in the OLAP environment.\n\u2022 Implemented Classification using supervised algorithms like Logistic Regression, Decision trees, KNN, Naive Bayes.\n\u2022 Involved with Data Analysis primarily Identifying Data Sets, Source Data, Source Meta Data, Data Definitions and Data Formats\n\u2022 Performed Decision Tree Analysis and Random forests for strategic planning and forecasting and manipulating and cleaning data using dplyr and tidyr packages in R.\n\u2022 Involved in data analysis and creating data mapping documents to capture source to target transformation rules.\n\u2022 Creating or modifying the T-SQL queries as per the business requirements and worked on creating role playing dimensions, fact-less Fact, snowflake and star schemas.\n\u2022 Wrote, executed, performance tuned SQL Queries for Data Analysis& Profiling and wrote complex SQL queries using joins, sub queries and correlated sub queries.\n\u2022 Involved in development and implementation of SSIS, SSRS and SSAS application solutions for various business units across the organization.\n\u2022 Developed mappings to load Fact and Dimension tables, SCD Type 1 and SCD Type 2 dimensions and Incremental loading and unit tested the mappings.\n\u2022 Wrote test cases, developed Test scripts using SQL and PL/SQL for UAT.\n\u2022 Worked with the ETL team to document the Transformation Rules for Data Migration from OLTP to Warehouse Environment for reporting purposes.\n\u2022 Transferred data from various OLTP data sources, such as Oracle, MS Access, MS Excel, Flat files, CSV files into SQL Server.\n\u2022 Performed data testing, tested ETL mappings (Transformation logic), tested stored procedures, and tested the XML messages.\n\u2022 Created Use cases, activity report, logical components to extract business process flows and workflows involved in the project using Rational Rose, UML and Microsoft Visio.\n\nEnvironment: R, SQL, Tableau, SSRS, Oracle, T-SQL, UNIX Shell Scripting, DB2.""]","[u""Bachelor's""]",[u'']
0,https://resumes.indeed.com/resume/7eb4e4242139f030,"[u'Database Engineer/Technical Marketing Engineer\nHomePocket - Santa Rosa Beach, FL\nSeptember 2016 to Present\nData/Database Engineer (Sept. 2016 - Present): Assisted JS developers with migration from MySQL to PostgreSQL backend (in AWS RDS) to include schema development and trigger/function creation. Worked on a mobile app for a real estate company using Ionic and Back& (MySQL). Set up the Backand (Back&) database and model and worked on the transforming and loading of JSON data into the database.\n\nTechnical marketing engineer and data analyst (May 2017 - Present): Developing marketing strategies and technical mechanism for client websites and social media campaigns using Search Engine Optimization (SEO) techniques and web and mobile apps analytics.', u'Marketing Engineer\nPagnatoKarp - Reston, VA\nMay 2016 to September 2016\nI apply web technology and digital\nmarketing platforms (such as a website, email system, CMS,\nCRM, or other software application) for the purpose of achieving marketing business goals. Main duties include:\ndaily management of sales and marketing CRM (Hubspot),\nreporting and analysis of effectiveness of direct mail\ncampaigns, and analysis and reporting of web traffic and\nSEM/marketing effectiveness.', u'Data Engineer\nSurfwatch Labs, Inc - Sterling, VA\nSeptember 2014 to April 2016\nAs a member of the SurfWatch cyber threat intelligence data analysis team, I served as lead data collection and management engineer and analyst working on a proprietary data collection, modeling and analytics platform utilizing Rackspace and AWS. In addition, my duties included in-depth Dark Web data research, data source identification, target tracking, collection and analysis, as well and business intelligence reporting (Platfora) in direct support of both customer request and corporate data collection agendas.\n\nAs a database engineer, I was responsible for the configuration and administration of all MongoDB and PostgreSQL\nenvironments to include Continuous Archiving and High\nAvailability. Used and managed Platfora instances backed by Hadoop and our PostgreSQL production database for cyber threat intelligence repositories.', u'Data Engineer\nObjective Solutions, Inc - Woodbine, MD\nJanuary 2014 to September 2014\nFollow-on contract): As a member of big data\nanalytic development team, used Hadoop\n(HDFS/MapReduce) and Accumulo to gather, store and analyze national security intelligence data sets. Primary\nduties included data flow management, data flow\ndocumentation, and life-cycle support to include age-off and purge/surge. Additional duties included CM and deployment of our production environments.', u'Data/Database Engineer\nCACI/Six3Systems/Novii Design - Fulton, MD\nSeptember 2008 to January 2014\nData Engineer (Dec. 2012 - Jan. 2014): Member of analytic development team using Hadoop (HDFS/MapReduce) plus Accumulo. Primary duties included data flow management, data flow documentation, and life-cycle support to include age-off and purge/surge. Additional duties included the evaluation of various business intelligence tools (Platfora, Tableau, Splunk) and their integration with Hadoop.\n\nDatabase Engineer (March 2012 \u2013 Dec. 2012): Worked on a production team for a web application utilizing Ruby on Rails with a MongoDB backend. Primary duties included working with the ingest team to redesign the Mongo logical schema. Additional duties included life-cycle support, version updates, and backup and recovery. Key milestone was the conversion of the single Mongo instance I&T system to a sharded cluster system using replica sets for high availability.\n\nDatabase Engineer (Sept. 2008 \u2013 Aug. 2010, March 2011 \u2013 March 2012): Assisted in the evaluation of the Netezza Data Warehouse as a candidate to replace multiple data flow systems which were using Oracle. This was done in an Agile environment. Duties included running concurrent query and ingest to determine the load on the system which was evaluated against requirements. Netezza was eventually selected for one data set and we began converting existing Oracle SQL analytics to Netezza. Additional duty included creating the ETL code using Pentaho Kettle for newer formats of the data. Upon returning, the project was expanded to include two additional data sets. Primary duties included data flow management to include schema design, testing of the feeds through the system, and analytic conversion. Also was a key member in the migration of the production system to new Netezza hardware. Duties spanning all phases included life-cycle support to include age-off, version updates, and backup and recovery.', u'Database Administrator/Software Engineer\nExpert Consultants, Inc - Frederick, MD\nSeptember 2003 to August 2008\nDatabase Administrator (April 2005 \u2013 Aug. 2008): Worked with a team developing a collaboration workspace using WebLogic App Server and Vignette Collaboration and Portal Suite. Duties have included creation of integration and production databases, backup and recovery strategies, and migration to Oracle 10g Release2 Real Application Clusters. Architecture includes Red Hat AS release 3 (Update 7), Windows 2000 and 2003, Oracle Cluster File System, and Oracle Automatic Storage Management.\n\nSoftware Engineer (Nov. 2004 \u2013 April 2005): Member of team responsible for evaluating existing infrastructure of a current Tomcat web application and providing technical data as to the future implementation of J2EE and more specifically the use of WebLogic 8.1 for leveraging various J2EE technologies.\n\nSoftware Engineer (May 2004 \u2013 Nov. 2004): Worked on team, which developed and maintained a system for processing, storage, and forward of data. Duties included porting over various utilities from existing C++ to Java, and enhancement of existing C code used in first release.\n\nSoftware Engineer (Oct. 2003 \u2013 May 2004): Member of development team for an enterprise mission application that joined multiple data feeds into a unified portal structure. Previous increment included use of Java server pages, a web service, EJB\u2019s, DAO\u2019s (Data Access Objects), DTO\u2019s (Data Transfer Objects), and an Oracle 9i suite of repositories. Future release included the MicroStrategy Business Intelligence Tool, which replaced the existing middle-tier. Duties included the set-up of the development environment which involved MicroStrategies and WebLogic 8.1 portal and application server (Solaris machine), building of MicroStrategy reports to be rendered in portlets, and porting over of existing middle-tier which will stand in conjunction with the MicroStrategies architecture.\n\nSoftware Engineer (Sept. 2003 \u2013 Oct. 2003): Worked on setting up a web services architecture for exposing services related to ECI\u2019s data transformation tool, GUPI (Graphic Universal Parsing Interface). Architecture included Tomcat 4.1.27, WASP Server for Java 4.6.1, WASP UDDI 4.6, and Apache Axis 1.1 for SOAP.']",[u'BA'],[u'University of Virginia\nJanuary 1995']
0,https://resumes.indeed.com/resume/d428969c4710d051,"[u'Research Data Analyst\nCardiovascular Institute of New Jersey\nJanuary 2018 to Present\nPredicting death and hospital readmission among subjects discharged after heart failure admission using Machine learning techniques like Neural Networks and Logistic Regression\nTechnology Used: H2O, Python Tensor Flow, R', u'IT Engineer\nEPC TRADING FZE, Sharjah - AE\nAugust 2015 to December 2016\nConnecting buyers and sellers within the country and outside the country to help growing businesses to expand their spectrum as we catered to many business and requirements like computers/laptops, construction materials, oil and gas etc.\n\u25cf Captured and maintained customer relations by catering to their needs and documenting their experience with us and rectified faults that occurred from our side.']","[u'Masters in Information Technology', u'Bachelor of Technology in Information Technology']","[u'Rutgers Business School New Brunswick, NJ\nJanuary 2017 to May 2018', u'College of Engineering Guindy, Anna University Chennai, Tamil Nadu\nAugust 2011 to June 2015']"
0,https://resumes.indeed.com/resume/71d939b82e612721,"[u""Data Quality Analyst\nOctober 2016 to June 2017\nApollo is encore's framework to analyze the customer data to identify each customer uniquely. It involved migrating data from old Encore business units on iSeries to Identity resolution engine by Fico, after the data is processed it is moved to a new data warehouse. The IRE tool uses algorithm to resolve identity of customers and their hidden relationships to reduce redundancy of customer data. Team size - 12 people.\n\n\u2022 Gathered business requirements and created process flow diagrams for business specification\n\u2022 Extensively used SQL queries for data validation\n\u2022 Collaborated with Development and Database Modeling teams for Integration, System, Functional, Regression and UAT testing of Apollo framework\n\u2022 Ensured data accuracy through the creation and implementation of business ETLs mapping over 2 million customers\n\u2022 Executed quality control checks in the form of SQL queries to enforce business rules\n\u2022 Analyzed FICO applications for testing - Debt Manager version 9 and Identity Resolution Engine\n\u2022 Created test plan for Unit testing and system integration testing\nEncore Capital Group San Diego, USA\n\nTechnologies Used: AWS, Cloud formation, Cloud watch, IAM, EC2, JSON, Sharepoint\n\nCloud team: The Cloud computing team deploys cloud infrastructure for different applications used internally by Encore. Working on IaaS, the team is responsible for managing and maintaining the cloud stack. Other responsibilities involve maintain servers, managing backups, DNS, managing encryption of sensitive data, security etc. Team size - 3 people."", u'Cloud Infrastructure Engineer\nJune 2016 to September 2016\n\u2022 Performed high level requirements gathering and solution design for clients\n\u2022 Created a client requirement gathering form on Sharepoint\n\u2022 Develop cloud reference architectures, standard cloud formation template, governance policies, and best practices.\n\u2022 Involved in building cloud infrastructure for 4 teams\n\u2022 Introduced a standard CF JSON file for all the applications which reduced the deployment time from 1-2 business days to 20 minutes deployment\n\u2022 Prepared DevOps MyEncore Cloud Site Runbook and Documentation\n\nHSBC Global Technology Center Pune, India\nTechnologies Used: Java, JSP/Servlet, JavaScript, SOAP, RESTful, Oracle, and IBM Web Sphere, MVC', u""Software Engineer\nInternational Travel Notification\nAugust 2012 to May 2015\nWhile traveling internationally, the customers notify the bank about the country they are visiting and the duration of their visit. The bank does not decline any international transaction for that duration. eCHAMPS application saves the details provided by customer and sends notification to different systems. Team size - 4.\n\n\u2022 Developed ITN module from scratch and integrate it to the main application\n\u2022 Managed a team of 3 and mentored contractors on eCHAMPS and company protocol\n\u2022 Designed and developed business logic using UML Modeling tools and translate it to use it in the technical documents\n\u2022 Developed new JSP pages, Java beans, mapper classes and work flows to save customer information and send notification to different systems\n\u2022 Used pagination to show customer travel details, 50 records per page\n\u2022 Assisted in GUI development using HTML, CSS, Javascript and jquery skills\neCHAMPS: eCHAMPS is a standard staff-facing browser-based application that serves as a front-end to Worldwide HSBC International Revolving Lending system (WHIRL). It provides a wide spectrum of consumer finance operations including Customer service, Fraud and Risk, Disputes and Chargebacks and Commercial Cards. Team size - 8.\n\n\u2022 Involved in analysis, design, development and implementation of 6 compliance projects for different HSBC businesses\n\u2022 Developed Servlets for retrieving/updating the data from mainframes backend\n\u2022 Unit tested and debugged the code and supported SIT and UAT\n\u2022 Conducted defect prevention activities for 8 regions with different code bases. It helped reducing approximately 60% of defects in unit and automation testing according to HP-Quality Centre test management tool\n\u2022 Worked on JAVA Sonar to make the legacy code compatible with the current coding standards to achieve an overall high-quality code base\n\u2022 Managed and analyzed log files to find information and facilitate problem resolution\nApple Pay: Apple pay is a contactless payment technology that helps customer pay in an easy, secure and private way. Added a flag on eCHAMPS screen to know if the customer is using apple pay to save the information about customer. This flag was added to all the flows related to credit card information and was used to notify different systems.\n\nSan Diego State University San Diego, USA\nData Analytics - FICO's Academic Engagement Program\nHome Equity Line of Credit (HELOC) Acquisition Risk data used for consumer credit risk model training. The was goal to assess the efficacy of current screening process to book accounts that will pay as negotiated and return the highest revenue. Team size - 4 people.\n\u2022 Followed Data analysis lifecycle, defined problem statement for the project\n\u2022 Used MS Excel to perform data cleaning and managing missing values\n\u2022 Performed parameter selection and calibration of the inputs to build model using R\n\u2022 Communicated the results and recommendation to FICO""]",[u'Master of Science in Management Information Systems'],[u'San Diego State University\nAugust 2015 to June 2017']
0,https://resumes.indeed.com/resume/ec9eb2a5a6f59140,"[u'Lead Security Engineer\nTestPros/USMAX - Kearneysville, WV\nNovember 2015 to Present\n\u2022 Lead a group of Security Engineers in maintaining, improving, and distributing organizational security policies.\n\u2022 Oversee organizational C&A documentation to include an organizational Cyber Security Plan, C&A templates, local IA Portal sites, and training documentation.\n\u2022 Provide guidance to Security Engineers on C&A activities, and POA&M remediation.\n\u2022 Revamped local the Cyber Security Plan from a traditional word document to an all inclusive web portal that improved the delivery time of security policy updates to information systems from 3 months, to near real time.\n\u2022 Member of USCG OSC Cyber Security Forum.\n\u2022 Conduct security reviews of system configuration management documents to determine security posture of the system to which they pertain as well as the security impacts to the interconnected systems.\n\u2022 Manage and perform AISSO functions for supported systems as part of Continuous Monitoring and interface with business systems throughout the FISMA Certification and Accreditation life cycle to evaluate and identify appropriate mitigation strategies to bring systems into compliance with established policy and industry guidelines.\n\u2022 Conduct NIST C&A activities for existing systems and systems under development; provide ongoing gap analysis of current policies, practices, and procedures as they relate to established guidelines outlined by NIST, OMB, and FISMA. Lead/participate in system/security policy and standards development, including writing guidelines, standards, procedures and other technical documentation.\n\u2022 Responsible for identifying unique system characteristics; interviewing key organizational personnel; working with business systems to compose requisite documentation; and mapping complex technical requirements, functionality, and capabilities to prescribed security controls, policies, and practices.\n\u2022 Lead a group of Security Engineers in maintaining, improving, and distributing organizational security policies.\n\u2022 Awarded Employee of the year for outstanding performance.', u'Fiber Optic Engineer\nITT Exelis - FOB Shank, Afghanistan\nFebruary 2013 to May 2013\nEngineer responsible for designing, installing and maintaining all fiber optic and copper infrastructures in a very hostile environment, under great pressure to ensure our war fighters were always at a 100% operational status.\n\u2022 Installed, terminated, and certified all communication links\n\u2022 Designed fiber optic networks that served three network environments\n\u2022 Inspected all manholes, and hand holes\n\u2022 Inventoried all tools and material\n\u2022 Ensured every job was completed professionally, and in a timely matter to ensure a fully operational status for all missions', u""Data Floor Team Lead\nGDIT - Kearneysville, WV\nFebruary 2011 to February 2013\nJuly 2013-March 2015\n\nResponsible for overseeing all Data Floor related operations, and ensuring maximum efficiency in OSC Data Center.\n\u2022 Manage and lead a technical team of IT Specialist responsible for maintaining Data Floor operations\n\u2022 Assist with operations in OSC's classified system spaces which include backup tape management, packaging and mailing of classified information, TCTO updates, CDF configuration management, equipment installs, and administrative assistance.\n\u2022 System Administrator of OSC Raritan KVM and Console system, Aperture Data Center Modeling tool, Emerson Universal Management Gateway, Emerson DSView, Site Scan, Tile Flow, and Rack Power Manager\n\u2022 System administration duties consist of ensuring systems have been security tested and are FISMA compliant\n\u2022 Lead Data Floor Process Improvement Board by initiating, approving, and documenting all processes\n\u2022 Excel in communicating and coordinating requests with all Data Floor customers\n\u2022 Manage users and groups for OSC KVM using Active Directory\n\u2022 Compile and present monthly briefings to OSC Command on project, and operation status'\n\u2022 Test applications to ensure optimal performance with different Java versions and browsers\n\u2022 Responsible for ensuring Quality Control on all Data Floor work\n\u2022 Lead weekly meetings to coordinate and schedule all work related to the Data Floor\n\u2022 Manage Data Floor inventory records, and order material as needed\n\u2022 Manage Visio drawings of Data Floor spaces""]","[u'BS in Information System Security', u'Diploma']","[u'American Public University Washington, DC\nJanuary 2020', u'Musselman High School Inwood, WV\nJanuary 2001 to January 2004']"
0,https://resumes.indeed.com/resume/26368998ee3195c8,"[u'Data Recovery Engineer\nWeRecoverData.com - Los Angeles, CA\nJanuary 2017 to Present\n\u2022 Perform data recovery on various data mediums: HDD, SDD, External HDD, mobile devices, etc.\n\u2022 Analyze file system issues on MacOS, Windows, Linux, Android and iOS operating systems.\n\u2022 Inspecting and repairing NTFS, HFS+, FAT, EXT and RAID file systems.\n\u2022 Replacement and repair of hard drive electronic components.\n\u2022 Running and writing task oriented shell scripts.\n\u2022 Conduct conference calls and remote sessions with cliental.\n\u2022 Provide customer service and technical assistance.\n\u2022 Write data recovery studies for company website.\n\u2022 Setup, maintenance and distribution of server FTP credentials to clients.', u'Lab Technician\nES Engineering LLC - Orange, CA\nOctober 2011 to January 2017\n\u2022 Set up, test and configure networks, desktops, laptops and printers\n\u2022 Design, implement and maintain onsite database using Microsoft Access & Excel\n\u2022 Perform desktop and laptop repairs on Dell, Lenovo and Apple systems\n\u2022 Diagnose, install, configure and repair computer software\n\u2022 Manage an onsite analytical laboratory\n\u2022 Perform various analytical chemistry methods on food waste samples\n\u2022 Trouble shoot, analyze and report statistical data to management\n\u2022 Perform experimental laboratory studies and pilots.\n\u2022 Submit monthly analytical reports to Air Quality Management Department\n\u2022 Test numerous pollutants for Santa Ana Watershed Project Authority', u'Lab Technician\nClinical Laboratory of San Bernardino - Grand Terrace, CA\nOctober 2002 to October 2011\n\u2022 Analyzed drinking and waste water samples for private and county sectors\n\u2022 Reported analytic results using Laboratory Information Management System\n\u2022 Analyzed samples using various automated instrumentation and software\n\u2022 Prepared chemical standards and reagents used in various analyses\n\u2022 Performed maintenance and repairs on laboratory equipment']","[u'Master of Science in Software Engineering', u'Bachelor of Science in Computer Science', u'Bachelor of Science in General education']","[u'Regis University Denver, CO\nJanuary 2019', u'Regis University Denver, CO\nMarch 2018', u'Crafton Hills Community College Yucaipa, CA\nJanuary 2010 to January 2013']"
0,https://resumes.indeed.com/resume/7a2bb593b53248de,"[u'Data Warehouse Developer\nWestern Health Advantage, CA\nJuly 2016 to Present\nEnvironment: Informatica Power Center 9.0.1, Oracle (SQL Navigator) 11.x, Remedy, Data Grip, Microsoft SQL Server 2016.\nResponsibilities:\n\u2022 Involved in Analysis, Requirements Gathering and documentation of Functional & Technical specifications.\n\u2022 Design, develop, and deploy ETL solutions to meet business requirements using Informatica Power Center.\n\u2022 Build new ETL mapping, workflows and reusable mapplets and transformations in order to extract the data from various sources (XML, Flat Files & Database tables) transform it per the business requirements and load the data into targets using Informatica Power Center.\n\u2022 Developed complex mappings using Lookups both connected and unconnected, Sorter, Joiner, Aggregator, Filter, Router, SQL transformations to transform the data as per the requirements.\n\u2022 Created Workflows using various tasks like Email, Event-wait and Event-raise, assignment, Timer, Scheduler, Control, Decision, Session in the Workflow Manage\n\u2022 Researched and resolved the issues regarding the integrity of data in the databases.\n\u2022 Used SQL-Overrides and filter conditions in source qualifier as required thereby improved the performance of the mapping involved in performance tuning\n\u2022 Communicating with business for new requirements and catering changes to the existing extracts.\n\u2022 Used Workflow Monitor to monitor the jobs, review error logs that were generated for each session, and resolved the\n\u2022 Conduct ETL performance tuning, trouble shooting, support and capacity estimation.\n\u2022 Optimized performance of Informatica session for huge data files by using partitioning, increasing block size, data cache size, sequence buffer length and target based commit interval\n\u2022 Identified and resolved any processing and/or procedural issues as they occurred minimizing any customer impacts.\n\u2022 Conduct thorough testing of ETL code changes to ensure quality of data produced.\n\u2022 Monitoring, maintain and support existing load process.\n\u2022 Prepared documentation on all assigned systems and databases, including business rules, logic, and processes\n\u2022 Perform data extract activities based on specific needs provided by the business\n\u2022 Formulates, implements, and enforces proper data collection policies and procedures.', u""Senior Consultant\nIBM\nAugust 2011 to March 2012\nEnvironment: BODS 3.0, Ascential Datastage 6.0, BI/BW Version 7.5 & 3.0, Business Objects XI R2, Web Intelligence, Desktop Intelligence, Info View, DB2/SQL and MSSQL 8\nResponsibilities:\n\u2022 Replicated all the transformations from Datastage into BO Data Services 3.0 ETL transforms and ensured the output was consistent according to the business logic and requirement from different sources.\n\u2022 Hands on experience using transformations like query, table comparison, look-up, Case, merge, SQL transformation.\n\u2022 Experience handling administrative jobs like scheduling, un-scheduling batch jobs, granting access/ permissions to the user etc.\n\u2022 Responsible for unit testing of logic and data.\n\u2022 Validating the load process of ETL to make sure the target tables are populated according the data mapping provided that satisfies the transformation rules.\n\u2022 Analyzed the business requirements and worked with different application teams to develop various reports.\n\u2022 Created various summary reports that involved use of formulas, Grouping, Parameters, and Selection Criteria to handle the customer's request from Tables, Stored procedures, Views and Command objects using Crystal Reports.\n\u2022 Prepared Unit test cases using Quality Center and developed detail design documents for each report.\n\u2022 Worked with Support team to complete design/code review.\n\u2022 Created and tested data flows, workflows, scripts, and jobs."", u""Software Engineer\nUnited Health Group\nSeptember 2010 to May 2011\nEnvironment: Business Objects XI- Release 2, Desktop Intelligence (Deski), SAS, Datastage, Oracle 10g, Windows OS\nResponsibilities:\n\u2022 Maintaining the existing universes, reports which have been migrated.\n\u2022 Creating new reports (On demand, Adhoc Reports, Summary Reports, Sub Reports, dynamic grouping, Cross-Tab, graphical, etc.) based on the user requirement\n\u2022 Making changes to an existing report as per user requirement.\n\u2022 Managed user profiles using Supervisor and Set object-level security and row level security restrictions, as per the client requirement\n\u2022 Built reports for various subject areas (Membership, Provider, Claims).\n\u2022 Single point of contact (SPOC) for the BO Service Desk.\n\u2022 Providing Business Objects access to new user. Extensively worked with security issues and user maintenance in Business Objects 6.5/XI/XIR2.\n\u2022 Processing the certification related to the 'Membership Subject area' and adding users to security table\n\u2022 Created custom hierarchies to support drill down reports.\n\u2022 Created the reports using Business Objects functionalities like Combined Queries, Slice and Dice, Drill down, @Functions, Cross Tab, Master Detail and Formulae etc.\n\u2022 Created users, groups and access rights using Central Management Console.\n\u2022 Implemented complex new Universes Linked with existing Universes from Data marts.\n\u2022 Managed user profiles using Supervisor and Set object-level security and row level security restrictions, as per the client requirement.\n\u2022 Migrated Business Objects XIR3 to Business Objects 4.0.\n\u2022 Created Crystal reports using formulas, Grouping, Parameters, cross-tab, sub reports, parameter values etc.\n\u2022 Created Reports like, Reports by Category, Reports by Region, and Reports by Time (Year, Quarter, and Month Date).\n\u2022 Develop canned Web Intelligence reports using functionality like Drill feature, Scope of Analysis, Variables, Rank, Sort, Breaks, Formatting, hyperlinked reports (Sub reports) and Sectioning etc.\n\u2022 Performed Unit testing, integration testing to verify the report functionality and debugged dashboard reports to verify data matches as expected."", u'Software Engineer\nTech Mahindra\nJanuary 2007 to September 2010\nEnvironment: BO Data Integrator (SAP BODI 11.7), Business Objects XI- Release 2, Oracle 10g, Windows OS, Designer, Broadcast Agent, Web Intelligence, TOAD 6.3, PL/SQL, ERWIN 4.0.\n\nResponsibilities:\n\u2022 Gathering requirements from users, extracting data and designing reports as per the business requirements.\n\u2022 Created SAP Data Integrator mappings, batch Jobs, Workflows and Data Flows to load the data warehouse, the mappings involved extensive use of simple and complex transformations like Key Generator, SQL Query Transform, Table Comparison, Case, Validation, Merge, lookup etc.\n\u2022 Migrated ETL Jobs by exporting and importing ATL files.\n\u2022 Created , Scheduled and Monitored Batch Jobs using BODI 11.7\n\u2022 Importing jobs from development to production server\n\u2022 Exception Handling using Data Integrator. Implemented SCD 2 using table comparison and history preserving transforms.\n\u2022 Designed dimension tables, fact tables, Summary and Aggregation tables according to the functional requirement specifications and loaded data to support reporting.\n\u2022 Good understanding of Match Transforms, Address Cleanse and Data Cleanse Transforms for US Data Cleansing in Data Quality.\n\u2022 Designed, developed, implemented and maintained Universes using Business Objects Designer, Resolved loops by creating Contexts and Aliases.\n\u2022 Checked Universes for Integrity and exported them to the Repository to make resources available to the users and groups.\n\u2022 Designed and developed various Universes by defining classes, objects, and defined Joins, Cardinalities between tables.\n\u2022 Created new Universes and fine-tuned existing ones by resolving join problems, setting up aggregate awareness, defining hierarchies, cascading LOV, @Functions , Breaks, Filters, Sorts, alerts, ranks, formulas Query prompt , Drill filters and more.']",[u'Bachelors in Technology in Computer Science and Engineering'],[u'JNTU']
0,https://resumes.indeed.com/resume/f2bddc07acbd0ff5,"[u'Data Center Technician\nTWITTER\nMay 2016 to Present\nReviewed and improved existing SQL queries and reports. Created new queries and charts. Built new dashboards for team awareness. (Tools: MySQL, Tableau, Jira, Confluence)\n\u2022 Participated in an effort that cleared a large backlog of server repair tickets.\n\u2022 Earned a reputation as a bash command-line guru.', u'Systems Administrator\nHEWLETT PACKARD ENTERPRISE - Pleasanton, CA\nAugust 2012 to April 2016\nAssisted in the evaluation of prospective tools for Data Center Infrastructure Management (DCIM).\n\u2022 Took a lead role in the implementation of the chosen DCIM tool. Involvement included: deployment and setup; data analysis and transfer; end user training; administrator training; establishing standards for data integrity, naming conventions, etc; assisting technical writer; technical liaison with vendor; and other tasks as needed.\n\u2022 Researched, chose, and deployed reporting tool. Deciding features included: cost, ease of use and cross-training, and ability to schedule automated reports.\n\u2022 Developed multiple reports from a complex SQL database.\n\u2022 Planned and implemented the deployment of hardware/system probes to extract discovered data from highly segregated secure environments.\n\u2022 Updated and enhanced multiple scripts, which pulled hardware and system information remotely for reporting, management, and administration purposes.\nContinued to act as a consultant on data center operations issues.', u'Data Center Team Lead\nAUTONOMY CORPORATION - Sacramento, CA\nAugust 2008 to August 2012\n\u2022 Trained and led a team of data center technicians.\n\u2022 Acted as on-site liaison to co-location vendor.\nContinued to have a hand in all aspects of data center operations.', u'Data Center Technician\nZANTAZ - Sacramento, CA\nAugust 2006 to August 2008\n\u2022 Independent installation, setup, and troubleshooting of Linux servers, including RAID array maintenance and VM infrastructure (Xen, KVM, Vmware ESXi).\n\u2022 Assisted network engineers with installation, setup, and troubleshooting of network equipment.\n\u2022 Trained and coordinated short-term helpers for support during larger projects.\n\u2022 Provided input during data center expansion planning.\n\u2022 Maintained an inventory of incoming and spare equipment and parts.', u'Data Services Engineer\nZANTAZ - Sacramento, CA\nAugust 2004 to August 2006\n\u2022 Assisted systems engineers with installation, setup, and troubleshooting of Linux servers.\n\u2022 Coordinated and executed backup media transfers.\n\u2022 Assisted with data center moves involving hundreds of servers.']",[],[]
0,https://resumes.indeed.com/resume/6d68f666a6896cba,"[u""DATA ENGINEER\n@WalmartLabs\nAugust 2016 to August 2017\nDesigned and developed the first revisions of WM Datalake, Walmart's central data repository for the complete\nAnalytics Division. \u2022 Built a data movement framework for Mainframe and Teradata to fill the lake that is currently\nbeing used to migrate 1000+ daily jobs."", u""SOFTWARE ENGINEER INTERN\n@WalmartLabs\nJanuary 2016 to July 2016\nContributed in the complete development cycle of VEGA, Walmart's in-house unified programming model for creating data processing pipelines. \u2022Set up the environment for continuous real-time processing 5k TPS(5GB/sec) of\nWalmart's store transaction data.""]","[u'Master of Science in Computer Science', u'Bachelor of Engineering in Computer Science']","[u'New York University Manhattan, NY\nMay 2019', u'PES Institute of technology Bengaluru, Karnataka\nAugust 2016']"
0,https://resumes.indeed.com/resume/782da1a166643dec,"[u""DATA/BUSINESS ANALYST (SERVICE ENGINEER)\nLEXMARK\nJune 2009 to Present\n\u2022 I have a proven ability to data mine, transform, and merge customer and internal data to create business intelligence and customer metrics using MS Excel, Siebel Analytics, and Minitab statistical analysis with multiple $10M+ annual sales renewal revenue outcomes. I performed numerous Six Sigma Black Belt analyses for large (Fortune 500) customers (1000+ units) which were reviewed and presented at an executive level internally and externally. I developed a Warranty Rate MS Excel data structure which took AS400 input and using the 'indirect' function and dynamic cell selection reducing a full time position to 1 hr/month. I developed a WW Extended Warranty rate projection framework with polynomial trend/ema projections and visualizations. I routinely use a MS Excel VBA tool I developed/wrote to rapidly prototype data mining/transformations capable of 1M cycles/hour accelerating analysis from 1 month to 1 hour. I drove a team as Project Manager to gather user requirements, user stories, and Key Performance Indicators to create a Business Objects (BOXI) Warranty Dashboard/Deep Dive Visualization platform for Engineering Quality review and customer investigation.\n\u2022 Additional role 2/01/2017 Global Service Product Manager For Channel Products\nIn addition to the duties of my Data Analyst (Service Engineer) role, I have extensive experience performing Six Sigma Black Belt analysis/engagements for Service/Development Cost Reduction Initiatives ($5M 2017, $1M+ 2018 to date) including financial impact. I gather requirements and drive into future product offerings. I have successfully launched Lexmark's new Open Channel line of products, and initiated 7 new programs since 2/01/2017. I co-host monthly global new product planning meetings driving global launch (NPI) readiness (150+touchpoints).\n\nGLOBAL SERVICE PRODUCT MANAGER FOR COLOR LASER LEXMARK STARTING 8/01/2007\n\u2022 I developed new product business cases for World Wide Service and attended Development Business Meetings. I drove to WW Service goals and signed off/escalated. I scheduled and distributed new product training and machines. I hosted monthly new product planning meetings and Service Readiness Review for Color Laser programs. I successfully launched 5 color laser products.\nDATA ANALYST (COLOR LASER SERVICE ENGINEER) LEXMARK STARTING 7/2/2005\n\u2022 I aggregated/filtered Color Laser products field performance data. I travelled as necessary in support of customer/management objectives. I developed business cases for new product function and current product in the field.. I aggregated and filtered service data by vendor and invoiced for warranty recovery ($.5M annually).""]",[u'BACHELORS DEGREE in BUSINESS ADMINISTRATION'],[u'UNIVERSITY OF KENTUCKY\nJanuary 1994']
0,https://resumes.indeed.com/resume/1f6f33f4db64594e,"[u'Sr. Data Engineer\nFunko - Walt Disney/Marvel\nSeptember 2017 to Present\n-Improved the ETL process by segregating the data movement into stored procs and SSIS packages.\n-Designed Tableau/Power BI based sales dashboards that is currently being consumed by as many as 350\ncustomers.\n-Wrote and implemented the architecture for planning system of records completely based on Excel that\nincorporates write back functionality to the SQL servers and cubes.\n-Performed data partition and migration completely on SQL that helped achieving efficiency of data modeling\nby 60%.\n-Interacted directly with the Director of analytics and CTO to get the architectural requirements and successfully implemented them across multiple data pipeline based projects.', u'SLQ BI Architect\nFunko - Walt Disney/Marvel - Redmond, WA\nNovember 2016 to September 2017\n-Designed and managed as many as 10 dashboards for windows and devices groups that users are using\ninteractively and regularly to monitor the progress and trends for all the devices for performance and marketing.\n- Wrote SCOPE scripts to move data from COSMOS to Azure database and on premise SQL server.\n-Extensively worked on MS BI stack to design and implement ETL involving data from heterogeneous data\nsources. Benefited the group by designing the Power BI based fan portal for windows, surface, Xbox.\n-Wrote predictive modeling algorithms and future business trends that helped business capturing the loopholes and accurate data points to consider.', u'Sr Data Analyst\nFunko - Walt Disney/Marvel - Redmond, WA\nFebruary 2016 to November 2016\n-Implemented web insights tool (Competitor cloud data) and re modified the existing architecture, benefited\nthe team to bring down the overall process/jobs execution time from a month to 6 days.\n- Single handedly designed RSIT POS Scorecard for universal store using Power BI that serves as a powerful\ntool in analyzing the patterns and trends across all Microsoft stores.', u'Technical Data Analyst\nBank of America - Jacksonville, FL\nJune 2015 to August 2015\n-Managed 2 projects on database migration and benefited company by reducing the maintenance cost.\n-Extensively used statistical tools & languages like SAS, SQL queries and Excel to analyze financial data and drawing conclusions & developing recommendations.', u'Data Analyst\nHSBC - Pune, Maharashtra\nJune 2011 to July 2014\n-Used ACL, SAS and SQL for Data auditing & analysis, creating data reports & monitoring all data for accuracy.\n-Extensively used Microsoft BI stack (SSIS, SSAS and SSRS) to solve complex data problems.\n-Produced regular comprehensive dashboards and reports to assist senior managers and engineers.']",[u'Master of Science in Business Intelligence in Business Intelligence'],"[u'University of North Carolina Charlotte, NC\nDecember 2015']"
0,https://resumes.indeed.com/resume/870b06caa12fc4db,"[u""Data Quality Analyst\nInfosys Limited - Hyderabad, Telangana\nJanuary 2015 to December 2015\n\u2022 Connected client's WALMART pharmacy data to the main data servers by updating and maintaining Central Data Setup.\n\u2022 Solved abends and monitored daily store-client transactions with 3rd party insurance claims using Mainframes, Excel and UNIX.\n\u2022 Built SQL queries to pull up the data and ensure the integrity of data from the data source and DataMart.\n\u2022 Used prioritization matrices to monitor and solve the high priority issues there by increasing the overall efficiency of the team's performance by 12%.\n\u2022 Implemented affinity diagram technique to systematize the collective opinion of the team."", u""Manufacturing Quality Engineer\nChemic Life Sciences Pharmaceuticals Pvt. Ltd - Hyderabad, Telangana\nJune 2014 to December 2014\n\u2022 Implemented snap back method of time study analysis and determined the time taken for each manufacturing process.\n\u2022 Executed Installation Qualification (IQ), Operational Qualification (OQ) and Performance Qualification (PQ) protocols to validate\nthe new freezer equipment. Performed Gage R&R analysis on air pressure gauges inside the reactors.\n\u2022 Investigated and dispositioned Non-Conformances by performing Root Cause Analysis and escalated the issues to CAPA's as necessary. Developed sampling plans for inspection methods and protocols using statistical analysis of data.\n\u2022 Implementing preventive maintenance schedule periodically and segregating issues into critical and non-critical defects.\n\u2022 Authored/ Updated standard operating procedures, Work Instructions to improve Quality control process."", u'Manufacturing Engineer Intern\nBharat Dynamics Limited - Hyderabad, Telangana\nJanuary 2013 to August 2013\n\u2022 Analyzed production of seamless rocket motor tube using flow forming technique to obtain the desired size by optimizing the rotation of the mandrel and set of three rollers.\n\u2022 Reduced the lead time by 20% initiating 5s standards and developed a process flow diagram for the shop floor operations.\n\u2022 Demonstrated the relation between the KPIs - material, roller geometry stagger, speed, feed, reduction percentage that effect\novality, thickness and mean diameter using MS- Excel, SPC.\n\u2022 Achieved reduction in the time take taken by each operator to retrieve equipment for lathe operators from the inventory by 10%\nusing Kanban techniques.\n\u2022 Implemented continuous improvement technique to optimize the feed rate to 50mm/min from 60mm/min for SAE4130 Steels.']","[u'Master of science in Industrial Engineering in Industrial Engineering', u'Bachelor of Engineering in Industrial in production']","[u'University of Texas Arlington, TX\nDecember 2017', u'Osmania University Hyderabad, Telangana\nJune 2014']"
0,https://resumes.indeed.com/resume/51494b49c8abe134,[u'Senior Data Analyst/Engineer\nMarch 2016 to Present'],"[u""Master's""]",[u'']
0,https://resumes.indeed.com/resume/fd30314ed9266639,"[u'Data Center Technician L2 (Critical Facilities Operation Engineer)\nRagingWire Data Centers - Ashburn, VA\nJune 2016 to Present\nResponsibilities:\n\n\u2022 Maintained and operated 300,000 sq. ft. of raised floor space for RagingWire Data Centers.\n\u2022 Operation and Maintenance and troubleshooting allover key Data Center support systems such as, (Diesel generators, Transformers, MSB\u2019s switchgear (Medium and low voltage), STS\u2019s, ATS\u2019s, UPS\u2019s, PDU\u2019s, , Crah\u2019s, Chillers, cooling towers, MAU\u2019s, pumps, motors, VFD\u2019s( HVAC system ), SCADA and building automation systems this equipment supports mission- Network Vaults and must maintain 100% up time.\n\u2022 Write to assigned schedules to our team Critical Facilities Operation Engineer (CFOps) the inspections needed for the maintenance of the building include writing the Standard Operating Procedure (SOP) and Method of Procedure (MOP) and Power Usage Effectiveness (PUE) Report and uploaded all reports to the RagingWire shear drive.\n\u2022 Working on electrical and mechanical equipment troubleshooting and operation and functional and monitoring by Building Management Systems (BMS/ IFIX/SCADA system).\n\u2022 Joining and supervising the maintenance activities in the building and submitted recommendations for equipment\u2019s that needs repairs or changed.\n\u2022 Take daily operational readings and ensuring the overall operation and maintenance of all electrical and mechanical equipment within the Data Center.\n\u2022 Daily check the Fire Alarm System devices of the building to ensure proper working conditions.\n\u2022 Order parts and supplies for maintenance and repairs.', u'Technical Supervisor ( Data Centers )\n(Contract) CLP - Ashburn, VA\nJune 2015 to May 2016\nResponsibilities\n\u2022 Prepare and install 480 / 208 / 120 Volt power distribution whips from low voltage 225 amp panels.\n\u2022 Perform normal daily operation and maintenance of power distribution systems including engine-generators, UPS, ASTS-PDU components.\n\u2022 Conduct daily power distribution systems readings and perform individual circuit readings using ammeter and BCM.\n\u2022 Maintain inventory of electrical components stock and prepare updated prescribed load list orders for stock replenishment.\n\u2022 Install low voltage rack mounted static transfer switches.\n\u2022 Perform low voltage distribution panel maintenance under energized conditions.\n\u2022 Assist vendor personnel in performing power distribution system maintenance including IR scanning, and other preventive and predictive maintenance.\n\u2022 Respond to contingencies to ensure restoration of system operations in the event of a component or system malfunction.\n\u2022 Trouble-shoot electrical systems and components when faults occur to ensure continuous operations.\n\u2022 Prepare and update operational and maintenance logs for electrical systems.\n\u2022 Perform other duties as assigned by supervisor.', u'Data Center Engineer ( Critical Facility Engineer)\n(Amazon Web Service Contractor) Insight Global - Sterling, VA\nMay 2014 to May 2015\nResponsibilities:\n\u2022 Direct a high-performing team of ten professionals charged with ensuring the 24/7 uptime of the AWS Data Center, operating over 150,000 sq. ft. of raised floor space. Oversee server integration from receiving to decommissioning.\nProvide comprehensive support to the critical environment and maintain the electrical/mechanical infrastructure. Coordinate all building facility enhancement, supervise vendor work, and partner with system integrators to ensure maximum productivity.\n\u2022 Operation and Maintenance and troubleshooting overall electrical, mechanical, and fire/life safety\n\u2022 equipment within the data center (diesel generators, Transformers, switchgear, ATS\u2019s, UPS\u2019s, PDU\u2019s, AHU\u2019s, Crah\u2019s, Chillers, UV water filtering, cooling towers, pumps, motors, VFD\u2019s( HVAC system ), and building automation systems , this equipment supports mission-critical devices (server racks /network racks) and must maintain better than 99.999% up time.\n\u2022 Write Change Management (CM) and Trouble Ticket (TT) to assigned schedules to technicians the inspections needed for the maintenance of the building including and Standard Operating Procedure (SOP) and Method Of Procedure (MOP) and Power Usage Effectiveness (PUE) Report and uploaded all reports to the amazon shear drive .\n\u2022 Working on electrical and mechanical equipment troubleshooting and operation and functional and monitoring by Building Management Systems (BMS/ SCADA system).\n\u2022 Joining and supervising the maintenance activities in the building and submitted recommendations for equipment\u2019s that needs repairs or changed.\n\u2022 Take daily operational readings and ensuring the overall operation and maintenance of all electrical and mechanical equipment within the Data Center.\n\u2022 Updated the safety feature warning devices of the building to ensure proper working conditions.', u'Construction & Project Management Specialist\n(Contractor)Samsung Engineering & the Kufan Group Engineering, Construction\nAugust 2010 to April 2014\nResponsibilities:\n\u2022 Managed the construction works of camping and facilities SAMSUNG Construction Company area in Badra Oil Field in southern of Iraq.\n\u2022 Managed the logistics encompasses all of the information and material flows throughout an organization. It includes everything from the movement of a product or from a service that needs to be rendered, through to the management of incoming materials and equipment, its delivery to the customer.\n\u2022 Develop electromechanical orbital welders for automatic and PLCs pipe welding, root weld inspection and pre and post processing equipment for Oil pipeline welding and pipe face cutting machine.\n\u2022 Developed a family of laser inspection devices for the internal root weld of small diameter pipes part for the oil refinery pipeline industry, reducing production downtime significantly.\n\u2022 Managed the laying and installation cables of telecommunications (Microwave Network), ABB SCADA, RTU, PLC equipment, Internet, Security System CCTV).\n\u2022 Managed the installation the transformers MV and switchgear (Substations) and Generators and Trenching and laying installation cables Utilities feeders. Managed Installation of trench cables and laying cables from (Solar system, Generators, ATS\u2019s, UPS\u2019s, PDU\u2019s) to switchgear.\n\u2022 Managed Installation, operation and maintenance of Data Center and HVAC system (Air-conditioning system, Chillers, UV water filtering, cooling towers, pumps, motors, VFD\u2019s)\n\u2022 Managed Installation of synchronization system between generators.', u'Electrical Engineer & SCADA Engineer including Data Centers\n(Contractor)Stanley Baker Hill (SBH) LLC & VERSAR (US Army Corps of Engineers)\nOctober 2003 to April 2010\nResponsibilities:\nOverall operation and management responsibility for SCADA Projects including Facilities Management /Data Centers , North region Med region South region. In addition to those duties Basim works with the US Army Corps of Engineers teams strategic corporate initiatives related to operations, policies, procedures and training, as well as helping to build new infrastructures into an international facilities management powerhouse.\nApplication experience includes industrial controls, and power automation solutions utilizing multiple communication protocols like DNP3, IEC 61850, PLCs (IEC 1131-3) and Modbus. Our goal is to guide potential customers toward the most efficient ways of automating an electrical substation. We recommend industrial and utility grade solutions from ABB, Siemens, and Schneider Electric.\n\u2022 Worked on the installation and testing of new SCADA systems & RTUs, PLCs in Fifty Five Substations type MV in Baghdad.\n\u2022 Worked on the installation and testing of new Three Data Centers for SCADA project.\nSpecialties: Providing solutions for Industrial Automation projects:\nPower Utility DCS, SCADA, RTU, PLC Protection & Control, Vision Systems & Sensors , Sensors , Inverters , Servo Amplifiers & Motors, PLC Hardware Configuration, RTU Hardware Configuration, Industrial Computer.\n\u2022 Operations and Maintenance HVAC, UPS, ATS, transformers, generators for data centers.\n\u2022 Devised logistics and phase planning for successful coordination and management of office moves.\n\u2022 Ensured/maintained certifications for office safety security systems, fire suppression, CCTV and card reader.\n\u2022 Working on electrical and mechanical equipment troubleshooting and operation and functional and monitoring by Building Management Systems (BMS/SCADA system).\nKey Accomplishments:\n\u2022 Managed $178.2M build out of Three SCADA Projects and three Data Centers.', u'SCADA Engineer/Mobil Cell Phone Telecommunication Systems /Data Centers\n( Contractor)General Systems Company\nNovember 2000 to September 2003\nResponsibilities:\n\u2022 Team member in the project of modernization and upgrading of the SCADA systems and Data Center of the National Control Center (NCC).\n\u2022 Led IT team to connect the RTUs (Remote Terminal Units) and PLC ( programming Logical Conrolin the power substations with SCADA Center in (NCC) to receive data.\n\u2022 Led the telecommunication & networks crew in performing several maintenance and networks upgrading jobs.\n\u2022 I was responsible for fire alarm and fire fighting system installation.\n\u2022 Supervised the installation of telecommunications networks (Cell Phone network systems installation and alignment and programming (Microwave antenna and antenna sectors))']",[u'B.Sc. in Electrical & Electronics Engineering'],[u'University of Technology\nJanuary 2000']
0,https://resumes.indeed.com/resume/be0502bb8c2600b8,"[u""BIG Data Engineer/Analyst\nDSE HEALTHCARE SOLUTIONS - Edison, NJ\nDecember 2016 to Present\n\u2022 Involved in Big data requirement analysis, develop and design solutions for ETL and Business Intelligence platforms.\n\u2022 Installed Kafka on Hadoop cluster and configured producer and consumer in java to establish connection from source to HDFS with popular hash tags.\n\u2022 Load real time data from various data sources into HDFS using Kafka with spark streaming.\n\u2022 Worked with various columnar storages like PARQUET and ORC.\n\u2022 Implemented ETL using python as language of choice in Spark (Py Spark) for faster testing and processing of data.\n\u2022 Worked with Spark2 (Dataset API) and Spark 1.6.3 (Data Frame and RDD APIs)\n\u2022 Involved in converting Hive/SQL queries into Spark transformations using API's like Spark native methods, Data Frames and python.\n\u2022 Analyzed the SQL scripts and designed the solution to implement using python.\n\u2022 Evaluating Spark code to look for various pain points and improving the performance of the solution to ensure resources are utilized more efficiently.\n\u2022 Evaluating the job Performance based upon Virtual Memory, Physical Memory and CPU peak usage with Pepper Data.\n\u2022 Performed transformations, cleaning and filtering on imported data using Spark Data Frame API, Hive, MapReduce, and loaded final data into Hive.\n\u2022 Involved in converting Map Reduce programs into Spark transformations using Spark python API.\n\u2022 Developed Spark scripts by using python and bash Shell commands as per the requirement.\n\u2022 Worked with NoSQL databases like HBase in creating tables to load large sets of semi structured data coming from source systems.\n\u2022 Scheduled the various parts of the job using oozie\n\u2022 Tools utilized included SQL Server (SSIS/SSAS/SSRS).\n\u2022 Successfully served as consultant on team responsible for implementing the largest and most complex client engagement in company history.\n\u2022 Key responsibilities included supporting requirements across stakeholder groups, data warehouse model design, ETL design hardware/infrastructure planning and procurement, development\nEnvironment: Hadoop, Hive, Spark Data Frame API, HBase, MapReduce, HDFS"", u'BIG Data Engineer\nExpress Scripts, NJ\nFebruary 2014 to December 2016\nThe project was to implement a data lake using Bigdata technologies to identify and exploit new sources of value that exist in an organization and to cultivate future opportunities for value creation and protection.\n\n\u2022 Responsible for a setup of 5 node development cluster for a Proof of Concept which was later implemented as a fulltime project by Express Scripts.\n\u2022 Responsible for Installation and configuration of Hive, Sqoop, Zookeeper and Oozie on the Hortonworks Hadoop cluster using Ambari.\n\u2022 Involved in extracting large sets of structured, semi structured and unstructured data from various sources like ftp servers, API calls.\n\u2022 Developed Sqoop scripts to import data from Oracle database and handled incremental loading on the point of sale tables using Sqoop Metastore.\n\u2022 Created Hive external tables and views, on the data imported into the HDFS.\n\u2022 Developed and implemented Hive scripts for transformations such as evaluation, filtering and aggregation.\n\u2022 Worked on partitioning and bucketing of Hive tables and running the scripts in parallel to reduce run-time of the scripts.\n\u2022 Developed User Defined Functions (UDF) in python if required for hive queries.\n\u2022 Worked with data in multiple file formats including Parquet, Sequence files, ORC and Text(delimited)/CSV.\n\u2022 Used Oozie Operational Services for batch processing and scheduling workflows dynamically.\n\u2022 Worked on creating End-End data pipeline orchestration using Oozie.\n\u2022 Developed bash scripts to automate the above process of Extraction, Transformation and Loading.\n\u2022 Very good experience in managing the Hadoop cluster using Ambari.\n\u2022 Created roles and user groups in Ambari for permitted access to Ambari functions\n\u2022 Working knowledge of MapReduce and YARN (Hadoop2) architectures.\n\u2022 Working knowledge on ZooKeeper.\n\u2022 Working knowledge on Tableau.\nEnvironment: Apache Spark, HDFS, Java, Map Reduce, Hive, HBase, Sqoop, SQL, Oozie, Cloudera Manager, Zoo Keeper, Cloudera.']","[u""Master's""]",[u'']
0,https://resumes.indeed.com/resume/76873a9a24c27d42,"[u'Associate Software Engineer\nMedeAnalytics, Inc. - Emeryville, CA\nJanuary 2015 to February 2017\nCreated SaaS dashboards by developing ETL code in T-SQL using MSSQL stored procedures, writing complex calculations in MDX, configuring OLAP cubes in SSAS, and maintaining internal reports/charts with XML. Assumed sole responsibility of development for key medical dashboard\n\u2022 Collaborated with product managers to improve existng medical cost-beneft theory based on Medicare regulatons to enhance key medical dashboard products\n\u2022 Ported OLAP cubes for existing products from SSAS to Pentaho Mondrian and contributed to migraton of ETL environment from MSSQL server to Teradata/Hive-on-Spark', u'Big Data Intern\nMedeAnalytics, Inc. - Emeryville, CA\nSeptember 2014 to January 2015\n\u2022 Cleaned data from medical vendors using R/regex and loaded data into Vertica\n\u2022 Learned the BI/Data Warehousing process and contributed to ETL development using vSQL', u'Intern\nPalo Alto Advisors, Inc. - Palo Alto, CA\nJune 2011 to August 2011\n\u2022 Aggregated key fnancial statstcs from spreadsheets using Excel VBA\n\u2022 Visualized data and recommended weekly stock choices']",[u'BA in Statistics'],"[u'UC Berkeley Berkeley, CA']"
0,https://resumes.indeed.com/resume/c260662a682221f0,"[u'Field Engineer II\nZiebel US - Houston, TX\nJune 2015 to Present\nPreviously Junior Field Engineer\nDevelopments\n* Increase in Successful Operative Capacity 10-Fold Despite Economic Downturn\n* Collaboration with Competitors and Business Partners to ensure Safety and Success of the team\n* Performance Tools & Tracking\nTraining of all onboarded senior field engineers', u'Clinical Data Manager\nWaldron Health Consultants\nJune 2012 to March 2014\nStudy Sponsors include: Genentech, Hoffman- La Roche, Novartis Pharmaceuticals, Bristol-Myers Squibb, Celgene, Amgen, Quintiles\nResponsibilities\n* Site Activation to Database Lock\n* Lean Management: Training of Staff: Protocol Compliance & Execution, Diagnostic Completion, HIPPA Compliance, and Delegation of Data Management Oracle, SQL, RAVE, EDC Systems, EPIC, Local MIS\n* Oversaw Quality Control, Verification Monitoring, and Deviation Reporting to IRB/WIRB']","[u'Bachelor of Science in Biochemistry Graduate', u""Master's in Business Administration""]","[u'Texas A&M University\nMay 2012', u'College of Agriculture']"
0,https://resumes.indeed.com/resume/f80705fa20a661d0,"[u'Data Analyst\nNielsen Catalina Ventures - Chicago, IL\nOctober 2017 to Present\nUsing neural nets to determine the factors in advertisement sector to drive growth. Home scan data from Nielsen matched with frequent shopper data from Catalina', u'Teaching Assistant\nUniversity of Illinois - Chicago, IL\nAugust 2016 to December 2016\nInstruct students to use Excel for Inventory Management and Forecasting, create Macros and Pivot table', u""Associate Systems Engineer\nTata Consultancy Services Ltd\nDecember 2015 to June 2016\nAnalyzed data using SAP HANA by building calculated view, attribute view constraining the query's response time and migrated data from ORACLE to SAP HANA"", u""Data Analyst\nETSC Computers Pvt. Ltd\nMay 2015 to December 2015\nAnalyzed the data using time series analysis for forecasting company's growth. Analyzed the trends in mass surveys\nPARTICIPATION\n\u2022 Kaggle - Developed statistical models using R on Kaggle and contributed to code correction and debugging\n\u2022 Github - Various projects in text mining, predictive modelling.""]","[u'Master of Science in Management Information Systems', u'in Research Intern', u'Bachelor of Technology in Electronics and Communication']","[u'University of Illinois at Chicago - Liautaud Graduate School of Business Chicago, IL\nDecember 2017', u'Loyola Medical Center\nJune 2017 to August 2017', u'Rajasthan Technical University\nMay 2015']"
0,https://resumes.indeed.com/resume/36e137dc59936c54,"[u'Senior Data Engineer\nPinterest - San Francisco, CA\nJuly 2017 to Present\nWorking on Big Data Platform Team to Switch Jobs from Qubole to In-house S3 Cluster. Challenge involves Running Jobs smoothly, Comparable Job run time, solving critical technical issues, data Audit and Validation.\n\nSkills: Amazon S3, Hive, Python and HBase']",[u'MCA in Master In Computer Science and Application'],"[u'Utkal University Bhubaneshwar, Orissa\nJanuary 1990 to January 1993']"
0,https://resumes.indeed.com/resume/ab2ff81e25d34de5,"[u'Data Base Administrator\nHospital Center Marc Jacquet\nJanuary 1999 to January 2003\nActivity: *I Management of Data Base and software\n""Administrative Management of the patient files""\n*/ Installation of workstation in configuration client-server\n/ Assistance and training the users\n\nTechnical Environment: Hewlett Packard IJP9000 with the operating system\nHP-UX (UNIX) - Data Base ORACLE 8- SQL', u'Analyst\nRegional InterHospitalier Center Villeneuve-St\nJanuary 1984 to January 1999\nActivity: */ Development&Maintenance and Installation of the software in COBOL ""Economic and Financial Management""\n*/Assistance and training the users\nTechnical Environment: HONEYWELL BULL DPS7000 with operating system GCOS 7', u'Engineer\nRegional Academic Hospital Center Amiens (Picardie-France)\nJanuary 1978 to January 1982\nat the\nRegional Office ""Organisation&Method and Quality""']","[u'in Application of Computer Techniques', u'', u'in Organisation', u'', u'Engineering in automotive']","[u""Higher School of Electricity (Ecole Superieure d'Electricite)\nJanuary 1983"", u'National Academy of Arts and Trades Paris, FR\nJanuary 1977 to January 1978', u'Conservatoire National des Arts et Metiers de Paris\nJanuary 1978', u'Higher School of Industrial Electricity\nJanuary 1970 to January 1973', u""Ecole d'Electricite Industrielle de Paris\nJanuary 1973""]"
0,https://resumes.indeed.com/resume/449e37e9418b72c6,"[u""Data Engineer\nDenver, CO\nFebruary 2017 to Present\nResponsibilities:\n\u2022 Worked on live 30 nodes Hadoop cluster running CDH 4.4\n\u2022 Worked with highly unstructured and semi structured data of 20TB in size.\n\u2022 Responsible for building scalable distributed data solutions using Hadoop.\n\u2022 Managing data from various file system to HDFS using UNIX command line utilities.\n\u2022 Involved in importing and exporting data between RDBMS and HDFS using Sqoop.\n\u2022 Creating Hive tables on top of the loaded data and writing hive queries for adhoc analysis.\n\u2022 Implemented Partitioning, Dynamic Partition, and Bucketing in Hive for efficient data access.\n\u2022 Performed querying of both managed and external tables created by Hive using Impala.\n\u2022 Developed Pig scripts for data analysis and perform transformation.\n\u2022 Implemented Pig as ETL tool to do transformations, event joins and some pre-aggregations before storing the data onto HDFS.\n\u2022 Developed Spark code and Spark SQL for faster testing and processing of data.\n\u2022 Involved in converting Hive SQL queries into Spark transformation using Spark RDD's, Python.\n\u2022 Developed UNIX shell scripts to load large number of files into HDFS from Linux File System.\n\u2022 Implemented Oozie workflow for Sqoop, Pig and Hive actions.\n\u2022 Exported the analyzed data to the relational databases using Sqoop.\n\u2022 Debugged the results to find if there is any missing at the outcome.\n\u2022 Analyzed large amounts of data sets to determine optimal way to aggregate and report on it.\n\u2022 Involved in performance tuning and fixing bugs."", u""Data Engineer\nArch MI - Walnut Creek, CA\nJanuary 2015 to January 2017\nResponsibilities:\n\u2022 Worked on Cloudera CDH 5.4 distribution of Hadoop.\n\u2022 Extensively working with MySQL for identifying required tables and views to export into HDFS.\n\u2022 Responsible for moving data from MySQL to HDFS to development cluster for validation and cleansing.\n\u2022 Responsible for creating Hive tables on top of HDFS and developed Hive Queries to analyze the data.\n\u2022 Developed Hive tables on data using different SERDE's, storage format and compression techniques.\n\u2022 Optimized the data sets by creating Dynamic Partition and Bucketing in Hive.\n\u2022 Used Pig Latin to analyze datasets and perform transformation according to requirements.\n\u2022 Implemented Hive custom UDF's for comprehensive data analysis.\n\u2022 Involved in loading data from local file systems to Hadoop Distributed File System.\n\u2022 Experience working with SparkSQL and creating RDD's using PySpark.\n\u2022 Extensive experience working with ETL of large datasets using PySpark in Spark on HDFS.\n\u2022 Developed ETL workflow which pushes web server logs to an Amazon S3 bucket.\n\u2022 Developed workflow in Oozie to automate the tasks of loading the data into HDFS and pre-processing with Sqoop script, Pig script, Hive queries.\n\u2022 Exporting data from HDFS environment into RDBMS using Sqoop."", u'SQL Developer\nAhmedabad, Gujarat\nFebruary 2013 to December 2014\nResponsibilities:\n\u2022 Involved in database development and creating SQL scripts.\n\u2022 Involved in Requirement Study, UI Design, Development, Implementation, Code Review, Validation, Testing.\n\u2022 Managed database related activities.\n\u2022 Designed tables and indexes.\n\u2022 Writing SQL queries to fetch the business data.\n\u2022 Developed Views, Sequence and indexes.\n\u2022 Created Joins and Sub queries involving multiple tables.\n\u2022 Analyzing SQL data, identifying issues and modifying the SQL scripts to fix the issues.\n\u2022 Involved in trouble shooting and fine tuning of databases for its performance and concurrency.\n\u2022 Involved in fixing bugs and different forms of testing including black and white box testing.\n\u2022 Handling issues regarding database, its connectivity and maintenance.\n\u2022 Manage the priorities, deadlines and deliverables of individual project and issues related to it.\n\u2022 Effectively prioritize work while considering business need and urgency.\n\u2022 Worked effectively and efficiently on multiple tasks and deadlines and produces high quality results.\n\u2022 Involved in performance improvement of web application for user friendly experience and solving a critical issue that happens in the production environment.']","[u'Masters of Science in Computer Science in Computer Science', u'Bachelors of Engineering in Computer Science & Engineering in Computer Science & Engineering']","[u'Northwestern Polytechnic University\nJanuary 2016', u'Gujarat Technological University\nJanuary 2012']"
0,https://resumes.indeed.com/resume/bebdc3fa1b9668c5,"[u'Data Scientist\nUtilidata, Inc - Providence, RI\nJanuary 2017 to October 2017\n02905', u'Electrical Engineer\nNUWC, Newport\nAugust 2016 to December 2016\nfor the US Navy and her allies. Various projects related to signal processing, control systems, and simulation of undersea sonar systems in Simulink.', u'Senior Engineer\nNavatek, ltd - South Kingstown, RI\nJanuary 2013 to August 2015\n02879', u'Design Engineer\nElectro Standards Laboratories - Cranston, RI\nMay 2012 to December 2012\n02921']","[u""Master's in Signal Processing/Control Systems"", u'Baccalaureate in Electrical Engineering']","[u'University of Rhode Island\nJanuary 2011', u'University of Rhode Island\nJanuary 2010']"
0,https://resumes.indeed.com/resume/eb4a991fdc928cf6,"[u'Data Engineer\nLinkedIn - San Francisco, CA\nMay 2017 to Present\n\u2022 Creating the dataset, moving, creating new subject area, testing, validating, reviewing and committing metrics in UMP onboarding (Hadoop-based platform).\n\u2022 Writing Hive and Pig parameters to the Script for creating tables.\n\u2022 Using Azkaban and Easy data to schedule workflow for Hadoop and Teradata.\n\u2022 Shrunk Teradata queries to reduce execution time and created wiki which explains the technical and business definition of the metrics.', u'Data Analyst\nChoice International Inc - Sunnyvale, CA\nNovember 2015 to October 2016\n\u2022 Provided simulations, data analyses and data visualizations with the R packages such as dplyr, tidyr and ggplot2.\n\u2022 Processed large sets of structured and unstructured data, used machine learning algorithms (K-means clustering, Na\xefve Bayes) to build data models using R packages such as rpart, randomForest, ROCR, tree.\n\u2022 Implemented the Support Vector Machine Learning algorithm to predict rooms occupancy.\n\u2022 Created Tableau Dashboards with interactive views, trends and drill downs along with user level security.\n\u2022 Integrated Hadoop into traditional ETL, accelerating the extraction, transformation, and loading of massive structured and unstructured data.', u'Business Data Analyst\nChoice International Inc - Sunnyvale, CA\nMay 2014 to September 2015\n\u2022 Prepared scripts to ensure proper data access, manipulation and reporting functions with R.\n\u2022 Formulate procedures for integration of R programming plans with data sources and delivery system\n\u2022 Produced databases, tools, queries and reports for analyzing, summarizing and root causing board failure data.\n\u2022 Created pivot tables and charts using worksheet data and external resources, modified pivot tables, sorted items and group data, and refreshed and formatted pivot tables.']","[u'Master of Science in Computer Science in Computer Science', u'Masters of Business Administration in Administration & Management', u'Bachelor of Commerce in Commerce']","[u'International Technological University San Jose, CA\nMay 2018', u'International Technological University San Jose, CA\nSeptember 2015', u'Mumbai University Mumbai, Maharashtra\nMay 2013']"
0,https://resumes.indeed.com/resume/80dd24eb2d476358,"[u'Data Management\nInstitute of Advance Research ,Shanghai University of Finance and Economics - Shanghai\nAugust 2016 to Present\n\u2022 Validate data accuracy and provide the data support across different departments\n\u2022 Analyst Shanghai Migrate kids data and provide the report for the policy maker\n\u2022 Identify qualified industry database according country level data and support quarterly forecasting report\n\u2022 Analyze and edit the paper for international Journal publish', u'Forecast Engineer\nData Management\nSeptember 2015 to June 2016\nMISO (contract through Anchor Point)\n\u2022 Support forecasting activities for operations; maintain load forecasting model\n\u2022 Monitor and evaluate accuracy of load forecasting, apply root cause analysis in fast pace environment\n\u2022 Work cross functionally with operations, support teams, and vendors\n\u2022 Report forecasting and summarized analysis monthly, responsible for forecast performance\n\u2022 Utilize multiple data sources to perform validations, ad-hoc analysis and reporting', u'Statistician\nDow AgroSciences Company\nSeptember 2014 to September 2015\ncontract through Kelly Service)\n\u2022 Perform statistical analysis on field trial data and interpret statistical results\n\u2022 Use power analysis for genomics development and provide the sample size for the field trial\n\u2022 Use LMM and GLMM and different tests to analysis field trial data and provide recommendations\n\u2022 Evaluate the business price model and proposed the leverage price for different Crop Production products\n\u2022 Evaluate National rebate program, proposed new incentive plan for different market', u'Data Analyst\nRegenstrief Institute\nNovember 2011 to November 2013\n\u2022 Extracted clinical trial data from Oracle database by using SQL in TOAD for various studies\n\n\u2022 Identified patients for clinical trial feasibility study and helped designing the recruit plan\n\n\u2022 Provided SAS programming support to Bio-statisticians for analyzing clinical trial data and Validated datasets, tables, listings and reports\n\n\u2022 Used SAS tools like Proc SQL to perform queries, join tables, create and manage tables\n\n\u2022 Involved in reporting statistical findings to work colleagues and senior managers and performed quality control of other programmers work and debug complex programming code\n\n\u2022 Exposure to Bioethics, IRB guidelines, HIPAA guidelines and Regulatory Submissions', u'SAS programmer\nVanderbilt University\nJune 2007 to July 2009\nCenter for Evaluation and Program Improvement (CEPI), Vanderbilt University\n\n\u2022 Generated reports analysis SAS datasets by using procedures such as Proc Transpose, Proc Format, Proc Summary, Proc Means, Proc Freq, Proc Report, Proc Tabulate, Proc Sql, SAS/ODS\n\n\u2022 Performed and managed data transformation and manipulations by using SAS data step statements such as, SAS Formats/Informats, Merge, Data _Null_, Set, Update, Functions and conditional statements, and by applying advanced SAS programming techniques, such as Proc Sql (Join/ Union), Proc Append, Proc Datasets, and Proc Transpose\n\n\u2022 Produced graphs by using SAS procedures Proc Gplot, Proc Freq Plot\n\n\u2022 Implemented various Statistical Analyses using SAS/STAT procedures such as Proc TTest, Proc GLM, Proc Univariate, PROC REG, Proc Mixed, Proc Logistic\n\n\u2022 Ability to work on multiple tasks simultaneously and meet project deadlines']","[u'Master of Arts in Economics', u'Bachelor of Engineering in Automation Industry']","[u'Vanderbilt University Nashville, TN, US\nDecember 2007', u'University of Science and Technology of China Hefei, CN\nMay 1999']"
0,https://resumes.indeed.com/resume/d633616ce1baf89a,"[u'Data Engineer Intern\nEverlasting Wardrobe Inc - New York, NY\nSeptember 2017 to Present\n\u2022 Develop a recommendation system using machine learning neural network for recommending the most suitable product for different customers which greatly increases sales.\n\u2022 Use Python tools (Pandas, FuzzyWuzzy, etc.) to develop data cleansing for ten thousand products data to greatly improve the products query latency. Classify data by features (e.g. Color, Gender, Genre, etc.). Save and import into the relational database.\n\u2022 Design a website called ""brand Glossary"" to provide centralized services (Recommendation System, Size Convertor, etc.) to improve user experiences. Use node.js as backend. Use Bootstrap, jQuery, Javascript as front end. Deploy on Heroku.\n\u2022 Develop Python programs to optimize freight transportation cost. Query Google Map Geocoding and Directions APIs for downloading cooperative companies coordinate. Match customers position to the nearest supply of goods.\n\u2022 Lead the product development processing including code review, unit test, integration test, deployment and production\nsupport.', u'Laboratory Research Assistant\nNew York University\nFebruary 2017 to May 2017\nConstructed a testbed on two servers to generate hundreds of Video on Demand with Docker. Collected video streaming jitter\nand other parameters of servers. Processed thousands of raw data from RTMPDUMP software with python and built modeling\nbased on the pattern of data.\n\u2022 Configured servers on California, Missouri, Georgia and New York and built a distributed video streaming system across the North America geographic region using Geni.net. Configured the network configuration of virtual machines and routers in the\nnetwork to connect the whole network.\n\nACADEMIC EXPERIENCE\nMachine Learning Project 1 - Convolutive neural networking for facial recognition November 2017-December 2017\n\u2022 Dataset from LFW (Labeled Faces in the Wild), which is a database of face photographs designed from face recognition\ncontaining more than 13,000 images of faces.\n\u2022 Use Docker to build environment and python multiprocessing to process an image on each CPU core.\n\u2022 Preprocessing images to detect, crop and center facial parts using landmarks filter sourced from CMU. And randomly flipping\noriginal images left and right, changing the brightness and contrast to increase dataset size.\n\u2022 Using Inception Resnet V1 as convolutional neural network. Set the triple loss as loss function, which could maximize the distance from negative examples. Get classification result by SVM classifier.\n\u2022 Get accuracy about 91% with more than 1,3200 different celebrities. Higher than the original accuracy (90%).\nMachine Learning Project 2 -Neural Network for Hand-write Digit Recognition\n\u2022 Implement the backpropagation algorithm for the neural network for handwritten digit recognition.\nThe written-digit training set has 5000 examples which are collected from the real data, each example is\na 20-pixel by 20-pixel grayscale image.\n\u2022 The neural network has three layers, which has 400 input layer units, 25 hidden layer units, and 10\noutput layer units. The output is a vector with 10 elements, which means the probability of each digit\nseparately. Finally, pick the maximum probability in these result as the predicted value.\n\u2022 The predicted result matches the real result very well with 95.06% accuracy.\nSelf-Learning Hadoop Ecosystem project January 2018 - Present\n\u2022 Develop MapReduce programs to parse the raw data, populate staging tables and store the refined data in partitioned tables in the MySQL database and Hive.\n\u2022 Design Spark script to process data. Develop PySpark to convert raw data in text, Json and ORC files into Resilient Distributed\ndataset, process into stages and store as files on local and HDFS.\n\u2022 Using Sqoop to operate databases and HDFS. Transfer data between Hive, HDFS, local and RDBMs using Sqoop. Create\ndatabases and tables in MySQL databases using Sqoop.\n\u2022 Create steaming processing pipeline using Flume, Kafka and Pyspark streaming. Get users input information and process into 3\nstages then store into the Hive real-timely.\n\u2022 Test raw data and execute performance scripts.\nFull-stack project to build a Spotify-like music website November 2017-December 2017\n\u2022 Design, Develop and Maintain a website that is used by music playing and recommending.\n\u2022 Requirement analysis, unit testing, integration testing, UAT testing and bug tracking and bug fixing.\n\u2022 Improved personalized event recommendation based on playing history and favorite artists and users.\n\u2022 Front end: Design webpage layout using bootstrap. Coding the UI function with JavaScript and JQuery. And pass request to server side.\n\u2022 Back end: Develop PHP programs as back side language to receive client request. Processing the user login request, playing\nmusic request and follow user and artist request. Send query to database and forward result back to webpage.\n\u2022 Data repository: Local MySQL database.']","[u'Master of Science in Electrical Engineering in Electrical Engineering', u'Bachelor of Science in Physics in Physics']","[u'New York University New York, NY\nSeptember 2016 to May 2018', u'Nankai University Tianjin, CN\nSeptember 2012 to May 2016']"
0,https://resumes.indeed.com/resume/d0f54ae4968eb0a7,"[u'Implementation Engineer\nEccovia Solutions - Salt Lake City, UT\nMay 2017 to Present\nSolutions Salt lake City, UT\n\u25cf Design and build software solutions to meet complex requirements of various non- profit\norganizations using SQL Server, HTML, CSS and JS\n\u25cf Exemplify subject matter expertise in relational data models, data analysis, and information system best practices.\n\u25cf Gained good experience in software development life cycle (SDLC) including requirements gathering, business\nanalysis, application design and development, unit testing, maintenance and documentation', u'Data Analyst Intern\nEmperitas.LLC - Salt Lake City, UT\nFebruary 2017 to May 2017\nEmperitas.LLC Salt lake City, UT\n\u25cf Collected data for secondary market research and competitive intelligence\n\u25cf Initiated and managed primary data, collected, and cleaned and analysed the resulting data using SQL and R', u'Graduate Assistant\nUniversity of Utah - Salt Lake City, UT\nNovember 2016 to May 2017\nSchool of business, University of Utah Salt lake City, UT\n\u25cf Promoted a common platform for community of data scientists, researchers,\neducators and government to share, analyze and arrive at insights from different datasets\n\u25cf Studied and shared the recent trends, developments and advancements in the field of intelligence and security informatics\nActivities\nServed as volunteer at Alta View Hospital (Salt Lake City) and American Red Cross.']",[u'MSIS in Information Systems'],"[u'University of Utah Salt Lake City, UT\nAugust 2016 to August 2017']"
0,https://resumes.indeed.com/resume/d2a26ae53947a3e2,"[u'NOC Engineer Tier II\nEze Software - Chicago, IL\nApril 2015 to Present\nResponsibilities included monitoring all network connections, responding\nappropriately to all monitoring alerts globally, and executing changes per the change control process. Was the eyes and ears of the company, and are\ngenerally the first responders to any outage or incident.\n\u25cf Effectively Communicated outages and incidents to the appropriate teams, and follow outlined escalation procedures\n\u25cf Acted as a reliable point of escalation to all groups and offices internal to Eze.\n\u25cf Strong understanding of entire Eze environment\n\u25cf Capable of handling most issues without assistance\n\u25cf Able to create training materials and document new procedures accurately\n\u25cf Capable of training new NOC employees\n\u25cf Leads larger projects of moderate scope and complexity in collaboration with other teams', u'Windstream Hosted Solutions - Franklin Park, IL\nJanuary 2014 to April 2015\nData Center Operations Tech I\n\u25cf Coordination of all telco requirements (voip, pots, modems, emergency\nconnections)\n\u25cf Installed and tested various copper cat6 cables connections, and fiber sm & mm\nconnections using fluke test equipment\n\u25cf Assembly of all data center it equipment, installation, de-installation, relocation,\nconfiguration, troubleshooting, administering and maintaining of servers\n\u25cf Understanding and ensure data center redundancy requirements\n\u25cf Manage third party vendors within a large data center to ensure compliance with change control, specifications and quality standards\n\u25cf Compliance with client support services approved processes\n\u25cf Providing on-site and remote troubleshooting and diagnosis of various servers and networking systems\n\u25cf Performing upgrades, configuration maintenance, and repairs of system\nhardware\n\u25cf Coordinate daily workloads\n\u25cf Install, level, and attach cabinets\n\u25cf Running of electrical whips under raised floor', u'Data Center Site Engineer\nRandstad Technologies - Elk Grove Village, IL\nApril 2013 to December 2013\nPerform structure cabling management / coordination of all telco requirements\n(voip, pots, modems, emergency connections)\n\u25cf Installed and tested various copper cat6 cables connections, and fiber sm & mm\nconnections using fluke test equipment\n\u25cf Assembly of all data center it equipment, installation, de-installation, relocation,\nconfiguration, troubleshooting, administering and maintaining of servers\n\u25cf Understanding and ensure data center redundancy requirements\n\u25cf Manage third party vendors within a large data center to ensure compliance with change control, specifications and quality standards\n\u25cf Compliance with client support services approved processes\n\u25cf Providing on-site and remote troubleshooting and diagnosis of various servers and networking systems\n\u25cf Performing upgrades, configuration maintenance, and repairs of system\nhardware\n\u25cf Coordinate daily workloads']",[u''],"[u'Computer Systems Institute Chicago, IL\nJanuary 2010 to January 2011']"
0,https://resumes.indeed.com/resume/fa6c708b9826fb38,"[u""Data Engineer\nPricewaterhouseCoopers\nApril 2014 to Present\n\u2022 Completed the End to End design and development of masking PII information in non-production environment with encrypted values using big data solution\n\u2022 Created Sqoop batch jobs to export and import data to/from the source MySQL database and developed Hive User defined function for masking PII information\n\u2022 Environment: Hadoop ecosystem, Sqoop, Hive, MySQL\nEDW and Self Service BI Implementation, WSFG, Ohio\nRole: Senior DW/BI Consultant\n\u2022 Came up with the ETL (Informatica) architecture and design for implementing EDW by reading data from close to 26 heterogeneous source systems\n\u2022 Developed and rolled out Data Governance Operating Model\n\u2022 Led Data Standardization workshops to standardize values across multiple source systems in EDW\n\u2022 Set up environment to support development and testing in agile methodology; fine-tuned all ETL processes to improve overall performance to meet defined SLA\n\u2022 Collaborated with project stakeholders and technical teams to identify scope, plan resources for each build and effectively communicate regarding any enhancements, changes, issues and defects that may impact development, data, workflow and/or functionality\n\u2022 Single handedly collaborated the promotion of ETLs into higher environments by coordinating activities with project team, peer development teams, DBA's, operations, release and scheduling teams, and QA team\n\u2022 Environment: Tableau 10, Oracle 11g, Informatica 9.6.1, Cognos 10, Erwin, SCRUM\nEnterprise Data Warehouse Implementation, Compsource Mutual, Oklahoma\nRole: Senior ETL/Reporting Consultant\n\u2022 Implemented Operational Data Store (ODS) and Enterprise Data Warehouse utilizing Guidewire's Datahub and InfoCenter solutions (SAP BODS)\n\u2022 Aligned team input with strategic direction with respect to data architecture and ETL design\\solution for PolicyCenter, ClaimCenter and BillingCenter databases\n\u2022 Operationalized daily delta data loads of both the persistent ODS and the EDW with good performance and high data quality and Fine-tuned all ETL Processes to improve performance\n\u2022 Provided guidance and leadership on specific delivery methodologies for EDW projects, such as technical requirements, design patterns, code reviews and testing procedures\n\u2022 Streamlined the processes, developed guidelines to receive input files/extracts from multiple users/sources to optimize process execution time\n\u2022 Conducted Daily SCRUM meetings and publishing weekly Status report for the customers\n\u2022 Environment: Oracle, SAP BO Data Services , Cognos 10,SQL Developer, TortoiseSVN, Rally, MS Excel, SCRUM, Guidewire infocenter and datahub\nPolicyCenter Data Migration, Capital Insurance Group, California\nRole: Senior ETL/Reporting Consultant\n\u2022 Created Data conversion ETL architecture (SAP BODS) and strategy that can be reused for all lines of business\n\u2022 Designed and developed robust ETL mappings that are flexible to handle data from heterogeneous legacy systems and cleansed and transformed data into Guidewire platform\n\u2022 Analyzed the Legacy systems and came up with field to field mapping between legacy source systems and Guidewire PolicyCenter platform for some of the guidewire entities\n\u2022 Defined an automated strategy to generate Error report which tell the status of data flow during each phase of migration\n\u2022 Environment: SAP BODS, Oracle 11g, SQL Server, Cognos 10, Guidewire Policycenter, TortoiseSVN, Rally, MS Excel\nInformation Management Strategy/Mobilization, ProAssurance, Alabama\nRole: DW/BI Consultant\n\u2022 Led the EDW and BI track and Created an Information Management architecture that would best suit the client's needs\n\u2022 Helped the client in planning the environment set up for EDW (Enterprise Data Warehouse) and FDW (Financial Data Warehouse) & BI Platform\n\u2022 Assisted the client in assessing the vendor that would best suit their EDW/FDW needs by participating in the data model demos and assisted in providing information for the vendors to come up with RFPs\n\u2022 Performed initial analysis of the existing reports and worked on creating a report rationalization approach for the client to rationalize 700+ current state reports\n\u2022 Created a project plan (estimation and budget) to implement EDW/FDW in phases for the multiple LOBs of the client. Also created an Organization chart for the Information management track\n\u2022 Environment: Oracle 11g, SQL Server, Cognos 10, MS Office"", u'Senior Associate\nCognizant Technology Solutions - Hartford, CT\nSeptember 2007 to April 2014\nRole: Senior ETL Consultant\n\u2022 Profiled and Analyzed the legacy data and came up with field to field mapping between legacy source systems and Guidewire entities to migrate data in sprints\n\u2022 Defined a conversion strategy to convert data in sprints which could be used by business for validation and also be used by other work streams like Configuration and Integration\n\u2022 Architected solutions for source systems integration, designed robust ETL mappings (Informatica) which would be flexible to handle data from heterogeneous legacy systems, validated, cleansed and transformed data into Guidewire staging tables\n\u2022 Executed the integrity checks defined in Guidewire and tackled the integrity check errors to prior to data load\n\u2022 Defined an automated strategy to generate Audit report, Error report and Reconciliation report which tell the flow of data during each phase of migration\n\u2022 Defined a Roll-out strategy to have a seamless migration of large volume of data\n\u2022 Environment: Informatica 9.6.1, Oracle 11g, SQL Server, MS Office, Guidewire Claimcenter, Autosys, Unix shell scriping, PL/SQL, Agile/SCRUM\nMSSB Merger, Hartford, Connecticut\nRole: Senior ETL Consultant\n\u2022 Served as Technical lead for the MSSB Merger project\n\u2022 Architected solutions for the merger, designed ETLs to automate the merger process and to support multiple mock runs requested by MSSB\n\u2022 Worked closely with Business Analysts/MSSB to understand the requirements and design as per the requirements\n\u2022 Provided design approach, guidance for functional and End to End design of the project\n\u2022 Environment: Datastage 7.5.3, Oracle 9i , Autosys, Unix shell scriping, PL/SQL\nOracle to Datastage Migration, Hartford, Connecticut\nRole: Senior ETL Consultant\n\u2022 Designed and reviewed ETL technical specification/Jobs/load strategy and their inter dependencies during migration from Oracle Pl/SQL to Datastage jobs\n\u2022 Analyzed and automated the existing manual processes in Oracle and improved the efficiency by designing Datastage parallel jobs\n\u2022 Coordinated with testing team to identify any difference in comparison of reports and fixed the issues\n\u2022 Designed Test Strategy for the Assembly Testing and triage Defect management\n\u2022 Identified the performance bottlenecks and address them by applying the best practices\n\u2022 Coordinated the Production implementation and integration with the existing systems\n\u2022 Environment: Datastage 7.5.3, Oracle 9i , Autosys, Unix shell scriping, PL/SQL\nDatastage Upgrade, Hartford, Connecticut\nRole: ETL/Reporting Lead Developer\n\u2022 Prepared the detail internal project plan and maintained the plan and served as the single point of contact from the offshore team and co-ordinated project reviews and maintained minutes of all meetings\n\u2022 Upgraded and migrated Datastage jobs from repository version 7.5 to version 7.5.3 and took care of all the inter-dependencies\n\u2022 Coordinated the testing of the ETL jobs/output in both the versions of Datastage and found the incompatible mappings and fixed those\n\u2022 Trained the production support and enhancement team on the newer version of the Datastage\n\u2022 Environment: Datastage 7.5.3, Oracle 9i , Autosys, Unix shell scriping, PL/SQL\nElectronic Data Interchange, Hartford, Connecticut\nRole: ETL/Reporting Developer\n\u2022 Provided on-going support and maintained EDI system that received data feeds from external vendors and generated position, transaction, price and commission feeds for broker dealers\n\u2022 Investigated the business issues and provided solutions/response to queries/requests/issues raised by Business teams/external vendors. Also analyzed the support issues/services requests and automated manual processes and saved time and cost to a great extent\n\u2022 Identified the performance issues in the Datastage batch jobs and fine-tuned the performance to meet SLAs defined by external broker dealers to send\n\u2022 Performed capacity planning, workload management and assignment of tasks to the team members\n\u2022 Environment: Datastage 7.5.3, Oracle 9i , Autosys, Unix shell scriping, PL/SQL']",[u'Bachelor of Engineering in Electrical and Electronics in Electrical and Electronics'],"[u'Anna University Chennai, Tamil Nadu\nMay 2003 to May 2014']"
0,https://resumes.indeed.com/resume/18cfc5d676f65b79,"[u'Data Analytics Consultant\nUniversity of Connecticut - Hartford, CT\nAugust 2017 to November 2017\n\uf0a7 Designed multiple solutions for client to improve overall business operations and marketing strategies\n\uf0a7 Developed pricing model to maximize revenue and provide consistency across all 8 franchises\n\uf0a7 Redesigned membership offerings to increase customer base and average customer lifetime value\n\uf0a7 Recommended action plan to reduce number of membership cancellation by 20%\n\uf0a7 Developed dashboard using Tableau to track KPIs, saving costs worth 1 FTE\n\uf0a7 Analyzed and proposed most effective marketing strategies based on ROI and member demographics', u'Senior Systems Engineer\nInfosys - Bangalore, India\nNovember 2010 to May 2014\n\uf0a7 Analyzed customer feedback system to categorize issues, define SLA and communication of ETA to customer\n\uf0a7 Implemented A/B testing to analyze web market for client\u2019s website and performed market analysis\n\uf0a7 Increased sales by redesigning client website to provide customer focused content and promote product marketing\n\uf0a7 Enhanced user experience on front-end by implementing visual charts and graphs using Tableau\n\uf0a7 Managed team of 5 and successfully delivered a project of customer login module creation\n\uf0a7 Developed statistical models to classify and predict the membership tier using various customer features\n\uf0a7 Created automation for daily task assignment process and track percentage completion and ETA\n\uf0a7 Created a tool for automated reporting of daily and monthly metrics']","[u'MS in Business Analytics and Project Management', u'Bachelor of Engineering in Computer Science']","[u'University of Connecticut Hartford, CT\nJanuary 2017 to April 2018', u'RGPV Bhopal, India\nAugust 2006 to June 2010']"
0,https://resumes.indeed.com/resume/eb661eac949668da,"[u'Data Analyst\nLiya International, LLC - Texas, US\nMay 2013 to Present\nResponsibilities\n\u2022 Collected customer information and analyzed customer data to improve the business.\n\u2022 Analyzed business volume and generated business report on a monthly basis.', u'Senior Data Analyst\nCTMG Co., Ltd. - Dalian, China\nOctober 2008 to October 2009\n\u2022 Conducted regression analysis and generalized linear models based on the data warehouse by using SAS and MS Excel.\n\u2022 Analyzed business volume and marketing cost on monthly basis, interpreted statistical results and prepared cost control reports to management.\n\u2022 Optimized forecasts continuously based on business development trend.\n\u2022 Prepared presentations based on summaries and reports to leaders and group members.', u'Software QA Engineer\nBT FRONTLINE (Dalian) Co Ltd - Dalian, China\nAugust 2006 to August 2008\n\u2022 Executed manual and automatic functional tests, designed the test process and test cases for 20 modules, completed 150 test cases per module and summarized the test results.\n\u2022 Conducted relational database data mining by using Access and SQL script.\n\u2022 Tracked the problems of each test and provided the analysis and solutions.\n\u2022 Designed test plans, analyzed the requirements, data collections and summarized performance of tests.\n\u2022 Created the test scripts, prepared the test data, and simulated the real executing environment.']","[u'MS in Applied Statistics', u'BS in Management of Engineering']","[u'New Jersey institute of Technology Newark, NJ\nJanuary 2011 to January 2012', u'Shenyang University of Technology ShenYang, China\nJanuary 2002 to January 2006']"
0,https://resumes.indeed.com/resume/3a9fda0c8d3e8da9,"[u'Data Base Administrator\nHospital Center Marc Jacquet\nJanuary 1999 to January 2003\nActivity: */ Management of Data Base and software\n""Administrative Management of the patient files""\n*/ Installation of workstation in configuration client-server\n/ Assistance and training the users\n\nTechnical Environment: Hewlett Packard HP9000 with operating system\nITP-UX (UNIX) - Data Base ORACLE 8- SQL', u'Analyst\nRegional InterHospitalier Center Villeneuve-St\nJanuary 1984 to January 1999\nActivity: *I Development&Maintenance and Installation of the software in COBOL ""Economic and Financial Management\n*/Assistance and training the users\nTechnical Environment: HONEYWELL BULL DPS7000 with operating system GCOS 7', u'Engineer\nRegional Academic Hospital Center Amiens (Picardie-France)\nJanuary 1978 to January 1982\nat the\nRegional Office ""Organisation&Method and Quality""']","[u'', u'in Application of Computer Techniques', u'', u'', u'', u'']","[u'International Institute of Management Paris\nJanuary 2004', u""Higher School of Electricity (Ecole Superieure d'Electricite\nJanuary 1983"", u'Institute for Higher Studies of Organisational Techniques\nJanuary 1977 to January 1978', u""Ecole d'Electricite Industrielle de Paris), >Ecole Superieure en Genie Electrique Rouen\nJanuary 1977"", u'Higher School of Electrical Engineering\nJanuary 1970 to January 1973', u""Institut d Etudes Superieures des Techniques d'Organisation""]"
0,https://resumes.indeed.com/resume/df29b1d57b329503,"[u""Lead Data Engineer & Project Manager\nTresata - New York, NY\nJuly 2016 to Present\n\u2022 Manage implementations of Tresata software in the Financial Services industry\n\u2022 Responsible for sprint planning and project scoping for financial service clients\n\u2022 Write and execute client contracts\n\u2022 Responsible for all client communication across Tresata's financial services clients\n\u2022 Report client opportunities and progress directly to CEO\n\u2022 Help Tresata's financial service clients with on premise capacity planning\n\u2022 Mentor and mange two people"", u""Data Engineer\nTresata - New York, NY\nJuly 2015 to July 2016\n\u2022 Install and configure Tresata software at client sites\n\u2022 Write and test new features for Tresata Software\n\u2022 Build and present demos of Tresata software for existing and potential clients\n\u2022 Migrate Tresata software into the client's production environment"", u'Solutions Team Intern\nTresata - Charlotte, NC\nJune 2014 to August 2014\n\u2022 Write user manuals for all components of Tresata Software\n\u2022 Script and present demos for potential clients\n\u2022 Use Tresata software to predict upsets during March Madness\n\nTECHNICAL SKILLS\n\n\u2022 Scala\n\u2022 Scalding\n\u2022 Spark\n\u2022 Hadoop\n\u2022 Linux, iOS, Windows\n\u2022 Elasticsearch (in dev and production clusters)\n\u2022 Drake\n\u2022 Bash Scripting\n\u2022 Microsoft Office\n\u2022 Atlassian JIRA\n\u2022 Atlassian Confluence']",[u'BA in Mathematics and Economics in Mathematics and Economics'],"[u'University of North Carolina at Chapel Hill Chapel Hill, NC\nMay 2015']"
0,https://resumes.indeed.com/resume/4af8ae6e3bce2511,[],[],[]
0,https://resumes.indeed.com/resume/ae697c7cfa7b9a67,"[u'Data Scientist\nAffine Analytics - Bengaluru, Karnataka\nSeptember 2016 to June 2017\n\u2022 Built a threefold recommendation engine for a retail client and also assisted the member scoring model team to utilize the recommendation engine to improve their model efficiency by capturing the loyal customers.\n\u2022 Was involved in visualization of the recommendation engine using state of the art dimensionality reduction techniques like PCA and t-SNE.\n\u2022 Experimented on retail data to figure out purchasing patterns using Recurrent Neural Networks in Keras.', u'Data Scientist\nShieldsquare - Bengaluru, Karnataka\nMarch 2016 to May 2016\nPerformed data analysis on the web traffic data of a client and designed a generic algorithm by figuring out the patterns using\nhistograms to reduce the web traffic from bots.', u'Engineer\nAlcatel-Lucent - Bengaluru, Karnataka\nDecember 2013 to March 2016\n\u2022 Developed a Linear Regression Model which predicts the performance parameters of a Network Management System.\n\u2022 Automated the web page testing using Selenium with Java along with generation of test reports.\n\u2022 Was involved in the end to end Alarm Testing for the devices that were newly introduced into the network and generated test\nreports for the same along with deployment of various modules into customer environment.\n\u2022 Developed reports and dashboards which monitor the performance of various parameters in a network by pulling in data from HBASE using REST queries.\n\nPUBLICATION\n\u2022 Distributed Vector Representation Of Shopping Items, The Customer And Shopping Cart To Build A Three Fold']","[u'Master of Science in Data Science in Data Science', u'Bachelor of Engineering in Computer Science in Overall Percentage Score']","[u'Indiana University Bloomington, IN\nMay 2019', u'Bangalore Institute of Technology Bengaluru, Karnataka\nJune 2013']"
0,https://resumes.indeed.com/resume/f584ca4eed5f6587,"[u'IT Data Intern\nNational Life Group - Addison, TX\nMay 2017 to Present\n\u2022 Working independently on designing automated voice and Chat Bot assistant for NLG on AWS & MS Azure cloud platform\n\u2022 Implementing image comparison using AWS Rekognition\u2019s deep learning API for image-based user authentication\n\u2022 Developed Azure App service and Bot Service in Node JS, C# to define Bot Clients and to consume NLG RESTful web services\n\u2022 Created Chat Bot in AWS Lex and Microsoft Bot Framework with Slack, Skype for Business, Direct Line & Web chat channel\n\u2022 Created application in Node JS in AWS Lambda function to consume NLG RESTful web services and interpret into textual message which can be delivered to Alexa, Lex, and Microsoft Bot framework\n\u2022 Defined user authentication using OAuth 2.0 protocol with Azure AD to serve the information to authorized users only\n\u2022 Designed models on Microsoft cognitive LUIS AI platform to understand the NLP to be used in Bots communication\n\u2022 Prepared Intent, custom slots, and utterances for LUIS and Alexa skills Kit for Bot design and Alexa response design\n\u2022 Designed Alteryx workflows and fetched data with secure OAuth access over Rest API to use in Bot application', u""Software Engineer\nLarsen & Toubro Infotech Ltd - Mumbai, Maharashtra\nNovember 2012 to May 2016\nAccomplished initial training with above average performance and systematic architectural project designing\nled to the best performance award amongst 80 colleagues\n\u2022 Worked in several roles such as software developer, team lead, project lead backup, business analyst for a\nproject comprises of 100+ team members to improve work efficiency and to reduce project costing\n\u2022 Contributed in most of the SDLC phases like requirement analysis, high-level design, detail level design,\nprototype designing, screen development, resource task planning, complexity point estimation, test case\npreparation, and impact analysis for a project which will be used globally 5000+ customers\n\u2022 Implemented independently several modules of web application, operated on database designing, database\nobject optimization, and its backup activities for which I was honored thrice by the star performer award\n\u2022 Trained 25+ team members on database standardization and optimization at the beginning of designing phase of a project which resulted in maintaining standards across the project\n\nCOMPETITIONS, LEADERSHIP EXPERIENCE & ORGANIZATIONS\n\u2022 Involved in volunteering in orphanage home as well as leprosy center\n\u2022 Led the team of 4 members in L&T Infotech during web application development.\n\u2022 Organized and hosted musical competition events in college's fun Ar\xeate 12, Cultural Fest, Apr 2012\n\u2022 Played taekwondo game at the district and state level, 2001\n\u2022 Secured silver medal in 1st Raigad District open Taekwondo Championship, 2001""]","[u'M.S. in Information Technology & Management', u'B.E. in Computer Engineering']","[u'The University of Texas at Dallas Dallas, TX\nMay 2018', u'The University of Mumbai\nMay 2012']"
0,https://resumes.indeed.com/resume/11a16815ac667428,"[u""Business Analyst\neBay - Bellevue, WA\nOctober 2017 to Present\nUtilize Business Intelligence tools (Tableau and SQL) to create effective reports of KPI's to drive strategic\ndecisions.\n\u2022 Facilitate team's weekly reviews on the metrics of forecasting outlook of eBay's business in China and Europe.\n\u2022 Effectively communicate forecasting results across business operation, promotion and social advertising\nteams.\n\u2022 Balance the needs and requirements from the internal and external stakeholders and manage their\nexpectations.\n\u2022 Engage with the key stakeholders (sales, marketing and product management) to understand and meet their\nanalytics needs on a continual basis."", u'Data Analyst\nFINRA - Rockville, MD\nAugust 2015 to April 2017\n\u2022 Detected potential fraud by implementing Latent Semantic Analysis(L.S.A.) in R and Anaconda Python\n\u2022 Successfully built User Interface (U.I.) by using Shiny Application to detect text similarity for generic\ndevelopment. The application was accepted by management board and was widely used by department\n\u2022 Unified item contents using Natural Language Processing (NLP) and text mining\n\u2022 Conducted random forests algorithm on survey data to determine optimal predictors for identifying returned\nbroker candidates\n\u2022 Performed cheating analysis using logistic regression with shrinkage methods (ridge regression, lasso), KNN to find suspected candidates in R\n\u2022 Identified overlap program versions through nonlinear modeling in R using parametric (logistic model) and non-parametric (smoothing splines)approaches\n\u2022 Applied time series analysis to forecast future exam volumes for CE Online in different vendors\n\u2022 Created ad-hoc reports using Oracle SQL and summarized results in pivot table\n\u2022 Performed pricing/volumes analysis with data visualization in Tableau\n\u2022 Generated monthly utilization reports including pivot chart, pivot table, slicer, financial analysis and mathematical calculation in Excel to drive business decision\n\u2022 Researched on Python packages (scikit-learn, pandas, numpy), Hadoop platform, Spark, Pig, Hive and evaluated potential benefit to technology team', u'Data Engineer\nBaanyan Software Services, Inc\nAugust 2014 to August 2015\nComposed SQL query in Oracle SQL Navigator to update solution software and improved Utilization Report in\nPivot Table\n\u2022 Coded in Java/Groovy to parse JSON / XML files\n\u2022 Performed Time Series analysis in R to predict iLink and Netlink customer volumes\n\u2022 Conducted data visualization with Tableau/Excel\nMarketing Data Analyst\n\u2022 Pulled large volume of data using query tools( SAS/SQL) from insurance database\n\u2022 Utilized SAS to clean and manipulate large datasets\n\u2022 Imported Excel format insurance data into MS Access and established Enhanced Entity Relationship (EER)\ndiagram\n\u2022 Conducted machine learning based predictive model to drive business decision and predict the future market\ntrend\n\u2022 Used Output Delivery System (ODS) facility to write an analytical report directing SAS output to HTML file which includes statistical tables, analysis summary, and data interpretation\n\u2022 Analyzed the quality of the output from analytical reports and discussed with business users for solutions to improve data quality\n\u2022 Quickly learned and mastered the Aviation/Marine/Specialty insurance policies\n\u2022 Participated in global conference calls to review progress of ongoing quality control of delivered reports']","[u'B.A. in Statistics & Mathematics', u'in Statistics']","[u'University of California, Berkeley Berkeley, CA\nAugust 2012 to May 2014', u'Santa Monica College\nFebruary 2010 to June 2012']"
0,https://resumes.indeed.com/resume/d47110efb3059e83,"[u'Data Analyst\nCisco Systems, Inc - Research Triangle Park, NC\nSeptember 2015 to Present\n\xb7 Work in tandem with internal business partners and sales development teams to gather detailed program requirements, manage multiple programs from creation to execution and all other phases in project life cycle.\n\xb7 Act as the primary data support for extremely large sales compensation programs, process $10M bonus payouts per quarter on average for more than 12K global sales agents.\n\xb7 Analyze millions of rows finance data from multiple relational databases, manipulate data by creating tables, views, writing complex queries, modifying stored procedures in MS SQL server\n\xb7 Update dashboard, maintain tracking reports, generate summary files by applying program rules and calculation logic, and provide ad-hoc reports to finance managers and operation directors.', u'Quality Assurance Engineer\nFoxconn/Q-Edge - Durham, NC\nApril 2015 to September 2015\n\xb7 Assist to manage incoming, in-process, outgoing inspections and supplier quality performance at a $360M electronics OEM.\n\xb7 Assist to implement company-wide operations and procedures to obtain ISO 9001:2008, China Compulsory Certification (CCC S&E), CQC audits and customer audit.\n\xb7 Reduce defective component inventory by 5% and process induced defects through close work with suppliers to redefine and streamline Return Material/on-site-verification process, work directly with customer/supplier manufacturing engineers to ensure quality specification precision.\n\xb7 Increase First Pass Yield by 6%, review, modify and approve standard operation procedures, initiate/implement on-the-job training, perform first article inspection on new products.']","[u'Master in Industrial and Systems Engineering', u'Bachelor in Industrial Engineering']","[u'North Carolina State University Raleigh, NC\nAugust 2012 to May 2014', u'Nanjing University of Technology Nanjing, CN\nSeptember 2008 to July 2012']"
0,https://resumes.indeed.com/resume/93ac818fbb214ebf,"[u'Data Engineer\nInnovative Information Technologies\nJuly 2017 to Present\nValue at Risk(VaR) is a popular mechanism used to measure risk of an investment. On-Demand Risk Management solution gives the ability to run VaR on a portfolio for any given date. It also provides the ability to compare portfolio VaR values across different days.\nResponsibilities:\n\u2022 Experience in importing and exporting the data from Relational Database Systems to HDFS by using Sqoop.\n\u2022 Created Hive tables using Avro format with Snappy compression.\n\u2022 Migrating various HiveQL queries into Spark SQL for generating data analysis reports with increased performance.\n\u2022 Developed Pig Scripts to do transformations and filtering on data residing in HDFS.\n\u2022 Used Spark API over Cloudera Hadoop YARN to perform analytics on data in Hive.\n\u2022 Used Jira for bug tracking and GIT for code management.\n\u2022 Continuous monitoring and managing the Hadoop cluster through Cloudera Manager.\nEnvironment: HDFS, Hive, Sqoop, Oozie, HBase, Spark SQL, Eclipse, Zookeeper, Spark core, Java, Git, Shell scripting, Cloudera.', u'Hadoop/Spark Developer\nGeeks-V Solutions - Glendale, CA\nAugust 2016 to July 2017\nPhysician Market Solver is a claims-based analytic solution that tracks and measures physician referral leakage. It provides information surrounding service lines, geographic markets as well as insights on referral networks and practice patterns. This will help healthcare marketers to understand their physician marketplace in order to make strategically informed decisions.\nResponsibilities:\n\u2022 Extensively used big data analytical and processing tools Hive, Spark Core, Spark SQL for batch processing large data sets on Hadoop cluster.\n\u2022 Extensively utilized Partitioning and Bucketing concepts in Hive and designed both Managed and External tables in Hive for optimized performance.\n\u2022 Migrating various HiveQL queries into Spark SQL for generating data analysis reports with increased performance.\n\u2022 Automated the jobs with Oozie and scheduled them with Oozie coordinator.\n\u2022 Developed a common framework to import the data from Teradata to HDFS and to export to Teradata using sqoop.\nEnvironment: CDH5.8, HDFS, Apache Spark, Map Reduce, Hive, Sqoop, Oozie, Impala, Teradata, Linux, Java, Eclipse.']","[u'Masters in Information Systems Technologies', u""Bachelor's in Mechanical Engineering""]","[u'Wilmington University', u'Jawaharlal Nehru Technology University']"
0,https://resumes.indeed.com/resume/359c44ecdb3df495,"[u'Associate Data Analyst\nStevens Institute of Technology - Hoboken, NJ\nMarch 2017 to Present\n\u2022 Performed daily reconciliation of sales for on-campus and off-campus merchants using MS Excel and visualizing results using Tableau\n\u2022 Developed Scripts to automate data transfer method for Stevens summer student database maintenance using Google scripts\n\u2022 Performed Off Campus Merchant revenue forecasting, market segmentation and conjoint analysis using SAS\n\u2022 Developed a project charter in collaboration with IT and other 3rd party companies for process optimization and evaluation', u'Software Engineer\nHSBC Global Technologies\nAugust 2015 to June 2016\n\u2022 Worked on AGILE methodology for Project Management and software development of Software products for online transaction process in Middle-East and implementing Six Sigma principles to improve business performance and streamline the customer review process\n\u2022 Implementing SCRUM product development techniques and IT strategies for online transaction and spearheaded in-depth analysis of transaction data that led to significant decrease in operating costs by 6%\n\u2022 Worked on Mainframe and AS400 for back-end development of Internet banking website of HSBC and performed database operations using SQL and other IBM tools for maintaining credit records', u'Data Engineer Intern\nEAGLET Gateway\nAugust 2014 to May 2015\n\u2022 Implement explicit real-time security functions, tracking resource utilization and retrieving system information on a board based Network.\n\u2022 Used Multi-threading approach to accommodate simultaneous service for up to 50 to 100 users in the network\n\u2022 Implemented socket programming for communication between server and client for small office networks\n\u2022 Maintained and updated documentation for the application and reviewed the progress\n\u2022 Implemented loading strategies to load access management data, logs to report user activity and behaviour in Excel.']","[u'Master of Engineering in Engineering Management', u'Bachelors of Electronics and Telecommunication Engineering in design, Industrial Management']","[u'Stevens Institute of Technology\nMay 2018', u'University of Pune\nJune 2015']"
0,https://resumes.indeed.com/resume/a63ea1c66a5e6d67,"[u'Test Engineer\nTech Mahindra - Redmond, WA\nJanuary 2017 to Present\nLab / Field Work\n\u2022 LTE RF/Protocol testing on R&S CMW500 for AT&T and T-Mobile carrier certification testing.\n\u2022 Performed end to end debugging and analysis of customer issues by reproducing them in the Lab.\n\u2022 Resolution of the call box configuration issues and feature updates in coordination with the respective vendors.\n\u2022 System level testing on Connectivity (Wi-Fi, Bluetooth).\n\u2022 On site coordination with a team of Software Developers to prepare a test suite as per updates in the test plan requirements.\n\u2022 Performed first-level debugging of issues found and generating test reports and defect reports on JIRA.\n\u2022 Performed weekly regression testing on latest firmware release for different OEMs such as Samsung, Motorola, Huawei, ZTE in GSM,\nUMTS, WCDMA and LTE network.\n\u2022 Performed GCF/PTCRB Certification for Conformance & Pre-Conformance Testing.\n\u2022 Created and Executed Field Test Plan for KPI, Full, Regression and Sanity test scenarios.\n\u2022 Performed field/Lab test cases for IRAT Handovers, CSFB, Reselection, SRVCC, CA, IOT (inter-operability Testing).\n\u2022 Ensured deliverables are prepared to satisfy the project requirements and schedule.', u'Data Analyst\nGTL Americas - Plano, TX\nNovember 2016 to December 2016\nField Work\n\u2022 Monitored 4G data service, handoff between UMTS and GSM upload/download activities, ping, short & long calls on multiple systems\nand VoLTE calls.\n\u2022 Collected and processed the required RF field data in order to help bring new sites on-air.\n\u2022 Collected and processed system baseline data in order to help troubleshoot drop-call and block-call problem areas. Determined if new\nsites are functioning properly during the initial baseline process.\n\u2022 Determined whether the new sites should remain on-air or be handed over for further RF optimization and Traffic data analysis.\n\u2022 Worked with the RF Engineer to perform real-time troubleshooting to help resolve problems like drop-call and block-call etc.\n\u2022 Created drive routes for the sites to be tested using MapInfo and IMNOS.\n\u2022 Created reports for post processing after the drive test.\n\u2022 Tested Tower cells for LTE Downlink, Uplink, VOLTE and CSFB throughputs for AT&T project.\n\u2022 Installed the telecom equipment (Frequency Scanner, GPS, USB Hubs and UEs).\n\u2022 Collected Stationary and Mobility test data and checking throughputs for stationary as well as Mobility tests.\n\u2022 Worked on tools such as JDSU, NEMO Outdoor, and QXDM.']","[u'Masters in Electrical Engineering in Electrical Engineering', u'Bachelors in Electronics and Instrumentation in Electronics and Instrumentation']","[u'University of Texas at Arlington Arlington, TX\nJanuary 2016', u'Maharshi Dayanand University Rohtak, Haryana\nJanuary 2010']"
0,https://resumes.indeed.com/resume/ec8cfa63f6329929,"[u'Data Analyst\nAmerican Airlines - Charlotte, NC\nMay 2017 to December 2017\nTyped work intructions from engineering order into work cards\n\u2022 Inserted graphics, figures and tables in studio\n\u2022 Added reference links to graphics, tables and figures\n\u2022 Compared engineering project to engineering order\n\u2022 Created monolithic pdf for engineering project\n\u2022 Provided timely and thorough communication to the team of engineers on the status of tasks and projects.', u'Data Coordinator\nHireRight, LLC - Charlotte, NC\nFebruary 2017 to May 2017\nEnter data into excel for processing\n\u2022 Perform quality checks over information processing tasks\n\u2022 Communicate internally with team and team lead daily', u""Data Migration Engineer\nSnyder's-Lance - Charlotte, NC\nSeptember 2016 to January 2017\nHelped Pioneer the data migration project in PLM.\n\u2022 Proficiently worked independently and in a team setting\n\u2022 Entered formulas/attached formuluas, ingredients, raw materials, shipping and handling instructions into system\n\u2022 Helped Transfer data from TDM and excel spreadsheets into PLM\n\u2022 Created specs for new products that were not accessible in the system"", u'Data Entry Specialist\nIron Mountain\nSeptember 2015 to September 2015\n\u2022 Entered medical files of patients into computer system with precision and accuracy\n\u2022 Placed files in a box after entering them into the system and stacked them on pallets', u""Operations Specialist\nWells Fargo - Charlotte, NC\nAugust 2013 to July 2015\n\u2022 Processed W2's during tax season, manual rendering with mail\n\u2022 Worked with checks, statements and vouchers for different clients\n\u2022 Entered dates and job numbers in Excel to insure proper delivery time was correct\n\u2022 Responsible for all of the paper work for New Customer Account Kits\n\u2022 Performed quality checks on all of the folders to ensure that the paper is in the correct order and without blemish\n\u2022 Made sure that contact information is clearly visible to customers for any questions or concerns Basic math calculations and counting is involved"", u'Data Entry Specialist\nProvidastaff - Huntersville, NC\nOctober 2014 to April 2015\n\u2022 Processed documents by reviewing data for deficiencies and making corrections where needed\n\u2022 Entered applicant data by inputting alphabetic and numeric information\n\u2022 Verified entered customer and account data by reviewing, correcting deleting and reentering data; combining data from systems when account information is incomplete\n\u2022 Created spreadsheets on Excel\n\u2022 Merged files to eliminate duplication of data\n\u2022 Analyzed information and paying close attention to detail is necessary', u'Data Entry Operator\nJP Morgan Chase Bank - Charlotte, NC\nMarch 2013 to October 2013\nEntered data with software 3i Infotech, verifying check amounts, entering stub amounts, verifying vouchers, and entering tax payers information with precision and accuracy', u""Sales Associate\nMacys - Bloomington, IL\nNovember 2011 to July 2012\nOrganized clothes, pricing merchandise, checking out customers, answering phone calls, make payments on customers' accounts, ordering merchandise from other stores, and providing quality customer service"", u'Data Entry Operator\nBridgeway - Bloomington, IL\nMarch 2011 to May 2011\nEntered data on 1040 tax forms into the computer system, typing 8000 keystrokes and above, paying attention to detail and accuracy']","[u'', u'']","[u'Southern Illinois University Edwardsville, Heartland Community College', u'Normal Community West High School']"
0,https://resumes.indeed.com/resume/790293b525423a3d,"[u""Product Design Engineer\nTranswall Office Systems\nApril 2015 to Present\nResponsibilities include new product design, updating old products and drawings, as well\nas entering data and product codes\n\u25cf Using SolidWorks and Microsoft Excel to complete tasks such as creating BOM's,\ncataloging custom model configurations, and storing parts' revisions\n\u25cf Helping the shop program CNC machines\n\u25cf Helping the manufacturing plant create faster production processes and put together new\nassembled product lines\n\u25cf Helping the CAD/ Architecture team with new drawings\n\u25cf Running a 3D printer for samples and prototypes\n\u25cf Attending and managing weekly meetings"", u'Engineer\nThe Nanz Company\nFebruary 2014 to December 2014\nResponsibilities include designing, drawing and updating various parts and products\n\u25cf Using both SolidWorks and Graphite programs to complete tasks and assignments\n\u25cf Working with project managers and sales to insure product placement and fitment\n\u25cf Using FileMaker to create and update product codes', u""Data entry based\nTelerX - Bethlehem, PA\nDecember 2012 to March 2013\nBethlehem, PA\nCall Center Operations\nDecember 2012 - March 2013 (Seasonal Position)\n\u25cf Responsibilities include providing superior customer service using effective interpersonal\nskills, which includes building rapport, listening effectively, remaining tactful and patient,\nusing appropriate questioning techniques, and empathizing when appropriate\n\n\u25cf Navigate within and properly use program systems to answer customer inquiries and complaints, explain products and services, and/or process customers' orders\n\n\u25cf Data entry based off of the days calls and customer requests\n\n\u25cf Deliver and maintain exceptional quality for each call/chat/order handled\n\n\u25cf Escalations, unresolved issues, and system deficiencies to the designated resources"", u'Retail Salesperson\nThe Gap Store - Easton, PA\nApril 2008 to June 2010\n3\n\n\u25cf Operated cash register and performed other duties as assigned.\n\n\u25cf Performed other duties such as merchandising, customer service, sales, etc.']","[u'Associate of Applied Science degree in Computer Aided Design', u'Certification']","[u'Northampton Community College Bethlehem, PA\nMay 2012', u'Kutztown University\nMarch 2006 to June 2007']"
0,https://resumes.indeed.com/resume/32f589c04bae0cc8,"[u'Data Analyst\nINDEPENDENT PROJECTS\nMarch 2018 to March 2018\nIndependent projects on machine learning, statistical inference and data investigation.\n\u25cf Tableau Story (Titanic): Designed Tableau story which details passengers survival dependence on age, gender, class and family on board. Used feedback to to enhance the design and make it user friendly.\n\u25cf Enron POI: Identified Employees (person of interest) who may have committed fraud based on the public Enron\nfinancial and email database. Analyzed, created and engineering features, tuned ML algorithm and validated results.\n\u25cf Explore and Summarize Wine: Analyzed two data sets of white and red wines to predict the quality of wine.\nExplored with uni, bi and multivariate and predicted two significant variables to predict wine quality.\n\u25cf Openstreetmap data wrangling: Used the New York region xml data file (2.8 gb) for investigation. Audited, cleaned and organized the data file using Python before storing in MongoDB for further analysis.\n\u25cf Machine learning: Explored Titanic survivor factors on data from Kaggle. Cleaned, formatted data, and used Random\nForest algorithm in R to predict survival of passengers based on their title, family, age and gender. Evaluated that\nwomen, children, people with family and title had higher chance of survival.\n\u25cf Data investigation: Analyzed baseball data to explore relation between player position and performance metrics and their salary earned using Python Numpy, Scipy, and Pandas libraries.', u'Research Assistant\nSPACE@VT, VIRGINIA TECH - Blacksburg, VA\nAugust 2008 to January 2014\nInvestigated ionospheric plasma in near Earth space environment with active experiments of aerosol and HF heating.\n\u25cf HF heating: Analyzed 100GB data with fast Fourier transform using IDL, to classify waves. Discovered novel H+\nwaves, postulated theory of origin with verification.\n\u25cf Aerosol: Developed computational fluid model using Fortran to simulate aerosol release in space plasma. Confirmed the results with linear theory using Matlab.', u""Software Engineer\nNETSOFT USA/YODLE/TRANZACT - New York, NY\nOctober 2006 to July 2008\nWorked as a software engineer for three clients.\n\u25cf Implemented a multi-threaded web service application that pulls all of Aetna's health care records (ASP .Net, C#)\n\u25cf Responsible for managing and providing complete support to the phasing out of legacy lead gen system. Developed a secure centralized login for users into old and new systems, and built stand-alone applications for monitoring and reporting (ASP .Net, C#, Java).\n\u25cf Implemented a web service that handled scheduling and rescheduling of all DIRECTV orders. It involved tasks such as request validation, error handling, scheduling appointments, etc. (ASP .Net, C#).""]","[u'Data Analyst Nanodegree', u'PhD in Electrical Engineering', u'MS in Electrical Engineering', u'BE in Electronics and Telecommunications']","[u'Udacity\nMarch 2018', u'Virginia Polytechnic Institute and State University\nMay 2013', u'Syracuse University\nMay 2006', u'University of Mumbai\nJune 2003']"
0,https://resumes.indeed.com/resume/02a8fe2bd23acab0,"[u'Data Engineer\nVerizon Wireless\nJanuary 2006 to January 2017\n-IPv4/v6 Control\nDeveloped web tool so called FNE (Fixed Network Engineer) to pull network traffic data + Proficient in following programming into SQL database. Using KPI (Key Performance Indicators) FNE will generate a languages:\ndashboard to monitor network capacity which helping engineers to avoid network - Pascal Cobol Fortran PHP\noverloaded. FNE tool also automates the process of IP (v4/v6) assignment for SoCal cell HTML Java Scripts\nsites network. RF engineering, Transport engineering, System Performance engineer,\n+ Well understanding in operation\nCell Tech, Operation Cell Tech groups are using this tool to speed up the process of activating cell sites for service specially for quick set up temporary cell sites so called\nsystem:\nCOLT/COW (mobile cell site) that are used for any quick/temporary special event. - Window Unix/Linux (Ubuntu)\n+ Database Structure:\nPast: Crystal Cathedral Seagate Hard Drive Arco Refinery: Data Analyst / - MySQL\nLAN-WAN Administrator. - Microsoft Access\n- Pick\n- Excel']",[u'BS in Computer Science'],"[u'University of California Irvine Irvine, CA']"
0,https://resumes.indeed.com/resume/d0b9d483d2e3db10,"[u'DATA SCIENTIST\nIkvox Software - Hyderabad, Telangana\nJanuary 2016 to December 2016\n\u2022 Derived actionable insights by processing large sets of structured and unstructured data by performing Statistical Analysis.\n\u2022 Processed the raw data from CSV files into organized form by applying Data cleaning techniques using R and Python.\n\u2022 Built predictive models by applying supervised and unsupervised machine learning algorithms like Regression, SVM & KNN.\n\u2022 Deployed the models into production using frameworks like Flask and Django on Microsoft Azure & AWS.\n\u2022 Created visualizations using BI tools like Tableau, Qlikview and Zoho reports for communicating insights to Stakeholders.\n\u2022 Automated the process of feedback evaluation by calculating the sentiment of the users based on text and numerical data.', u'SYSTEMS ENGINEER\nInfosys Ltd - Hyderabad, Telangana\nAugust 2014 to January 2016\n\u2022 Designed web & mobile applications using HTML5, JavaScript, JSP, CSS, BootStrap and developed PL/SQL scripts by writing procedures for retrieving and scheduling data imports from relational databases.\n\u2022 Ensured the best performance of applications by identifying bottlenecks and devise a solution to solve it. Analyzing performance of applications and servers by using various monitoring tools.']","[u'Master of Science in Computer Science in Computer Science', u'in Electrical & Electronics Engineering']","[u'UNIVERSITY OF NORTH CAROLINA AT CHARLOTTE Charlotte, NC\nMarch 2017 to Present', u'JAWAHARLAL NEHRU TECHNOLOGICAL UNIVERSITY Hyderabad, Telangana\nAugust 2010 to May 2014']"
0,https://resumes.indeed.com/resume/5a95a95aee21ecc9,"[u'Research Assistant\nEconomics Department, Rice University\nApril 2017 to January 2018\nFinished the R version realization for the empirical analysis in the paper ""Panel Estimators and the Identification of Firm-Specific', u'Rotational Data Engineer\nCIDI, O2O Big data center, Shanghai, China - Shanghai, CN\nDecember 2015 to May 2016\n\u2022 Assisted Crawling product data from top 15 e-commerce websites.\n\n\u2022 Created real machine learning models to classify the target clients and Evaluated the model with the designed metrics.\n\n\u2022 Programmed related static web pages for the consultant platform.', u'Data Analyst\nZheshang Securities, Research Centre - Hangzhou, CN\nJuly 2015 to September 2015\nChina\n\u2022 Assisted in ferrous metal products research via investment value analysis.\n\n\u2022 Applied time series analysis and forecast upon stock prices and yields trend via GARCH family.\n\n\u2022 Perfected our database of ferrous metal industry by tracking and prepossessing related important news and events and compiled\n\nweekly and monthly analytical reports.', u""Data Analyst\nNational Bureau of Statistics - Xi'an, CN\nMay 2015 to July 2015\nChina\n\u2022 Led a team to complete metrics design and data collection for over 500 small and micro enterprises.\n\n\u2022 Conducted Feature Engineering and established panel data regression model.\n\n\u2022 Clustered enterprises, compared to historical data and gave recommendation on policy support.""]","[u'Master in Statistics in Computational Economics', u'Bachelor in Statistics in Financial Engineering', u'in Chinese Young Development Program']","[u'Rice University Houston, TX\nAugust 2016 to May 2018', u""Xi'an Jiaotong University Xi'an, CN\nSeptember 2012 to June 2016"", u'Columbia University New York, NY\nJune 2014 to August 2014']"
0,https://resumes.indeed.com/resume/4bd73b57341484cc,"[u'Data Engineer\nCrowdAnalytix\nMarch 2016 to August 2016\nText Mining / Automated Product Cataloging for leading E-commerce companies: Designed high precision\nprediction scripts that predict product attribute values from product description based on pattern recognition\nusing regular expressions and rule-based programming in Perl. This approach ensures high precision which is\nbusiness critical for e-commerce companies and has an added advantage over the traditional machine learning\napproach as it has the flexibility to enforce certain complex rules and priorities.\n\u2022 Real-time Platform / Backend Engineering / Android: Implemented an asynchronous high-performance\nWeb-socket server that maintains an all-time up connection with Android devices for real-time analytics\nusing Netty, an event-driven network application framework in JAVA. Also built the underlying network\nlogic for the corresponding Android application and pipelining using Kafka.\n\u2022 Web-Crawling / Data Warehousing: Developed numerous web crawlers and parser wrappers in Perl / Python.\nApplications to carry out any required transformations and normalization of large scale data with Apache Spark to source data for machine learning applications followed by tweaking the predictions from the machine\nlearning models to achieve a higher precision.\n\u2022 Automation and data processing and pipe-lining: Built a dynamic code generator script in Perl that takes data\nfields as input and generates an equivalent fault-tolerant parser script using regular expressions. The generated\nscript is then used for low-memory JSON conversions with nested data extraction support.', u""Software Engineer\nJustDial\nJuly 2014 to March 2016\n\u2022 DevOps / Web-crawling:\n* Developed over 500 web-crawlers to source data for India's leading local search engine, JustDial. The\ndeveloped crawlers involved fetching and structuring of dealer information, product cataloging for the online shopping domain, movie, events and exhibition information for the entertainment domain, food\nmenu and hotel information etc..\n* Designed an automated web-crawler system that deploys and initiates crawlers on distributed clusters\nwhich accelerated the crawling speed substantially.""]","[u'Master of Science in Computer Science', u'Bachelor of Engineering in (B.E.), Computer Science']","[u'California State University Long Beach, CA\nJanuary 2018', u'Visvesvaraya Technological University\nJune 2014']"
0,https://resumes.indeed.com/resume/7b54ddf28db498e1,"[u'System Administrator\nE-commerce.com - Columbus, OH\nFebruary 2017 to Present\nSystem administration with Windows, VMWare, Hyper-V, XenCenter, NetBackup, SAN and NAS', u'System Administrator\nInstalled Building Products - Columbus, OH\nApril 2013 to November 2016\nSystem administration with Windows, VMWare, Hyper-V, XenCenter, NetBackup, SAN and NAS, Microsoft Exchange, SQL', u'Data Center Engineer\nChase Bank - Columbus, OH\nOctober 2010 to April 2013\n\u2022 VMWare, Hyper-V, and XenCenter creation and administration\n\u2022 Active Directory administration\n\u2022 Deployment and decommissioning of physical hardware\n\u2022 Cable and server physical troubleshooting\n\u2022 Infrastructure server and cabling\n\n\u2022 AD and Group Policy administrator\n\u2022 Microsoft Exchange administrator\n\u2022 VMWare creation and administration\n\u2022 SAN, NAS, and backup administrator\n\u2022 Desktop, laptop, network, server, printer, phone support\n\n\u2022 Infrastructure server and cabling\n\u2022 Cable and server physical troubleshooting\n\u2022 Deployment and decommissioning of physical hardware']","[u'in Chemical Engineering, Russian, French, Japanese']",[u'Colorado State University']
0,https://resumes.indeed.com/resume/067d7bbdb2e641f1,"[u""Data Analyst\nCapital One, Richmond\nSeptember 2017 to Present\n\u2022 Monitored customer behavior and their credit score before targeting them with offers for credit cards as part of our data risk and compliance procedures.\n\u2022 Mitigated the data issues checking invalid customer entries using SQL Server and Excel and made the data more efficient.\n\u2022 Analyzed heavy spenders on our card portfolio and provided them cash-back categories on the most used ones, so we can retain them and acquire their trust.\n\u2022 Performed data manipulation with Numpy and Pandas in Python and used Python to automate AWS processes.\n\u2022 Used Numpy for numerical analysis and automated the existing scripts for performance calculations using Numpy and SQL alchemy.\n\u2022 Worked with business owners to get reporting requirements on C1's compliance and regulatory monitoring reports.\n\u2022 Created dashboards and identified key metrics using Tableau and published them on Server which helped end customers to understand data on the fly with usage of quick filters.\n\nEnvironment: SQL Server, Teradata, Tableau, MS Excel, Python, AWS, UNIX"", u'Data Engineer\nCitibank\nNovember 2016 to August 2017\nResponsibilities\n\u2022 Responsible to gather requirements from the Testers and format them as per the business needs, model the requirements to match the current architecture and to successfully implement business logic.\n\u2022 Involved in creating Hive tables, loading with data and writing hive queries which will run internally in map reduce way. Hands-on experience in Partitions, Bucketing concepts in Hive, designing both Managed and External tables in Hive to optimize performance.\n\u2022 Solved performance issues in Hive and Pig scripts with understanding of Joins, Group and aggregation and how does it translate to MapReduce jobs.\n\u2022 Wrote programs in Python for automating AWS processes with Redshift and Teradata databases.\n\u2022 Used Pandas API to put the data as time series and tabular format for eastern timestamp data manipulation and retrieval. Used Scikit-learn library for classification and clustering of large datasets.\n\u2022 Used matplotlib, seaborn and bokeh to visualize data with and generate graphs with pandas.\n\u2022 Performed troubleshooting, fixed and deployed many Python bug fixes for both customers and internal customer service team. Used Python to automate data scripts from the web and created scripts in Python for manipulating, parsing and converting data in excel sheets.\n\u2022 Designed and developed yearly, monthly reports related to the marketing and financial departments by using SQL, MS Excel, MS Access, and Teradata.\n\nEnvironment: Teradata, SQL Server, Tableau, MS office, MS Excel, Python, AWS, Windows, UNIX', u'Data Analyst\nNationwide Insurance - Columbus, OH\nFebruary 2014 to September 2016\nResponsibilities\n\u2022 Designed and developed various monthly and quarterly business monitoring excel reports by writing Teradata SQL and using in MS Excel pivot tables.\n\u2022 Implemented ETL using SQL Server Integration Services (SSIS) in OLTP and Data warehouse environment. Defined cubes, dimensions, user hierarchies, aggregations in an OLAP environment (SSAS). Migrated data from Oracle to SQL Server data warehouse and to generate the reports using SSRS for different SSAS Cubes.\n\u2022 Created Set, Multiset, Volatile and Derived table and wrote shell scripts to automate data loads using Teradata utilities such as Mload and Fload. Optimized high volume tables in Teradata using various join index techniques, join strategies and hash distribution methods.\n\u2022 Developed UNIX shell scripts to run batch jobs and communicate logs to the users. Automated Teradata SQL scripts in UNIX by using shell scripting. Scheduled Teradata BTEQ Scripts in UNIX using CRONTAB.\n\u2022 Used Python programs to automate the process of combining the large SAS datasets and Data files and then converting as Teradata tables for Data Analysis.\n\u2022 Worked in AWS Environment for loading data files from Legacy Unix Systems to Unix system.\n\u2022 Worked with Lists, Dictionaries, Sets and tuples in Python.\n\u2022 Automated process of generating the Excel reports reading from Teradata tables, SAS Data Set using Pandas Data frame, calculation within cell and formatting like adjusting Cell Width, combining the cells etc.\n\nEnvironment: SQL Server, Teradata, Tableau, Python, AWS, UNIX Shell Scripts, SAS, CSV, TSV', u'BI Developer\nComcast Cable Communication Inc - Philadelphia, PA\nMarch 2012 to January 2014\nResponsibilities:\n\u2022 Analysis of functional and non-functional categorized data elements for data profiling and mapping from source to target data environment.\n\u2022 Developed scripts using Teradata advanced techniques like Row Number and Rank Functions, developed Teradata SQL scripts using various characters, numeric and date functions, developed Teradata SQL scripts using OLAP functions like rank and rank () Over to improve the query performance while pulling the data from large tables.\n\u2022 Implemented indexes and collected statistics while creating tables and worked on performance tuning and query optimization for increasing the efficiency of the scripts, extracted data from existing data source and performed ad-hoc queries by using SQL.\n\u2022 Written several shell scripts using UNIX Korn shell for file transfers, error logging, data archiving, checking the log files and cleanup process.\n\u2022 Developed Enterprise Application in an AGILE environment using Python. Wrote Python scripts to fetch data from one data source and write it to different data source.\n\u2022 Performing data management projects and fulfilling ad-hoc requests per user specifications by utilizing data management software programs and tools like Perl, Toad, MS Access, Excel and SQL.\n\u2022 Written SQL scripts to test the mappings and Developed Traceability Matrix of Business requirements mapped to Test scripts to ensure any Change Control in requirements leads to test case update.\n\nEnvironment: - SQL Server, MS office, Business Objects Clear Quest, Erwin, Clear Case, Teradata', u'SQL Developer\nAccenture - Bengaluru, Karnataka\nAugust 2011 to February 2012\nResponsibilities\n\u2022 Writing (Back-end) SQL code to implement business rules through triggers, cursors, procedures, functions, and packages using SQL Plus Editor or TOAD.\n\u2022 Created SQL Packages for easier application design and improved Performance. Created SQL Procedures using Native Dynamic SQL and Cursor Variables.\n\u2022 Migrated data from Flat Files to Oracle database using SQL Loader. Developed Ad-hoc reports as per the requirement in Oracle Reports.\n\u2022 Fixed customized dashboards in the charts and associated with the slider control using Interactive reporting.\n\u2022 Involved in trouble-shooting production database problems and report problems.\n\u2022 Developed user documentation for all the application modules. Responsible for writing test plan documents and unit testing for the application modules.\n\nEnvironment: SQL, SQL Plus, SQL Loader, Oracle, UNIX, Windows.']","[u'Master of Science in Data Analytics', u'Bachelor of Technology in Electronics and Communications']","[u'George Mason University\nDecember 2016', u'SRM University\nMarch 2012']"
0,https://resumes.indeed.com/resume/eb48b83cd1055b72,"[u'Data Scientist\nPrecise Leads - New York, NY\nOctober 2014 to Present\nFurnish stakeholders with analytics, insights and recommendations enabling effective strategic planning across all areas. Report directly to CEO and involved with many key business decisions on many levels of the organization.\n\uf0b7 Analyzed and processed complex data sets for deep-dive statistical analysis and predictive modeling including techniques such as random forest, clustering, association rule analysis, logistic regression and hypothesis testing.\n\uf0b7 Entrusted with lead cost and pricing adjustments in a real time bidding environment.\n\uf0b7 Spearheaded analytics for our call center division by identifying KPIs and optimizing conversion rates.\n\uf0b7 Discovered industry trends to enhance marketing strategies and improve customer growth across our B2B network comprised of over 1500 insurance agents.\n\uf0b7 Managed relationships with our affiliates and implemented Service Level Agreement models to establish benchmarks in meeting our performance expectations of these partners.\n\uf0b7 Forecasted seasonal changes in consumer traffic and distribution corresponding to each insurance vertical.\n\nQuantifiable Results:\n\uf0b7 Propelled call transfer revenue by 110% from 2016 to 2017\n\uf0b7 Identified inefficiency in marketing campaigns that saved over $2 million in profits in 2015 alone\n\uf0b7 Boosted customer retention rate by 10% year over year', u'Network Operations Center Engineer\nF1 TECHNOLOGY GROUP - New York, NY\nJune 2006 to Present\nResolve enterprise IP outages and manage internal change control as a key member of the client service/support team providing 24/7/365 oversight for private equity network operations.\n* Review, analyze, and prioritize outages based on various client-side and internal factors, and monitor ticketing and alert systems.\n* Manage the flow of information in and out of the office, and foster productive working relationships with clients, contracted service providers, and field personnel.']","[u'Master of Science in Statistics', u'Bachelor of Arts in History']","[u'RUTGERS UNIVERSITY New Brunswick, NJ', u'NEW YORK UNIVERSITY New York, NY']"
0,https://resumes.indeed.com/resume/10ffcbb9e6e01279,"[u'Data Analyst\nEazyfi - Mumbai, Maharashtra\nJuly 2016 to December 2016\nSpearheaded in-depth analysis of retail market customer data to increase customer\nacquisition by 15%\n\u2022 Utilized Python and tableau to generate and present insights to management\n\u2022 Worked closely with CEO to make marketing strategies based on the insights\n\u2022 Developed new areas of sales through mining social network data thus improving visibility by 20%', u'Co-Founder\nMumbai, Maharashtra\nAugust 2014 to October 2016\n\u2022 Created an online platform for students to get high quality videos of Physics\n\u2022 Designed, Developed and Maintained a WordPress website using social media extensively\n\u2022 Collaborated with educators to record videos and get it online\n\u2022 Integrated online sales of DVDs for rural students and generated revenues worth two lakhs', u""Assistant System Engineer\nTata Consultancy Services - Mumbai, Maharashtra\nJuly 2012 to July 2014\nWorked on the production environment of Adobe Systems for Enterprise Application\nIntegration middleware using TIIBCO\n\u2022 Designed and developed integration services using TIBCO Business Works 5.x, Designer\n5.x and EMS 4.x\n\u2022 Created new dashboards in Adobe Connect Service Gateway for senior management to easily and quickly gather metrics from a variety of interfaces to analyze and effect staff\nperformance\n\u2022 Performed functional testing on Hendrix, the call center interface for adobe employee\nPROJECTS\n( 1ST SEM) 1) Mattel HBR Case Analysis: To solve manufacturing irregularities that resulted in massive\ntoy recalls through Transaction cost economics, Strategy maps, Service innovations, Risk\nanalysis and creating Service blueprints. Proposed technological solutions keeping in mind\nmanagement and a detailed report explaining possible scenarios.\n2) Database Project: Data modelling, database design, requirement analysis, designing logical\nmodels, and ERDs of a dummy credit card department of a bank to generate meaningful\ninformation for management from the data.\n3) IOT project: Remote monitoring system for temperature and humidity using IOT sensors.\nStored the data for further analysis in Microsoft azure database and created dashboard using\npower BI. Meaningful application for farmers and scientists.\n4) Startup project: Created business plan, prototypes, marketing plan, financial budget and forecasting and pitch for an App 'Share' to increase community sharing of commodities.""]",[u'Master of Science in Management Information System'],"[u'UNIVERSITY AT BUFFALO, THE STATE UNIVERSITY OF NEW YORK\nMay 2018']"
0,https://resumes.indeed.com/resume/c3168f7f2b75fc69,"[u'Operation support engineer\nAugust 2015 to April 2017\nMonitoring Exploratory and development wells Algeria\n\nTechnical Training History\nDate: Course:\nJuly 2004', u'Data analyst\nSeptember 2007 to August 2015\nExploratory and development wells Libya, Chad and Algeria', u'TDC Data Engineer\nBasic Mud - Tunis, TN\nDecember 2006 to May 2007\nSfax, Tunis\n\nReserval +\nDundee Scotland\n\nALS3 B\nTunis\n\nGeonext 2 (GN4)\nELC traning, France\n\nSafety Training History\nDate: Course:\nMay 2004\n\nJune 2007 2013 H2S, and fire fighting. Libya.\n\nOff shore safety, Egypt\n\nOff shore safety, tripoli libya.']",[u'BA in Geology'],[u'seven april university libya\nApril 2004 to September 2007']
0,https://resumes.indeed.com/resume/f83082555a3c804d,"[u'Founder\nreadygroup.io\nNovember 2017 to Present\nLeading a team of 3\nCreated a web property to help people adopt Amazon Alexa Devices for home automation.', u""Data Engineer\nZeeto.io\nJune 2017 to Present\nSupporting Product, Engineering, Sales, Media, Data Science, and Accounting Departments.\nDeveloped Docker Java back-end api's for time-series reporting and campaign management.\nDeveloped Docker React front-end api's for campaign management.\nBuilt EC2 Kafka Connect data pipeline consuming 50+ Kafka topics dumping to AWS S3 data lake.\nIntegrated server-less services for continuous data ingestion using AWS Lambda.\nEnded dashboard/reporting requests with a business-friendly data schema available on Looker."", u""Data Analyst\nZeeto.io\nMay 2016 to June 2017\nProvided data analytics and insight across 4 properties, 600+ coreg campaigns, and 500+ linkout campaigns in 50+ verticals.\nAutomated offer performance projections using Python.\nDocumented monetization algorithm, configurations, procedures, and best practices.\nImplemented data storage, ETL's, procedures, and purging for new property deployments.\nCreated and maintained monetization flow detail reports using Pentaho Report Designer.\nAssisted deploying monetization codebase using Jenkins CI.\nCreated reporting aggregation tables using Pentaho Data Integration.\nSimulated user behavior and revenue generating events using a Monte Carlo Model."", u'Junior Data Analyst\nZeeto.io\nMay 2015 to May 2016\nPerformed descriptive and inferential statistical analysis of daily web traffic and A/B tests.\nIntegrated Google Adsense APIs to MySQL database using Python.\nDeveloped real-time analytic reporting dashboards using Tableu.\nCross validated and audited Mongo DB, MySQL DB, and Google Analytics data.\nAssisted creating performance driven critical alerts using MySQL and Python.\nCreated Entity Relationship Diagrams for new data warehouse schema.\nDeveloped Data Flow Diagrams for business processes.\nDeployed campaign creative testing in a Bayesian model framework.\nInvestigated campaign CPL requirements and expected lead volume.', u'Management Associate\nGerdau Reinforcing Steel\nDecember 2013 to January 2015\nIntensive two-year, entry-level program with multiple projects in all aspects of rebar placing industry.\nDeveloped accurate cost models based on project scopes.\nDiscovered newfound profitability in multi-story commercial buildings.\nInterfaced with customers, owners, laborers, management, accounts payable, foreman, and engineers.\nProjects\nReadyGroup.io Founder\nLuisMeraz.com Personal Site\nAlexaReady.com Wordpress blog for the Amazon Alexa Devices\nNational HS Graduation Analysis Submitted web app for Everybody Graduates AT&T Dev Post competition.\nCamera Location App Developed a camera app that allows sharing superimposed photo geolocation with friends.\nComputational Analysis Created MATLAB scripts to simulate open chains, bar linkages, and compression of materials.']",[u'B.S. in Mechanical Engineering'],"[u'University of California San Diego, CA\nSeptember 2009 to June 2013']"
0,https://resumes.indeed.com/resume/9ca504c222335c07,"[u'Data Engineer\nSchneider Electric - Andover, MA\nMay 2017 to March 2018\nResponsibilities:\n* Provided data analysis for client engagements across a range of technical industries.\n* Assisted to build analytic tools to manage data and streamline data analyses using R and SQL Server.\n* Develop framework, metrics and reporting to ensure progress can be measured, evaluated, and continually improved.\n* Support the development of performance dashboards on Tableau that encompass key metrics to be reviewed with senior leadership.\n* Built complex SQL queries, generated reports from different sources for various business needs.\n* Provided sales forecasting using predictive models and what-if scenarios.\n* Created statistical models (Machine Learning algorithms) and cluster groups (K-Means) for target marketplaces\n* Worked closely with data scientists to assist on feature engineering, model training frameworks, and model deployments.\n* Used Python to extract information from XML files and wrote Python scripts to clean raw data.\n* Performed data extraction and data wrangling using Pandas and Numpy modules in Python.\n* Maintain database instances and execute SQL queries for data validation.\n* Performed aggregation and missing value imputation using SQL and Python.\n* Experience in using PyCharm editor for writing the python scripts which helped in code analysis, debugging and integrated unit testing.\n* Prepare workflow document reports for future reference.', u""Data Analyst\nEMC Corporation - Bengaluru, Karnataka\nFebruary 2015 to July 2016\nResponsibilities:\n* Analyzed business data on daily basis and resolved data discrepancy to improve data quality\n* Assisted database team in revamping the database from scratch and created ETL workflows on the existed data.\n* Manipulated various tables within Oracle SQL to obtain desired data for application.\n* Predicted customers preference based on order history to maximize sales efficiency using logistic regression.\n* Identified the products with high risk of loss and updated inventory using MS Excel and MS Access.\n* Supported customer service department and increased customers' positive feedback rate from 87% to 93%\n* Worked in conjunction with development team and senior management in preparation of data mapping reports, validation reports, and business reports.\n* Contributed in ensuring all implementations consistent with Software Development Lifecycle (SDLC) standards.\n* Created Dashboards in Tableau using features such as worksheets, functions and actions.\n* Trained and mentored other Data Analysts\n* Involved in troubleshooting and fine-tuning of databases for performance and concurrency.\n* Initiated pilot projects for various data science tasks including advanced NLP and neural nets.\n* Worked in Cross Functional team environment and capability to handle multiple projects simultaneously\nincluding Onsite Offshore co-ordination"", u'Data Analyst\nCognizant - Bengaluru, Karnataka\nNovember 2013 to January 2015\nResponsibilities:\n* Actively monitored database for any issues and troubleshooting of level 1 issues.\n* Programmed complex SQL queries for Views, Functions, Triggers, and Stored Procedures.\n* Involved in designing and developing reports in MS SQL Server environment by developing SSRS reports and managing dashboards on Office 365.\n* Created SSAS Tabular Models for sales reporting.\n* Created incident documentation that includes information about the results of the root cause analysis, approvals.\n* Involved in performance tuning of ETL jobs and SQL queries.\n* Performed and visualized analysis of the export-import databases with Tableau, Pivot tables in Excel.\n* Produced reports and/or data sets for ad hoc requests.\n* Trained new team members to understand marketing concepts and processes and to effectively use Excel and other tools.\n* Executed SQL Queries with all types of Joins to validate data from backend.\n* Performed regression analysis to see the factors causing payment defaults using Excel.\n* Provided 24 X 7 Production Database on Call Support on rotation basis', u'Systems Engineer\nCognizant - Bengaluru, Karnataka\nMay 2012 to October 2013\nStorage/Database\nResponsibilities:\n* Managed storage environment by coordinating with developers, DBAs, and other teams to determine storage requirements and to optimize it.\n* Allocated/de-allocated space from various storage products, fabric troubleshooting, and basic switch configurations, and handled all incidents/requests within defined SLAs under ITIL framework.\n* Modeled EMC Storage solutions after analyzing and assessing customer data and performance criteria, and providing them to Presales field within defined SLAs and quality standards.\n* Involved in coordination with other cross-functional teams to execute critical production Change, Problem and Incident Response procedures towards compliance and assuring seamless Service Delivery.\n* Effectively size newer and better storage solutions for core EMC arrays, Array upgrades, competitive array replacements using Engineering best practices.']","[u'Masters in Computer Science in Computer Science', u'Bachelor of Technology in Information technology']","[u'University of North Carolina Charlotte, NC\nJanuary 2017', u'Vellore Institute of Technology Vellore, Tamil Nadu\nJanuary 2012']"
0,https://resumes.indeed.com/resume/922f460b037cca83,"[u'Programmer', u'Big Data Engineer']",[],[]
0,https://resumes.indeed.com/resume/88242ab0affdc26a,"[u""Graduate Data Engineer\nCincinnati Children's Hospital\nSeptember 2017 to April 2018\n\u2022 Analyzed and parsed huge CSV files using python.\n\u2022 Efficiently combined CSV files which totaled to 1 terabyte using python.\n\u2022 Improved the performance (10x) of the retrieval by applying parallel processing on the sliced results.\n\u2022 Currently working on deploying ES Server on the cluster and then perform data visualization."", u'Software Engineer\nOracle Corporation\nJuly 2016 to August 2017\n\u2022 Worked on Subscription Plan Management (SPM) application, which supports billing of Oracle Cloud Subscriptions.\n\u2022 Modified code to improve the time complexity for some of the background processes of this product using Java, Hibernate\nand PL/SQL.\n\u2022 Written many PL/SQL Stored Procedures and modified Triggers in this application.\n\u2022 Developed the Oracle Cloud Subscription Billing and Usage Reconciliation code which triggers emails if there is any\nmismatch in the subscription credits.\n\u2022 Took initiative to design the database schema for one of the core web projects named ""SPM Dashboard"".\n\u2022 Designed User Interface for this project using Oracle JET, HTML, CSS, jQuery, Oracle Database and Rest Web Services.']","[u""Master's in Computer Science in Computer Science"", u'Bachelor of Engineering in Computer Science in Computer Science']","[u'University of Cincinnati Cincinnati, OH\nAugust 2017 to April 2018', u'Osmania University Hyderabad, Telangana\nOctober 2012 to May 2016']"
0,https://resumes.indeed.com/resume/cb3a5ac94493f81d,"[u'Data Center Hardware Engineer\nBloomberg L.P - New York, NY\nAugust 2015 to Present\nTeam Leader: Nelson Valentin, nvalentin@bloomberg.net (212)-802-6600\n\u25cf Install, configure and upgrade servers from Sun, IBM, HP and others as well as map out, run and connect all LAN and SAN network cabling and hardware\n\u25cf Work with vendors such as Cisco, Juniper, HP, SUN, IBM, Radware and more to resolve RMA issues\n\u25cf Analyzing & monitoring system logs files to determine hardware, software & performance issues\n\u25cf Writing BASH scripts for automating processes\n\u25cf Working with Bloomberg proprietary portals, switches and (BDMS) Bloomberg data management Services\n\u25cf Troubleshooting server hardware via CLI\n\u25cf Troubleshooting network connections/hardware via CLI\n\u25cf Troubleshooting SAN connections\n\u25cf Configuring management interfaces, network interfaces']","[u""Bachelor's in Information Technology and Informatics""]","[u'Rutgers University - School of Communication and Information New Brunswick, NJ\nSeptember 2012 to August 2015']"
0,https://resumes.indeed.com/resume/caa70c77e088cb5f,"[u'Oracle Database Administrator\nRegional Electoral Court of Parana - TRE-PR - Curitiba, PR\nJanuary 2013 to July 2017\nProvided 24x7 production database administration and on call support in off hours\nWorked with Oracle technical support for critical ORA-600 database errors by raising service requests to analyze the root cause\nPerformed day-to-day tasks such as creating and modifying database objects such as tables, indexes, views, sequences, functions, procedures, triggers and packages. In addition, assigning roles and privileges to database users and providing security for the databases\nManaged Oracle 11g to Oracle 12c multitenant Container and deleting legacy databases\nMainly managed database backup and recovery and recovered database as needed\nPerformed regular database healthy checkup and managed troubleshooting for database contention and performance bottlenecks. Monitored storage usage and worked with application team to add more datafiles/tempfiles as needed\nMonitored Oracle database performance tuning, and responded to alerts using OEM (Oracle Enterprise Manager)\nSupported development teams by tuning SQL queries, and implementing tables, views and best indexing solutions for applications in DEV/TEST/PROD environments\nDeveloped automatic database administration routines in Linux shell scripting and Python\nMonitored Oracle 11g Real Application Clusters in production environment\nUsed RMAN to perform daily online/offline backups and recover database as needed\nInstalled and configured VMWare templates with Oracle 12c and Oracle 11g software installed on Red Hat Enterprise Linux systems, in order to offer rapidly launching of new Oracle database servers for DEV/TEST/PROD environments\nCreated an Oracle Enterprise Manager Cloud Control 13c in a Linux system running Oracle WebLogic Application Server, in order to centralize management of all Oracle IT resources, such as DEV/TEST/PROD database servers\nInstalled and managed Oracle Application Express and Oracle REST Data Services in Oracle 11g DEV/TEST/PROD databases\nImplemented RMAN backup methodology project and Oracle Flashback on production databases. Designed, documented and executed backup and recovery Linux shell scripts in a centralized RMAN catalog', u'Oracle Data Warehouse Engineer\nSuperior Electoral Court - TSE - Bras\xedlia, DF\nJune 2006 to December 2012\nDesigned and developed ETL routines with PL/SQL and Linux shell scripts for data consolidation of the National Electoral Registration System (150+ millions Brazilian electors)\nDesigned and developed PL/SQL packages and ETL/ELT mapping in Oracle 10g Warehouse Builder and Oracle 10g Data Integrator to capture data related to Brazilian official elections\nInstalled, configured and managed Oracle Warehouse Builder, Oracle Data Integrator and Oracle BIEE repositories for DEV/TEST/PROD environments\nSupported optimization of SQL query performance for materialized views, implementation of partitioning on tables and global/local indexes, fact tables data compression and use of parallelism on SQL queries for Biometric and Georeferencing Data Marts projects']","[u'PgDip in Master Business Administration - IT Governance', u'Bsc in Computer Science']","[u'Euro American College Bras\xedlia, DF\nJanuary 2010 to January 2011', u'Federal University of Goias (UFG) Goi\xe2nia, GO\nJanuary 1989 to January 1993']"
0,https://resumes.indeed.com/resume/09ee5eb732c83c11,"[u'IT Security Specialist\nMerck Inc\nAugust 2017 to Present\n\u2022 Wide knowledge of current IT security tools \u2013 hardware and software security\n\u2022 Good understanding of IT business security practices and procedures\n\u2022 Familiarity with TCP/IP network topology (LAN/WAN), routers, and related protocols\n\u2022 Ability to design and distribute materials promoting best security habits\n\u2022 Ability to design, produce, and deliver compelling security-related training classes\n\u2022 SQL, .Net, MySql,HTML, Java.', u'Data Conversion Engineer\nEllkay Inc\nJanuary 2014 to January 2015\n\u2022 Managed security appliances and resolution.\n\u2022 Handled OS patch distribution.\n\u2022 Executed security vulnerability scans.\n\u2022 Aided centralized anti-virus systems.\n\u2022 Formulated detailed security reports.\n\u2022 Imparted support to customer support staff.\nData Conversion Engineer\nEllkay Inc.\nJanuary 2014 - 2015']","[u""Bachelor's Degree in Information Technology in Information Technology"", u'Associate Degree in Information Technology in Information Technology']","[u'New Jersey Institute of Technology Newark, NJ', u'Bergen Community college Paramus, NJ']"
0,https://resumes.indeed.com/resume/2bbf83a28dcdbd73,"[u'DATA SCIENCE ANALYST (INTERN)\nAll Health Network - Denver, CO\nJuly 2017 to Present\nUsed SQL to extract and transform the data from Freud environment and load the structured data into the Smart Care environment. This\nreduced the service calls from 6000 to 4000.\n\u2022 Conducted queries via Partners EHR/EMR system and output in SQL Server database as part of Readmission Project.\n\u2022 Developed algorithm to convert insurance-orientated ICD-9 codes to clinical practice meaningful disease classification using Python.\n\u2022 Conducted data analysis using logistical model, KNN and random forest method to identify high readmission risk patient and improved the accuracy (C-scores) by 30 percent.\n\u2022 Extracted twitter data using Python and did text mining analysis with BeautifulSoup (Python) and SAS E-Miner to improve the AHN facilities\nwhich increased the occupancy rate by 12%.\n\u2022 Built complex SQL reports to audit $2.5 million of pay and insurance benefits for over 150 individual records.\n\u2022 Designed and developed various analytical reports from multiple data sources by blending data on a single Worksheet in Tableau Desktop\n\u2022 Involved in the planning phase of internal & external table schemas in Hive with appropriate static and dynamic portions for efficiency.', u'DATA ANALYST\nTata Consultancy Services - Hyderabad, Telangana\nMarch 2014 to July 2016\n\u2022 Responsible for analytical data needs, handling of complex data requests, reports, and predictive data modeling\n\u2022 Designed ad-hoc queries with SQL in Cognos ReportNet. Examined reports and presented findings in PowerPoint and Excel.\n\u2022 Used Anomaly & Fraud detection techniques with SAS E-Miner for the American Express client resulting in reduction of 22% of fraudulent\ncases.\n\u2022 Reporting of frauds, missed transactions, forecast, user behavior using Tableau in direct weekly cross-functional team meetings for continuous\nprocess improvement.\n\u2022 Implemented Agile Scrum practices for project implementation which reduced the project touch time by 300 man-hours and cost reduction\nof $30,000/year.\n\u2022 Conducted statistical analysis to leverage the results to drive brand decision making and survey development resulting in 4 new projects\nfrom business partners.', u""SYSTEMS DATA ENGINEER\nFebruary 2011 to February 2014\n\u2022 Evaluated performance of 300+ stores for Nielsen clients based on key metrics and identified opportunities to enable stores to meet and exceed their financial targets through increased sales.\n\u2022 Created data lake by extracting customer's data from various data sources into HDFS. This includes data from Teradata, Mainframes, RDBMS,\nCSV and Excel.\n\u2022 Involved in optimization of SQL scripts and designed efficient queries to query data\n\u2022 Developed the SQL table schema for the effective storage of the customer data.\n\u2022 Involved in preparing design and unit and Integration test documents\n\u2022 Developed an internal web-scraper tool for inspection of ad-hosting on websites using google, URLLib, Beautiful Soup packages in python.""]","[u'MASTER OF SCIENCE IN BUSINESS ANALYTICS in Statistics', u'in Neural Networks and Artificial Intelligence']","[u'University of Colorado Denver, CO\nAugust 2016 to December 2017', u'JNTU(Jawaharlal Nehru Technological University) Hyderabad, Telangana\nJuly 2006 to May 2010']"
0,https://resumes.indeed.com/resume/85e125a9e8672ab9,"[u""Data Scientist\nKEYBANK - Cleveland, OH\nJanuary 2017 to Present\n\u2022 Part of the KeyBank Analytics Team providing the insight into banking & payment products through metrics, visualizations, and models.\n\u2022 Developed a Tableau deposit dashboard to help product and sales team to manage $32B commercial deposit portfolio through KPIs. Replaced manual process of ETL with new data pipeline to speed up the decision-making process from 7 days to 2 hours. Logged over 3k view since launch and maintained the user's retention rate around 70%.\n\u2022 Discovered $150M cross-selling opportunity through text mining of clients ACH statements (>1.5B records) and product usage history. Designed a 360-degree product recommendation dashboard for the sales team in relationship review. The algorithm behind recommendation contains both predefined business rules and association model result.\n\u2022 Organized weekly meeting with CEO and CFO to review monthly performance in financials, sales, and product. Collaborated with 5 developers to complete 50+ visualization projects and monitored the progress of each project.\n\u2022 Coached 20 CMU graduate students to finish their capstone projects related to online web portal design, customer social network analysis, and card fraud analysis & prevention."", u""Data Scientist Intern\nKEYBANK - Cleveland, OH\nJune 2016 to December 2016\n\u2022 Analyzed Commercial Card Platform in different aspects(size, industry, interchange rate, ramp-up period, risk grading, decline reasons)\n\u2022 Utilized Mckinsey GCI benchmark data and created a Tableau dashboard for Sales to recommend card spending opportunities through merchant categories.\n\u2022 Utilized current KeyBank Clients' ACH and check transaction pattern to create a real-time cross-selling opportunities tool through tuning parameters like % of vendor matching and rebate %.\n\u2022 Lead 4 CMU student consultants in designing the prototype of the commercial card online portal including data pipeline, front-end web design, benchmark and spend prediction(ARIMA) modeling."", u'Software Engineer\nFocusFeed - Pittsburgh, PA\nOctober 2015 to March 2016\n\u2022Contributed to the web application development that allows users to customize their favorite cloud solutions (Slack, Google, Dropbox\u2026) into one single view. Demo video of website: https://youtu.be/7F68s3byVPQ\n\u2022Implemented Google Calendar RESTful API into the Java Server and enabled users to view calendars side-by-side to avoid conflicts. Enabled users to create, update and delete events with GMT time, attendees and location proximity info\n\u2022Tested Google and Twitter APIs through Postman and different browsers\n\u2022Worked closely with other severs developers through JIRA and Git to ensure that the product was efficient and bug-free']",[u'Master of Information Systems Management in Data Mining'],"[u'CARNEGIE MELLON UNIVERSITY Pittsburgh, PA\nAugust 2015 to December 2016']"
0,https://resumes.indeed.com/resume/d4aecbfa8298756b,"[u""Data Engineer\nCognizant Technology Solutions\nApril 2014 to Present\nAnthem Inc., previously known as WellPoint, Inc., is the largest for-profit managed health care company in the Blue Cross and Blue Shield Association. It was formed when Anthem Insurance Company acquired WellPoint Health Networks, Inc., with the combined company adopting the name WellPoint, Inc. Broker Portal is an application which is used by Anthem's registered brokers to sell the health care plans to the potential customers as well as help the existing subscribers.\n\nWorking as a Data Engineer in the SDR portfolio. This was a Cloudera based Hadoop eco system. The project was to identify the potential customer who visit the anthem website for purchasing the health plan but returned. Information of those customers were collected and analyzed by analytics team. In addition to it various claims, encounter and eligibility data were extracted from HDFS by LOBs using hive query and shared to multiple vendors and management.\n\nResponsibilities:\n\u2022 Used Spark Streaming, Python to consume messages from KAFKA and process the messages in real time i.e. flatten the JSON structure and persist to HDFS.\n\u2022 Work with BA's, Product owners and end users to define process requirements and Acceptance criteria.\n\u2022 Design and develop ETL work flows using spark SQL for delivery of data from HDFS into PostgreSQL and downstream data marts and files.\n\u2022 Design and develop Spark programs in python to filter, transform data using RDD, Dataset/Data frame APIs.\n\u2022 Used Hive to store data output from streaming jobs and used for random read/write of data\n\u2022 Automate the pipeline by building and scheduling OOZIE workflows."", u""Senior QA Engineer\nCognizant Technology Solutions\nJuly 2010 to March 2014\nDescription:\nHealth Net, Inc. is among the United States of America's largest publicly traded managed health care company. The company's HMO, POS, insured PPO and government contracts subsidiaries provide health benefits to approximately 6.6 million individuals in four western states i.e CA, WA, OR and AZ through group, individual, Medicare, Medicaid and TRICARE and Veterans Affairs programs. Health Net's behavioral health services subsidiary, MHN, provides behavioral health, substance abuse and employee assistance programs (EAPs) to approximately 7.3 million individuals in various states, including the company's own health plan members. The company's subsidiaries also offer managed health care products related to prescription drugs, and offer managed health care product coordination for multi-region employers and administrative services for medical groups and self-funded benefits programs.\n\nResponsibilities:\n\u2022 Participate and preparing the Integration Test plan and migration check for promotion of the project from Dev to Prod.\n\u2022 Assisted developer in debugging efforts by conducting root cause analysis.\n\u2022 Used Quality Center to store all test results / track defect.\n\u2022 Work actively with developer to identify high risk feature, database configuration.\n\u2022 Created Requirement Traceability Matrix to map between the requirements and test case.\n\u2022 Wrote various complex SQL queries, join, and store procedures for backend testing.\n\u2022 Design test case and test scenario for member enrollment process.\n\u2022 Work with Business team involved in the project to understand the requirement\n\u2022 Write and track the defects to assess the quality of the project.\n\u2022 Participate in daily status, defect tracking meeting.\n\u2022 Ensure the implemented system meets the requirement of the Business community.\n\nTesting Environment: SQL, Windows 10, Linux, WebLogic, Java, Selenium and Jira."", u'Senior Developer\nCognizant Technology Solutions\nAugust 2008 to June 2010\nDescription:\nMaster Compliance Application (MCA) is used for processing the Customer Application for opening any type of new account with the KeyBank. The group which owns MCA application is known as Customer Identification Unit (CIU).\nThe Customer Applications for opening an account will undergo compliance Checks via CIU application. CIU authenticates whether the application is eligible for opening an account in KeyBank or not. CIU interfaces with various internal and external systems to perform compliance Checks including Active Customer Check, verifying Customer SSN, Address, Birth Details etc. If the Customer Verifications shows some fraudulent or incorrect data, it leads to the denial of the account to customer.\nThis CIU application is a Web enabled application for capturing and processing customer data and interacts with the external systems such as LexisNexis and various other systems for the performing compliance checks. The Application Data is stored in DB2 Database.\nThe CIU application receives Customer applications from various external systems, Mainframe Online systems such as ACAPS, SB2 running on CICS, batch system such as CIX, SDS, Broad ridge and Web Based Applications such as LeaseLogix, Creditware, LOS, TGS2, LNQ.\n\nResponsibilities:\n\u2713 Analyze program, proc, JCL, and identify the modules which required changes.\n\u2713 Coding program and unit testing.\n\u2713 Maintenance and Enhancement for the home service module.\n\u2713 Run Test/Model and UAT cycles and resolve abends.\n\u2713 Support System, Integration and UAT testing.', u'Developer\nSatyam Computer Systems Ltd\nDecember 2005 to December 2006\nThe project is basically involved in changing the stop code value in customer status screen which resulted as rejection of Insurance and tax disbursement in related reports.\n\nContribution:\n\u2713 Analyze program, proc, JCL, and identify the modules which required changes.\n\u2713 Coding program and unit testing.\n\u2713 Maintenance and Enhancement for the home service module.\n\u2713 Run Test/Model and UAT cycles and resolve abends.\n\u2713 Support System, Integration and UAT testing.', u'Developer\nPatni Computer Systems\nOctober 2003 to November 2005\nDescription:\nCommon Entry processes the files from 1) Settlement (Sales & Returns), 2) Remittance\n(Payments & Adjustments) and 3) File Maintenance (Non-monitory transactions such as\nName/Address changes, GEM score updates). As part of this project existing clients were\nconverted to FDR by setting up new paths.\nContribution:\n\n\u2713 Analyze program, proc, JCL, and identify the modules which required changes.\n\u2713 Coding program and unit testing.\n\u2713 Maintenance and Enhancement of existing modules.\n\u2713 Run Test/Model and UAT cycles and resolve abends.\n\u2713 Support System, Integration and UAT testing.']",[u'Bachelor of Engineering in Mechanical'],[u'Utkal University']
0,https://resumes.indeed.com/resume/90836bea7592f9c2,"[u'Data Engineer\nTwine Data - Los Angeles, CA\nJune 2017 to Present\n- Built data pipelines using big data tools on Amazon cloud using Apache Spark to query and process huge diverse data used to drive dashboard and ad marketing.\n- Employed Amazon Athena to analyze data directly on S3\n- Worked with Redhshift as storage and data querying engine.\n- Implemented data analytics engine and built chartio dashboards for the sales and accounting team make informed decisions and control data flow.\n- Combined aws lamda, S3, RDS,EMR,Athena,Redshift, EC2 and other technologies to implement efficient data processing platforms\n- Developed and maintained web-application to handle clients on boarding and automate aws operations using Spring boot, JSP, JQuery and other front end technologies.', u""Computer and Data Engineer\nDire Dawa University - Dire Dawa, Ethiopia\nSeptember 2012 to July 2016\nAs a research and education institute, the university focuses on research as one of its core missions. One of the research focus is artificial intelligence and machine learning.\n\nResponsibilities:\nDesigned and implemented enterprise Java applications using Spring, Hibernate, JSP, and other Java technologies. Built data pipeline workflow from one stage to the next including ingestion, ETL, analysis, and visualization. Applied Hadoop technologies to analyze and process Big data and used machine learning algorithms to accomplish classification and recognition tasks.\n\n\u2714 Designed and implemented enterprise Java applications using Spring, Hibernate, JSP and other Java technologies.\n\u2714 Build data processing pipelines including ingestion, storage, ETL, analysis, and visualization.\n\u2714 Apply Hadoop technologies to analyze and process huge data\n\u2714 Apply machine learning algorithms to accomplish classification and forecasting tasks.\n\u2714 Used Cassandra and HDFS to store terabytes of annotated speech datasets.\n\u2714 Conducted a research project to analyze which parts of the world are the most affected by global warming and performed a future forecast using apache Spark.\n\u2714 Applied MapReduce to build a DNS database from Amazon's Common crawl datasets of more than 50 TB.\n\u2714 Conducted research projects that involve data analysis and deep learning.\n\u2714 Used spark SQL, Spark MLlib, Spark Streaming to parallelize data processing\n\u2714 Wrote Hive commands to process structured data.\n\u2714 installed and configured a 50 node Hadoop cluster.\n\u2714 Developed Store Management System, a Spring MVC application for managing store inventory for various university departments.\n\u2714 Used R to develop an application capable of extracting, analyzing, and visualizing the results of academic and non- academic data of a diverse student population, with trend tracking, data shifts, and reporting.\n\u2714 Designed and implemented Automated Letter Inbox Integration, a mail scanning application capable of digitizing inbound and outbound mail for online access, with optical character recognition engine to make the scanned documents searchable.\n\u2714 Applied naive Bayes classification algorithm to conduct sentiment analysis on twitter streams using Spark MLlib and Spark streaming.\n\u2714 Built a Lip Sync hing engine, a machine learning project to classify phonemes in a speech and map the phonemes to the corresponding lip movements of a 3D avatar."", u'Graduate Assistant\nDire Dawa University - Dire Dawa\nSeptember 2009 to August 2010\n- Graduate Assistant in Higher Education Institute.\n- Java Development and Teaching Assistant']","[u'Master of Science in Computer Science', u'Masters of Science in Electronic Engineering', u""Bachelor's Degree in Electrical Engineering""]","[u'Maharishi University of Management Fairfield, IA\nAugust 2016 to June 2019', u'Campus Groep T, KU Leuven Leuven, Belgium\nSeptember 2010 to July 2012', u'Haramaya University Haramaya, Ethiopia\nSeptember 2005 to July 2009']"
0,https://resumes.indeed.com/resume/5de56d754c0a0ae6,"[u'Data Analyst Intern\nCT DMHAS\nMarch 2017 to Present\nPerformed factor analysis and built item response theory model using R and SPSS to validate the factors influencing problem\ngambling using gambling survey data\n\u2022 Developed logistic regression model using R to identify problem gamblers and performed diagnostic tests to check scale\nconstruct validity across demographic segments\nResearch Assistant\n\u2022 Performed data collection, preparation and data management using MS Access and Advanced Excel\n\u2022 Conducted quantitative data analysis on correctional population survey data to generate visualizations and draw insights\n\u2022 Performed multiple regression, significance testing, ANOVA, and cluster analyses to identify relationships between decision\nvariables and to assess the effectiveness of the recovery programs', u'Software Engineer\nTesco Bengaluru\nJuly 2014 to July 2016\n\u2022 Developed and tested modules using C# and SQL to reduce runtime of data load in production servers by 30% at stores\n\u2022 Performed data transformation and root cause analysis using ETL tools to generate business reports\n\u2022 Interacted with stakeholders to understand business scenarios and gather functional requirements\n\u2022 Re-engineered SQL modules and performed data analysis of product inventory at 3,000 stores to improve sales\n\u2022 Performed exploratory data analysis and presented insights using Tableau on assortment optimization data to understand\ncustomer behavior and aid store clustering\nANALYTICS PROJECTS\nSocial Media Analytics | Text Mining using Quora Question Pairs | R, Python\n\u2022 Performed text analysis using the concepts of Natural Language Processing to identify similar question pairs\n\u2022 Extracted features to apply predictive modeling techniques like Logistic Regression and XGBoost to classify similar questions\nMarketing Strategy | IMDb Rating for Movies | SAS JMP, Tableau\n\u2022 Processed data for 5000 movies from IMDb using principal component analysis and data imputation\n\u2022 Predicted gross revenue and categorized movies using decision tree and linear regression. Proposed suggestions based on\nFacebook likes and gross revenue for the most profitable movie genres\nSurvival Analytics | Employee Attrition | SAS Base\n\u2022 Built predictive model using survival analysis techniques to identify factors causing employee attrition based on the demographic distribution of the data\n\u2022 Developed Gamma, Exponential and Weibull models to determine attrition rate of employees\nPredictive Modeling | Analysis of Customer Credibility | R, Tableau\n\u2022 Performed exploratory analysis and predicted if a customer will default on loan payment using logistic regression and decision tree for German Credit Data\nTime Series Forecasting | Bike Sharing Demand | SAS Enterprise Miner\n\u2022 Developed ARIMA model to forecast bike rental demand and analyzed the effect of season, holiday, and weather on demand on weekly and monthly basis']","[u'Master in Business Analytics and Project Management', u'Bachelor of Engineering in Computer Science & Engineering']","[u'University of Connecticut Hartford, CT\nDecember 2017', u'Visvesvaraya Technological University\nJune 2014']"
0,https://resumes.indeed.com/resume/964484a9c96fd160,"[u'Oceanographic Data Analyst\nAdvanced Resource Technologies Inc (NOAA/NOS/CO-OPS) - Silver Spring, MD\nJanuary 2015 to Present\nResponsibilities\nPart of the Operational Engineering Team for the National Oceanic and Atmospheric Administration Center for Operational Oceanographic Product and Services (NOAA CO-OPS). Provided engineering and technical support for the installation and operation of tidal, meteorological, water level, and currents stations.\n\nDuties\n-Analyzed incoming environmental data for quality assurance\n-Configured and maintained sensor and station metadata for meteorological, currents, and water level stations\n-Provided engineering support for hydrographic surveys, NGS remote sensing surveys, and vertical datums projects\n-Reviewed surveying leveling records and field documentation to quality assess benchmark networks\n-Developed new standards and procedures for maintenance and operation of current meters\n-Tested new software used for analysis of water level/currents data and metadata and assisted in debugging software\n-Prepared and reviewed engineering drawings of stations and sensor configurations\n\n\nSkills Used\n-MATLAB scripting for data analysis and data corrections\n-SQL queries and scripts, Unix/Linux\n-CAD and drafting experience (AutoCAD)\n-Software testing and quality assurance\n-Mapping and GPS software (GoogleEarth, ArcGIS)\n-Field experience with surveying and leveling\n-Writing of technical documentation and standard operating procedures\n-Analysis of geodetic and surveying data to monitor benchmark network stability and for establishment of new datums\n-Analysis of oceanographic and meteorological data\n-Microsoft Office, Excel, PowerPoint, Adobe Photoshop, Adobe Acrobat', u'Intern Project Engineer\nAllan Myers - Fallston, MD\nJune 2010 to November 2014\nResponsibilities\nSeasonal work during the summers of 2010, 2011, 2012, and 2014. Worked both as a laborer and intern project engineer on a variety of infrastructure and civil engineering projects.\n\nDuties:\n-Utilized GPS and surveying data to develop 3D topographic models of job sites for quantity take off/estimating\n-Surveyed jobsites with GPS equipment for final as-built models\n-Estimated and totaled production and material costs for infrastructure projects\n-Analyzed engineering contract drawings to determine necessary material and ecological impacts\n-Modeled elements from contract drawings in Sketchup/Autodesk\n-Managed and assisted scheduling keystones for completion of projects\n\n\nSkills Used\n-Surveying and usage of GPS equipment (eg. Trimble robotics)\n-GPS, mapping, and 3D topographic modeling software (Trimble Business Center,ArcGIS)\n-CAD and drafting experience (AutoCAD, Solidworks)\n-3D Modeling software (Solidworks, SketchUp)\n-Microsoft Office, Excel, PowerPoint, Adobe Photoshop, Adobe Acrobat, Bluebeam\n-Estimating and analyzing quantities required for infrastructure projects\n-Analysis of engineering project drawings']",[u'BS in Physical Sciences'],"[u'University of Maryland College Park, MD\nJanuary 2009 to January 2014']"
0,https://resumes.indeed.com/resume/9694008e05f83345,"[u""Systems Engineer\nSyngenta Crop Protection - Omaha, NE\nJanuary 1995 to January 2017\nModified and performed troubleshooting duties on third-party-developed PLC programs for Allen-Bradley\ncontrollers to identify programming errors, optimize automated functionality, improve process monitoring and alarms, and add process instrumentation and ingredients. Collaborated with contracted parties to develop new PLC\nprograms and routines to support production expansions and to convert existing facilities from GE-based systems to\nRockwell Automation controllers. Set up and managed Rockwell-based operator interfaces running FT View SE on desktops and thin clients and FT View ME on PanelView graphic terminals.\n\nConverted the interconnectivity of an existing GE-based PLC system from wired serial communications to industrial\nethernet for increased data bandwidth capacity and speed. Managed projects to expand the fiber optic network,\nthereby expanding business and process control network access to other areas of the plant. Upgraded the production operator HMI's from an outdated UNIX-based product to a Windows-based Graphical User Interface\n(GUI) system. Developed, modified, and maintained ladder-logic programs for a networked GE FANUC PLC\ninfrastructure and operator interfaces to accommodate new configurations and changes in product formulations,\nstorage, granular mixing, packaging, and water treatment. Developed, maintained, and expanded the industrial\nethernet communications for the addition of new controllers and other IP-addressable devices.\nHome: (402) 291-8262 \u2022 Mobile: (402) 676-3618\nhanson68123@yahoo.com\n\nSyngenta Crop Protection )\nServed as site technical lead for corporate-wide initiatives, including the conversion of the site's telephony system from a local PBX to a global IP phone service, the deployment of new PC client hardware and operating system\nupgrades, and the establishment of wireless networking throughout selected facilities at the site. Administered\nserver, network, security, and printing services as the site's sole IS/IT Site Services Manager until 2000. to provide local interim support as business IT services transitioned to third-party remote support. Configured and set up network printers, audio/visual equipment, and conference/IP phones. Performed in-house desktop and\nlaptop repairs."", u'Data Communications Engineer\nAlliedSignal Technical Services Corporation - Houston, TX\nJanuary 1993 to January 1994\ncontracted through Bay Area Technologies, Houston TX)\n\nCoordinated the network connectivity of over 200 personal computers and printers as part of a departmental move to new facilities.', u""Engineer\nLockheed Engineering and Sciences Company - Houston, TX\nJanuary 1984 to January 1992\nDesigned an infrared receiver capable of tracking a transmitting source over free space while simultaneously\ndemodulating the received signal for output to a video monitor. Performed troubleshooting tasks on malfunctioning audio headsets and subassemblies designated for space flight extravehicular activity (EVA).\nDeveloped a fiber optic link for providing two-way video, audio, and data communications between robotics work\nstations. Designed and implemented a test console for audio speaker quality evaluations. Conducted tracking tests, as part of a test department team, to evaluate radio-frequency communication quality and reliability with spacecraft as it traversed within the ground station's line of sight.""]","[u'Bachelor of Science in Electrical Engineering', u'Bachelor of Arts in Pre-Engineering']","[u'Texas Tech University Lubbock, TX\nJanuary 1984', u'Abilene Christian University Abilene, TX\nJanuary 1984']"
0,https://resumes.indeed.com/resume/1ec891fb2181ea3a,"[u""Data Analyst (Junior)\nAmida Technology Solutions - Washington, DC\nMay 2017 to Present\nUsed Spark & Spark SQL to clean, process & transform economic indicators in 2GB data from the World Bank into a format\nsuitable for clustering & other analytics models to be applied to it.\n\u2022 Analyzed network data obtained from the World Bank for countries' trade relations through the years using algorithms in\nNetworkX API, & presented in Jupyter notebook.\n\u2022 Developed interactive D3.js visualizations for trade network data.\n\u2022 Performed market analysis of the health-IT industry & presented findings to company stakeholders for business development.\n\u2022 Built the backend of a microservice for messaging in Node using REST API, Express, Sequelize, Mocha framework & PostgresDB.\n\u2022 Parsed XML in Python to convert into CSV format; processed CSVs in JavaScript to convert into JSON format."", u'Associate Software Engineer\nAccenture Services Pvt. Ltd - Mumbai, Maharashtra\nAugust 2015 to July 2016\nMumbai, India\n\u2022 Used Spark to parse XML files using XML-library, ingested data into Cassandra NoSQL DB using Spark.\n\u2022 Gathered requirements & designed data models.\n\u2022 Used DataStage as an ETL tool, built various stages to extract data from sources systems, loaded the data into oracle DB.']","[u'MS in Software Engineering', u'Bachelor of Electronics Engineering in Mining']","[u'George Mason University Fairfax, VA\nMay 2018', u'K. J. Somaiya College of Engineering, University of Mumbai Mumbai, Maharashtra\nMay 2015']"
0,https://resumes.indeed.com/resume/9c5ae2cb3400b397,"[u'Data Science Practicum\nExperian - Irvine, CA\nJanuary 2018 to Present\n1. Develop a solution to discriminate consumers likely to file personal bankruptcy over a 24-month period\n2. Assess results against industry benchmark and logistic regression models', u'Marketing Analyst\nSocialCity - Bengaluru, Karnataka\nJanuary 2017 to September 2017\n1. Increased market coverage by 50\\% in a six-month period by planning various initiatives, social media campaigns, re-targeting, and referral marketing\n2. Led initiative to build statistical models for growth, engagement, retention, and communicated results to product team', u'Software Engineer\nIngersoll-Rand - Bengaluru, Karnataka\nAugust 2012 to June 2014\n1. Improved the energy efficiency of buildings by 15\\% by developing connectivity protocols - BACnet, Modbus, in collaboration with IBM required for building automation and control networks\n2. Managed clients Le M\xe9ridien and Oberoi, and performed data profiling and analysis to identify data trends (SQL)\n3. Assessed business requirements to create design specification documents, system cases, and design models']","[u""Master's in Analytics"", u'MBA in Marketing', u""Bachelor's in Electronics and Communication""]","[u'UC Irvine Irvine, CA\nAugust 2017 to Present', u'Symbiosis International University Pune, Maharashtra\nJune 2014 to May 2016', u'NIT Calicut Calicut, Kerala\nJuly 2008 to June 2012']"
0,https://resumes.indeed.com/resume/84c71df5a95a52af,"[u'Data Engineer\nGeneral Electric Digital\nMay 2017 to Present\nFinance Data Lake (FDL) is EDW, which is ingested from various source systems (ERPs), our internal users wanted to see their data in different views in OBIEE, Tableau and Spotfire reporting tools.\n\nResponsibilities:\n\u25cf Created complex reports as per the requirements.\n\u25cf Created dashboard prompt, variable prompt.\n\u25cf Crated interactive dashboards.\n\u25cf Used Pivot views, Narrative views, and table views to display the reports.\n\u25cf Configured security using session variables.\n\u25cf Configured object level security\n\u25cf As an OBIEE SPOC, I am handling OBIEE L1 team.\n\u25cf Debugging and resolving issues.\n\u25cf Supporting Ingestion team during new releases and patch deployments.\n\nEnvironment: OBIEE12c, Talend, HVR Greenplum, UNIX', u'Data Engineer\nGeneral Electric Digital - Bengaluru, Karnataka\nPresent\nBangalore.\n\u27a2 Worked as Solutions Engineer for Oracle. Bangalore from May 2016 to May 2017.\n\u27a2 Worked as Project Engineer for Wipro infotech. Cochi from April 2013 to May 2016.', u'OBIEE Developer\nOracle India Pvt Ltd\nMay 2016 to May 2017\nInnovation Management: Modern Manufacturing\nResponsibilities:\n\u25cf Created complex reports as per the requirements.\n\u25cf Created dashboard prompt, variable prompt.\n\u25cf Crated interactive dashboards.\n\u25cf Used Pivot views, Narrative views, and table views to display the reports.\n\u25cf Configured security using session variables.\n\u25cf Configured object level security\n\u25cf Created UPK for presentations.\n\u25cf Part of Oracle BI community to resolve BI related issues.\n\nEnvironment: OBIEE11.1.1.6/OBIEE 11.1.1.7, OBIEE 12c, BICS, Fusion\n\nProject 4', u""OBIEE Developer\nWipro - Kochi, Kerala\nAugust 2014 to May 2016\nFGB is a leading bank in gulf country which divides their business into Retail and FINCON Domains. They want their daily transaction as a Report on the basis of weekly monthly and yearly.\n\nResponsibilities:\n\u25cf Collected requirement and interact with end user in each stage of development.\n\u25cf Prepared FSA and TSA\n\u25cf Worked on Physical, Business and Presentation Layers.\n\u25cf Worked on OBIEE Admin activities like Weblogic Server maintainance, RPD Backup's etc.\n\u25cf Created complex reports with respect to the business user requirement.\n\u25cf Used Pivot views, Narrative views, and table views to display the reports.\n\u25cf Worked on dash board prompts and page prompts.\n\u25cf Developed Reports and Dashboards as per client requirement in OBIEE 11g.\n\u25cf Done testing and debugging of existing SQL queries.\n\u25cf Used Pivot views, Narrative views, graph views and table views to display the reports.\n\u25cf Wrote SQL Queries and created Hidden Prompts on requirement basis.\n\u25cf Prepared the UTC's and STC's for the Client with good success rate.\n\u25cf Worked on RPD migration project.\n\u25cf Created repository variables (static and dynamic)\n\u25cf Supporting for Daily loading activities.\n\u25cf Implemented Usage tracking and got appreciation from the client.\n\nEnvironment: OBIEE11.1.1.6/OBIEE 11.1.1.7, ODI 11.5, Oracle11g, UNIX\n\nProject 3"", u""OBIEE Developer\nWipro - Kochi, Kerala\nApril 2013 to August 2014\nThe Al Rajhi Bank is a Saudi Arabian bank and the world's largest Islamic bank by capital based on 2006 data. They want their daily transaction as a Reports on the basis of weekly, monthly, Quarterly and yearly.\n\nResponsibilities:\n\u25cf Collected requirement and interact with end user in each stage of development.\n\u25cf Wrote SQL Queries and created Hidden Prompts on requirement basis.\n\u25cf Created complex reports with respect to the business user requirement.\n\u25cf Used Pivot views, Narrative views, graph views and table views to display the reports.\n\u25cf Worked on dash board prompts and page prompts.\n\u25cf Worked on Selection steps, calculated items/groups.\n\u25cf Developed Reports and Dashboards as per client requirement in OBIEE 11g.\n\u25cf Developed reports with union requests(multiple subject areas)\n\u25cf Done testing and debugging of existing SQL queries.\n\u25cf Prepared the UTC's for the Client with good success rate.\n\nEnvironment: OBIEE11.1.1.7, Essbase 11.1, Informatica 9.x, Oracle11g/10g, UNIX\n\nProject 2""]",[u'Bachelor in computer applications'],"[u'Sahyadri Science College Shimoga, Karnataka']"
0,https://resumes.indeed.com/resume/4f59d0f01ddda255,"[u'Devops Engineer\nPeterson Technology Partners\nOctober 2016 to Present\n\u2022 Leveraged AWS cloud services to build secure, highly scalable and flexible systems for fortune 100 customer\n\u2022 Worked with CloudFormation, Terraform and Packer to automate the infrastructure deployment\n\u2022 Automated the management of servers using configuration management tools like Ansible\n\u2022 Monitored AWS environment using CloudWatch and used Amazon Kinesis to analyze real-time streaming data\n\u2022 Provisioned and configured log management tools like Splunk and ELK to retrieve useful log data.\n\u2022 Developed AWS lambda functions using python, NodeJS and used python to automate daily tasks\n\u2022 Worked on building microservices using docker and Kubernetes in POC environment', u'Devops/Data Engineer\nCloudwick\nMay 2015 to October 2016\n\u2022 Developed Puppet modules to automate deployment, configuration, and lifecycle management of key clusters and deployed a Jenkins environment for continuous integration/builds\n\u2022 Secured Hadoop clusters using MIT Kerberos, ACLs, Sentry and worked on Kerberos authentication for cluster and user management with LDAP on AD\n\u2022 Developed Spark applications to process streams of real-time data from various sources with Spark Streaming and used Spark SQL to query Streaming Data\n\u2022 Analyzed the data by performing Hive queries and running Pig scripts to know user behaviors regarding shopping habits.\n\u2022 Created Pig scripts to filter the user IP, location and time of logging\n\u2022 Developed Hive queries to process the data and generate the data cubes to help visualizing data for business analysts.', u'SQL developer\nCyber Village Solutions\nMarch 2012 to September 2013\n\u2022 Handled common database procedures, such as upgrade, backup, recovery, migration, etc.\n\u2022 Developed use case diagrams, class diagrams, database tables, and provided mapping between relational database tables and object-oriented java objects.\n\u2022 Developed bash scripts for application deployments as well as system monitoring']","[u'Master of Science in Information Technology Management', u'Bachelor of Engineering in Information Technology Engineering']","[u'The University of Texas at Dallas Dallas, TX\nDecember 2015', u'The Manipal Institute of Technology, Manipal University\nMay 2011']"
0,https://resumes.indeed.com/resume/e1ba5a4bb691a96b,"[u'Big Data Intern\nLennox International\nMay 2017 to December 2017\nResearch and Development\n\u2022 Extended Hive and Pig core functionality by writing custom User Defined Functions to process sensor data\n\u2022 Optimized hive scripts using MapReduce and TEZ engines to reduce the query run time and complexity\n\u2022 Implemented sensor data collection in Hadoop Distributed File System (blobs) using HDInsight Hadoop Cluster\n\u2022 Developed Stream Analytics jobs for extracting data from Event Hubs and push it into Azure Blob Storage\n\u2022 Developed Hadoop cluster creation/deletion script using Azure Automation Jobs to minimize cluster runtime\n\u2022 Created real-time dashboards on Azure Event Hubs for Support Team to provide on-call assistance to customers\n\u2022 Ported machine learning algorithms developed originally in MATLAB to Spark using Pyspark\n\u2022 Developed algorithms for fault detection using SparkML with data scientists, business analysts for insights\n\u2022 Created Proof of Concept for Predictive Modeling using in-memory data processing with Apache Spark', u'Systems Engineer\nTata Consultancy Services (TCS)\nJuly 2014 to July 2016\nFinancial Markets and Data Warehouse - Common Wealth Bank of Australia (CBA)\n\u2022 Integral part of Oracle Data Warehouse application team - Defined and created jobs to transform and load\nmillions of records daily using SQL loader, C program, Unix scripts, Job Information Language and Autosys\n\u2022 Automated the backup and restore functionalities of Production schemas in Unix using Oracle data pump\n\u2022 Loaded the transactional data from various applications through ETL batch processing using shell scripts\nand SQL; improved the efficiency with SQL tuning by Implicit Cursors, Bulk Collect, ForALL, Adding Hints']","[u'Master of Science in Computer Science in Data Science', u""Bachelor's in Electronics and Communication""]","[u'The University of Texas at Dallas Richardson, TX\nAugust 2018', u'Jawaharlal Nehru Technological University Kakinada, Andhra Pradesh\nOctober 2010 to May 2014']"
0,https://resumes.indeed.com/resume/e7d390583ad38580,"[u'Network Administrator\nTWB Leadership Academy - Lancaster, TX\nAugust 2013 to Present\nResponsibilities\nI was responsible for handling internet services / Hardware.\n\nAccomplishments\nInstalled hardware and internet services.\n\n\nSkills Used\nCustomer Service. Basic technical issues. Hardware Setup. Software Installation.', u'Data Entry\nBuy the Book Cafe - Duncanville, TX\nAugust 2013 to November 2013\nResponsibilities\nData entry. Processed Orders.\n\nAccomplishments\nSet up Network at the work environment so that students and other customers could use the internet at use. Processed orders in a timely manner so that customer support rates were high which improved her ratings and raised the bottom line.\n\n\n\nSkills Used\nInformation Technology. Workflow.', u'Production Engineer\nS.O.W Ministries - Lancaster, TX\nFebruary 2012 to October 2013\nResponsibilities\nSound Booth/Light management/AV Engineer/Computers and Network.\n\nAccomplishments\nBrought ease of access to church programs. Brought entertainment with lights which drew more people in.\n\n\nSkills Used\nAV Skills. Information Technology.']",[u'in Science'],"[u'TWB Leadership Academy. Cedar Valley College, Lancaster, TX\nJanuary 2013 to January 2014']"
0,https://resumes.indeed.com/resume/08fe009d4a18d5a8,"[u'Data Analyst\nApple Inc - Austin, TX\nAugust 2017 to February 2018\n\u2022 Curating incoming data from other countries and using it in a corporate environment for a large-scale mapping application\n\u2022 Fielding and analyzing data from multiple global markets and used to help better an international business product\n\u2022 Responsible for overall accuracy using client specific sources as well as internet and social media sources\n\u2022 Edit data per client specific protocols and policies once data has been deemed inaccurate\n\u2022 Analyst may work on special side projects specific to major market updates within client applications', u'Engineer Intern\nSamsung Austin Semiconductor - Austin, TX\nMay 2016 to August 2016\n\u2022 Constantly sought process improvements in order to achieve efficiency, better quality, service reliability and overall outstanding customer experience.\n\u2022 Collaborated with team members on annual projects.\n\u2022 Transferred R scripts into Python in order to be able speed the calculations required for step data from 4 min to 2 min and checked if tables were being updated correctly.\n\u2022 Created procedures, schema and SQL snippets to speed up the coding time by 20%.\n\u2022 Helped HR with events planning.']",[u'Master of Science in Applied Mathematics in Applied Mathematics'],"[u'The University of Texas at San Antonio San Antonio, TX\nMay 2017']"
0,https://resumes.indeed.com/resume/7547e7e4aec3480a,"[u'Data Science Intern\nWolters - New\nOctober 2017 to December 2017\nCFPB Analysis\n\u2022 Analyzed the CFPB dataset to find the important features that contribute towards the approval and the denial of loans\n\u2022 Performed detailed analysis on segments with respect to race and gender to explore the fair lending of loans using Python and MS Excel functions\nCTLS (Lien Solutions)\n\u2022 Imported data from Oracle database into Hadoop (HDFS) using SAS and merged the tables using multiple joins\n\u2022 Performed feature engineering to analyze the characteristics of the debtors and the secure parties\n\u2022 Transformed the code from SAS to PySpark and leveraged the cluster functionality to optimize the time taken for variables creation', u'Software Engineer\nWipro Technologies - Bangalore Urban, Karnataka\nOctober 2014 to July 2016\nWipro Identity Cloud (WIC)\n\u2022 Performed Ad-hoc analysis by running SQL queries in database and assisted the clients to get a better understanding of the application usage by leveraging MS Excel functions\n\u2022 Performed a detail RCA during server outage and provided insights to the clients and the service providers']","[u'Master of Science in Business Intelligence & Analytics', u'Bachelor of Engineering in Electronics & Instrumentation Engineering']","[u'STEVENS INSTITUTE OF TECHNOLOGY Hoboken, NJ\nDecember 2017', u'ANNA UNIVERSITY Chennai, Tamil Nadu\nMay 2014']"
0,https://resumes.indeed.com/resume/94ac2583214edd5b,"[u'Operating Engineer\nCHI/ST LUKES HEALTH - Houston, TX\nDecember 2012 to September 2016\nStart up and shut down of central plant PMs on related equipment. Calling and overseeing contractors. Mentoring and training other staff. And also because of being a medical office tower we were also overseen by the DNV. Also considered a critical environment so life safety was very demanding being done every second Saturday of the month. PMs were followed paperwork. ATS operation fire pumps switch gear was monitored closely. I also have a background with DATA CENTER operations.', u'Building Engineer\nAramark Correctional - Houston, TX\nMay 2009 to July 2012\nchanged Delany flush valves HVAC repairs replaced sprinkler heads replaced ballast,switches and outlets. Fire alarm resets. Sliding door repair.', u'Data Center Engineer\nCARTER & ASSOCIATES - Atlanta, GA\nSeptember 2005 to September 2008\nOperating Switchgear, ATSs, Monitoring UPS, Gensets,Veeder-root. Had to demonstrate tying Substations together weekly testing of your knowledge was done.', u'Service Technician\nCarr Services inc - Dallas, TX\nFebruary 1998 to September 2005\nKitchen and Bath repairs and construction ac repairs split systems and rooftops home repairs.', u'Building Engineer\nHines Limited Partnership - Portland, OR\nJune 1996 to November 1997\nData center engineer central pl\xe0nt building startup insight 600 Life safety calibrated stats direct acting and reverse acting.', u'Building Engineer\nThe Weiss Company - Seattle, WA\nJune 1992 to June 1994', u'Building Engineer\nHarbor Properties Incorporated - Seattle, WA\nJune 1991 to May 1992', u'Building Engineer\nABM/Martin Smith Incorporated - Seattle, WA\nJanuary 1989 to June 1991\nClean up around building replaced ballast repaired Sloan flush valves replaced filters on AHUs helped with building pneumatics.']",[u'Certificate in HVAC & R'],"[u'Renton Technical College Seattle, WA']"
0,https://resumes.indeed.com/resume/317e9d7a7bc0715c,"[u'Data Scientist Intern\nWabtec Railway Electronics - Dallas, TX\nJune 2017 to December 2017\n-Contributing to building an analytical sub-system on Microsoft Azure.\n-Designed and developed several dashboards using d3 JS, Azure TimeSeries Insights, Tableau, PowerBI\n-Developed code on PySpark to process data 90% quicker.\n-Designed and developed SQL triggers.\n-Extracted data from SQL database, NoSQL database for analyzing\n-Transformed unstructured data to structured format to save in SQL database and NoSQL database (Azure CosmosDB)\n-Processed and applied machine learning algorithms to analyze and show information on dashboards that predict failure of railway components.', u'Systems Engineer\nTata Consultancy Services\nJanuary 2014 to January 2016\nWorked as a Java developer on a product lifecycle management tool (PLM) used by Honeywell Aerospace Division, USA.\nResponsibilities included developing and supporting the PLM application, initiating client meetings every week.\nReceived 100% Customer Satisfaction Index for 2 years in a row and ""Best Team"" Award thrice for good teamwork.']","[u'Master of Science in Arlington', u'Bachelors in Technology']","[u'University of Texas Arlington, TX\nSeptember 2016 to Present', u'GITAM University Visakhapatnam, Andhra Pradesh\nJanuary 2010 to January 2014']"
0,https://resumes.indeed.com/resume/85d1e6082b19c715,"[u""Sr. Data Analyst\nIntegrichain, Inc - Philadelphia, PA\nFebruary 2017 to Present\nWrote a Python script to significantly cut department time and resources preforming QA checks for one of the company's product offering.\n\u2022 Utilizing Python, wrote a fully automated script that queries SQL database across multiple customers and returns detailed CSV files for invested personnel - Created a dynamic script that allows users to define parameters and returns detailed CSV files containing all relevant information e.g. misalignment with inventory, discrepancy in costs and quantities of drugs, etc."", u'Sr. Data Analyst\nIntegrichain, Inc - Philadelphia, PA\nSeptember 2016 to Present\nVerify the integrity of the data pipeline, and assist enrich data operations (EDO) department with visual aid and supplementary analytics.\n\u2022 Maintaining and verifying the incoming sales data for customers. - Map channel data to points of care across the United States and additionally diagnose critical errors with data streams.\n\u2022 Responsible for data loading and reaching out to trade partners in cases where data was not loaded correctly. -Perform daily QA checks on incoming EDI files, notify invested personnel and run ad-hoc ETLs when necessary.', u'Personal\nStock Market Algorithm\nMarch 2016 to Present\nPrimary motivations for developing this algorithm was to expand my skillset and explore different avenues of coding.\n\u2022 Wrote code Using Quatopian API to generate a data frame of the S&P 500 for an interval of 30 days\n\u2022 Implemented the sk-learn.preprocessing, numpy, and TensorFlow library to create a basic machine learning pipeline that is scalable', u'Chemical Engineer\nPolySciences Inc., a Healthcare Manufacturing Company - Warrington, PA\nJune 2015 to August 2016\nResponsible for production processes for polymers and monomers that are used in biomedical devices/contact lens.\n\u2022 Laboratory manufacturing for a specific shift - ensuring high quality product supplied to big pharma.\n\u2022 Drafted and presented multiple improvements of current chemical processes and validated improvements in accordance to standards set by the company. - Improved the production process of a monomer used for biomedical devices by 50%']","[u'Bachelor of Science in Chemical Engineering in Finance', u'']","[u'Drexel University, College of Engineering Philadelphia, PA\nJune 2015', u'Johns Hopkins University\nJanuary 2011 to January 2015']"
0,https://resumes.indeed.com/resume/6c5a0c867bcf9b6b,"[u'Data Analyst\nESIS Inc\nJanuary 2018 to Present\nIdentifying the probability that a claimant hires an attorney through predictive modeling.\n\u2022 Using deep learning to classify the claimant who have the propensity to hire an attorny.\n\u2022 Developing the model to devise interventions to eliminate the need for an attorney.', u'Data Analyst\nAviage Systems\nDecember 2017 to Present\n\u2022 Optimizing speed changes for pilots in to achieve fuel savings through training and testing given dataset in Python.\n\u2022 Researching possible parameters and factors, building models at various time scales, and designing a lookup table and presentation', u'Software Engineer\nTavant Technologies - Bengaluru, Karnataka\nSeptember 2016 to July 2017\nBangalore, India\n\u2022 Led a 4 member team on building WebApis responsible for processing data stored in NMLS database and projecting it to the client code.\n\u2022 Analysed databases using MySQL and Tableau to design and create analytic reports for multiple regions.\n\u2022 Designed new dashboards and templates by modifying, enhancing and adapting the existing systems and integrated new features or improvements, with the aim of improving business efficiency and productivity.\n\u2022 Assisted clients in addressing complex business problems through application of relevant technology.\n\u2022 Communicated effectively with clients to clearly obtained, identified and documented business requirements.', u'Software Engineer\nAccenture - Bengaluru, Karnataka\nJanuary 2014 to August 2016\nBangalore, India\n\u2022 Promoted to software engineer after six months based on proven leadership skills\n\u2022 Led a team of three to develop a comprehensive database system and supported implementation and utilization\n\u2022 Analysed data and partnered with internal teams to drive operations excellence\n\u2022 Analysed requirements and provided optimized system solutions and system design documentations, developed detailed project plans and tracked the performances, managed and controlled the scope']","[u'Master of Science in Business Analytics', u'Bachelor of Technology in Computer Science']","[u'W. P. Carey School of Business at Arizona State University Tempe, AZ\nMay 2018', u'Lovely Professional University\nJuly 2013']"
0,https://resumes.indeed.com/resume/4cb7c1096d17c672,"[u'Big Data Engineer\nVintech Solutions\nApril 2017 to Present\nInvolved in Stream processing using spark and flink on the data ingesting using kafka and build analytical models out of that for downstream applications. Used hive for archival reporting on the historical data.', u'Big Data Engineer\nGCN Media Publishing\nMarch 2016 to March 2017\nMigrate the ONEcount platform from its current MySQL architecture to an appropriate, big data engine(s) so that it can scale to meet the growing demands of expanding customer base. Change existing data pipeline based on LAMP architecture to big data based toolkit.', u'Big Data Engineer\nMicron Technologies\nDecember 2014 to December 2015\nData warehousing\nBuild data warehouse to ingest data from multiple sources and analyze data to reflect internal systems. Implemented processing using spark and migrated existing workflows from MapReduce for more efficiency.', u'Data Scientist (Intern)\nVintech Solutions\nJune 2015 to August 2015\nProduct Demand Forecasting Data Scientist Intern\nEvaluate different machine learning techniques for the demand forecast of different products and adjust the production. This includes data ingestion cleaning using Hadoop environment and implementation of different classification techniques.', u""Big Data analyst\nMorgan Stanley\nJanuary 2012 to November 2014\nBuild Data Warehouse for archiving data, develop data access layer to provide unified data access platform for downstream applications and support various data analytics. HDFS and MongoDB are used as storage layer for warehouse depending on the required response times from the warehouse. The data ingestion flow is built using Flume, Sqoop and Kafka.\n\nClient: Western Union\nSocial Media Influencer Data Analyst\nPerform Sentiment analysis of different users from the data extracted from various social networking sites like Facebook, Twitter, Blogs and review websites. The data is ingested and cleansed using Map-Reduce. Polarity of users is evaluated using Open NLP package and Bag-of-Words approach.\n\nClient: Barclays\nMarket Trend Analyzer Big Data analyst\nAggregate and analyze log files generated by the web servers and stock transactional information and derive useful information to identify patterns of market trends and trader's interests. The future predictions are made about the scrip's from the past market trends and integrating that information from the news and implementing different machine learning techniques on them.\n\nClient: Nielsen Holdings\nAssort Man Hadoop Developer.\nStudy the effects of different variables in market on a particular product by analyzing transactional and operational Logs. Regression is implemented for SKU Rationalization, filling distribution gaps and to identify regional effects, effects of competitors, study and minimize effects of cannibalization.\n\nPricing Insights Hadoop Developer.\nEfficient pricing strategy is determined by analyzing prices in different areas and the prices of competitors. The pricing information is extracted from various databases using Sqoop and combined with log information ingested using Flume and cleaned and pricing strategy is developed by analysis on cleaned dataset.""]","[u'Master of Science in Computer and Information Sciences', u'Bachelor of Technology in Electronics and Communications Engineering']","[u'Purdue School of science, IUPUI\nDecember 2015', u'Jawaharlal Nehru Technological University\nMay 2011']"
0,https://resumes.indeed.com/resume/a2e0265862c3e626,"[u'Data Scientist\nAffine Analytics - Bengaluru, Karnataka\nNovember 2016 to July 2017\nBangalore, India\n\u2022 Worked for client, Sears Holdings\n- Implemented gradient boosting method (LightGBM) to build a logistic classifier (Dataset size: 1 Million) to output\npurchase propensity of the member to visit store next month for a business unit\n- Engineered new features by building embeddings thereby encrypting the buying pattern of a member, improving\nmodel accuracy by 4%\n- Built a gradient boosting model to capture the member engagement over mailers sent for Shop-Your-Way, Sears and K-Mart separately', u'Senior Analytics Engineer\nRobert Bosch - Bengaluru, Karnataka\nJuly 2015 to November 2016\nBangalore, India\n\u2022 Implementing supervised and unsupervised Machine Learning algorithms to solve different business problems\n- Developed a two stage SVM model (0.92 precision, 0.96 recall) to identify anaemia severity by using data from non-invasive sensors; Used agglomerative clustering to tackle class imbalance in the data; Part of research project at John Hopkins University\n- Identified Anomalies in vehicle test data files using Kullback-Leibler divergence; Implemented the solution in\nApache Spark\n- Engineered a regression model to classify the dispensed beverage through signal taken from the vibrations of coffee vending machine and predicted raw material requirements; Used Independent Component Analysis (ICA)\ntechnique to remove noise from the data\n\u2022 Building tools useful for the department as a part of Centre of Excellence team\n- Worked on a text mining platform to build modules to clean text (lemmatization, punctuation & numbers removal) and extract features (Sentence body detection, POS-tagging, key words, Chunk parser); Used nltk & gensim\nlibraries in python (English language); Hosted the tool on the cluster which could be accessed using API call\n- Built a tool for internal use in R to accommodate for forecasting techniques like exponential smoothing, ARIMA and Holt-Winters along with data preprocessing modules\nPROJECTS\nHypothesis testing Data Analysis\n\u2022 Analyzed credit card data for different hypothesis testing; Used ANOVA, Multivariate regression to identify factors for credit limit, default rate and analyze buying patterns over six months period.\nMovie Recommender Fundamentals of Computing\n\u2022 Implemented machine learning models from scratch on movie-lens data set. Compared performance of three different\nmodels: Collaborative filtering, K-means, hybrid model.\nDissertations, IIT Kharagpur Guide Prof. P. K. Ray\n\u2022 Used Holt-Winters model to improve throughput of the inventory management system by 3%; Optimized the inventory\nreplenishment process of spares and consumables at TATA Bearings, Kharagpur\n\u2022 Modeled product scheduling with machine breakdown and normally distributed processing time; Employed evolutionary\nsearch algorithm, to arrive at schedules with reduced variability of 8% compared against genetic algorithm']","[u'Master of Science in Applied Mathematics and Statistics', u'B.Tech']","[u'Stony Brook University\nJanuary 2017 to January 2019', u'Indian Institute of Technology Kharagpur, West Bengal\nJanuary 2010 to January 2015']"
0,https://resumes.indeed.com/resume/f967d48c8efeb481,"[u'Data Analyst\nJoy Global Inc. - Mount Vernon, IL\nJanuary 2017 to Present\n\uf0ae Integrated the use of analytical tools (MATLAB, Excel, Power BI) to extract and visualize operational data and conduct predictive analysis of mining equipment\n\uf0ae Performed queries to extract and correlate time-series data to discover trends in mining productivity\n\uf0ae Provided analytical support for risk monitoring by investigating equipment anomalies and assessing machinery efficiency', u'Quality Control Analyst\nDelta Companies, Inc - Marion, IL\nFebruary 2016 to January 2017\n\uf0ae Analyzed engineering data and made use of case management tools\n\uf0ae Coordinated hard rock sampling and conducted laboratory testing of materials\n\uf0ae Reviewed & documented technical specifications', u'Field Data Engineer\nWeatherford International - Pittsburgh, PA\nJune 2014 to November 2015\n\uf0ae Assembled, tested, and troubleshot electrical and mechanical equipment for oil/gas drilling\n\uf0ae Programmed and deployed high resolution subsurface data acquisitions systems\n\uf0ae Delivered real-time drilling data and acted as a liaison between engineers and drilling personnel\n\uf0ae Diagnosed equipment operation and resolved issues in a timely manner', u'Research Assistant\nSouthern Illinois University - Carbondale, IL\nAugust 2012 to July 2014\n-Investigated data obtained from seismic devices; created databases, processed data in MATLAB and identified seismic sources by conducting advanced statistical modeling.\n\uf0ae Developed and optimized cutting-edge algorithms to visualize seismic signals (in MATLAB)\n\uf0ae Conducted statistical analyses of seismic waveforms; published results in scientific journals']","[u""Master's in Business Analytics"", u'Masters in Geology(Geospatial Analysis)', u'B.S in Geology']","[u'Iowa State University\nAugust 2016 to May 2018', u'Southern Illinois University\nAugust 2012 to May 2016', u'Brigham Young University Rexburg, ID\nJanuary 2009 to January 2012']"
0,https://resumes.indeed.com/resume/5960f44a490ac524,"[u'Data Engineer\nAumet - Mountain View, CA\nOctober 2016 to April 2017\nResponsible for an API that provides a paid feature in the Aumet Platform. The API provides all the News that is Relevant to Medical Business, the News is scraped from local newspaper\nwebsites in the MENA region, and is then classified using machine learning.\n\nThe API is built using Ruby on Rails, the data is scraped using Python Scrapy, and the machine\nlearning is done with the scikit-learn Python library.\n\nThis Startup was also part of the 500 startups batch #19, in which I got the opportunity to receive training at the 500 startups offices at Mountain View, Ca. The training was in various business skills for kick starting a successful startup.', u'Back-end Engineer\nBlue Kangaroo\nJune 2016 to October 2016\nResponsible for developing many micro features, fixing bugs, refactoring old legacy code, and improving back end system architecture.\nCollaborated with the data science team to improve the data for better results.\nWorked on: Ruby on Rails REST API, PHP YII2 back-end, C++ and Node.js back-end services,\nSQL and NOSQL (Mongo, Redis) databases, and search engines (Elasticsearch)', u'Research Assistant\nUniversity Research Group\nOctober 2015 to May 2016\nOctober 2015 to May 2016)\nWorked in a research group for implementing precise algorithms to solve combinatorial\noptimization problems in various domains. The research group was primarily concerned with combinatorial problems in compiler optimization, but i mainly worked on an exact branch &amp;\nbound solver for the Traveling Salesman Problem, which was also used to solve the Gene\nSequencing Problem by reduction.', u'Web Development Trainee\nEastNets\nSeptember 2013 to October 2013\nTrained in full stack web development, using ASP.NET (Web Forms, MVC, Web Pages)']",[u'Bachelors in Computer Science'],[u'Princess Sumaya University for Technology\nSeptember 2012 to June 2016']
0,https://resumes.indeed.com/resume/4e2d1723a9e1faa0,"[u'Data Engineer Intern\nXinma Real Estate Company\nApril 2016 to August 2016\n\u2022 Implemented a distributed web scraper to collect real-time information about real estate information using RabbitMQ and MongoDB.\n\u2022 ETL (extract, transform and load) data into HBase with Hadoop, and analyzed real estate data with TensorFlow.\n\u2022 Built a website using Django framework, RESTful API and HBase, providing search functions for salesman, such as searching for real estate\ninformation like average house prices, sizes and comparison charts based on location.']","[u""Master's in Information Technology"", u""Master's in Chemical Engineering"", u""Bachelor's in Chemical Engineering""]","[u'Carnegie Mellon University Pittsburgh, PA\nAugust 2017 to August 2018', u'Carnegie Mellon University Pittsburgh, PA\nAugust 2016 to May 2017', u'Tianjin University\nSeptember 2012 to July 2016']"
0,https://resumes.indeed.com/resume/2a4af24d87cca9cd,"[u""Data Scientist\nIndiana University\nNovember 2017 to November 2017\n\u2022 Implemented probabilistic character recognition using hidden markov models, trained on 10000 sentences and an optical character string.\n\u2022 Tested the model on 40 different test images of sentences attenuated with noise and acquired word accuracy of 85\npercent.\n\u2022 Implemented k-nearest neighbours algorithm in Python on 32000 training images from Flickr to determine the orientation of 1000 test Flickr images.\n\u2022 Achieved an accuracy of 83 percent and also studied the effect of the parameter 'k' graphically in R.\n\nData Scientist - Indiana University November 2017\n\u2022 Extracted live tweets using Flume and Twitter API and stored the tweets into Hadoop distributed file system(HDFS).\n\u2022 Conducted sentiment analysis of tweets using Apache Hive and find the most trending hashtag.\n\u2022 Trained a Na\xefve Bayes classifier on 10000 tweets and tested 2000 tweets to classify them according to their location in Python."", u'Data Analyst\nUniversity of Mumbai\nJune 2017 to June 2017\n\u2022 Performed analysis of supermarket data by loading transaction logs into HDFS and analyzed it using Apache Pig and Apache Hive.\n\u2022 Migrated transaction logs from MySQL to HDFS using Sqoop.\n\u2022 Executed analysis for insights like top 10 customers, chain wise sales, top 10 customers in each chain, top 10 brands,\nmost bought products in each chain etc.', u'Software Engineer\nUniversity of Mumbai\nApril 2017 to April 2017\n\u2022 Developed a web application that would provide automation to various exam cell activities using JavaFX, FXML and Oracle for the database.\n\u2022 Used JavaMail API and Java Activation Framework for automatic E-Mailing of Hall-tickets and results.\n\u2022 Incorporated features like automatic allocation of exam answer sheets which reduced paper work to a great extent.\n\u2022 Incorporated Tableau as a feature which would enable professors analyse and assess overall student performance.', u'Intern\nDusane Infotech (I) Pvt. Ltd\nJune 2015 to July 2015\n\u2022 Collaborated with top professionals and had a chance to learn essential concepts of java and web scripting.\n\u2022 Assisted in developing web applications using frameworks and web scripting languages like Html, CSS, JSP,\nServlet, JSF, Primefaces.']","[u""Master's in Data Science"", u""Bachelor's in Information Technology""]","[u'Indiana University, School of Informatics\nMay 2019', u'University of Mumbai\nJune 2017']"
0,https://resumes.indeed.com/resume/d74aee39095c2c9c,"[u'Big Data Engineer\nEricsson - Seattle, WA\nFebruary 2017 to October 2017\nResponsibilities:\n\u2022 Perform Trouble Shooting and Customer Support\n\u2022 Design and Modify EEA web UI.\n\u2022 Design and drive the implementation of data platform functional and non-functional requirements, MapR, HDFS/HBase applications working closely with system engineer.\n\u2022 Continuous integration and release new version of EEA.\n\u2022 Be responsible for the quality of the technical solution.\n\u2022 Experience in designing spark data pipeline.\n\u2022 Experience in using HUE for scheduling and monitoring oozie workflow and coordinator.\n\u2022 Extensively worked on analyzing data using HiveQL, Pig Latin, and custom Map Reduce programs.\n\u2022 Worked on using different file formats like JSON, Sequence files, AVRO file, Parquet file formats.\n\u2022 Used Spark SQL and Hive SQL to process structured and un structured data.\n\u2022 Extensive knowledge in writing and analyzing complex SQL queries, stored procedures, database tuning, query optimization and resolving key performance issues.\n\u2022 Hands on Experience in Writing Python Scripts for Data Extract and Data Transfer from various data sources.\n\u2022 Design and develop state-of-the-art deep-learning / machine-learning algorithms for analysing the image and video data among others\n\u2022 Experience with Tensor Flow, and other Deep Learning frameworks.\n\nEnvironment: Hbase, Hadoop, Web UI, EEA component, Redhat, Spark, Python', u'Big Data DevOps Engineer\nInfosys - Seattle, WA\nOctober 2016 to February 2017\nResponsibilities:\n\u2022 Ingestion multiple data source with Kinesis Stream, and loading data into S3 bucket\n\u2022 Install, deploy, update and maintenance Cloudera Big Data ecosystem in EC2 instance\n\u2022 Build Zeppelin-Spark application for data validation.\n\u2022 Design Redshift database for data science and BI team.\n\u2022 Working with PM to create Jira project and git repository.\n\u2022 Develop Python script and Linux script for daily report.\n\u2022 Test and debug potential security issue in Big data ecosystem.\n\nEnvironment: Spark, Cloudera, EC2, Redshift, S3, Linux-Redhat, VPC, Cloud Formation.', u'Big Data Engineer\nInfosys - Phoenix, AZ\nSeptember 2014 to October 2016\nResponsibilities:\n\u2022 Managing large scale data processing on the MapR BigData platform.\n\u2022 ETL integration using python and Spark with Business Intelligence tool, like Datameer, Tableau, and Pantaho.\n\u2022 Responsible for designing Data Architecture for a Big Data initiative to perform data analytics.\n\u2022 Using Java Eclipse with Maven project, modify and deploy Datameer customer-plugin to American Express Plantinum Environment.\n\u2022 Performed advanced procedures like high priority ETL processing, using the in-memory computing capabilities of Spark-pyspark..\n\u2022 Working on unix-based system, like Centos, Redhat. With excellent linux script skill, participate data transfer automation processing.\n\u2022 Extracted data from Oracle SQL and MySQL to HDFS using Java JDBC.\n\u2022 Integrate several database and data warehouse schemas to fetch the data from different tables.\n\u2022 Migrate Datalink and Workbook from SVN repository to product environment using python script.\n\u2022 Design encrypt and decrypt password architecture for usecase team and infrastructure team..\n\u2022 Deploy new schema of Hbase for Data Science to apply customer behaviour algorithm model into product environment.\n\u2022 Strong experience working with real time streaming applications and batch style large scale distributed computing applications using tools like Spark Streaming, Kafka, Flume, MapReduce, Hive etc.\n\u2022 Good knowledge on spark components like SparkSQL, MLib, Spark Streaming and GraphX, involved in ingestion and parsing of JSON file format using Spark.\n\u2022 Designed and developed RDD Seeds using Scala and cascading.\n\u2022 Developed custom aggregate functions using Spark SQL and performed interactive querying.\n\u2022 Data ingestion to HBase and Hive using spark streaming.\n\u2022 Worked with various clients with day to day requests and responsibilities.\n\u2022 Involved in analyzing system failures, identifying root causes and recommended course of actions.\n\u2022 Worked on Hive for exposing data for further analysis and for generation of files from different analytical formats to text files.\n\u2022 Management and scheduling of Jobs on a Hadoop cluster.\n\u2022 Experience design with Spark and EC2.\n\u2022 Familiar with AWS S3 and EC2 architect.\n\nEnvironment: Python, Spark, Java, Hadoop, Datameer, Linux', u'Hadoop Engineer(Java Developer)\nSAMSUNG SDS - Seoul, KR\nJanuary 2012 to September 2014\nResponsibilities:\n\u2022 Worked on a live Hadoop production environment with 60 nodes\n\u2022 Worked with highly unstructured and semi structured data of 40 TB in size\n\u2022 Designed and developed Pig ETL scripts to process data in a Nightly batch to perform analysis on the responses and times for various customer issues.\n\u2022 Created Pig UDFs and Macros to improve reusability of code and modularizing the code.\n\u2022 Developed Hive scripts for end user / analyst requirements for ad-hoc analysis\n\u2022 Very good understanding of Partitions, Bucketing concepts in Hive and designed both Managed and External tables in Hive for optimized performance\n\u2022 Solved performance issues in Hive and Pig scripts with understanding of Joins, Group and aggregation and how does it translate to MapReduce jobs.\n\u2022 Worked in tuning Hive and Pig scripts to improve performance.\n\u2022 Created Hive UDFs to encapsulate complex query logic for specific data analysis requirements.\n\u2022 Good experience in writing MapReduce programs in Java on MRv2 / YARN environment.\n\u2022 Good experience in troubleshooting performance issues and tuning Hadoop cluster.\n\u2022 Good working knowledge of using Sqoop in performing incremental imports from Oracle to HDFS.\n\u2022 Experience in using Sequence files, AVRO and HAR file formats.\n\u2022 Good experience in working with compressed files and related formats.\n\u2022 Developed Oozie workflow for scheduling and orchestrating the ETL process\n\u2022 Hands on experience with HBase and its architecture\n\u2022 Performed data analysis with HBase using Hive external tables to HBase\n\u2022 Very good understanding of Single Point Of Failure (SPOF) of Hadoop Daemons and recovery procedures\n\u2022 Experience in setting up Cloudera CDH4 nodes on Vmwore WorkStation\n\u2022 Worked with the infrastructure and admin team in designing, modeling, sizing and configuring Hadoop cluster of 60 nodes\n\u2022 Good working knowledge of Impala\n\u2022 Good knowledge in using job scheduling and monitoring tools like Oozie and ZooKeeper\nWell versed in Core Java.\n\u2022 Experience in developing solutions to analyze large data sets efficiently\n\u2022 Experience in Data Warehousing and ETL processes.\n\u2022 Knowledge of Star Schema Modeling, and Snowflake modeling, FACT and Dimensions tables, physical and logical modeling.\n\u2022 Strong database, SQL, ETL and data analysis skills.\nEnvironment: Hadoop, HDFS, Pig, Sqoop, HBase, Ubuntu, Linux Red Hat.']",[u'Certification in Big Data Engineering'],[u'University of California']
0,https://resumes.indeed.com/resume/1e4fb8ea57c0a8a8,"[u'Data Collection Engineer\nGlobal Wireless Solutions\nJanuary 2014 to Present\n\u2022 Configure the test equipment and record data on predefined route in an assigned market every day.\n\u2022 Ensure the integrity for the data, transfer the data to the office via FTP and communicate progress status to management as required.\n\u2022 Work with different wireless/cellular networks technologies and protocols\n\u2022 Navigate the data collection vehicle on the appropriate drive route using hard copy maps and mapping software such as DeLorme Street Atlas.\n\u2022 Safely transport GWS vehicle and data collection equipment between U.S. markets.\n\u2022 Set-up, takes down, test, and troubleshoot problems while in the field on laptops, cellular phones, and data collection equipment as needed via phone and/or Web session.\n\u2022 Work in cooperation with data administrators, supervisors, managers, and administrative staff in GWS office.\n\u2022 Maintain company driving ethics within markets.\n\u2022 Travel within the continental United States up to 100% of time.\n\u2022 Data Collection\n\u2022 Drive Testing\n\u2022 E911 Data Collection\n\u2022 4G, 3G GSM /EVDO/ EDGE/ CDMA/ UMTS/ LTE, NSB Optimization and Site Integration\n\u2022 Simultaneous Multi-Carrier/Multi-Technology Network Scan & UE Tests.\n\u2022 Performing Single site verification, stationary drives using download and upload scripts and generate good throughput on each sectors on LTE and UMTS Sites.\n\u2022 Analyzed drive test data on TEMS Investigation or NEMO Outdoor for network analysis, site validation, and problem identifications.\n\u2022 Outdoor and Indoor benchmarking. / Walk Testing\n\u2022 Cluster drives, troubleshooting drives and equipment setting for drive testing.t\n\u2022 E911 LIVE VOLTE Testing\n\u2022 E911 Simulated VOLTE Testing\n\u2022 E911 LIVE UMTS Test\n\u2022 E911 Simulated UMTS Testing.\n\u2022 NSB (New Site Built) Testing.\n\u2022 Mobility Integration\n\u2022 Test calls to 911 operators and record information.\n\u2022 Configure and record data on the predefined route in the assigned market.\n\u2022 Data transfer via FTP to the corporate office.\n\u2022 Navigation and mapping of assigned routes']",[u'in Computer Technology Course'],[u'Collin County College\nJanuary 2004 to January 2005']
0,https://resumes.indeed.com/resume/4896bb53692e66a9,"[u'Network Engineer/ Datacenter Engineer\nAtos- County orange - Santa Ana, CA\nJuly 2017 to Present\nResponsibilities:\n\u2022 Experience in data center, working for projects like transformation and operations.\n\u2022 As a dedicated network engineer on the WAN Transformation project, I lead phone calls that involved other network engineers and technicians from Atos. Configured Cisco routers and switches for over 100 sites.\n\u2022 Configured Cisco 1841 and 1721 routers using OSPF and BGP routing protocols, as well as 2950,2960,3560,4000, 6500,7K and 9K series switches to work with newly installed AT&T routers and Level 3 routers.\n\u2022 Designed network for government agencies.\n\u2022 Configured Cisco devices with various protocols like VRF (Virtual Routing and Forwarding), OSPF and BGP as per design and the number of users in a site.\n\u2022 Troubleshoot issues related to VLAN.\n\u2022 Performed IOS upgrades for Cisco routers and switches remotely by using TACACS.\n\u2022 Hands on experience working with tools like ServiceNow, SevOne, CMDB, Netsys and TACACS.\n\nEnvironment: Cisco 3750/3550/3500/2960 switches and Cisco 3640/12000 /7200/3845/3600/2800 routers, Cisco ASA5510, Checkpoint, Aruba Controllers 6000, 3600, 3400,650, Cisco Nexus9K/8K/7K/5K, Palo Alto', u""Data Center Network/Security Engineer\nWells Fargo - Minneapolis, MN\nMay 2014 to June 2017\nResponsibilities:\n\u2022 Implementing the necessary changes such as adding, moving and changing as per the requirements of business lines in a data center environment.\n\u2022 Involved in Switching Technology Administration including creating and managing VLANS's, Port security, Trunking, STP, Inter-VLAN routing, LAN security etc. Deploying Layer 2 security in Server Farms by configuring switch for 802.1x port based authentication.\n\u2022 Monitoring the network traffic with the help of Qradar and Cisco IPS event viewer.\n\u2022 Remediation of firewall rules from checkpoint firewalls to Cisco ASA firewalls and their implementation.\n\u2022 Worked on Aruba Wireless LAN Implementation for 11n Infrastructure Across the Corporate Network.\n\u2022 Managing and implementation of PORs (port open requests) based on the requirements of various departments and business lines.\n\u2022 Implementing and troubleshooting (on-call) IPsec VPNs for various business lines and making sure everything is in place.\n\u2022 Installing and configuring new cisco equipment including Cisco catalyst switches 6500, Nexus 7010, Nexus 5548 and Nexus 2k as per the requirement of the company.\n\u2022 Worked primarily as a part of the security team and daily tasks included firewall rule analysis, rule modification and administration.\n\u2022 Configuring & managing around 500+ Network & Security Devices that includes Juniper (Netscreen) Firewalls, F5 BigIP Load balancers and 3DNS, Blue Coat Proxies and Plug Proxies.\n\u2022 Adding and modifying the servers and infrastructure to the existing DMZ environments based on the requirements of various application platforms.\n\u2022 Working closely with Data center management to analyze the data center sites for cabling requirements of various network equipments.\n\u2022 Experience with F5 load balancers and Cisco load balancers (CSM, ACE and GSS).\n\u2022 Implement the firewall rules using Netscreen manager (NSM).\n\u2022 Implementation of UTM and IDP in SRX, SSG and ISG Firewall.\n\u2022 24x7 on-call escalation support as part of the security operations team.\n\u2022 Working configuration of new VLANs and extension of existing VLANs on/to the necessary equipment to have connectivity between two different data centers.\n\u2022 Managing and providing support to various project teams with regards to the addition of new equipment such as routers switches and firewalls to the DMZs.\n\u2022 Adding and removing checkpoint firewall policies based on the requirements of various project requirements.\n\u2022 Implementing IPsec and GRE tunnels in VPN technology.\n\u2022 Supporting project test teams in analyzing the bandwidth utilization.\n\u2022 Managing and upgrading IOS image files and taking configuration back-up.\n\u2022 Handling enterprise outages effectively and driving towards the resolution. Coordination of fault escalations in conjunction with the 1st high-level technical management of high priority or technically complex calls.\n\u2022 Working with Capacity management on network bandwidth utilization reporting of the sites WAN link and vendor co-ordination for new site turnovers / WAN links.\n\u2022 Preparing Metrics report detailing on SLA performance of tickets and process quality report to analyze team performance & discussion on the improvement areas (By monthly).\n\u2022 Providing training to new comers and effectively working towards a process quality improvement in the Team.\n\u2022 Excellent Troubleshooting Skills and Customer Centric approach.\n\u2022 Strong Knowledge in working with F5 Load Balancers and their Implementation in various Networks.\n\nEnvironment: Cisco 2948/3560/4500/3560/3750/3550/3500/2960 6500, 7K and 9K switches and Cisco 3640/12000 /7200/3845/3600/2800 routers, Cisco Nexus 7K/5K/2K, Cisco ASA5510, Checkpoint, Aruba Controllers 6000, 3600, 3400,650: windows server 2003/2008: F5 BIGIP LTM."", u""Network Engineer\nUSAA - San Antonio, TX\nMarch 2013 to April 2014\nResponsibilities:\n\u2022 Working in high availability Nexus Environment and provide Level 3 Network Support.\n\u2022 Configuring, upgrading and verifying the NX-OS operation system.\n\u2022 Converting PIX rules over to the Cisco ASA solution.\n\u2022 Basic and advance F5 load balancer and Cisco ACE configurations, general troubleshooting of the F5 load balancers.\n\u2022 F5 3DNS Load balancer GTM\n\u2022 Providing Level 3 support to customers, resolving issues by attending to conference calls.\n\u2022 Providing Level 3 Engineering and Support to other internal network engineers and contractors.\n\u2022 Designing and installing new branch network systems. Resolving network issues, running test scripts and preparing network documentation.\n\u2022 Translating Cisco IOS Route maps to Cisco IOS XR Routing policies.\n\u2022 Working with Cisco Nexus 2148 Fabric Extender and Nexus 5500 series to provide a Flexible Access Solution for datacenter access architecture.\n\u2022 Ensuring problems are satisfactorily resolved in a timely manner with focus in providing high level of support for all customers.\n\u2022 Working with BGP, OSPF protocols in MPLS Cloud.\n\u2022 Establishing VPN Tunnels using IPSec encryption standards and also configuring and implementing site-to-site VPN, Remote VPN.\n\u2022 Working with Juniper JUNOS operating system and working on M and MX series routers.\n\u2022 Providing daily network support for national wide area network consisting of MPLS, VPN and point-to-point site.\n\u2022 Configuring HSRP between the 3845 router pairs of Gateway redundancy for the client desktops.\n\u2022 Configuring GLBP, VLAN Trunking 802.1Q, STP, Port security on Catalyst 6500 switches.\n\u2022 Responsible for service request tickets generated by the helpdesk in all phases such as troubleshooting, maintenance, upgrades, patches and fixes with all around technical support.\n\u2022 Configuring, Monitoring and Troubleshooting Cisco's ASA 5500/PIX security appliance, Failover DMZ zoning and configuring VLANs/routing/NATing with the firewalls as per the design.\n\u2022 Converting Cisco IOS to Cisco IOS XR configurations.\n\u2022 Migration checkpoint Juniper, and Palo Alto Firewalls.\n\u2022 Configuring BGP, MPLS in Cisco IOS XR.\n\u2022 Configuring Virtual Device Context in Nexus 7010.\n\u2022 Configuring multiple route reflectors within a cluster.\n\u2022 Working on HP open view map for Network Management System and Ticketing.\n\u2022 Working on a broad range of topics such as routing and switching, dedicated voice access, planning and implementation, large-scale high-visibility outages, change management coordination, proactive monitoring and maintenance, disaster recovery exercise and core network repairs.\n\u2022 Involved in L2/L3 Switching technology administration including creating and maintaining VLANs, Port security, Trunking, STP, Inter Vlan Routing, LAN security.\n\u2022 Working on security levels with RADIUS, TACACS+.\n\u2022 Configured CIDR IP RIP, PPP, BGP and OSPF routing.\n\u2022 Involved in the configuration & troubleshooting of routing protocols: MP-BGP, OSPF, LDP, EIGRP, RIP, BGP v4. Configured IP access filter policies.\n\u2022 Configuration and management of NEXUS network in the existing network infrastructure.\n\u2022 Created LAB setup with 7k and 5K NEXUS switches for application testing.\n\u2022 Hands-on experience with WAN (ATM/Frame Relay), Routers, Switches, TCP/IP, Routing Protocols (BGP/OSPF), and IP addressing.\n\u2022 Configuring STP for switching loop prevention and VLANs for data and voice along with Configuring port security for users connecting to the switches.\n\u2022 Ensure Network, system and data availability and integrity through preventive maintenance and upgrade.\n\u2022 Involved in L2/L3 Switching Technology Administration including creating and managing VLANs, Port security, Trunking, STP, Inter-Vlan routing, LAN security.\n\nEnvironment: Cisco 3750/3550/3500/2960 switches and Cisco 3640/12000 /7200/3845/3600/2800 routers, Cisco ASA5510, Checkpoint, Aruba Controllers 6000, 3600, 3400,650, Cisco Nexus7K/5K, Palo Alto"", u""Network Engineer\nCanny Technologies - Hyderabad, Telangana\nJanuary 2012 to February 2013\nResponsibilities:\n\u2022 Involved in Configuring and implementing of Composite Network models which consists of Cisco7600, 7200, 3800 series routers and Cisco 2950, 3500, 5000, 6500 Series switches.\n\u2022 Basic and advance F5 load balancer configurations, including migrating configurations from Cisco ACE to F5 and general troubleshooting of the F5 load balancers.\n\u2022 F5 Networks BigIP Load Balancer.\n\u2022 Designed and implemented VLAN using Cisco switch catalyst 1900, 2900, 5000 & 6000 series.\n\u2022 Modified internal infrastructure by adding switches to support server farms and added servers to existing DMZ environments to support new and existing application platforms.\n\u2022 Built site-to-site IPSec VPNs over Frame-relay & MPLS circuits on various models of Cisco routers to facilitate adding new business partners to new and existing infrastructures.\n\u2022 Analyzed customer application and bandwidth requirements, ordered hardware and circuits, and built cost effective network solutions to accommodate customer requirements and project scope.\n\u2022 Configured routers and coordinated with LD Carriers and LECs to turn-up new WAN circuits. Configuring, Maintaining the Routers and Switches and Implementation of RIP, EIGRP, OSPF, BGP routing protocols and trouble shooting.\n\u2022 Experience with Project data and voice documentation tools & experience with developing network design documentation and presentations using VISIO\n\u2022 Possess good experience in configuring and troubleshooting WAN technologies like MPLS, T1, T3, DS3 and ISDN.\n\u2022 Responsible for implementing QOS parameter on switching configuration.\n\u2022 Involved in Design and Implementation of complex networks related to extranet clients.\n\u2022 Troubleshooting the Network Routing protocols (BGP, EIGRP and RIP) during the Migrations and new client connections.\n\u2022 Manage operational monitoring of equipment capacity/utilization and evaluate the need for upgrades; develop methods for gathering data needed to monitor hardware, software, and communications network performance.\n\u2022 Worked towards the key areas of the project to meet SLA's and to ensure business continuity. Involved in meetings with engineering teams to prepare the configurations according to the requirement.\n\u2022 Creating change tickets according to the scheduled network changes and implementing the changes.\n\u2022 Configuring STP for switching loop prevention and VLANs for data and voice along with Configuring port security for users connecting to the switches.\n\u2022 Ensure Network, system and data availability and integrity through preventive maintenance and upgrade.\n\u2022 Responsible for configuration and maintenance of a collapsed core network of 90 switches and routers along with maintenance of existing Wireless network.\n\u2022 Configuration and maintenance of the routers running on existing EIGRP and BGP protocol with 7200 router and 6500 core. Also configured route filtering using distribute list with route maps and ACL's.\n\u2022 Configured BIG IP F5 load balancer for cluster / server farm load balancing to increase resource availability and provide redundancy.\n\u2022 Configured network access servers and routers for AAA Security (RADIUS/ TACACS+)\n\u2022 Regularly performed firewall audits around Checkpoint Firewall-1 solutions for customers\n\u2022 Provided tier 3 support for CheckPoint Firewall-1 software to support customers\n\nEnvironment: Cisco 7200/3845/3600/2800 routers, TACACS, EIGRP, RIP, OSPF, BGP, VPN, Ether Channels, Fluke and Sniffer, F5 Load Balancer, Cisco ACE, RIP, EIGRP, BGP, OSPF, Checkpoint"", u'Network Engineer\nNet metric Solutions - Hyderabad, Telangana\nApril 2009 to December 2011\nResponsibilities:\n\u2022 Responsible for designing and implementation of customer network infrastructure\n\u2022 Negotiate hardware and software circuit contracts, Network Migrations\n\u2022 Redesign office copper and fiber cable plant scalability.\n\u2022 Configuring Cisco and Juniper devices (Router & Switches)\n\u2022 Dynamic routing protocol configuration (RIP, RIP V2).\n\u2022 Nat and IPsec configuration on Cisco Routers.\n\u2022 Involved in configuration of IP SLA.\n\u2022 Commission and de-commission network links and network hardware.\n\u2022 Planning, Implementation and servicing the Network Infrastructure.\n\u2022 Project Management for New Setup, Upgrade and Expansions.\n\u2022 Installation, configuration and troubleshooting for HP ProLiant servers across US region with the help of location teams and remote-in tools.\n\u2022 Implemented and configured routing protocols like EIGRP, OSPF, BGP\n\u2022 Troubleshot the issues related to routing protocols\n\u2022 Preparation of proper network cabling diagrams prior to installations and supervision of the LAN cabling contractor for compliance to industry standards for Ethernet.\n\u2022 Responsible for maintenance and utilization of VLANS, Spanning-tree, VTP of the switched multi-layer backbone with catalyst switches\n\u2022 Remote support with the help of VPN and Citrix applications.\n\u2022 Responsible to create, maintain and document the process and support procedures.\n\u2022 Handled the security patch management for servers and desktop environment.\n\nEnvironment: Cisco 7200/3845/3600/2800 routers, TACACS, EIGRP, RIP, OSPF, BGP, VPN, Checkpoint, Aruba Controllers 6000, 3600.', u'Jr.Network Engineer\nNetworkers Home - Bengaluru, Karnataka\nJune 2008 to March 2009\nResponsibilities:\n\u2022 Configuring and troubleshooting multi-customer ISP network environment.\n\u2022 Involved in network monitoring, alarm notification and acknowledgement.\n\u2022 Implementing new/changing existing data networks for various projects as per the requirement.\n\u2022 Troubleshooting complex networks layer 1, 2(frame relay, ATM, Point to Point, ISDN) to layer 3 (routing with MPLS, BGP, EIGRP, OSPF and RIP protocols) technical issues.\n\u2022 Providing support to networks containing more than 2000 Cisco devices.\n\u2022 Performing troubleshooting for IOS related bugs by analyzing past history and related notes.\n\u2022 Carrying out documentation for tracking network issue symptoms and large scale technical escalations.\n\u2022 Managing the service request tickets within the phases of troubleshooting, maintenance, upgrades, fixes, patches and providing all-round technical support.\n\u2022 Commissioning and Decommissioning of the MPLS circuits for various field offices.\n\u2022 Preparing feasibility report for various upgrades and installations.\n\u2022 Installation and maintenance of new network connections for the customers.\n\u2022 Configuring all the required devices and equipment for remote vendors at various sites and plants.\n\u2022 Installing new equipment to RADIUS and worked with MPLS-VPN and TACACS configurations.\n\u2022 Installing and maintaining local as well as network printers.\n\u2022 Validating existing infrastructure and suggesting new network designs.\n\u2022 Working on creating new load balancing policies by employing BGP attributes including Local Preference, AS-Path and Community, MED.\n\u2022 Installing and maintaining Windows NT Workstations and Windows NT Server.\n\u2022 Providing technical support to LAN & WAN systems.\n\u2022 Monitoring Memory/CPU on various low end routers in a network.\n\u2022 Design and Implemented OSPF and EIGRP on various sites for routing enhancement, high availability, and reducing administrative overhead\n\u2022 Co-ordinate with ISP on each step of designing BGP to understand their restrictions and our needs\n\u2022 Redesigned IP scheme to improve existing summary routes, CPU process, memory utilization and convergence time\n\u2022 Played a major role in designing, co-ordination and support while migrating from Verizon to Sing Tel on all sites\n\u2022 Designed templates for offshore team for Cutover; guided and distributed responsibilities for the same.\n\nEnvironment: Cisco 7200/3845/3600/2800 routers, TACACS, EIGRP, RIP, OSPF, BGP, VPN']",[u'Bachelors of Technology in Technology'],"[u'JNTU Hyderabad, Telangana\nJanuary 2007']"
0,https://resumes.indeed.com/resume/11fe566ddb6d0226,"[u'Industrial Engineer, Contractor\nvia Aerotek Inc - Richmond, VA\nJanuary 2017 to Present\nWorked with team members in analyzing and configuring routing sequences for their legacy operating system to help in the company wide conversion to Oracle and to generate labor cost analysis\n\u2022 Created and implemented efficient routing sequences for their operating system to calculate total labor cost and to help determine work measurement standards and utilization of workers\n\u2022 Analyzed current Bill of material (BOM) to help determine issues within the each builds routing sequence.\n\u2022 Conducted Time Studies and generated tables for BOM analysis and engineering support for over 40,000 parts.\n\u2022 Studied sequences of operations, the flow of materials, and areas of duplication to provide project management support\n\u2022 Used Excel generation and manipulation for cost analysis and process reporting purposes for each area of production.', u'Data Manager\nJ&G Consulting Services - Richmond, VA\nJune 2010 to October 2016\nDeveloped data collection and evaluation methodologies, including format design, project criteria and requirements, data compilation, relevancy and usage:\n\u2022 Troubleshoots data submission errors and data error issues for multiple personnel\n\u2022 Assisted with reports and data extraction when needed\n\u2022 Formulated techniques for quality data collection to ensure adequacy, accuracy and legitimacy of data\n\u2022 Established and maintained electronic records management system for all incoming and outgoing correspondence.\n\u2022 Created a systematic and reliable computerized customer database using Microsoft Access\n\u2022 Provided direct technical and administrative support in the delivery of vocational rehabilitation services to enable clients to obtain gainful employment']",[u'Bachelor of Science in Industrial and Systems Engineering'],"[u'North Carolina A&T State University Greensboro, NC\nMay 2016']"
0,https://resumes.indeed.com/resume/56039a4e51d3741d,"[u'Data Analyst\nAnmol LLC - Milwaukee, WI\nSeptember 2016 to Present\nMajor responsibilities are:\n\u2022 Creating a Robust database design for the organization and reviewing the data on a daily basis and make sure that the data is properly inserted to the system.\n\u2022 Recommending specific operating system, software\u2019s and works on cost analysis and gets it approved by the management of the organization and implement new database system for users/clients.\n\u2022 Create reports and write small programs to test and analyze the database.\n\u2022 Manage data from the beginning to the end on a project basis\n\u2022 Preparing specification documents for the users\n\u2022 Responsible for all the data inside the database, frequent backup of the database, in order to make sure that data is not lost and keep records of the archives\n\u2022 Analyze and interpret results using standard statistical tools and techniques\n\u2022 Provide concise data reports and clear data visualizations for management\n\u2022 Design, create and maintain relational databases and data systems', u'Data Collection & Monitoring Assistant\nPeshawar\nOctober 2015 to August 2016\nThe key objectives are as follows:\n\n\u2022 Provide Elementary & Secondary Education Department and its development partners with a viable means of ensuring that education sector reforms are efficiently developed and effectively implemented\n\n\u2022 Support the establishment of performance monitoring mechanisms\n\n\u2022 Deliver objective information on a monthly, quarterly and annual basis to donors, civil society, parents and other stakeholder for reviewing the performance of government and its officials\n\n\u2022 Increase public awareness of the status of school facilities and infrastructure and the level of education service delivery in the province, district and the school level\n\n\u2022 Provide access to information on key indicators to increase social accountability\n\n\u2022 Increase social accountability to improve governance, service delivery outcomes, planning and resource allocations to the sector\n\nMain projects: Electronics project\n\u25e6 RFID based home/building automation and access control.(final year project)\n\u25e6 Antenna designing.\n\u25e6 Digital communication modulation schemes.\n\u25e6 FPGA based mini projects.\n\u25e6 PLC based mini projects.\n\u25e6 SCADA.\n\u25e6 Microcontroller based projects.\n\nComputer Science Projects and IT projects\n\u25e6 Web designing and development (semester project)\n\u25e6 C# (windows form application and data base)\n\u25e6 Development of a feature-rich, practical online leave management system.\n\u25e6 Development of an online sales and inventory Management system.\n\nComputer Software: Microsoft Office, Matlab & Simulink, Maple, Xilinx, AutoCAD, Pspice, Work bench Programming: C/C++, OOP, VHDL, PLC, Assembly, HTML, PHP.', u'technical Co-ordinator/ QA Engineer\nKFW Green - Peshawar\nMarch 2013 to October 2015\nstar Pakistan FATA project.\n\n* Major responsibilities are is to Provide technical support, HSSE, System Analyst, Quality check, troubleshoot and resolve telecommunications- related problems, Which may include training and instructing others on the use of various equipment/services, Performing program changes, prioritizing tasks and service level goals, Maintain and monitor inventories of equipment, including phones, telecommunication and installed parts and other equipment.\n* Pashto linguist at tele-medicine project.\n\nIndependent Monitoring Unit Khyber Pakhtunkhwa Pakistan', u'Site Engineer/ Microwave engineer\nZTE-CM-Pak BTS Swap Project - Chittagong\nFebruary 2012 to March 2013\n\u2022 The entire installation configuration, commissioning and testing of SDR and Power Equipment.\n\u2022 Troubleshooting of SDR Equipment at different sites of CM-PAK (Zong).\n\u2022 EASS Configuration and Patching.\n\u2022 Installation, commission and testing Bts.\n\nProject: ZTE-Telenor Pakistan Vega Project..\nRESPONSIBILITIES:\n* Quality check of installed equipment\n* Health Safety Environment(HSE) and HSSE Trainer.\n* Experience of commissioning & Integration on iPasolink(NEC).\n* Experience of ZTE SDR Installation & Commisioning\n* Line of Sight verification, Reports verifications, Fresnel zone clearances, link sizing, link power budget.\n\nPakistan telecommunication swat January - March 2009\n* Worked as an internee.\n\nKFW Bankengruppe GERMANY Funded project (Fata secretariat Peshawar Pakistan):']","[u'Bachelor in Electronics Engineering', u'certification in English language', u'Government in Mathematics', u'']","[u'Comsats University Abbottabad Pakistan\nJanuary 2008 to January 2012', u'National institute of modern languages\nJuly 2009 to August 2009', u'Government degree college\nJanuary 2005 to January 2007', u'HIRA secondary school Baidara Matta Swat Pakistan\nJanuary 1995 to January 2005']"
0,https://resumes.indeed.com/resume/31e9e5f5101de84c,"[u'Big Data Engineer\nZILLOW GROUP/TRULIA - San Francisco, CA\nApril 2012 to December 2017\nBash, Hadoop, SQL, Hive, Presto, AWS, Jenkins, Git\nData Warehouse Team\n\u25e6 Lead Engineer/Lead Devops for our Data Warehouse Hadoop Cluster. Responsible for keeping up the health of the cluster. Noteworthy tech includes Yarn, Tez, M/R, ORC, Presto and container monitoring.\n\u25e6 Lead Engineer and architect for our ETL Pipelines. Jenkins is our job scheduler; Bash is our JCL. SQL on Hadoop is our ""engine"". All these are written in well-defined functional patterns. No code slop allowed.\n\u25e6 Presto Evangelist. I maintain this infrastructure separately from our ops team. In addition I enjoy contributing where I can to this group outside of Zillow.\n\u25e6 Longtime database architect. I enjoy using SQL to employ ETL paradigms and have authored an in-house sql client that can seamlessly interface with hive, mysql and presto.\n\u25e6 Member of a horizontal team working to build a real-time data capture system. This technological stack is heavy on Lambda Architecture but as of this writing we are bootstrapping our data via a Syslog \u2192 Hive \u2192 Kafka \u2192 Hbase pipeline.', u'contractor\nBLACKROCK - San Francisco, CA\nJanuary 2011 to January 2012\nto WA). This was all Solaris based and involved being our own DBA and sysadmin. We installed and configured the new databases -- and automated the procedure to sync the data daily after trading hours. This was all automated using shell, perl, mysql and Sybase utilities.\n\u2022 Worked on a broker scorecard application to pull metrics from a data warehouse and visualize via Excel graphs and then bundle dynamically into a powerpoint presentation using live links. We automated over 500 charts this way into decks customized for specific brokers. This was all automated via Ruby.\n\u2022 Wrote a daily ETL application to pull Prime Broker securities financing data from Oracle database to Sybase IQ db while enriching with reference data. This was done with shell, Oracle and Sybase sql clients using dynamic sql.\n\nMORGAN STANLEY, New York 1998 - 2011', u'BANKERS TRUST - New York, NY\nJanuary 1996 to January 1998', u'MERRILL LYNCH - New York, NY\nJanuary 1995 to January 1996', u'CALLAN ASSOCIATES - San Francisco, CA\nJanuary 1989 to January 1994', u'SOFTOOL CORP\nJanuary 1984 to January 1988']","[u""Bachelor's of Science in Computer Science""]",[u'Michigan State University\nJanuary 1983']
0,https://resumes.indeed.com/resume/3383cd6d5400df3e,"[u""Senior Data Engineer/Team Lead\nRiskonnect Inc - Atlanta, GA\nJuly 2012 to March 2018\nResponsibilities:\n\u2713 Get on calls with clients and Business Analysts, and Successfully designed the technical mapping\ndocumentation, interpreting the business logic.\n\u2713 Understand the required Salesforce model and work with the product team to create the cloud\ninfrastructure.\n\u2713 Coordinate with data providers to ensure on time delivery of the source data, coming from disparate\nenvironments and formats\n\u2713 Understand and write complex TSQL queries, views, stored procedures and cursors to massage the data\n\u2713 Build SSIS packages, use different types of tasks and containers in it, like Data Flow Task, Execute\nSQL Task, for each loop container, Sequence Container to name a few to perform proficient ETL\nprocess\n\u2713 Successfully Build Datawarehouse and Data Models using SSAS\n\u2713 Identify inefficiencies in the process and create out of the box solutions to gain more than 50% efficiency\n\u2713 Load the huge amount of data into the 'Salesforce' cloud model for each of the clients using\n'SF_BULKOPS', 'SF_REFRESH' and 'SF_REPLICATE' syntax\n\u2713 Wrote and fine-tuned complex SQL queries and stored procedures to fulfill reporting requests,\nautomation, and performance enhancement\n\u2713 Occasionally create Cognos reports to be integrated with salesforce\n\u2713 Provide training to the onsite and off shore teams, & create training documentations\n\u2713 Automate processes using Visual cron tool and create best practices documents\n\u2713 Anchored various critical projects to successful completion on time\n\nEnvironment"", u""Data Analyst & Programmer\nCognizant Technology Solutions - Chennai, Tamil Nadu\nJuly 2012 to March 2018\nJuly 2012 - Mar 2018\n\nResponsibilities:\n\u2713 Successfully improved the accuracy of data reported by writing complex TSQL queries and stored\nprocedures\n\u2713 Increased the efficiency of the process by 50%, by implementing SSIS packages as per the business\nlogic\n\u2713 Used various transformations like Look Up, OLE DB Command, and Conditional Split etc to transform the data as per the requirement\n\u2713 Provided better access to the reported data for MIS reporting, by building complex SSRS reports\n\u2713 Wrote SQL Queries and modified existing procedures in DB2\n\u2713 Successfully designed and built data models using both star schema and snow flakes schema\n\u2713 Built OLAP cubes using SSAS 2008\n\u2713 Built efficient dashboards for measuring KPI's using Performance Point Server\n\u2713 Created SharePoint sites and integrated the reports and dashboards into it\n\u2713 Development of SharePoint sites using SharePoint designer 2007 and 2010\n\u2713 Migration of dashboards and reports from SharePoint designer 2007 to 2010 version\n\u2713 .NET framework was used as the front end in the project\n\u2713 Analyzed .NET codes and implemented an equivalent logic in SQL Server\n\u2713 Occasionally Worked on Data Mining Analysis as per the requirement\n\u2713 Was part of the complete life cycle of the project, starting from requirements gathering up to the successful go live finish\n\nEnvironment"", u'SSMS, SSIS\nJanuary 2012 to January 2012\n2012, Salesforce, Visual Cron, Cognos 10, Microsoft Office (Word/\nExcel/Power Point)\n\n2', u'SharePoint Designer\nChennai, Tamil Nadu\nJanuary 2007 to January 2010\nNET Framework, DB2\n\nCognos Report Developer Chennai, TN, India', u'Changepond Technologies\nJuly 2008 to February 2009\nUnderstand the Banking data model\n\u2713 In depth understanding of the project model\n\u2713 Development of the reports required using Cognos 8 report studio\n\u2713 Delivery of the Reports as per the requirement\n\u2713 To keep track of the changes regarding the data model and the reporting requirement and implement the same\n\u2713 Preparation of Test Case document\n\u2713 Code reviews on defect fixes\n\nEnvironment\nSQL Server 2008, SSMS, Cognos, Microsoft Office (Word/ Excel/Power Point)', u'SSMS, SSRS\nJanuary 2008 to January 2008\n2008, SSAS 2008, SSIS 2008, Performance Point Server,']",[u'Bachelor of Technology in Electronics & Communication Engineering'],[u'Anna University\nJanuary 2004 to January 2008']
0,https://resumes.indeed.com/resume/285b3dd44f41e5c6,"[u""Project Manager\nVerizon\nMarch 2011 to February 2018\n\u2022 Implement new projects and changes\n\u2022 Analyze existing processes and procedures\n\u2022 Implement, Lead changes and/or requirements, develops related standards required to develop engineering packages and implement new systems\n\u2022 Works with external vendors and internal departments to include: Technology, Finance, Procurement, and Logistics to obtain specified equipment for deployment into the network\n\u2022 Handled Customer escalations, Work with Quality Assurance\n\u2022 Ensures the implementation of the Network, Hardware and software using SDLC methods\n\u2022 Completion on schedule and ready to support customers and the hand off to Operations and IT departments\n\u2022 Develops site specific Engineering packages for site buildout and site closeout\n\u2022 Negotiation with internal group for specific expertise for the project\n\u2022 Develops and evaluates engineering processes and standards for Metro Ethernet, Wireless, VOIP, Customer Education, Core, Data Center Migrations and Metro Networks to ensure maximum efficiency\n\u2022 Works with Marketing and IT teams to support new product development\n\u2022 Implementation of Engineering related projects\n\u2022 Circuit grooming for cost savings\n\u2022 Cisco UCS, Amazon AWS migration, Automation Tools\n\u2022 Project Managed QA Testing for new development and updates to all automated testing in a\n\u2022 acceptance test driven development and Agile environment.\n\u2022 Researched and gained insight from others for a budget approval submission and approval of 1.5 million for QA Testing.\n\u2022 Tasked with unit testing, functionality testing, regression testing, integration testing, black box,\n\u2022 white box, and UAT (user acceptance testing).\n\u2022 Communicated status and testing results to management and business stakeholders via reporting.\n\u2022 Demonstrated reliability by taking necessary actions to continuously meet required deadlines and goals.\n\u2022 Preparation of Project reports and reporting to upper Management\n\u2022 Worked with vendor for RFQ, better solutions for network design and new equipment and software decisions\n\u2022 Operational post implementation and Change Management review\n\u2022 Attained purchase orders and budget approvals\n\u2022 Project Management interface between the business, IT, other FSG groups and other Finance Transformation offices\n\u2022 Drive shared test team to execute, status, and resolve Functional Unit (FUT), Integrated (ITC), and User Acceptance Testing (UAT)\n\u2022 Flexibility and collaborative work approach to solve complex problems and to develop customer specific solutions - taking into account multiple internal and external stakeholder groups and different level of stakeholder groups\n\u2022 Top-Management) with various/changing stakeholder demands/requirements and intersections\n\u2022 IT, Finance, AP, SC, Top-management level)\n\u2022 Compliance to policies and procedures or direct impact on the re-design of existing policies and procedures\n\u2022 Identify and manage project resource needs and staffing process including subject matter/part-time resources\n\u2022 Lead deliverable acceptance and invoice approval process\n\u2022 Establish and oversee effective issue resolution, change control, and risk management processes\n\u2022 Maintain project plan (and phase gate reviews) and report semi-weekly status dashboards at project, Operations Committee, Key Stakeholders, and Executive Steering Committee levels\n\u2022 Manage compilation of schedule, status, and metric updates at project and team levels; report appropriately to all levels of governance including executive\n\u2022 Lead evaluation of go/no-go criteria for technical release and business cutover. Oversee implementation of method and tools which enable each phase. XO Communications, LLC/Verizon Network Engineer Cut over of new services being provided on XO's Broadband and SONET network. Essential Duties and Responsibilities: Technician should provide a professional quality installation to insure XO is delivering a positive Customer Experience"", u'Project Manager\nXO Communications, LLC/Verizon\nMarch 2011 to March 2011', u'Network Engineer\nVerizon\nMarch 2008 to March 2011', u'XO Communications, LLC/Verizon\nFebruary 2007 to March 2008\n\u2022 Ensures project/orders are delivered on time, striving to exceed customer expectations. Effectively communicates dependencies and project status to customer and executives. Drives to achieve/exceed monthly revenue goals for the business. Applies solid knowledge of the organization, influencing others through effective matrix management skills to ensure a positive outcome for both the customer and XO\n\u2022 Performs milestone tracking and implementation focus across a large cross-functional team with little to no supervisory guidance. Proactively manages change in scope, identifying potential issues and devises contingency plan or alternate solution with least impact to the customer. Organizes/manages network overbuilds and outside plant construction in support of customer project/order, working directly or through organizational resources as available as needed\n\u2022 Builds strong relationships with downstream partners ensuring cohesive cross functional collaboration\n\u2022 Demonstrates knowledge of and proficiency in installation procedures for standard, less complex products and services. Effectively applies company best practices, however understands when a creative out of the box solution is appropriate; analyzes operational performance data to provide continuous improvements for the Service Delivery organization.\nLAN/WAN support and troubleshooting', u'Data Support Repair\nFebruary 2004 to February 2007\n\u2022 Data Support Repair LAN/WAN support and troubleshooting\n\u2022 Data/Voice set-up and maintenance and support, TCP/IP, Frame Relay, ATM, Cisco Pix, Cisco WAP, VPN, Checkpoint firewall configuration, rule-sets, and troubleshooting\n\u2022 Cisco Ethernet switch, Cisco router support and configuration\n\u2022 T-1, VOIP, DSL support (Test and Turn-up) and network cabling\n\u2022 DNS, DHCP, SMS, RAS connections, hardware and software upgrades\n\u2022 Used ticketing system to work tickets, followed up with customers, meet SLA', u'Data Engineer\nACS\nAugust 2003 to January 2004\n\u2022 LAN/WAN support and troubleshooting\n\u2022 Data/Voice set-up and maintenance and support, TCP/IP, IPX/SPX, Frame Relay, ATM, Cisco Pix, Cisco WAP, VPN, Checkpoint firewall configuration, rule-sets, and troubleshooting\n\u2022 Cisco Ethernet switch, Cisco router support and configuration\n\u2022 T-1, VOIP, DSL support (Test and Turn-up) and network cabling\n\u2022 DNS, DHCP, SMS, RAS connections, hardware and software upgrades\n\u2022 Used ticketing system to work tickets, followed up with customers, meet SLA\n\u2022 Unix Scripting, Avaya PBX, Project Management of new implementations and Chaired conference calls. . US Air Force - Honorable Discharge']","[u'', u'']","[u'Richland Community College\nJanuary 2015', u'University of Maryland\nJanuary 1990 to January 1992']"
0,https://resumes.indeed.com/resume/3c144554604823eb,"[u'Data Scientist\nEarthnetworks R&D Group - Germantown, MD\nOctober 2014 to April 2018\n- Develop an energy consumption prediction model for demand response, Reduce 5% total energy consumption in peak time.\n- Create a HVAC schedule ranking algorithm for WeatherBug monthly score card, to help user archive better energy efficiency.\n- Creatively crawled real estate data to build a statistic model to solve any house size estimation problem for Connective Saving Program.\n- Improve and automate the weekly report email generation and delivery process, integrate database, restful API with AWS Lambda.\n- Design a web crawler framework to greatly simplify the work and codebase by 50% for energy price, census, historical thermo data crawler.', u'Data Scientist\nThe EagleForce Inc - Herndon, VA\nFebruary 2014 to October 2014\n- Full Stack developer, develop entire ETL process and software that performs data acquisition, wrangling and storage.\n- Analyze Electronic Health Record (EHR), disease and drug data, build monitor/alert system to help day care facilities prevent accident and give them real-time alerts.\n- Integrate server code (Ruby) with scientific code (Python), bridge gaps between software engineering and scientist.', u'Data analyst/Test engineer\nCybioms Corporation - Rockville, MD\nSeptember 2013 to February 2014\n- Perform signal process analysis and data visualization on the measurement data to demonstrate business story.\n- Analyze measurement and instrument log data to quickly identify internal problem.\n- Develop automated testing tools in python to manage hardware stack.']","[u'M.S. in Electrical and Computer Engineering', u'M.S. Electrical Engineering and Information System in Machine Learning', u'B.S. Observation Control Techniques and Instruments in Electro-Mechanical Engineering']","[u'The George Washington University Washington, DC\nSeptember 2011 to May 2013', u'Xidian University XiAn\nSeptember 2008 to July 2011', u'Xidian University XiAn\nSeptember 2003 to July 2007']"
0,https://resumes.indeed.com/resume/9707bf42900fc00c,"[u'SQA Engineer\nRenewData, an LDiscovery Company\nJanuary 2011 to January 2016\n* Developed, modified, and executed software test plans in an Agile environment while maintaining documentation of test results using Jira and Microsoft Team Foundation Server.\n* Reviewed, created and performed end-to-end testing based on requirements to ensure code conformed to specifications.\n* Consulted with development engineers in resolution of defects.\n* Gathered requirements from users and translated them into user stories and test cases.\n* Authored Admin and User guides for fully developed products.', u'Litigation Data Analyst\nRenewData, an LDiscovery Company\nJanuary 2009 to January 2011\n* Coordinated and tracked various eDiscovery projects progress and workflow from kick-off through production for multiple projects.\n* Performed project analysis, report generation and research as needed.\n* Managed client requests, including but not limited to: data processing within LAW, Ipro and Digital Reef, database creation and customization, data ingestion and production management.\n* Acted as final layer of quality control for any work product being delivered to a client.\n* Coordinated efforts across multiple operational and support project teams, ensuring interdependencies across departments flowed smoothly and completed in a timely manner to adhere to project deadlines.\n* Managed all phases of analytics and indexing support, including but not limited to: loading, quality control and troubleshooting related to, dtSearch index, Lucene index, Equivio and Anagram.\n* Solid understanding of a variety of litigation databases and electronic discovery tools, including Relativity, LAW Pre-Discovery, Ipro eCapture, Digital Reef, Nuix, Summation and Concordance.\n* Proficient in communicating, both written and verbal, to all levels of audiences (whether technical or non), while maintaining a professional demeanor under pressure and multitasking in a fast-paced environment.', u'Claims Data Analyst\nGreat American\nJanuary 2006 to January 2009\n* Analysis of claims data to provide insight into department objectives.\n* Software quality assurance testing of new features and upgrades to existing software.\n* Consolidated monthly production reports and audits.', u""Supervisor\nU.S. Army\nJanuary 2000 to January 2004\n* Supervised a 5 soldier team to include monthly evaluations, counseling and job training.\n* Planned and coordinated training seminars, high-level management meetings and special events.\n* Responsible for tracking company's cash flows; directed fundraising activities.""]","[u'Associate of Applied Science in Software Testing Specialization', u'Associate of Science in Business Administration']","[u'Austin Community College Austin, TX\nJanuary 2015 to January 2016', u'Austin Community College Austin, TX\nJanuary 2004 to January 2006']"
0,https://resumes.indeed.com/resume/ec4f2f12c9234788,"[u'Data Analyst\nHaworth college Western Michigan University\nJanuary 2018 to April 2018\nDepartment of computer Information System- Haworth college Western Michigan University\n\u2022 I was selected for this project based on my academic records and extracurricular activities\n\u2022 It is a project on how IoT is used in health care systems. We analyze different protocols and architecture used in different devices used in healthcare like the ECG data collector etc.\n\u2022 I created a database using SQL about the hospitals and health centers nearby and analyzed the trend for developing more IOT related devices in healthcare.', u'Intern\nToshiba Pvt Ltd\nJuly 2017 to July 2017\n\u2022 Studied the organizational Structure of the organization\n\u2022 Contributed by analyzing inefficiencies and suggesting improvements\n\u2022 Helped me to understand other organizational structures and workflows effectively', u'Intern\nTVS & Sons Pvt. Ltd\nApril 2017 to July 2017\n\u2022 Done internship under various department as part of course.\n\u2022 Worked with dealer management system software, Accepta - a financial software, Travelex software etc.\n\u2022 Responsible to meet the target for the month of May 2017 in the Insurance department including both new\ncustomers and renewals.', u'WLAN Test Engineer\nBroadcom Pvt Ltd\nJuly 2015 to May 2016\n\u2022 Worked with USA team using virtual meetings and VPN connection.\n\u2022 Worked under the project Samsung s7phone WIFI chip and developed Test plan for various testing.\n\u2022 Automating the testing processes using scripting languages -TCL and shell.\n\u2022 Wireshark and Ethereal based sniffer analysis of WLAN features for test case debugging.\n\u2022 Testing, debugging and analysis experience in latest 802.11K standard.\n\u2022 Responsible for Broadcom longevity driver stress/regression testing.\n\u2022 Maintaining Broadcom System Performance Compatibility Longevity WiFi devices lab stations.\n\u2022 Test plan development for different WLAN software features.\n\u2022 Debug and execute test cases on Broadcom WiFi modules in different platforms, station bring up, driver\nqualification, configuration, monitoring, and reporting bugs/issue.\n\u2022 Authorized to setup/maintain SVT (Software Verification Testing) lab.']","[u""Master's in Business Administration in Relevant Coursework"", u""Master's in Business Administration in Relevant Coursework"", u'', u'Bachelor of Technology in TMR', u'in Career Guidance and Placement Unit Coordinator']","[u'Western Michigan University\nAugust 2017 to July 2018', u'Christ University Bengaluru, Karnataka\nJune 2016 to July 2017', u'Centre For Social Responsibilities Christ University\nJune 2016 to July 2017', u'University of Kerala Bengaluru, Karnataka\nAugust 2011 to June 2015', u'Government Engineering College Barton Hill\nAugust 2011 to June 2015']"
0,https://resumes.indeed.com/resume/51eb3d90cd31e015,"[u'Senior Software Engineer\nKeySoftware Inc - Moorpark, CA\nJanuary 2014 to January 2016\nAs Business Analyst\n\u2022Participated in requirements discovery sessions with various teams of Orthonet to gather business requirements.\n\u2022Analyzed business requirements and create software requirements specification document which will aid in creating system design document.\n\u2022Performed GAP and Risk analysis of the existing system and evaluated benefits of the new system whenever new features are requested.\n\u2022Attended daily scrum meetings with the development team to discuss the status of the project tasks.\n\u2022Worked with QA to design test plan and test cases for User Acceptance Testing (UAT).\n\u2022Created release document that has the code release details and testcases and deployment details and along with system integration tests to check system integrity after the release.\n\u2022Conducted demo and release notes to end-users as part of administrative closure process to get formal acceptance from customers about deliverables.\n\nAs Software Engineer\n\u2022Customized focus review applications for 30 (as of Nov2016) different clients by creating the new database for each contract and encapsulating parts of the application based on the client login and user level.\n\u2022Developed ETL applications that can upload multiple filetypes.\n\u2022Developed custom user controls using standard WinForms.NET libraries and created an internal API.\n\u2022Developed consistent DB schema structures for avoiding scalability issues in future.\n\u2022Extensive coding of TSQL and usage of SQL job agents for auto-scheduling procedure runs and Triggers for logging data corrections.\n\u2022Organized all C# code, libraries, and internal APIs into MVC software architecture.\n\u2022Initiated agile methodologies into our team and usage of version control system like TFS and created in-house project management system using Excel spreadsheets.', u'Software Engineer\nRxNT - Annapolis, MD\nFebruary 2012 to December 2013\nMy role is to design, develop and maintain Practice Management (ePM) website.\nGathered business requirements with customers and internal support team.\nSprint planning and daily discussion with the support team and analyzing business requirements.\nDevelopment of website using Jquery, JavaScript, AJAX, HTML, CSS, C#, VB, ASP.NET, and MSSQL Databases.\nCoordinate with testing team to define test cases and deployment strategies\nDeveloped an ePM API to communicate with other RxNT websites.\nDeveloped C# controller library that checks for errors in data and prerequisite information that is specific to each client. (a.k.a data scrubbing)\nDeveloped an API in C# that encapsulates all related PDF documents and claim information in specific format in XML\nDeveloped a scheduling system in SQL database that can store different scheduling time for performing various actions (claims filing, creating patient records, etc.)\nDeveloped a C# library to calculate various amounts (credits, the amount due, adjusted amounts, cumulative amounts for a member)for creating patient-statements.\nDesigned UI and developed web pages to file a claim, edit details of the claim, used frames for PDF viewer to view medical documents, payment information, patient information. Used AJAX for asynchronous calls in webpages.\nWorked with Telerik and ComponentArt user controls to create UI rich web pages for reporting, applying filters on a grid. This required extensive coding in JavaScript and C#.\n\nCreated a website for Sales and Marketing team to get analysis reports, client usage reports, quarterly reports for their presentations.\nUsed Google Analytics for analyzing usage of websites.\n\nLearned VBScript to work on old websites\nDeveloped and maintained webpages related to RxNT in-house CRM website.\nDesigned practice management website by coordinating with the support team and clients feedback. Created GUI prototypes on paper to discuss with support team if requirements are met.', u'Graduate Research Assistant: Data Analyst\nKansas State University\nJanuary 2010 to May 2010\nDeveloped programs in Python for Plants DNA Data Analysis. This Software was used for DNA Analysis of Wheat and Rice Plant Families. This sped up the Analysis and Reports Generation of various Projects.']","[u'Master of Science in Software Engineering', u'Bachelor of Science in Computer Science Engineering']","[u'Kansas State University Manhattan, KS\nJanuary 2009 to January 2011', u'CVR College of Engineering Hyderabad, Andhra Pradesh\nJanuary 2005 to January 2009']"
0,https://resumes.indeed.com/resume/19422f2585ad53a3,"[u'Data Analyst\nSun Life Financial - Gurugram, Haryana\nDecember 2015 to June 2017\n\xb7 Optimized SQL queries and views for reporting applications by fine-tuning the indexes; improved processing time by 12 %\n\xb7 Computed trend analysis on Active vs Terminated Contracts. Built production similar data utilizing R and predicted policies with high probability of termination employing SAS predictive analytics and implemented visualizations on Tableau, resulted in reducing terminated policy by 14%\n\xb7 Engineered process to provide graphical representation using Tableau for Agent satisfaction index by capturing results from 13 different reports; resulted in increasing agent satisfaction index by 2 points\n\xb7 Collaborated with client to understand data needs, built visualization models using Tableau representing Client Demographic; facilitated the organization to understand client requirement\n\xb7 Automated KPI measurement for 9 teams by developing 9 new Jira dashboards; saved 80 manual hours per month', u'Software Engineer\nNTT DATA Services - hyderbad, Telangana\nNovember 2013 to June 2015\n\xb7 Re-engineered process for processing Tax-Free Savings Account policies; proposed a new Java \u2013 COBOL hybrid architecture; reduced project upgrade cost by 100,000 Dollars; received Gem Award from the Project Director for outstanding contribution\n\xb7 Played instrumental role in migrating legacy mainframe applications designed to handle Business Owner application to JAVA\n\xb7 Estimated cost and evaluated market risk of moving to DevOps compatible development environment, analyzed various software tools, calculated project feasibility; resulted in implementation of agile methodology in the development process\n\xb7 Devised optimized process for unit testing mainframe applications, cut down unit testing time by 16% per project\n\xb7 Executed root cause analysis for critical mainframe applications; ensured 100 % batch availability\n\xb7 Created custom reports to track batch running stability using SAS and SQL; increased batch stability by 19%\n\xb7 Conducted POC for ETL (Extract, Transform and Load) of DB2 tables to test the feasibility of moving to Oracle database\n\xb7 Led a team of 3 members to implement cross team audit process, lowered project documentation error by 23%']","[u'Masters In Management Information System in Management Information System', u'in Bachelor of Technology in Electronics and Communication Engineering']","[u'Mays Business School, Texas A&M University\nJune 2017 to May 2019', u'University of Allahabad (Central University of India) Allahabad, Uttar Pradesh\nAugust 2009 to May 2013']"
0,https://resumes.indeed.com/resume/5433ef2e929c6040,"[u'Senior RF Engineer\nT-Mobile - Irvine, CA\nAugust 2016 to Present\n- Worked with the Capacity team in the Market 3-year strategic planning; responsible for identifying the appropriate solution based on growth predictions.\n- Responsible for CIQ creation for new, relocated, or modified sites; including: small cells, DAS, and macro sites.\n- Member of the RF Engineering team responsible for troubleshooting and optimizing seven clusters in Southern California market.\n- Conducting daily LTE performance assessment then resolution on the worst performing cells with respect to: congestion, leakage, and low downlink throughput.\n- Conducting daily network KPI analysis to assess performance degradation on top movers then use RF engineering methodologies to analyze the root cause for access failures, dropped calls, mobility failures, and/or reduced traffic then resolve.\n- Resolving customer\u2019s complaints by analyzing drive test data then optimizing using RF network parameters or physical site modifications. In some B2B cases, working with Development and ACS teams to design and implement a dedicated solution.', u'Senior Data Engineer\nSprint - Irvine, CA\nNovember 2014 to June 2016\nResponsibilities\n+ Market lead for CDMA and LTE network data analysis and reporting for LA Metro, North LA, Orange County, Inland Empire, and San Diego markets.\n+ Developed automation tool for CDMA network to pull Configuration Management (CM) data directly from OMP, parse, and store data inside SQL database. Performed complex analysis on stored data to check for any first or second tier PN conflicts. Generated Excel-based spreadsheets to represent cell configuration and status (CellRef), handover statistics per neighbor relation, and remote electrical tilts (RET).\n+ Developed automation tool for LTE network to pull Configuration Management (CM) data directly from SAM (Alcatel-Lucent network) and U2000 (Huawei network), parse and store data inside SQL database. Performed complex analysis on the combined data from ALU and HUA networks to generate combined neighbor relations and check for any PCI collision or confusion. Generated Excel-based spreadsheets to represent cell configuration and status (CellRef), neighbor relations, and Management Object (MO) parameters. Developed two-dimensional neighbor search algorithm to identify missing first-tier neighbors. Developed complex data queries to identify incompliancy in network parameters. Benchmarked and tracked changes to key performance parameters. Evaluated network trials by running pre-post statistical analysis identifying any issues in performance.', u'RF Engineer\nT-mobile/MetroPCS\nFebruary 2012 to November 2014\n- Working with the RF team as a cluster engineer conducting network KPIs analysis, troubleshooting, and optimization for GSM, UMTS, and LTE technologies. Pinpointing the root cause for congestion, access failures, dropped calls, and low throughput. Use RF engineering methodologies for implementing optimal solutions and fine-tuning of radio network parameters.\n- Appointed as Southern California Market lead for the UMTS PCS second carrier deployment project; managing deployment schedule and priorities, running pre-launch audits and database updates, and post launch performance monitoring and optimization.\n- Key member in the Capacity team for the UMTS network responsible for identifying congested sites and providing short and long term recommendations to mitigate.\n- Conducting drive test data analysis and investigation using Actix.', u'System Validation Engineer\nIntel Corporation\nMarch 2011 to February 2012\nJoined the CSI (Complementary Solutions and IP) group as a System Validation Engineer responsible for defining a global test plan and test procedures for validating Intel NFC solution on several Intel-based Platforms and Windows operating systems.\n\n+ Conducted NFC power characterization testing using NI-4141 DC Power Source and NI-4071 DMM.\n\n+ Conducted NFC system Performance testing using protocols sniffer to verify device compliance with ISO-14443A, ISO-14443B, ISO-18092, FeliCa, and ISO-15693 protocols.', u""Network Software Engineer\nIntel Corporation\nMarch 2010 to March 2011\nJoined the MWG (Mobile and Wireless Group) as a Network Software Engineer responsible for providing technical guidance in setting-up, calibrating, testing, and automating the performance system emulator for Intel's WiMAX solution. Validated PHY and MAC layer KPIs against various WiMAX Base Station vendors, such as Samsung, Motorola, and Huawei. Conducted manual and automated Performance testing on Intel's WiMAX solution.\n\n+ Owned Intel's 4G WiMAX solution Performance analysis; establishing the analysis tool for Performance results post-processing and creating the methodologies to extract the target key performance indicators (KPI's).\n\n+ Owned Intel's 4G WiMAX solution Field analysis; establishing the analysis tool for Field results post-processing and creating the methodologies to extract the target key performance indicators (KPI's)."", u'Visiting Professor\nCSU, Fresno\nAugust 2007 to January 2010\nJoined the Electrical & Computer Engineering department as a visiting professor to help in teaching undergraduate courses and labs, supervise senior design projects, and provide undergraduate and graduate students advising.\n\n+ Courses taught:\n- Electric circuits I & II.\n- Electromagnetics I & II.\n- Electronics I & II.\n- Digital design.\n- Signals and systems.\n- Senior design I & II.\n- Electronics lab I & II.\n- Circuits lab I & II.\n- Controls lab.\n\n2. RESEARCH & TECHNICAL PROJECTS\n\n* Design and Implementation of Wireless Local Area Networks (WLAN)\nDeveloped a wireless system for a small computer network using FM transceivers via the RS-232 port. The system supported FTP and email exchange. Developed the software of the system using Visual Basic.\n\n* Coverage Estimation of Typical Urban GSM Network\nDesigned an algorithm to estimate GSM network coverage in an urban area. The algorithm was verified by simulation using Matlab and it was capable of estimating the optimal number of base stations required to cover specific service area and the optimal base station antenna location, including capacity analysis based on expected system load. The design accounted for geographic topology of the area under coverage, multipath fading, and co-channel interference.\n\n* Simulation of Physical Layer Performance in Various Communication Channels\nSimulated physical layer performance in AWGN, Rayleigh-fading, and Rician-fading channels to investigate the effect of noise and interference on image data, and investigate the gain of forward-error correction techniques, such as Convolutional coding.\n\n* Modeling & Simulation of Optical Waveguides\nDeveloped a one-dimensional model to predict the transmission properties of photonic crystal fibers (PCFs) suitable for long-haul communication systems. The proposed model was capable of evaluating the normalized propagation constant, dispersion, effective area, and leakage loss with a good degree of accuracy and much higher efficiency when compared to published results that are based on numerical techniques.']","[u'Ph.D. in Electrical Engineering', u'M.S. in Electrical Engineering', u'B.Sc. in Electrical Engineering']","[u'Virginia Tech Blacksburg, VA\nMay 2006', u'Virginia Tech Blacksburg, VA\nMay 2003', u'The University of Jordan\nJune 2001']"
0,https://resumes.indeed.com/resume/0d99321cb13dede4,"[u'Data Analyst, Leader\nBig Data Center - Beijing, CN\nSeptember 2015 to August 2016\n\u2022 Led 15 group members to accomplish on-line data analysis for the business team.\no Designed ETL (Extract, Transform and Load) data flow mappings of six cube-models for the data mart.\no Collected, cleaned and extracted data for cube-models, and accomplished data visualization in Tableau.\n\u2022 Led 15 group members to develop 1300+ monthly business reports and trained six new members in SQL.\no Designed normative work processes to improve efficiency, run monthly team meetings, summarized our monthly\nwork and made presentations for managers and team members.\n\u2022 Researched and explored analytical models, such as value/stability evaluation, churn prediction and retention strategy.', u""Data Engineer\nMarch 2013 to August 2015\n\u2022 Developed 100+ complex SQL stored procedures for reports, built dataflow and analyzed behavioral data.\no Loaded data from multiple source systems, investigated and analyzed 100+ data trends with SQL and MS Excel for marketing and operations, such as channels, terminal devices, usage, consumption, and products.\no Detected customers' patterns and 20+ abnormal data trends, and identified their causes."", u'Intern, Decision Support System\nIT Consulting Department, Accenture - Beijing, CN\nDecember 2011 to February 2013\nDeveloped, tested and debugged 70+ ETL modules from different data source systems, including Finance, HR,\nSupply Chain, etc., to data mart using Informatica.\n\u2022 Developed RPDs and 40+ dashboards & reports using OBIEE and responded actively to meet demands of clients.\n\u2022 Assisted clients in solving data issues and maintained the performance of the system.']","[u'Master of Engineering in Engineering', u'Bachelor of Engineering in Engineering']","[u'Beihang University (BUAA) Beijing, CN\nSeptember 2010 to January 2013', u'University of Electronic Science and Technology of China (UESTC) Chengdu, CN\nSeptember 2006 to July 2010']"
0,https://resumes.indeed.com/resume/32a96fa71c7072e4,"[u'Data Engineer\nGlobal Algorithmic Institute - New York, NY\nFebruary 2017 to Present\n\u2022 Cloud service: Constructed & maintained data pipeline on Google & IBM Cloud (Spark, Hadoop, GPU, EC2, Database)\n\u2022 Data engineering: Built API to scrape & process data from multiple resources (Quandl, Bloomberg, Intrinio, Yahoo) with Python & PySpark (HDFS); Deployed MongoDB & SQL database on Cloud & performed data ETL.\n\u2022 ML strategy: Developed trading strategy with machine learning (SVM, RandomForest, Logistic, Boosting & ECT)', u'Data Scientist\nRebellion Research - New York, NY\nJune 2017 to December 2017\n\u2022 Data engineering: Designed data pipeline to perform data processing & feature engineering including missing data processing, abnormal & outlier detection, feature generation, normalization & encoding with SQL, Python & PySpark\n\u2022 NLP: Built API to extract News Data of target topics from Gdelt database (53 TB); Processed News data with NLP to extract features including TF-IDF, Sentiment, Topic Analysis, LDA & Text Summarization.\n\u2022 Strategy: Sought alphas in US Equity with machine learning & deep learning (Earnings, Fund Flow, VIX, HMM)\n\u2022 Risk models: Built risk models to optimize existing strategies (Skewed-t, OGARCH)', u'Founder, Trader\nDalian Ruijin Capital - Dalian, CN\nAugust 2014 to September 2016\n\u2022 Fund raising: Raised $10 million through marketing on social media (Wechat, Blog & Website)\n\u2022 Strategy: Developed strategies based on fundamental research, technical analysis & statistic models\n\u2022 Trading: Managed $ 18.5 million (peak value), end up with 52.31% cumulative return ($ 5.23 M)']","[u'Master of Arts in Statistics in Statistics', u'Bachelor of Science in Applied Mathematics in Applied Mathematics']","[u'Columbia University New York, NY\nSeptember 2016 to December 2017', u'Dongbei University of Finance & Economics Dalian, CN\nSeptember 2012 to July 2016']"
0,https://resumes.indeed.com/resume/f6b6eb9c52105cdf,"[u'Data Scientist\nMY DR NOW - Chandler, AZ\nJune 2017 to Present\n\u2022 Reduced appointment scheduling time by automating provider search and augmenting insurance credentialing information.\n\u2022 Reduced time in referral process by automating and migrating tasks out of emr into custom editable reports.\n\u2022 Performed data forecasting for call center department to improve staffing efficiency\n\u2022 Employed machine learning to identify missing data in patient information to use it towards improving customer satisfaction.\n\u2022 Formulated SQL queries for ad-hoc reports including custom gamification of multiple ancillary teams.\n\u2022 Generated and automated detailed reports using visualization software SiSense and Power BI\n\u2022 Performed data wrangling to automate data migration using Python, VBA from multiple platforms into reporting tools.\n\u2022 Generated automated alerts for managers and providers to update on individual and department level statistics\n\u2022 Formulated CLV for different groups of patients to identify business revenue profitability and improve marketing strategies.', u'Data Scientist, Internship\nGreen Living Magazine - Scottsdale, AZ\nJuly 2016 to May 2017\n\u2022 Diagnosed demographics of magazine readers using SQL queries to strengthen marketing strategy and boost sponsorship.\n\u2022 Gathered and processed unstructured data in MS Excel, implementing advanced pivot tables to analyze the data.\n\u2022 Developed new analytical projects as a research programmer by identifying problems and writing improved procedures.\n\u2022 Communicated the analyzed results with top management, and collaborated in decision making for magazine articles.\n\u2022 Managed multiple ongoing projects to ensure objectives and timelines are met.', u'Operations Assistant\nASU SkySong - Tempe, AZ\nAugust 2015 to May 2017\n\u2022 Collaborated in Data Analytics project to develop a database of clients for Economic Development Staff.\n\u2022 Administrated data on client invoices by providing company metrics, and translated ad-hoc tasks into software solutions.\n\u2022 Developed and maintained relation with clients, advising them towards organizing events with ASU SkySong.\n\u2022 Supervised all client conference issues, and coordinated with team and team lead on facility operations.\nPROJECTS:', u'Data Analyst, Industrial Engineer\nTime Series Analysis in Healthcare - Tempe, AZ\nSeptember 2016 to November 2016\n\u2022 Implemented Time Series Analysis on real world data for total population in waiting list at National Health Services, London.\n\u2022 Gathered potential models from JMP, performed cross-validation identifying best forecast capability model.\n\u2022 Examined Exponential Smoothing methods, ARIMA models with seasonal effect and Vector Auto Regression models.', u'Data Analyst, Industrial Engineer\nData Mining Project to Build Classification Model\nSeptember 2016 to November 2016\nPerformed preprocessing in Weka to achieve equal class population and analyzed various classifiers to select best fitting\nmodel that predicted class for new observations. Analyzed Bayes Net, Na\xefve Bayes, J48, and Random forest classifiers\n\u2022 Implemented SMOTE (Synthetic Minority Over sampling Technique) and Ada-Boosting M1 with Random Forest.', u'Data Analyst, Industrial Engineer\nDecision Support System for Taiwan Federal Aviation Administration\nMarch 2016 to May 2016\n\u2022 Generated decision support system in MS Access to maintain database about commercial airlines serving in Taiwan.\n\u2022 Shaped queries, forms and reports in visual studio to increase functionality of database in SQL workbench and MS Access.\n\u2022 Conducted data migration using Python, designed database, data models, ETL processes, and BI reports through SQL.', u'Data Analyst, Industrial Engineer\nRegression Model\nSeptember 2015 to November 2015\n\u2022 Engineered a model that will provide rating for a soccer team based on its performance, and past statistics.\n\u2022 Synthesized Regression Analysis with R on ongoing league data for 90 soccer teams across 16 variable regressors.\n\u2022 Executed model adequacy, multicollinearity and model validation tests in R and Minitab generating reliable model.\nImplementing Uber Rush in Tempe Spring-2017\n\u2022 Instituted vehicle routing with multiple pick up and drop off in a route with environment friendly vehicle constraint.\n\u2022 Built a mathematical model in AMPL that provides optimized route to a rider to process multiple customers simultaneously.\n\u2022 Developed in-depth understanding of online network transportation, incorporated it for bike riders in Tempe.']","[u'Master of Science in Industrial Engineering', u'Bachelor of Technology in Mechanical Engineering']","[u'Arizona State University Tempe, AZ\nMay 2017', u'Nirma University\nJune 2015']"
0,https://resumes.indeed.com/resume/9cd5e83eaa7ea657,"[u'Senior Network Engineer\nVista Outdoor - Anoka, MN\nMarch 2015 to March 2018\nResponsibilities:\n\u2022 Working for an ambitious new company, I took a vital role in largescale network projects during a corporate spin off.\n\u2022 Worked to establish budgets and coordinated with accounting to order, pay and depreciate network equipment for new projects and large site refreshes.\n\u2022 Throughout my time at Vista Outdoor I took the lead in many network integration projects for new acquisitions. I gathered information, identified obstacles and communicated between departments to provide network and security solutions.\n\u2022 Directly Configured and managed documentation and configuration for Network Routers, Firewalls, Controller based Wireless, DNS, NAC(802.1x Forescout counteract), managed SSL certificates, and managed radius in Cisco ISE. \u2022 Troubleshoot network issues throughout the enterprise.', u""Network Engineer\nCisco Systems\nOctober 2014 to March 2015\nResponsibilities included:\n\u2022 Performing documentation of network resources at layer 2 and 3.\n\u2022 Working as the onsite resource for a major transition to monitoring and managing a 2000+ site network for a fortune 50 company.\n\u2022 Working to facilitate coordination between our client's team and Cisco Cloud management.\n\u2022 Performing highly technical troubleshooting and following through on discovered bugs with Cisco\nTAC. Performing detailed analytics using bulk data to illustrate cost value savings and to demonstrate\nclient needs and technology gaps."", u'Network Engineer\nVerus Corporation - Fridley, MN\nJuly 2013 to September 2014\nResponsibilities include:\n\u2022 Working directly with clients, I assess network and infrastructure needs, then design and implement\nappropriate solutions\n\u2022 Manage large scale network systems\n\u2022 Provide a focus on systems security while maintaining accessibility and integrity of the network\n\u2022 Advanced routing and fail-over design that provides the customer with redundancy as well as scalability to support future expansion\n\u2022 Working directly with ISPs on QOS and SLA performance issues\n\u2022 Hardware maintenance and upgrades on network and server equipment\n\u2022 Managing projects: Setting deadlines and ensuring we meet them. All while keeping lines of communication open between the customer and all involved parties.', u'Data Network Specialist\nUnited States Marine Corps - Camp Lejeune, NC\nAugust 2008 to July 2013\nResponsibilities included:\n\u2022 Acting as part of a team that manages thousands of network devices spread across 4 military bases\n\u2022 Configuring and physically installing Cisco and Enterasys networking equipment\n\u2022 Monitoring devices remotely using CA-Spectrum, Enterasys Net-Sight, and HP Open View\n\u2022 Configuring and troubleshooting VPNs\n\u2022 Performing firmware upgrades for network devices locally and remotely\n\u2022 Performing physical upgrades for network devices\n\u2022 Organizing new installations in a clean and logical manner\n\u2022 Cable Management; Labeling etc.\n\nIf interested, Contact me at Gregory.Hiel@Gmail.com,\nCell (612) 380 3029 or find me on LINKEDIN.\nhttp://www.linkedin.com/in/gregoryhiel 2']","[u'', u'in Marine Corps Communication', u'']","[u'Western Governors University\nPresent', u'Electronics School', u'Blaine High School']"
0,https://resumes.indeed.com/resume/eb59168f9ef01ea0,"[u'Data Engineer\nVericred, Inc - New York, NY\nNovember 2016 to Present\n\u2022 Build script using R to extract and transform health insurance data from varying file formats into a user-friendly data file for healthcare brokers, researchers and other users\n\u2022 Write script to automate manual data entry, increasing process efficiency by 90% and reducing error in data entry', u'Advanced Project in Biostatistics\nCUNY School of Public Health - New York, NY\nFebruary 2018 to May 2018\n\u2022 Pioneer on a research manuscript assessing the association of childhood sexual assault and poor birth outcomes such as, low birthweight, preterm birth and small for gestational age and the possible mediating paths between the exposure and outcomes\n\u2022 Analyze the association using univariate regression and multivariate regression and analyzing the mediating paths using path analysis', u'Data Administrator\nVericred,Inc - New York, NY\nJanuary 2016 to November 2016\n\u2022 Researched and collected insurance rate filings for all 50 states; extracted and transformed the health insurance data to be consistent, and in a user-friendly language\n\u2022 Manually built data files filled with comprehensive information on insurance benefits for clients', u""Research Assistant\nGreater New York Hospital Association - New York, NY\nSeptember 2013 to January 2016\n\u2022 Secured the accuracy of Health Information Tool for Empowerment's (HITE) website data by conducting research and contacting key stakeholders\n\u2022 Analyzed Google Adwords data to find ways to optimize clicks and drive site traffic to the site\n\u2022 Collected HITE's monthly web statistics to keep a track of monthly traffic on the website""]","[u'Master of Public Health in Biostatistics', u'Bachelor of Science in Public Health in Public Health', u'in Study Abroad']","[u'School Of Public Health New York, NY\nSeptember 2015 to May 2018', u'Falk College for Sport and Human Dynamics Syracuse, NY\nMay 2013', u'Syracuse University London\nSeptember 2011 to December 2011']"
0,https://resumes.indeed.com/resume/26ee6478ee613c64,"[u""Data scientist\nSamhsa - Rockville, MD\nJanuary 2017 to Present\nDescription: The Substance Abuse and Mental Health Services Administration is a branch of the U.S. Department of Health and Human Services.\n\nResponsibilities:\n\u2022 Acquire, clean using Talend and structure data from multiple source including external and internal databases\n\u2022 Perform data extraction, Manipulation, analysis and data mining using SQL\n\u2022 Develop and execute processes for accurate data capture across all clients to obtain key insights and relationships to overall business objectives using Statistical Hypotheses Modeling\n\u2022 Analyzed data and predicted end customer behaviors and product performance by applying machine learning algorithms using Spark MLlib.\n\u2022 Worked on enormous amounts of data to enhance customer value or reduce non-credit losses.\n\u2022 Identify and extract entity, and discover knowledge from structured and unstructured content.\n\u2022 Converted chunks of text into more formal representations using Natural language processing.\n\u2022 Generation of semantic graph based on invoice analysis to mine and identify executed action using the open source SADLlanguage\n\u2022 Perform data extraction, manipulation, cleaning, analysis, modeling and data mining using R programming in R Studio\n\u2022 Utilized Spark, Scala, Hadoop, HBase, Kafka, Spark Streaming, MLLib, R, a broad variety of machine learning methods including classifications, regressions, dimensionally reduction etc.\n\u2022 Exploratory analysis and model building to develop predictive insights and visualize, interpret, report findings and develop strategic uses of data.\n\u2022 Analyst and developer an expert system used to mine invoice data and identify executed actions using a mix of VBA and SADL Language\n\u2022 NLTK, Stanford NLP, RAKE to preprocess the data, entity extraction and keyword extraction.\n\u2022 Utilized Booted Decision Tree, Linear and Bayesian Linear Regression Machine Learning models in Microsoft Azure to develop and implement interactive Webservice predictive models\n\u2022 Executed ad-hoc data analysis for customer insights using SQL using AWS Hadoop Cluster.\n\u2022 Used External Loaders like Multi Load, T Pump and Fast Load to load data into Teradata Database, Involved in analysis, development, testing, implementation and deployment\n\u2022 Exploratory data analysis using R and PYTHON to deep dive into internal and external data to diagnose areas of improvement to increase efficiency\n\u2022 Created different charts such as Heat maps, Bar charts, Line charts, etc.,\n\u2022 Building, publishing and scheduling customized interactive reports and dashboards using Tableau Server.\n\u2022 Visualize patterns, anomalies, and future trends by applying predictive analytic techniques\n\u2022 Designed, built and deployed a set of R modeling APIs for customer analytics, which integrate multiple machine learning techniques for various user behavior prediction (CLTV, marketing funnel propensity models etc.) and support multiple marketing segmentation programs.\n\u2022 Lead the company's machine learning and statistical modeling effort including building predictive models and generate data products to support customer segmentation, product recommendation and allocation planning; prototyping and experimenting ML/DL algorithms and integrating into production system for different business needs.\n\u2022 Built models using decision trees, segmentation, regression and clustering intelligent decision models to analyze customer response behaviors, interaction patterns and propensity\n\nEnvironment:MLLib, SAS, regression, logistic regression, Hadoop 2.7.4, NoSQL, Teradata, Python, MySQL, Linux, R, Numpy, Pandas, Tableau, Excel, HTML, CSS, Bootstrap, OLTP, random forest, OLAP, HDFS, ODS, NLTK, SVM, JSON, XML and MapReduce"", u'Data scientist\nSociete Generale Americas Securities\nOctober 2015 to December 2016\nDescription: SG Americas Securities, LLC provides investment banking services. It focuses on capital markets, securities, underwriting, mergers and acquisitions, derivatives, and trading services. The firm also provides clearing, settlement, and custodial services.\n\nResponsibilities:\n\u2022 Utilized Apache Spark with Python to develop and execute Big Data Analytics and Machinelearning applications, executed machine learning use cases under Spark ML and Mllib.\n\u2022 Analyzed large data sets using R and used regression models to predict future data using R forecasting and visualized them in tableau\n\u2022 Maintained SQL scripts to create and populate tables in data warehouse for daily reporting across departments\n\u2022 Created effective visualizations using tableau and investigated a dataset to create data visualizations to tell trends and patterns in data\n\u2022 Worked in MySQL database on simple queries and writing Stored Procedures for normalization and renormalization.\n\u2022 Worked on machine learning on large size data using Spark and MapReduce.\n\u2022 Used SQL language to write queries inside the SQL server database.\n\u2022 Maintained SQL scripts to create and populate tables in data warehouse for daily reporting across departments.\n\u2022 Performed data analysis by using Hive to retrieve the data from Hadoop cluster, SQL to retrieve data from Oracle database.\n\u2022 Used SQL language to write queries inside the SQL server database\n\u2022 Performed preliminary data analysis using descriptive statistics and handled anomalies such as removing duplicates and imputing missing values.\n\u2022 Analyzed large sales data of Cisco devices and used R for predicting the future sales using regression models\n\u2022 Created alerts and notifications for system errors, insufficient resources, and fatal database errors.\n\u2022 Used the Django Framework to develop the application.\n\u2022 Collaborated with data engineers, wrote and optimized SQL queries to perform data extraction from SQL tables\n\u2022 Responsible for preparing the existing SQL platform for upgrades that were installed as soon as they were released. Assisted in creating and presenting informational reports for management based on SQL data\n\u2022 Created histograms, bar charts, frequency plots, computed 3 measures of distributions: the mean, median and mode\n\u2022 Provided dedicated supports for development, testing and production SQL Server environment.\n\u2022 Performed daily tasks including backup and restore by using SQL Server 2014 tools like SQL Server Management Studio and SQL Server Agent\n\u2022 Used tableau to visualize data from a given dataset. Used R to do statistical modeling and did data transformation before using the data in tableau and visualizing it. Wrote R scripts and connected with tableau using an external ODBC in tableau.\n\u2022 Build SQL queries for performing various CRUD operations like create, update, read and delete\n\u2022 Worked on NOSQL databases like MongoDB, HBase.\n\u2022 Extracted data from HDFS and prepared data for exploratory analysis using data munging.\n\nEnvironment:CDH5, HDFS, Hadoop 2.3, Hive, Impala, AWS, Linux, Spark, python, Tableau Desktop, SQL Server 2014, Microsoft Excel, Matlab, Spark SQL, Pyspark.', u'Data Analyst/Data Modeler\nGE Capital - Norwalk, CT\nDecember 2013 to September 2015\nDescription:At GE Capital worked at the project named ALLL. The objective of this project was to create a central repository for data storage, historical analytics, and federal requests. As a Business/Data analyst I worked closely with business and technical team to implement central repository for users so they can store data securely and get the proper report at the end of the quarter.\n\nResponsibilities:\n\u2022 Designed and Developed Oracle11g, PL/SQL Procedures and UNIX Shell Scripts for Data Import/Export and Data Conversions.\n\u2022 Worked on Data modeling using Dimensional Data Modeling, Star Schema/Snow Flake schema, and Fact & Dimensional, Physical & Logical data modeling\n\u2022 Data exploration, Data Profiling, Data Quality and ETL to load and transform huge data sets.\n\u2022 Data Profiling, Data Analysis; identify and implement business rules to uniquely identify Securities.\n\u2022 Worked on logical and physical modeling of various data marts as well as data warehouse using Taradata14.\n\u2022 Gathered and analyzed existing physical data models for in scope applications and proposed the changes to the data models according to the requirements.\n\u2022 Custom Asset Classification using Python\n\u2022 Performed Data Validation and Data Cleaning using PROC SORT, PROC FREQ and through various SAS formats.\n\u2022 Developed Tableau Dashboard for Enterprise Security Reporting and Analytics\n\u2022 Assess the data quality using python scripts and provide the insights.\n\u2022 Implemented R packages for data manipulation\n\u2022 Worked in using Teradata14.1 tools like Fast Load, Multi Load, T Pump, Fast Export, Teradata Parallel Transporter (TPT) and BTEQ.\n\u2022 Prepared Data Visualization reports for the management using R.\n\u2022 Create MDM base objects, Landing and Staging tables to follow the comprehensive data model in MDM.\n\u2022 Created jobs, alerts to run SSIS, SSRS packages periodically. Created the automated processes for the activities such as database backup processes and SSIS, SSRS Packages run sequentially using SQL Server Agent job and windows Scheduler.\n\u2022 Utilized SDLC and Agile methodologies such as SCRUM\nEnvironment:ERwin9.1, Teradata14, Oracle11g, PL/SQL, UNIX, Agile, Azure, TIDAL, MDM, ETL, BTEQ, SQL Server2008, Netezza, DB2, SAS, Tableau, UNIX, SSRS, SSIS, T-SQL, MDM, Informatica, SQL.', u'Business Analyst/Data Analyst\nBDIPlus Company - New York, NY\nSeptember 2012 to November 2013\nDescription:BDIPlus is a Strategy Consulting and Software company focused on providing capabilities and solutions that result in the creation of forward-looking and long lasting competitive advantage. Our solutions reflect our unparalleled technological expertise and our deep domain knowledge of the Financial Services and Insurance industries.\nResponsibilities:\n\n\u2022 Collaborated with business partners from multiple disciplines to elicit, document, prepare and manage business requirements package for stakeholder sign-off and delivery to technical teams.\n\u2022 Broke down Intense analysis into multiple sessions for all the team members to get a clear understanding on the requirements and made sure there are no show stoppers.\n\u2022 Documented requirement artifacts utilizing industry standard diagram techniques to enhance the clarity of definition, including: process flows, context diagrams, use cases, Wireframes, etc.\n\u2022 Established meaningful traceability between related requirements. Assisted the Project Manager &Business Analyst in communicating to project stakeholders on project progress and risks and collected feedback.\n\u2022 Created Use-Cases and Business Use-Case Model after accessing the status and scope of the project and understanding the business processes.\n\u2022 Provided project support to the Project Manager& reviewed the efficiency and effectiveness of service delivery.\n\u2022 Worked with third-party vendors when documenting Statement of Work (SoW).\n\u2022 Acted as a liaison between the business and IT design and delivery teams.\n\u2022 Interfaced with Business Partners, Technical resources (i.e. Solution Engineers, Systems Analysts, Developers, Architecture, and Quality Assurance) to translate and simplify requirements, ensure requirements are met and verify that the implemented solution meets the requirements.\n\u2022 Collaborated with QA team and SMEs to ensure adequate test coverage.\n\u2022 Interacted with various end user teams during beta testing to gather feedback on application usability.\n\u2022 Extensively worked on IBM RTC to track aspects of work such as work items, source control, reporting, and build management.\n\u2022 Conducted User Acceptance Testing (UAT) and collaborated with the QA team to develop the test plans, test scenarios, test cases, test data to be used in testing based on business requirements, technical specifications and/or product knowledge.\n\u2022 Performed due diligence and identified possible production failure scenarios, created incident tickets in Service Now and communicated effectively with the development team and the business units.\n\u2022 Expertise in Design and development of CMDB (Configuration Management) in Service Now.\n\u2022 Worked extensively on SoapUI for testing SOAP Web Services functional testing, REST APIfunctional testing.\n\u2022 Worked on Jira for Average age report, Pie chart report, resolution time report, user workload report, version workload report, and workload pie chart report & HPQC environment to help the Quality Assurance team in defect tracking.\n\u2022 Extensively worked on tableau for creating Pie chart, Line chart, and Grid chart, created interactive dashboards, created interactive maps.\n\u2022 Used Axure RP to create Workflow, Sitemaps, Templates, widgets.\n\u2022 Extensively worked on Requirement Management Plan\n\u2022 Worked on SQL for testcase execution.\n\nEnvironment:Windows, MS Office (MS Word, MS Excel, MS PowerPoint), Quality Center, Axure RP Pro, Jira, JAVA, IOS, .Net, Rally, MS Visio, SharePoint.', u'Business Analyst\nHSBC Bank - Bengaluru, Karnataka\nDecember 2010 to August 2012\nDescription:HSBC Bank India, is an Indian subsidiary of UK-based HSBC Holdings plc, is a bank with its operational head office in Mumbai. It is a foreign bank under the Banking Regulation Act, 1949 and thus is regulated by the Reserve Bank of India (RBI).\n\nResponsibilities:\n\u2022 Assisted Project Manager and Senior Management with Project Plans, Timeline and Workforce.\n\u2022 Coordinated in Sprint Planning to identify and monitor the activities performed in each iterative.\n\u2022 Conducted GAP Analysis and Impact Analysis to define the changes of the project.\n\u2022 Interacted with user groups to derive the Business and Functional Requirement Documents.\n\u2022 Participated in Front Office Analytics and Pricing, including Asset Classes valuation against Bloomberg (future/forward, options, FX options, Eurodollar/interest rate future, CFD, warrants, etc.) using Black Sholes Model and cash flow discount model.\n\u2022 Worked with accountants to write specification on NAV, local/base MV, P/L, Cr/Dr, etc. to prepare performance reports, trial balances and financial statements.\n\u2022 Calculated intrinsic value and time value, and Greeks (delta, gamma, vega, theta).\n\u2022 Set up securities (T-Bonds, IR Futures, Eurodollar and Futures, IRS, CDS, Commodity Futures and Options, etc.) in Front Arena for trading.\n\u2022 Constructed yield curves (Libor, Swap Rates) and volatility surfaces to serve as basis for valuation.\n\u2022 Assisted in defining business process flows for Street-to-Fund Reconciliation and Hedge Fund-to-PB Reconciliation.\n\u2022 Analyzed Bloomberg data and streamline of loading Security Master, time series data and prices from Bloomberg and Reuters.\n\u2022 Worked with FAs to retrieve and set up benchmarks like BEY, EAR, Expected Return, Market Risk, Sharpe Ratio, NPV and IRR, etc.\n\u2022 Created Test Plans and Test Cases and coordinated team effort on Functional Testing, Migration Testing, Performance Testing, Regression Testing.\n\u2022 Coordinated with Identifying defects and gaps in the core products, managing bugs and assisted development of enhancements to meet business requirements in JIRA Platform.\n\u2022 Defined testing methods and created/executed test, assisted with end-to-end UAT.\n\nEnvironment:Agile, UML, Rational Requisite Pro, Enterprise Architect, MS Word, MS Excel, MS Outlook, MS PowerPoint, MS Project 2010, Seapine, MS Visio, Rational Rose, .Net, SAP, SQL Server, JIRA.', u'Software Engineer\nRanbaxy Pharmaceuticals - Hyderabad, Telangana\nMay 2009 to November 2010\nDescription:Ranbaxy Laboratories Limited is an Indian multinational pharmaceutical company that was incorporated in India in 1961. The company went public in 1973 and Japanese pharmaceutical company Daiichi Sankyo acquired a controlling share in 2008.\n\nResponsibilities:\n\u2022 Worked in development of applications especially in UNIX environment and familiar with all its commands.\n\u2022 Design, develop, test, deploy and maintain the website.\n\u2022 Designed and developed the UI of the website using HTML, CSS and JavaScript.\n\u2022 Build SQL queries implementing functions, packages, views, triggers, and tables.\n\u2022 Using Subversion version control tool to coordinate team-development.\n\u2022 Designed and developed data management system using MySQL.\n\u2022 Handled all the client-side validation using JavaScript.\n\u2022 Responsible for debugging and troubleshooting the web application.\n\u2022 Handling the day to day issues and fine tuning the applications for enhanced performance.\n\nEnvironment: MySQL, Linux, HTML, XHTML, CSS, AJAX, JavaScript, SQL, MySQL, Apache Web Server, UNIX.']","[u""Bachelor's""]",[u'']
0,https://resumes.indeed.com/resume/b7fcd51bce24372d,"[u""Data Science Consultant\nCARLSON ANALYTICS LAB - Minneapolis, MN\nJuly 2017 to December 2017\nClient: An American automotive manufacturer\n\u2022 Managing a team of 5 as an engagement manager and serve as the primary communication contact between client, Carlson School and project team\n\u2022 Optimizing the promotion strategy to increase market share by building region level fractional logit models and developing a randomized experiment for client to measure the impact of offers plan in future\n\nClient: Mall of America - Customer behavior analysis\n\u2022 Led a culturally diverse team of 5 analysts to improve experience of customers visiting Nickelodeon Universe amusement park. Scraped website for feature engineering and profiled customers using clustering. Visual networks and association rules were run on the clusters to identify high and low satisfaction drivers\n\nClient: Mall of America - Forecast footfall using prediction model\n\u2022 Predicted daily footfall in Nickelodeon Universe amusement park for 2018 decreased the error rate by 12% leveraging ensemble models\n\nClient: PricewaterhouseCoopers - case competition 2nd place\n\u2022 Profiled customers of PwC's auto insurance client using k-prototype clustering after univariate analysis to identify high and low profitability characteristics with R and Tableau."", u'Software Engineer\nFORD MOTOR COMPANY - Chennai, Tamil Nadu\nJuly 2016 to April 2017\n\u2022 Automated car parts and maintenance pricing reports by modifying the backend thus reducing analysis and decision-making time by 5%\n\u2022 Communicated with cross-functional teams working on orders of parts to vehicles and resolved 20-25\nreport analysis issues per month\n\u2022 Designed SAP BI digital training program for incoming employees, reducing the training costs by 20%', u'Specialty Developer\nJuly 2015 to July 2016\n\u2022 Interacted with 5 project owners and formulated migration plans for their applications and collaborated with stakeholders to mine car sales and warranty claims data and identify loss factors\n\u2022 Migrated 4 SAP applications with 800 Business Intelligence reports from XIR 3.1 Desktop Intelligence to\nBO 4.2 Web Intelligence\n\nDATA SCIENCE PROJECTS\n\u2022 Sports Analytics - Analyzed soccer leagues data on MicroStrategy to perform SWOT analysis of teams and players to aid in strategic decision making and team formations in the future\n\u2022 Selecting effective brand ambassador with twitter profile analysis on AWS- Scraped tweets of web\ninfluencers and ran topic modelling and sentiment analysis to understand the emotion being generated in public. Monitored real-time performance and stored reactions on tweets by the selected ambassador using\nspark streaming and DynamoDB\n\u2022 Healthcare Analytics case competition - After performing feature selection on 939 variables, predicted the possibility of inpatient admission and the number of readmissions for a patient using SVM\n\u2022 ERP database inconsistency solution - Decreased manual data cleaning time by 70% using\nFuzzyWuzzy and demonstrated the application by creating a GUI interface with Python']","[u'Master of Science in Business Analytics', u'Bachelor of Technology in Electronics and Communication']","[u'UNIVERSITY OF MINNESOTA Minneapolis, MN\nMay 2018', u'VIT UNIVERSITY Vellore, Tamil Nadu\nMay 2015']"
0,https://resumes.indeed.com/resume/39ce75033b2682ef,"[u'Legal Assistant\nMay 2011 to Present\nSupported solo practitioner in the performance of tasks related to both trial and appellate level cases. Assisted in researching complex legal issues using the Colorado State Bar Association database.\n\nInterpreted and evaluated contracts and agreements, court orders, depositions, motions and pleadings, exhibits, and the trial/hearing record. Discussed relevant points with principal; suggested case strategies for initial, response, or reply pleadings and motions.\n\nPrepared trial and hearing exhibits and pre-trial discovery documents, assisted in drafting motions, briefs, and other pleading documents. Proofread documents prepared by the principal and proposed changes in both content and structure. Recommended procedures for streamlining, automating, and increasing the efficiency of the practice.', u""Service Product Manager\nQuantum Corporation - Englewood, CO\nMay 2015 to November 2015\nResponsibilities\nDeveloped service parts: part names, part numbers, and pricing, using calculations based on new product and using similar products. Developed 5000+ new service parts.\n\nAccomplishments\nDeveloped pricing tools to save time and improve accuracy, through customer interface to determine requirements and explain capabilities. One tool, developed for internal use using Visual Basic, allowed generation of thousands of parts, previously a two week job, in 30 minutes. Another tool, designed for the field pricing force using vlookups, permitted 100% accurate instant quotes, previously a confusing time intensive task. The field pricing tool incorporated Excel's security to constrain salespeople's ability to input data into fixed fields or to input data outside of fixed ranges. Provided documentation and training to end users.\n\n\n\nSkills Used\nVisual Basic, Excel, pricing, analysis, documentation, and training."", u'Data Quality Engineer\nTHE COMEX GROUP - Greenwood Village, CO\nSeptember 2012 to April 2014\nAnalyzed data and processes to isolate data errors and determine their source; resulting in improved accounting. Reduced discrepancies between revenue data sources from 82% to 2% through data flow mapping, isolating incidents that created misalignments, and then developing and documenting standardized new analysis processes to work around those incidents. Reduced them to 0% by reconciling remaining issues on an exception basis. As a special project, assigned to research telecom accounts payables, and uncovered $2500/month in cost savings.', u'Data Analysis Engineer\nDISH NETWORK - Englewood, CO\nAugust 2011 to November 2011\nTemporary contract position. Tasked to review and improve company operations. Discovered a potential reduction in expenses from revision of existing scrapping strategies. Developed and implemented a project resulting in a cost savings of $160K/year. Also managed a project to decrease expenses associated with reported instances of NPF (No Problem Found) returned receivers. Reduced program costs through redefining metrics and improving processes.', u'Data Analysis/Failure Analysis Engineer\nECHOSTAR - Englewood, CO\nNovember 2006 to August 2011\nReviewed data for inconsistencies and directed efforts to resolve any issues. Collaborated with management and IT to define, track, and present service/repair performance metrics. Designed/refined data acquisition and analysis/presentation processes. Developed analyses to determine real damages in a breach of contract case, which EchoStar won. Managed department websites in HTML and Sharepoint.', u'Reliability Engineer\nHONEYWELL - Colorado Springs, CO\nAugust 2002 to March 2006\nPerformed system and subsystem failure analyses resulting in reduced downtime. Developed, defended, and implemented allocation technique to link system and segment level metrics requirements, resulting in meeting contract requirements. Analyzed usability of data from several different data sources and platforms, published user guides to interpreting data. Managed on-going effort to remove incorrect/conflicting data and to track and correctly allocate marginal missions. Negotiated complex contractual reliability requirements in spite of interleaving, sometimes conflicting requirements documents and measured metrics. Performed ISO-9000 audits on purchasing, quality control, software development, and project management offices. Served as departmental Safety representative.', u'Supplier Program Manager\nSUN MICROSYSTEMS - Broomfield, CO\nOctober 2000 to September 2001\nOversaw seven service suppliers. Improved delivery performance. Resolved problems with inconsistent data tracking. Scorecarded suppliers. Evaluated audit forms and audited suppliers. Wrote Depot Procedure Change Orders (DPCOs). Implemented data recording format and Failure Analysis (FA) requirements for suppliers. Coordinated back charge payments. Implemented quality test reporting at suppliers. Negotiated repair supplier contract. Investigated board failures, coordinated supplier FA. Wrote test script. Executed test scripts.', u'Quality Engineer/Business Analyst\nSTORAGE TECHNOLOGY, INC - Louisville, CO\nJanuary 1998 to July 1999\nDeveloped and documented queries and analysis methodology from the ground up. Extracted, integrated, analyzed, trended, and presented P&L data, including variance between projected and actuals. Tested new databases. Managed projects to improve financial performance. Determined metrics to be tracked based upon usefulness, availability, and data quality. Retrieved data using Brio Query.']","[u'Master of Science in Business Administration', u'Master of Science in Environmental Engineering', u'Bachelor of Science in Mechanical Engineering']","[u'University of Phoenix', u'Colorado School of Mines', u'University of New Mexico']"
0,https://resumes.indeed.com/resume/189da3dd77d5a6e7,"[u'Data Analytics Coop\nFidelity Investments - Boston, MA\nJanuary 2017 to August 2017\n\u2022 Worked with Risk Analytics team to automate data pipelines using Python, R, statistics, artificial intelligence by providing ongoing reports with the help of visualization tools like Tableau, Neo4j, D3js and MS Excel\n\u2022 Used Python, R and KNIME to web scraping and data wrangling processes for structured as well as unstructured data\n\u2022 Developed a Firm Initiative to automate the process of migrating data from web portal to MS Excel with the help of Web Scraping technique and Rest API in Python which reduced the deliverable time by 70%\n\u2022 Used Sqoop for ETL process and HIVE to fetch the data for Tableau and Power BI business reporting tools\n\u2022 Enhanced the performance of SQL query by using TOAD to maintain the Oracle time taking session\n\u2022 Optimized data collection and validation process using Python, Web API, MS Excel for Oracle and Hadoop databases', u'Senior Software Engineer\nCapgemini - Mumbai, Maharashtra\nMarch 2012 to July 2015\n\u2022 Designed and advanced the Healthcare and Finance Java modules as per requirements for business process automation\n\u2022 Projected the Selenium and UFT automation framework tool for Regression Testing of different web applications\n\u2022 Participated in all the Scrum/Agile sprint meetings, discussed and configured the tasks utilizing JIRA and Confluence\n\u2022 Performed REST API, usability, performance and integration testing to reduce 50% of manual work of UI content team\n\u2022 Developed SQL queries to validate the Insurance and Financial data in Sybase, Oracle, MySQL to test the ERP\n\u2022 Automated and presented data findings, reports and solutions to the management to improve decisions and operations\n\u2022 Awarded Value Initiative twice in a row for the automation of business processes in Finance and Healthcare which boosted the productivity by 30%']","[u'Master of Science in Information Systems in Information Systems', u'Bachelor of Engineering in Computer Engineering in Computer Engineering']","[u'Northeastern University, College of Engineering Boston, MA\nDecember 2017', u'Mumbai University Mumbai, Maharashtra\nAugust 2007 to July 2011']"
0,https://resumes.indeed.com/resume/2b1c315e82416ee7,"[u'Data Engineer\nWalmart Stores Inc - Bentonville, AR\nJanuary 2013 to Present\nSupports Information Systems Division with data analytics by using internal tools to optimize the supply chain. Work closely with Data Scientist to determine their needs for research analysis to support business decisions.\n\u2022 Integrated new data management technologies and software engineering tools into existing structures.\n\u2022 Supported code/design analysis, strategy development and project planning.\n\u2022 Built high performance algorithms, predictive models and proof of concepts to understand and predict behavior of the supply chain.', u'Business Analyst\nArkansas Blue Cross Blue Shield - North Little Rock, AR\nJanuary 2003 to January 2012\nImplemented next-generation technologies and processes to drive a team environment of continuous improvement. Drove growth by focusing on customer service, which propelled efficiency gains and significant time- and cost-savings.\n\u2022 Managed provider/payer relationships using healthcare applications for Centers for Medicare & Medicaid.\n\u2022 Demonstrated effective accounting and ad-hoc reports summarizing analytical data.\n\u2022 Facilitated ""no-fault"" internal and external audits of medical claims.']",[u'Bachelor of Information Systems in Information Systems'],"[u'Arkansas Tech University Tech, Arkansas, US']"
0,https://resumes.indeed.com/resume/ebcca9d957a9e2c5,"[u'Software Engineer Intern\nTetration Analytics - Palo Alto, CA\nSeptember 2016 to Present', u'Data Science Intern\nEnvestnet - Redwood City, CA\nJune 2016 to August 2016\n\u2022 Part of Data Science team, doing exploratory and predictive analysis on unstructured data to gain insights and develop an income prediction model using R.\n\n\u2022 Implemented Dynamic Regex Generator to determine transactions from a given set of unstructured data.', u'Software Engineer\nDruva Data Solutions - Pune, Maharashtra\nJanuary 2014 to July 2015\n\u2022 Automated various scenarios using Python to gain 20% performance improvement\n\n\u2022 Analyzed functionality of various features related to Data Deduplication, Storage, Data Backup etc.', u'Software Engineer\nSymantec\nJuly 2011 to December 2013\n\u2022 Worked in Performance team to calculate compliance score for various modules across physical data centers and Cloud\n\n\u2022 Automated data evaluation using SQL to gain 20% performance improvement']","[u'', u'BE in Computer Science']","[u'San Jose State University San Jose, CA\nJanuary 2015 to January 2017', u'Pune Institute of Computer Technology Pune, Maharashtra\nJanuary 2007 to January 2011']"
0,https://resumes.indeed.com/resume/d3fb4d86281eaef7,"[u'Quality Engineer.Data Engineer;Black Belt\nNorthern Engraving\nJune 2015 to September 2016\nNorthern Engraving, La Crosse, WI\nProgram Launch Engineer, Quality Engineer (Contract)\n\u2022 Implemented number of successful projects to reduce waste at the plants and improve Plant OEE and KPIs\nsimultaneously improved the integrity of quality management systems and organizational intelligence.\nEvaluated and managed the key DMAIC and design for six sigma projects to meet customer requirements\nand expectations. Increased and sustained yield from 65% to 90% for West Salem Plant and 55 to 80% in\nSpring Grove. Solved number of manufacturing & quality issues, using data analysis tools and reduced\nprocess variations, defects, extra processing steps as result improved the continuous flow process, reduced\nthe lead time and helped to achieve company financial goals. The plant and overall company sales exceed\n$137 million in 2015. The overall estimated plant savings in 2015 ~ $500,000 and ~$275,000 in 2016\n\u2022 Secured and successfully launched New Products for Ford Motor Company, lead CAPA activities, lead\nKaizen events and Lean Six Sigma Projects for Chrysler, Tesla and General Motors projects.\n\u2022 Worked with customers and suppliers on the engineering changes and PPAP elements to ensure that the\nprocesses consistently manufacture parts with less variation.\n\u2022 Facilitated the preventive maintenance improvement actions and equipment optimization to reduce program\nrisks and failures;\nNissan North America, Canton, MS\nQuality Engineer, Supplier Quality Engineer (Contract)\n\u2022 Worked with parts inspection & warranty teams to resolve parts quality/supplier & manufacturing issues for\ncurrent car & truck production;\n\u2022 Worked with warranty concerns and conducted several tests to re-create failures of components and\nprovided feedback to suppliers to improve the quality of parts;\n\u2022 Solved vehicle safety, miss shipment and downtime issues for the chassis suspension part; Closely worked\nwith Tier 1 stamping supplier on part\'s quality issue using data analysis techniques; Provided the input to\nthe risk management plan and reliability related risks for new Nissan Titan HD and proposed the design\nchange and new quality and manufacturing requirements for the supplier and assembly process at Nissan;\nEPG Companies, Maple Grove, MN\nNew Product Development Engineer; Project Engineer\n\u2022 Developed & designed over 15 new & modified products at EPG including data logger, meters, control\npanels, flow sensors. Designed 3D models of sum drainers, SurePumps(TM) & NW Disconnects utilizing 3D\npackages;\n\u2022 Released number of installation drawings of guide rail disconnect system for end users as well as testing\ninstructions, manuals & product guides;\n\u2022 Provided input to the risk management plan to avoid, control and mitigate project risks;\n\u2022 Developed and integrated engineering solutions to repetitive failures and other problems related capacity,\ncost and product quality;\nAvtovaz-Renault-Nissan, Togliatti, Russia\nLaunch/Project Manager, Parts Quality Manager\n\u2022 Successfully launched on-time new vehicle Nissan Almera in Russia, approved 220 new and modified upper\nbody purchased parts for start of Start of Production (SOP) & approved PPAP;\n\u2022 Managed ANPQP & capacity activity for the new Nissan Almera as well as the international cross functional\nteams involved in all areas within Nissan project-focused departments.\n\u2022 Conducted number of Supplier Audits to meet TS 16949 Quality Management System Requirements - 15\nlocal suppliers together with Nissan NML and NTCE. Conducted number of Kaizen events at Supplier Site\nto secure Nissan Almera launch.\n\u2022 Traveled to Nissan Global Operational Center in Japan to attend prototype vehicle confirmation events to\nachieve supplier readiness for SOP and quality targets;\nAVTOVAZ, Togliatti, Russia\nManager, Product Development, Lighting Systems\nParticipated in over 10 new product launches while managing & developing design of lighting systems for\nprospective Kalina, Priora, Nadezhda, 2110FL, Chevi-Niva, Samara and Oka-2 models.\n\u2022 Managed complete vehicle development cycle of lighting systems from creative design through production\nlaunch\n\u2022 Developed and Implemented new rapid prototyping process for DfM and DFS of lighting devices\nsimultaneously improved product quality and reliability and safety, reduced time-to-market, and improved\ncustomer satisfaction;\n\u2022 Improved the lighting application reliability and established quality and reliability requirements for suppliers\nAVTOVAZ, Togliatti, Russia\nProduct Development and Reliability Engineer, Lighting Systems\n\u2022 Designed over 20 lighting applications including LED technology & patents for new prospective cars in\nRussia, ""LADA"".\n\u2022 Designed and released number of production design drawings on time to meet program milestone\nrequirements\n\u2022 Designed new lighting applications and optical systems, conducted LAB verification and validation testing\nfor lighting applications;\n\u2022 Worked with life data and warranty issues to improve the product reliability, quality and requirements for\nsuppliers;\n\u2022 Worked with production departments & Tier 1 suppliers through pre-production phase until SOP.\n\u2022 Together with R&D and experimental production department selected & tested new materials as well as\nimproving existing designs of serial lighting systems to reduce cost & to improve quality']","[u'Master of Electro in Mechanical Engineering', u'M.B.A. in Business Administration & Management in Business Administration & Management']","[u'Togliatti State University', u'University of Phoenix Phoenix, AZ']"
0,https://resumes.indeed.com/resume/8ded22337e166f66,"[u'NOC Engineer\nNTT DATA - Camas, WA\nJuly 2013 to Present\n\u2022Troubleshoot, diagnosed, installed, upgraded, configured, and repaired computer systems and network system components with Windows 2008 & 2012 R2 Server, Cisco IOS, Linux, Remedy, Proactive, Oracle VM, VMware, Wireshark, Visio, Project, and Outlook\n\u2022Monitored networks for state and government agencies as well as global corporate organizations with HIPAA and ITIL training\n\u2022Operated remote desktop consoles in order to monitor the performance of computer systems and networks\n\u2022Patched servers using WSUS\n\u2022Performed routine network startup and shutdown procedures, and maintained control records\n\u2022Remote administration of server networks and virtual machines\n\u2022Perform backup reports for clients and troubleshoot failed backups', u'Data Center Technician Internship\nDaimler Trucks North America - Portland, OR\nJuly 2013 to December 2013']","[u""Bachelor's in Network Administration"", u'Associate in Cisco Network Administrator', u'Associate in Microsoft Network Technology', u'Associate in Diesel Technology']","[u'Western Governors University Seattle, WA\nJanuary 2015 to March 2018', u'Clark College Vancouver, WA\nJanuary 2011 to January 2013', u'Clark College Vancouver, WA\nJanuary 2011 to January 2012', u'Clark College Vancouver, WA\nJanuary 2001 to January 2003']"
0,https://resumes.indeed.com/resume/1034de8da652efc6,"[u'Big Data Engineer\nVintech Solutions\nApril 2017 to Present\nInvolved in Stream processing using spark and flink on the data ingesting using kafka and build analytical models out of that for downstream applications. Used hive for archival reporting on the historical data.', u'Big Data Engineer\nGCN Media Publishing\nMarch 2016 to March 2017\nMigrate the ONEcount platform from its current MySQL architecture to an appropriate, big data engine(s) so that it can scale to meet the growing demands of expanding customer base. Change existing data pipeline based on LAMP architecture to big data based toolkit.', u'Data warehousing\nMicronTechnologies\nDecember 2014 to December 2015\nData warehousing\nBuild data warehouse to ingest data from multiple sources and analyze data to reflect internal systems.Implemented processing using spark and migrated existing workflows from MapReduce for more efficiency.', u'Data Scientist (Intern)\nVintech Solutions\nJune 2015 to August 2015\nProduct Demand Forecasting Data Scientist Intern\nEvaluate different machine learning techniques for the demand forecast of different products and adjust the production. This includes data ingestion cleaning using Hadoop environment and implementation of different classification techniques.', u""Big Data analyst\nMorgan Stanley\nJanuary 2012 to November 2014\nBuild Data Warehouse for archiving data, develop data access layer to provide unified data access platform for downstream applications and support various data analytics. HDFS and MongoDB are used as storage layer for warehouse depending on the required response times from the warehouse. The data ingestion flow is built using Flume, Sqoop and Kafka.\n\nClient: Western Union\nSocial Media Influencer Data Analyst\nPerform Sentiment analysis of different users from the data extracted from various social networking sites like Facebook, Twitter, Blogs and review websites. The data is ingested and cleansed using Map-Reduce. Polarity of users is evaluated using Open NLP package and Bag-of-Words approach.\n\nClient: Barclays\nMarket Trend Analyzer Big Data analyst\nAggregate and analyze log files generated by the web servers and stock transactional information and derive use fulin formation to identify patterns of market trends and trader's interests. The future predictions are made about the scrip's from the past market trends and integrating that information from the news and implementing different machine learning techniques on them.\n\nClient: Nielsen Holdings\nAssortMan Hadoop Developer\nStudy the effects of different variables in market on a particular product by analyzing transactional and operational Logs. Regression is implemented for SKU Rationalization, filling distribution gaps and to identify regional effects, effects of competitors, study and minimize effects of cannibalization.\n\nPricing Insights Hadoop Developer\nEfficient pricing strategy is determined by analyzing prices in different areas and the prices of competitors. The pricing information is extracted from various databases using Sqoop and combined with log information ingested using Flume and cleaned and pricing strategy is developed by analysis on cleaned dataset.""]","[u'Master of Science in Computer and Information Sciences in Computer and Information Sciences', u'Bachelor of Technology in Electronics and Communications Engineering', u'']","[u'Purdue School of science, IUPUI\nDecember 2015', u'Jawaharlal Nehru Technological University\nMay 2011', u'John Hopkins University']"
0,https://resumes.indeed.com/resume/b3f0627e42c5883c,"[u""Data Engineer\nSapient Nitro - Bangalore, KARNATAKA, IN\nJune 2015 to June 2017\n\u2022 Created a recommender's section on a test e-commerce site as an internal POC\n\u2022 Collaborated with team in developing innovative strategies for distributed data partitioning\n\u2022 Developed code using Scala on AWS Redshift and Cassandra to store products viewed by a user\nProject: IDIOM\nRole: Data Engineer\n\u2022 Developed an automated Big Data ETL framework using Spark Scala and Shell Scripting based on AWS\n\u2022 Recommended efficient solutions for complex requirements to clients\n\u2022 Collaborated with Data Scientists for parallelizing and automating explanatory & predictive algorithms\n\u2022 Created a highly configurable Big Data Validation framework with error reporting within 3 months\n\u2022 Analyzed and resolved multiple Out-Of-Memory Issues in large clusters with more than 2TB memory\n\u2022 Performed optimizations in parallel processing to decrease processing time by 60%""]","[u'Master of Science in Business Analytics and Project Management', u'Bachelor of Engineering in Electrical and Electronic Engineering']","[u'University of Connecticut School of Business Hartford, CT\nAugust 2017 to Present', u'Vellore Institute of Technology Vellore, IN\nJuly 2011 to May 2015']"
0,https://resumes.indeed.com/resume/b3117a28b16b9b5f,"[u""Data Architect/Senior Data Analyst\nMultiplan - Rockville, MD\nOctober 2017 to Present\nResponsibilities:\n\u2022 Designed the ER diagrams, logical model (relationship, cardinality, attributes, and, candidate keys) and physical database (capacity planning, object creation and aggregation strategies) as per business requirements and generated DDL statements using ER/Studio.\n\u2022 Design and Maintenance Data Governance- complete metadata, database change logs, Sub models and associated support / maintenance procedures\n\u2022 Effectively articulated reasoning for data model design decisions and strategically incorporated team member feedback to produce the and fully comply with data quality standards, architectural guidelines and designs.\n\u2022 Performed Insurance Claim Data profiling using Toad and documented steps for better Enterprise Data integration based on file type, claim type, claim line level data.\n\u2022 Create cross-functional Dashboards and business unit's specific dashboards using Tableau visualization such as maps and bubble charts etc. to provide visibility into customer billing demographics and insurance sales metrics\n\u2022 Documented business processes and their service bottlenecks\n\u2022 Support Grid Gain In memory data grid platform for enhanced data handling across organization and drive business intelligence projects and initiatives\nTechnical: Tableau 10, ER Studio, TOAD, Visual paradigm, Informatica, Oracle DB, Grid Gain, PL-SQL Python 2.7, Java, Unix, Visual Paradigm, HIPPA and CMS standards"", u""Enterprise Data Modeler/Senior Data Analyst\nCHIA - Boston, MA\nMarch 2014 to July 2017\nResponsibilities:\n\u2022 Designed and Developed logical & physical data models, dashboards for OLTP and OLAP applications\n\u2022 Gathered business requirements, working closely with business users, project leaders and developers.\n\u2022 Involved in Data flow analysis, Data modeling, Physical database design, forms design and development, data conversion, performance analysis and tuning.\n\u2022 Used ERwinto create Enterprise data model for effective model management of sharing, dividing and reusing model information and design for productivity improvement along with IBM industry data model customizations.\n\u2022 Presented the data scenarios usingTableau visualizations and excel mockups to view the data better.\n\u2022 Responsible for Data profiling of APCD, Acute Hospital, Case Mix & MA Hospitals, Insurance data\n\u2022 Responsible for Data Loading and Quality Assurance\n\u2022 Created regular and ad hoc reporting dashboards forpower users using Tableau and SRSS services\n\u2022 Worked on a POC's to deploy cloud and OBIEE services for easy data migration services\n\u2022 Documented user manuals, program documentation, training& operational procedures\nTechnical: Tableau, OBIEE, SSRS, Erwin 9.5, IBM Infosphere Data Architect, SSMS, T-SQL, PL-SQL, N-SQL, IBM Data models, AWS, Oracle 11g, Netezza, SQL Server 2014-08, Aginity Workbench, HIPPA and CMS standards"", u""Enterprise Data Modeler/Data Analyst\nUW Health, WI\nSeptember 2013 to March 2014\nResponsibilities:\n\u2022 In coordination with Project manager, Lead the BI team of 2 offshore resources and 2 onsite resources\n\u2022 Explored functional areas to revise data models and Conducted fit gap analysis with SME's to design and develop an enterprise data model based on different subject areas\n\u2022 Created Enterprise Data dictionary, Tech-design specifications document and STT document of data migration pipeline for legacy user data from cross business units of different applications data housed on IBM DB2, Sql server, Access and Teradata and excel platforms which were housed in multiple sites of UW financial and UW Hospital organizations\n\u2022 Designed new data model for migrating legacy financial data incoming frompeople soft financials and human resource data from people soft HR systems into netezza data warehouse\n\u2022 Create Test Plans, Test Cases and performed unit and integration testing\nTechnical: Erwin, Oracle 10g, MS Sqlserver 2012, IBM Db2, Access, Aginity Workbench, Netezza, SQL Server 2014-08, Teradata 12.0, Epic, Python, Visio, MS Project, Shell scripting"", u'Research Assistant Data Engineer\nCAMT - Rolla, MO\nAugust 2011 to August 2013\nResponsibilities:\n\u2022 Worked with Boeing design teams to understand the development process and document data flow\n\u2022 Conducted JAD sessions with design teams from Vanderbilt, Penn State and Georgia tech universities to drive consensus & attainment of research goals\n\u2022 Performed manual testing, regression testing and documented to results\n\u2022 Used bug tracking tool to track and notify defects for cross functional teams\n\u2022 Designed Use-Case Diagrams, Activity Diagrams, Sequence Diagrams\nTechnical: Matlab, Visio, Systems Engineering, LabVIEW, CAD/CAM, Automation & Robotics', u'Engineer\nTransvahan Technologies - Bengaluru, Karnataka\nJanuary 2011 to May 2011\nResponsibilities:\n\u2022 Liaise between business & technical personnel to ensure alignment with development teams\n\u2022 Facilitate weekly meetings with clients to capture bottlenecks and explore potential solutions\n\u2022 Identify and reconcile errors in logistics data to ensure accurate business requirements\n\u2022 Developed presentations for clients, explained and recommended solutions\n\u2022 Trained power users in utilizing information through dashboards\nTechnical: Access, Excel, Sales force, MS Visio, MS Office, MS Project']","[u""Master's""]",[u'']
0,https://resumes.indeed.com/resume/05cead52f7d533ad,"[u'Data Scientist\nArine - San Francisco, CA\nJanuary 2018 to Present\nStartup in California, Dec 2017 - Current\n\u2022 Work with AWS technology stack to process healthcare data. This involves writing code in Python, Spark, ML, SQL, Postgres.\n\u2022 Develop algorithms to calculate medication adherence, and to generate insights that impact patient health outcomes using machine learning techniques.\n\u2022 Integrate third party healthcare data (e.g. drug information, medication diagnoses, national provider identifier) into a single repository.\n\u2022 Write production quality code and unit tests.\n\u2022 Work closely with members of Data Science, Cloud Services, Clinical teams.', u'Associate Instructor\nIndiana University Bloomington\nAugust 2016 to Present\n\u2022 Tutoring graduate students in topics on machine learning. For topics covered, please refer to skills on my web page (http://pages.iu.edu/~vmarni/)\n\u2022 Assisting with the grading of programming assignments, exams, and projects.', u""Data Scientist Intern\nProteus Digital Health - Redwood City, CA\nMay 2017 to August 2017\nEffects of medication adherence on Heart Failure Hospital Re-admissions in Inovalon Administrative Claims Dataset.\n\u2022 Statistical modeling to evaluate the time-dependent association between medication adherence and re-admission events in heart failure patients with comorbidities.\n\u2022 Built a unique NDC Drug database to map drugs to pharmacy claims to classify drugs into heart failure medication classes.\n\u2022 Worked on one of the nation's largest healthcare data sets (>50 GB) in which patients are joined longitudinally across insurance plans.\nTechnologies: AWS EMR Cluster with Hadoop-2.7.3, Hive-2.3, Spark-2.1 (Pyspark, SparkR, Spark-SQL), AWS-S3."", u'Project Engineer\nWipro Technologies\nJuly 2014 to December 2015\n\u2022 Worked on preprocessing raw data and generate useful features specific to tasks. Improved my statistical skills.\n\u2022 Worked with RDBMS databases to gather the required data required for backend support operations.']","[u'Master of Science in Data Science in Data Science', u'in Electronics & Communication']","[u'Indiana University', u'SRM University']"
0,https://resumes.indeed.com/resume/833051a78c723df2,"[u'Robotics Engineer\nJLL - Monee, IL\nJuly 2017 to Present', u'Feild Technician\nArmor Technologies - DeKalb, IL\nApril 2015 to August 2016', u'Data Network Specialist\nDepartment of Defense United States Marine Corps\nJuly 2009 to July 2014\nManaged and troubleshot cyber network communications ensuring consistent computer asset connectivity via military networks for over 500 Marines.\n\u2022 Supervised and educated 10-15 Marines daily in the operations of network infrastructure configurations.\n\u2022 Ensured Marines were capable and qualified in their Military Occupational Specialty (MOS) for Mission Success.\n\u2022 Assisted in the development and re-certification of the Communications Military Occupational Specialty Roadmaps.\n\u2022 Facilitated yearly courses including: Personally Identifiable Information (PII), Cyber Awareness Training, and Information Assurance Awareness.\n\u2022 Responsible for and trusted with the handling of $3.6 million worth of equipment in the Combat Operation Center which included routers, switches, servers, computer assets, and printers.\n\u2022 Designed and created network diagrams for the success of over 45 Missions.']",[u'Military Education in Computer Science'],"[u'MCCES Camp Pendleton, CA\nOctober 2009 to October 2010']"
0,https://resumes.indeed.com/resume/c71f1306c2293bd0,"[u'Data Analyst\nSamsung Electronics America - Coppell, TX\nAugust 2017 to Present\n\u2022 Collect and distribute customer demand, and make it available to everybody involved in the forecasting process\n\u2022 Analyze return trends and identify the key drivers of returns of products and product groups\n\u2022 Gather, analyze and validate all data that will be used in preparing statistical forecasts\n\u2022 Prepare reports and data that will be used in the forecast review process - part of the Operations Planning process\n\u2022 Analyze trends and identify opportunities for improving the accuracy of future sales forecasts\nManage overseas logistics', u'Manufacturing Engineer Intern\nMartin Sprocket & Gear - Fort Worth, TX\nMay 2016 to August 2016\n\u2022 Gathered and interpreted relevant data to help save cost by modifying shipping materials, cutting $28,000 in shipping cost\n\u2022 Improved CNC machine cycle times through time motion study & the identifying of bottlenecks\n\u2022 Applied disc/belt oil skimmers to save cost on CNC machine filter, saving $20,000 annually\n\u2022 Initiated Lean tactics such as 5S improvement and DMAIC towards Tool Box Area\n\u2022 Developed Activity Report comparisons with CNC machines versus employee analysis\n\u2022 Redesigned the Dynamic Balancing Area in the Power and Transmission Warehouse\n\u2022 Managed multiple tasks and projects simultaneously in a professional manner through tracking cycle times, completed parts and completing projects assigned', u'Production Assistant\nCVE Technology Group, Inc - Plano, TX\nJune 2015 to August 2015\n\u2022 Maintained products by following the guidelines through the processes of receiving, production, and shipping in a warehouse environment for refurbished mobile devices, tablets, and other electronic devices\n\u2022 Managed shipment priorities and production goals based on incoming shipments, inventory, and outgoing logistics of products\n\u2022 Provided daily and weekly reports to upper management on warehouse activities\nResolved discrepancies with third parties based on contractual agreements']",[u'Bachelor of Science in Industrial Engineering'],"[u'The University of Texas at Arlington Arlington, TX\nMay 2017']"
0,https://resumes.indeed.com/resume/c8d72fc792defdb7,"[u'Data Migration Specialist\nZayo Group LLC / AboveNet Inc - Boulder, CO\nJune 2012 to March 2017\n\u2022 Maintained existing data migration program with occasional upgrades and enhancements.\n\u2022 Performed data migration enforcement tasks.\n\u2022 Furnished support to field services and technical groups as needed.\n\u2022 Utilized existing data scripts and processes.\n\u2022 Conducted data structures and movement evaluations.\n\u2022 Executed data migration in coordination with management and technical services personnel.', u""Fiber Engineer\nAboveNet Inc. / Zayo Group LLC - Addison, TX\nAugust 2007 to June 2012\n\u2022 Analyze OTDR traces, evaluate characteristics of fiber, and document test results.\n\u2022 Planning and designing fiber routes\n\u2022 Design and document fiber splicing and fiber patch assignments for fiber circuits.\n\u2022 Work with Construction PM's and/or field personnel to ensure quality and timely delivery of our services.\n\u2022 Maintain a positive attitude and strong work ethic that can provide high productivity with minimal supervision.\n\u2022 Perform other duties as requested by supervisor"", u'Technical Supervisor\nTime Warner /Spectrum - Farmers Branch, TX\nJuly 1995 to September 2006\n\u2022 Supervised 15 techs\n\u2022 Responsible for training\n\u2022 Hiring, Firing\n\u2022 Corrective actions\n\u2022 Handling escalated customer situations\n\u2022 Dealing with High Profile customers\n\u2022 Mid-year and end of year reviews\n\u2022 Weekly meetings\n\u2022 Maintaining customer satisfaction\n\u2022 Maintaining Team morale.']",[u'Vocational in Electronics Engineering'],"[u'Southern Institute of Business Technology Richardson, TX\nJune 1986 to September 1989']"
0,https://resumes.indeed.com/resume/7c0ced42c442cc28,"[u'Data Analyst Intern\nDouglas Elliman Real Estate - New York, NY\nJune 2017 to August 2017\n- \x9f Built database in MS Access for NYC new residential developments; calculated major metrics in SQL for company use and designed user-friendly search engine, increasing data acquisition speed by 70%\n- Analyzed data in R, including visualization and trend analysis to assist in the production of seasonal insight reports\n- Extracted data in Python from websites to analyze real estate listings, offering prices and building features\n- Guided the research and marketing team to maintain and update the database', u'Data Analyst Intern\nDeloitte - Beijing\nDecember 2015 to February 2016\n- Performed exploratory analysis on clients\u2019 income data and extracted unusual finance activities for risk recognition\n- Generated seasonal income analysis dashboards in VBA, including metrics calculation and trends analysis\n- Developed database of history income data in SQL Server for advisory team use, improving data querying efficiency', u'Business Intelligence Engineer Intern\nApp Annie - Beijing\nJune 2015 to December 2015\n- Design and published automatic ETL monitoring product for internal stakeholders by connecting Tableau, data warehouse and databases together, which reduced workload by 80%\n- Designed BI visualization products in Tableau for mobile app publishers, including key metrics like downloads estimates, revenue estimates and user demographics\n- Developed ETL workflows from multiple databases to cloud data warehouse in HiveQL environment for data integration\n- Conducted troubleshooting for various data issues in ETL jobs and fixed bugs, reducing future issues by 40%']",[u'MS in Operations Research'],"[u'Columbia University New York, NY\nSeptember 2016 to February 2018']"
0,https://resumes.indeed.com/resume/fe13b4bac133d0cd,"[u'Data Scientist\nCreighton University - Omaha, NE\nFebruary 2017 to Present\n\u2022 Leading a team of Big Data engineers.\n\u2022 Architecting data flows with Spark, Hive, Cassandra, and Python.\n\u2022 Developing complex PL/SQL queries for Oracle databases.\n\u2022 Implementing data integrations with SQL Server Integration Services (SSIS).\n\u2022 Administering Linux (CentOS 7) and Windows Server 2012 servers.', u'Data Programmer/Analyst\nMWI Direct - Lincoln, NE\nJuly 2015 to February 2017\n\u2022 Designing and maintaining SQL Server databases.\n\u2022 Producing machine learning and business intelligence solutions in Python, R, and RedPoint Data Management.\n\u2022 Designing print and web layouts in Photoshop, PReS, and PReS Connect.\n\u2022 Maintaining legacy Visual FoxPro applications and porting them to RedPoint Data Management, SQL Server Integration Services, C#, Python, and R.', u'Field Engineer\nFive Nines Technology Group - Lincoln, NE\nFebruary 2015 to July 2015\n\u2022 Provided first-class technical support for Windows 7 and Windows 8.1 users.\n\u2022 Administered servers and networks for a broad range of small- and medium-sized businesses.\n\u2022 Automated systems administration with PowerShell.\n\u2022 Resolved problems as quickly and thoroughly as possible.\n\u2022 Recognized twice for outstanding \u201ccustomer savvy.\u201d', u'Support Engineer\nFive Nines Technology Group - Lincoln, NE\nSeptember 2014 to February 2015\n\u2022 Provided first-class technical support for Windows 7 and Windows 8.1 users.\n\u2022 Administered servers and networks for a broad range of small- and medium-sized businesses.\n\u2022 Resolved problems as quickly and thoroughly as possible.\n\u2022 Recognized twice for outstanding \u201ccustomer savvy.\u201d', u'Data Curation Intern\nLogos Research Systems - Bellingham, WA\nMay 2014 to December 2014\n\u2022 Curated and analyzed cultural data from ancient literature using MySQL and SQLite databases, Python, and proprietary software.', u'Teaching Assistant\nUniversity of Iowa - Iowa City, IA\nAugust 2012 to May 2014\n\u2022 Communicated complex material in understandable ways.\n\u2022 Named a \u201cFirst-Year Student Champion\u201d in 2012.\n\u2022 Received the Heidel Award for Teaching Assistants in 2014.\n\u2022 Managed the production of high-quality web products.', u'Graduate Researcher\nDigital Studio for Public Arts and Humanities - Iowa City, IA\nJanuary 2012 to May 2014\n\u2022 Partnered closely with faculty and staff to produce excellent public, digital scholarship.\n\u2022 Provided education and technical support to students, faculty, and staff.\n\u2022 Summarized technical research for non-specialist audiences.']","[u'MA', u'BA']","[u'University of Iowa Iowa City, IA\nJanuary 2015', u'Union University Jackson, TN\nJanuary 2010']"
0,https://resumes.indeed.com/resume/8d760f97690bc281,"[u'Data Analyst\nAffline Analytics - Bengaluru, Karnataka\nJune 2015 to November 2015\n\u2022 Work with large amount of unstructured data which can be preprocessed and structured by appropriate tools.\n\u2022 Strong ability to analysis of statistics and generate reports in given time.\n\u2022 Built ability to develop and maintain good relationship and communication with team members at all level.\n\u2022 Responsible for generating reports in terms of data visualization using R or Excel tools.', u'Data Engineer\nAztec Software and Technology Services PVT.LTD - Bengaluru, Karnataka\nAugust 2014 to April 2015\n\u2022 Create and maintain optimal data pipeline architecture.\n\u2022 Assemble large, complex dataset that meets functional / nonfunctional business requirements.\n\u2022 Worked with Data wrangling and Data preprocessing.\n\u2022 Responsible for Integration of large data with different sources and formats.']","[u'Master of Science in Computer Science', u'Bachelor of Engineering in Electronics and Communication Eng']","[u'New Jersey Institute of Technology Newark, NJ\nDecember 2017', u'Shree Devi Institute of Technology\nJuly 2014']"
0,https://resumes.indeed.com/resume/18cdcbfdbfdb531f,"[u'Data Analyst (Assistant System Engineer)\nTata Consultancy Services Ltd - Chennai, Tamil Nadu\nSeptember 2015 to July 2017\n\u2022 Created Tableau dashboards to analyze application performance patterns and delivered insights to clients\n\u2022 Developed ETL workflows to integrate, transform and summarize data from multiple sources using SQL Server\nIntegration Services (SSIS)\n\u2022 Prepared Excel Dashboards and conducted Profitability Analysis for projects undertaken by the firm\n\u2022 Built complex SQL queries for retrieval of data to be used as inputs for Tableau and QA automation scripts\n\u2022 Trained over 100 freshers on SQL and Tableau, and designed TCS internal assessment modules\n\u2022 Requirement gathering from clients and worked on ""Request for Proposals (RFP)"" solutions\n\u2022 Received ""ILP Harbinger"" award for showcasing exponential growth in training period among all new hires']","[u'Master of Science in Business Analytics Project Management', u'Bachelor of Technology in Mechanical Engineering']","[u'University of Connecticut School of Business Hartford, CT\nDecember 2018', u'Jawaharlal Nehru Technological University\nMay 2015']"
0,https://resumes.indeed.com/resume/c2e384891bf1b6c1,"[u""Associate Data Engineer\nAcxiom Corporation\nJanuary 2017 to Present\n* Identified, created and defined application requirements for Workday integrations both inbound and outbound\n* Ensured delivery of meaningful data to the right client's\n* Defined data structures over multiple platforms through API's, Web-Services, and report writing\n* Assisted and lead in the design phase of projects over several segments of the business including human capital, payroll, security, and financial management systems\n* Documented project specifications by tracking work through ServiceNow and Atlassian JIRA in an Agile environment\n* Coordinated with the project teams and customers - provided regular status reports and updates - proposed changes as necessary\n* Reviewed, monitored and troubleshot scheduled jobs and ensured successful completion\n* Produced internal integrations on the Workday Cloud Platform as well as report writing and calculated field development\n* Conducted integrations testing for semi-annual Workday releases\n\n* Served as a front-line Workday support specialist in regards to report writing, calculated fields, integrations, and security changes/requests\n* Worked with business process configurations dealing with Workday triggers to external cloud integrations systems including SnapLogic"", u""Delivery Analyst\nAcxiom Corporation\nJanuary 2016 to December 2016\n* Delivered data, data products, and information that drove our client's multi-channel marketing capabilities. Understood the details of our client's data and Acxiom's products to ensure that our clients and/ or third party vendors receive clean, enhanced, segmented, ready-to-market data.\n* Strove daily to improve quality, and adhere to standard processes and procedures. Worked with clients and/or third-party vendors to begin to understand requirements and escalated issues with a focus on maintaining a seamless process flow.\n* Analyzed, troubleshot, and modified processes and data to ensure efficient delivery of services to Acxiom clients. Maintained a\nbroad understanding of Acxiom's processing and product capabilities and utilized that understanding to optimize delivery of services.\n* Assisted in marketing services\n\nPersonal Achievements/Activities""]",[u'Bachelor of Administration in Management Information Systems'],"[u'University of Central Arkansas Conway, AR\nMay 2017']"
0,https://resumes.indeed.com/resume/727dc39c953210b8,"[u'Associate Engineer\nVEQTOR - Kankakee, IL\nMay 2017 to Present\nCleaning Validation', u'Data Entry Specialist\nAbbott Laboratories - Abbott Park, IL\nMay 2016 to Present\n-Collaborated with senior IT staff and coworkers to optimize data output\n-Corrected high volumes of job worldwide\n-Provided innovative solutions to the overall data entry process\n-Reported correction totals and erroneous requisitions to Sr. Manager']",[u'B.S. in Chemical Engineering'],"[u'Iowa State University Ames, IA\nDecember 2016']"
0,https://resumes.indeed.com/resume/899f569dd16451df,"[u'Data Center Hardware Engineer\nBloomberg L.P\nJune 2017 to Present\n\u2022 Rack and Stack Devices in cabinet.\n\u2022 Monitoring network devices.\n\u2022 Decommission old servers physically from the cabinet and data base as well.\n\u2022 Gather log reports from GUI & CLI of servers on error reported devices.\n\u2022 Opening cases with vendors on the reported issues and follow up with vendors.\n\u2022 Installed Avocent ACS 6048 terminal server and configured baseline script.\n\u2022 As a part of operations team take part in daily operations.', u'Network Engineer Intern\nPRESOURCETEK LLC\nJuly 2016 to December 2016\n\u2022 Configured Static routes on CLI using GNS3 emulator.\n\u2022 Configured Dynamic routing protocols like RIP V1, RIP V2, EIGRP, OSPF, BGP on GNS3 command line interface.\n\u2022 Configured redistribution between different routing protocols.\n\u2022 Configured Virtual Private Networks such as site -to site and remote VPN using GNS3 emulator using IPsec.\n\u2022 Configured Network Address Translation.\n\u2022 Created standard and extended ACLs on CLI using GNS3.\n\u2022 Created VLANs and assigned IP addresses to each VLAN using Cisco packet tracer.\n\u2022 Configured trunk links between VLANs and configured frame tagging protocols like ISL and 802.1Q.\n\u2022 Configured STP for loops prevention on GNS3 CLI.\n\u2022 Configured Redundancy protocols like HSRP, VRRP and GLBP on GNS3 CLI.']","[u'Master of Science in Electrical Engineering', u'Bachelor of Engineering in Electronics and Communication']","[u'Texas A&M University\nMay 2016', u'Jawaharlal Nehru Technological University Hyderabad\nMay 2013']"
0,https://resumes.indeed.com/resume/36c6385b14a49c83,"[u'Big Data Analytics Engineer\nAccenture - Bengaluru, Karnataka\nJune 2015 to July 2017\n\u2022 Setup the big data environment and Developed business specific reports using Hadoop, Pig, Hive, HBase, and Cassandra\n\u2022 Developed PIG scripts for handling business transformations and data analysis\n\u2022 Created SQOOP jobs for large data transfer between Hadoop and RDBMs\n\u2022 Implemented the spark for heavy data to build daily reports consisting highest usage data, bill on daily and weekly basis\n\u2022 Written complex queries in Hive, Spark SQL APIs to generate daily and weekly reports', u'Database Consultant\nMicrosoft Corporation\nJanuary 2014 to June 2015\n\u2022 Designed databases, stored procedures, reports, and data input interfaces using SQL Server 2010, 2012\n\u2022 Test databases, perform bug fixes and optimize database systems for performance and operational efficiency\n\u2022 Worked on ETL reports using Tableau and created statistics dashboards for Analytics\n\u2022 Generated Weekly Excel reports using pivot-tables, charts from SQL database and shared with delivery leads and team members\n\u2022 Automated a tool to set user access level, privileges and permissions for database reducing quarterly cost and manual efforts\n\u2022 Performed analysis and presented results using SQL, SSIS, MS Access, Excel, and Visual Basic scripts']","[u'Master of Science in Business Analytics and Information Systems in Business Analytics and Information Systems', u'Bachelor of Engineering in Computer Science in Computer Science']","[u'University of South Florida Tampa, FL\nAugust 2017 to December 2018', u'Graphic Era University Dehra Dun, Uttarakhand\nAugust 2009 to June 2013']"
0,https://resumes.indeed.com/resume/9cbb13b2e1b51324,"[u""Project Engineer / Data Analyst\nWipro Technologies Ltd - Pune, Maharashtra\nOctober 2014 to April 2017\nClient: Novartis August 2015-April 2017\n\u2022 Worked in team to migrate data from existing relational database system to Hadoop ecosystem using Sqoop.\n\u2022 Configured Hive metastore and implemented partitioning for extraction of user's demographic and asset data.\n\u2022 Setup Hadoop environment for POC migration and did 2SLS regression analysis in Spark R to determine Change deployment cost.\n\u2022 Build and scheduled automated job for collection of performance metrics in incident and problem reported by users.\n\u2022 Build weekly and monthly visualizations in Tableau for continuous monitoring and improvement of Incident and Change metrics.\nClient: Standard Bank October 2014-July 2015\n\u2022 Analyzed software asset's license reports to successfully interpret probable costs for renewal using logistic regression model in SAS.\n\u2022 Worked with ETL team on transformations of license data in Informatica tool such as aggregator, sorter, filter.\n\u2022 Automated and scheduled reconciliation job daily to link discovered asset with their Configuration Items relationships.\n\u2022 Designed active links and filters for the application which significantly reduced the time taken to upload data by on-site clients.""]","[u'Masters of Science in Business Analytics and Information Systems', u'Bachelor of Technology in Electronics and Communication Engineering']","[u'University of South Florida Tampa, FL\nDecember 2018', u""I.T.E.R S'O'A University Bhubaneshwar, Orissa\nJune 2014""]"
0,https://resumes.indeed.com/resume/ade52b5114739564,"[u'Data Engineer\nWest Corporation - Omaha, NE\nMarch 2016 to Present\nWest develops technology-enabled communications products and services, with high amounts of data storage. Worked on designing statistical analysis model and visualizing results using SAS, SQL Server Reporting Services and Tableau\nResponsibilities\n* Performed extraction, manipulation, processing, analysis and storage of various types of data.\n* Created models to analyze process flow by performing AB Testing, analyze product differentiation by using Cluster analysis and Market Basket analysis, analyze performance based on Day of Week (DoW) and Time of Day (ToD) analysis, analyze customer behavior based on Logistic Regression analysis.\n* Refined performance of analytical scripts using SAS and maintained its functionality throughout its life cycle.\n* Created visualizations and reports using Tableau to monitor performance of the analytical model\n* Generated Ad-hoc Report, Dashboards, Sub-Reports, Drilldown-Reports and PivotTable Reports.\nEnvironment: SAS Base with Proc SQL, SQL Server 2012, Tableau, Excel', u'Business Intelligence Developer\nNeutron USA\nAugust 2012 to September 2014\nNeutron USA is an IT company reseller with data stored in a data warehouse, used to generate leads for marketing purposes. Designed, developed and maintained database applications using SQL Server 2012.\nResponsibilities\n* Participated in JAD sessions with different departments to capture and document system requirements.\n* Created database objects like stored procedures, Triggers, User-defined Functions, Views etc.\n* Refined the performance of various SQL scripts and stored procedures using stored procedures.\n* Involved in creating SSIS Packages to perform ETL on multiple input sources like flat files, Excel sheets, and OLTP/ OLAP servers into SQL Server Staging Database.\n* Generated Ad-hoc Report, Dashboards, Drilldown-Reports, Drillthrough-Report and linked reports\nEnvironment: MS SQL Server 2012, Windows Server 2008R2, ER-Win, SSIS, SSRS, Excel']",[u'Master of Science in Information Systems with Data Analytics'],"[u'University of Nebraska Omaha Omaha, NE\nJanuary 2016']"
0,https://resumes.indeed.com/resume/c141099e1d18fd02,"[u'Data Network Engineer\nTELEFONICA EMPRESAS - Caracas, Distrito Capital\nAugust 1993 to November 2015\nData network engineer, ip address administrator, switching and routing specialist, configuring Nodes B (2G, 3G, 4G), Manage WAN Operation and Performance, Manage Internet Connections, Bandwidth Requirements, and Plan for Future Expansions, Maintain Networks and Related Components, Install and Maintain Servers and Switches, Provide District Wide Hardware/Software Repair, Maintenance, and Installation, Recommend Design and Selection of Emerging Technology Components, Seek and Maintain Needed Certifications as Technology Emerges, Perform Other Duties as Directed, Troubleshooting Experience']",[u'associate degree computer engeniering in Comunications'],"[u'Franklin Institute of Boston Tulsa, OK\nJanuary 1993 to January 1995']"
0,https://resumes.indeed.com/resume/f71cf0f7e6b13404,"[u""Business Analyst Engineer\nSessionM - Boston, MA\nJanuary 2017 to February 2018\nGathered requirements for client reports and support client inquiries regarding their data and reports\n\u25cf Creation and automation of custom client reports regarding metrics not provided by our\nplatform using scripts written in Shell, Bash, and Ruby\n\u25cf Created reports to monitor the performance of campaigns and promotions\n\u25cf Make recommendations to product regarding metrics that are frequently asked for by clients\nthat aren't a part of the product"", u""Software Data Engineer\nJanuary 2015 to January 2017\nCreated and modified existing Ruby scripts to perform ETL on data through CDH4, Hive,\nHadoop, Oozie, S3, Redshift, and MySQL\n\u25cf Helped manage, troubleshooted, and supported data pipelines for different clients running on linux servers\n\u25cf Supported and helped create a product recommendation engine written in R which utilized\ndata generated from Spotify's logistic matrix factorization for implicit feedback written in\nPython by performing ETL of data"", u""Software Quality Assurance Engineer\nSeptember 2013 to January 2015\nCreate, update, and execute test plans for web and mobile applications using TestLink\n\u25cf Document bugs, test new features, and keep track of issues through Jira\n\u25cf Web GUI testing and automation using Selenium and Java\n\u25cf Make recommendations for our product's UI/UX and provide feedback based off of testing\nresults and personal experience""]",[u'Bachelor of Computer Science in Computer Science'],"[u'Wentworth Institute of Technology Boston, MA\nAugust 2014']"
0,https://resumes.indeed.com/resume/d762db2f2292cea9,"[u'Software Engineer\nQualcomm - San Diego, CA\nSeptember 2017 to September 2017\n\u2022 Developing Flight Automation Tool for testing the performance of features like Visual Inertial Odometry,\nVisual Obstacle Avoidance in Dragonfly Drones. Designing tools for executing regression, sanity and unit\ntest suits.', u'Data Service Operations Engineer Intern\nCymer - San Diego, CA\nJune 2017 to August 2017\nSan Diego, US\n\u2022 Designed DSO Dashboard (Data Service Operations) website for better tracking of lasers and their re- spective servers. Provided better website navigation experience by reducing webpages load time from 10\nsec to 100 msecs and improving backend database performance with load-reduction and load-balancing.']","[u'M.S. in Computer Science in Computer Science', u'B.Tech. in Computer Science in Score']","[u'San Diego State University\nMay 2018', u'Ambedkar Institute of Technology Delhi, Delhi\nJune 2016']"
0,https://resumes.indeed.com/resume/b474102653ffda91,"[u'Data Engineer Intern\nSert\xfcrner - New York, NY\nSeptember 2017 to December 2017\n\u2022 Consolidated generic alternatives of branded drugs by developing web crawler and extracting data into a common report (Scrapy, Python, SQL), which resulted in competitive deals for client (healthcare provider)\n\u2022 Linked pharmaceutical companies with their multiple warehouse addresses and obtained 3 closest locations, potentially saved up to 16% on logistics expenses', u'Software Systems Engineer\nDiebold Nixdorf - Mumbai, Maharashtra\nJuly 2013 to April 2016\n\u2022 Customized marketing campaigns using segmentation, customer lifetime value (CLV) and increased leads by 11%\n\u2022 Predicted fraud likelihood at different ATMs using logistic regression in Python, identified key factors for fraud occurrence and optimized cost of installing anti-fraud devices at sensitive locations\n\u2022 Designed early warning dashboard in Tableau to detect frequent card captures and control user complaints\n\u2022 Developed ETL job to integrate transaction level, customer level and third-party user behavior data\n\u2022 Aggregated transactions across multiple channels (mobile, web, ATM & branch), identified user transaction behavior and suggested ways to encourage self-service transactions among senior customers\n\u2022 Performed A/B testing on marketing campaigns, selected ad layouts with consistently high conversion rates and inferred consumer preferences based on winning layout characteristics\n\u2022 Highlighted differences in transaction sequence across customer segments, adjusted duration and placement of ads to enhance readability of users belonging to different age groups']","[u'MS in Business Analytics and Project Management', u'Bachelor of Engineering in Computer Science']","[u'University of Connecticut Hartford, CT\nAugust 2016 to December 2017', u'University of Mumbai Mumbai, Maharashtra\nJune 2009 to June 2013']"
0,https://resumes.indeed.com/resume/5b7e06c1afc05f7c,"[u'Data Analyst\nUNU technology INT - Fremont, CA\nJanuary 2017 to Present\n\u2022 Process raw data from multiple data sources, analyze actionable information for visualization, reports, and dashboards\n\u2022 Conduct data cleaning: solve the missing data issue and transform data using MySQL and R\n\u2022 Create dashboards by Excel and Tableau using Pivot tables and VLOOKUP to generate insights and inform business decisions\n\u2022 Build and maintain macros, templates, recurring metrics and reports, improve efficiency and accuracy\n\u2022 Partner with cross-functional teams in creating business models that support major initiatives and projects\n\u2022 Leverage reporting knowledge to extract targeted datasets from the database, distill insights into the business, and communicate them clearly to management\n\u2022 Drive small to medium projects that help build the company into the most customer-centric platform', u'Data Analyst\nLoving Tree Academy - Fremont, CA\nNovember 2016 to January 2017\n\u2022 Identified, gathered, and manipulated data from internal and external sources to provide a more complete picture of our business relationships\n\u2022 Researched and processed data change management requests\n\u2022 Developed reports, dashboards, and processes to continuously monitor data quality and integrity\n\u2022 Supported marketing and pricing team in the day-to-day execution of projects and initiatives with data\n\u2022 Proactively identified and implemented data improvements, enhancements, and system customizations that meet business requirements', u'IT Engineer\nDNV GL - Beijing, CN\nNovember 2012 to August 2014\nBeijing, China\n\u2022 Participated in IT related projects per requests\n\u2022 Defined hardware and network requirements\n\u2022 Created, managed, and utilized high performance relational databases and proprietary software\n\u2022 Queried and mined large data sets to examine call-in center data and filter for targeted information using traditional as well as predictive/advanced analytic methodologies\n\u2022 Assisted in preparing and presenting complex written and verbal materials (reports, findings and presentations)\n\nPROJECTS\nProject: Business analysis of the new restaurant\n\u2022 Selected, cleaned up and sorted data\n\u2022 Applied regression analysis to analyze each variable such as the number of competitors, the population, the average income, number of cars owned by family and the average age of residents\n\u2022 Predicted sales in the same store in the future year using linear regression and seasonal ARIMA\n\nProject: Use machine learning to classify documents\n\u2022 Performed a data cleaning, including removing training data for missing labels or features, deleting non-alphanumeric characters and transferred text to lower case\n\u2022 Classified documents as spam and legitimate e-mail by comparing logistic regression, KNN-based methods, and k-means clustering algorithms\nProject: West Nile Virus Prediction\n\u2022 Analyzed train data, weather data and GIS data predicted if West Nile virus is present, for a given time, location, and species\n\u2022 Summarized data, visualized data by heatmap, histogram, frequency table, and etc.\n\u2022 Compared Logistic Regression, Random Forest, CART Trees and Neural Network to determine optimal method by choosing the best model.']",[u'M.S. in Statistics'],[u'California State University\nSeptember 2014 to June 2016']
0,https://resumes.indeed.com/resume/5c8f67c9a56a89d9,"[u""HADOOP DATA ENGINEER\nWellCare - Tampa, FL\nMarch 2016 to Present\nArchitecture of new data analytics pipelines and migration of data to cloud.\nArchitected a pipeline to receive, resolve, normalize, route, persistence flows.\nDesign and development of integration workflows.\nDevelop automation and processes to enable teams to deploy, manage, configure, scale, monitor applications in Data Centers and in AWS Cloud.\nManaged highly available and fault tolerant systems in AWS, through various API's, console operations and CLI.\nUsed Amazon Web Services (AWS) like Amazon S3 and Amazon EC2.\nMigration of content from old data warehouse to AWS Redshift data ware house for columnar data storage.\nDesign roles and groups for users and resources using AWS Identity Access Management (IAM) and managed network security using Security Groups, and IAM.\nUtilized Cloud watch to monitor resources such as EC2, CPU memory, Amazon to design high availability applications on AWS across availability zones.\nCreated and maintained continuous build and continuous integration environments in SCRUM and Agile projects."", u'HADOOP DATA ENGINEER\nInternational Paper - Atlanta, GA\nJanuary 2015 to March 2016\nArchitected big data systems on AWS using AWS tools and Redshift database.\nWorked on AWS to create, manage EC2 instances and Hadoop Clusters. Involved in connecting Pentaho 7.0 to target database to get data.\nUsed ETL to transfer the data from the target database to Pentaho to send it to reporting tool MicroStrategy.\nUsed Zookeeper for various types of centralized configurations, GIT for version control, Maven as a build\ntool for deploying the code.\nmoved the data from Hortonworks cluster to AWS EMR cluster.\nInvolved in running Hadoop jobs for processing millions of records and data gets updated on daily and weekly basis.\nContinuous data integration from Mainframe systems to Amazon S3 which is connected via Attunity an ETL tool.\nDocumentation of the tasks and the issues is done.\nPOC involved in loading data from LINUX file system to AWS S3 and HDFS.\nWorked on AWS to create, manage EC2 instances and Hadoop Clusters.\nRan Spark jobs on top of RAW data and transforming\nthe data to generate the desired output files.\nCreated both internal and external tables in Hive and developed Pig scripts to preprocess the data for analysis.\nBuilt a Full-Service Catalog System which has a full workflow using Elasticsearch, Logstash, Kibana, Kinesis, CloudWatch.\nExperience in monitoring tools like Nagios and Amazon Cloudwatch to monitor major metrics like Network packets, CPU utilization, Load Balancer Latency etc.\nManaged all the bugs and changes into a production environment using the JIRA tracking tool\nIntegrated JIRA with CI/CD Pipeline as defect tracking system and configured workflows to automate deployment and issue tracking.', u'HADOOP DATA ENGINEER\nUST Global - Aliso Viejo, CA\nMay 2013 to December 2014\nAnalysis of end user requirements and business rules based on given documentation and worked closely with tech leads and Business analysts in understanding the current system.\nAnalyzed the business requirements and involved in writing Test Plans and Test Cases.\nInvolved in creating Hive Tables, loading with data and writing Hive queries which will invoke and run Spark jobs in the backend.\nDesigned and implemented Incremental Imports into Hive tables.\nInvolved in collecting, aggregating and moving data from servers to HDFS using Apache Flume\nExperienced in managing and reviewing the Hadoop log files.\nImplemented the workflows using Apache Oozie framework to automate tasks\nInvolved in Setup and benchmark of Hadoop /HBase clusters for internal use.\nCreated and maintained Technical documentation for launching Hadoop Clusters and for executing Pig Scripts.\nWrote SQL queries to perform Data Validation and Data Integrity testing.\nWorked on UML diagrams for the project use case.\nCreated both internal and external tables in Hive and developed Pig scripts to preprocess the data for analysis.\nMonitoring Hadoop scripts which take the input from HDFS and load the data into Hive.\nDesigned appropriate partitioning/bucketing schema to allow faster data retrieval during analysis using HIVE.\nWorked on various file formats like AVRO, ORC, Text, CSV, Parquet using Snappy compression.\nCreated PIG scripts to process raw structured data and developed Hive tables on top of it.', u'DATA ENGINEER\nBrite Systems, Inc - Indianapolis, IN\nJanuary 2012 to May 2013\nInvolved in creating Hive tables, loading with data and writing Hive Queries, which will internally run a Map Reduce job.\nImplemented Partitioning, Dynamic Partitions and Buckets in Hive for optimized data retrieval.\nConnected various data centers and transferred data between them using Sqoop and various ETL tools.\nExtracted the data from RDBMS (Oracle, MySQL) to HDFS using Sqoop.\nUsed the Hive JDBC to verify the data stored in the Hadoop cluster.\nWorked with the client to reduce churn rate, read and translate data from social media websites.\nGenerated and published reports regarding various predictive analysis on user comments. Created reports and documented various retrieval times of them using the ETL tools like QlikView and Pentaho.\nPerformed the performance and tuning at source, Target and Data Stage job levels using Indexes, Hints and\nPartitioning in DB2, ORACLE and DataStage.\nPerformance tuning in the live systems for ETL/ELT jobs.\nWrote database objects like Stored Procedures, Triggers for Oracle, MS SQL\nGood knowledge in PL/SQL, hands on experience in writing medium level SQL queries\nGood knowledge in Impala, Spark/Scala, Shark, Storm.\nExpertise in preparing the test cases, documenting and performing unit testing and Integration.\nIn-depth understanding of Data Structures and Optimization.', u'DATA SYSTEMS SPECIALIST\nIngalls Memorial Hospital - Harvey, IL\nJanuary 2011 to January 2012\nPerformed data entry functions while creating daily reports on treatment programs through in-house engineered records system.\nAccountable for the timely, efficient upgrades of patient health information databases for the behavioral health services and support services to end user terminals and physician practices requiring strong attention to detail, organizational skill, and the ability to maintain tight delivery schedules.\nAccountable for data updates into health information database for HIPPA compliance by logging treatment interventions using secure patient health information management access terminal.', u'DATABASE SPECIALIST\nWSW Health Partners, Inc - Olympia Fields, IL\nJanuary 2010 to January 2011\nResponsible for the day-to-day database and management operations of a psycho- therapy terminal databases for client access service delivery and customer account management with an average of $400,000-$600, 000 in annual revenue;\nAccountable for data updates into patient health information database for HIPPA regulatory compliance by logging various aspects of client treatment interventions programs using secure health information management access terminal;\nManaged the in-house operational database for electronic health records;\nPerformed other mission critical business needs functions as assigned.', u'Services Coordinator\nThe University of Yaound\xe9 I - Yaound\xe9, Cameroon\nJanuary 2000 to December 2009\nMigration of systems data from different databases and platforms to MS-SQL databases at a leading regional research institution;\nProvided exceptional levels of support and customer service to all University guests, visiting researchers and exchange faculty / students within the Divisions of Research and Cooperation;\nResponsible for updating inter-University research databases and implementing regulatory compliance mandates through the coordination of research accreditations;\nResearch database administration including installation, configuration, upgrades, capacity planning, performance tuning, backup and recovery in a high-transaction and fast pace environment.']",[u'Bachelor of Arts in Arts in Communication'],[u'The University of Yaound\xe9 I Yaound\xe9\nSeptember 1994 to October 1999']
0,https://resumes.indeed.com/resume/368efcac85c03de8,"[u""DevOps Engineer\nFrugalops.com - Fairfax, VA\nNovember 2016 to Present\nFairfax, VA November 2016 - Present\nDevOps Engineering\nSupported enterprise client AWS infrastructure. Worked as team and individual contributor to various automation\nprojects including CloudFormation, CodePipeline, Lambda etc. I was able to optimize client's cloud infrastructure\nmore efficient, less costly and more scalable.\nCI/CD\nImplemented gitlab yml style CI/CD pipeline for continuous integration and continuous deployment in\nDev/testing/staging/production environment with AWS CodePipeline. Lambda, docker, git, Cloudfront and Python\nSDK etc were leveraged in the process.\nApplication Deployment\nDeployed a dynamically scalable worker tier application through CloudFormation with Python Lambda, CloudWatch and EC2 auto scaling group.\nOpsWorks Automation\nUsed knife command, cookbooks, data bags and node objects json files to manage EC2 instances with auto scaling for both AWS OpsWorks Chef Automation and standalone Chef server."", u'Graduate Student in Data Science\nThe George Washington University - Washington, DC\nAugust 2016 to Present\nContainerized Application with Docker\nPut development environment and configuration into code and deployed it to decouple infrastructure requirements from the application environment.\nDeep Learning on Computer Vision\nImplemented Python-based deep learning framework to do pattern cognition for ten objects, with accuracy over 0.92.']",[u'Master of Science in Data Science'],"[u'The George Washington University Washington Washington, DC\nAugust 2016 to Present']"
0,https://resumes.indeed.com/resume/51b9109ab561dd7c,"[u'Research Assistant\nStevens Institute of Technology - Hoboken, NJ\nJune 2016 to August 2016\nClassroom Management App\n\u2022 Created a database using SQL and created a website using Java Script\n\u2022 Extracted data using python\n\u2022 Get data using PIR sensors which is connected to raspberry pi', u'Data Analyst\nMarch 2016 to May 2016\n\u2022 Analyzed all the food logs in a database system\n\u2022 Converted data and loaded into the online web based data systems', u'Junior Embedded Engineer\nElectronics Corporation of India Limited - Hyderabad, ANDHRA PRADESH, IN\nJanuary 2015 to May 2015\n\u2022 Developed a voice controlled Robot, using Android technology with ARM 7 Processors.\n\u2022 Used Keil microvision to code the robot and give instructions\n\u2022 Used Bluetooth technology to set connection with robot and mobile device']","[u'Master of Science in Electrical Engineering', u'in Tanner', u'in Analysis on Yahoo Music Re', u'Bachelor of Science in Electronics and Communication Engineering', u'Diploma in Computer Science']","[u'Stevens Institute of Technology Hoboken, NJ\nDecember 2016', u'Stevens Institute of Technology Hoboken, NJ\nSeptember 2016', u'Jawaharlal Nehru Technological University Hyderabad, ANDHRA PRADESH, IN\nMarch 2016', u'Jawaharlal Nehru Technological University Hyderabad, ANDHRA PRADESH, IN\nMay 2015', u'NIIT Hyderabad, ANDHRA PRADESH, IN\nMay 2015']"
0,https://resumes.indeed.com/resume/62914e3bcf6e80e4,[u'Sr Data Engineer'],[u'Masters in Computer Science'],[u'Texas A&M']
0,https://resumes.indeed.com/resume/aa47c36f5ac4aed1,"[u""Sr. Big data Engineer\nDMI Ventive - Symmes, OH\nMay 2016 to Present\nResponsibilities:\n\u2022 Extensively involved in Design phase and delivered Design documents in Hadoop eco system with HDFS, Hive, Pig, Sqoop and Spark with Scala.\n\u2022 Collected the logs from the physical machines and the Open Stack controller and integrated into HDFS using Kafka.\n\u2022 Involved in the high-level design of the Hadoop architecture for the existing data structure and Business process\n\u2022 Worked with clients to better understand their reporting and dash boarding needs and present solutions using structured Agile project methodology approach.\n\u2022 Worked on analyzing Hadoop cluster and different Big Data Components including Pig, Hive, Spark, HBase, Kafka, Elastic Search, database and SQOOP.\n\u2022 Involved in loading disparate datasets into Hadoop Data Lake, this would be available to the data science team to predict the future.\n\u2022 Developed Data Mapping, Data Governance, Transformation and Cleansing rules for the Master Data Management (MDM).\n\u2022 Implemented Partitioning, Dynamic Partitions and Buckets in HIVE for increasing performance benefit and helping in organizing data in a logical fashion.\n\u2022 Installed Hadoop, Map Reduce, HDFS, and developed multiple Map-Reduce jobs in PIG and Hive for data cleaning and pre-processing.\n\u2022 Experienced in pulling the data from Amazon S3 bucket to Data Lake and built Hive tables on top of it and created data frames in Spark to perform further analysis.\n\u2022 Used cloud computing on the multi-node cluster and deployed Hadoop application on cloud S3 and used Elastic Map Reduce (EMR) to run a MapReduce.\n\u2022 Explored MLlib algorithms in Spark to understand the possible Machine Learning functionalities that can be used for use case.\n\u2022 In preprocessing phase of data extraction, we used Spark to remove all the missing data for transforming of data to create new features.\n\u2022 Worded with commercial distribution of Hadoop including Hortonworks production HDP, Cloudera CDH and AWS (EMR, S3, and EC2).\n\u2022 Developed data pipeline using Flume, Sqoop, Pig and Java MapReduce to ingest customer behavioral data and financial histories into HDFS for analysis.\n\u2022 Involved in loading data from UNIX file system to HDFS using Flume and HDFS API.\n\u2022 Configured Spark streaming to receive real time data from the Kafka and store the stream data to HDFS.\n\u2022 Participated in design reviews, code reviews, unit testing and integration testing.\n\u2022 Developed RDD's/Data Frames in Spark using Scala and Python and applied several transformation logics to load data from Hadoop Data Lake to Cassandra DB.\n\u2022 Exported the analyzed data to the NoSQL Database using HBase for visualization and to generate reports for the Business Intelligence team using SAS.\n\u2022 Created Hive tables as per requirement as internal or external tables, intended for efficiency.\n\u2022 Implemented installation and configuration of multi-node cluster on the cloud using Amazon Web Services (AWS) on EC2.\n\u2022 Created and maintained Technical documentation for launching Hadoop Clusters and for executing Hive queries and Pig Scripts\n\u2022 Worked with Elastic MapReduce (EMR) and setting up environments on Amazon AWS EC2 instances.\n\u2022 Used JIRA for bug tracking and GIT for version control.\nEnvironment: Hadoop 3.0, HDFS, hive 2.3, Pig, Sqoop, Spark 2.2, Scala, Hbase 1.2, Kafka, Elastic Search, MapReduce, MLlib, Flume 1.8, Python, AWS, Web Services, GIT, JIRA, MDM"", u""Sr. Hadoop/Data Engineer\nKellogg's - Battle Creek, MI\nJanuary 2015 to April 2016\nResponsibilities:\n\u2022 Worked with systems engineering team to plan and deploy new Hadoop environments and expand existing Hadoop clusters with agile methodology.\n\u2022 Worked on evaluation and analysis of Hadoop cluster and different big data analytic tools like Hbase and Sqoop.\n\u2022 Developed MapReduce programs to perform data filtering for unstructured data.\n\u2022 Worked on analyzing Hadoop cluster and different big data analytic tools including Pig, Hive and Impala.\n\u2022 Successfully loaded files to hive and HDFS from MongoDB, Cassandra and Hbase.\n\u2022 Worked on Classic and Yarn distributions of Hadoop like the Apache Hadoop, ClouderaCDH4 and CDH5.\n\u2022 Created and altered HBase tables on top of data residing in Data Lake.\n\u2022 Worked on analyzing, writing Hadoop MapReduce jobs using Java API, Pig and Hive.\n\u2022 Created and manage S3 buckets and policies for storage and backup purposes.\n\u2022 Worked on developing ETL processes to load data from multiple data sources to HDFS using Flume and Sqoop.\n\u2022 Performed structural modifications using MapReduce, Hive and analyze data using visualization/reporting tools.\n\u2022 Worked in the cluster disaster recovery plan for the Hadoop cluster by implementing the cluster data backup in Amazon S3 buckets.\n\u2022 Installed and configure Zookeeper service for coordinating configuration-related information of all the nodes in the cluster to manage it efficiently.\n\u2022 Involved in converting Cassandra/Hive/SQL queries into Spark transformations using Spark RDD's in Scala and Python.\n\u2022 Used SQL queries, Stored Procedures, User Defined Functions (UDF), Database Triggers using tools like SQL Profiler and Database Tuning Advisor (DTA).\n\u2022 Worked with multiple teams and understanding their business requirements for understanding data in the source files.\n\u2022 Created end to end Spark applications using Scala to perform various data cleansing, validation, transformation and summarization activities according to the requirement.\n\u2022 Explored with the Spark for improving the performance and optimization of the existing algorithms in Hadoop using Spark Context, Spark-SQL, Data Frame, Pair RDD's, YARN.\n\u2022 Worked collaboratively to manage build outs of large data clusters and real time streaming with Spark.\nEnvironment: Hadoop 3.0, Hbase 1.2, Sqoop, MapReduce, Pig, Hive 2.3, Impala, HDFS, MongoDB 3.6, Cassandra, Pig, Zookeeper, SQL queries, Spark, Scala, Python, YARN"", u""Sr. Data Architect/Data Modeler\nBed Bath & Beyond - Union, NJ\nApril 2012 to December 2014\nResponsibilities:\n\u2022 Designed the Logical Data Model using ER Studio with the entities and attributes for each subject areas.\n\u2022 Worked with BTEQ to submit SQL statements, import and export data, and generate reports in Teradata.\n\u2022 Defined and deployed monitoring, metrics, and logging systems on AWS.\n\u2022 Responsible for technical Data governance, enterprise wide Data modeling and Database design.\n\u2022 Implemented data warehouse designs to collect and extract/transform/loading of legacy data to core SAP system.\n\u2022 Designed the data marts using the Ralph Kimball's Dimensional Data Mart modeling methodology using ER Studio.\n\u2022 Incorporated business requirements in quality conceptual, logical data models using ER Studio and created physical data models using forward engineering techniques to generate DDL scripts.\n\u2022 Advised on and enforces data governance to improve the quality/integrity of data and oversight on the collection and management of operational data.\n\u2022 Implemented Dimensional Modeling using Star and Snow Flake Schema, Identifying Facts and Dimensions, Physical and logical data modeling using ER Studio\n\u2022 Responsible for Big data initiatives and engagement including analysis, brainstorming, POC, and architecture.\n\u2022 Demonstrated expertise utilizing ETL tools, including SQL Server Integration Services (SSIS), Data Transformation Services (DTS), and Data Stage and ETL package design, and RDBMS systems like SQL Servers, Oracle, and DB2.\n\u2022 Review and Patch of Netezza and Oracle environments including DB2, OS and Server firmware.\n\u2022 Designed and Deployed high-performance, custom applications at scale on Hadoop /Spark.\n\u2022 Selected the appropriate AWS service based on data, compute, database, or security requirements.\n\u2022 Used Flume extensively in gathering and moving log data files from Application Servers to a central location in Hadoop Distributed File System (HDFS) for data science.\n\u2022 Extracting Mega Data from Amazon Redshift, AWS, and Elastic Search engine using SQL Queries to create reports\n\u2022 Involved in architecting Hadoop clusters Translation of functional and technical requirements into detailed architecture and design.\n\u2022 Used Oozie workflow engine to manage interdependent Hadoop jobs and to automate several types of Hadoop jobs such as Java MapReduce Hive, Pig, and Sqoop.\n\u2022 Performed Data Analysis, Data Migration and data profiling using complex SQL on various sources systems including Oracle and Netezza.\n\u2022 Designed ER diagrams (Physical and Logical using ER Studio) and mapping the data into database objects.\nEnvironment: ER Studio, BTEQ, SQL, Teradata, AWS, Oracle, RDBMS, Netezza, Hadoop, Spark, HDFS, Flume, Amazon Redshift, Elastic Search, Oozie, MapReduce, Hive, Pig, Sqoop"", u""Sr. Data Analyst/Data Modeler\nComcast - West Chester, PA\nSeptember 2010 to March 2012\nResponsibilities:\n\u2022 Analyze the OLTP Source Systems and Operational Data Store and research the tables/entities required for the project.\n\u2022 Designing the measures, dimensions and facts matrix document for the ease while designing.\n\u2022 Created data flowcharts and attribute mapping documents, analyzed the source meaning to retain and provide proper business names following the very stringent FTB's data standards.\n\u2022 Developed several scripts to gather all the required data from different databases to build the LAR file monthly.\n\u2022 Designed ER diagrams, logical model and physical database for Oracle and Teradata as per business requirements using Erwin.\n\u2022 Developed numerous reports to capture the transactional data for the business analysis.\n\u2022 Developed complex SQL queries to bring data together from various systems.\n\u2022 Responsible for technical data governance, enterprise wide data modeling and database design.\n\u2022 Organized and conducted cross-functional meetings to ensure linearity of the phase approach.\n\u2022 Collaborated with a team of Business Analysts to ascertain capture of all requirements.\n\u2022 Created multiple reports on the daily transactional data which involves millions of records.\n\u2022 Used Joins like Inner Joins, Outer joins while creating tables from multiple tables.\n\u2022 Created Multi set, temporary, derived and volatile tables in Teradata database.\n\u2022 Implemented Indexes, Collecting Statistics, and Constraints while creating tables.\n\u2022 Utilized ODBC for connectivity to Teradata via MS Excel to retrieve automatically from Teradata Database.\n\u2022 Developed various ad hoc reports based on the requirements\n\u2022 Designed & developed various Ad hoc reports for different teams in Business (Teradata and Oracle SQL, MS access, MS excel)\n\u2022 Developed SQL Queries to fetch complex data from different tables in remote databases using joins, database links and formatted the results into reports and kept logs.\n\u2022 Involved in writing complex SQL queries using correlated sub queries, joins, and recursive queries.\n\u2022 Delivered the artifacts within the time lines and excelled in the quality of deliverables.\n\u2022 Validated the data during UAT testing.\n\u2022 Performing source to target Mapping\n\u2022 Involved in Metadata management, where all the table specifications were listed and implemented the same in Ab Initio metadata hub as per data governance.\n\u2022 Developed Korn Shell scripts to parallel extract and process data from different sources simultaneously to streamline performance and improve execution time in a parallel process for better time, resource management and efficiency.\n\u2022 Used Teradata utilities such as TPT (Teradata Parallel Transporter), FLOAD (Fastload) and MLOAD (Multiload) for handling various tasks.\n\u2022 Developed Logical data model using Erwin and created physical data models using forward engineering.\nEnvironment: Erwin 8.0, Teradata 13, TOAD, Oracle 10g/11g, MS SQL Server 2008, Teradata SQL Assistant, XML Files, Flat files"", u""Data Analyst\nFreshdesk - Chennai, Tamil Nadu\nJune 2008 to August 2010\nResponsibilities:\n\u2022 Analysis of functional and non-functional categorized data elements for Data Migration, data profiling and mapping from source to target data environment. Developed working documents to support findings and assign specific tasks.\n\u2022 Participated in requirements session with IT Business Analysts, SME's and business users to understand and document the business requirements as well as the goals of the project.\n\u2022 Used and supported database applications and tools for extraction, transformation and analysis of raw data\n\u2022 Developed complex T-SQL code such as Stored Procedures, functions, triggers, Indexes, and views for the business application.\n\u2022 Involved in complete SSIS life cycle in creating SSIS packages, building, deploying and executing the packages all environments. (QA, Development and Production)\n\u2022 Created SSIS Packages for migration of data to MS SQL Server database from other databases and source like Flat Files, MS Excel, Sybase, CSV files.\n\u2022 Optimized stored procedures using temp tables and indexing strategies to increase speed and reduce runtime.\n\u2022 Automated processes from MS Access and Excel and rewrote to SQL views and tables.\n\u2022 Developed reports for users in different departments in the organization using SQL Server Reporting Services (SSRS).\n\u2022 Designed report models based on user requirements and used report builder to generate the reports.\n\u2022 Used tools (Excel and SQL) to analyze, query, sort and manipulate data according to defined business rules and procedures.\n\u2022 Performed data mining on data using very complex SQL queries and discovered pattern.\n\u2022 Extensively used MS Access to pull the data from various data bases and integrate the data.\n\u2022 Developed SQL, BTEQ (Teradata) queries for Extracting data from production database and built data structures, reports.\n\u2022 Performed in depth analysis in data & prepared weekly, biweekly, monthly reports by using SQL, SAS, Ms Excel, Ms Access, and UNIX.\nEnvironment: T-SQL, SSIS, MS SQL, MS Excel, MS Access, SQL queries, BTEQ, UNIX""]","[u""Bachelor's""]",[u'']
0,https://resumes.indeed.com/resume/bfb28f17dbfd02d4,"[u'Senior Data Engineer, SE\nShiva Enterprises Ltd\nMay 2015 to May 2016\n\u2022 Developed datasets and maintained the quality assurance, and became best of the quality management.\n\u2022 Commissioning and maintenance of 33 KV, 11KV, 0.416 KV three phase transmissions and distribution lines along with associate control and distribution panels. Familiar with maintenance of power and distribution transformer (ONAN/ONAF) up to 21.6 MVA along with various types of switchgears.\n\u2022 Familiar with three-phase protection system used in 33/11KV substation for general power supply.\n\u2022 Acquainted with traction protection system.\n\n\u2022 Overhead/underground transmission and distribution pertaining to both low tension (LT) and high tension (HT) supply system and its protection systems.\n\u2022 Familiar with testing and commissioning of 132/25KV, 33/11KV, 11/0.416KV substations.\n\u2022 Learnt about design and installation of switched and static capacitor banks in LT as well as HT supply.\n\u2022 Learnt about design, installation and commissioning of pumps and pumping station.\n\u2022 Familiar with lighting and centralized air conditioning systems.\n\u2022 Familiar with operation and maintenance of a Diesel Generator (of 500 KVA rating).\n\u2022 Also, familiar with SCADA system used in Railway traction system.\n\u2022 Had hands-on training on numbers of energy conservation measures for domestic as well as distribution system.\n\u2022 Quality control and various commissioning works.\n\nCareer Fair Student Associate, IIT (Illinois Institute of Technology, Chicago)\n\nI have worked as a career fair student associate at Illinois Institute of Technology, Chicago where my role was assisting the employers and helping the students with their queries throughout the fair.']","[u'Master of Science in Information Technology and Management', u'Bachelor of Technology in Electrical & Electronics Engineering']","[u'Illinois Institute of Technology\nDecember 2018', u'Chattisgarh Swamy Vivekanand Technical University Bilaspur, Chhattisgarh\nMay 2015']"
0,https://resumes.indeed.com/resume/a43b4164f5dba4aa,"[u""Data Management Engineer\nAcxiom Inc - San Mateo, CA\nFebruary 2001 to May 2003\nResponsible for scoping, analysis, design, and execution of high-quality data solutions as part of the client's interaction with Digital Impact\n\u2022 Participated in the production and delivery of mass email marketing campaigns to Fortune 500 clients\n\u2022 Managing our clients' subscriber databases, including creating and updating their lists and segmenting data for targeted email marketing campaigns\n\u2022 Monitor the campaign post-deployment to validate completion\n\u2022 Troubleshot any problems that occurred with client data\n\u2022 Worked directly with account managers to improve system and to ensure client needs were met\n\u2022 Implemented solutions involving customer data (append, analysis, integration).\n\u2022 Worked closely with product managers and customers in defining roles and systems to optimize performance with products and services."", u""Production Engineer\nEtracks Inc - San Mateo, CA\nJanuary 2000 to February 2001\nManage client's data, analyze and import data into the database using tools like TOAD, database queries and SQL Loader.\n\u2022 Manage, create and deploy email marketing campaigns""]",[u'Bachelor of Engineering in Electronics and Telecommunication Engineering'],[u'Bombay University\nJanuary 1998']
0,https://resumes.indeed.com/resume/d8a286eab2f86cdb,"[u'Google\nJanuary 2016 to Present\nOpen Source/Kaggle/Project Work | January 2016 - Present\nAutonomous Vehicle Navigation with Q-Learning\nApplied reinforcement learning to build a simulated vehicle navigation agent in Python using custom\nQ-Learning algorithm. Modeled a complex control problem in terms of limited available inputs and designed a scheme to automatically learn an optimal driving strategy.\nFacebook Location Detection\nBuilt predictive models using anonymized Facebook check-in marker data to determine mobile user\ncheck-in locations.\nText Sentiment Analysis with Deep Learning\nAnalyzed text of written reviews from IMDB and Amazon using NLP and deep learning (Google\nWord2vec algorithm)', u'Data Engineer\nHouston GMAT - Houston, TX\nJanuary 2014 to Present\nBuild predictive models using scientific Python stack (Python, pandas, sklearn) on large datasets\ngenerated by internal testing software\n\u25cf Generate aptitude/affinity groups using conditional probability/overlap matrix models/k-means\nclustering/deep learning for predictive modeling\n\u25cf 60-70% of students experience 50-100 point increase in scores within 1-2 months', u'Content Specialist\nEdge Marketing and Procurement - Houston, TX\nAugust 2013 to December 2013\nCrawled/extracted data with regex/Java to create centralized source of rich product content and exported to CSV/processed for in-house SQL server\n\u25cf Automated content generation processes/large-scale tasks with ahk scripts\n\u25cf Rebuilt content storage sections of company website using HTML, CSS, JavaScript/repaired and edited\nPHP scripts']","[u""Bachelor's Degree in Computer Science and Engineering""]","[u'University of California Los Angeles, CA\nJanuary 2008']"
0,https://resumes.indeed.com/resume/97d75772af016b52,"[u'SQL DBA and Data Analyst\nKRPR Group, Inc - Omaha, NE\nMay 2017 to Present\n\u2022 Manage ongoing Security maintenance access\n\u2022 Responsible for creating Tables and Indexes\n\u2022 Wrote SQL queries and used SQL Developer tool to query the data\n\u2022 Created and maintain existing SSIS packages that loads data from data sources like Oracle, SQL Server Instances and Teradata\n\u2022 Written several SQL queries using Access database connecting thru Oracle DSNs\n\u2022 Create Linked Servers and maintain\n\u2022 Conducted weekly status review on each task with Developers\n\u2022 Responsible for extracting Data from Oracle and store data into SQL Server databases\n\u2022 Upload flat files data from flat files and excel sheet into SQL Server tables for processing monthly payments for Vendors', u""Data Analyst\nTek Systems - Client Lincoln Financial Group\nDecember 2016 to May 2017\n\u2022 Responsible for extracting data from Oracle databases for Business partner's review\n\u2022 Conducted Data evaluation and Validation\n\u2022 Wrote SQL queries and used SQL Developer tool to query the data\n\u2022 Extracted several data extracts from Oracle into a spread sheet for Business partners review\n\u2022 Written several SQL queries using Access database connecting thru Oracle DSNs\n\u2022 Conducted weekly status review on each task\n\u2022 Weekly one on one meeting with SCRUM master to make sure deliverables were met on time"", u'Data Analyst\nJanuary 2016 to November 2016\nMobile Digital Payment Project\nCRI - Mutual of Omaha\n\n\u2022 I led an effort in gathering requirements from business partners and developed processes and workflow concepts\n\u2022 Conducted weekly meetings, on a regular basis, with product owners\n\u2022 Organized meeting with Business users to prepare and update Business process requirements and system requirements\n\u2022 Documented business requirements for each business processes\n\u2022 Created User stories and subtasks in JIRA and conducted daily reviews\n\u2022 Performed extensive research on existing data and presented to team\n\u2022 Worked on deliverables and SOW to present to Steering committee', u""Data Analyst\nUnion Pacific RailRoad - Omaha, NE\nMarch 2011 to September 2015\n68102\n\nI led a data migration project by coordinating the tasks/sub-tasks that was involved within a Team and across the Team members. Conducted test cycles and addressed issues surfaced and got them resolved. I have highlighted some specific examples that outline the nature of the job responsibilities I managed as below -\n\n\u2022 Analyzed business processes to identify functional gaps and points of external integration\n\u2022 Mapped business processes to SAP functionalities\n\u2022 Prepared description and flow-charts of business processes\n\u2022 Provided advice to business groups to analyze, design and deliver SAP functionalities\n\u2022 Developed technical and functional specifications\n\u2022 Provided the technical studies for data conversion to SAP system\n\u2022 Documented support processes to deliver best results training to end users\n\u2022 Provided system production support and functional design for project enhancements\n\u2022 Recognized and resolved common SAP HR system process problems\n\u2022 Analyzed and researched the legacy systems for SAP implementation\n\u2022 Gathered business requirements, designed business processes diagrams\n\u2022 Analyzed the business processes and mapped to SAP functionality\n\u2022 Organized data migration, testing processes and provided training\n\u2022 Provided resolutions to optimize SAP system service delivery, security and authorization\n\u2022 Created test scenarios and conducted test cycles\n\nDatabase Administrator\nMaintain critical Oracle database production applications. Managed and monitored the data flow from several data sources to targets. Ensure data being transmitted to several downstream systems by monitoring (24x7) the critical applications.\n\n\u2022 New Development - Designed database entities/attributes using Erwin Models\n\u2022 Ongoing maintenance - Resolved 128 incidents related to data discrepancies\n\u2022 Data Transformation - Imported GE data into UP systems using SQL Server DTS packages\n\u2022 Resolved data discrepancies - Written several SQL statements that addressed several discrepancies found between list of mileage with total travel mile, new mileage and latest mileage\n\u2022 Presentation - Presented 'Award recognition software' to higher management"", u'System Consultant\nUNMC - Omaha, NE\nMarch 2009 to February 2011\n68198\n\nI was responsible for small/medium critical development and production applications. Managed and monitored development cycle (dev, test and production). Ensured and maintained data model using Erwin between environments by monitoring (24x7) the critical applications using database tools.\n\n\u2022 Designed and Developed several DTS and SSIS (ETL) packages to extract data from Oracle and SQL Server databases\n\u2022 Created several SSRS reports and cold fusion reports\n\u2022 Created dashboard that refences other reports\n\u2022 Provide database design, administration and support\n\u2022 Provide support for multiple production, development, and test environments\n\u2022 Written batch and modified scripts and automated using scheduler', u'Project Engineer\nTexas Instruments - Dallas, TX\nMarch 2008 to February 2009\nMicrosoft SQL Server:\n\u2022 Experience in Microsoft SQL Server 2005\n\u2022 SQL using SQL Server\n\u2022 Good understanding and Querying Relational Data Using Microsoft SQL Server\n\u2022 Worked with Data Using SQL Server 2005\n\u2022 Written Queries Using Microsoft SQL Server 2005 (Transact-SQL)\n\u2022 Maintained Microsoft SQL Server 2005']",[u'Master of Computer Science in Computer Science'],"[u'Bellevue University Omaha, NE\nJanuary 2013']"
0,https://resumes.indeed.com/resume/3601202db611d561,"[u'Data Entry/Warehouse Processor\nCheney Bros. Inc\nJanuary 1998 to Present\n\u2022 Performed data entry look-ups\n\u2022 Printed receiving labels and license plate data\n\u2022 Finding/scanning barcodes on products & inputting quantities into system\n\u2022 Verified information accuracy\n\u2022 Prepared items for repack department', u'Project Engineer Assistant\nHathaway/Dinwiddie Construction Co\nJanuary 1990 to January 1997\n* Coordinated shop drawings\n* Collated shop drawings and forwarding to subcontractors and field super intendents\n* Drafted RFIs to architects\n* Processed docs with submittal numbers']","[u'', u'in General Studies']","[u'Rio Hondo College of Carpentry Whittier, CA\nJune 1994', u'Nathaniel Narbonne High School Harbor City, CA\nJune 1984']"
0,https://resumes.indeed.com/resume/de96257d5f523bc6,"[u'Infrastructure Engineer\nAsystec - Atlanta, GA\nJanuary 2017 to Present\n* Support systems integration testing, component integration testing and user acceptance testing\n* Proficiency with network hardware and technologies\n* Developed, modify and improve client infrastructure solutions\n* Knowledge of infrastructure solutions (especially Microsoft), cloud technologies, networking, data center operations, platform migration and enterprise directories\n* Helped define and establish IT operations, processes, procedures and toolsets to manage and maintain client solutions\n* Strong analytical and troubleshooting skills to diagnose application/platform issues and get to the root cause of problem\n* Communicated clearly with boundary partners Enterprise solution design, OS administration, network engineering teams\n* Maintained existing servers in the cloud/custom Data center.\n* Working independently with little or no supervision- Capacity Planning and Implementation\n\n* Established IT operations, processes, procedures and toolsets to manage and maintain client solution', u'Data Center Engineer\nQTS, Inc - Atlanta, GA\nJanuary 2016 to January 2017\n* Performed maintenance/engineering tasks as required.\n* Adhere to Standard Operating Procedures and safety procedure and insure compliance by vendors, customers, and other parties\n* Coordinated maintenance efforts with outside contractors, vendors, customers and other parties as may be necessary concerning general building systems\n* Maintained positive communication with customers, owners, property management, contractors and vendors\n* Responded to emergency situations (fire protection, evacuation and equipment failures etc.) to include customer concerns in a timely manner to eliminate and/or minimize possible downtime and inconvenience\n* Tracked inventory of general building parts, materials and supplies to insure adequate stock is maintained always\n* Ensured upgrades, retrofits, build-outs, maintenance and operations\n* Purchased parts and supplies as necessary per the purchase approval process\n* Assist with monitoring and logging equipment as required.\n* project management\n* familiar with service now throughout the Company', u'Customer Support Engineer\nDELL/EMC - Atlanta, GA\nJanuary 2014 to January 2016\n* Lifted, Move/lift equipment (e.g., lift kit) weighing up to 25 lbs. to a height of up to 5 feet to Move/lift equipment (e.g., batteries to be placed onto the lift kit) weighing up to 65 lbs. to a height of up to 2 feet\n* Configured and support the process for wireless and VPN connectivity, network addressing, infrastructure design, session management, TCP/IP Networking knowledge\n* Tracked, monitor, and follow up on inquiries or requests to ensure client satisfaction\n* Unix / Linux or Windows experience / knowledge\n* Maintained account notebooks. Responsible for the maintenance of assigned inventory and test equipment\n* Supported and maintain SAN FC, FCoE and iSCSI Storage arrays and storage related hardware and software issues in a managed services environment and participation in on-call rotation\n* Understand of SAN architecture and best practices\n* Problem analysis and resolution of storage support issues working with Help Desk Ticket System\n* Ensured IT solutions meet requirements for security, availability, capacity, resiliency, and performance in a way that is efficient and supportable, reducing overall support costs.\n* Understand industry leading solutions and trends for assigned technologies and applying those as appropriate for the client', u'Operations Assembler\nAdecco Engineering - Atlanta, GA\nJanuary 2013 to January 2014\n* Replaced Network switches, Rack top switches, Line cards, entry-level fiber cabling training\n* Hardware maintenance on machines in data center servers\n* Assembled / disassemble and populate / depopulate equipment racks\n* Disassembled broken or failed hardware equipment and replace parts per defined process using hand and / or power tools\n* Run / install Ethernet and fiber optic cable per defined process\n* Linux operations, Ran commands to checks links, machine status\n* Detailed instructions and perform other tasks as directed\n* Tracked, monitored, and followed up on inquiries or requests to ensure client satisfaction']",[u''],"[u'Westwood College Atlanta, GA\nJanuary 2013']"
0,https://resumes.indeed.com/resume/87d16b82164093c4,"[u""Supplier Quality Engineer\nCISCO SYSTEM, INC - San Jose, CA\nSeptember 2015 to Present\nSupply Chain Organization\n\u2022 Developing a reliability prediction model in R using survival analysis and machine learning technic to measure field return component, as a result of early sensing problematic suppliers\n\u2022 Generating a comprehensive methodology to assess/rank supplier performance\n\u2022 Performing data analysis on concerning manufacturing failure rate, supplier yield, customer field return data and other defect parameters utilizing statistical techniques to identify suppliers' potential areas of improvement\n\u2022 Monitoring supplier's post process change and validate post quality event activities"", u'Data Scientist Associate\nCISCO SYSTEM, INC\nFebruary 2017 to August 2017\ndata science training program\n\u2022 Completed 3 training course including: machine learning, machine learning application, and statics analysis\n\u2022 Hand-on experience on classification, Regression, Clustering, and Text Mining\n\u2022 Built a recommender system using collaborative filtering, to provide customized set of products for each customer', u'Component Engineer\nCISCO SYSTEM, INC - San Jose, CA\nFebruary 2013 to August 2015\nSupply Chain Organization\n\u2022 Conducted Product Life Cycle Management (PLM), commodity development strategy and roadmap, to optimize component sourcing /development process\n\u2022 Managed the component qualification as well as Process Change Notice (PCN) procedure for responsible components, ensure products maintain high quality\n\u2022 Supported component quality, maintenance, as well as failure analysis containment and supply continuity activity\n\u2022 Providing technical leadership and support for component related issues internal to Cisco (Business Units, Design Engineering, Product Operations and Manufacturing Operations) and for external partners Suppliers in a cross-functional environment\n\u2022 Engaged proactively with suppliers to influence component designs for improved manufacturability, reliability, quality, and, testability']","[u'Associate in Data Scientist', u'Master of Engineering in Electrical Engineering in Electrical Engineering', u'Bachelor of Science in Electrical Information Science and Technology in Electrical Information Science and Technology']","[u'NORTH CAROLINA STATE UNIVERSITY\nFebruary 2017 to August 2017', u'ARIZONA STATE UNIVERSITY Tempe\nJanuary 2011 to December 2012', u'XIAMEN UNIVERSITY Xiamen, CN\nAugust 2004 to May 2008']"
0,https://resumes.indeed.com/resume/eab2f3ac2a288b53,"[u""Data Scientist and Algorithm Developer\nWayne State University - Detroit, MI\nJanuary 2016 to Present\nIn this position, I am serving as Data Analytics Expert and Predictive Modeling Researcher to design, develop, and implement data analytics algorithms for Virtually Guided Resistance Spot Welding Project (VRWP). The goal of VRWP is generating data-driven solutions for weldment design in the automotive industry. This project is a joint project (Wayne State University, Ford Motor Company and Digital Manufacturing and Design Innovation Institute (DMDII))\nMajor Responsibility: Design, development, and/or implementation of data mining and machine learning algorithms\n\nSample works developed for VRWP\n\u2022 Constructed and tuned the parameters of predictive models.\n. Artificial Neural Net (ANN)\n. Deep Neural Net (DNN)\n. Decision Tree (RF, M5, CART)\n. Lazy models K Nearest Neighbor (KNN) and KStar\n. Statistical regression models\n. Support Vector Machines (epsilon, nu, SMO)\n\u2022 Designed and implemented Progressive Sample Size Planning (PSSP) algorithm to reduce the cost of process\n\u2022 Implemented feature engineering algorithms\n\u2022 Designed and implemented Robustness Assessment Algorithm (RAA) based on Bootstrapping techniques\n\u2022 Devised accuracy and precision assessment methodology for predictive algorithms based on statistical hypothesis testing techniques (t and Levene's tests)\n\u2022 Designed and implemented Simulated Weld Lobe Generation Algorithm (SWLGA) based on Monte Carlo simulation technique\n\u2022 Implemented Signal to Noise (S/N) data quality analysis methodology based on Taguchi experiments\n\u2022 Integrated Ford Motor Company engineering and tolerance specifications to measure the quality of data\n\u2022 Conceptualized and devised a novel algorithm performance assessment measure called Mean Tolerance-base Error\n\u2022 Standardizing coding system for the predictive algorithms"", u'Predictive Maintenance Engineer & Data Analyst\nTabriz Oil Refining Company\nJanuary 2009 to January 2013\nPrimary role: predictive maintenance of capital-intensive physical assets, risk and failure assessment and prevention, intelligent asset maintenance management, Computerized Maintenance Management Systems\n(CMMS) expert, planning and scheduling preventive and predictive maintenance programs, organizational\nrealignment and process reengineering, deploying data-driven smart maintenance management systems.\nSecondary role: energy management, energy data analysis and consumption analysis and prediction, design and deployment of energy management soft systems.']","[u'PhD in Industrial and Systems Engineering', u'MS in Computer Science in Computer Science', u'MS in Mechanical Engineering']","[u'Wayne State University Detroit, MI\nMay 2018', u'Wayne State University Detroit, MI\nMarch 2018', u'Amirkabir University of Technology Tehran, IR\nJanuary 2006']"
0,https://resumes.indeed.com/resume/dab69eeedd83cf76,"[u'Data Warehouse Engineer Senior\nCareFirst BlueCross BlueShield - Washington, DC\nOctober 2016 to Present\n\u2022 Helped design, develop, test, and deploy Informatica PowerCenter workflows to populate a new relational Oracle data warehouse. Designed an automated and reusable structure to run each subject area of the database.\n\u2022 Developed testing plans with data governance and testing teams to ensure data integrity and eliminate bugs.\n\u2022 Trained over ten new associates on CareFirst best practices, administration and development techniques, and the company tech stack.', u'Data Warehouse Engineer Specialist\nSeptember 2014 to October 2016\n\u2022 Administered a 100 server SQL Server environment. Completed user tickets for user access, DDL changes, table and database copies, and general environment changes.\n\u2022 Streamlined, tuned, and documented existing production deployment activities for SSRS and SSIS monthly projects.\n\u2022 Led a team of five associates in building and testing a three node SAS Office Analytics Linux environment. Migrated 45 users to the new environment from a single node Windows environment, saving analysts hours of processing time.\n\u2022 Managed infrastructure and storage for 350 business intelligence and data integration environments. Developed Python and PowerShell scripts to monitor all environments.', u'Data Warehouse Engineer Technician\nSeptember 2013 to September 2014\n\u2022 Administered fourteen Informatica PowerCenter and PowerExchange environments. Maintained a 99% environment uptime through patches, server maintenance, and environment optimization.\n\u2022 Created a design and code review process for all Informatica deployments. Ensured adherence to all performance guidelines and business requirements. Assisted in tuning all code that did not meet standards.\n\u2022 Installed and maintained two Informatica Cloud environments. Developed Informatica Cloud ETL integrating Salesforce and Oracle data sources.\n\u2022 Assisted in upgrading all PowerCenter environments from 9.5.4 to 9.6.1.\n\u2022 Oversaw an effort to test and implement AnalytiX DS. Successfully consolidated mapping specification documents from disparate spreadsheets into the tool.\n\u2022 Helped run disaster recovery tasks for the Informatica Administration team.']",[u'Bachelor of Science in Mathematics'],"[u'American University Washington, DC\nSeptember 2009 to May 2013']"
0,https://resumes.indeed.com/resume/2148a87eefcf18f7,"[u""Data Processing Officer\nQual Source Solutions - Surat, Gujarat\nFebruary 2016 to July 2016\nMaintaining back end of German Dating site 'LOVOO'.\nFiltering, censoring data, photos, posts, comments to make that site legal and legitimate for minors, and other user.\nAnalyze user uploaded data and categorized it in delete, warn, Ok, evil contents according to company given guidelines.\nCategarizing user data(Images and other data) in different categories and validating user account, user info, managing reset and forgot password, and other features.\n\nWorking in team, meeting up tight deadlines and providing great quality with efficiency and making sure that it is bug free.\nTools: Chrome Extensions, PHP, MySQL, JS, HTML"", u'Junior Engineer\nSmart Automation - Surat, Gujarat\nDecember 2014 to February 2016\nServed as a valuable team member with 7 other members working on various projects and solutions.\nDeveloped a System that manages whole network of city bus transportation of City Link and BRTS.\n\nBy Analyzing complex technical and business information, and design and implement Database structure and using strong programming and data structure knowledge, we transform requirements into design concepts.\n\nSystem analyze previous database and frequency of travelers it will manage frequency of vehicles and routes.\n\nUsing hand-on experience of JAVA and Database structures and utilize JAVA Swings for GUI development.\nMoreover, worked with other systems like microcontroller and embedded system to build devices like warping controller, axis controller, etc.\n\nTechnologies Used : C, C++, JAVA, HTML5, CSS3, PHP, Java Script, AJAX and MySQL\nIDEs: Keil 4, Atmel Studio', u'Software Engineer\nAppin Technology lab - Surat, Gujarat\nJune 2014 to December 2014\nResponsible for design and development of software for manufacturing industry and design and develop Business Intelligence dashboards to provide better analytics and support.\n\nServed integral role in team of 5 members and manage tight deadlines by providing quality product within time.\n\nUsing detailed oriented approach by Interacting actively with workers on supply chain and manufacturing teams for requirement gathering,\n\nUsing JAVA, JAVAFX, HTML5 and CSS3 foo designing and development of Dashboard applications that work on real time and generates report to improve efficiency and decision making.\n\nAlso use Adobe Illustrator to improve UI.\nProvide support with troubleshoot and debug issues.']","[u""Master's""]",[u'']
0,https://resumes.indeed.com/resume/c74f78033540a230,"[u'Research Assistant\nRecords and Information Management Services, University of Illinois\nAugust 2017 to Present\nFormulated algorithms to perform textual analysis on a large dataset of emails and to obscure Personally\nIdentifiable Information in the dataset.\n\u2022 Exported the manually trained (tagged as public or restricted) items from one tool to another using VLOOKUP\nbased on date and time as the key to match the document id across two tools to save time from manually training the tool and tagging again.\n\u2022 Analyzed and evaluated multiple text analysis tools like Advance eDiscovery, ePADD, Recommind which use\nMachine Learning and predictive analysis for detecting sensitive data.', u'Software Engineer\nNokia Networks - Chennai, Tamil Nadu\nJune 2016 to June 2017\n\u2022 Designed and developed a feature that enhanced the data-type support in message attributes\n\u2022 Developed a distributed state machine spanning across systems - Nokia Access Virtualizer and Access Management\nsystem, to monitor and notify the counterpart on state changes. Ensured the ahead-of-time delivery of this critical\npiece in a major customer requested feature.\n\u2022 Fixed a number of bugs and made enhancements for inclusion in future code releases and patches', u'Data Analyst Intern\nHeroTalkies Entertainment Private Ltd - Chennai, Tamil Nadu\nJuly 2015 to May 2016\n\u2022 Analyzed user behaviour on various platforms and generated reports for product development\n\u2022 Built a dashboard to consolidate analytics data from various sources\n\u2022 Helped detect piracy and password sharing amongst users from analysing user activity, there-by saving the company thousands of dollars in operational expenses.']","[u'Master of Science in Information Management', u'Bachelor of Engineering in Computer Science and Engineering']","[u'University of Illinois at Urbana-Champaign Urbana-Champaign, IL\nAugust 2017 to December 2018', u'Madras Institute of Technology, Anna University\nAugust 2012 to May 2016']"
0,https://resumes.indeed.com/resume/b383c193d7b2146b,"[u""Data Scientist\nNew York Data Science Academy - New York, NY\nJune 2017 to September 2017\nNew York, NY Jun 2017 - Sep 2017\nData Science fellowship involving hands on R, Python, SQL, Spark, Hadoop and Hive\ndevelopment as well as machine learning, big data, visualization, web scraping, statistics and analytics\n\u2022 Collaborated with an ad-tech firm to analyze over ~350GB of household-level data; leveraged Principal\ncomponent analysis to deal with high-dimensional data, K-means clustering to find the relationships in data,\nlocation analyses and supervised learning methods to develop propensity recommendations (e.g., users with specific household behaviors more likely to buy through direct mail channels)\n\u2022 Applied random forest, gradient boosting and linear regression machine learning algorithms combined with feature engineering to minimize the mean absolute error of Zillow's home value in a kaggle competition team\n\u2022 Leveraged scrapy tool to web scrape Hackmageddon.com and conducted text mining analyses using Natural\nLanguage Processing and K-means clustering methods to identify and categorize prevalence, target areas, and what attack they used to drive up overall cost of cyber-attacks worldwide\n\u2022 Analyzed NYC Vehicle Motor Collision data and conducted exploratory data analyses to understand patterns in frequency and location for NYC collisions and pedestrians getting injured due to collisions -- built an interactive\nShiny application to visualize vehicle collisions and most pedestrians injured in real time"", u'Software Engineer Data\nNarola Infotech - Gujarat, IN\nFebruary 2013 to April 2015\n\u2022Migrated the client facing database from Postgres to MongoDB leading to a 90% decrease in query response times\n\u2022Developed SQL code to identify, analyse, and interpret trends in large datasets; retrieve data from SQL to clean up data systems and wrote data migration script to transform existing database\n\u2022Loaded the aggregate data into a relational database for reporting, dash boarding and ad-hoc analyses, which revealed ways to lower operating costs and offset the rising cost of programming\n\u2022Collaborated with project managers, engineering teams, and client representatives to coordinate design and implementation of application; led end-to-end management of project deliverables to ensure on-time delivery\n\u2022Implemented the database infrastructure, database enhancements, database performance tuning and Provide remote support for maintenance of databases and servers\n\u2022Database design, data modelling, data warehousing using SQL, Teradata SQL assistant and ETL tools']","[u'Master of Science in Information', u'Bachelor of Science in Computer Science']","[u'New York Institute of Technology New York, NY\nJanuary 2016 to Present', u'Rajasthan Vidyapeeth\nAugust 2008 to April 2012']"
0,https://resumes.indeed.com/resume/d47c985fdd3f609b,"[u""Data Engineer\nBDNA Corporation - Mountain View, CA\nJanuary 2016 to Present\nDesigned and implemented cloud based Asset Discovery on AWS Infrastructure which enabled better cloud utilization\nmetrics and software licensing decisions\n\u2022 Improved query retrieval time by 90 percent by integrating EC2 instances with S3 and Athena\n\u2022 Designed and developed application data tier using MYSQL NDB Cluster to achieve horizontal scalability\n\u2022 Implemented AWS Role/User only based access to S3, encryption of data at rest in MYSQL, SSE-KMS encryption for S3 bucket and Athena query results to improve security posture of the application\n\u2022 Managed applications in AWS and familiar with SDK's and core services including Python Boto3, EC2, IAM, S3 etc.\n\u2022 Enhanced BDNA Data Platform by designing and developing connectors using SQL queries to extract customers IT data from different discovery data sources (IBM ILMT, SCCM, Service Now, HP UD, Tanium, HP NA, HP NNMI etc.)\n\u2022 Optimized and tunes SQL queries to manage and troubleshoot existing Extract Transform Load (ETL) pipeline"", u""Data Engineer\nPayPal - Boston, MA\nJanuary 2015 to August 2015\nBoston, MA\nSoftware Engineering Co-op Jan 2015 - Aug 2015\n\u2022 Increased performance and scalability by migrating data from Oracle to Cassandra using Spring Data Cassandra\n\u2022 Performed cluster monitoring, tuning and querying by utilizing Datastax OpsCenter and DevCenter tools\n\u2022 Designed and developed Tableau reports with respect to various KPI's such as sales and transactions\n\u2022 Automated and enhanced SoapUI regression tests for major platform changes to facilitate CI in an agile environment\n\u2022 Identified and fixed regression issues using Groovy scripts, Splunk and Jenkins Job""]","[u'Master of Science in Information Systems', u'Bachelor of Engineering in statistics']","[u'Northeastern University Boston, MA\nAugust 2013 to December 2015', u'Rajiv Gandhi Technical University\nJune 2009 to June 2013']"
0,https://resumes.indeed.com/resume/5b5e92cc972ffbad,"[u'Data Analyst\nAggrandize Digital Solutions\nSeptember 2016 to July 2017\nImplemented regression analysis in R, after performing data munging on structured, and semi-structured data; Predicted\nsales trends, and product discount effects to influence business decisions and saved the company around 2 million rupees\n\u2022 Designed user-friendly, self-explanatory dashboards using Tableau for the marketing team to streamline the process of examining marketing, and outsourced data and saved 400-man hours during my tenure\n\u2022 Effectively used funneling techniques in Mixpanel and advised the development team on the redesign of the application', u""System Engineer\nIBM India\nNovember 2014 to August 2016\nImplemented topic modeling statistical model on large volume of semi-structured data of 3 million lines from the system\ncrash log details of AT&T's enterprise customer's billing web application using Java and prevented 7 million system crashes\n\u2022 Redesigned the navigation of the application by building a subdivision in the applet side; Programmed the division's\nfeatures impacting the entire billing system and saved 6000 days of time for the customers over a period of one year\n\u2022 Redesigned and improved the UX through HTML and CSS using best design practices and saved 3000 user days in one year\n\u2022 Designed the workflow for the Business team, which covers six continents and 3.5 million businesses, revamping the water\nflow methodology to agile methodology, thereby reduced the issues raised on the existing tickets by 50%\n\u2022 Developed and delivered a training plan to educate team on the AT&T billing system architecture; Created training guides and devised schedules, expediting the process of seven new staff becoming qualified, in only 12 days vs. one-month target"", u'Associate System Engineer\nIBM India\nNovember 2013 to October 2014\nEmployed Bayesian Networks in IBM System G interface to identify probable root causes for failures in healthcare plans for\nNASCO & BC/BS and saved 520-man hours for the test team, and 2.5 million dollars for NASCO over a period of ten years\n\u2022 Modeled structured data from multiple sources using Python for the system and created interactive visualizations\n\u2022 Analyzed healthcare plans in mainframe env. and collaborated with developers to rectify requirement and functional issues\n\u2022 Launched new processes to facilitate improved information flow between Business Analysts, Developers, and Testers']","[u'Masters in Information Management', u'in Computer Science']","[u'Syracuse University Syracuse, NY\nMay 2019', u'College of Engineering\nMay 2013']"
0,https://resumes.indeed.com/resume/b1485fd229b1ca47,"[u'Product and Sales data\nJanuary 2018 to Present\nBusiness Intelligence for retail data using SAP HANA Jan 2018 - Present\n\u2022 Created database tables in SAP HANA and loaded the tables with Customer, Product and Sales data.\n\u2022 Designed the cube with the dimensional and fact tables.\n\u2022 Visualized the cube and created reports in SAP Lumira.\nBusiness Intelligence for retail data using SAP BW Jan. 2018 - Present\n\u2022 Created SAP infobjects to hold the master data and key figures.\n\u2022 Modelled the multidimensional star schema with facts and dimensional tables.\n\u2022 Loaded the master data into dimensional table and measures into the fact table using ETL process.\n\u2022 Created queries using SAP BeX and created reports using SAP Design Studio.\nDatabase Modelling and Implementation of Hospital database Aug. 2017 - Dec 2017\n\u2022 Modelled entity relationship for hospital database using MS Visio.\n\u2022 Implemented the database design in MS Access and performed various operations on the database.\n\u2022 Created reports out of the data from the database.', u'Systems Engineer\nTATA CONSULTANCY SERVICES - Chennai, Tamil Nadu\nMay 2015 to June 2017\nDesigned and developed middleware applications to connect different enterprise systems using agile\nmethodology for a transformation project from a legacy system.\n\u2022 Reduced the project cycle time by 3 months by developing complex interfaces with fewer defects.\n\u2022 Built a business model, which was used to induct new joiners to the team which saved the onboarding time\nand the customer satisfaction as well.\n\u2022 Supervised 3 entry-level associates and trained them for successful completion of multiple interfaces.\n\u2022 Learned a new technology in 2 weeks and conducted training sessions on the same, which facilitated in the completion of a task 2 weeks prior to the deadline.\n\u2022 Created a report based on the defect metrics using excel and proposed a way to reduce the number of defects.\nIt resulted in 15% increase in the quality of the deliverable.\n\nACADEMIC PROJECT']","[u'M.S. in Information Technology & Management', u'B.Tech. in Electrical and Electronics Engineering']","[u'The University of Texas at Dallas Dallas, TX\nMay 2019', u'SASTRA University\nApril 2015']"
0,https://resumes.indeed.com/resume/25e0fcb3ee6e753b,"[u'Data Analyst\nUniversities Space Research Association (USRA) - Washington, DC\nSeptember 2016 to Present\nUniversities Space Research Association (USRA) is a nonprofit corporation chartered to advance\nspace-related science, technology and engineering. As a Data Analyst, I am responsible for analyzing\n\nVLBI data and automating operations. My accomplishments include: **position requires security clearance\n\u25cf VLBI operations; receive and correlate data from experiments conducted by radio telescopes\naround the world to generate EOP data\n\u25cf Developed scripts in Python to assist in VLBI operations and analysis of data before and after correlation\n\u25cf Graphical analysis of scientific data, presented to business users\n\u25cf Lead multiple projects that involved creating new database and loading new sets of data,\nscrubbing and analyzing the data to present to scientists\n\u25cf Design new tables in conjunction with existing data models and automated loading of those\ntables using various tools', u""Software Engineer (Intern)\nCisco - San Jose, CA\nJune 2015 to August 2015\nDeveloped and implemented Python automation scripts which tested different conference\ncall scenarios on Cisco's various IP phone models\n\u25cf Created a diagnostics dashboard in Python to take scraped results from the web and show\nthe status of the tests\n\u25cf Configured Git repositories to store projects that were being developed\n\u25cf Effectively communicated analytical methods and technologies through visual and oral\npresentation to Directors, End Users and Peers to ensure Client requirements were met""]","[u""Bachelor's in Mathematics and Computer Science""]","[u'University of Illinois at Chicago Chicago, IL\nAugust 2012 to May 2016']"
0,https://resumes.indeed.com/resume/7c07c39df8bb7184,"[u""Big Data Engineer\nEdge Park Medical, OH\nJanuary 2017 to Present\nProject Description: Cardinal at Home (CARES) program is technology transformation for Edge Park medical system ( a subsidiary of Cardinal Health).This program concentrates on Patient onboarding/management (order, insurance check, and referral etc), contract & Pricing maintenance and execution, Order intake and release etc. This program also focusing on functionality as Patient and Payer billing, Provider management and Claim submission, AR Management (collections, returns and refunds, cash receipt and posting) and Medical documentation management modules.\n\nEnvironment: Hadoop, Cloudera, Talend, Python, Spark, HDFS, Hive, Pig, Sqoop, DB2, SQL, Linux, Yarn, NDM, JIRA, Informatica, Windows & Microsoft Office, Tableau.\n\nResponsibilities:\n\u2022 Used Spark API over Cloudera Hadoop YARN to perform analytics on data.\n\u2022 Exploring with the Spark improving the performance and optimization of the existing algorithms in Hadoop using Spark Context, Spark-SQL, Data Frame, Pair RDD's, Spark YARN.\n\u2022 Worked on batch processing of data sources using Apache Spark, Elastic search.\n\u2022 Involved in converting Hive/SQL queries into Spark transformations using Spark RDDs, Scala.\n\u2022 Worked on migrating PIG scripts and MapReduce programs to Spark Data frames API and Spark SQL to improve performance.\n\u2022 Able to assess business rules, collaborate with stakeholders and perform source-to-target data mapping, design and review.\n\u2022 Created scripts for importing data into HDFS/Hive using Sqoop from DB2.\n\u2022 Loading data from different source(database & files) into Hive using Talend tool.\n\u2022 Conducted POC's for ingesting data using Flume.\n\u2022 Used all major ETL transformations to load the tables through Informatica mappings.\n\u2022 Created Hive queries and tables that helped line of business identify trends by applying strategies on historical data before promoting them to production.\n\u2022 Worked on Sequence files, RC files, Map side joins, bucketing, Partitioning for Hive performance enhancement and storage improvement.\n\u2022 Developed Pig scripts to parse the raw data, populate staging tables and store the refined data in partitioned DB2 tables for Business analysis.\n\u2022 Worked on managing and reviewing Hadoop log files. Tested and reported defects in an Agile Methodology perspective.\n\u2022 Conduct/Participate in project team meetings to gather status, discuss issues & action items\n\u2022 Involved in reports development using reporting tools like Tableau. Used excel sheet, flat files, CSV files to generated Tableau adhoc reports.\n\u2022 Provide support for research and resolution of testing issues.\n\u2022 Coordinating with Business for UAT sign off.\nProject 2:"", u'Technical Lead\nVISA Inc\nAugust 2015 to December 2016\nProject Description: This Project is about Risk management system for Visa. Protecting the customers is a top priority. Visa Risk Manager provides with integrated, proprietary tools to dynamically respond to fraud as it occurs. When there is a transaction, the system downloads the alerts and store in the Database. The other systems make a request to know about the status of the alerts. Based on the Account information, the status will be sent. The VRM Transaction Feedback web service is used by clients to pass the feedback to Visa for the case creation alerts it received from previous downloads. Feedback can be either the status update or creating block or exclude on the PAN or combination of both.\n\nEnvironment: Hadoop, HDFS, Pig, Hive, MapReduce, Sqoop, Oozie, Nagios, Ganglia, LINUX, Hue\n\nResponsibilities:\n\u2022 Worked on Hadoop cluster using different big data analytic tools including Pig, Hive, and MapReduce\n\u2022 Collecting and aggregating large amounts of log data using Apache Flume and staging data in HDFS for further analysis\n\u2022 Worked on debugging, performance tuning of Hive & Pig Jobs\n\u2022 Created HBase tables to store various data formats of PII data coming from different portfolios\n\u2022 Implemented test scripts to support test driven development and continuous integration\n\u2022 Worked on tuning the performance Pig queries\n\u2022 Involved in loading data from LINUX file system to HDFS\n\u2022 Importing and exporting data into HDFS and Hive using Sqoop\n\u2022 Experience working on processing unstructured data using Pig and Hive\n\u2022 Supported MapReduce Programs those are running on the cluster\n\u2022 Gained experience in managing and reviewing Hadoop log files\n\u2022 Involved in scheduling Oozie workflow engine to run multiple Hive and pig jobs\n\u2022 Assisted in monitoring Hadoop cluster using tools like Nagios, and Ganglia\n\u2022 Created and maintained Technical documentation for launching Hadoop Clusters and for executing Hive queries and Pig Scripts\nProject 3:', u'Data Analyst\nCITI, India - IN\nApril 2013 to July 2014\nEnvironment: Excel, SSIS, Oracle, MS SharePoint, Erwin, Python\n\nResponsibilities:\n\u2022 Worked as a Data Analyst to generate data models using Erwin data modeler and developed rational database system.\n\u2022 Extracted data from various data sources applied functions, complex formulas and conditions to clean raw data, applied V-lookups and Index-match functions to merge and filter information from various data sets for reporting and statistical gathering purpose.\n\u2022 Involved in all phases of data mining, data collection, data cleaning, developing models, validation and visualization.\n\u2022 Involved with Data Analysis primarily identifying data sets, source data, source meta data, data definition and data formats.\n\u2022 Implemented metadata repository, maintaining data quality, data cleanup procedures, transformations, data standards, data governance program, scripts, stored procedures, triggers and execution of test plans.\n\u2022 Worked along with ETL, BI and DBA teams to analyze and provide solutions to data issues and other challenges during the implementation of the OLAP model.\n\u2022 Performed end to end Informatica ETL testing for custom tables by writing complex SQL queries on the source database and compared them with the results against the target database.\n\u2022 Converted raw data into processed data by merging, finding outliers, errors, trends, missing values and distributions in the data.\n\u2022 Created and designed reports that will use gathered metrics to infer and draw conclusions of past and the future behavior.\n\u2022 Documented the complete process flow to describe program development, logic, testing and implementation application integration and coding.\nProject 4:', u'Data Analyst\nCITI, India - IN\nMay 2012 to March 2013\nEnvironment: SQL, Excel, SSIS, MS-SQL Databases, MS SharePoint 2010\n\nResponsibilities:\n\u2022 Reviewed, evaluated, designed, implemented and maintained reporting to support current business initiatives.\n\u2022 Successfully completed training on in house databases, extracted data from various data sources, applied Excel functions to transform raw data in business information required for reporting and data analysis purposes.\n\u2022 Created SQL code to identify, analyze and interpret trends in large datasets and also retrieve data from SQL to clean up data systems.\n\u2022 Created complex formulas and conditions to clean raw data, applied V-lookups and Index-match functions to merge and filter information from various data sets for reporting and statistical gathering purposes.\n\u2022 Converted raw data to processed data by merging, finding outliners, errors, trends, missing values and distributions in the data.\n\u2022 Worked closely with the ETL, SSIS, SSRS Developers to explain the complex data transformation using Logic.\n\u2022 Lead data discovery, handling structured and unstructured data, cleaning and performing descriptive analyses, and storing as normalized tables for dashboard.']","[u""Master's of Science in Computer Science in Computer Science"", u'in Information Technology']","[u'New York Institute of Technology New York, NY', u'Anna University']"
0,https://resumes.indeed.com/resume/ba7f12344e1d2483,"[u""Medical Billing Specialist\nCastle Medical India Pvt. Ltd\nApril 2016 to April 2017\nTechno Park, Trivandrum, Kerala, India\nThe multinational company offers reliable and timely drug monitoring services.\n\u2022 Demographics Entry\n\u2022 Charge Entry\n\u2022 Submission of primary and secondary insurance claims\n\u2022 Payment Posting\n\u2022 Preparation of Various Reports\n\u2022 AR Follow up/ AR Analysis\n\u2022 Verifies patient's insurance eligibility"", u'Data Analyst\nCastle Medical India Pvt. Ltd\nJuly 2013 to March 2016\nTechno Park, Trivandrum, Kerala, India\nThe multinational company offers reliable and timely drug monitoring services.\n\u2022 Analysis of clinical samples on LC-MS/MS (confirmation) and AU640 Immunoassay Screening for more than 150+different pain medications and drug of abuse\n\u2022 Training co workers and new hires on lab procedures and data analysis, particularly on the software associated with the High Performance Liquid Chromatography /mass spectrometer.\n\u2022 Monitoring the instruments remotely and make sure that chromatograms are done without any interruption.', u'Software Engineer\nKameda Infologics Pvt. Ltd\nDecember 2010 to June 2013\nTechno Park, Trivandrum, Kerala, India\nKameda Infologics Pvt Ltd is a dominant force in the healthcare industry since 2000 with a vision to develop world-class software products specializing in Healthcare Management.\n\u2022 Analyzing new requirements and providing better solution for the same\n\u2022 Completely Involved in end to end phase of the project\n\u2022 Design and develop business user controls\n\u2022 Design and code reports using crystal report\n\u2022 Managing oracle database objects which includes Stored Procedures, Functions, Views and Triggers\n\u2022 Preparing use case and design documents\n\u2022 Create the interactive reports using WPF Flow Document\n\u2022 Technology: WPF, C#.Net, Oracle 10g, VS 2008\n\u2022 Platform: .Net Framework 3.5']","[u'Master of Science in Computer Applications', u'Bachelor of Science in Computer Science']","[u'School Of Technology And Applied Sciences, M G University Regional Center Pathanamthitta, Kerala\nSeptember 2007', u'KVVS College Of Science and Technology Adoor, Kerala, IN\nMay 2004']"
0,https://resumes.indeed.com/resume/126dd75ac52ae7c8,"[u'University of Wyoming - Laramie, WY\nAugust 2015 to Present\n08/2015 - Present Wyoming, U.S.A.\nAchievements/Tasks Courses\nHandling various emergency situations on campus past normal Basic Drilling Engineering Reservoir Mechanics\noffice hours. Petroleum Production Systems Well Log Interpretation\nEnsuring safety on campus. Petroleum Economics Multi-component\nThermodynamics\nWell Bore Operations', u'Intern - Engineer\nGeologic Hazards\nDecember 2017 to January 2018\nAchievements/Tasks\nIndia', u'Data Assistant\nUniversity of Wyoming - Laramie, WY\nMay 2016 to August 2016\nMinor in Geology\nAchievements/Tasks University of Wyoming\nGathering data from people all over the US for various assigned 08/2017 - Present Wyoming, U.S.A.\nprojects and putting it in the system. Courses\nTelephone Interviewing. Physical Geology Petroleum Geology']",[u'in Petroleum Engineering General'],[u'University of Wyoming College of Engineering and Applied Sciences\nAugust 2016 to August 2017']
0,https://resumes.indeed.com/resume/1f79ca99cbbbfdaa,"[u'Sr. Network Engineer\nT-Mobile - Parsippany, NJ\nAugust 2016 to Present\nResponsibilities:\n* Responsible for configuration, maintenance and troubleshooting routing protocols BGP, EIGRP and OSPF on Cisco Routers 7613, 7201, 2812, 2811 and 3945E.\n* Worked on the implementation of branch routers such as Juniper MX80, MX104 and perform Junos upgrades.\n* Acquired hands on experience in supporting and troubleshooting 250 above remote locations within the VPN throughout US and Canada.\n* Provided 24/7 On-Call assistance to the clients in all WLAN network related issues in a rotation basis.\n* Worked in a team of designing and deploying stand-by datacenter of the giant enterprise network.\n* Direct report to Senior WLAN Engineer/AVP & Responsible for all WAN, LAN, VOIP, Security & IPT on his absence along with weekend support.\n* Deploying and decommission of VLANs on core Nexus 7K, 5K and its downstream devices and also configure 2k, 3k,7k series Routers.\n* Worked on projects/WLAN/systems/issues of small to medium complex enterprise network.\n* Provided wireless operations support and maintenance for network systems as a primary team member.\n* Working experience on PA-5020, PA-3020 series Palo Alto firewalls and Panorama.\n* Strong hands on experience in installing, troubleshooting, configuring Dell switches and Dell routers.\n* Hands on experience deploying and troubleshooting IP phone systems (Avaya wireless and Polycom).\n* Good Hands on experience in deploying, troubleshooting and configuring Cisco Meraki Layer 2 and Layer 3 switches like MS 225, MS 250 and MS 350.\n* Acquired knowledge in working with Meraki wireless access points MR 16, MR 18, MR 32, MR 42, MR 72 etc.\n* Given support for RADIUS and TACACS+ servers for network device access control.\n* Involved in physical stacking and racking of Network equipment in huge datacenter environment.\n* Configuration on BIG IP (F5) Load balancers and also monitored the Packet Flow in the load balancers.\n* Perform installs, configure and troubleshooting on statefull inspection firewalls and inline/passive IPS/IDS sensors.\n* Worked on F5 BIG-IP LTM 8900, Citrix and Netscalar configured profiles, provided and ensured high availability.\n* Experience in providing Security using F5 BIG-IP APM and ASM.\n* Configure and Manage site-to- site IPSEC VPN with different partners. Troubleshoot remote access services like Cisco WLAN clients and for the users to access their enterprise network.\n* Knowledge of Intrusion Detection, DMZ, encryption, IPsec, proxy services, Site to Site VPN tunnels, MPLS/VPN, SSL/VPN.\n* Performed VRF on routers to separate router table into two routing tables.\n* Configuring rules and Maintaining Palo Alto Firewalls & Analysis of firewall logs using various tools\n* I have a good experience working with the Trouble Tickets on F5 Load balancers.\n* Gather customer requirements for network project and prepare HLD, BoM and LLD\n* Provided application level redundancy and availability by deploying F5 load balancers LTM\n* Designed and implemented DMZ for Web servers, Mail servers & FTP Servers using Cisco ASA5500 Firewalls.\n* Worked extensively on Cisco ASA 5500(5510/5540) Series, experience with convert Cisco ASA to Juniper SRX solution.\n* Network security including NAT/PAT, ACL, on ASA and Juniper SRX Firewalls.\n* Implemented Site-to-Site VPNs over the internet utilizing 3DES, AES/AES-256with Juniper SRX 550Firewalls.\n* Experience in managing and resolving incident tickets generated by ticketing tool Service Now.\nEnvironement: Cisco ASA, Palo Alto Firewalls, F5 Load balancers, RADIUS and TACACS+ servers, Meraki Layer 2 and Layer 3 switches like MS 225, MS 250 and MS 350, Nexus 7K, 5K devices, Meraki wireless access points MR 1ga6, MR 18, MR 32, MR 42, MR 72 routing protocols BGP, EIGRP,OSPF and BGP ,ACL, WLan, Cisco Routers 7613, 7201, 2812, 2811 and 3945E, and Juniper MX80, MX104.', u""Sr. Network Engineer\nAtos - Santa Ana, CA\nSeptember 2015 to July 2016\nGovernmental IT Outsourcing Services, Inc.\nOrange County (DMZ & Transformation)\n\nResponsibilities:\n* Worked extensively in Configuring, Monitoring and Troubleshooting Cisco ASA's 5585.\n* Responsible for Cisco ASA firewall administration, Rule Analysis, Rule Modification.\n* Implementation of Access Lists for allowing/blocking desired traffic.\n* Provided support for VOIP applications, Call Manager, Call Center and other WLAN network equipment.\n* Packet capturing, troubleshooting on network problems, identifying and fixing problems.\n* Experience working in Datacenters environment, configuration changes as per the needs of company.\n* Troubleshot layers 1, 2, and 3 of the OSI model on a daily basis with customers and partners.\n* Experience in migration of VLANS.\n* Performed Configuration on ASR 9K Pairs includes HSRP, Bundle Ethernet Config, Assigning DHCP profiles\n* Configuring objects such as Load Balancer pools for local traffic management on F5 Load Balancers\n* Configuring VLANs/routing/NATing with the firewalls as per the network design.\n* Extensive Knowledge on the implementation of Cisco ASA 5500 series and checkpoint R 75 firewalls\n* Implemented, configured BGP WAN routing, converting OSPF routes to BGP (OSPF in local routing).\n* Installed and maintained Cisco and F5 Load Balancer and documentation\n* Installing, configuring Cisco Catalyst switches 6500, 3750 & 3550 series and configured routing protocol OSPF, EIGRP, BGP with Access Control lists implemented as per Network Design.\n* Implementing, configuring, and troubleshooting various routing protocols like RIP, EIGRP, OSPF, and BGP etc.\n* Realignment and modification of BGP from the MPLS routers.\n* Created documents for various platforms including Nexus 7k, and ASR1k enabling successful deployment of new devices on the network\n* Strong knowledge in reviewing HLD,BoM and LLD for network projects according to customer requirements.\n* Configuring, implementing and troubleshooting VLAN's, VTP, STP, Trunking, Ether channels.\n* Designing, implementing LAN/WAN configurations on Cisco 5K, catalyst 6500 switches.\n* Experience configuring Virtual Device Context in Nexus 7k series switch.\n* Strong knowledge on networking concepts like TCP/IP, Routing and Switching.\n* Designed, configured, implemented site-site VPN on cisco ASA 5500 firewall.\n* Having strong knowledge in AAA implementation on servers.\n* Implemented, configured redundancy protocols HSRP, VRRP, GLBP for Default Gateway Redundancy.\n* Provided Security using F5 BIG-IP APM and ASM.\n* Experience with configuring Load Balancing methods in F5 LTM and also configured the virtual server.\n\nEnivorment: Cisco ASA 5500 series and checkpoint firewalls, F5 Load Balancer, TCP/IP, Routing and Switching, VLAN's, VTP, STP, Trunking, Ether channels, AAA implementation on servers, Nexus 7k, and ASR1k, VLANS and WLAN. Cisco Catalyst switches 6500, 3750 & 3550, routing protocol OSPF, EIGRP, BGP with Access Control lists and maintain Brocade VDX 6740 and ICX 7250 switches."", u""Sr. Network Engineer\nFacebook - Menlo Park, CA\nMay 2013 to July 2015\nResponsibilities:\n\u2022 Involved in Substantial Configuration & validation prior to implementation of Nexus 7K, 5K & 2K connecting to blade servers.\n\u2022 Management of F5 LTMs & GTMs(DNS) to improve web application delivery speed and replication through and between distributed global data centers\n\u2022 Worked on F5 LTM, GTM series like 4000, 5000 for the corporate applications\n\u2022 Experience in configuring Virtual Device Context in Nexus 7010.\n\u2022 Experience with building Ultra Low Latency Data Center Design End-to-end design approach.\n\u2022 Design, implementation and operational support of routing/switching protocols in complex environments including BGP, OSPF, EIGRP, Spanning Tree, 802.1q, etc.\n\u2022 Hands on Experience with Spirent test center.\n\u2022 Experience in Configuring, upgrading and verifying the NX-OS operation system.\n\u2022 Deploying and decommissioning of Cisco Routers, Cisco switches and their respective software upgrades.\n\u2022 Performing the Firewall ACL requests change for various clients by collecting source and destination details.\n\u2022 Policy management and changes in Juniper SRX firewall\n\u2022 Administration and troubleshoot of Juniper SRX 3000, 4000, 5000 Series firewall..\n\u2022 Performing troubleshooting on slow network connectivity issues, routing issues that involves OSPF, BGP and identifying the root cause of the issues.\n\u2022 Configure Corporate, Wireless and Lab Devices which includes Bandwidth Upgrade, Adding New Devices, Decom the Devices, Testing(Pilot).\nEnvironment: Juniper SRX 3000, 4000, 5000 Series firewall F5 Load Balancer, F5 LTMs & GTMs(DNS) , TCP/IP, Routing and Switching, VLAN's, VTP, STP, Trunking, Ether channels, Nexus 7k, and ASR1k, VLANS and WLAN, routing protocol OSPF, EIGRP, BGP with Access Control lists."", u""Sr. Data Network Engineer\nAT&T - Middletown, NJ\nJuly 2011 to April 2013\nResponsibilities:\n\u2022 Configured and designed LAN networks with Access layer switches such as Cisco 4510, 4948,\n4507 switches.\n\u2022 Setting up VLANS and configuring ISL trunk on Fast-Ethernet channel between Switches.\n\u2022 Configuring Virtual Chassis for Juniper switches EX-4200,Firewalls SRX-210.\n\u2022 Configuring VLAN, Spanning tree, VSTP, SNMP on EX series switches.\n\u2022 Understand the JUNOS platform and worked with IOS upgrade of Juniper devices.\n\u2022 Designed and implemented security policies using ACL, firewall.\n\u2022 Design and configuring of OSPF, BGP on Juniper Routers and SRX Firewalls.\n\u2022 Configured routing policy for BGP. Switching related tasks included implementing VLANs and configuring ISL trunk and 802.1Q on Fast-Ethernet channel between switches.\n\u2022 Troubleshoot and technical support for Global wide area network consisting of Multi-Protocol label switching MPLS, VPN and point-to point site.\n\u2022 Troubleshooting on network problems with Wire shark, identify problem and fix.\n\u2022 Understand the JUNOS platform and worked with IOS upgrade of Juniper devices\n\u2022 Redistribution of routing protocols and Frame-Relay configurations.\n\u2022 Configuring and troubleshooting type of routing to route traffic flow per customer requirement as primary, backup/load balanced and load splitting.\n\u2022 Configured VLANs on a switch for inter-VLAN communication. Configured VLAN Trunking Protocol (VTP) on Core Switches. Configured various LAN switches such as CISCO CAT 2900, 3550, 4500, 6509 switches for STP, VTP Domain, VLAN, Trunking, Fast Ether Channel configuration.\n\u2022 Documented all the work done by using Visio, Excel & MS word.\n\nEnvironment: F5 Load Balancer,Switching, VLAN's, VTP, STP, Trunking, inter-VLAN communication Ether channels, VLANS and WLAN, routing protocol OSPF, EIGRP, BGP with Access Control lists , JUNOS platform and worked with IOS upgrade of Juniper devices."", u'Network Support Engineer\nOriental Insurance - IN\nMarch 2009 to June 2011\nResponsibilities:\n* Assigned a project to set up LAN. Worked on the entire project from cabling to IP addressing assignment.\n* Supporting wide range of products from Cisco Systems, Troubleshooting of Routers and Switches.\n* Configuration and installation of Cisco Routers and Switches.\n* Configuring and working with Cisco Routers and Switches using protocols like RIP, EIGRP, and OSPF.\n* Manage the Routing protocol and Encapsulation Protocol.\n* Trouble shooting of WAN connectivity problems.\n* Configured STP for loop prevention and VTP for Inter-VLAN Routing.\n* Configuration of Standard and Extended access-lists.\n* Coordinating with Filed Engineers in trouble shooting of WAN connectivity problems.\n* Coordinating with tasks assigned to Field engineers.\n* Configured VPN for the remote and site-to-site access.\n* Part of troubleshooting team for checkpoint firewall.\n* Worked on Access Control List (ACL), NAT/PAT rules.\nEnvironment: Cisco Routers, Switches, WAN, LAN, Trouble Shooting, Checkpoint Firewall, Access Control List (ACL), NAT/PAT rules.']",[u'Bachelors of Technology in Computer science engineering'],[u'Jawaharlal Nehru Technological University']
0,https://resumes.indeed.com/resume/6ad2011a65e5c052,"[u'Analyst Engineer (Data Analyst)\nDeepak Galvanising and Engineering Industries Pvt. Ltd - Hyderabad, Telangana\nJune 2014 to July 2016\nEvaluated operations and conducted analysis for a 54000MT manufacturing unit of micro wave towers, transmission lines, cellular tower, and power substation for Airtel, and Vodafone. Active role in all aspects of the process including requirements gathering, product analysis, and\nperformance.\n\n\u2022 Led and managed the business operations team to lower the costs by 25% in transmission lines, increasing its efficiency from 92% to 94%.\nAlso, provided drilldown dashboard analysis using R, and Tableau\n\u2022 Developed operational solutions using Excel, and built an optimization model for calculating economics and determining impact on total\nsystem\n\u2022 Forecasted product quality using Arima time series model with 95% accuracy for cellular towers which prevented overstocking of products,\nresulted in quarterly savings of up to 15%\n\u2022 Actualized various manufacturing initiatives using logistic regression, time series, and market basket analysis, using various packages in R,\nwhich in turn reduced manual intervention and quality assurance effort', u""Production Engineer - Intern\nOswal Rubbers Pvt. Ltd - Hyderabad, Telangana\nMarch 2013 to May 2013\nTen-week program with a concentration in production, development and engineering.\n\u2022 Implemented an agile framework and managed a team of 40 for increasing output per day\n\u2022 Slashed payroll/benefits administration costs 30% by negotiating pricing and fees, while ensuring the continuation and enhancements of services,\nhelp develop strategies for marketing company's major product - Co-extruder Polyethylene""]","[u'Master of Science in Information Systems and Operations Management', u'in Advanced Probability and Statistics']","[u'University of Florida, Hough Graduate School of Business Gainesville, FL\nMay 2018', u'SRM University Chennai, Tamil Nadu\nApril 2014']"
0,https://resumes.indeed.com/resume/92241ccd069142bc,"[u'Systems Engineer\nInfosys - Pune, Maharashtra\nMay 2014 to July 2016\n\u2022 Configure Storage solution in SAN/NAS environment\n\u2022 Created change implementation plan and implemented of the new storage allocation for application teams as business requirements.\n\u2022 Configured and managed the Aggregate, volume and qtree in storage system\n\u2022 Configured Disaster Recovery site for business-critical applications using snap mirror.\n\u2022 Performed necessary upgrades on Netapp (7 mode and Cluster mode) and Isilon OneFS to bring in accordance with SOE (Standard Operating Environment) to any avoid bugs.\n\u2022 Preformed ORC (Operational Readiness Checks) for the newly installed storage controller with vendor.\n\u2022 Designed and Implemented Disaster Recovery Solution with RecoverPoint, VMAX AND VNX.\n\u2022 Implementing EMC ViPR in the Environment\n\u2022 Migrated 200 TB of data from Netapp to Isilon using isi_vol_copy tool under Prod and Non-Prod environment.\n\u2022 Configured and managed backup as per client requirement on Snap Protect and Commvault Simpana.\n\u2022 Troubleshoot performance issues on Netapp and Isilon.', u'Data Processor\nReed Global - Manchester, Greater Manchester\nOctober 2011 to June 2013\n\u2022 Monitoring of NAS and SAN environment\n\u2022 Handling NetApp FAS 2020, FAS 3020, FAS 6030, FAS6080 and FAS 2700 Series Storage\n\u2022 Creation of new cifs, NFS shares\n\u2022 Handling Volume / Snapshot full issues.\n\u2022 Working on inodes Full issues.\n\u2022 Creation of new iscsi, FCP luns and group.\n\u2022 Monitoring of Snap mirror between Source and DR\n\u2022 Faulty disk replacement\n\u2022 Monthly Report of all activity\n\u2022 Troubleshooting of problems related to CIFS, NFS shares and ISCSI, FC Lun.\n\u2022 Creation of new aggregates, Volumes and qtrees\n\u2022 Creation and scheduling of snapshots as per requirement\n\u2022 Configuration of snap mirror with DR.']","[u""Master's in Information Studies"", u'M.Sc. in Business Information System', u'B.Tech. in Computer Science Engineering', u'']","[u'Trine University Angola, IN\nJanuary 2017 to May 2018', u'University of Bolton Manchester, Greater Manchester\nJanuary 2010 to September 2011', u'CVSR College of Engg., JNT University Hyderabad, Telangana\nAugust 2005 to May 2009', u'Sri Aurobindo International School']"
0,https://resumes.indeed.com/resume/771ea975f6525cd4,"[u""Data Engineer\nL&T Infotech - Mumbai, Maharashtra\nJune 2015 to July 2017\n\u2022 Designed an automation tool to solve functional issues faced by the clients, resulting in a decrease manpower.\n\u2022 Implemented data mappings to integrate disparate source data into the staging layer and, transform and load into Oracle Database using PL/SQL procedures, reducing manual effort by 80%.\n\u2022 Designed an optimized new client onboarding process by developing Standard PO and Sales Order conversion from Legacy system to Oracle Apps R12.\n\u2022 Developed Big Data Solutions that enabled business and technology teams to make data-driven decisions.\n\u2022 Checked Hadoop feasibility and used language PIG LATIN for loading & transformations.\n\u2022 Performed import of data from OLTP environment to HDFS and creating hive tables on top of the datasets. Developed Hive UDF's in Map Reduce to extend the functionality of hive operations.\n\u2022 Communicated weekly with users and business analysts to understand business processes, gathered and documented requirements and created functional and technical specification documents."", u'Analyst Intern\nBEL Lighting\nAugust 2013 to June 2014\n\u2022 Designed File Management System based on RDBMS to optimize client management by converting data stored in physical files into digital form using MS-Access and MySQL.\n\u2022 Developed interactive dashboards and stories with drill downs showing details of the price data for 30 specific food items across several months in 8 countries using Tableau.\n\nACADEMIC PROJECT']","[u'M.S., Business in Analytics', u'B.E. in Electronics and Telecommunication']","[u'The University of Texas at Dallas Dallas, TX\nMay 2019', u'K.J. Somaiya College of Engineering Mumbai, Maharashtra\nMay 2015']"
0,https://resumes.indeed.com/resume/dd1b2158502a9e04,"[u""Consultant\nTrueFit Ventures LLP - Mumbai, Maharashtra\nFebruary 2017 to July 2017\n\u2022 Designed a Spin-the-Wheel reward system for prospective clients leading to a 10% increase in client conversion rate.\n\u2022 Incorporated a 'Health Bar' in the premises for members to meet their pre-workout and post-workout nutrition needs\nresulting in a 7% increase in the monthly revenue."", u""Data Engineer\nM76 Analytics - Mumbai, Maharashtra\nJune 2016 to January 2017\nDeveloped the back end of the 'Reporting & Exploring' and the 'Business Planning' modules for Mego, a proprietary\ndecision support system to enable data-driven firms to perform business strategy optimization.\n\u2022 Explored, and analyzed machine learning techniques to advance the effectiveness of the decision support system and developed modular R scripts for the same in accordance with the design specification.\n\u2022 Performed customer segmentation and cohort analysis for a leading E-commerce grocery company, LocalBanya.\n\u2022 Organized relevant transactional details like average order size, SKUs, and products to profile all possible types of purchase baskets into 5 statistically exclusive categories of baskets.\n\u2022 Mined customers into logical cohorts based on their demographic parameters such as gender, age, and zone.\n\u2022 Created customer segments from possible combinations of basket types and customer cohorts to enable them to develop market penetration strategies, and promotional schemes for each segment.""]","[u'Master of Information Systems Management in Information Systems Management', u'Bachelor of Technology in Computer Science and Engineering']","[u'Carnegie Mellon University Pittsburgh, PA\nDecember 2018', u'SRM University Chennai, Tamil Nadu\nMay 2016']"
0,https://resumes.indeed.com/resume/36d0aea45a4cce50,"[u'Big Data Engineer\nVintech Solutions\nApril 2017 to Present\nInvolved in Stream processing using spark and flink on the data ingesting using kafka and build analytical models out of that for downstream applications. Used hive for archival reporting on the historical data.', u'Big Data Engineer\nGCN Media Publishing\nMarch 2016 to March 2017\nMigrate the ONEcount platform from its current MySQL architecture to an appropriate, big data engine(s) so that it can scale to meet the growing demands of expanding customer base. Change existing data pipeline based on LAMP architecture to big data based toolkit.', u'Data warehousing\nMicronTechnologies\nDecember 2014 to December 2015\nData warehousing\nBuild data warehouse to ingest data from multiple sources and analyze data to reflect internal systems.Implemented processing using spark and migrated existing workflows from MapReduce for more efficiency.', u'Data Scientist (Intern)\nVintech Solutions\nJune 2015 to August 2015\nProduct Demand Forecasting Data Scientist Intern\nEvaluate different machine learning techniques for the demand forecast of different products and adjust the production. This includes data ingestion cleaning using Hadoop environment and implementation of different classification techniques.', u""Big Data analyst\nMorgan Stanley\nJanuary 2012 to November 2014\nBuild Data Warehouse for archiving data, develop data access layer to provide unified data access platform for downstream applications and support various data analytics. HDFS and MongoDB are used as storage layer for warehouse depending on the required response times from the warehouse. The data ingestion flow is built using Flume, Sqoop and Kafka.\n\nClient: Western Union\nSocial Media Influencer Data Analyst\nPerform Sentiment analysis of different users from the data extracted from various social networking sites like Facebook, Twitter, Blogs and review websites. The data is ingested and cleansed using Map-Reduce. Polarity of users is evaluated using Open NLP package and Bag-of-Words approach.\n\nClient: Barclays\nMarket Trend Analyzer Big Data analyst\nAggregate and analyze log files generated by the web servers and stock transactional information and derive use fulin formation to identify patterns of market trends and trader's interests. The future predictions are made about the scrip's from the past market trends and integrating that information from the news and implementing different machine learning techniques on them.\n\nClient: Nielsen Holdings\nAssortMan Hadoop Developer\nStudy the effects of different variables in market on a particular product by analyzing transactional and operational Logs. Regression is implemented for SKU Rationalization, filling distribution gaps and to identify regional effects, effects of competitors, study and minimize effects of cannibalization.\n\nPricing Insights Hadoop Developer\nEfficient pricing strategy is determined by analyzing prices in different areas and the prices of competitors. The pricing information is extracted from various databases using Sqoop and combined with log information ingested using Flume and cleaned and pricing strategy is developed by analysis on cleaned dataset.""]","[u'Master of Science in Computer and Information Sciences', u'Bachelor of Technology in Electronics and Communications Engineering']","[u'Purdue School of science, IUPUI\nDecember 2015', u'Jawaharlal Nehru Technological University\nMay 2011']"
0,https://resumes.indeed.com/resume/2f6eeb9b9ec76956,"[u'Data Engineer\nmPulse Mobile - Encino, CA\nOctober 2016 to Present\nCreated scikit-learn models and Flask APIs for language detection, question classification, sentiment analysis, and various SMS parsing applications. Wrote and automated SQL reports for client inquiries. Worked on a rules-based recommendation engine to automate user workflow tailoring.', u'Data Analyst\nDataScience - Culver City, CA\nFebruary 2016 to August 2016\nDeveloped nightly batch scripts for generating thousands of models for life-time value from prototype Jupyter notebooks. Explored data for analyses using SQL (AWS) and scientific Python modules. Worked on projects ranging from lifetime value to customer churn analysis for various clients to help then better understand the behaviour of various cohorts.', u'Researcher/Developer\nMIT Media Lab\nAugust 2015 to January 2016\nPlayful Systems Group\nBuilt new features for DeepView in preparation for the Millionaire Chess tournament, in collabo- ration with GM Maurice Ashley. It aims to extrapolate on the Stockfish chess engine data to create\nnon-expert level insights from tournament chess positions.', u'Data Mining Research Assistant\nCSUN - Northridge, CA\nJanuary 2015 to January 2016\nDeveloped a pipeline for the cleaning, classification, and clustering of students in a large academic\ndataset for the purpose of creating an early-warning system to detect graduation rates using scikit-\nlearn.', u'Machine Learning Research Assistant\nCSUN\nMarch 2015 to September 2015\nImplemented a machine learning solution with scikit-learn for the classification of smartwatch motion\nsensor data to predict PIN entry on smartphones.']",[u'BS in Mathematics'],"[u'California State University-Northridge Northridge, CA\nJanuary 2011 to January 2015']"
0,https://resumes.indeed.com/resume/46d9e0d6aab6c85f,"[u'Process Engineer\nStencil Master, Inc - San Jose, CA\nSeptember 2015 to January 2018', u'Data Quality Analyst\nGoogle Express, Vaco - Mount View, WI\nSeptember 2014 to April 2016', u'Airman\nUS Air Force - various\nDecember 2003 to November 2014']",[u'BS in Electrical Engineering'],"[u'San Francisco State University San Francisco, CA\nAugust 2010 to January 2014']"
0,https://resumes.indeed.com/resume/04c7531d477007fa,"[u'Social Chairman\nPhi Gamma Delta - Norman, OK\nMay 2015 to Present\n* Created and handled budgets ranging up to $70,000\n* Negotiated with entertainment firms and other business types\n* Setup Philanthropies, Mixers, and other fraternity events\n* Delivered weekly updates to 180+ members', u'Project Data Engineer\nCapRock Global Solutions - Oklahoma City, OK\nJune 2017 to August 2017\n\u2022 Created raw data to assist the main consultants of the company\n\u2022 Specialized in Time/Motion Studies\n\u2022 Focused study was in the HealthCare Industry', u'Colonial Country Club - Fort Worth, TX\nJune 2013 to January 2016\nBag Boy: Outside Services Department\n* Requires personal knowledge of individual members\n* Maintain and preserve the course and its utilities\n* Communication is key in this job']",[u'Bachelor of Science in Business Administration Finance'],"[u'University of Oklahoma Norman, OK\nApril 2018']"
0,https://resumes.indeed.com/resume/8fd55baaf81f997a,"[u""Data Management Analyst / Salesforce Administrator\nArchdiocese of New York - New York, NY\nMarch 2017 to Present\n\u2022 Salesforce Administration: Managing the Region's Salesforce database, including adding to and maintaining all records contained in the Salesforce Customer Relationship Management (CRM) database; identifying and eliminating duplicates within the CRM system using Demand tools and Data Loader, certifying the overall accuracy of the data across related systems.\n\u2022 Salesforce Development: Implementing security and sharing rules at object, field, and record level. Designing and implementing custom Visualforce pages and apex classes.\n\u2022 Data Analysis and Reporting: Analyzing (Historical and future prediction) and creating interactive reports and dashboards using Tableau. Performing ad-hoc requests using SQL queries to extract and format information from multiple data sources.\n\u2022 Marketing Analysis: Analyzing the market trends, conducting online surveys and presenting it to higher management. Performing market research, analyzing consumer's tastes using Google Analytics.\n\u2022 SQL and ETL Integration: Managing various applications (ZenDesk, TADs, SMART, Cornerstone) and data integration between these applications using SQL Server Integration Services.\n\u2022 Advance Excel: Performing financial and budget analysis. Entering, maintaining and cleaning data; managing the process of eliminating inconsistent data using advanced Excel functions and formulas."", u""Salesforce Data Analyst\nAmerican Professional Management Inc - New York, NY\nJune 2016 to November 2016\n\u2022 Requirement Gathering: Planned and conducted requirements elicitation meetings to collect functional and non-functional requirements relating to client's Salesforce technology enhancement and initiatives.\n\u2022 Salesforce Administration: Develop, improve and manage Salesforce.com configuration and customization with fields, custom objects, page layouts, reports, dashboards, workflows, rules, formulas, data validations.\n\u2022 Data Governance: Maintained data quality (including the elimination and prevention of duplicates and inaccuracies), Perform data updates, create and manage users and profiles.\n\u2022 Salesforce Data Management: Responsible for all the activities related to configuring Data Loader and Data Import Wizard, uploading data in CSV files into salesforce.com. Interacted with SalesForce.com premium tech support team on a regular basis.\n\u2022 Training and Maintenance: Assisted in designing and creation of training material and conducted internal training sessions for business users on Salesforce technology functionalities."", u'Analyst Engineer\nTelesoft Pvt Ltd - Mumbai, Maharashtra\nJune 2014 to November 2014\n\u2022 Business Analysis and Visualization: Analyzed historical requests of ad-hoc reports and created Excel pivot tables, V-Lookups, Index and Match that enabled end-users to ascertain information timely by creating the necessary tables, views, functions and cursors using SQL.\n\u2022 Requirement Gathering: Responsible for gathering Business Requirements and System Specification from users. Translating Business System Design document into the technical design document.\n\u2022 Business Intelligence: Participated in Design meetings and Brainstorming sessions to improve process flows. Created high-level process flows for proposed options for the team to review.\n\u2022 Project Management: Liaise between business and technical personnel to ensure a mutual understanding of processes and application. Evaluate risks related to requirements implementation, testing processes and project communications.\n\u2022 Testing and Quality Assurance: Developed test plans based on requirements documentation, perform database queries to identify test data and create test procedures /test cases with expected results.']","[u'M.S. in Information Systems', u'B.E. in Computer Engineering']","[u'New Jersey Institute of Technology\nJanuary 2015 to January 2017', u'Atharva College of Engineering, University of Mumbai\nAugust 2011 to May 2014']"
0,https://resumes.indeed.com/resume/7dc95c3b9e1a9cf5,"[u'Manager\nMay 2016 to Present', u'Assistant Manager\nMetro PCS\nAugust 2015 to May 2016', u'Field Engineer\nBoost Mobile\nJuly 2012 to July 2015', u'Data Entry\nMID State Consultants\nJanuary 2014 to March 2014']",[u'High School Diploma'],[u'Cooper Academy\nJune 2012']
0,https://resumes.indeed.com/resume/cdbd3899691be23a,"[u""Data Scientist III\nEARLY WARNING SERVICES - Scottsdale, AZ\nJanuary 2012 to January 2018\nDeveloped and implemented various machine learning models with banking and credit cards data.\n\u2022 Performed Text Mining and analyzed P2P transactions' memo containing a brief description to gain information for marketing team to set marketing strategy.\n\u25e6 Developed normalization methods to word, sentence, multiple sentences, then transformed memos to single idea.\n\u25e6 Executed Sentiment Analysis and classified a person's sentiment status from his / her memos after stemming, removing stop words / neutral words, cumulated word sentiment scores to memo scores, then cumulated memo scores to classifying if account holder is positive, negative, or neutral.\n\u25e6 Provided Marketing Analysis (Association Rule) by analyzing if certain combinations of payment categories that occur together frequently among P2P transactions.\n\u2022 Created Mistype Model as part of the Company's ID related project.\n\u25e6 Compared names in application against the mode (most frequent found) name in all historical data per SSN, Phone, Address, and Driver License # for those that don't match, but close enough, were classified their mistype events (skipped / extra) letter, fat finger caused mistypes, etc.\n\u25e6 Applied various Machine Learning models to fit the data and determined which combinations were likely done by fraudsters (intentional mistype).\n\u2022 Created Money Mules' Credit Card Account Spending Profile - Money mule is a person who transfers $ acquired illegally and criminal uses a mule to hide the criminal's true identity and location.\n\u25e6 Created (card present) credit card spending profile (1) per account and (2) merchant category code and account.\n\u25e6 Applied Clustering algorithm to separate patterns\n\u25e6 Applied Principal Component Analysis reducing dimensions and built control charts (Statistical Process Control) around principal components to detect any unusual patterns.\n\u2022 Created ACH accounts' profiles and applied clustering algorithms to establish finite number of patterns.\n\u2022 Performed Network Analysis where the memo contains certain keywords that could be involved in fraud activities and visualized the fraud network using Gephi software.\n\u2022 Improved the logic to categorize what the ACH transaction is for, i.e., income, card payment, etc. The results were used to help all other Analytic projects to create features for what each banking transaction is for.\n\nPeter J. Chung pchung3000@yahoo.com\n\nEARLY WARNING SERVICES )\n\u2022 Supported Common Purchase Point (CPP) project, scoring merchants where credit cards were compromised by tracing all common merchants visited by compromised credit / debit cards. Created features such as velocity (physical distance from one card present transaction to next per time) and applied multiple machine learning models to fit the data.\n\u2022 Performed quarterly / yearly model maintenances to validate if the models still fit the data after a few years of use.\n\u2022 Supported Income Verification project to verify an account holder's income.\n\u2022 Performed Marketing Analysis to know whether traditional media or twitter have higher impact on #P2P transactions for competitors. So, analyzed each competitors' #P2P transactions identified in ACH data against periodic tweet reaches and periodic traditional media impressions."", u'Data Engineer & Data Analyst\nMERRILL LYNCH / BANK OF AMERICA - Scottsdale, AZ\nJanuary 2006 to January 2012\nPerformed various statistical analyses and data ETL for various formats.\n\u2022 Executed Monte Carlo Simulation, simulating stock prices to valuate stock options and market performance awards.']","[u'PhD', u'Master of Science in Industrial Engineering', u'Bachelor of Science in Mathematics']","[u'Arizona State University Tempe Tempe, AZ', u'Arizona State University Tempe, AZ', u'New Mexico State University Las Cruces, NM']"
0,https://resumes.indeed.com/resume/18290113c8e2632d,"[u'Data Analyst\nSK&A - Irvine, CA\nJuly 2012 to August 2017\nETL (extract-transform-load)\nData analysis\nData processing', u'Software Engineer\nInfosend, Inc - Fullerton, CA\nAugust 2007 to July 2010\n\u2022 Programming in Visual FoxPro 8.0 and 9.0.\n\u2022 Imposing data.\n\u2022 Reporting applications.\n\u2022 Design/Program reports through Crystal Reports 11 and VFP.\n\nProjects:\n\u2022 IndyMac Bank advertisement, commercial and private direct mailers.', u'Lead Software Engineer\nInnovative Accounting Systems, Inc - Rancho Santa Margarita, CA\nDecember 2005 to August 2007\n22521 Avenida Empressa Ste.111 Rancho Santa Margarita, CA 92688 (949).589.1400\nDec 2005 - Aug 2007\n\n\u2022 Programming in Visual FoxPro 6.0, 8.0 and ASP.\n\u2022 Engineering custom Pro Series, ERP, Vision Point and CRM applications.\n\u2022 Tech support.\n\u2022 Server maintenance and setup.\n\nProjects:\n\u2022 Sage Accpac ERP Accounting and Operations.\n\u2022 Sage CRM & Contact Management.\n\u2022 Vision Point Accounting Software.', u'Software Engineer\nAction Mailing Incorporate - Tustin, CA\nAugust 2005 to December 2005\n\u2022 Programming in Visual FoxPro 6.0.\n\u2022 Data processing and mail-merging.', u'Lead Software Engineer\nGoldenwest Diamond Corporation - Tustin, CA\nMay 2002 to August 2005\n\u2022 Programming in Visual FoxPro 6.0, 8.0.\n\u2022 Engineering and maintaining custom developed applications.\n\u2022 Maintaining SBT ACCPAC accounting applications.\n\u2022 Tech support.\n\u2022 Networking maintenance and support.\n\nProjects:\n\u2022 RUBY: Custom POS, Inventory and Accounts System - Lead Programmer, Designer.\n\u2022 SBT: Accounting System - Analyst and support.', u'Lead Software Engineer\nSouthTech Systems, Inc - Santa Ana, CA\nJune 2000 to March 2002\n\u2022 Programmed in Visual FoxPro 5.0, 6.0.\n\u2022 Projects involve client-server interface and database development.\n\u2022 Designed, developed and engineered document management data/image storage and retrieval systems for Orange County offices.\n\u2022 Composed training manuals and specification documents.\n\nProjects:\n\u2022 VITALS 2000 Record System - Lead Programmer, Designer.\n\u2022 CFD (Campaign Filing & Disclosure) - Lead Programmer, Designer.\n\u2022 Cashier/Fee Accounting System - Project Manager, Programmer.']","[u'Diploma in Client in Server Architecture', u'B.S. in Computer Engineering and Computer Science', u'A.A. in Liberal Arts']","[u'Computer Learning Center', u'California State University', u'Rancho Santiago College']"
0,https://resumes.indeed.com/resume/7329e6818bd0dde7,"[u'Cost Data Analyst\nDodge\nAugust 2016 to Present\n*SQL querying\n*Creating Predictive Modeling using Excel to price products\n*Monitoring a Predictive Models\n*Advanced Cost Research and development Marketing\n*Maintained Mainbox, uploading Dashboards\n*Custom user experiences in BI services and client engineering development', u'Cost Engineer\nTiteflex (Smiths Tubular Systems)\nJuly 2015 to August 2016\n*Developed/standardized a cost model template-Advanced Excel-pivots, slicers, vlookups, sumifs, nested-ifs, SQL\n*Compute cost factors - prepare multi-million dollar bids, selecting vendors\n*Determine Labor and Bill Of Materials-Ensure no missing processes/operations and ensured that the BOM was accurate\n*Quote package with data pulled from multiple departments to support sales\n*Analysis of quote packages for risk and ""what if"" scenarios\n*Central cost estimating database-Access and database training management\n*Analyze estimated costs to actual costs-look at previous quotes to determine variances\n*Cost trend analysis-year to year cost variances to help sales/management determine margin/price in million dollar quotes', u'Design Assistant\nNorth Hartland Tool Corporation - North Hartland, VT\nJune 2011 to September 2015\n4 summers)\n*Drafting and modeling fixtures and clamps in UG N\n*Updated old UG files\n*Downloaded CAD files and parts\n*Counted Inventory and assessed maintained an accurate account of inventory\n*Gathered quotes and prices from vendors\n*CNC operator - GD&T (in China)\n*Interpreter (traveled to China) and trained workers in CNC, Lathe and Surface Grind']","[u'', u'B.A. in Economics and B.S.M.E. in Economics', u'in Physics']","[u'PSU/Manchester Community College\nJuly 2015 to June 2016', u'Union College Schenectady, NY\nSeptember 2011 to June 2015', u'Dartmouth College Hanover, NH\nSeptember 2010 to June 2011']"
0,https://resumes.indeed.com/resume/954cc0b41f57f126,"[u'SENIOR DATA ENGINEER\nOppenheimer Funds\nJanuary 2015 to Present\nDefine and build financial data architecture for a Global Asset Manager with more than\n$240b AUM.', u'DATABASE DEVELOPER\nProdigy Resources\nJanuary 2014 to January 2015\nDesigned and built reporting for an industry-pacing restaurant with more than $1.3b annual\nrevenue.', u'BUSINESS ANALYST\nMasTec Advanced Technologies\nJanuary 2012 to January 2014\nDesigned and built reporting for a leading telecom and infrastructure company with over\n$3b in annual sales.']",[u'Bachelor of Mathematics in Mathematics'],[u'American Public University']
0,https://resumes.indeed.com/resume/1171bc6632f40bfe,"[u'Lead Data Scientist/Lead Business Analyst\nGenpact - Sanford, FL\nNovember 2014 to Present\nSenior Data Analyst, Business Analyst (SQl/Oracle/ETL Developer/ Tableau/SAS/Data warehouse/Business Object/Salesforce.com/Statistics), Genpact (GE Oil & Gas)\n\nNov 2014 \u2013 June 2017_ IN US on H4_29th July 2017 to till Date (Looking for C2C or H1b transfer)\n\nProjects: Churn-Analysis, Long-Tail Dashboard analysis, Control-Solution, Digital Solution New logos, SFDC, FPT, Market Campaign 2017\nKey Responsibilities:\n- Communicating effectively with external customers and internal teams. Collecting and analyzing the project\u2019s business requirements and transferring the same knowledge to analytics team.\n- Evaluating the data collected through task analysis, business process, surveys and workshops.\n- Providing suggestions to the analytics team during the development stage of product to meet the customer\u2019s business needs.\n- Preparing accurate and detailed requirement specifications documents.\n- Experience with Forecasting and data science algorithms including decision trees, probability networks, association rules, clustering, regression, and neural networks and having experience in Statistics (hypothesis, regression)\n- Creating complex queries in SAP Business Object.\n- Creating complex stored procedure and SQL queries, Developing complex SQL code using complex joins, group by, having, sub queries, correlated sub queries, functions, user defined functions. Stored procedures using temporary tables, cursors, table type variables.\n- Design, build and launch new data extraction, transformation and loading processes in production with using of Python based ETL\n- Manage data warehouse plans for a product or a group of products.\n- Interpret data, analyze results using statistical techniques and provide ongoing reports with using of R and SAS script.\n- Created reports in Tableau on geo based parameters, Created queries in Salesforce.com for opportunity data, comparison with analysed data.\n- Worked on strategically important projects for various clients in GE Oil and Gas industry in data analytics and building Tableau Dashboard\n- for Sales and Marketing domain. Customer retention (Sales, Order and Pipe line, Backlog) is critical for business and should be an integral part of its go-to-market strategy, so we segregated data and identified the loyal, lost and somewhat loyal customer using SQL and Tableau as Visualization. Connecting with different tables by using joins getting data into tableau and Appling Data blending by connecting different data sources in single workbook. Deep experience with the design and development of Tableau visualization solutions. Working on generating various dashboards in Tableau server using various sources.\n- Experience with creation of users, groups, projects, workbooks and the appropriate permission sets for Tableau server logons and security checks, Creating the reports using Tableau Desktop and Tableau Server functionalities like Dashboard, Filters, Marks, Parameters, Calculated field and action Filters, Experience in Building comprehensive dashboards and reports using Tableau Desktop and Business Objects.\n- Having good experience on creation of static and dynamic parameters, Forecast, Scheduling Dashboards and Data Extracts as per user needs. Publishing workbooks in tableau server for multiple users.\n- Communicating effectively with external customers and internal teams to deliver product\u2019s.', u'Data Scientist/Sr S/w Engineer\nValens-Ducis Services Pvt LTD\nJuly 2013 to October 2014\nProjects: Covidien\nKey Responsibilities:\n- Analysis of the source system and design the ETL flow.\n- Prepare design specification.\n- Prepare the Source - To - Mappings.\n- Wrote complex stored procedure and SQL queries. Created reports in Tableau on geo based parameters.\n- Created linear regression, forecasting, Trend using several parameters in excel to project the sales of services.\n- Integrated predictive models, regressions analysis etc. coded in SQL-Oracle with Tableau.\n- ETL methodology for Data Extraction, transformations, loading using SSIS.\n- End to end project management including effort estimate, planning, task allocation and monitoring.', u""Data Scientist/Sr S/w Engineer\nPhillips\nMay 2012 to June 2013\nProjects: ICAP 2.0, MR- - Modules: Export, Import, Scanner, Portal, And Report.\nDescription:\nThis is a Pre development project. ICAP Platform 2.0 is a highly productive, next generation development and deployment platform for hybrid multi-modality imaging solutions, based upon 'best of breed' assets from existing PMW and Portal platforms and from the industry.\nKey Responsibilities:\n- Created storyboards, dashboards with different charts like dual axis charts, lollipop charts, donut charts, area charts etc. in tableau 9.0.\n- Generated Tableau Dashboards with filters, quick filters, context filters, global filters, parameters and calculated fields on Tableau reports.\n- Combined multiple data sources by joining tables using Data Blending.\n- Worked on groups, sets & calculated functions including table calculations. Generated reports with trend line, reference line & bands.\n- Worked on various reports using best practices and different visualizations like Bars, Lines and Pies, Maps, Scatter plots, Gantts, bubbles, Histograms, Bullets, Heat maps and Highlight tables.\n- Understood entire business process to fix problems related to data on existing dashboards. Highlighted discrepancy in data.\n- Wrote complex SQL code for SQL server 2012 database. Created views, tables as per the requirements.\n- Created Churn analysis dashboards, Customer life time value reports.\n- Created Quadrant analysis, Survey data analysis.\n- Managed dashboards on Tableau server. Created nodes, load balancer, authorization, user filters and checked server activities.\n- Created predictive analysis, linear regression, Anova analysis in R and integrated solutions in Tableau."", u'Data Analyst\nData Ware house\nSeptember 2010 to March 2011\nProjects: Centricity Prenatal, Patient - restore, MBLink, OBLink, HL7\nDescription: The Centricity Prenatal Clinical Information System (formerly QS Prenatal) supports efficient documentation of mothers and infants throughout the prenatal continuum of care\nKey Responsibilities:\n- Created and managed tables in SQL, Wrote SQL queries and validated data in tables for reporting. Enhanced the performance of queries.\n- Wrote Proc SQL queries and SAS commands to fetch and analyse data from excel files and pushed it in Tableau.\n- Gathered requirements, objectives, and features for reports. Led a small team to create rich graphic dashboard.\n- Created and maintained dashboards using calculations, parameters in Tableau 8.2.\n- Extensively used Tableau features like Data blending, Extracts, Parameters, Filters, Calculations, Context filters, Hierarchies, Actions, Maps etc\n- Build complex calculations using advanced functions (ATTR, DATEDIFF, STR, IFs, nested IFs, OR, NOT, AND, SUMIF, COUNTIF, LOOKUPS and QUICK TABLE calculations).\n- Extensive experience in using various reporting objects like facts, attributes, hierarchies, filters, parameters, and calculated fields etc.\n- Created Dashboard reports for the key performance indicators for senior managers and business units\n- Designed & deployed rich graphic visualization with drill down, drop down menu option & parameterized using Tableau.\n- Wrote SQL and UNIX shell scripts and used putty to automate the execution of SQL scripts, manage them on server.\n- Used Pilot work design and Test Cover Tool to review the scripts of group and modify the already existing scripts as per huge requirements.\n- Performed different types of testing: Unit Testing, Integration Testing, Functional Testing, System Testing, Stress Testing, Performance Testing, Usability Testing, Acceptance Testing, Regression Testing, Beta Testing, Alpha, Load Testing']","[u'MSC in Computer Science', u""Master's in Classical Music (Vocal) in Hindustani Classical Music""]","[u'MCU University Bhopal, Madhya Pradesh\nJuly 2009', u'IKSVV, Khairagirh, India Sanford, FL\nJuly 2006 to April 2008']"
0,https://resumes.indeed.com/resume/805138fefbdc7e68,"[u""Big Data Engineer\nBeyondSOA Inc - Herndon, VA\nAugust 2017 to August 2017\nResponsibilities:\n\u2022 Worked with business teams and created Hive queries for ad hoc access.\n\u2022 Continuous monitoring and managing the Hadoop cluster through Cloudera Manager.\n\u2022 Involved in review of functional and non-functional requirements\n\u2022 Responsible to manage data coming from different sources.\n\u2022 Installed and configured Hadoop ecosystem like HBase, Flume, Pig and Sqoop.\n\u2022 Loaded daily data from websites to Hadoop cluster by using Flume.\n\u2022 Involved in loading data from UNIX file system to HDFS.\n\u2022 Creating Hive tables and working on them using Hive QL.\n\u2022 Created complex Hive tables and executed complex Hive queries on Hive warehouse.\n\u2022 Wrote MapReduce code to convert unstructured data to semi structured data.\n\u2022 Used Pig to extract, transformation & load of semi structured data.\n\u2022 Installed and configured Hive and also written Hive UDFs.\n\u2022 Develop Hive queries for the analysts.\n\u2022 Developed workflow in Oozie to automate the tasks of loading the data into HDFS and pre-processing with Pig.\n\u2022 Cluster co-ordination services through ZooKeeper.\n\u2022 Collected the logs data from web servers and integrated in to HDFS using Flume.\n\u2022 Creating Hive tables and working on them using Hive QL.\n\u2022 Worked on Hive for exposing data for further analysis and for generating transforming files from different analytical formats to text files.\n\u2022 Design and implement Map Reduce jobs to support distributed data processing.\n\u2022 Supported MapReduce Programs those are running on the cluster.\n\u2022 Involved in HDFS maintenance and loading of structured and unstructured data.\n\u2022 Wrote MapReduces job using Java API.\n\u2022 Designing NoSQL schemas in Hbase.\n\u2022 Wrote the shell scripts to monitor the health check of Hadoop daemon services and respond accordingly to any warning or failure conditions.\n\u2022 Involved in Hadoop cluster task like Adding and Removing Nodes without any effect to running jobs and data.\n\u2022 Developed the Pig UDF'S to pre-process the data for analysis.\n\u2022 Involved in Hadoop cluster task like Adding and Removing Nodes without any effect to running jobs and data.\n\nEnvironment: Hadoop, MapReduce, HDFS, Hive, Pig, HBase, Java, Cloudera Linux, XML, MySQL, MySQL Workbench, Java 6, Eclipse, Cassandra."", u'Jr. Java Developer\nBristlecone India LTD - Pune, Maharashtra\nSeptember 2013 to November 2015\nResponsibilities:\n\u2022 Worked with several clients with day to day requests and responsibilities.\n\u2022 Involved in analysing system failures, identifying root causes and recommended course of actions.\n\u2022 Integrated Struts, Hibernate and JBoss Application Server to provide efficient data access.\n\u2022 Involved in HTML page Development using CSS and JavaScript.\n\u2022 Developed the presentation layer with JSF, JSP, JAVA Script technologies.\n\u2022 Designed table structure and coded scripts to create tables, indexes, views, sequence, synonyms and database triggers. Involved in writing Database procedures, Triggers, PL/SQL statements for data retrieval.\n\u2022 Developed the UI components using JQuery and JavaScript Functionalities.\n\u2022 Designed database and coded PL/SQL stored Procedures, triggers required for the project.\n\u2022 Used Session and FacesContext of JSF Objects for passing content from one Bean to other.\n\u2022 Designed and developed Session Beans to implement business logic.\n\u2022 Tuned SQL statements, Hibernate mapping, and Web Sphere application server to improve performance, and consequently met the SLAs.\n\u2022 Created the EAR and WAR files and deployed the application in different environment\n\u2022 Engaged in analysing requirements, identifying various individual logical components, expressing the system design through UML diagrams using Rational Rose.\n\u2022 Assisted in designing, building, and maintaining database to analyze life cycle of checking and debit transactions.\n\u2022 Involved in running shell scripts.\n\u2022 Extensively used HTML and CSS in developing the front-end.\n\u2022 Deployed and tested application on Web sphere Application Server.\n\u2022 Designed and Developed JSP pages to store and retrieve information.\n\nEnvironment: Java, J2EE, JSP, Javascript, JSF Sun RI, Ajax4JSF, Spring, XML XHTML, Hibernate, Oracle9i, PL/SQL, SOAP Web service, Web Sphere, Oracle, JUnit, SVN.']","[u'Masters in Information Assurance in Information Assurance', u'Bachelor in Electronics and Communication Engineering in Electronics and Communication Engineering']","[u'Wilmington University New Castle, DE', u'Jawaharlal Nehru Technological University']"
0,https://resumes.indeed.com/resume/92d9a4695059b4b9,"[u'Director Data Solutions\nSorenson Media - Salt Lake City, UT\nJune 2017 to Present\n\u2022 Leverage AWS cloud infrastructure to house all data platforms, digital platforms, networks, security, budget, and strategy.\n\u2022 Lead and align corporate data strategy to facilitate a centralized data hub to satisfy all product and digital platform use cases.\n\u2022 Architect the global data hub, real-time data pipeline, digital platform middleware, functional data warehouses, and data lake.\n\u2022 Develop solutions with Apache Nifi, Spark, Kafka, Kinesis, S3, EMR, Glue, Athena, Lambda, EC2, ECS, EKS, and Ansible.\n\u2022 Engineer corporate data repositories, RDBMS, NoSQL, distributes systems, DevOps, deployment, and orchestration.\n\u2022 Create big data frameworks to process petabytes of data through push architecture receiving two trillion messages a day.\n\u2022 Responsible for data governance, enterprise disaster recovery, and enterprise archive for all business systems and processes.\n\u2022 Organize a durable, reliable, accurate, and accessible data lake to facilitate a single source of truth for any environment.\n\u2022 Manage priorities and solutions in a continuous discovery and continuous delivery model to enhance user experience.', u'Director Data Management\nCharles Schwab - Austin, TX\nJune 2016 to June 2017\n\u2022 Direct all efforts to produce effective analytical architectures, semantic layers, info layers, repositories, data hubs, and data lakes.\n\u2022 Drive multiple projects in parallel, sourced by petabytes of data, that enables speed to business insights.\n\u2022 Budget, plan, coordinate, and prioritize business requirements to deliver agile solutions across all lanes of business.\n\u2022 Troubleshoot data quality issues, identifying root cause, analyzing a solution, and communicating the resolution.\n\u2022 Create and implement standard practices around data governance, data quality, lineage, dictionaries, and automatic validation.\n\u2022 Proactively engage with business partners to define needs, efforts, use cases, and introduce new data asset opportunities.', u'Senior Manager Data Engineering\nExtra Space Storage - Salt Lake City, UT\nJune 2013 to June 2016\n\u2022 Responsible for overall success and growth of business intelligence, big data, and data analytics efforts enterprise wide.\n\u2022 Architected infrastructure for the EDW, OLTP, Data Science, and Marketing Platform which includes 50 terabytes of data.\n\u2022 Modeled the data warehouse by implementing Ralph Kimball and Bill Inmon methodologies for over 20 data marts.\n\u2022 Derived insights by leveraging big data environments as well as live operational data science processes.\n\u2022 Maintained and created over 200 ETL jobs by using SQL Server, SSIS, SAP Data Services, and Pentaho Data Integrator.\n\u2022 Provide expert guidance for future data growth and processing needs including scale up and out solutions.', u'Data Engineer\nEmployer Solutions Group - Salt Lake City, UT\nJune 2012 to June 2013\n\u2022 Modeled multiple data marts by building and maintaining ETL pipelines in Oracle, SAP Data Services, and SSIS.\n\u2022 Developed SAP BO Universes, SSAS tabular models, OLAP SSAS cubes, interactive dashboards, and KPI reports.\n\u2022 Displayed expert SQL, PL/SQL, and T-SQL skills by improving query performance to process data in seconds and minutes.', u'Manager Banking Systems\nCaterpillar - Solar Turbines - San Diego, CA\nAugust 2011 to August 2012\n\u2022 Managed an international team of 15 to execute banking and cash operations in over 110 countries and 800 accounts.\n\u2022 Analyzed cash flow operations with Excel reports, PowerPivot reports, and PowerView dashboards for the executive team.\n\u2022 Configured over 600 user profiles with payment controls, reporting, risk management, role security, and major project financing.']","[u'Master of Science in Predictive Analytics in Predictive Analytics', u'Bachelor of Science in Financial Economics in Financial Economics']","[u'Northwestern University Chicago, IL\nSeptember 2014 to December 2020', u'Brigham Young University Rexburg, ID\nJanuary 2009 to July 2011']"
0,https://resumes.indeed.com/resume/408d1f6080d8f03c,"[u'Data Migration/Network Engineer\nBusiness Computer Technicians (BCT) - Tukwila, WA\nNovember 2015 to Present\n\u2022 Design, operate and troubleshoot the IP network.\n\u2022 Migrating Microsoft Access and Microsoft SQL Server databases to MYSQL.\n\u2022 Linux Shell scripting for backup, deployment, maintenance, and data loss prevention purposes.\n\u2022 Desing and Implementing advanced automation and orchestration solutions using Jenkins and Docker.\n\u2022 Maintain and operating the Asterisk call manager servers.\n\u2022 Writing PHP scripts to automate database migration processes.\n\u2022 Designing Web-based application to facilitate routers configurations based Laravel platform\n\u2022 Behavioral Application testing based on Behat and selenium grids.', u'Network Engineer\nIranian Offshore Oil Company (IOOC)\nMarch 2007 to March 2013\nBushehr, Iran\n\n\u2022 Design and implementing wireless networks (Point-to-point, point-to-multipoint)\n\u2022 Design and implementing fiber optical networks.\n\u2022 Design, operate and troubleshooting the IP backbone network.\n\u2022 Design and programming web-based PHP applications for ticketing system and request management.\n\u2022 Designing network security solutions for the backbone network.']",[u'Bachelor of Science'],[u'Azad University of Iran\nJanuary 2005 to January 2009']
0,https://resumes.indeed.com/resume/81a930d801477fab,"[u""Machine Learning/Data Scientist\nCanon - Jamesburg, NJ\nJune 2017 to Present\nDescription:Canon U.S.A., Inc. is a leading provider of consumer, business-to-business, and industrial digital imaging solutions to the United States and to Latin America and the Caribbean markets.\n\nResponsibilities:\n\u2756 Assisted the project with Python programming, coding and running QA on the same from time to time.\n\u2756 As an Architect design conceptual, logical and physical models using Erwin and build datamarts using hybrid Inmon and Kimball DW methodologies.\n\u2756 Exploratory data analysis using R to deep dive into internal and external data to diagnose areas of improvement to increase efficiency\n\u2756 Utilized Boosted Decision Tree, Linear and Bayesian Linear Regression Machine Learning models in Microsoft Azure to develop and implement interactive Webservice predictive models\n\u2756 Lead the company's machine learning and statistical modeling effort including building predictive models and generate data products to support customer segmentation, product recommendation and allocation planning; prototyping and experimenting ML/DL algorithms and integrating into production system for different business needs.\n\u2756 Acquired and cleaned using Talend and structure data from multiple source including external and internal databases.\n\u2756 Implemented end-to-end systems for DataAnalytics, DataAutomation and integrated with custom visualization tools using R, Mahout, Hadoop and MongoDB.\n\u2756 Worked with several R packages including knitr, dplyr, SparkR, CausalInfer, spacetime.\n\u2756 Worked with Data Governance, Data quality, data lineage, Data architect to design various models and processes.\n\u2756 Perform a proper EDA, Univariate and bivariateanalysis to understand the intrinsic effect/combined effects.\n\u2756 Created SQLscripts and analyzed the data in MS Access/Excel and Worked on SQL and SAS script mapping.\n\u2756 Responsible for design and development of Python programs to prepare transform and harmonize data sets in preparation for modeling.\n\u2756 Handled importing data from various data sources(GOVWIN), performed transformations using Spark, and loaded data into HDFS.\n\u2756 Interaction with Business Analyst, SMEs and other Data Architects to understand Business needs and functionality for various project solutions.\n\u2756 Created visualization and trends for Government spending on Defense DoD, Navy DoN, FDA, DOA, NIH, etc.\n\u2756 Worked on different data formats such as JSON, XML and performed machine learning algorithms in Python.\n\u2756 Work independently or collaboratively throughout the complete analytics project lifecycle including data extraction/preparation, design and implementation of scalable machine learning analysis and solutions, and documentation of results.\n\u2756 Developed and maintained data dictionary to create metadata reports for technical and business purpose.\n\u2756 Involved with Data Analysis Primarily Identifying Data Sets, Source Data, Source Meta Data, Data Definitions and Data Formats\n\u2756 Designed data models and data flow diagrams using Erwin and MSVisio.\n\u2756 Independently coded new programs and designed Tables to load and test the program effectively for the given POC's using with Big Data/Hadoop.\n\u2756 Developed, Implemented & Maintained the Conceptual, Logical&Physical Data Models using Erwin for Forward/ReverseEngineered Databases.\n\u2756 As an Architect implemented MDM hub to provide clean, consistent data for a SOA implementation.\n\u2756 Lead the development and presentation of a dataanalytics data-hub prototype with the help of the other members of the emerging solutions team\n\u2756 Established Data architecture strategy, best practices, standards, and roadmaps.\n\u2756 Worked with Hadoop ecosystem covering HDFS, HBase, YARN and MapReduce.\n\u2756 Involved in business process modeling using UML.\n\u2756 Performed datacleaning and imputation of missing values using R.\n\nEnvironment: Teradata 13.1, Informatica 6.2.1, Ab Initio, Business Objects, Oracle 9i, PL/SQL, Microsoft Office Suite (Excel, Vlookup, Pivot, Access, PowerPoint), Visio, VBA, Micro Strategy, Tableau, ERWIN, Machine Learning, Microsoft Azure."", u'Data Scientist/ Machine Learning Engineer\nAmerican Family Insurance - Madison, WI\nMarch 2016 to May 2017\nDescription:American Family Insurance, also abbreviated as AmFam, is a private mutual company that focuses on property, casualty, and auto insurance, and offers commercial insurance, life, health, and homeowners coverage as well as investment and retirement-planning products.\n\nResponsibilities:\n\u2756 Developed the logical data models and physical data models that confine existing condition/potential status data fundamentals and data flows using ERStudio\n\u2756 Reviewed and implemented the naming standards for the entities, attributes, alternate keys, and primary keys for the logical model.\n\u2756 Responsible for performing Machine-learning techniques regression/classification to predict the outcomes.\n\u2756 Used Python and Spark to implement different machine learning algorithms including Generalized Linear Model, SVM, Random Forest, Boosting and Neural Network\n\u2756 Build Machine Learning models in predicting product quality as early as possible\n\u2756 Created the conceptualmodel for the datawarehouse using Erwindatamodeling tool.\n\u2756 Used External Loaders like Multi-Load, TPump and FastLoad to load data into Oracle and Database analysis, development, testing, implementation and deployment.\n\u2756 Used R, Python and Spark to develop variety of models and algorithms for analytic purposes\n\u2756 Performed Multinomial Logistic Regression, Random forest, Decision Tree, SVM to classify package is going to deliver on time for the new route.\n\u2756 Performed data analysis by using Hive to retrieve the data from Hadoop cluster, SQL to retrieve data from Oracle database.\n\u2756 Utilized Spark, Scala, Hadoop, HBase, Kafka, Spark Streaming, MLLib, python, a broad variety of machine learning methods including classifications, regressions, dimensionally reduction etc.\n\u2756 Used Natural Language Processing (NLP) for response modeling and fraud detection efforts for credit cards.\n\u2756 Prepared multi-class classification data for modeling using one hot encoding, applied unsupervised and supervised learning methods in analyzing high-dimensional data.\n\u2756 Implemented public segmentation using unsupervised machinelearning algorithms by implementing k-means algorithm using Pyspark.\n\u2756 Explored and Extracted data from source XML in HDFS, preparing data for exploratory analysis using data munging.\n\u2756 Design and model the reporting data warehouse considering current and future reportingrequirement\n\u2756 Worked with data compliance teams, Data governance team to maintain data models, Metadata, Data Dictionaries; define source fields and its definitions.\n\u2756 Created stored procedures using PL/SQL and tuned the databases and backend process.\n\u2756 Worked with Data Scientist in order to create a Data marts for data science specific functions.\n\u2756 Performed data analysis and data profiling using complex SQL on various sources systems including Teradata, SQLServer.\n\u2756 Involved in analysis of Businessrequirement, Design and Development of Highlevel and Low-leveldesigns, Unit and Integration testing.\n\nEnvironment: Erwin 8, Teradata 13, SQL Server 2008, Oracle 9i, SQL*Loader, PL/SQL, ODS, OLAP, OLTP, SSAS, Informatica Power Center 8.1, Spark, Scala, Hadoop, HBase.', u""Data Scientist\nJP Morgan - San Antonio, TX\nNovember 2014 to February 2016\nDescription: JPMorgan Chase & Co. is an American multinational banking and financial services holding company headquartered in New York City. It is the largest bank in the United States, the world's sixth largest bank by total assets, with total assets of US$2.5 trillion, and the world's second most valuable bank by market capitalization\n\nResponsibilities:\n\u2756 Worked as a Data Modeler/Analyst to generate Data Models using Erwin and developed relational database system.\n\u2756 Analyzed the business requirements of the project by studying the Business Requirement Specification document.\n\u2756 Extensively worked on DataModeling tools ErwinDataModeler to design the datamodels.\n\u2756 Designedmapping to process the incremental changes that exists in the source table. Whenever source data elements were missing in source tables, these were modified/added inconsistency with third normal form based OLTP source database.\n\u2756 Designed tables and implemented the naming conventions for Logical and PhysicalData Models in Erwin 7.0.\n\u2756 Designedlogical and physical data models for multiple OLTP and Analytic applications.\n\u2756 Extensively used the Erwin design tool &Erwin model manager to create and maintain the DataMart.\n\u2756 Wrote simple and advanced SQLqueries and scripts to create standard and adhoc reports for senior managers.\n\u2756 Collaborated the data mapping document from source to target and the data quality assessments for the source data.\n\u2756 Identified and targeted welfare high-risk groups with Machine learning algorithms.\n\u2756 Conducted campaigns and run real-time trials to determine what works fast and track the impact of different initiatives.\n\u2756 Used Graphical Entity-Relationship Diagramming to create new database design via easy to use, graphical interface.\n\u2756 Created multiple custom SQLqueries in TeradataSQL Workbench to prepare the right data sets for Tableau dashboards\n\u2756 Perform analyses such as regression analysis, logistic regression, discriminant analysis, cluster analysis using SAS programming.\n\u2756 Used Expert level understanding of different databases in combinations for Data extraction and loading, joiningdata extracted from different databases and loading to a specific database.\n\u2756 Co-ordinate with various business users, stakeholders and SME to get Functional expertise, design and business test scenarios review, UAT participation and validation of financial data.\n\nEnvironment: Erwin r7.0, SQL Server 2000/2005, Windows XP/NT/2000, Oracle 8i/9i, MS-DTS, UML, UAT, SQL Loader, OOD, OLTP, PL/SQL, MS Visio, Informatica."", u'Data Scientist/Data Modeler\nBetchel corporation - San Jose, CA\nMarch 2013 to October 2014\nDescription: Bechtel is one of the most respected global engineering, construction, and project management companies. Together with our customers, we deliver landmark projects that create long-term progress and economic growth.\n\nResponsibilities:\n\u2756 Configured the project on WebSphere 6.1 application servers\n\u2756 Communicated with other Health Care info by using Web Services with the help of SOAP, WSDL JAX-RPC\n\u2756 Deployed GUI pages by using JSP, JSTL, HTML, DHTML, XHTML, CSS, JavaScript, AJAX\n\u2756 Used SAX and DOM parsers to parse the raw XML documents\n\u2756 Used RAD as Development IDE for web applications.\n\u2756 Translated business requirements into working logical and physical data models for Staging, Operational Data Store and Data marts applications.\n\u2756 Implemented metadata management as one part of data governance. Worked with the business users to populate business glossary.\n\u2756 Imported model to business glossary and performed integration of the technical and business terms.\n\u2756 Maintenance in the testing team for System testing/Integration/UAT\n\u2756 Used Log4J logging framework to write Log messages with various levels.\n\u2756 Conducted Design reviews and Technical reviews with other project stakeholders.\n\u2756 Implemented Microsoft Visio and Rational Rose for designing the Use Case Diagrams, Class model, Sequence diagrams, and Activity diagrams for SDLC process of the application\n\u2756 Work on SQL Data warehouse using Azure for designing services to handle computational and data-intensive queries in database.\n\u2756 Reverse Engineered existing Relational and data vault database systems as there were no existing data models for them.\n\u2756 Created test plan documents for all back-end database modules\n\u2756 Guaranteeing quality in the deliverables.\n\u2756 Implemented the project in Linux environment.\n\nEnvironment: R 3.0, Erwin 9.5, Tableau 8.0, MDM, QlikView, MLlib, PL/SQL, HDFS, Teradata 14.1, JSON, HADOOP (HDFS), Map Reduce, Pig, Spark, R Studio, MAHOUT, JAVA, HIVE, AWS.', u""Business Analyst /Data Analyst\nICICI Bank - Hyderabad, Telangana\nOctober 2011 to February 2013\nDescription: ICICI Bank, stands for Industrial Credit and Investment Corporation of India, is an Indian multinational banking and financial services company headquartered in Mumbai, Maharashtra, India, with its registered office in Vadodara. In 2017, it is the third largest bank in India in terms of assets and third in term of market capitalization.\n\nResponsibilities:\n\u2756 Analyze business information requirements and model class diagrams and/or conceptual domain models.\n\u2756 Managed the project requirements, documents and use cases by IBM Rational RequisitePro.\n\u2756 Assisted in building an Integrated logical data design, propose physical database design for building the data mart.\n\u2756 Gather & Review Customer Information Requirements for OLAP and building the data mart.\n\u2756 Responsible for defining the key identifiers for each mapping/interface\n\u2756 Responsible for defining the functional requirement documents for each source to target interface.\n\u2756 Coordinated meetings with vendors to define requirements and system interaction agreement documentation between client and vendor system.\n\u2756 Enterprise Metadata Library with any changes or updates.\n\u2756 Document data quality and traceability documents for each source interface.\n\u2756 Performed document analysis involving creation of Use Cases and Use Case narrations using Microsoft Visio, in order to present the efficiency of the gathered requirements.\n\u2756 Analyzed business process workflows and assisted in the development of ETL procedures for mapping data from source to target systems.\n\u2756 Worked with BTEQ to submit SQL statements, import and export data, and generate reports in Terra-data.\n\u2756 Worked with ETL Developers in creating External Batches to execute mappings, Mapplets using Informatica workflow designer to integrate Shire's data from varied sources like Oracle, DB2, flat files and SQL databases and loaded into landing tables of Informatica MDM Hub.\n\u2756 Responsible for full data loads from production to AWS Redshift staging environment.\n\u2756 Responsible for creating Hive tables, loading data and writing hive queries.\n\u2756 Calculated and analyzed claims data for provider incentive and supplemental benefit analysis using Microsoft Access and Oracle SQL.\n\u2756 Establish standards of procedures.\n\u2756 Generate weekly and monthly asset inventory reports.\n\u2756 Document all data mapping and transformation processes in the Functional Design documents based on the business requirements\n\nEnvironment: SQL Server 2008R2/2005 Enterprise, SSRS, SSIS, Crystal Reports, Windows Enterprise Server 2000, DTS, SQL Profiler, and Query Analyzer."", u""Data Analyst\nHindustan Zinc Limited - Udaipur, Rajasthan\nApril 2009 to September 2011\nDescription: Hindustan Zinc is a Vedanta Group company in zinc, lead and silver business. We are one of the world's largest integrated producers of zinc and are among leading global lead and silver producers. We are one of the lowest cost producers in the world and are well placed to serve the growing demand of Asian countries.\n\nResponsibilities:\n\u2756 Developed the logical data models and physical data models that confine existing condition/potential status data fundamentals and data flows using ER Studio\n\u2756 Involved in analysis of Business requirement, Design and Development of High level and Low-level designs, Unit and Integration testing\n\u2756 Reviewed and implemented the naming standards for the entities, attributes, alternate keys, and primary keys for the logical model.\n\u2756 Performed data analysis and data profiling using complex SQL on various sources systems including Teradata, SQL Server.\n\u2756 Designed, Build the Dimensions, cubes with star schema and Snow Flake Schema using SQL Server Analysis Services (SSAS).\n\u2756 Created the conceptual model for the data warehouse using Erwin data modeling tool.\n\u2756 Worked with Data Scientist in order to create a Data marts for data science specific functions.\n\u2756 Determined data rules and conducted Logical and Physical design reviews with business analysts, developers and DBAs.\n\u2756 Translate business and data requirements into Logical data models in support of Enterprise DataModels, ODS, OLAP, OLTP, Operational Data Structures and Analytical systems.\n\nEnvironment: Erwin r9.0, Informatica 9.0, ODS, OLTP, Oracle 10g, Hive, OLAP, DB2, Metadata, MS Excel, Mainframes MS Visio, Rational Rose, Requisite Pro, Hadoop, PL/SQL, etc.""]","[u""Bachelor's""]",[u'']
0,https://resumes.indeed.com/resume/cf0d80e6c5f94ba0,"[u'Software Engineer, Intern\nAnt Financial\nJune 2017 to August 2017\n\u2022 Worked in an Agile team of 4 engineers developing a web application for internal data management.\n\n\u2022 Designed and built the modularized, performant User Interface using vue.js framework.\n\n\u2022 Designed and implemented a CLI tool for efficient indexing and cross-referencing search in a massive code\n\ndata expanding OpenGrok in Python.\n\u2022 Carried out graphical processing of Realtime chart by means of e-chart for clearer and easier information\n\npresentation.\n\u2022 Used eslint to modify the code format and learnt the usage of Git.', u'Data Engineer, Intern\nJezoe Technology\nJune 2016 to August 2016\n\u2022 Grasped the LinkedIn website and analyzed the data using Spark to provide supports for the teams\n\n\u2022 Complete auto JSON generator in Excel and visualize part of the Json file in R using ggplot\n\nSelected Projects']","[u'M.S.E in Computer Science', u'B.S.E in Business Analytics Information and Technology']","[u'New York University New York, NY\nSeptember 2017 to Present', u'Rutgers University New Brunswick, NJ\nSeptember 2013 to March 2017']"
0,https://resumes.indeed.com/resume/3dd68e73e4d8ead7,"[u'Financial Engineer\nFreddie Mac - Federal Home Loan Mortgage Corporation - Washington, DC\nFebruary 2017 to Present\n\u2022 Built and supported trading risk management system in Investments and Capital Markets divisions for MBS trading and hedging\n\u2022 Performed and supported waterfall daily/weekly/monthly pricing run, maintained daily trading risk report run for portfolio\ndashboard, calculated fixed income analytics and PnL for traders and accounting team\n\u2022 Created dashboard monitor using Python, Java, HTML and JavaScript and sent automatic result email. Generated additional close\nprices by using VBA in Outlook, provided data access advisory for trading team and quant team using SQL\n\u2022 Developed regression test suite for Key Issues report, Portfolio2 report, LnC dashboard, Dashboard View and Expected daily\npricing run by Java, Slim and Shell script. Tested regression suite in Linux and windows environment using gradle, git\n\u2022 Addressed and analyzed production and non-prod issue using Excel pivot table and VBA by working with FE, business\nengineering, databases, trade capture and Nimble DevOps team\n\u2022 Prepared train release every 2 weeks, wrote and enhanced code for trading risk system cover by JUnit test, integration test', u'Data Analyst Intern\nPrimeAlpha Technologies - New York, NY\nMay 2016 to August 2016\n\u2022 Maintained database/data system which stores more than 300 hedge funds performance data from 36 countries\n\u2022 Tested front-end of website, updated managers and investors data based on DDQ and cleaned unstructured data\n\u2022 Interpreted trading model and analyzed portfolio management strategy using statistical techniques\n\u2022 Created and executed test cases on Net Return Script automation tool']","[u'MS in Financial Engineering', u'in Computer Science', u'BS in Computer Science in Software Engineering']","[u'New York University Tandon School of Engineering New York, NY\nJanuary 2017', u'University at Buffalo, the State University of New York Buffalo, NY\nSeptember 2013 to May 2014', u'Beijing University of Technology Beijing, CN\nMay 2013']"
0,https://resumes.indeed.com/resume/5309dcf81e77a810,"[u""Senior Database Administrator/ Systems Engineer\nSSAI\nApril 2009 to Present\n\u2022 Work with developers, system stakeholders and IT security professionals as a team for a nation's mission critical satellite communication system and lead database administration, systems analysis, monitoring and troubleshooting. Ensure the integrity, availability and functionality of the operational systems that deliver no latency real time data to the scientific community\n\u2022 Manage and administrate systems database, including: database installation and configuration; profiling and cleansing; performance tuning; stored procedure development and debugging; database backup and restore\n\u2022 Develop custom application based on unique business logics to improve data collection/ distribution, and automate critical and tedious management tasks\n\u2022 Migrate legacy systems database from Oracle to SQL Server, including: rewriting code, stored procedures, and triggers accordingly; creating new database connections; security and user access control\n\u2022 Create routine and ad hoc system status reports for management and users to make informed decisions\n\u2022 Create and maintain organization annual meeting websites"", u'Data Architect\nBeeryRio\nJuly 2007 to December 2008\n\u2022 Analyzed and visualized customer data to provide valuable and actionable insights for proposals\n\u2022 Created and developed Guidelines to ensure quality assurance compliance\n\u2022 Produced support for the completion of projects within budget and deadlines']","[u'MS in Information Systems in Information Systems', u'BS in Architecture']","[u'University of Maryland Baltimore, MD', u'Beijing University of Civil Engineering & Architecture']"
0,https://resumes.indeed.com/resume/ae7b8e0ac97a6083,"[u'Data Engineer\nSyntel ltd - Pune, Maharashtra\nDecember 2013 to July 2017\n\u25cf Worked on various phases of Product development life Cycle in variety of technical areas like Design, development, unit testing and production support\n\u25cf Developed end-to-end data pipelines which include data extraction, data ingestion, publishing data on Tableau server,and automation of pipelines under fast paced Agile Scrum environment\n\u25cf Imported data from various data sources, performed transformations using Hive, MapReduce, loaded data into HDFS and extracted the data from MySQL into HDFS using SQOOP\n\u25cf Exported the analyzed data to the relational databases using SQOOP for visualization and to generate reports for the BI team. Advanced SQOOP scripts for moving data between Relational database, HDFS, and S3 storage\n\u25cf Optimized Hive pipelines in Data lake by implementing Partitioning, and bucketing concepts for improving performance. Accomplished and Improved performance of Spark Pipelines by implementing repartition to manage resources efficiently\n\u25cf Established, tested, and validated pricing model with 93% confidence for predicting margin based on numerous metrics by leveraging Pandas, NumPy, SciPy. Achieved visualizations by performing statistical analysis on pricing data for exploring various patterns, correlations, and covariance based on different metrics\n\u25cf Cultivated Tableau dashboards by leveraging published data sources from Tableau server. Established Tableau Server administrative tasks by creating AD groups, managed subscriptions for the views, adding users, managing security, and tracking performance of dashboards.', u'Data Engineer\nSyntel Ltd - Pune, Maharashtra\nDecember 2013 to July 2017']",[u'M.S. in Computer Science'],"[u'Illinois Institute of Technology Chicago, IL\nSeptember 2017 to Present']"
0,https://resumes.indeed.com/resume/cc1c40ea689c55cf,"[u'Software Engineer Intern/Data Analyst\nNIIT Technology Ltd\nJanuary 2016 to December 2016\n* Designed a Hospital Management System using C++ and Oracle during internship period.\n* Conducted data preparation, outlier detection, data manipulation, transformation, and cleansing using R language.\n* Designed and developed a Spam Detection tool using Machine Learning Techniques and Python\n* Effectively interpreted the trends or patterns in the data sets by visualizing through scatter plots, bar plots and various other statistical techniques using Python.\n* Create and maintain detailed and customizable data analysis, business intelligence, and visualization reports/tools to support client requirements and requests.\n* Review, continuously monitor and ensure data consistency across projects, and initiatives across standards and formats through the delivery life cycle.\n* Create and maintain reports, tables, dashboards, and other data visualizations of project- and service-related metrics through expert-level use of tools including Tableau, Microsoft Excel, and internal client utilities and tools']","[u'Master of Science in Information Technology Management in Data Analytics', u'Masters in Computer Application in Computer Application', u""Bachelor's in Computer Application in Computer Application""]","[u'GOLDEN GATE UNIVERSITY San Francisco, CA\nJanuary 2017 to January 2018', u'VIVEKANANDA INSTITUTE FOR PROFESSIONAL STUDIES Delhi, Delhi\nJanuary 2013 to January 2016', u'SIRIFORT COLLEGE OF COMPUTER TECHNOLOGY AND MANAGEMENT Delhi, Delhi\nJanuary 2010 to January 2013']"
0,https://resumes.indeed.com/resume/6870945030200177,"[u'ESG DATA ANALYST\nCompuSystems, Inc - Downers Grove, IL\nJuly 2015 to Present\n\u2022 Maintain data integrity for trade show\n\u2022 Create HTML Emails for Show Management\n\u2022 Setup and test badge map files for specialized shows to ensure devices scan and record information correctly\n\u2022 Troubleshoot technical problems alongside a small team of people\n\u2022 Setup and maintain online ordering sites and related database items for the exhibitor services group\n\u2022 Assist with Field Engineer duties as needed', u'FIELD ENGINEER\nCompuSystems, Inc - Downers Grove, IL\nJanuary 2014 to July 2015\n\u2022 Setup and maintain local provide networks for trade show registration\n\u2022 Setup and maintain onsite servers\n\u2022 Program and test devices for Exhibitor Lead Retrieval\n\u2022 Troubleshoot onsite issues', u""QMC TECHNICIAN\nKens Beverage, Inc - Plainfield, IL\nJuly 2012 to January 2014\n\u2022 Test and calibrate the beverage system at McDonald's stores\n\u2022 Repair the equipment in a timely manner as needed\n\u2022 Report any and all issues that need to be address to Coca-Cola and to Store Management""]",[u'ASSOCIATES OF APPLIED SCIENCE in APPLIED SCIENCE'],"[u'DeVry University Addison, IL\nJanuary 2010 to December 2013']"
0,https://resumes.indeed.com/resume/7761ba988379ce0a,"[u""Data Analyst\nBandwidthX - Carlsbad, CA\nJune 2016 to Present\n\u25cf Track and monitor the data usage of the customers from the internet service providers and analyze it to enhance the quality of the internet connection at certain access points\n\u25cf Scrutinize transactional and locational data of the users to show the coverage of the company's solutions.\n\u25cf Design ad-hoc queries to address analytical insights for the business to better understand the mobile customer behaviors\n\u25cf Perform data cleaning and cross validation, and present the data anomalies in the database to ensure the quality and accuracy of the data\n\u25cf Develop internal reports to exhibit patterns in the growing user base\n\u25cf Construct and manage external customer reports with KPIs using Tableau"", u'Software Engineer Intern\nPioneer Inc - Seoul, KR\nJuly 2015 to August 2015\n\u25cf Collaborate to support different types of management systems and resource planning for corporations\n\u25cf Develop a sales management solution to small vendors\n\u25cf Participate in the development of inventory tracking system']","[u'B.S. in Statistics in Statistics', u'B.S. in Economics in Economics']","[u'University of California Los Angeles, CA\nSeptember 2012 to March 2016', u'University of California Los Angeles, CA\nSeptember 2012 to March 2016']"
0,https://resumes.indeed.com/resume/388c7293f6f3ecf8,"[u'NETWORK ENGINEER\nMarch 2017 to Present\n-Configuring and upgrading Nexus 2k, 5k, 7k, 9k (core switches) and 3800s, 4500s (access\nswitches)\n-Providing L2 and L3 escalated support to 1000 plus users per migration\n-Authenticate and authorize end-devices using Cisco Identity Service Engine (ISE)\n-Troubleshoot any network related issues\n-Monitoring network activity using Solarwinds\n-Creating and updating nodes on Solarwinds\n-Using infoBlox to create networks for different sites and assign DHCP ranges\n-Assisting my team to configure DHCP on hospital devices such as IP phones, Zebra\nwristband, label printers, HP printers, MRV 1000, and other critical devices\n-On call escalate troubleshooting\n-Revisiting completed network closets to ensure proper device functionality', u'DATA CENTER TECHNICIAN, ZAYO\nDell\nSeptember 2016 to March 2017\nTroubleshoot servers, switches, and client systems\nBroad knowledge of HP, Dell, IBM, Cisco, EMC and NetAPP servers\nConfiguring servers\nknowledge of EIGRP, OSPF, DNS and DHCP\nSetting up Vlans and activating ports on switches\nSupporting multiples datacenters over 10000+ servers\nAble to replace dimm, hard drive, system board/ blade, battery/ power supply on servers\nAble to set up ILO on HP servers and iDRAC on Dell servers\nPower cycling/ power drain of customer equipment\nDiagnosis hardware and software on servers\nProvided technical support for internal customers\nRack and stack, cabinet setup, and power strip installation\nInstallation and termination of Fiber and Coppers\nMonitor, maintain and fix servers, operating systems, network configurations, software\napplications']",[u'Associates in Information Systems Technology'],[u'NORTHERN VIRGINIA COMMUNITY COLLEGE\nJanuary 2017']
0,https://resumes.indeed.com/resume/21f348b78796b236,"[u'Data engineer intern\nSamsung R&D Institute - Beijing\nMay 2017 to August 2017\nMade person profile about APP part for Bixby (virtual assistant of cellphone).\n\uf09e Collected the App information from three APP store by web crawler, and analyze the data simply.\n\uf09e Used methods of classification models (SVM, RNN, fastText) to classify APP and game, and analyze the performance.\n\uf09e Add the tags to each category, design some strategies for recommender system.', u'Researcher intern\nPeking University - Beijing\nNovember 2015 to February 2016\nStudied the deep learning by Caffe.\n\uf09e Built the Caffe platform, and analyze the Caffe code to learn inner mechanism.\n\uf09e Ran models of Imagenet classification, and analyze the performance.\n\uf09e Used the Caffe to make other deep learning study.']","[u'Master in Computer Science in Computer Science', u'B.S. in Computer Science & Technology in NLP']","[u'The George Washington University Washington, DC\nSeptember 2016 to Present', u'Beijing Jiaotong University Beijing, CN\nSeptember 2012 to July 2016']"
0,https://resumes.indeed.com/resume/8a787ab6464ec785,"[u'Data Center Engineer\nWells Fargo N.B\nJanuary 2006 to Present\n\u2022 Participated in updating of computer room to current environmental requirements, including hot isle containment, sealed for fire suppression and rewired networking racks, raising floor temperature to reduce energy consumption.\n\u2022 Oversaw Infrastructure Technology communications between teams in different areas of the company to be monitored for smooth completion.\n\u2022 Assisted in troubleshooting any issues stemming from computer room with result of less issues and smoother communications, including war room coordination when needed to resolve and assess damage.\n\u2022 Condensed racks, refreshed processes, and eliminated excess equipment and wires.\n\u2022 Coordinated replacement of mainframe, sending old mainframe to Disaster Recovery location. \u2022 Participated in automation of climate control monitored 24/7.\n\u2022 Monitored and reviewed weekly change control meetings to assure smooth operation, resulting in improved relations between IT and customer.\n\u2022 Accountable for 5000 server racked installations, network cabling and electrical connections in a 100,000 sq. ft. Data Center.\n\u2022 Implemented, Tracked and reported Data Center items including server installation change requests, server issues, scheduled network outages, IMAC, backup library outages, Power request via Remedy, STSP & pac2k\n\u2022 Maintained Netzoom for asset locations, cable maps, switch port assignments, server administration, shipping locations to reflect new server installation requirements and old server decommissions.\n\u2022 Sarbanes-Oxley compliance.', u""Networking Engineer\nDMZ Engineering - Union, N.B, CA\nJanuary 2000 to January 2006\nResponsible for integrating merger related equipment and Philosophies. Designed solutions for Extranet communications. Employee Dial in solutions, Maintained the Core and all edge devices. Wrote and maintain all Nokia Firewalls running Checkpoint Rules for incoming and outgoing communications. Used VIPS in Foundry switches for Load Balancing. Engineered and implemented a new Parallel Core across the footprint using Cisco 6500 as L2/L3 Engines. Devised New BGP AS's, OSPF areas. Design, Implement, and Support Internet inbound/outbound networks. Documentation of policies and procedures. Performing monitoring, troubleshooting and resolution of issues involving all systems, databases, applications, and networks within the Internet Channels and DMZ. Attending application development meetings for my team & providing input from a support/operations perspective. Charged with development and maintenance of platform configuration, internal documentation and drawing standards, engineering practices, and production checklists for the secure network environment.\n\u2022 Install and configure Checkpoint firewalls to enforce corporate security policy\n\u2022 Install, configure, and troubleshoot Nokia firewall appliances\n\u2022 Install, configure, and troubleshoot Cisco and Foundry routers and switches"", u'Network Engineer\nUnion, N.B, CA\nJanuary 1992 to January 2000\nResponsible for the Entire OS/2 Comm manger Footprint of Check processing. Maintained hardware and software revisions for each site in the footprint. We maintained Statement data, currency in flux, and adjustment data for ever client of the bank. I migrated the Check Processing off of Optical Storage using Microsoft NT drive mappings for an increase in cost effective storage space as well as 200% increase processing times. I was the sole Engineer to establish a new Check processing center in the Plaza Building In Philadelphia for the CoreStates Merger. I also trained the staff and wrote support documents.', u'Technician\nCentral Intelligence Agency\nJanuary 1990 to January 1992\nMost of my duties were classified as well as the equipment.', u""High Speed Data Specialist\nU.S. Army\nJanuary 1986 to January 1990\nRepair of High speed analog / Digital recorders. Watkins John Receivers, Demodulators, FFT's, Satellite base communications, High density Array antennas.""]",[u'Other'],[u'']
0,https://resumes.indeed.com/resume/6bd710bfaa70b045,"[u""Software Engineer/Data Specialist\nDirect Marketing Solutions\nApril 2016 to September 2017\nWorked with a team of developers to create data processing programs and Web applications using C#, JavaScript, and SQL. Gained\nexperience using Visual Studio and writing code in an MVC pattern. Also processed large amounts of data for clients' electronic and paper\nmailing campaigns. Learned to use Mercurial and Subversion repositories to back up code.\n\n\u25cf Partnered with Account Managers and other team members to ensure product quality for clients.\n\u25cf Developed cutting-edge applications to quickly process data.\n\u25cf Created normalized database schemas to store client data."", u""Software Engineer/Online Store Manager\nJune 2013 to September 2013\nSummer position as the creator of the web portion of a local pet store based in Silverton, Oregon. Also wrote company software using\nEclipse, Java, and Swing for the GUI component. This software allowed cashiers to log purchase records. Took photos of merchandise and edited them using Adobe Photoshop for posting online.\n\n\u25cf Partnered with owner and employees to determine the best way to build the website and advertise products.\n\u25cf Constructed company software to make employees' jobs easier.\n\u25cf Strengthened Web design skill and exercised graphic design knowledge learned in college.""]","[u'Bachelor of Science in Computer Science', u'Associate in General Studies']","[u'Western Oregon University Monmouth, OR\nSeptember 2013 to June 2016', u'Chemeketa Community College Salem, OR\nSeptember 2010 to June 2013']"
0,https://resumes.indeed.com/resume/9dacd00116795dcf,"[u'Data Engineering Intern\nCalifornia Design Den - San Ramon, CA\nSeptember 2017 to December 2017\n\u2022 Designed and implemented highly scalable data warehouse as a central repository on GCP for business intelligence and analytical solutions.\n\u2022 Built new ETL packages using Python and Talend Big Data Integration tool. New packages included detailed workflow of data imports from Oracle NetSuite ERP system.\n\u2022 Managed Google Cloud MySQL instance on Compute Engine, using Google Data Prep as a staging tool to feed data into the db.\n\u2022 Optimized SQL queries and wrote custom views to create rich interactive Tableau reports, assisting in implementation of dynamic pricing strategy across multiple clients.\n\u2022 Created dynamic dashboard for senior stakeholders assisting them in major financial and operational decisions.', u'Data Engineering Intern\nGobble - Palo Alto, CA\nMay 2017 to August 2017\n\u2022 Created data models, solution designs and managed data quality for cloud-based menu recommendation system.\n\u2022 Engineered data warehouse solution on AWS Redshift with S3 as the data lake, thus automating the complete process of menu creation on front end.\n\u2022 Developed ETL pipeline to structure, transform and cleanse data from multiple sources using Python and SparkSQL.\n\u2022 Leveraged cloud technologies such as AWS Lambda and EC2 to deploy the system in production environment.\n\u2022 Worked in an agile test-driven-development environment using tools like Git, Zube and Circle CI to achieve\ncontinuous integration, continuous deployment.', u'Technology Consultant\nAT&T - Bengaluru, Karnataka\nMarch 2011 to December 2015\n\u2022 Spearheaded multiple rounds of commercial room pilot with stakeholders to capture requirement and demo design functionality prior to network migration.\n\u2022 Planned and executed risk mitigation, rollback and contingency plan specific to each client to ensure availability, operability, and stability of all network assets.\n\u2022 Developed SQL scripts to fetch network compliance data and created a weekly audit report for senior leadership.\n\u2022 Performed quantitative analysis and provided recommendations on customer complaint date for MIS domain\nusing MS SQL and Tableau which improved customer satisfaction index by 6%.', u'Support Engineer\nIBM - Bengaluru, Karnataka\nJanuary 2010 to January 2011\nImplementation, maintenance and upgrading of Web Servers, DNS, DHCP, Hosting Controllers and Mail Servers.']","[u'M.S in Information Technology and Management', u'B. Tech in Engineering']","[u'The University of Texas at Dallas Dallas, TX\nDecember 2017', u'Punjab Technical University\nJuly 2009']"
0,https://resumes.indeed.com/resume/4b374d450d11962c,[u'Data Engineer\nFMTC - Remote\nPresent'],[],[]
0,https://resumes.indeed.com/resume/b5098182f4ef9972,"[u""Quality Systems Engineer\nWeatherford International - Houston, TX\nOctober 2014 to December 2016\nManaged quality programs that enhanced product design and services. Assisted in the return to vendor process; responsible for writing and updating standard operating procedures using technical writing skills. Maintained FMEA's, Process Control System, ISO 9001 related compliance regulations, auditing, and change control. Systems testing for verified data analysis of testing results. Comply and communicate with regulation agencies. Quality Management System (QMS) & related tools /processes to accelerate business results, as well as identify, measure and improve key performance indicators (KPIs) such as Cost of Poor Quality (COPQ) and on time delivery (OTD)."", u'Quality Engineer\nGeneral Electric Oil and Gas - Houston, TX\nSeptember 2013 to October 2014\nManage the document control activities for compiling Data Books including verifying documentation for accuracy, completeness, and ensuring all procedures have been fulfilled, and verifying procedures & processes. File and submit commercial Data Books to customers based on customer requirements; ensure product traceability. Develop and maintain a comprehensive filing system and computer database for all documentation. Reviewed Non-Destructive Testing (NDT) processes, Quality Control Plans (QCP) or Inspection and test plans (ITP), mechanical testing such as tensile, hardness, charpy, etc.', u'Manufacturing Supervisor\nHewlett Packard - Houston, TX\nFebruary 2013 to September 2013\n\u2022 Accountable for supply and demand activities for multiple product lines, ensuring that performance (inventory, cost, etc.) meets global and regional planning requirements.\n\u2022 Responsible for hiring, setting and monitoring of annual performance plans, coaching, and career development; ensures that the proper tools are in place to support the team and the processes\n\u2022 Ensures the delivery of products and services that meet the performance metrics defined by next level sales track record management', u'Quality Data Analyst\nEngine Management - China Lake, CA\nNovember 2009 to February 2013\nVX-31)\n\n\u2022 Expert with working and utilizing tracking databases using software.\n\u2022 Responsible for reporting to senior leaders using Microsoft Excel technologies.\n\u2022 Expert analyst ensuring engine configuration planning; responsible for ensuring all serialized\ncomponents and all inspections match the documentation and aircraft.\n\u2022 Monitoring and managing Quality Management Plan collaborating across business functions.\n\u2022 Successfully served as a liaison between the Administration department, Project Manager, and site\nmanager using data-driven models.', u'U.S. Navy - Lemoore, CA\nOctober 2002 to June 2007\n\u2022 Provided database expertise maintaining Oracle Lifecycle Management System retrieving, updating, and functioning as the administrator for several systems.\n\u2022 Successfully installed, configured, and upgraded desktop hardware UNIX and peripherals to include: network cards, printers, routers, mice and add-in boards.\n\u2022 Advised the Organization with the current information about information security technologies and related regulatory issues.']","[u'BACHELOR OF SCIENCE in (B.S.), BUSINESS MANAGEMENT']","[u'UNIVERSITY OF PHOENIX Ridgecrest, CA\nNovember 2012']"
0,https://resumes.indeed.com/resume/3119ccce2f2d5d6e,"[u'Data Analyst Intern\nIndustrial Information Resources\nSeptember 2017 to March 2018\n\u2022 Gathered market research from North America, Asia, Middle East and Oceania using strong verbal communication skills and attention to details\n\u2022 Validate, disseminate, and update global market intelligence about industrial plants and their spending activities around the world\n\u2022 Analyze historical data to provide actionable insight into trends and patterns that can be used to improve decision making and identify investment opportunities\n\u2022 Perform data maintenance activities such as data entry, data validation, data cleaning to ensure the integrity of the data being captured and analyzed', u'Enterprise Systems Engineer\nLayer3 Ltd\nDecember 2013 to December 2015\n\u2022 Researched vendors and performed Cost Benefit Analysis on leading opportunities to ensure investments were worthwhile\n\u2022 Designed and implemented call-center procedures for the support staff which improved response time by 30%\n\u2022 Collaborated with the Sales/Marketing team to identify potential business options and demonstrated with prototypes to get them on-board']","[u'Master of Science in Industrial in Industrial/Engineering Management', u'Bachelor of Engineering in Electrical and Electronics Engineering in Electrical and Electronics Engineering']","[u'University of Houston\nJanuary 2016', u'University of Agriculture\nApril 2008 to April 2013']"
0,https://resumes.indeed.com/resume/e8abd7920e08f7e1,"[u'Data Engineer\nGEOLOG International\nJanuary 2012 to January 2017\nAlgeria\nMissions and tasks:\n- Rig up/down mudlogging unit equipement in the well site ;\n- Monitoring the operations and drilling parameters during drilling ;\n- Reporting each performed operation and happened event in the well site ;\n- Preserve drilling and gas mesurement data as a different files ;\n- Evaluation and preserve geological sample ;\n- Maintenance, repairs and calibration of any faulty apparatus or sensor ;\n- Participation in safety meetings and reporting any unsafe act ;\n- Discussing any unexpected event with the supervisors and keep communicating.', u'Chemist\nSALD, Production\nJanuary 2011 to January 2012\nAlgeria\nMissions and tasks:\n- Pr\xe9paration of all kinds of detergents ;\n- Caracterisation of the product by performing tests and laboratory measurements ;\n- Quality control of the final product ;\n- Participation in developement of production process and maintenance of equipements.', u'Chemist\nSidi Bel Abbes\nJanuary 2006 to January 2011\nAlgeria Missions and tasks:\n- Responsable for bleach production unit.\n\nOffice (Word, Excel ), design, animation and simulation softwares (3DSMax, ).']","[u'', u'Diploma in Engineering']","[u'University S\xe2ad Dahlab\nJanuary 2012', u""University of M'Hamed Bouguerra\nJanuary 2006""]"
0,https://resumes.indeed.com/resume/916c34b525a0233d,"[u'Data Entry\nAlmond Diamond\nMarch 2017 to Present\nData enter', u'Marketing Sales\nNovember 2016 to November 2016', u'Asistente Administrativo\nMINSAP - Cuba, MO\nJanuary 2013 to January 2015\nDocumentation\nFiling\nMetting Support\nOrganization\nSupply Managment\nInventory Control', u'Engineer\nPROSA.SA Paper factory\nJanuary 2012 to January 2013\nTechnical Department\n\n\u2022 Energy efficiency\n\u2022 Organization of Processes\n\u2022 Quality Control']",[u'in Industrial Engineer'],"[u'University of Matanzas ""Camilo Cienfuegos""\nJanuary 2006 to January 2011']"
0,https://resumes.indeed.com/resume/baed55f57f47bdd9,"[u'Data Network Engineer\nNew York Community Bancorp\nJanuary 2014 to Present\n- Hands on experience with cabling and configuring Cisco routers and Cisco switches\n- Configure NAT, PAT, and ACLs to permit and deny IP traffic on the internet routers and ASA firewalls\n-Configure VLANs, port security, Port channels, Trunking on Cisco switches including Nexus Switches\n- Perform OS upgrades on Cisco routers and switches\n- Troubleshoot/work with vendor on site MPLS and DMVPN circuit/bandwidth issues\n-Configure network access servers and routers for AAA Security and Cisco ACS\n-Configure and manage users and devices in RSA, Cisco prime, ACS and NetQos\n-Create rules on riverbed to optimize or bypass network traffic\n-Set up captures in routers/sniffer and analyze packets in Wireshark\n-Cabling (Straight through cable, serial cable, rollover cable, crossover cable)', u'Voice and Data Network Engineer\nSouthern Westchester BOCES\nJanuary 2013 to January 2014\nConfiguring users, phones and voicemails in Call Manager and Cisco unity\n- Patch, rack and stack Cisco routers and switches and establish connectivity\n- Troubleshoot end user PC/switch connectivity issues\n- Visited clients to set up local area networks']",[u'Bachelor of Science in Computer Engineering in Computer Engineering'],"[u'Farmingdale State University Farmingdale, NY\nJanuary 2010 to January 2014']"
0,https://resumes.indeed.com/resume/c47304839482ed1e,"[u'Lead Data Engineer\nQuaero - Charlotte, NC\nJuly 2014 to Present\n\u2022 Lead data warehouse solutions for multiple clients. Focus includes but is not limited to: data model design, ETL processing, system integration, customer data matching and hygiene, data auditing and profiling, database tuning and scalability, and business intelligence reporting. Data ingestion and integration via behavioral data streams (Omniture, Doubleclick, Crosswise, Tapad, Google Analytics, etc.)\n\u2022 Lead technical estimations in level of effort for client change orders, and prospect proposals while assisting in the design and development. Translates high-level requirements into well-designed solutions that meet the business and technical goals.\n\u2022 Develops and maintains effective partnerships across the organization while leading and facilitating technical design and code walk-through sessions.', u'Database Engineer II\nQuaero - Charlotte, NC\nDecember 2011 to July 2014\n\u2022 Lead Data Quality related issues. Provide technical expertise and best practices for database design, development and deployment efforts, database support; including data load, performance tuning, job scheduling and monitoring.\n\u2022 Lead the development & support of data validation systems, metadata and ETL infrastructure with the help of tools like SSIS, SSRS, T-SQL.\n\u2022 Responsible for scoping, design and development of database enhancements.\n\u2022 Work collaboratively with team members to analyze current systems and recommend and implement changes to current systems. Mentor and provide technical leadership to junior team members', u'Database Engineer I\nQuaero CSG Systems - Charlotte, NC\nMay 2010 to December 2011\n\u2022 Lead functional requirements. Aiding others in data management, designing and developing database architectures.\n\u2022 Provide operations support to resolve issues proactively and with utmost urgency. Identify inefficiencies in current databases and implement improved solutions\n\u2022 Extend, improve and support SSIS processes including data imports (XML, csv, Excel). Extensive proficient development on SQL Server database environment']","[u'Master of Science in Technology in Technology', u'Bachelor of Science in Computer Engineering in Computer Engineering']","[u'Purdue University\nJanuary 2008 to December 2009', u'Purdue University\nAugust 2006 to December 2007']"
0,https://resumes.indeed.com/resume/ac069c2f1931d0da,"[u'Optimization Data Scientist\nMonsanto - St. Louis, MO\nAugust 2017 to March 2018\nHybrid field assignment to maximize the yield potential\n\u2022 Data extraction, cleaning, preparation, designed labeling technique, feature extraction and build a machine learning model using linear regression model to predict right order of the assignment in the field with an AUC > 0.83\n\u2022 Building an optimization framework for product assignment to fields of a grower based on operational constraints (R and CPLEX)\n\u2022 Decomposed the multi objective mixed integer model to decrease computational time from 3 hours to less than 10 minutes\n\u2022 Developed a optimization and machine learning framework for different growers based on the their past data and agricultural practices\n\u2022 Validation metrics to validate the framework by retro analysis, comparing previous assignment with the optimal assignment\n\u2022 Built a simulation model to randomly assign the products for validation of the optimization model\n\u2022 Used an automated framework to download the data from free online APIs\n\u2022 Clustering of the fields using hclust/kmeans based on the driving distance between them and built a heuristic for selecting number of clusters to be made.', u'Research Assistant\nNorthern Illinois University - DeKalb, IL\nAugust 2015 to May 2017\nThesis on Lagrangian Relaxation Approach to a Multi-Echelon Consolidation of Perishable Products\n\u2022 Developed a mixed integer programming (MIP) model for optimizing the transportation and consolidation of perishable products through a consolidation center\n\u2022 Optimized the number of FTL, LTL units to be used while transporting perishable products to reduce transportation cost, inventory cost and the deterioration cost\n\u2022 Developed the model in C++ using the concert technology in the IBM ILOG CPLEX 12.6 solver\n\u2022 Improved the performance of the model by reducing the computational time from 4 hours to 2 hours per experiment, using Lagrangian relaxation and a heuristic which simultaneously optimizes using Cplex solver and makes the model feasible using the heuristic.\n\u2022 Researched through the literature of various optimization techniques and supply chain techniques (e.g. inventory management, shipment consolidation, branch and bound algorithm, genetic algorithms, gradient decent etc.)', u""Senior Engineer\nOrient Cement Limited\nJuly 2012 to October 2014\nContinuous Improvement, Maintenance & Construction of 3-Million-ton Cement plant\n\u2022 Experienced working in project management, construction and erection of a new cement plant\n\u2022 Created, updated, tracked and circulated Master Supply & Delivery Schedule; Verified vendor's delivery time lines with respect to construction schedule; Expediting material Supply by in-house follow ups, visits with Suppliers and their sub-contractors\n\u2022 Participated in daily review meetings with cross functional teams and top management to track the productivity, KPI's, process mapping and continuous improvement activities\n\u2022 Managed the budget development, scheduling and execution of various capital projects such as installation of new machinery and Lean projects""]","[u'Master of Science in Industrial and Systems Engineering', u'in Engineering Information Systems', u'Bachelor of Engineering in Mechanical Engineering', u'']","[u'Northern Illinois University DeKalb, IL\nMay 2017', u'University using\nJanuary 2015', u'National Institute of Technology Warangal, Telangana\nMay 2012', u'cultural and technical fests of National Institute of Technology Warangal, Telangana']"
0,https://resumes.indeed.com/resume/13dc82ef8e92ccd4,"[u""Systems Implementation Engineer/Windows Security Engineer\nQTS Data Centers - Suwanee, GA\nFebruary 2015 to Present\nResponsible for upgrading multiple geographically disparate ESXi 5.0/5.1 clusters each comprised of 8 hosts to ESXi 5.5 while maintaining system uptime for all VMs in highly critical DCIM infrastructure.\n\u25c6 Led migration and upgrade of DCIM Wonderware facilities management servers from Windows 2008 R2 to Windows Server 2012 R2 while maintaining system uptime and providing for near seamless cut-over\n\u25c6 Responsible for new customer implementations of Virtual and Physical servers both in internal private cloud QVI environment as well as Enterprise Cloud public cloud utilizing vCloud Director\n\u25c6 Windows Security Engineer focused on PCI, SOX and FedRAMP compliance. Vulnerability management and remediation for all Windows Servers in scope.\n\u25c6 Working knowledge of information system security standards and best practices (e.g., access control and system hardening, system audit and log file monitoring, security policies, and incident handling).\n\u25c6 Perform and/or coordinate regular security scans and assessments of existing or new infrastructure utilizing Nessus vulnerability scanner.\n\u25c6 Perform remediations by applying patches, registry edits and/or Group Policy modifications in order to remediate serious SSL vulnerabilities such as Poodle and Heartbleed.\n\u25c6 Work with System/Application owners at different levels in the organization to understand their respective security needs and assist with implementing practices and procedures consistent with the company's Information Security Policy.\n\u25c6 Architected and implemented Alert Logic Log Manager solution that meets compliance requirements and identifies security issues across the environment. The system collects, aggregates and analyzes log data and provides a single point of egress for log traffic.\n\u25c6 Responsible for all new DRaaS implementations utilizing Zerto which is a hypervisor agnostic replication technology with the ability to migrate/replicate user workloads from customer on-premise location into our Enterprise Cloud based on vCloud Director."", u'Systems Engineer\nStrategic Link Consulting - Atlanta, GA\nMay 2008 to February 2015\nArchitected new VMware ESXi environment consisting of 6 hosts in a 2node clustered configuration with an EqualLogic iSCSI SAN array attached to each cluster for physical to virtual (P2V) conversion of server environment as part of hardware refresh initiative\n\u25c6 Managed various areas of support including systems connectivity, account provisioning, e-mail access (Outlook, Outlook Web Access, and Outlook Express), Cisco AnyConnect VPN.\n\u25c6 Maintained and monitored support for all TCP/IP network configurations including VLAN and ACL configuration.\n\u25c6 Managed a broad range of installation, upgrade, roll-out, and troubleshooting projects for Windows-based networks with focus on computer/network systems running Tran dot Com proprietary lending software.\n\u25c6 Managed/maintained Active Directory infrastructure consisting of three domains spread across 5 sites. Manage Group Policy, network services such as DHCP, DFS replication, DNS, WINS, file/print services, and WDS system image deployment.\n\u25c6 Configured and deployed Windows Server 2003 R2 DFS Replication to consolidate file storage from remote sites for centralized backup, restore and archive.\n\u25c6 Developed and implemented process and environment for pre-deployment testing of Microsoft operating system and application patches and updates via WSUS.\n\u25c6 Repurposed older Dell and SuperMicro servers as part of Dev\\QA environment build rather than pricing out new equipment\n\u25c6 Deployed WDS system imaging solution for rapid corporate desktop/laptop deployment\n\u25c6 Configured LanSweeper network inventory system as part of accounting audit/compliance initiative and was able to quickly map out and identify assets based on IP location\n\u25c6 Provide a single point of contact for users to report problems/escalate issues via ReadyDesk web-based ticketing system.', u""Systems Engineer\nBluePoint Data Inc - Marietta, GA\nMay 2007 to May 2008\nArchitected and deployed Microsoft server systems to provide network services including DNS, WINS, and DHCP; email; file storage; intranet; and databases.\n\u25c6 Responded to incidents in a timely manner utilizing SalesForce.com, web-based ticket management system\n\u25c6 Diagnosed and resolved various problems relating to server hardware/software and specific operating environments, including Citrix Presentation Server, VMware Virtual Infrastructure Client, Windows 2003 Active Directory\n\u25c6 Maintained accurate inventory of systems and updated documentation as changes were made to customer's environments\n\u25c6 Responsible for administration of monitoring tools such as IPmon and Profiler toolsets\n\u25c6 Responsible for new customer environment OS deploys and Active Directory implementations\n\u25c6 Led datacenter migration project moving 300 servers and associated networking hardware to new state of the art data center facility in another state"", u'Data Center Engineer\nQuality Technology Services - Suwanee, GA\nDecember 2006 to May 2007\nProvisioned client co-location installations\n\u25c6 Installed cables, PDUs, and prepared environment for customer equipment\n\u25c6 Racked and stacked servers and other network hardware within a data center environment\n\u25c6 Decommissioned redundant equipment and cabling\n\u25c6 Troubleshooting server hardware/connectivity\n\u25c6 Light Cisco router/switch configuration\n\u25c6 Performed upgrades, configuration maintenance, and repairs of system hardware\n\u25c6 Assembled and configured ready servers (Dell, HP, IBM, and SuperMicro)\n\u25c6 Responded to customer requests utilizing Remedy ticketing system\n\u25c6 Provided remote ""hands and eyes"" troubleshooting support\n\u25c6 Imaged servers and workstations using Remote Installation Services (RIS) and Norton Ghost', u'Network Administrator/Instructor\nInteractive College of Technology - Chamblee, GA\nAugust 2003 to August 2006\nWorked with IT department in setting corporate standards for hardware and software for both servers and desktops\n\u25c6 Provided technical instruction for a classroom size of 25 students on Microsoft, A+/Net+ and Windows Active Directory administration.\n\u25c6 Executed day-to-day maintenance and troubleshooting of 120 node Ethernet LAN\n\u25c6 Designed and implemented coursework and lab projects for students']",[u'Bachelor of Business Administration in Managerial Sciences'],[u'Georgia State University\nMay 2006']
0,https://resumes.indeed.com/resume/f044c4cc795e0e20,"[u'Data Analyst\nWalkwater Technologies at Google - San Jose, CA\nJuly 2017 to Present\n- Providing data analysis with an emphasis on areas for improvement Create or refactor dashboards to assist operational decision making and team wide understanding.\n- Programming using SQL,HTML language to automate and customize dashboards.', u""Software Engineer\nWipro Technologies. Inc\nMay 2010 to April 2014\n- Oracle Applications Development, Implementation, and Testing in various modules of Oracle E-Business Suite (Oracle EBS R12/11i) which includes Order Management (OM), Inventory (INV), finance modules (AR, AP, and GL).\n- Programming using SQL and PL/SQL and Unix Shell scripting language.\n- Advanced PL/SQL Developer. Extensively worked on Advanced PL/SQL Concepts, PL/SQL Tables, PL/SQL Procedures.\n- Expertise in implementing RICE (Reports, Interfaces, Conversions, Extensions) components in the area of technical\ndesign, development, Integration testing and deployment.\n- Involved in Coding, unit testing, code migration, UAT testing and production deployment.\n- Led the team on a high priority project to completion in Agile environment.\n- Client interaction and communication ranging from understanding the requirements to project Implementation.\n- Involved in converting the business documents into technical design documents (TDD) and work with boundary\nteam to approve the technical and integration design.\n\nTechnical Stack:\nLanguage: PL/SQL, SQL, Unix, XML.\nEnvironment: Oracle Applications R12 and 11i\nIDE's: Toad.\nDatabases: Oracle 10 &11g.\nScripting Language: Shell Script.""]","[u'M.S. Computer Engineering in Architecture', u'Bachelor of Engineering in Electronics Engineering']","[u'San Jose State University San Jose, CA\nMay 2017', u'Shivaji University\nMay 2009']"
0,https://resumes.indeed.com/resume/21820999e5e54b19,"[u'Engineer\nQUEST GLOBAL\nNovember 2016 to August 2017\n\u2022 Performed numerical analyses and model simulations on military jet engines for aerospace manufacturer Pratt & Whitney.\n\u2022 Programmed scripts in Python to organize data files and facilitate repair analyses process.\n\u2022 Used Excel to calculate stresses on jet engine tools and provided technical recommendations.\n\u2022 Managed multiple projects, communicated with managers and ensured projects met deliverable deadlines.', u'Data Analyst Intern\nNEW YORK CITY TRANSIT DEPARTMENT - New York, NY\nAugust 2015 to February 2016\n\u2022 Performed data analyses in R programming for the automatic current traction motors from buses and successfully established oil criteria that helped reduce the cost of motor replacement program in NYC transit department.\n\u2022 Managed oil database and calculated project budgets in Excel to successfully increased project efficiency.', u""Design Engineer Intern\nNEW YORK UNIVERSITY - New York, NY\nJune 2014 to August 2014\n\u2022 Assisted design team to check and improve client companies' design drawing in electrical diagrams.\n\u2022 Designed solar-power system by sizing conductors, conduits, modules and inverters in Excel.""]","[u'MS in Financial Engineering', u'BS in Mechanical Engineering']","[u'NEW YORK UNIVERSITY, TANDON SCHOOL OF ENGINEERING Brooklyn, NY\nMay 2019', u'NEW YORK UNIVERSITY, TANDON SCHOOL OF ENGINEERING Brooklyn, NY\nSeptember 2012 to May 2016']"
0,https://resumes.indeed.com/resume/70fa7dde6d9dabc0,"[u'Test Engineer\nCIGNA - HEALTHSPRING\nAugust 2017 to Present\nCreating Subject Matter Documents as per Application features.\nQNXT Benefit & Claims Testing\nValidation And Verification of Insurance Claims\nMapping Insurance Claim Documents.\nAlso working for in house applications.\nWorked on CMS rules for medicare and medicaid.\nODAG & CDAG\nClaim creation and Validation.\nSupporting CMS Universe Audit Validation', u'Test Analyst\nJanuary 2017 to August 2017\nManual Testing\nCreating Test Cases\nAnalyzing User Stories\nCreating Subject Documents.', u'Data Analyst\nValidating Agency Forms\nJuly 2016 to December 2016\nFinding Missing Information and raising the defect.\nVerification of Templates.']","[u""Bachelor's""]",[u'University of Engineering and Technology\nJanuary 2008 to January 2012']
0,https://resumes.indeed.com/resume/230db6ece87a60ef,"[u'Data Analyst\nNational Marine ma - C\nMarch 2017 to Present\n\u2022 Reduced erroneous data 9% by imputing missing historical data using domain knowledge and decision trees\n\u2022 Lead analyst and liaison for NMMA\u2019s 100+ reporting manufacturers constituting 80% of the U.S. recreational boating market and 6 Billion dollars in yearly sales\n\u2022 Employed Python and VBA-Macros, along with SharePoint and Tableau, to identify and present market trends and inflection points in quarterly ad hoc projects\n\u2022 Cut publication time of monthly-released reports in half by employing a semi-autonomous Python program\n\u2022 Collaborated with marketing and government relations teams to produce 270 infographics for Congressional Districts and States using primary and secondary data sources', u""Mechanical Engineer Intern\nHyperion Technology Group - Tupelo, MS\nMay 2015 to August 2015\n\u2022 Collaborated in a cross-functional team on multimillion dollar defense contracts\n\u2022 Designed a portable windscreen for Hyperion's infrasound sensor in use by the U.S. Army\n\u2022 Drafted drawings for a customer's Hunting Blind Patent no.: USD 784,800""]","[u'Master of Science in Industrial Engineering', u'Bachelor of Science in Mechanical Engineering']","[u'University of Illinois at Chicago Chicago, IL\nAugust 2016 to December 2017', u'The University of Mississippi Oxford, MS\nAugust 2012 to May 2016']"
0,https://resumes.indeed.com/resume/1ca6abb138cefeb7,"[u""Data Scientist\nIBM Watson - Cleveland, OH\nJuly 2015 to Present\nDescription: Metric risk adjustment model levels the playing field for the reporting of patient outcomes, adjusting for the differences in severity of illness among patients. Predict the patient decease based on the other patient decease patterns.\n\nResponsibilities:\n\u2022 Working as a data scientist, responsible for creating new and maintain existing statistical, machine learning models for patient matching, risk prediction and grouping, Clarifying doubts and guide team resides at multiple geographic locations.\n\u2022 Used logistic regression on EMR patient data over ten thousand variable automatically extracted from EMR data.\n\u2022 Used 10 year risk estimates model based on the multivariate regression equations. Measure of calibration to accurately measure the absolute level of risk.\n\u2022 Uses Likelihood ratio test to measure the model fit.\n\u2022 Perform risk reclassification analysis to access the utility of risk prediction models.\n\u2022 Evaluated model and train the model using variable cases, performed hypothesis testing for patient matching models.\n\u2022 Designed and implemented Explorys's scalable natural language processing architecture by combining open source tools that fit well within our current parallel processing patterns. It enables us to enhance our offerings to customers with even more data points extracted from vast amounts of unstructured text.\n\u2022 Used Python NumPy, SciPy, Pandas packages to perform dataset manipulation.\n\u2022 Used R to write multivariate regression models and Risk predication algorithms.\n\u2022 Heavily used complex Sql queries to retrieve, validate data from Hbase and vertica database.\n\nTools and Technologies used: R, Python, Hadoop/Hive/Pig/Impala, Vertica, Statistics & Machine Learning\n\nProject: Kroger products option mining"", u'Data Scientist\nComputer Sciences Corporation - Cleveland, OH\nMarch 2015 to Present\nUSA.', u'Data Scientist\nKroger Corporation Inc - Cincinnati, OH\nMarch 2015 to July 2015\nDescription: Perform the sentimental analysis on the data collected for Kroger product through reviews, surveys or other means.\n\nResponsibilities:\n\u2022 Created work counts and data dictionary to analysis the people opinions or sentiments.\n\u2022 Find the Term frequency matrix based on the data dictionary.\n\u2022 Generated term frequency matrix, word clouds and apriori algorithm written using R.\n\nTools and Technologies used: Text Mining, R, SQL, Excel\n\nProject: Job Recommendation Engine', u""Data Scientist\nZettamine Technologies - ANDHRA PRADESH, IN\nOctober 2014 to February 2015\nDescription: This is a project to provide a solution for a Job Portal and have the following objectives:\n\n\u2022 Improve the search and Recommender Performance (Matching and Recommender)\n\u2022 Scale up all the applications that can handle millions of customers and the job requirements\n\u2022 Generate Recommendations for more than millions of customers and publish those recommendations by Customized emails to Consumers\n\u2022 Predicting Top 10 jobs that most matches job seeker's requirements from the available jobs\n\u2022 Utilize User Behavior tracking for building even sharper recommender systems\n\nResponsibilities:\n\u2022 Cleaned & Processed sample job seekers and postings dataset provided by client.\n\u2022 Perform data classification using model based and Bayesian and write algorithms to prepare clusters.\n\u2022 Analyzed clusters by plotting them using various plots like entropy plot, uncertainty plots, Cluster Density Plots etc\n\u2022 Applied content based filtering in job seekers and posting datasets to recommend jobs to job seekers.\n\nEnvironment: R, Machine Learning, Data Mining, Hadoop, Hive, Impala\n\nProject: Insurance Customer Retention Analysis"", u'Data Consultant\nCSC India Pvt. Ltd - Hyderabad, Telangana\nAugust 2010 to February 2015\nHyderabad, India.', u'Data Scientist\nZurich Insurance - Hyderabad, Telangana\nJanuary 2014 to October 2014\nDescription: Project is to overcome the issues faced by insurers due to fall in customer retention rate, Fraud identification and prevention cost, increased operational Cost etc.\n\nResponsibilities:\n\u2022 Cleaned & Identified sample data into a single view of the customer to start the analysis.\n\u2022 Applied clustering technique know customer segment.\n\u2022 Build Propensity model to predict the likelihood of an outcome, how likely a customer is to default, churn or lapse,\n\u2022 Calculated customer value metric to priorities customer based on probability and value.\n\u2022 Created Dashboards using Tableau to exposed the analysis in Dashboards.\n\nTools and Technologies used: Python, SQL Server, Tableau, Excel\n\nProject: Response Assessment Module (Phase I, II & III)\nClient: Zurich - North America\nLocation: Hyderabad, India', u'Data Analyst\nCSC India Pvt. Ltd\nApril 2012 to December 2013\nDescription - Response Assessment Module (RAM) formalizes, standardizes, and documents the assessments and audits the IT Security of Zurich. RAM lets you create a Questionnaire and take informed decisions based on the responses given from the Questionnaire.\nThere are around 199 Questions for all the Business Units. Each Business Unit has a specific set of questionnaires to be answered.\nResponsibilities:\n\n\u2022 Involved in analyzing user specifications for workability, completeness and business flow.\n\u2022 Extracted data from various sources like SQL Server 2008/2005, DB2, .CSV, Excel and Text file from Client servers and through FTP.\n\u2022 Developed, deployed and monitored SSIS Packages for data warehouse created for project.\n\u2022 The packages created included a variety of transformations, for example Slowly Changing Dimensions, Look up, Aggregate, Derived Column, Conditional Split, Fuzzy Lookup, Multicast and Data Conversion.\n\u2022 Reviewed & Tested packages, Reports and Cubes, fixing bugs (if any) using SQL 2008 Business Intelligence Development Studio.\n\u2022 Created cubes and dashboards to show to Zurich assets trends to higher management.\n\nEnvironment: Windows XP, SQL 2008, SSIS, SSRS', u""Data Warehousing Consultant\nRcard ETL - Hyderabad, Telangana\nNovember 2011 to April 2012\nDescription - Rcard ETL process involves extracting, transformation and loading of data from 6 different types of feeds. The data for the Rcard database would come from different sources like RCS, CLR, Schedule-F, MAD, Custom Excel sheets (there are 6 of them) and Recoverable Bad Debt. Below are the important features of Rcard.\n\u2022 Determine the accuracy of ending balances to allow management sign-off.\n\u2022 Track reconciliation progress by manager/analyst/contract and provide a quantitative tool to evaluate each individual's performance.\n\u2022 Assist in determining staff headcount and allocation between teams\n\u2022 Identify items that need correction and hold individuals/BU's accountable for correction by providing detailed reports to senior management.\n\u2022 Identify collection issues to assist with reporting, analysis and schedule F processing.\n\u2022 Provide other miscellaneous management and analytical tools.\n\nResponsibilities:\n\u2022 Converted existing Base SAS packages into SSIS 2005.\n\u2022 Analyze and document the SAS code logic and prepare the database design for SSIS packages.\n\u2022 Developed SSIS package to pull the data from various heterogeneous sources like Excel, Flat files, Db2, Nettezza and Sql server.\n\u2022 Converted reports generated during the process into SSRS technology.\n\u2022 Developed Stored Procedures, Triggers, and SQL scripts for performing automation tasks.\n\u2022 System testing of bug fixes / enhancements and Support of the application RcardETL application.\n\nEnvironment: Windows XP, SQL 2005, SSIS 2005, SSRS 2005"", u'ETL Developer\nCLR ETL - Hyderabad, Telangana\nJanuary 2011 to November 2011\nDescription - CLR (Ceded Loss recovery) system is used to capture facultative, treaty loss and payment information for ceded reinsurance. All losses are imported monthly and sorted by claim number within agency numbers. Payments are collected and recorded in CLR throughout the month. Reconciliation and aging is performed as part of the monthly process. CLR also feeds the client supported Schedule F system with unapplied cash and aged subledger information.\n\nResponsibilities:\n\u2022 Evaluates existing manual process in application and proposed automation of process to business in order to reduce manual intervention and save the customer cost.\n\u2022 Enhance and maintain the application using power builder 10.\n\u2022 Estimate, Design, develop and implement new ETL process based.\n\u2022 Developed Stored Procedures, Views, and T-SQL scripts.\n\u2022 Developed SSIS packages to pull data from mainframe flat file.\n\u2022 Developed reports using SSRS and in excel format generated as part of during ETL process.\n\u2022 System testing of bug fixes.\n\nEnvironment: Windows XP, SQL 2005, SSIS, SSRS', u'Consultant - Application Development\nMass Litigation System - Hyderabad, Telangana\nOctober 2010 to December 2010\nClient: Zurich - North America\nLocation: Hyderabad, India\nDuration: Oct 2010 to Dec 2010\nRole: Consultant - Application Development\n\nDescription - Mass LITT (Latent Injury Toxic Tort) is a tool to assist in managing the complex litigations against the Zurich Insured.\n\nResponsibilities:\n\u2022 Migrated application frontend from Power Builder 6.5 to 12.5 and backend from Sybase ASE 12.5 to MS Sql Server 2008.\n\u2022 Converted mainframe/Unix jobs into SSIS technology.\n\u2022 Managed individually all migration and development activities of the project.\n\u2022 Presented Demos/Prototypes for the migrated or new developed application part to customer time to time.\n\u2022 Participated in Client review and status meetings.\n\u2022 System testing of bug fixes / enhancements.\n\u2022 Production Support of the application.\n\nEnvironment: Windows XP, SQL Server 2008, SSIS, Sybase ASE 12.5\n\nProject: Mastermind Application', u'Senior Software Engineer\nBroadview Security - ANDHRA PRADESH, IN\nAugust 2009 to July 2010\nDescription - MasterMind application used in-house from client PCs is the Window based client server application, created using Power Builder and SQL server technologies.\nBHS offers homebuilders and individuals building homes expanded technology options in their new homes by providing a one-stop source for security, structured wiring (advanced phone/cable systems), home networks, home theater, multi-room speaker systems and other low-voltage products.\n\nResponsibilities:\n\u2022 Involved in Database design and development.\n\u2022 Developed various Reports using SSRS, Customized existing reports according to the functional specifications.\n\u2022 Performed Tuning on the SQL queries, making the Procedures runs faster and more efficiently.\n\u2022 Client communication.\n\u2022 Developed SSIS packages extracting data from various sources oracle, Flat file and sql server.\n\nEnvironment: Windows XP, SQL 2005, SSIS\n\nProject: Dealer Funding Automation', u'Sr. Software Engineer\nTrianz Inc - Hyderabad, Telangana\nMarch 2009 to June 2010\nTill year 2009 it was Blue Ally, a division of Mega soft Ltd. and now acquired by Trianz Inc.) Hyderabad, India.', u'Senior Software Engineer\nBroadview Security - Hyderabad, Telangana\nMarch 2009 to August 2009\nDescription - Dealer Funding Automation is created to automate the process to compute and store dealer funding payment/deduction items using the business rules defined in some reports of Mastermind application. Several Processes are created to automate the reports computation and these processes are scheduled to run on a specific day and time and can be run manually. The funding items (payment/deductions amounts) retrieved are joined with cost center and GL Account and then stored so that an end user can export the funding records per dealer related to a funding date into an excel file.\n\nResponsibilities:\n\u2022 Analyzed Requirement, Designed and developed.\n\u2022 Involved in Database design and development.\n\u2022 Developed Complex reports using SSRS.\n\u2022 Created SSIS package to extract the data from Oracle database and Developed complex SSRS reports.\n\nEnvironment: Windows XP, SQL 2005, SSRS, SSIS\n\nProject: Enterprise (EMain)', u'Software Engineer\nBirla Soft India Ltd - Noida, Uttar Pradesh\nOctober 2008 to March 2009\nNoida India Operations.', u""Software Engineer\nGE (UK) - UTTAR PRADESH, IN\nOctober 2008 to March 2009\nDescription - ENTERPRISE & Watchdog application used in house from client PCs is the Window based client server application, created using Power Builder and SQL server technologies. Application provides the automated solution for Corporate Banking. It manages all the prime functionalities for corporate banking accounts such as Receipts, Payments, Assets, Credit Limits, Transactions, Sales and Stock adjustments Foreign Exchange and GL-Accounts with their detailed reports and statements.\n\nResponsibilities:\n\u2022 Developed complex Stored Procedures, Triggers, functions and SQL scripts.\n\u2022 Troubleshoot database related issues in a timely fashion.\n\nEnvironment: Windows XP, SQL 2005\n\nProduct: Reckoner\nClient: Britania Allied Ltd. (South Africa)\nLocation: New Delhi, India\nDuration: Dec 2006 to Mar Oct 2009\nRole: Software Engineer\n\nDescription - Reckoner ERP is a multi company, multiuser, multi location, online ERP\nsystem. Reckoner contains various modules like Core, Finance, Material, Sales, Hrm, Prouction &\nPlanning, Real state, Projects etc. Reckoner is an Enterprise Resource Planning system that provides\nefficient supply chain management at every level of operation. Its an information technology system\nthat integrates data captured at all different stages and provides managers a clear, complete\nperspective on their business across national, currency, and organizational boundaries. The benefit is\nthat companies streamline their operations and perform activities that add value efficiently and effectively. Reckoner ERP is designed to integrate all external and internal functional areas in a process\n-based environment to ensure a seamless link for all transactions between suppliers, customers and across different departments of an organization. Reckoner offers you a critical tool for dynamic\nplanning. It incorporates an innovative document based workflow system that maps the documents\nonto the ERP system and provides an easy way of authorizing and signing them.\n\nResponsibilities:\n\u2022 Primary responsibilities - analysis of functional specification, preparation of technical\nspecification.\n\u2022 Develop Complex queries, stored procedures, Triggers, Functions, views and optimized them highly\n\u2022 Written highly optimized queries to meet the product standard in terms of wait time.\n\u2022 Played a key role in the development of the product.\n\u2022 Meeting aggressive timelines and customer's expectation\n\nEnvironment: Windows Vista, SQL 2000/2005, Sybase ASE 12.5"", u'Software Engineer\nAscomp Technology Pvt. Ltd - Delhi, Delhi\nDecember 2006 to October 2008\nNew Delhi, India.\n\nProject Details\nProject: Patient Risk Prediction and Grouping']","[u'Bachelor of Technology in Computer Science and Engineering', u'']","[u'U.P. Technical University', u'Business Analytics form Indian school of Business Hyderabad, Telangana']"
0,https://resumes.indeed.com/resume/e7c44f1d6dbc8eea,"[u'Data Analyst/Jr. Data Engineer\nInfosys Ltd\nFebruary 2014 to December 2015\nRoles & Responsibilities:\n\u2022 Understanding the B2B architecture between the client and the customers.\n\u2022 Understanding, ingesting and querying data using SQL and ensure smooth flow end to end.\n\u2022 Resolving any anomalies in the data or network related errors in the event viewer.\n\u2022 Make sure there is no duplication of the data.\n\u2022 Monitor BizTalk servers.\n\u2022 Handling multiple SQL jobs.\n\u2022 Handling tickets by priority.\n\u2022 Communicating with the client directly to resolve any issue.\n\u2022 Reporting SLA compliance & daily and weekly KPIs.\n\u2022 Knowledge transfer process for new hires or new team members.']","[u'Master of Science in Computer Science', u'Bachelor of Technology in Computer Science']","[u'University of Houston Clear Lake Houston, TX\nDecember 2017', u'Mahatma Gandhi Institute of Technology Hyderabad, Telangana\nMay 2013']"
0,https://resumes.indeed.com/resume/694c7501bec5bc84,"[u'Sr Data Center Engineer\nCapital One - Plano, TX\nAugust 2013 to Present\nResponsible for the build out of Volo Data Center with emphasis on Fabric Path and OTV Implementation. Create Low Level Design and Engineering Order to implement changes specifically for Capital One - Data Center Strategy.\n\u2022 Work directly with multiple vendors to build DR Data Center located in Volo, IL. (Cisco, VCE, EMC)\n\u2022 Project includes: Design/Implementation of Out-of-band management, Migration to M2/F2 Nexus modules to prepare for Fabric Path, Hardware Migration and Software Upgrades on Nexus 7000/6000 product.\n\u2022 Hardware includes: Nexus 7010/6004/6001', u'Data Center Network Implementations Engineer\nCisco Systems - Richardson, TX\nOctober 2011 to August 2013\nPractice Cisco Data Center design principles to build industry leading data centers. Part of team that built Cisco Allen data center which is an enterprise class facility for critical Cisco business applications including IT production, engineering, and multi-tenant businesses. Part of DCNI team responsible for implementation projects involving 15+ global data centers.\n\u2022 Projects included: Internal/DMZ POD builds, UCS cluster implementation, Nexus 7k/5k/2k/1k turn-ups, out-of-band Management design/implementation, WAAS Optimization implementation and maintenance, Nexus 1000v implementations/migrations/upgrades, and IPv6 code upgrades.\n\u2022 Hardware includes: Nexus 7010/7018/5010/5020/5548/5596/2048/1000v/1010, UCS Fabric Interconnects 6100/6200/6248UP/6296UP, UCS C200 rackmount series, UCS 5108 blade chassis.\n\u2022 Software environment: Cisco IOS/NX-OS, UCS Manager, vCenter 4.1, Nexus1000v (VSM/VEM), SecureCRT, Wireshark.', u'Network Operations Engineer\nFujitsu Corp - Richardson, TX\nOctober 2010 to October 2011\nParticipant on a team to build out data center for the NOC network in the Richardson campus. Duties included hardware-setup (rack and stack), provisioning, troubleshooting Catalyst switches and multiple vendor servers.\n\u2022 Worked with service providers to turn up MPLS circuit for remote sites. Configured BGP on CE equipment to assure connectivity between backup sites.\n\u2022 Hardware included: 10 Catalyst 3750, 4 Catalyst 2951, 4 Checkpoint Firewall, and HyperV servers.\n\u2022 Software environment: Solarwinds Orion, Netsmart 1500, Monolith NMS, and Siebel ticketing system.', u'Sr Customer Support Engineer\nAvaya / Nortel Networks - Richardson, TX\nJune 2000 to October 2010\nProvided 24x7 technical support to global Nortel customers across multiple vendor platforms: Ethernet Routing Switch 8600, CS1000 VoIP, Cisco Router/Catalyst, and Callpilot voicemail.\n\u2022 Exposure to numerous customer network architectures providing disaster recovery support on a remote basis.\n\u2022 Worked on Clarify tickets to troubleshoot and upgrade customer outages within the 30 Minute Outage Recovery agreement.\n\u2022 Identify and isolate design ""deficiencies"" on both hardware and software Nortel product lines; report them back to the appropriate design team via tracking tool. Track technical problems and encourage design resolutions; and ensure they were implemented in the field.\n\u2022 Lab testing and demos for Nortel ERS8600 integration with CS1000 and Cisco Router/Catalyst.', u'Project Lead\nFreightliner Corp - Dallas, TX\nJune 1999 to October 1999\nWorked as a consultant for Freightliner Y2K group which primary duties include 100% travel to remote sites to migrate/configure/troubleshoot customer equipment (mostly Cisco routers/ catalyst switches), install and upgrade proprietary software packages. Familiar with VPN, FR and ATM technologies, routing protocols BGP, EIGRP and OSPF. Personal duties include leading a team to represent Freightliner Corp. to each of these privately owned truck dealerships.']",[u'BA'],"[u'University of Texas at Austin Austin, TX\nJanuary 1998']"
0,https://resumes.indeed.com/resume/095c985fda25eda5,"[u""Lead Software Engineer\nASWhite\nJanuary 2014 to January 2016\nLed development of eDocument, a heavy data-driven web app which managed 20-years' worth of insurance data, written in Bootstrap, jQuery, Java, SpringBoot, Hibernate, MySQL.\n\u25cf Analyzed and resolved several Rest API bottlenecks, optimized sluggish SQL queries.\n\u25cf Adopted micro-service pattern to build scalable services, scaled the app from 50 to 1000+ users.\n\u25cf Used Jenkin for continuous integration.\n\u25cf Managed 2 remote teams (in total 12 developers)\n\u25cf Practiced Scrum (as Certified Scrum Master) to develop large-scale enterprise software in 2 years.\n(see reference letters)"", u'Data Engineer\nKLAMR\nJanuary 2012 to January 2013\nDeveloped web crawlers (C#) to collect 20-millions of location data in Bay area of SFO.\n\u25cf Developed ""RestAPI runner"" which query social media API (Facebook API, Yelp API, Google Place API, FourSquare API) to survey the popularity of predefined places and discover the trend.\n\u25cf Developed tools to group similar data and remove duplication (PostgreSQL with PostGIS library).\n\u25cf Developed tools to clean, standardize, and normalize data of 20 million records (C#)', u'Software Engineer\nVnResource\nJanuary 2007 to January 2012\nLed development of desktop Profile Management and Payroll Computing software using C#, Entity Framework, NHibernate, MSSQL\n\u25cf Enhanced computing speed of Payroll Computing module to 2X by optimizing lambda expression\n\u25cf Hired and managed development team of 10 developers\n\nTECHNICAL EXPERIENCE\nSide Projects']","[u'Master of Science in S.E', u'CERTIFICATE in Leadership Challenge', u'Bachelor of Science in Mathematics']","[u'San Jose State University\nMay 2018', u'Royal Melbourne Institute of Technology (RMIT)\nJanuary 2015', u'University of Science\nMay 2008']"
0,https://resumes.indeed.com/resume/2ac66569d9add912,"[u'Data Engineer\nRiskonnect, Inc - Chicago, IL\nApril 2017 to Present\nSkills: SQL Server SSIS T SQL Salesforce CRM Tableau Visual Cron Jitter Bit Microsoft Office Suite\n\u2022 ETL: Design reusable ETL packages using SSIS to perform inbound and outbound data transformations for Insurance claims and financials data from various TPA sources\n\u2022 Query optimization: Develop complex SQL scripts, functions, stored procedures and triggers for query optimization to improve the Salesforce CRM response time by 30%\n\u2022 Business Intelligence Reporting: Provide data solutions in Risk Management for clients by building business intelligence\nreports and performance dashboards using Tableau and Power BI\n\u2022 Troubleshooting: Facilitate application support by monitoring, investigating and resolving various data load issues in the production environment', u""Data Analytics Intern\nBMW Technology Corporation - Westmont, IL\nJune 2016 to December 2017\nSkills: R Power BI SQL Server Data Modeling Microsoft Office Suite Data Modeling\n\u2022 Data analytics: Analyzed the mobile application 'BMW Connected' and designed dashboards using Power BI to derive\ninsights on customer behavior\n\u2022 Database modeling: Developed a relational database schema for mobile application using MS SQL Server and built SQL\nqueries to perform DML operations for reporting\n\u2022 Predictive Analytics: Built a predictive model by analyzing user data from mobile application to develop customer retention\nstrategy"", u'Data Analyst Intern\nLoyola University Health System - Westmont, IL\nSeptember 2015 to December 2015\nSkills: SPSS SQL SQL Server Data simulation Tableau\n\u2022 Data analysis: Collected, cleansed and analyzed the call center data to identify bottlenecks in the patient scheduling system\n\u2022 Business Intelligence Reporting: Designed a data simulation model on the mined data and recommended a variable staffing\nplan for the year 2016 to reduce the operating costs by 13% and call abandonment rate under 3%', u""Data Analyst\nAccenture Services - Bengaluru, Karnataka\nJune 2011 to November 2014\nSkills: PL/SQL Oracle Tableau Microsoft Office Suite\n\u2022 Development: Developed project plan and built custom interfaces, extensions and reports using PL/SQL for ERP system\n\u2022 Automation: Automated an error handling tool in core financial module to generate the weekly reports that identified\nrecurring data issues which reduced the turnaround time by 75%\n\u2022 Data Warehousing: Designed warehouse schemas and data-marts using dimensional modeling to facilitate reporting\n\u2022 Business Intelligence Reporting: Developed performance dashboards for monthly operational reports using Tableau that\nincreased operational effectiveness by 11%.\nPROJECTS\n\u2022 Developed classification and regression based data mining models to improve cost effectiveness of a national veteran\norganization's direct marketing campaign for fundraising. (Tool: R)\n\u2022 Developed machine learning model to predict the driving alertness to avoid road accidents (Tools: SPSS, R, Tableau)\n\u2022 Designed and developed performance dashboard for South west airlines comparing on-time performance statistics against its competitors (Tools: Tableau, R)""]","[u'MS in Information Systems', u'BE in Industrial Engineering']","[u'University of Illinois at Chicago Chicago, IL\nDecember 2016', u'Anna University\nMay 2011']"
0,https://resumes.indeed.com/resume/32cf523eec79d9c3,"[u'Software design engineer, machine learning data scientist\nTATA CONSULTANCY SERVICE - Redmond, WA\nApril 2015 to Present\nDuties and Technologies used:\n\u2022 Azure Subscription Automation VM CPU usage and auto scaling machine learning modeling.\nUsing algorisms:\nARIMA (auto regression integrated moving average),\nNNet (feed-forward neural networks with a single hidden layer, and for multinomial log-linear models)\nMulti-Classes Neural Networks classifications\nWelch two samples statistics test for model evaluations\nTime Series Anomaly Detection\nK-Means clustering\nSigmoid Activation\nPlatform:\nAzure Machine Learning Studio with R Scripts, SQLite, Storage Blob\nData analysis exploration visualization:\nMS Power BI\n\u2022 Cosmos Scope big data extracting and parsing in huge telemetry JSON format raw data stream to build\nsearch traffic & revenue analysis funnels. Providing simple and clear data visualization for search team. Post - sale monetization customer behave analysis based on customer purchase daily cosmos big data.', u'Software design engineer III\nTRYGSTAD TECHNICAL SERVICE - Bellevue, WA\nMay 2008 to August 2014\nDuties\n\u2022 Continue Integration Service Central Database & Reporting Site design / development\n\u2022 Continue Integration Service Automation System development and data base performance improvement.', u'Data engineer\nSHANGHAI HUAWEI MICROSYSTEM CO., LTD - SHANGHAI, CN\nJanuary 1999 to January 2006\nDuties / Technologies used:\nProject / product - civic information management application system\n\u2022 Leading an agile development team working closely with customers, area experts and data scientists to provide supervision system analysis, total solutions, data standards, distribution architectures for Shanghai\nMunicipal & Civic Archiving Information Center.\nTechnologies used:\n\u2022 Front: HTML + CSS + ASP / JSP + OWC web report components\n\u2022 Middle part: Java class EJB, COM wrappers\n\u2022 Back end: T-SQL/PL SQL + SQL Server DTS, OLAP\n\u2022 Animation: Adobe Flash Scenes & action scripts']",[u'in Computer Software Engineering'],"[u'Shanghai Computer Manufacturing College Shanghai, CN']"
0,https://resumes.indeed.com/resume/4817747a69e14088,"[u'Senior Data Engineer\nVisa Inc - Denver, CO\nApril 2015 to Present\nSenior Engineer/Solution Architect on a highly visibile built from scratch settlement platform for VISA\nDPS.\nEvaluation of current architecture and design of the new one allowing horizontal unlimited scalability\nArchitected a continuous Delivery platform with Jenkins, artifactory pro\nCurrently engineering a streaming solution with kafka, connect and hadoop platform\nSingle point of contact within the team for troubleshooting all hadoop related issues\nExtreme Coding - Java Map Reduce, Hive, Avro, Parquet Impala, Shell Scripting\nProvided technical mentorship to junior team members, performing code and design reviews, enforcing\ncoding standards and best practices.', u'Architect/ Lead Engineer\nCognizant Technologies Ltd\nJanuary 2014 to April 2015\nResponsible for data ingest, analysis and search of log messages across Travelport a leading travel\ncommerce platform\nArchitecture, design and Implementation of an enterprise logging system using Hadoop, HBase, HDFS,\nHive, morphlines and cloudera search\nInstallation, configuration and tuning of a 45 Node Hadoop Cluster on RHEL 6.4\nMentored Linux system administrators on Hadoop Installation, configuration and upgrades\nAchieved 4 times increased through put by moving to distributed systems from a NAS based system\nReduced average search time to 29 ms\nDeveloped asynchronous client applications, web services and JMS layers\nDelivered the project in less than 6 months.', u'Software Engineer\nInfosys Technologies\nMarch 2006 to February 2014\nDeveloped Web and Enterprise Applications using technologies Java / J2EE, Spring, Struts and Hibernate\nDeveloped multithreaded java batch applications\nLead a 4 member team for implementing APIs for sears.com market place\nMaintained production applications and managed SLAs and metrics for performance\nAutomation Testing and Load Testing of web services and batch applications\nImplemented a hybrid xml parser for processing large xml files.', u'Technology Analyst\nInfosys Technologies - San Francisco, CA\nMay 2012 to January 2014\nDesign and Development of a big data warehouse for e commerce content\nDeveloped ELT pipe lines from RDBMS to Hadoop using sqoop, mapreduce pig and hive\nEvaluation and customization of analytic tools including Jasper Reports and Pentaho\nScaled mysql datastore using sharding concepts\nProactive tuning and optimization of sql queries.']",[u'Bachelor of Technology in Technology'],[u'University of Kerala']
0,https://resumes.indeed.com/resume/c2633812cf7868f2,"[u""Movie Recommender System\nOctober 2017 to December 2017\nA recommender system based on Item Collaborative Filtering using Hadoop MapReduce in Java\n-Got data from Netflix Prize Challenge, merged 17770 movie files into one big input file.\n-Built Multiply co-occurrence matrix and users' rating matrix, generated recommendation list.\n-Created a Hadoop cluster on Amazon Elastic MapReduce with 6 nodes, run program on it, recommended\n5 movies to each user according to movies' relativity and user's rates."", u'GitHub\nSeptember 2017 to December 2017\n-Used C# to create object pool: objects can be spawned and reused multiple times to save the memory.\n-Developed three different levels and added level selection UI.\n-Added the ""slow down"" feature for the monsters in the map.', u'GitHub\nSeptember 2017 to October 2017\n-Developed a 2D Maze Search graphical class using incremental development.\n-Implemented the maze data file-reading by using java I/O method.\n-Designed a backtracking algorithm for efficiently searching the maze path.\n-Used BFS to find the shorted path from entrance to exit in the maze.\nGitHub: https://github.com/zhouyumeng1077/Maze-Search', u'Data Engineer Intern\nZTE Corporation\nApril 2017 to July 2017\nBuild a voice Recognition System.\n- Extracted large-scale voice signal samples, stored characteristic parameters in HDFS.\n- Calculated vector-mean and covariance matrix, conducted Principal component analysis in Hadoop.\n- Calculated distance between input and given models, recognized voice signal in high precision.\nAwarded an outstanding intern.', u'Software Engineer Intern\nChina Mobile\nJuly 2016 to September 2016\nDesigned a li-fi system to realize information transmission by the visible light, drove a digital modulation\nchip by C, transmitted songs in more than 3 meters long.\n-Performance improvement in Massive MIMO 5G System: came up with using resolution ADC and DAC:\nderived formulas and analyzed in MATLAB, found optimum solutions to reduce costing and power\nconsumption up to 80%.']","[u'Master of science in Computer Engineering in Computer Engineering', u'Bachelor of Science in Electrical Engineering in Electrical Engineering']","[u'University of Southern California Los Angeles, CA\nAugust 2017 to May 2019', u'Beijing Jiaotong University Beijing, CN\nSeptember 2013 to July 2017']"
0,https://resumes.indeed.com/resume/248102215ba53883,"[u'Jr. Data Analyst\nNew Jersey Institute of Technology\nMay 2017 to December 2017\n\u2022 Reporting: Compiled and analyzed student data (absences, course/semester withdrawal, course drop off) to generate timely reports\n\u2022 Excel analysis: Incorporated VLOOKUP, macros and data validation functions in Microsoft Excel to generate spreadsheets and pivot tables;\nPerformed excel analysis (built charts and graphs) to derive relationship and dependency among data variables', u'Treasurer and Vice President - Public Relations\nIndian Graduate Student Association\nJanuary 2017 to December 2017', u'Software Engineer\nCap Gemini India Private Limited\nAugust 2015 to August 2016\nPerformed tasks in the SDLC Agile environment under the supervision of a Senior Business analyst\n\u2022 Data Extraction: Extracted data from various databases, designed and developed ETL jobs using SQL queries, functions and transformations\nlike Key Generator, Table comparison, History preserving and Validation (in SAP BODS)\n\u2022 SDLC Agile: Involved in the software development of products with Agile methodology (SCRUM) and participated in JAD sessions, scrum\nplanning and review sessions\n\u2022 Documentation: Maintained different type of documents as required by various teams (SRS-System Requirement Specification, BRD- Business Requirement Document, RTM - Requirement Traceability Matrix, user stories, test cases for User Acceptance Testing)\n\u2022 Visualization: Created interactive dashboards using Tableau and QlikView to interpret trend lines, patterns and display forecasts\n\u2022 Business Modeling: Created visual models supporting data analysis with work flow diagrams and prototypes (UML, BPMN)']","[u'Master of Science in Information Systems in Data and Management, Project Management', u'Bachelor of Engineering in Electronics and Communication in Electronics and Communication']","[u'New Jersey Institute of Technology Newark, NJ\nDecember 2017', u'BMS Institute of Technology Bengaluru, Karnataka\nJune 2015']"
0,https://resumes.indeed.com/resume/9d711cc1ea4a8591,"[u'Data Scientist\nKensho Technologies Inc - Cambridge, MA\nApril 2016 to Present\nProject: Credit Card Fraud\n\nResponsibilities:\n\u2022 Communicated and coordinated with other departments to collect business requirement\n\u2022 Worked on miss value imputation, outliers identification with statistical methodologies using Pandas, Numpy\n\u2022 Participated in features engineering such as feature creating, feature scaling and One-Hot encoding with Scikit-learn\n\u2022 Tackled highly imbalanced Fraud dataset using undersampling with ensemble methods, oversampling and cost sensitive algorithms\n\u2022 Improved fraud prediction performance by using random forest and gradient boosting for feature selection with Python Scikit-learn\n\u2022 Implemented machine learning model (logistic regression, XGboost) with Python Scikit- learn\n\u2022 Validated and selected models using k-fold cross validation, confusion matrices and worked on optimizing models for high recall rate\n\u2022 Implemented Ensemble Models with majority votes to enhance the efficiency and performance\n\u2022 Designed rich data visualizations with Tableau 9.4\n\nEnvironment: Python (scikit-learn, pandas, Numpy), Machine Learning (logistic regression, XGboost), Gradient Descent algorithm, Bayesian optimization, Tableau', u'Data Engineer\nNorth Shore Medical Center - Salem, MA\nSeptember 2015 to April 2016\nResponsibilities:\n\u2022 Implement advanced ETL processes to load and integrate large datasets (SSIS, MSSQL server, R)\n\u2022 Cleaned and manipulated complex healthcare datasets in order to create the data foundation for further analytics and the development of key insights (MSSQL server, R, Tableau, Excel)\n\u2022 Modelled the data relationship to provide assistance for business decision making (R, Tableau)\n\u2022 Reported and dash boarded analytical results to client (R, Tableau, Excel)\n\nEnvironment: SSIS, MS-SQL Server, R, Tableau, MS-Excel', u'Data Engineer\nInternastic Technologies Inc - Mumbai, Maharashtra\nJanuary 2012 to August 2015\nProject 1: Overall Satisfaction Analysis\n\u2022 Identified what factors could influence the overall satisfaction of consumers. Range (1-5)\n\u2022 considered the SMG (Service Management Group) database survey results in analyzing the impact on overall customer satisfaction\n\u2022 Used Ordinal logistic regression methodology in explaining the importance of features\n\u2022 The analysis involved predicting the overall satisfaction - ordinal rating, by analyzing the impact of each independent factors in explaining the output\n\u2022 Packages used: MASS package, polr function for Ordinal logistics regression model\nEnvironment: R Studio, SQL Server, Dplyr, Tidyr, ggplot2, Tableau, MASS package - polr function\nProject 2: Annual Marketing Budget Allocation\n\u2022 Responsible for Data collection and data preparation and normalizing the data\n\u2022 Used SQL, ETL tool, R Studio, and Python for data preparation\n\u2022 Supported data consultants in the data modeling phase\n\u2022 Used Constraint optimization algorithms (USED EXCEL) to optimize the marketing budget\n\u2022 Used Time Series Models - decomposition of time series, trend, and seasonality detection, forecasting and exponential smoothing in predicting the market share and brand share to allocate the Marketing budget\nEnvironment: R (dplyr, Tidyr, ggplot2, reshape2, fpp, forecast packages), Python (pandas, NumPy, scikit-learn),SQL Server, Excel\nProject 3: Sentiment Analysis\n\u2022 Used SMG (Service Management Group) database for most of the analysis used SQL for data preparation\n\u2022 Sentiment analysis using public comments from SMG database (POC Stage- Used Python (NLTK) for analysis)\n\u2022 Monthly Media Activity intelligence & Competitor Pricing reports for top level management\n\u2022 Overall satisfaction analysis using descriptive statistics\nEnvironment: SQL, Python (NLTK), Excel']","[u""Master's""]",[u'']
0,https://resumes.indeed.com/resume/79c08e8fd7554508,"[u""Data Engineer\nDollar Shave Club - Los Angeles, CA\nJune 2017 to Present\nInvolved in loading data from traditional databases to Hadoop data storage (HDFS, HBase) using Sqoop and written analytical queries for processing the data using Hive.\n\u25e6 Performed data analysis through Pig scripts, MapReduce and Hive, storing and managing the data in NoSql databases like HBase and Cassandra.\n\u25e6 Collected streaming data using Spark streaming and stored in column oriented database HBase and implemented spark Sql queries and Hive queries to convert the data into spark RDD's, worked on implementing spark jobs using Scala.\n\u25e6 Loading the source data coming into DSC MySql database to Amazon Redshift using the data pipelines\n\u25e6 Parsing the data while loading it into Amazon S3 from Redshift after processing using the Spark.\n\u25e6 Worked on Scala for writing several spark jobs in real time applications for processing the customer data stored in local data mart that we are using as the target storage\n\u25e6 Involved in writing some of machine learning algorithms available in Spark MLlib like ALS and Decision tree based algorithms for predictions.\n\u25e6 Worked on developing scripts to perform business transformations on the data using Hive and pig.\nSkills\n\u2022 Programming/Scripting Languages: C, Java, Pig, JavaScript, HTML5& Scala\n\u2022 Developer Tools: Visual Studio, Android Studio, IntelliJ IDEA, PyCharm\n\u2022 Databases: MySQL, SQL, HBase, Cassandra, Redshift, S3"", u""Java Developer\nCapgemini - Chennai, Tamil Nadu\nJune 2015 to December 2015\nResponsibilities:\n\u2022 Implemented the project as per the Software Development Life Cycle (SDLC).\n\u2022 Developed the web layer using Spring MVC framework.\n\u2022 Implemented JDBC for mapping an object-oriented domain model to a traditional relational database.\n\u2022 Created Stored Procedures to manipulate the database and to apply the business logic according to the user's specifications.\n\u2022 Involved in analyzing, designing, implementing and testing of the project.\n\u2022 Developed UML diagrams like Use cases and Sequence diagrams as per requirement.\n\u2022 Developed the Generic Classes, which includes the frequently used functionality, for reusability.\n\u2022 Exception Management mechanism using Exception Handling Application Blocks to handle the exceptions.\n\u2022 Designed and developed user interfaces using JSP, JavaScript, HTML and Struts framework.\n\u2022 Involved in Database design and developing SQL Queries, stored procedures on MySQL.\n\u2022 Developed Action Forms and Action Classes in Struts frame work.\n\u2022 Programmed session and entity EJBs to handle user info track and profile based transactions.\n\u2022 Involved in writing JUnit test cases, unit and integration testing of the application.\n\u2022 Developed user and technical documentation.""]","[u'Master of Science in Computer Science', u'Bachelor of Technology in Computer Science and Engineering']","[u'University of Missouri Kansas City, MO\nMay 2017', u'Jawaharlal Nehru Technological University\nMay 2015']"
0,https://resumes.indeed.com/resume/da61a6520da74ad0,"[u""Data Center Operations Consulting Engineer\nDept. of IT and Telecommunications, Government Agencies - Brooklyn, NY\nJanuary 2017 to Present\nMonitor the Outlook Inbox and ITSM/Remedy ticket queue\n\u25cf Respond to NetCool, NetIQ, HP Open View and Performance Monitor alerts\n\u25cf Generate System Change Requests for multiple data centers Manage numerous concurrent activities across hybrid platforms in several data centers\n\u25cf Coordinate various data center requests of all hardware moves/adds/changes\n\u25cf Plan and execute customer configuration maintenances and modifications\n\u25cf Perform network system deployments and hardware break-fix across many sites\n\u25cf Install devices within cabinets and complete all cabling needs as required\n\u25cf Configure IP KVMs, IP PDUs and Console Servers, such as Avocent and DSView\n\u25cf Troubleshoot copper and fiber connectivity issues, including installation testing\n\u25cf Formulate Data Center Infrastructure capacity planning\n\u25cf Documented Data Center policies, procedures and industry best practices\n\u25cf Create and implement special projects and initiatives as needed\n\u25cf Support all government agencies such as 911, 311, FDNY, PDNY, Mayor's office and commissioners office.\n\u25cf Support the lan patching and connectivity\n\u25cf Support the phone system installations for 4 buildings."", u'Data Center Operations Manager\nQTS Data Centers - Chicago, IL\nMarch 2016 to December 2016\nResponsible for day-to-day management of operations and engineering teams.\n\u25cf The teams supported approximately 350,000 square feet of data center and facility space across multiple data centers within the complex.\n\u25cf Responsible for the management of maintenance, critical infrastructure, construction, surveillance, access control, vendor management, change management, budgeting, documentation, development and implementation of procedures, space planning, capacity management, reporting of key metrics and performance indicators, structured cabling and deployment coordination.\n\u25cf Management of 24x7x365 data center support technician groups.\n\u25cf Responsible for onsite customer support, security checking process, facility walkthroughs, technical remote hand/eyes support of customer and managed services environments.\n\u25cf Management of data center design and construction process for facilities..\n\u25cf Perform review of design documentation and provide feedback/direction for the design.\n\u25cf Responsible for the Planification and installation of Cable management within a Data Center. This would include CAt5/Cat6 and Fiber( Single mode and Multimode) for customer from small to large scale.\n\u25cf Worked closely with the facility team, vendors, suppliers and the city of Chicago team to be able to open the facility on schedule.\n\u25cf Worked with the sales team in order to provide customers the very best solution for their needs.\n\u25cf Extensive knowledge of Racking and Stacking all types of equipment/hardware such as Patch panels, all types of servers, routers, firewalls, San Devices, Core switches, EMC equipment, Tape Libraries.\n\u25cf Extensive knowledge and hands on experience on build outs such as single rack layouts, cages with multiple racks, Suites with 50 racks and above. This also includes building ladder racks and installing fiber runners.\n\u25cf Managed a team of 18 people and responsible for day to day operations.\n\u25cf Responsible for all Commissioning and Decommissioning of Hardware and customers.\n\u25cf Managed tickets and customer communication for any activity that requires DC Operations assistance.\n\u25cf Knowledge of Hvac, Power and power distribution, Fire and security procedures and protocols to secure the building as well to meet OSHA compliance.\n\u25cf Developed and implemented asset management systems and procedures to ensure all physical assets are accounted for. his including tracking equipment from ordering through disposal of equipment.\n\u25cf Responsible for managing multiple tasks/projects at the same time to be able to meet critical deadlines..\n\u25cf Responsible for the data center layout and documentation. This included the rack elevations, equipment, port layout, cabling layout for single racks, cages, suites meet me rooms and internal POP rooms.\n\u25cf Responsible for technical support to all our customers work with the facility team and assist them when they needed assistance.\n\u25cf Mentor the team and provided leadership in order to provide world class customer service to all our customers, partners as vendors and business partners across the enterprise', u""Data Center Operations Director\nDetroit, MI\nNovember 2014 to February 2016\nProvide the leadership, tactical execution and business acumen necessary to support the diverse site needs. Responsible to coordinate, synthesize and integrate all aspects of corporate real estate management, financial analysis, and maintenance of all natures; maintenance contracting for service to critical equipment, master planning for the facility's data center floor and new construction.\n\u25cf Responsible for managing multiple teams of the facility which oversees maintenance, physical security, data center operations and business continuity and disaster recovery procedures for all sites locally as well as overseas..\n\u25cf Responsible for the day-to-day operation of the DC Operations for Nexcess.\n\u25cf Ensured that all DC Operations tasks were running smoothly, efficiently and in a direction agreed upon by management and contractual agreements from the clients..\n\u25cf Promoted the highest standard of customer service (internally and externally) for all staff members as well as clients.\n\u25cf Developed and implemented policies, controls and procedures for the department including but not limited to hardware health monitoring, cable management, hardware replacement, equipment audits, emergency and procedures.\n\u25cf Ensured policies and procedures were implemented for maximum uptime for clients.\n\u25cf Managed remote operations (utilizing remote hands) for all remote colocation facilities utilized by Nexcess.\n\u25cf Managed tickets and customer communication for any activity that requires DC Operations assistance.\n\u25cf Developed and implemented asset management systems and procedures to ensure all physical assets were accounted for. This included tracking equipment from ordering through disposal of equipment. Monitoring and tracking equipment being RMAd.\n\u25cf Provided effective and inspiring leadership. Developed a broad and deep knowledge of all systems that affect the department and leaded by example.\n\u25cf Promoted a culture of high performance and continuous improvement that values learning and commitment to quality - especially as it pertains to customer service.\n\u25cf Lead, coached and developed training programs to ensure all team members were educated and capable of any task within the department.\n\u25cf Improved upon current policies and procedures to ensure smooth operation of the department.\n\u25cf Worked with the facilities department to cross train DC Operations team members on HVAC, UPS, Generator systems to provide updates to management and facilities in the event of a critical facilities issues.\n\u25cf Responsible for staffing and interviews.\n\u25cf Worked with the marketing and sales team to improve sales and implemented processes and procedures that allowed customers boarding to be more efficient and faster."", u""Data Center Operations Senior Engineer\nQuality Technology Services - Miami, FL\nMay 2011 to November 2014\nResponsible for all aspects of the Data Center operations.\n\u25cf Provided world customer service to all our customers in general.\n\u25cf Constantly working with departmental teams and business partners to achieve internal goals while meeting the customer's needs.\n\u25cf Work with many carriers while doing fiber optic installations for our customers when requested.\n\u25cf Work on hardware installation for our customer as well as internal projects when doing maintenance or upgrades to our current systems/infrastructure.\n\u25cf Ensure that all projects are always on target in order to meet critical deadlines and meets the customer's' requests.\n\u25cf Provided technical support to all our customers work with the facility team and assist them when they need assistance.\n\u25cf Responsible for documenting all procedures and processes for different areas of the business while complying with our standards as well as PCI mandates.\n\u25cf Provide assistance and training to our security staff.\n\u25cf Constantly involved on all Tours, potential prospects and sales.\n\u25cf Always promoted new services and new business to our customers so they know we are always there for them.\n\u25cf Developed solutions to increase efficiencies of the business and reduce cost while complying with PCI compliance.\n\u25cf Responsible for the quality assurance as well as testing while making sure all meets the highest standards and PCI compliance regulations.\n\u25cf Worked on many build outs from the ground up as the Data Center continued to grow while providing the highest customer service and world class assistance to all our customer as well as our business partners.\n\u25cf Worked on equipment upgrades, rack and stack as well as buildouts.\n\u25cf Worked on cross connects and copper and fiber optics installations for customers.\n\u25cf Responsible for the shipping and receiving for customers.\n\u25cf Troubleshooted network issues..\n\u25cf Performed inventory audits on a monthly basis.\n\u25cf Processed all customers remote hands and eye / tickets while on shift.\n\u25cf Performed backups for customers and shipped them off site via Iron mountain.\n\u25cf Prepared, Ran, dressed and tested any cables ( copper, coax or fiber) for customers and new build outs.""]","[u'in Computer Science', u'']","[u'Borough of Manhattan Community College\nSeptember 1988 to May 1991', u'George Washington High School\nMay 1986']"
0,https://resumes.indeed.com/resume/c78cae8e4be8a2b2,"[u'Data Analyst\nWT2 Solutions. Inc - Somerville, NJ\nJuly 2016 to August 2016\n\u2022 Collected the data of cellphone towel of Telecommunication Company (Windstream, Alltel, etc.)\n\u2022 Sorted, organized, reviewed and corrected the data for errors or incompatibilities in Teradata database.\n\u2022 Designed Sql queries to retrieve data from database and exported spreadsheets as required.\n\u2022 Analyzed data by technical procedure and software to get the conclusion or result.', u'Plot Plan Engineer\nEast China Electric Power Design Institute (Top one in Industry) - Shanghai, CN\nJuly 2010 to July 2015\nProject engaged was awarded as the Best Engineering Design Award of Shanghai.\n\u2022 Specialized in design of thermal and new energy power plants both domestic and abroad. Focused on designing, improving and implementing an integrated system made up of men, materials, equipment, energy and information. Designed engineering drawings by communicating with other engineers and prepared reports and presented to superiors and government departments.\n\u2022 Engaged in plant site selection, plant overall planning, plant general layout, vertical arrangement, comprehensive pipeline arrangement, road design and earthwork calculation. Through the design and adjustment process, managed to get a plan with optimal function and high efficiency.']","[u'MS in Industrial and Systems Engineering', u'BS in General Layout and Transportation Engineering']","[u'Rutgers University-New Brunswick New Brunswick, NJ\nSeptember 2015 to May 2017', u""Xi'an University of Architecture and Technology Xi'an\nSeptember 2006 to June 2010""]"
0,https://resumes.indeed.com/resume/597385e7ecd99c1d,"[u""Data Analyst\nKonnections - Mumbai, Maharashtra\nAugust 2015 to July 2016\nActed as Data Analyst Lead for a technology startup specializing in design and development of commercial websites\nand mobile apps.\n\u2022 Used data analysis for storytelling and turning data into actionable insights.\n\u2022 Produce daily, weekly and monthly reports to track KPI's and build customized dashboards in Google Analytics\n\u2022 Created, gathered and analyzed campaign performance data to find trends and predict future sales.\n\u2022 Performed market analysis to efficiently achieve objectives, increasing sales of service by 20%.\n\u2022 Used SQL for maintaining ETL processes to gather product performance data.\n\u2022 Developed ad-hoc reports and presented visualizations as Tableau Dashboards using time series analysis, bar graphs,\nscattered plots etc.\n\u2022 Documented key findings and recommendations and presented it to the team to help improve strategies and operations."", u'Data Analyst\nSun Life Financial\nAugust 2013 to July 2015\nCreated Test Cases based on the Business requirements that is Source to Target Detailed mapping document\nTransformation rules document.\n\u2022 Involved in Data mapping specifications to create and execute detailed system test plans. The data mapping specifies\nwhat data will be extracted from an internal data warehouse, transformed and sent to an external entity.\n\u2022 Involved in extensive DATA validation using SQL queries and back-end testing\n\u2022 Used SQL for Querying the database in UNIX environment\n\u2022 Designed separate test cases for ETL process (Inbound & Outbound) and reporting\n\u2022 Involved with Design and Development team to implement the requirements.\n\u2022 Developed and Performed execution of Test Scripts manually to verify the expected results\n\u2022 Design and development of ETL processes using Informatica ETL tool for dimension and fact file creation\n\u2022 Involved in Manual and Automated testing using QTP and Quality Center.\n\u2022 Performed Black Box - Functional, Regression and Data Driven. White box - Unit and Integration Testing (positive\nand negative scenarios)\n\u2022 Defects tracking, review, analyzes and compares results using Quality Center.\n\u2022 Participated in the MR/CR review meetings to resolve the issues.\n\u2022 Trained and mentored new team recruits while leading their initial project orientation sessions.', u""Data Quality Analyst /Test Engineer\nNational Australian Bank\nFebruary 2011 to March 2013\n\u2022 Performed Data warehouse and Automation quality assurance for Waterfall and Agile projects.\n\u2022 Validated ETL data flows using shell scripts from upstream OLTP systems to OLAP systems.\n\u2022 Automated Regression test suite using QTP/UFT, which led to 40% reduction in Data Verification efforts\n\u2022 Effectively utilized SQL and Shell scripting skills to successfully deliver Database Testing project with excellent\ncoverage. Received coveted GEM award.\n\u2022 Reduced 35% of manual efforts in Test Preparation phase by scripting VBA macro application using HP ALM's API.\nWas featured in Infosys's Hall of Fame.\n\u2022 Prepared interactive dashboards using Tableau to validate upstream data extracts.\n\u2022 Experienced in handling Functional, Non-Functional & Regression Testing.\n\u2022 Trained and mentored new team recruits while leading their initial project orientation sessions.\n\u2022 Assisted senior management with project estimations, effort billing and cost tracking activities\n\u2022 Performed Requirement Analysis, Project Plan Designing, Execution and E2E Defect Tracking.""]","[u'Master of Science in Business Intelligence & Analytics in Business Intelligence & Analytics', u'Bachelor of Engineering in Engineering']","[u'Stevens Institute of Technology Hoboken, NJ\nAugust 2016 to December 2017', u'MDU University\nJune 2006 to May 2010']"
0,https://resumes.indeed.com/resume/ba2616f476f096a8,"[u'Engineer/ Data Analyst\nVox Technologies - Richardson, TX\nSeptember 2016 to Present\n\u2022 Creating and maintaining various relational databases.\n\u2022 Devising ways of extracting data from one or more source files and Databases in order to gather client data requirements.\n\u2022 Developing, and Modifying Queries with SQL used for manipulating and retrieving data from multiple tables in the database while ensuring data integrity.', u'Production Engineer\nAscon Chemicals - Birmingham\nDecember 2010 to May 2016\n\u2022 Inspected production site to monitor progress in production and ensured that every Lubricant meet desired specifications and scheduled quarterly inspections and maintenance of Blending Tanks, pipes, Pumps and valves and Boilers\n\u2022 Tested for the Kinematic Viscosity of lubricants at different temperatures using viscometer tubes and viscometer baths and extrapolated to get the Viscosity Index of every lubricant produced.\n\u2022 Tested and generated formulations to specify the quantity of base oil and additives to be used for blending on production floor.\n\u2022 Conducted training sessions of newly employed Plant Technicians on various chemical operations.']","[u'Master of Science in Systems Engineering and Management', u'Master of Engineering in Petroleum and Environmental Technology', u'Bachelors of Engineering in Petroleum Engineering']","[u'University of Texas Dallas, TX\nSeptember 2017 to May 2019', u'Coventry University Coventry\nSeptember 2012 to April 2014', u'University of Benin Benin City\nMarch 2004 to December 2008']"
0,https://resumes.indeed.com/resume/c9597c1f47810797,"[u'Data Engineer\nAcademic Project - Los Angeles, CA\nJanuary 2017 to March 2017\nLarge-Scale Data Mining: To analyze data from different perspectives and retrieve useful information through data with Machine Learning Skills\n\u2022 Utilized Python to implement different Regression Analysis including Linear Regression, Polynomial Regression,\nRandom Forest, Neural Network and L1-L2 Regularization to find the best way to predict or classify data\n\u2022 Tested the generalization of the model with 10-fold cross-validation to handle over-fitting and use the Root Mean Square Error', u'Data Engineer\nAcademic Project - Los Angeles, CA\nOctober 2016 to December 2016\nMySQL Database System: To develop a website about movies and actors with the support of MySQL\ndatabase system, which provide an interface for users to interact with data\n\u2022 Created relational database system with MySQL and set up integrity constraints\n\u2022 Deployed a website that based on PHP, which could execute various queries for a movie database including adding new\ninformation and comments on movies and actors\n\u2022 Implemented B+ Tree index for the SQL in C++ to insert and delete entries so that the database could have\nsignificantly less time to retrieve data from the database', u'Software Engineer\nNational Instruments\nApril 2016 to August 2016\nGoal: To provide a new feature called Timed Loop on Real Time Embedded System for customers to have\ncurrent state information and error messages\n\u2022Built a C++ API from the static library and made sure the new library follows the same coding convention\n\u2022Developed Timed Loop under Agile Development Methodology and iterated the development process\n\u2022Updated the implementation to Perforce to keep track of the version\n\u2022Tested the CPU idle rate with LabVIEW of the Timed Loop after modification with 5% less CPU utilization', u'Team Leader and Software Developer\nAntique Music Box\nAugust 2015 to December 2015\nTo build an embedded system that could streaming music with Bluetooth and SD card through UART and SPI module\n\u2022 Developed a program in C that can utilize microcontroller to control Vacuum Florescent Display to display song name\n\u2022 Read data from SD card via SPI module and streaming music through Bluetooth Module controlled via UART\n\u2022 Established interrupt for multitasking on PIC33 for playing music as well as showing spectrum of music on VFD', u'Junior Research Assistant\nThe Chinese University of Hong Kong\nJuly 2014 to August 2014\nGoal: To retrieve and analyze data of movement information of robot NAO on MATLAB, which would be\nused to do the Man Machine Interaction research\n\u2022 Retrieved and analyzed data of all the joints from robot NAO and use Dynamic Time Wrapping method to interpret all\nthose data and create a general joints movement data for NAO in MATLAB\n\u2022 Normalized all data from different experiment subjects and design movements for robot NAO']","[u'Master of Science in Electrical Engineering', u'Bachelor of Science in Electrical Engineering']","[u'University of California Los Angeles, CA\nSeptember 2016 to December 2017', u'Purdue University West Lafayette, IN\nAugust 2012 to December 2015']"
0,https://resumes.indeed.com/resume/922243a4b1d948a4,"[u'Software Engineer/Data Scientist\nTickPredict, LLC - Chicago, IL\nNovember 2017 to Present\n\u2022 Developed live tick data feed, real time orderbook and FIX trading engine in Python, C/C++ for cryptocurrency exchanges. Built a high frequency market making trading application in Linux.\n\u2022 Created tools and scripts for proprietary signals analysis and statistical modeling in Numpy, Pandas. Assisted to research and perform latency analysis for cryptocurrency trading.\n\u2022 Learnt AWS cloud platform, docker, blockchain and cyptocurrency fundamental.', u'Software Engineer/Trading Analyst\nBDF Trading LP - Chicago, IL\nMay 2017 to October 2017\n\u2022 Developed an algorithmic trading system in C#/WinForm and implemented two trading strategies: single-threaded, trend following strategy; multithreaded, pairs trading strategy.\n\u2022 Backtested various trading strategies in R and Matlab. Investigated machine learning and bayesian econometrics. Improved a pairs trading strategy by using kalman filter.\n\u2022 Completed comprehensive training program covering foreign exchange market, trading fundamental, proprietary supply/demand strategy and trading system.', u'Software Engineer\nBank of America Merrill Lynch - Chicago, IL\nMay 2014 to May 2015\n\u2022 Developed new functionalities and components in Python and C# for Margin Call applications.\n\u2022 Assisted in database development and maintenance in Toad (Oracle Database Management tool)\n\u2022 Participated in designing and providing programming estimates using agile development practices.', u""Software Engineer\nZ Capital Partners, L.L.C - Lake Forest, IL\nAugust 2013 to April 2014\n\u2022 Contributed C#.NET and ASP.NET code to company's proprietary software and website.\n\u2022 Developed a suite of tools to automate data retrieval and storage process for variety of data sources.\n\u2022 Created stored procedures to extract data in SQL Server which resulted in over 20 reports setup."", u'Automation Engineer Intern\nInfinium Capital Management - Chicago, IL\nMay 2013 to July 2013\n\u2022 Studied and improved Python scripts for Market Data Feed Handler and Order Router in Linux.\n\u2022 Worked with developers to identify new testing needs and created test scenarios.']","[u'M.S. in Quantitative Finance', u'B.S. in Computer Science']","[u'Illinois Institute of Technology Chicago, IL', u'Northwestern Polytechnic University']"
0,https://resumes.indeed.com/resume/587967c60b3c48eb,"[u'Data center Technician\nVIRTUSTREAM - San Francisco, CA\nApril 2017 to July 2017\n\u2022 Installed, operated, maintained, repaired and modified Server, storage system and network.\n\u2022 Gathered requirements, run FC and Cat 6\n\u2022 Performed PM for daily and monitor requirements engineering and system usability datacenter\n\u2022 Open ticket for access and customer.\n\u2022 Daily inventory for receiving form Fedex and UPS', u'Data Center Technician\nDATAPIPE - San Jose, CA\nApril 2015 to July 2016\nData center Operation\n\u2022 Installed, operated, maintained, repaired and modified Hardware for customer.\n\u2022 Provided information and technical support to customers.\n\u2022 Gathered requirements, created new Colo and cage for customer\n\u2022 Performed run and lay out requirements Fiber cables and CAT6, CAT5.\n\u2022 Conducted performance analysis on Colo round.\n\u2022 Interfaced with customer personnel to provide quality service and feedback on problem evaluation and resolution.', u'Customer Engineer\nEMC - Santa Clara, CA\nJanuary 2006 to January 2014\n\u2022 Solved many customer troubleshooting problems over the phone, webex.\n\u2022 Install and configuration of EMC Storage system.\n\u2022 Performed server software and hardware installations, testing, troubleshooting and maintenance of systems.\n\u2022 Updated hardware and software at correct revision levels. and FCO\n\u2022 Teamed with Sales, Marketing, Customer Support, and Professional Services to ensure customer satisfaction.\n\u2022 Maintained accurate inventory and return parts in a timely manner.', u'Field Engineer\nSUN MICROSYSTEMS - Santa Clara, CA\nJanuary 2001 to January 2006\n\u2022 Provided ""above and beyond"" service by helping customers resolve software issues obligations.\n\u2022 Performed server software and hardware installations, testing, troubleshooting and maintenance of systems.\n\u2022 Updated hardware and software at correct revision levels. Documented all service work for new installations.\n\u2022 Reviewed and recommended tool techniques and processes that improved problem identification for new products.', u'Customer Service Engineer\nCompaq - Fremont, CA\nJanuary 1999 to January 2001\n\u2022 Tested UNIX/ Open MVS system for high end Server, SAN, Server and tape libraries.\n\u2022 Used Compaq analyzer, performing diagnostics to resolve complex or uncommon or intermittent Functional failures.\n\u2022 Led and participated with Design Engineers from Quality, Test and Development to isolate faults and to debug new and existing products.\n\u2022 Maintained records of log error from the field data to aid in establishing product reliability and failure and to debug new and existing products.', u'Senior Field System Technician\nHITACHI DATA SYSTEM - Santa Clara, CA\nJanuary 1995 to January 1999\n\u2022 Installed, operated, maintained, repaired and modified HDS disk array and mainframe system.\n\u2022 Gathered requirements, created new product final system test process flows.\n\u2022 Performed requirements engineering and system usability testing for field problems.\n\u2022 Conducted performance analysis on laptop used in System disk array & Ethernet device drivers.', u'Test Engineer\nIBM - San Jose, CA\nJanuary 1990 to January 1995\n\u2022 Assisted in development of Disk array FC Adapter.\n\u2022 Helped debug IBM controller card for disk array from system level to board level.\n\u2022 Installed & Assisted in debugging system level issues under IBM SAN test program.\n\u2022 Developed training procedure & conducted sessions of IBM FC disk drive for failure analyst technician level 1 & 2 support of Sun storage and OEM product.\n\u2022 Provided customer support and resolved several customer problems involving complex disk failed.']",[u'A a in A.A. Computer Science'],"[u'American River College Sacramento, CA\nJune 1976 to August 1978']"
0,https://resumes.indeed.com/resume/8cf75c51fbc09774,"[u'Data Engineer\nFAST INC\nOctober 2016 to Present\nLocation: Texas\nFAST INC. (Fund Accounting Software Technologies)\nEmail: hello@jamesss.io\nResponsibilities include data analysis, data conversions, database management,\nPhone: 806 786 6325 reporting, & documentation:\nTwitter: @_ _jamesssio_ _ \u2022 Solely responsible for conversion of client data to migrate into our backend.\nGithub: jmssmth \u2022 Manages a data conversion schedule of over 25+ cities at any given time.\n\nLinkedIn jamesssio \u2022 Manages close to 100 databases and backups on production, staging, and test servers.\n\u2022 Have written to-date over 5,500+ words of process documentation.\nWebsite: jamesss.io\n\u2022 Spends time daily analyzing/querying client datasets for business case decisions.\n\u2022 Works with Implementation and Support to solve ad-hoc client data issues.\n\u2022 Contacts clients directly to initiate onboarding and schedule data extractions.']","[u'Programming & Computation in Computer Science', u""Bachelor's of Applied Science in Business Administration""]","[u'MITx\nJanuary 2016', u'Wayland Baptist University']"
0,https://resumes.indeed.com/resume/727c264ce4f727b5,"[u""Loan Admin Data Analyst\nBayview Loan Servicing, LLC\nFebruary 2017 to Present\n\u2022 Analyzing, interpreting and mapping loans data from Banks/Prior Servicers in order for the data to be ingested into the Bayview Loan Servicing systems (BLS).\n\u2022 Analyzing, interpreting raw data of loans and categorizing it in order to enter the loans with the correct category (Bankruptcy, Foreclosure, Short Sale, Deed in Lieu, etc.) into the BLS's systems.\n\u2022 Data mining, data import/export between various data sources.\n\u2022 Manipulating, cleaning and processing data using Microsoft SQL, Excel and Access.\n\u2022 Proficient in data migration from sources like SQL Server, Text files, Excel to BLS Systems.\n\u2022 Knowledge of BLS's multiple systems and applications used for data ingestion and reconciliation.\n\u2022 Maintaining the integrity of SQL Database and reporting any issues to the architects.\n\u2022 Communicating with different business units regarding process improvement and efficiency.\n\u2022 Enhance business infrastructure through data and project analysis, resulting in improved team performance, cost reduction and effective time and resource management.\n\u2022 Develop new ways to improve streamline processes.\n\u2022 Completing daily, weekly and monthly project deadlines on time, according to clear project plan requirements.\n\u2022 Analyzing raw data, drawing conclusions and developing recommendations. Performing ad-hoc analysis and reporting using V-lookups and Pivot Tables.\n\u2022 Develop spreadsheets to assist in data and project analysis.\n\u2022 Plan, develop and facilitate complex projects from inception through completion, while delivering project on schedule and maintaining a cost effective execution."", u""Data Analyst\nICE Portal\nAugust 2015 to February 2017\n\u2022 Acquire data from primary or secondary external data sources (Hilton Hotels, Wyndham Hotels, Expedia, and Priceline) and maintain databases/data systems (internal databases).\n\n\u2022 Interpret data, analyze results using statistical techniques and provide ongoing reports.\n\n\u2022 Identify, analyze, and interpret trends or patterns in complex data sets.\n\n\u2022 Filter and 'clean' data, review reports to locate and correct problems.\n\n\u2022 Work closely with management to prioritize business information (Prioritize information asked by hotels or website managers (Expedia, Trivago, and Priceline).\n\u2022 Locate and define new process improvement opportunities (Generate innovated ideas and make requests to the IT team in order to smooth our internal process).\n\n\u2022 Use statistical methods to analyze data and generate useful business reports.\n\n\u2022 Data Mapping.\n\n\u2022 Play the manager role of a company that works for ICE PORTAL.\n\n\u2022 Identify and recommend new ways to save money by streamlining business processes (Proposing ideas to improve the workflow and therefore the efficiency and profitability of the company)."", u'Sales Representative\nMazda of Doral Auto Dealership\nApril 2014 to June 2015\n\u2022 Greet arriving customers.\n\u2022 Explain the features of various models and apprise car shoppers of financing options and warranties.\n\u2022 Knowledgeable of the cars to answer questions about gas mileage, the sizes of engines and the colors each model comes in.\n\u2022 Accompany test drivers for liability reasons and to prevent thefts, as well as to answer questions that arise on the road.\n\u2022 Responsible for negotiating trade-in and car prices with customers.\n\u2022 Coordinate financing of the car through the finance department.\n\u2022 Coordinate repairs, servicing and cleaning of cars through the customer service department.', u'Data Analyst\nCommercial Bank\nOctober 2011 to September 2013\n\u2022 Data mining to reveal financial patterns in savings accounts (Running daily scripts to monitor status and activities on bank accounts).\n\u2022 Demographic data mining to provide bank with groups and customers profiles (Ex: providing weekly reports of savings accounts and activities to better understand the dynamics of different types of customers grouped by age).\n\u2022 Proactively guide the bank in preventing financial problems before they occur.\n\u2022 Recommend to the bank new services according to detected trends in user needs.\n\u2022 Acquire data from primary data sources (Excel, .txt, .xml) and maintain databases, using SQL scripts.\n\u2022 Work closely with management to prioritize business information.', u'Software Engineer\nTropical Geographic Institute\nNovember 2009 to July 2010\n\u2022 Software development for the following projects:\n- Project 1: Development of intranets.\n- Project 2: Development of a website for the Survey of the National Spatial Data Readiness Index. Project developed using PHP and CMS Drupal.\n- Project 3: Development of the Tropical Geographic Institute website. Project developed using CMS Drupal.\n- Project 4: Maintenance of the website for the GEF-PNUD Sabana-Camaguey project. Project developed using CMS Drupal.\n\u2022 Manipulating, cleaning and processing data using Microsoft SQL, Access and Excel.', u'IT Support\nStainless Steel Importing Company (ACINOX)\nSeptember 2008 to November 2009\nManipulating and processing data using Microsoft SQL and Excel.']",[u'B.S. in Computer Engineering'],"[u'Havana University of Technologies J. A. E. Havana, CU\nJuly 2008']"
0,https://resumes.indeed.com/resume/5208a11275618081,"[u'Data and Business Intelligence Analyst\nBoston, MA\nMay 2017 to September 2017\n\u2022 Developed and Maintained databases on MySQL\n\u2022 Processed and optimized Data for business solutions using R and Python, thus increased the efficiency by 4%\n\u2022 Created and Maintained Dashboards and Reports on the Business Intelligence Tool Yellowfin 7.2 for Data analysis and visualization\n\u2022 Web Development using Laravel 5.4 framework and phpMyAdmin\n\u2022 Actively coordinated with system administrators for system improvements and enhancements through AWS', u""System Engineer\nInfosys Pvt. Ltd - Bengaluru, Karnataka\nJune 2014 to April 2016\n\u2022 Developed and maintained the Application 'LiFE400' on the AS400 platform for an insurance Client\n\u2022 Automated the common procedures of the application through compilation of the regular procedures, thus enhancing the revenue by 6.2% by saving 5hours/week\n\u2022 Performed troubleshooting, escalating and follow-up on the P3 and P4 Production support issues\n\u2022 Predicted the number of incoming production issues using statistical predictive analysis and hence designed tools for the reduction of the same in the production environment by 8%""]","[u'Master of Science in Engineering Management in Relevant', u'Bachelor of Engineering in Electronics and Telecommunication in Electronics and Telecommunication']","[u'Northeastern University Boston, MA\nMay 2016 to May 2018', u'University of Mumbai Mumbai, Maharashtra\nJuly 2010 to June 2014']"
0,https://resumes.indeed.com/resume/10aec6c25eaff235,"[u'Data Center Engineer\nSprint - Irving, TX\nJanuary 2006 to January 2017\n\u2022 Supervised service contractors over maintenance of UPS, electrical, mechanical and fire alarm systems.\n\u2022 Coordinated facilities change management system to review work orders for the critical environments.\n\u2022 Managed raised floor space allocation to balance electrical power and cooling load demands.\n\u2022 Provided proactive and reactive maintenance of the electrical, mechanical and raised floor infrastructure resulting in 99.998% uptime.\n\u2022 Developed Methods of Procedure to ensure continuity in the data center environment.\n\u2022 Requested and analyzed vendor proposals for annual operational maintenance budget and capital investment projects ensuring high level of service and flat operating expenses.\n\u2022 Collected and tracked data on electrical consumption for executive presentations and corporate social responsibilities satisfying green initiative with EPA for reduction in carbon footprint.\n\u2022 Successfully completed data center expansion projects to increase electrical and mechanical capacity.\n\u2022 Serviced and maintained all critical facilities infrastructure including UPS, electrical generation, HVAC mechanical, fire suppression systems and raised floor.', u'Chief Engineer\nCrescent Real Estate - Fort Worth, TX\nJanuary 1994 to January 2006\nFort Worth, Texas 1994 - 2006\nA commercial real estate company that develops, leases and manages commercial office and resort properties.\n\nChief Engineer\n\u2022 Managed the operations and maintenance of company assets to ensure reliability and tenant satisfaction.\n\u2022 Supervised a team of 5 operating and maintenance engineers.\n\u2022 Developed and coordinated periodic and preventive maintenance procedures.\n\u2022 Consistently observe and implement opportunities to decrease building operating costs and improve maintenance operations.\n\u2022 Review operating expenses to ensure costs stay within projected budgets.\n\u2022 Monitored monthly utility costs to track usage and ensure proper billing.\n\u2022 Review and monitor tenant finish construction projects to ensure company construction standards are met.\n\u2022 Coordinate and implement capital and property improvement projects.\n\u2022 Proposed operating and capital improvement budgets to maintain building currency and efficiency.\n\u2022 Negotiate service contracts for mechanical maintenance, fire life safety maintenance, electrical maintenance and landscape.\n\nSPECIAL PROJECTS\n\u2022 Coordinated and managed six roof replacements projects equaling over 135,000 sq. ft.\n\u2022 Negotiated, coordinated, and supervised the installation of Building Automation Systems.\n\u2022 Procured and implemented several building Fire Alarm System replacements.\n\u2022 Negotiated, coordinated and supervised Cooling tower replacement\n\u2022 Coordinated and supervised parking garage structural repairs and on grade concrete replacements.\n\u2022 Coordinated and supervised property compliance with Americans with Disabilities Act.\n\u2022 Coordinated building glazing and resealing projects.\n\u2022 Implemented landscape renovation projects to remove ill-adapted trees, provide shade to buildings and improve the look of the property.']",[u'Certification in Facility Maintenance'],"[u'Northlake College Irving, TX']"
0,https://resumes.indeed.com/resume/511ea9366cfcfbae,"[u'Data Intern\nCopart Inc - Dallas, TX\nMay 2017 to December 2017\nMachine Learning and Visualization:\n* Predicted auction prices with GRADIENT BOOSTING model in R, to attract 1000s of sellers\n* Invented counter bidding algorithm using linear regression and Java, leading to 16% boost in sales\n* Crafted 9 rich Tableau Stories and several graphs in python (Bokeh, seaborn) for data insights\n* Collaborated with team to capture data requirements and data resources; Designed Backtesting validations\nHadoop, Data Modeling and ETL:\n* Architected dimensional model data warehouse of 11 star schemas, SAP BO universe and SSIS packages\n* Applied Spark transformations for analytics on 20+ JSON and log files after ingesting them into HDFS with Flume\n* Conducted Root Cause Analysis, adopted java coding practices & R Style Guide and documented the team-wiki', u""Data Analyst Intern\nDomino's Pizza - Dallas, TX\nJanuary 2017 to April 2017\nA/B testing, Regression, Net Lift Model, Clustering:\n* Leveraged the above models to filter out 1.7% of the US customers for successful Direct Marketing\n* Exploited SAS Data Management to integrate data on 435 coupons and buying behavior in 15 market sectors\n* Structured QlikView data model by resolving loops and synthetic keys, to craft rich associative dashboards\n* Communicated the analysis and recommendations for customer acquisition and cross-selling to the executives"", u""Business Intelligence Engineer\nBOEING USA - Chennai, Tamil Nadu\nOctober 2013 to June 2016\nData Extraction, Manipulation and Reporting:\n* Incorporated advanced SQL and designed 14 JSP reports to impress customers about aircraft improvements\n* Devised ETL data loads across data lakes, staging area and data marts to cater reporting needs\n* Performed data cleaning of raw big data XMLs with XSL, and extracted them with XQuery using XSD\nDesign Thinking and SQL tuning:\n* Achieved 'BOEING PRIDE' award for reducing report load time by 5 - 6 minutes via query performance optimization\n* Scripted Stored Procedures, Views and Triggers as needed; Reduced lookups with Indexes and Partitions\n* Preached Design Thinking principles to team and created POCs being SME; Configuration Manager for deployments""]","[u'M.S. in IT and Management', u'B.E. in Electronics and Instrumentation']","[u'The University of Texas at Dallas Dallas, TX\nMay 2018', u'Anna University\nApril 2013']"
0,https://resumes.indeed.com/resume/e20a62aedd183997,"[u""Data Engineer (Spark Oriented)\nXCG Design Corp - New York, NY\nOctober 2017 to Present\nBig data analysis of consumer behavior for e-commerce website\nEngaged in the development and implementation of analyzing tools for consumer behavior based on Spark Core, Spark SQL and Spark Streaming. My client is an e-commerce website. Lots of customers will search products and place orders from their website. My duty is to get and manipulate the data we need and provide the data to product manager and data analyst to do further processing.\n\n1. Analyzed consumer behavior (Spark Core and SparkSQL).\nFilter specific customers based on question of interest. Define aggregate function, random sample, get top 10 page views/orders of the hot commodities. Based on specific date, calculate top 3 products for each area. Tune performance and troubleshooting.\nhttps://github.com/naweima/ConsumerBehavior-Spark/blob/master/src/main/java/com/ibeifeng/sparkproject/spark/session/UserVisitSessionAnalyzeSpark.java\n\n2. Developed dynamic blacklist (Spark-Streaming).\nPut customers who clicked one ad over 100 times into blacklist. Calculate ads click stream data for each city. Calculate top 3 hot ads and click stream trend within 1 hour. Save the output into MySQL. Tune performance and troubleshooting.\n\n3. Calculated website conversion rate\n\nResponsibilities:\n\u2022 Experienced with tune performance, including JVM/Shuffle/Transformation and typical tune performance.\n\u2022 Experienced with troubleshooting and data skew for clusters.\n\u2022 Worked with Hadoop Ecosystem components like HDFS, Spark, Hive, Pig, Zookeeper and Shell scripting.\n\u2022 Involved in creating Hive tables, loading and analyzing data using hive queries.\n\u2022 Implemented performance tuning by using Partitions, Broadcasts, Efficient Joins and Pair RDD's.\n\u2022 Optimization of algorithms in Hadoop using Spark Context, Spark-SQL, Data Frames.\n\u2022 Implemented Spark RDD transformations, Actions to implement business analysis.\n\u2022 Migrated the needed data from MySQL into HDFS using Sqoop and importing various formats of flat files into HDFS.\n\u2022 Experience in real time analytics with Apache Spark (RDD, DataFrames and Streaming API), involved in creating SparkSQL Queries.\n\u2022 Good working knowledge in Linux shell environments using command line utilities.\n\u2022 Hands on experience in Spark Streaming with Kafka(Consumer).\n\u2022 Validated the Dstream and created generated new Dstream and saved the data into HDFS.\n\nEnvironment: Hadoop 2.0, Spark 1.5 (Core, SQL, Streaming), Java 1.7, Scala 2.11, MySQL, Hive 0.13, CDH 5, Flume 1.5, Zookeeper 3.4.5, Kafka 2.9, Eclipse"", u'Integration Engineer (Spark Oriented)\nEricsson - Plano, TX\nJune 2017 to September 2017\nEricsson Expert Analytics\nEricsson Expert Analytics (EEA) is a multi-vendor, big data analytics product for mobile operators who want to capitalize on their network data.\nThe data is collected from various interfaces of the network domains, correlated by the built-in engine and stored in databases (Hbase) and HDFS to allow evaluation and observation of customer Quality of Service and Quality of Experience.\n\nResponsibilities:\n\u2022 Collected data from 4 data sources, including Ericsson probes, EBM, BS and intel CPE.\n\u2022 Maintained health check for all five layers of EEA.\n\u2022 Generated health check report for all data sources.\n\u2022 Modified all hard code in the shell scripts.\n\u2022 Write shell scripts, SQL and create test data.\n\u2022 Troubleshooting script issues and performing design and code reviews.\n\u2022 Implemented Data Integrity and Data Quality checks using shell scripts.\n\u2022 Worked with Hadoop Ecosystem components like HDFS, Hive, Pig, HBase, Zookeeper and Shell scripting.\n\u2022 Experience in working with Hadoop/Big-Data storage and analytical using tools like SSH, Putty.\n\nEnvironment: Hadoop 2.0, Flume 1.7, Zookeeper 3.4.11, Putty 0.7, WinSCP 5.11.1', u""Data Programmer\nSMU - Dallas, TX\nSeptember 2016 to April 2017\nBuilding Predictive Models for University Rankings using R\nEstablished predictive models, association rules and cluster analysis based on university ranking in real world and provided important features as well as prediction results.\nPredictive Models: Random forest, Artificial neural network and Gradient boosting decision tree.\n\nUsing Random Forest and Logistic Regression to Predict Romance of Student using Python\nEstablished random forest and logistic models based on multivariate database of students' s romance (Yes, No) and provided important variables as well as prediction results of test data.\n\nANOVA and SLR Analysis of Dementia Data using SAS\nEstablished linear model based on multivariate database of clinical patients (nondementia, converted, dementia) profiled by OASIS (Open Access Series of Imaging Studies) with SAS and provided potential clues for clinical prediction.\nMethods: Kruskal-Wallis, Rank-sum, Post-hoc comparison, Planned comparison, Linear regression\n\nResponsibilities:\n\u2022 Experience in modeling using machine learning algorithms. Supervised learning (Logistic Regression, Random Forest, Gradient Boosting, Neural Networks), Unsupervised learning (Clustering)\n\u2022 Experienced in developing codes in R, Python, SAS, SPSS and Excel\n\u2022 Experience with Scikit-learn, Pandas, Numpy, Matplotlib and Seaborn Python libraries during development life cycle\n\u2022 Experienced in developing tools to support strategic business decision making and forecasting\n\u2022 Good skills in working with large datasets, and using advanced data analysis (SQL)\n\nEnvironment: R Studio 0.99, Python 3.5, SAS 9"", u'Data Engineer\nMingsi Market Consulting Company - Beijing, CN\nJune 2013 to June 2015\nReal-time detector of DDOS attacks\nEngaged in the development, implementation, improvement and maintenance of a real-time detector for botnet attacks using Flume, Kafka, Spark-streaming and successfully developed a message-based screen-and-detect application that sourced the ip-address on the fly to cope with Distributed Danial of Service attack (DDOS). The detector would calculate all ip address that belongs to DDOS attacks, added them to blocklist. Developed Spark scripts by using Scala and shell commands as per the requirement.\n\nPredictive modeling of telecom defaulters in python\nConducted a prediction task of telecom defaulters. Evaluated different algorithms, tune hyper-parameters for optimal performance for each algorithm, combine the outputs of the different algorithms into a single ensemble.\nPredictive Models: Logistic regression, Random forest, Multilayer perceptron, SVM\n\nResponsibilities:\n\u2022 Used Pig as an ETL tool to do Transformations, joins and some pre-aggregations before storing data into HDFS.\n\u2022 Driven to architect Big Data solutions on multiple platforms using data analytics.\n\u2022 Experience in developing complex SQL queries, Stored Procedures, Functions.\n\u2022 Installed, configured, and updated Linux machines, with Ubuntu and CentOS. Deploy and install applications.\n\u2022 Experience in ingesting the streaming data to Hadoop clusters using Flume and Kafka.\n\u2022 Build predictive modeling using Logistic regression, Random forest, Multilayer perceptron and SVM.\n\nEnvironment: Python 3.3, JupyterNotebook 4, Hadoop 2.0, Spark 1.0, Java 1.7, Scala 2.11, MySQL, CDH 3,Hive 0.12, Flume 1.5, Zookeeper 3.4.5, Kafka 2.9, Eclipse', u'Data Analyst\nBeijing District Community Center - Beijing, CN\nJune 2011 to May 2013\nProgrammed a highly automated Economic Security Assessment (ESA) platform via statistical learning method that conducts pre- and post-test analysis and makes sound inference to the service efficiency (as a latent parameter).\n\nResponsibilities:\n\u2022 Consulted with department director in the employment and refinement of fundraising solutions for non-profit groups via a SAS/macro-based finance model.\n\u2022 Good use of various statistical procedures including PROC CONTENTS, PROC FREQ, PROC MEANS, PROC REPORT, PROCGPLOT, PROC BOXPLOT, PROC CORR, PROC GLM, PROC ANOVA, PROC LOGISTIC other SAS/STAT or SAS/GRAPH procedures.\n\u2022 Experienced in producing RTF, HTML and PDF files using SAS/ODS, well versed with creating HTML Reports for financial data using SAS ODS facility.\n\nEnvironment: SAS 9, SPSS 19']","[u'Master in Applied Statistics and Data Analytics in Applied Statistics and Data Analytics', u'Bachelor in Statistical Science in Statistical Science']","[u'Southern Methodist University Dallas, TX', u'Beijing University of Technology Beijing, CN']"
0,https://resumes.indeed.com/resume/0f7f55c50e8bf3fb,"[u'CONSULTANT MANAGER\nSOGETI USA, LLC\nOctober 2015 to April 2017\nServed as the main Level III Project Database Administrator resource/backup Scrum Master of scrum teams for Data Lake Ingestion projects. Lead the database analysis, design, and build effort for the database migration from Teradata to Oracle 11g and Pivotal Greenplum databases. Tasks/achievements include but not limited to:\n\u2022 Restructured the logical data model of the transactional and data warehouse pieces of the application from Teradata and moved the application schema changes to Oracle and Pivotal Greenplum databases using ERwin Data Modeler and ERStudio Data Architect.\n\u2022 Ensured that the database designs fulfill the requirements, including data volume, frequency needs, and the long-term growth of data base3d from user stories filed in Jira.\n\u2022 Assisted in deciding the cross-application data standards, data distribution standards, and tuning strategies. Worked with other architects to ensure that all components will work together to meet goals and performance goals as defined in the requirements. Identified and communicated any cross-area or cross-release issues that may affect other areas of the project\n\u2022 Ensured that the programmers and operations database administrators responsible for developing the database thoroughly understand the requirements and designs as part of IT Service Management\n\u2022 Reviewed the database deliverables throughout development to ensure quality and traceability to requirements and adherence to all quality management plans and standards.\n\u2022 Reviewed and revised the FDM Oracle database parameters and application SQL code, and implemented performance fixes by applying SQL plan baselines, SQL profiles, or rewriting the SQL code. Designed and implemented Oracle Resource Manager to the Oracle databases to manage database resource allocation and scheduling for database sessions and reassessed and implemented changes in system and objects statistics gathering for FDM application database tables for optimal SQL execution.\n\u2022 Revised logical structures of application tables to make use of Oracle Materialized views and Query Rewrites to improve on SQL execution of ETL and ELT jobs for data lake data propagation.\n\u2022 Performed basic Oracle DBA functions such as database refreshes using Oracle datapump, performance tuning, data modeling, availability, backup, and recovery using RMAN, ASM, RAC, and database flashback, monitoring using Oracle Grid Control and other DBA functions as needed. Designed and created the Sogeti Training environment for DataStax Cassandra DB learning and certification initiatives.\n\u2022 Served as the main contact point for technical issues filed in ServiceNow by Informatica team and MRO/OSB teams in loading SAP data files to Oracle databases.', u""DATA ENGINEER III\nHEWLETT PACKARD ENTERPRISE SERVICES, LLC\nAugust 2012 to October 2015\nLed Oracle Data Masking and Sub-setting project as the Technical/Solutions Architect involving HP and Oracle consultants using Oracle Enterprise Manager 12c and 11g to mask PII/PHI present in the databases as part of contractual obligations to the client. Served as the primary Data Architect and data modeler for the Affordable Care Act Implementation for the state of California. Tasks/achievements include but not limited to:\n\u2022 Worked with the customer and end users to define application and technical requirements based from ITIL V3 and CMMI standards\n\u2022 Defined the structure of the application and technical architecture, ensuring that the structure meets the security business requirements and performance goals, and ensuring that the technical direction is consistent with the client's long-term direction.\n\u2022 Reviewed and integrated all requirements for the application, including functional, security, performance, quality, and operations requirements.\n\u2022 Reviewed and integrated the technical architecture requirements for the development, execution, and operations environments. Obtained stakeholder buy-in for application and technical designs.\n\u2022 Reviewed application and technical architecture deliverables throughout development to ensure quality and traceability to requirements. Led the application and technical architecture analysis, design, and implementation, and ensure that the product fulfills the requirements.\n\u2022 Ensured that the database designs fulfill the requirements, including data volume, frequency needs, and the long-term growth of data. Assisted in finding the cross-application data standards, data distribution standards, and tuning strategies.\n\u2022 Performed basic Oracle DBA functions such as database refreshes using Oracle datapump, performance tuning, data modeling, availability,\nbackup, and recovery using RMAN, ASM, RAC, and database flashback, monitoring using Oracle Grid Control and other DBA functions as needed."", u'DATA ENGINEER III\nHEWLETT PACKARD PHILIPPINES\nSeptember 2009 to July 2012\nLed the HP Application DBA team for the EXADATA migration project that involved all Optima databases (DW and OLTP). Served as the main Siebel Application DBA. Tasks/achievements include but not limited to:\n\u2022 Provided advance technical consulting and advice to others on proposal efforts, solution design, system management, tuning and modification solutions for DB performance issues. Assisted the technical architect to plan and test disaster recovery actions.\n\u2022 Built, tested, and implemented changes to the databases, analyzed and revised disaster recovery plans, and operations documentation needed by approved work requests. Designed, supported and enhanced design data dictionaries, physical and logical database models using ERwin Data Modeler, and performance tuning.\n\u2022 Maintained the Configuration Management database for the Data Management team. Developed instructions and maintained operational documents and provided training for the operations domain to perform data dictionary back-up and recovery.\n\u2022 Monitored database performance and space requirements, and perform or supervise database performance tuning.\n\u2022 Provided instructions and trained operations team for routine maintenance and support tasks.\n\u2022 Provided second tier support for problems outside the scope of operations personnel and addressed all issues assigned in HP Service Manager.', u'SENIOR SOFTWARE ENGINEER\nPHILIPPINES DELIVERY CENTER, ACCENTURE INCORPORATED\nOctober 2006 to July 2009\nSpearheaded the Advanced Application Support team for Rapid Application Development Project for a German bank. Spearheaded the conversion team in migrating the data of the Logistics application. Tasks/achievements include but not limited to:\n\u2022 Led the team in customizing and supporting more than fifty strategic applications, which are written in different technologies such as Oracle PL/SQL, MS Access, VBA, Java, and T-SQL.\n\u2022 Monitored capacity allocation required in performing application support as needed and escalate capacity shortage as required to the Delivery Lead and ensured compliance with SOX Audit Requirements.\n\u2022 Developed good working relationship with internal clients to understand and discuss their needs.\n\u2022 Ensured all source control procedures are followed, technical and application project documentation is complete and up-to-date, and proper release procedures are followed.\n\u2022 Served a Quality Manager for the projects making sure all Development and Operational work and deliverables adhere to CMMI and ITIL standards and guidelines']",[u'Bachelor of Science in Computer Science'],[u'DE LA SALLE UNIVERSITY Manila\nJanuary 2000 to January 2004']
0,https://resumes.indeed.com/resume/02d01124db860935,"[u'Data Analyst\nTexas A&M University - College Station, TX\nJune 2016 to Present\n\u2022 Utilize SQL Server to provide data for survey and adhoc requests.\n\u2022 Update and create Tableau dashboards based on student data.\n\u2022 Document and improve procedures for reporting data to various institutions.', u'Mechanical Design Engineer\nForum Energy Technologies - Houston, TX\nNovember 2014 to April 2015\n\u2022 Provided customer support by answering technical product and situational questions seen on the field.\n\u2022 Organized and fixed existing drawings in the database to be in compliance with the new design policies.\n\u2022 Created procedures per API to standardize and streamline product design.', u'R&D Engineer\nBaker Hughes Incorporated - Houston, TX\nJanuary 2013 to August 2014\n\u2022 Performed calculations and created performance data envelopes for tools in Excel using pivot tables.\n\u2022 Provided quality assurance by collecting and analyzing manufacturing data in order to make the appropriate\ndesign changes to reduce product defects.\n\u2022 Created test procedures, reports and custom fixtures as well as conducted testing to troubleshoot defective\nequipments.']","[u'Master of Science in Data Science', u'Bachelor of Science in Mechanical Engineering']","[u'Indiana University Bloomington Bloomington, IN\nAugust 2016 to Present', u'Baylor University Waco, TX\nAugust 2008 to December 2012']"
0,https://resumes.indeed.com/resume/2c9bf40eb92ce87c,"[u'Data Analyst Intern\nSawyer Studios - New York, NY\nSeptember 2017 to December 2017\n- Analyzed and interpreted a range of data type\u2019s sources from Facebook, Instagram, YouTube, Google AdWords API and other tools for ad campaigns to predict real-time bidding (RTB).\n- Build machine learning models to form strategies for individual advertisement campaigns.\n- Designed interactive dashboards, preparing ad-hoc daily, weekly and monthly reports using Datorama and Shiny R.\nTechnology: R, RStudio, R Shiny, MySQL, SQL Workbench/J, Trade Desk, Datorama, MS Excel, DCM', u'System Engineer Intern\nWolters Kluwer - New York, NY\nJuly 2017 to August 2017\n- Responsible for monitoring the application and servers and recommend agile members what they can do best to increase the performance of the system using New Relic and Splunk tool.\n- Implemented queries using SQL to resolve ad-hoc issues to ensure data consistency for reporting purposes.\n- Designed and visualized reports, charts, and tables detailing the factors that affected company\u2019s software.\nTechnology: SQL Server, Python, MS Excel, New Relic, Splunk', u'Data Analyst Internship\nStrides Software Solutions Pvt. Ltd - Mumbai, Maharashtra\nMay 2014 to August 2014\n- Extracted interpreted and analyzed data to identify key metrics and transform raw data into meaningful, actionable information.\n- Conducted statistical analysis for data sets, drew conclusions and made suggestions.\n- Created visually impactful dashboards in Excel and Tableau for data reporting by using pivot tables and other Excel functions.\nTechnology: R, Python, MS Excel, Tableau']","[u'Masters in Information Technology', u'B.E in Computer Engineering']","[u'Rutgers University-Newark Newark, NJ\nAugust 2015 to December 2017', u'University of Mumbai Mumbai, Maharashtra\nAugust 2012 to May 2016']"
0,https://resumes.indeed.com/resume/9a2c7aee564a7c7f,"[u""Data Science Intern\nZions Bancorporation\nJune 2017 to August 2017\no Developed and migrated data ingestion pipelines for Banking applications using Streamsets.\no Developed generic python scripts to automate data ingestion stages of Streamsets application.\no Developed unit test cases and designed workflow for mobile sign-on application's fraud detection module."", u""Big Data Developer\nAmdocs\nJuly 2015 to July 2016\no Developed applications to automate and simplify the Call Data Records and Digital Data Records applications.\no Designed and developed continuous integration of Amdocs' Big Data framework on Jenkins.\no Developed and delivered automated BI, Data Reconciliation reports for Telecom applications.\no Designed and implemented AT&T's big data backup and recovery for Hortonworks clusters.\no Completed a POC to maintain Referential Integrity and foreign key concept in Amdocs' Big Insights.\no Tested Amdocs' Big Data Insights framework's integrity in AT&T Environment."", u""System Engineer/Software Developer\nTata Consultancy Services\nJanuary 2012 to July 2015\no Designed and developed Mainframe's SAR reports to monitor daily POS activities.\no Designed and developed daily, weekly and bi-weekly employee management reports.\no Automated user's security administration requests and participated in single sign-on for multiple applications.\no Collaborated with teams in agile development and developed Unit test cases to productionize jobs.""]","[u'MS (Computer Science in Data Mining', u'BTech in Computer Science']","[u'California State University Fullerton\nMarch 1970 to May 2018', u'Uttar Pradesh Technical University\nMarch 2000 to May 2011']"
0,https://resumes.indeed.com/resume/c7add5eed2575d33,"[u'President/Owner\nLightspeed Technical Services - Eureka, IL\nFebruary 2014 to Present\nAccomplishments:\n\u2022 Built company from the ground-up, profiting $100K within the opening year\n\u2022 100% client rating garnered through over 1000 service events and installations\n\u2022 Successfully partnered with general contractors on new site construction to meet every deadline\n\u2022 Designed/constructed the entire data and VoIP backbone for a new $7M law office\n\u2022 Notable clientele that includes AT&T, Cisco, Avaya, IBM and various nationwide retail and banking institutions\n\nResponsibilities:\n\u2022 Secures new information technology contracts\n\u2022 Performs day to day operations. Includes installations and upgrades for:\n\n\u25e6 Servers\n\u25e6 Switches\n\u25e6 Routers\n\u25e6 VoIP\n\u25e6 Security / Surveillance\n\u25e6 Structured cabling infrastructure', u'Data Center Engineer\nCaterpillar - East Peoria, IL\nNovember 2011 to February 2014\nAccomplishments:\n\u2022 Held data center to 100% uptime over a two year period\n\u2022 Immediately and precisely dispatched with the Caterpillar emergency response team to secure data center during a large scale flooding event, resulting in a commendation from the Army Corps of Engineers\n\u2022 Prevented catastrophic shut down after primary and redundant cooling failed by remaining calm and implementing nonstandard cooling to the Midwest cluster - the heart of the data center\n\nResponsibilities:\n\u2022 Maintained secure data center\n\u2022 Performed changeover requests\n\u2022 Provided planning and installation of equipment as well as fiber optic and copper structured cabling\n\u2022 Planning, implementation, and repair of liquid cooling system, air cooled units, and redundant power\n\u2022 Provided oversight to utility power input, automatic transfer switches, and diesel generator power']",[u''],"[u'Spartan Aeronautical University Tulsa, OK\nJanuary 1999 to January 2001']"
0,https://resumes.indeed.com/resume/0915a662e9d0cec8,"[u'Inventory management system - Project lead\nOctober 2017 to December 2017\n\u2022 Led a team of 5 members to create an application for an Inventory management system\n\u2022 Implemented using MSSQL DDL statements, stored procedures, SQL Triggers and functions with defined business constraints and performed CRUD operations with a response time of 50 ms', u""Data Engineer- Data Integration\nLowe's services Pvt Ltd\nJanuary 2016 to August 2017\n\u2022 Designed and developed new integrations for services from sterling, siebel, click, CPnI, dotCom\n\u2022 Provided testing support and triages during QA from CIT through UAT testing, coordinated\ndeployment activities during all major projects and releases, fixed time critical production incidents\n\u2022 Configured loadbalancer groups, XML managers, count monitors, web application firewall, TCP proxy\nservices and multi-protocol gateways, count monitors, frameworks and utilities for better visualization\n\u2022 Created and implemented shell scripts for performance improvements and automation of domain\nbackups, status alerts to save time and manpower and reduce CPU utilization from 95 % to 68 %\n\u2022 Received an award for 'Continuous Process and Delivery excellence' - March 2017""]","[u'Master of Science in Data Analytics in Statistics', u'Bachelor of Engineering in Computer Science in Analysis and Design of Information Systems']","[u'Northeastern University Boston, MA\nSeptember 2017 to May 2019', u'Visvesvaraya Technological University\nAugust 2011 to June 2015']"
0,https://resumes.indeed.com/resume/ceebeabae0a33e88,"[u'Data Engineer\nCompas Technology LLC - San Francisco Bay Area, CA\nJune 2016 to Present\n\u2022 Analysis of database tables as they relate to a data warehouse model and determination of likely intra and inter-database relationships.\n\u2022 Assess the quality and consistency of data stored in a source database and develop recommendations for data cleansing based upon data warehouse business rules.\n\u2022 Prepare a Data Quality and Schema Structure Report, which document findings regarding the database analysis work performed.\n\u2022 Prepare technique documentation on data migration process.\n\u2022 Create mappings between the fields in the source and target systems as well as identifying relationships within a source (implicit foreign keys, lookup tables, etc.).\n\u2022 Leverage mappings to create an ETL process to move the data from the source system to the target system. This includes data cleaning and value mapping.\n\u2022 Performance tuning of queries and tables.\n\u2022 Optimizing / Creating Stored Procedures.\n\u2022 Documentation of source schemas to be used by other Data engineer.\n\u2022 Utilize Jira and SourceTree for bug tracking and coding tracking.', u'Data Analyst\nTri City Health Center - Fremont, CA\nDecember 2015 to Present\n\u2022 Perform quality data measurement in data warehouse and ensure meaningful use apply for all clinical data.\n\u2022 Automate/configure daily/monthly report for stockholder.\n\u2022 Lead efforts to ensure data integrity and validity and perform ongoing quality assurance of all data and reports.\n\u2022 Gather and identify business need for ad-hoc reporting as needed.\n\u2022 Suggest new business work flow and implementation for quality measure.\n\u2022 Ensure data mapping in I2I system is correct and generate UDS report for federal government.\n\u2022 Develop incident system to keep track the occurrences and analyze data.\n\u2022 Develop/implement web base portal for clinical user to submit patient complain.\n\u2022 Develop dashboard tough Tableau, SSRS, and excel.\n\u2022 Develop SSRS and pivot table for end user to perform simply data task.\n\u2022 Serve as project lead to ensure integrity and validity of annual UDS data submission.\n\u2022 Improve the quality of data and information by working with users to identify reporting needs, define report specifications, and develop reports to meet business requirements, turning the reports/processes over to end users\u2019 control whenever possible.', u'Senior Data Analyst\nHumana INC - Phoenix, AZ\nMay 2015 to September 2015\nPerform data extraction, aggregation, analysis, and quality checking from multiple data sources to create and monitor tests and programs using tried and true platforms like SQL and SAS in a manufacturing and/or warehousing environments.\n\u2022 Use the newest and most relevant technologies, like Hadoop, R, and Python, to uncover new insights that enable new opportunities\n\u2022 Partner with management to drive new reporting or solutions that improve our ability to operate efficiently and improve continuously\n\u2022 Initiate data mining and analysis activities to explore big data resulting in suggestions that lower costs and drive efficiencies\n\u2022 Spearhead proactive investigations into data issues that impact reporting, analysis, or execution\n\u2022 Perform ad hoc analytics on large/diverse datasets\n\u2022 Help build business cases and lead definition of data strategies that support business strategy', u'Data Analyst\nHealthways INC. - Chandler, AZ\nMay 2013 to May 2015\nIntake of membership files and analyze data using T-SQL.\n\u2022 Responded to business requests, including research and analysis of member data.\n\u2022 Communicated the status of file loads, issues and resolutions to stakeholders.\n\u2022 Analyzed Health plan files to confirm that the format conforms to intake requirements.\n\u2022 Documented changes in health plan files and formats, and logic to process.\n\u2022 Performed maintenance and Operational Support of automation solutions around file intake and processing.\n\u2022 Follow change control process to maintain intake configurations and effect data fixes.\n\u2022 Create EDI translation companion guide. Ad-hoc report creation and analyze.', u""Intranet Web Development\nON Semiconductor - Phoenix, AZ\nJune 2012 to August 2012\nRedevelopment of HR intranet website via Share Point (over 3 month's period).\n\u2022 Re-designed intranet interface by introducing visualization content in website and revising navigation bar to be more efficient.\n\u2022 Updated employee information regarding benefits, new hires, online management training.\n\u2022 Improved Share Point efficiency by removing old files, website and data."", u""Data Analyst\nGreater Phoenix Real Estate Alliance - Phoenix, AZ\nMarch 2012 to June 2012\nResearched and analyzed housing market and located houses on MLS for foreign investors and answered questions on transactions.\n\u2022 Prepared contracts, submitted offers, and communicated with seller's most up to date information after CEO approval.\n\u2022 Responsible to pull data from MLS and do analysis in excel.\n\u2022 Updated web design and social media networking, incorporated the BBB award into company marketing."", u'Office Assistant/Data Analyst\nGreat Wall Chinese Medicine LLC - Scottsdale, AZ\nNovember 2010 to March 2012\nProvided IT support, software update, software installation, software troubleshoot.\n\u2022 Maintained the network system by upgrade the firewall.\n\u2022 Re designed the web site to more professional, send newsletter to subscriber for monthly health update. Medical billing and coding.']",[u'Bachelor of Science in Computer Information Systems'],"[u'Business Arizona State University Tempe, AZ']"
0,https://resumes.indeed.com/resume/0e5f52c65b392df2,"[u'Data Engineer Intern\nMJ Intelligent System Ltd - Shanghai\nJune 2016 to August 2016\n\u2022 Extracted, cleaned and restored operational data using SQL queries.\n\u2022 Conducted exploratory data analysis using Python modules.\n\u2022 Provided plots for senior data analysts using \u2018matplotlib\u2019 module in Python.', u'Process Engineer\nMichelin Group - Shanghai, CN\nJuly 2013 to June 2015\n\u2022 Monitored daily processes and long period production performance trends.\n\u2022 Focused on process problems, analyzed production data and carried out diagnosis.\n\u2022 Communicated multiple departments and took actions to solve process problems.\n\u2022 Conducted A/B test and analyzed the results to optimize the production process.\n\u2022 Participated in the Pallet Truck Project which reduced the green tire distortion by 0.07%.\n\u2022 Used Excel to make post capacity reports through production data analyzing and visualization.\n\u2022 Used Access to store and manage production data as much as 20K records per day.']","[u""Master's in Business Intelligence & Analytics"", u'Bachelor of Engineering in Industry Engineering']","[u'Stevens Institute of Technology Hoboken, NJ\nJanuary 2016 to May 2018', u'Tongji University Shanghai, CN\nSeptember 2009 to June 2013']"
0,https://resumes.indeed.com/resume/2c4f5d55e9692720,"[u'RF Data Collector Engineer\nAirtel tower LLC (Quad Gen Wireless Solutions)\nMarch 2017 to Present\n\u2022 Train new employee on drive testing procedures and methods\n\u2022 Troubleshoot equipment issues and site issues with the Rf engineer team.\n\u2022 Responsible for creating voice and data setup for short call, long call, and packet call and making workspaces', u'RF Data Collector Engineer\nMobilcomm (Quad Gen Wireless Solutions)\nAugust 2016 to Present\n\u2022 Responsible for creating voice and data setup for short call, long call, and packet call and making workspaces.\n\u2022 Developed drive routes using Microsoft Streets Atlas.\n\u2022 Support team member troubleshoot kit issues with TeamViewer', u'RF Data Collector Engineer\nCell packet technology LLC (Quad Gen Wireless Solutions)\nJuly 2014 to August 2016\n\u2022 Setup and secure RF equipment to multiple company vehicles\n\u2022 Attending weekly meetings, advanced training workshops and conferences on personal techniques, time management, and effective communication\n\u2022 Acquired knowledge of NEMO Outdoors and JDSU software and trainned new coworkers on the equipment\u2019s.']",[u'Batchlaur Degree in Computer Science/cyber security.'],"[u'ITT Tech Technical Institute Atlanta, GA\nJune 2009 to June 2013']"
0,https://resumes.indeed.com/resume/bdcd51f849e51ccb,"[u'Researcher\nPenn State University - State College, PA\nAugust 2017 to Present\no Working in HealthCare to develop Deep Learning solution for the premium problem of ankle fracture detection from X-RAY images that will reduce misclassification problem incurred due to high false positive rate from diagnosis by radiologists.\no Developed Data pipeline from PACS systems for image access and set up crucial hardware and software for the research.\no Demonstrating a positive increase in classification by developing Vanilla Convolutional Neural Networks(CNN) and implementing transfer learning approach along with the region of interest detection in Tensorflow in Python.', u'Data Engineer\nHewlett-Packard Enterprise - Chennai, Tamil Nadu\nSeptember 2014 to June 2016\no Worked on a data warehousing project called as \u2018Nucleus\u2019 which provides business intelligence services to a Global Telecommunication industry.\no Worked as a member of PayGo Run & Operate team which ensures proper and timely delivery of notifications and reports to the business users.\no Understood the requirements of the clients and implemented work-around graphs that fixed issues by 30% in production phase by automating manual tasks, fixed daily issues in the system and reduced number of job failures by 40%.']","[u""Master's in Electrical""]","[u'Penn State State College, PA\nAugust 2016 to August 2017']"
0,https://resumes.indeed.com/resume/6734fceeeab868cc,"[u'Data Engineer\nThird Eye Data - Santa Clara, CA\nJune 2017 to February 2018\nResponsibilities:\n\u2022 Joined on the team of an ongoing project. Discussed with my mentor for better optimal implementation of Spark, AWS and Cassandra.\n\u2022 Implemented the idea where spark ML libraries were used to run various models on the No-SQL data and do that using AWS and Cassandra to see if there is any change in performance.\n\u2022 Assisted in some problem solving for the ongoing project.\n\u2022 Built relational databases in SQL server of several flat files of partner information from several large (5-10 GB) flat files in Python. Used logistics regression and random forests models in R/Python to predict the likelihood of partner participation in various marketing programs. Designed and developed visualizations and dashboards in R /Tableau that surfaced the primary factors that drove program participation and identified the best targets for future targeted marketing efforts.\n\u2022 Worked on a cloud-based application that will be using Amazon Web Services(AWS) EC2 as the service and will execute SQL commands on the SQL database installed on the EC2 instance. All of the code was written in Java. And perform the time performance taken to execute the commands 1000, 5000 and 10,000 times.\n\u2022 Used Python and SOAP protocol that would get the data from ndfd XML servers in a XML format and print that data in a understandable format.\n\u2022 Involved in software Unit, functional, system, User Acceptance, UI testing.', u'Data Scientist\nGroupon - Chicago, IL\nJanuary 2016 to December 2016\n\u2022 Responsibilities:\n\u2022 Collaborated with business to understand company needs and devise possible solutions\n\u2022 Analyzed and solved business problems, and found patterns and insights within structured and unstructured data\n\u2022 Cleaned, analyzed and selected data to gauge customer experience\n\u2022 Implemented new statistical and mathematical methodologies as needed for specific models or analysis\n\u2022 Used algorithms and programming to efficiently go through large datasets and apply treatments, filters, and conditions as needed\n\u2022 Created meaningful data visualizations to communicate findings and relate them back to how they create business impact\n\u2022 Implemented various Machine Learning modeling techniques with Java, Python and Matlab as the programming language:', u'Data Engineer\nDesign Innova, New Delhi - New Delhi, Delhi\nAugust 2013 to August 2014\n\u2022 Developed a queue management system that would fetch queue numbers and display them on the screen.\n\u2022 Worked on developing a front-end that would get that data in JSON format using Java and display that data on the screens using HTML, Javascript and Jquery.\n\u2022 Worked on various Data modeling techniques.', u'Software Engineer Intern\nAT&T - Noida, Uttar Pradesh\nMay 2013 to August 2013\n\u2022 Project based on technologies such as Raspberry Pi, Java, Pi4J api and PHP/Javascript.\n\u2022 Creating a LAMP server in order to have the server record values from the raspberry pi and upload them on a website using PHP/Javascript and those values can be accessed from any device having connection to internet.']","[u'Master of Computer Science and Engineering in Computer Science and Engineering', u'Bachelor of Computer Science and Engineering in Computer Science and Engineering']","[u'University of Tex as at Arlington Arlington, TX', u'Amity University Noida, Uttar Pradesh']"
0,https://resumes.indeed.com/resume/108b2bdb09dd8341,"[u'Big Data Engineer\nCredit Fraud Prediction Model (Freelance) - New York, NY\nJuly 2017 to February 2018\nTrained a Fraud Prediction model and created a customizable dashboard for credit monitoring agencies.\n\u25cf Collated Real-time streaming data from credit agencies such as Transunion & Experian, performed data cleaning and fed the data into Kafka.\n\u25cf Simulated Credit risk scenarios and used logistic regression along with decision tree based ML algorithms to predict aforesaid output.\n\u25cf Transformed Kafka loaded data using Spark-streaming with Scala and Python.\n\u25cf Used Sci-kit learn, Pandas, Numpy and Tensor flow to determine insights from data and created a trained credit fraud detection model for batch data.\n\u25cf Created the framework for the dashboard using Tableau and optimized the same using open source Google optimization tools.\n\u25cf Deployed model using RESTful APIs and used Dockers to facilitate multi-environment transition.\n\u25cf Databases used in the project were Hive and Cassandra for batch data.\n\u25cf Streaming data was stored using Amazon S3 deployed over EC2 and EMR cluster framework apart from in-house tools.', u""Data Engineer\nCredit Fraud Prediction Model (Freelance) - New York, NY\nJuly 2017 to February 2018\n\u2022 Designed a Coupon Purchase Prediction system and an automated model based on a recommendation engine intended for the same purpose.\n\u2022 Real-time streaming data was analyzed from E-bay, Macy's, Walmart, Nike and ETL guidelines were applied to it for cleaning and pre-processing data using SQL.\n\u2022 Cleaned and congruous data was then streamed using Kafka into Spark and manipulations were performed on real time data with Python and Scala.\n\u2022 Built the Machine learning based coupon purchase recommendation engine by training the model on historical purchase data of customers across the retail hemisphere.\n\u2022 Simulated real-time scenarios using the Sci-kit learn and Tensor flow libraries on Batch data for training model and model was later used in real-time models.\n\u2022 Stored the time-series transformed data from the Spark engine built on top of a Hive platform to Amazon S3 and Redshift.\n\u2022 Facilitated deployment of multi-clustered environment using AWS EC2 and EMR apart from deploying Dockers for cross-functional deployment.\n\u2022 Visualized the results using Tableau dashboards and the Python Seaborn libraries were used for Data interpretation in deployment."", u'Data Scientist\nProduct Innovation Program - New York, NY\nOctober 2016 to June 2017\nDeveloped the next generation actuation system accentuating a high fidelity agile prototype for Magna International based out of Graz, Austria in collaboration with a multicultural and multidisciplinary team of engineers from all across Europe and presented the same at the International Design factory Gala at Graz, Austria by May 2017.\n\u25cf Assimilated all use cases related to agnate historical models (based on previous mechanical models and model data using python & R) and identified a singular strategy to build a revolutionary working prototype.\n\u25cf Process oriented data analyst experienced in interpreting and analyzing data. Reduced operating costs by 15%. Furnish insights, analytics, and business intelligence needed to guide decisions\n\u25cf Design was patented and is now an ambitious project being developed by Magna International TM.', u'Data Engineer\nWebsite Refactoring and Redesign of Pace University Website - New York, NY\nJanuary 2016 to December 2016\n\u2022 Redesigned the Architecture of the Pace website and submitted a proposal for an architecture overhaul of the Pace University Website.\n\u2022 Interviewed potential users and stakeholders, analyzed multiple use cases using python & isolated the vulnerabilities.\n\u2022 Developed active social media presence on Facebook and Twitter, accomplishing an average monthly increase in active users up to 46% and overall page likes up to 74%.\n\u2022 Supervised testing of multiple users with an optimized model thereby increasing site views of the pace website from a mere 52% to 78% and research time on page was optimized to 45%.\n\nLEADERSHIP\nLed the Product support team at Dell for over a year and handled a team of SME\'s at Symantec Chennai, India\n\nOrganized the annually held cultural event ""Diwali 2015"" & ""Diwali 2016"". New York, USA', u'Global Student Ambassador\nPace University International Relations - New York, NY\nJanuary 2015 to January 2016\nManaged & marketed the study abroad programs during the New York, USA\nPace international week 2015 & 2016.\n\nGlobal Student Ambassador at Pace University International Relations New York, USA\n\nStudent Ambassador at Uncubed New York, USA', u'Product Support Associate\nSYMANTEC - Chennai, Tamil Nadu\nOctober 2014 to August 2015\nManaged a team of 16 as Product Support Associate for more than 2500 global clients in the United States, United Kingdom, and Australia for a period of 6 months\n\u25cf Responsible for penetration testing of corporate networks and simulated virus infections on computers to assess network security & Presented a report to the Development Team to assess the intrusions.\n\u25cf Coordinated with product leads to identify problems with Norton products and acted as a liaison between the Development Team and Quality Team to ascertain the efficiency of the product', u""Network Consultant\nSUTHERLAND - Chennai, Tamil Nadu\nFebruary 2013 to October 2014\nWorked as network consultant with Dell Technical Support for over 1000 US customers\n\u25cf Reported common issues faced with Dell products and fostered feasible solutions making least changes in physical design of the product and rendering maximum throughput by alleviating the defects\n\u25cf Managed a team of 15 and monitored overall team progress, achieved preset objectives, generated consolidated reports reviewing the team's performance and articulated ways to improve both company and team operations.\nPROJECTS""]","[u'Master of Science in Information Systems', u'Bachelor of Engineering in Computer Science']","[u'Pace University, Seidenberg School of CSIS New York, NY\nJune 2017', u'Anna University, BSA Crescent Engineering College Chennai, Tamil Nadu\nJanuary 2010 to October 2012']"
0,https://resumes.indeed.com/resume/15fe42e1b670f692,"[u'Hadoop Architect & Big Data Engineer\nHorizon Blue Cross Blue Shield of New Jersey - Iselin, NJ\nOctober 2016 to Present\nArchitected the Big Data Architecture to create the foundation of this Enterprise Analytics initiative in a Hadoop-based Data Lake.\nCreated a POC involved in loading data from LINUX on premises ecosystem to Amazon S3 using Redshift, DynamoDB, and HDFS using MongoDB and CassandraDB (Hortonworks Hadoop).\nEnsured HIPPA compliance and security of sensitive data using hashing, MD5 SQL encryption, Kerberos.\nImplemented Spark using Scala, and utilized DataFrames and Spark SQL API for faster processing of data.\nWorked on AWS to create, manage EC2 instances, and Hadoop Clusters.\nUsed secure VLANS for data transfer security to secure VPC on AWS.\nFollowed ITIL best practices for ensure data and infrastructure integrity.\nFollowed Six Sigma for process efficiency and quality performance.\nCreated Data Modeling and implemented Redshift instance on Amazon.\nDeveloped shell scripting to automate the data flow of daily tasks.\nCreated both internal and external tables in Hive, and developed Pig scripts to preprocess the data for analysis.\nInvolved in converting HiveQL/SQL queries into Spark transformations using Spark RDDs, Python and Scala.\nUse of spark, python, hive, pig in constructing pipelines and queries.\nUsed Cassandra and MongoDB to work on JSON files.\nWorked on Cassandra query language to load the bulk of data and execute queries.\nDesigned appropriate partitioning/bucketing schema to allow faster data retrieval during analysis using Hive.\nPerformance tuned Spark jobs for setting batch interval time, level of parallelism, and memory tuning.\nUsed Spark-Streaming APIs to perform necessary transformations and actions on the real-time data using Bedrock data management tool.\nInvolved in migrating jobs to Spark, using Spark SQL and DataFrames API to load structured data into Spark clusters ENVIRONMENT\nAWS\nEC2\nRedshift\nCassandra\nRDBMS\nSQL\nJSON\nScala\nPython\nHIVE\nHIVE QL\nPOC\nHDFS\nSpark\nPerformance Tuning\nOptimization\nMemory Tuning\nSpark API\nSpark SQL\nSpark Transformations\nSpark Data Frames\nBedrock\nProcess Automation', u'Bid Data Architect & Engineer\nAC Nielsen - Oldsmar, FL\nAugust 2015 to October 2016\nArchitected the Big Data Architecture to create the foundation of this Enterprise Analytics initiative in a Hadoop-based Data Lake.\nCreated a POC involved in loading data from LINUX file system to AWS S3 and HDFS.\nDeveloped shell scripting to automate the data flow of daily tasks.\nCreated both internal and external tables in Hive, and developed Pig scripts to preprocess the data for analysis.\nUsed Cassandra to work on JSON documented data.\nWorked on Cassandra query language to load the bulk of data and execute queries.\nInvolved in converting HiveQL/SQL queries into Spark transformations using Spark RDDs, Python and Scala.\nDesigned appropriate partitioning/bucketing schema to allow faster data retrieval during analysis using Hive.\nPerformance tuned Spark jobs for setting batch interval time, level of parallelism, and memory tuning.\nUsed Spark-Streaming APIs to perform necessary transformations and actions on the real-time data using Bedrock data management tool.\nInvolved in migrating jobs to Spark, using Spark SQL and DataFrames API to load structured data into Spark clusters\nInvolved in converting HiveQL/SQL queries into Spark transformations.\nInvolved in transforming the relational database to legacy labels to HDFS, and HBASE tables using Sqoop and vice versa.\nCreated Hive tables and dynamic partitions, with buckets for sampling and working on them using Hive QL.\nCreated HBase tables to store variable data formats of data coming from different portfolios.\nImplemented Spark using Scala, and utilized DataFrames and Spark SQL API for faster processing of data.\nUsed Sqoop job to import the data from RDBMS using Incremental Import.\nExported analyzed data to relational databases using Sqoop for visualization, and to generate reports for the BI team.\nWrote shell scripts for exporting log files to Hadoop cluster through automated processes.\nDeveloped Scala scripts and UDFs using DataFrames and RDD in Spark for data aggregation, queries and writing data back into OLTP system through Sqoop.\nWorked with various compression techniques to save data and optimize data transfer over network using Lzo, Snappy, etc. ENVIRONMENT\nAWS\nEC2\nRedshift\nCassandra\nRDBMS\nSQL\nJSON\nHIVE\nHIVE QL\nPOC\nHDFS\nSpark\nScala\nPython\nPerformance Tuning\nOptimization\nMemory Tuning\nPartitioning\nBucketing\nSchema\nSpark API\nSpark SQL\nSpark Transformations\nSpark Data Frames\nBedrock\nBatch Processing\nProcess Automation', u'AWS Hadoop Cloud Data Architect\nCitizens Insurance - Tallahassee, FL\nJune 2014 to August 2015\nInvolved in writing incremental imports into Hive tables.\nWorked on importing and exporting tera bytes of data using Sqoop from HDFS to Relational Database Systems and vice-versa.\nImporting and Exporting data into HDFS using Sqoop.\nWorked on AWS to create, manage EC2 instances, and Hadoop Clusters.\nCreated Data Modeling and implemented Redshift instance on Amazon.\nDeveloped shell scripting to automate the data flow of daily tasks.\nCreated both internal and external tables in Hive, and developed Pig scripts to preprocess the data for analysis.\nTransformed the logs data into data model using Pig and written UDF functions to format the logs data.\nExperienced on loading and transforming of large sets of structured and semi structured data from HDFS\nthrough Sqoop and placed in HDFS for further processing.\nInvolved in transforming data from legacy tables to HDFS, and HBase tables using Sqoop.\nExtensively used transformations like Router, Aggregator, Normalizer, Filter, Joiner, Expression, Source Qualifier, Unconnected and connected lookup, Update strategy and store procedure, XML transformations along with error handling and performance tuning.\nDesigned appropriate partitioning/bucketing schema to allow faster data retrieval during analysis using Hive.\nDesigned jobs using DB2 UDB, ODBC, .Net, Join, Merge, Lookup, Remove duplicate, Copy, Filter, Funnel, Dataset, Lookup file set, Change data capture, Modify, Row merger, Aggregator and Peek, Row generator stages.\nPerformance tuned Spark jobs for setting batch interval time, level of parallelism, and memory tuning.\nInvolved in converting HiveQL/SQL queries into Spark transformations. ENVIRONMENT\nAWS\nEC2\nRedshift\nCassandra\nRDBMS\nSQL\nJSON\nHIVE\nHIVE QL\nPOC\nHDFS\nSpark\nScala\nPython\nPerformance Tuning\nOptimization\nMemory Tuning\nSpark API\nSpark SQL\nSpark Transformations\nSpark Data Frames\nPartitioning\nBucketing\nSchema\nPig', u'Big Data Cloud Engineer / Architect\nOmnium Financial Services - Chicago, IL\nMay 2013 to June 2014\nMigrated Big Data Architecture to cloud created on AWS using AWS tools and database instances with Hadoop HDFS to create a data lake in cloud.\nUsed HBase to store majority of data which needed to be divided based on region.\nInvolved in benchmarking Hadoop and Spark cluster on a TeraSort application in AWS.\nCreated multi-node Hadoop and Spark clusters in AWS instances to generate terabytes of data and stored it in AWS HDFS.\nWrote Spark codes to run a sorting application on the data stored on AWS.\nDeployed the application jar files into AWS instances.\nUsed the image files of an instance to create instances containing Hadoop installed and running.\nDeveloped a task execution framework on EC2 instances using SQS and DynamoDB.\nDeveloped Scala scripts and UDFs using DataFrames and RDD in Spark for data aggregation, queries and writing data back into OLTP system through Sqoop.\nDesigned a cost-effective archival platform for storing big data using Hadoop and its related technologies. Created Data Modeling and implemented Redshift instance on Amazon.\nDesigned appropriate partitioning/bucketing schema to allow faster data retrieval during analysis using Hive.\nInvolved in developing Pig Scripts for change data capture and delta record processing between newly arrived data and already existing data in HDFS.\nImplemented data ingestion and cluster handling in real time processing using Kafka.\nImplemented workflows using Apache Oozie framework to automate tasks. Used Spark-Streaming APIs to perform necessary transformations and actions on the real-time data using Bedrock data management tool.\nUsed Spark SQL and DataFrames API to load structured data into Spark clusters\nENVIRONMENT\nAWS\nEC2\nRedshift\nCassandra\nRDBMS\nSQL\nScala\nPython\nHIVE\nHIVE QL\nPOC\nHDFS\nSpark\nPerformance Tuning\nOptimization\nMemory Tuning\nSpark API\nSpark SQL\nSpark Transformations\nSpark Data Frames\nTeraSort\nPartitioning\nBucketing\nSchema\nData Lake', u'Big Data Engineer\nReal Page, Inc - Carrollton, TX\nOctober 2011 to May 2013\nExtensively worked on performance optimization of hive queries by using map-side join, parallel execution and cost based optimization.\nDeveloped shell scripting to automate the data flow of daily tasks.\nCreated both internal and external tables in Hive, and developed Pig scripts to preprocess the data for analysis.\nDesigned appropriate partitioning/bucketing schema to allow faster data retrieval during analysis using Hive.\nPerformance tuned Spark jobs for setting batch interval time, level of parallelism, and memory tuning.\nDeveloped Spark scripts by using Scala shell commands as per the requirement.\nUsed SCALA to store streaming data to HDFS and to implement Spark for faster processing of data.\nUsed Spark-Streaming APIs to perform necessary transformations and actions on the real-time data using Bedrock data management tool.\nInvolved in migrating MapReduce jobs to Spark, using Spark SQL and DataFrames API to load structured data into Spark clusters\nInvolved in converting HiveQL/SQL queries into Spark transformations.\nExecuted tasks for upgrading clusters on the staging platform before doing it on production cluster.\nPerformed maintenance, monitoring, deployments, and upgrades across infrastructure that supports all Hadoop clusters.\nInstalled and configured various components of the Hadoop ecosystem.\nOptimized HIVE analytics, SQL queries, created tables, views, wrote custom UDFs, and Hive-based exception processing.\nInvolved in transforming the relational database to legacy labels to HDFS, and HBASE tables using Sqoop and vice versa.\nReplaced default Derby metadata storage system for Hive with MySQL system.\nSet-up QA environment and updated configurations for implementing scripts with Pig.\nConfigured Fair Scheduler to allocate resources to all the applications across the cluster.\nDeveloped custom FTP adaptors to pull the clickstream data from FTP servers to HDFS directly using HDFS File System API.\nENVIRONMENT\nHadoop\nSpark\nSpark API\nSpark Data Frames\nSpark Streaming\nScala\nPython\nMapReduce\nRDBMS\nSQL Queries\nHIVE\nHIVE QL\nHDFS\nPerformance Tuning\nOptimization\nMemory Tuning\nTransformations\nPartitioning/bucketing\nSchema\nClusters\nSqoop\nFTP', u'WINDOWS AND VMWARE ADMINISTRATOR\nT-Systems M\xe9xico - Puebla, MX\nSeptember 2007 to September 2011\nWindows management for virtual environments VMware and Hyper-v and Physical servers. Where I managed the whole Windows and VMware infrastructure automating and reducing time, human errors and costs, I was able to automate processes and repetitive work, also to keep the windows infrastructure up and running reducing down times from patching servers each month to keep them update and manage security controls for audit propose all following ITIL processes.\n\nTransitioned Windows server from USA to M\xe9xico taking operations from customer infrastructure to the current environment.\nAutomated the inventory for more than one thousand Windows servers\nTransitioned and transformed projects from the customer to the T-systems standards\n\n\n\nENVIRONMENT\nWindows Server 2000, 2003, 2008, 2008 R2, 2012 and 2012 R2\nMicrosoft Hyper-V\nVMWare']","[u""Bachelor's Degree in Information Technology in Information Technology""]",[u'Universidad Popular Aut\xf3noma del Estado de Puebla Puebla\nAugust 2007 to August 2013']
0,https://resumes.indeed.com/resume/14c12befa3e4c1d8,"[u'Data Analyst\nClassBoat - Pune, Maharashtra\nJune 2015 to June 2016\nAnalyzed data for inconsistencies and patterns for attributes using advanced MS Excel functions\n\u25cf Fine-tuned the SQL queries for optimal performance and implemented changes to the data model, reducing data quality issues from 6% to 2%\n\u25cf Effectively used data blending feature in Tableau and built customized interactive Dashboards in Tableau using marks, action, filters, parameter, and calculations\n\u25cf Improved content curation by incorporating Google Analytics that led to increase in the web traffic and boost in user sign-ups by 27%\n\u25cf Presented biweekly business strategies and SWOT analysis resulting in 20% improved schedules and behavior', u'Associate Software Engineer\nAccenture - Pune, Maharashtra\nJuly 2014 to May 2015\n\u25cf Developed a module for a data migration application to accelerate the data transfer process\n\u25cf Implemented UI automated testing which improved the process speed by 40%\n\u25cf Worked directly with the Manager to recommend new technologies and process improvements\n\u25cf Implemented Agile SCRUM through incremental and iterative development of modules']","[u'M.S. in Information Technology and Management', u'B.E. in Computer Engineering']","[u'The University of Texas at Dallas Dallas, TX\nMay 2018', u'The University of Pune\nMay 2014']"
0,https://resumes.indeed.com/resume/3afc04b6c104d98b,"[u""Data Analyst\nInfosys Ltd - Chennai, Tamil Nadu\nApril 2015 to July 2016\nBANKLINE - ORDERING OF SMARTCARDS & CAP CARDS\n\u2022 Performed descriptive analysis on smart card manufacturer data (size of 1GB) and identified the KPI's for ordering the smart card.\n\u2022 Created jobs and T-SQL to order the smartcard for divested customers automatically and deployed successfully in production environment.\n\u2022 Supervised a team of five members and involved in training, mentoring, and allocating task"", u'System Engineer\nInfosys Ltd - Chennai, Tamil Nadu\nApril 2013 to April 2015\nBANKLINE - EXTRACT TRANSFORM LOAD (ETL) & NON ETL\n\u2022 Worked with business users and business analyst for requirements gathering and analyzing\n\u2022 Converted business requirement into high-level and low-level design documents.\n\u2022 Created common reusable ETL Jobs, stabilized the application and reduced the time consumption by 10%\n\u2022 Tuned SQL queries and stored proc to optimized performance and increased the efficiency of the application by 20%']","[u'M.S. in Business Analytics', u'B.E. in Mechanical Engineering']","[u'The University of Texas at Dallas Dallas, TX\nMay 2018', u'Anna University\nJuly 2012']"
0,https://resumes.indeed.com/resume/1ee7e7d536766116,"[u'Data Engineer/Data Analyst\nBEIJING SHENJIZHINENG Co., LTD\nMay 2017 to August 2017\nDesigned key index and hypothesis to find shortcomings in games. Validated hypothesis through experiments and modeling. Improved user retention rate.\nBuilt a data pipeline that automates the process of data cleaning, key index calculation, and prediction.\nWrote analysis reports, explaining the approaches used to find problems in games through data visualization. Communicated repeatedly with clients to revise methods.', u'Machine Learning Researcher\nEnterprise Informatization Lab,Beijing University of Posts and Telecommunications - Beijing\nMay 2015 to May 2016\nHadoop cluster (including HBase) setup and maintenance with three slave nodes and one master node.\nDesigned and implemented models through Hadoop and HBase to detect online restaurants spam reviews.\nResearched into stream classification algorithms, improving the speed as well as the accuracy of the spam reviews detection model.\nWrote and published a paper in the International Journal of Modeling and Optimization Vol 6. (http://www.ijmo.org/vol6/501-MS16019.pdf)\n\nProject']","[u'Master in Information Systems Management', u'BACHELOR OF SCIENCE in ENGINEERING']","[u'Carnegie Mellon University Pittsburgh, PA\nSeptember 2016 to May 2018', u'Beijing University of Posts and Telecommunications\nJanuary 2011 to January 2015']"
0,https://resumes.indeed.com/resume/ef23dcb14a0079b9,"[u'lead, data analytic/engineer\nIdentified actionable insights and recommendations to support marketing operation - making to continually improve efficiency']","[u""Bachelor's in Computer Engineering""]","[u'Cal Poly San Luis Obispo, CA\nJanuary 1999 to January 2003']"
0,https://resumes.indeed.com/resume/7cc5128af01bb6bd,"[u'Data Engineer\nSujitech LLC\nMay 2017 to Present\nhuang654@purdue.edu\no Set-up recommendation system by collaborative filtering\n765-479-5059\no Gather user information from social networks to build user profiles linkedin.com/in/locoda\no Visualize user relationships to demonstrate\nlocoda.github.io\n\nRESEARCH EXPERIENCE SKILLS', u'Research Assistant\nCLAN Labs\nJune 2016 to May 2017\nData crawling\no Project ""On Tensor Train Subspace Anomaly Detection via Oversampling""\nStatistical modeling\no Focus on inter-dimension relationships (or inter-feature relationships) on high-\nData visualization\ndimensional datasets, mostly images and videos']","[u'Master of Science in Computer Science', u'Bachelor of Science in Computer Science', u'Bachelor of Science in Mathematical Statistics']","[u'Purdue University West Lafayette, IN\nAugust 2017 to May 2019', u'Purdue University West Lafayette, IN\nAugust 2014 to May 2018', u'Purdue University West Lafayette, IN\nAugust 2014 to May 2018']"
0,https://resumes.indeed.com/resume/526e3bfd3ba725bc,"[u""Data Scientist\nKotak Mahindra Bank\nSeptember 2016 to July 2017\nTechnologies: Python, SAS, MySQL, MATLAB, Microsoft Revolution R\n\u2022 Helped build a 'Customer Persona' based approach for digital marketing of the bank's products. It was accepted as the bank's analytics motto for the Financial Year 2017-18.\n\u2022 Developed a Logistic Regression model to predict whether a customer will pay income tax through the bank's facilities\n\u2022 Implemented an unsupervised learning model to cluster the text of customer complaints to improve customer\nexperience.\n\u2022 Analyzed cash withdrawal transactions at an ATM level to find non performing machines and locations.\n\u2022 Developed a strategy to implement a customer feedback system based on the demography of each and every branch\n\u2022 Analysis of demonetization of 500 and 1000 Rupee Notes on the bank's Digital Channels and Call Centers"", u""D.C.S. Engineer\nEmerson Process Management\nJune 2015 to March 2016\nInterfaced and tested third party PLC with Distributed Control System(DCS) using Modbus Protocol\n\nPROJECTS\n\u2022 Artistic Neural Style Transfer Using Convolutional Neural Networks Jan '18 - Present\n- Currently working on transferring the content information and the texture information from two separate\nimages and morph them into one image by minimizing the style and content loss simultaneously using VGG19\narchitecture.\n\u2022 Collaborative Filter based Recommender System Sep '17 - Dec '17\n- Developed User based and Item based filters to build a Movie recommender on the MovieLens dataset. Different\nmodels were built by tweaking the similarity metric(Pearson's correlation and Cosine similarity) and the number of nearest neighbors that go into the predicted rating of a movie by a user. Achieved a RMSE of 0.86.\n\u2022 Email Fraud Analysis May '16 - Aug '16\n- Developed a Naive Bayes bag of words model to predict whether an incoming email is a fraud and spam email or not. Compared this algorithm with other classification algorithms like logistic regression, KNN classification and\ndecision trees.""]","[u""Master's of Science in Computer Engineering"", u'Certificate', u'Bachelor of Technology in Instrumentation & Control Engineering']","[u'Rochester Institute of Technology Rochester, NY\nAugust 2017', u'S.P. Jain School of Global Management Mumbai, Maharashtra\nFebruary 2016 to August 2016', u'Vishwakarma Institute of Technology Pune, Maharashtra\nAugust 2011 to May 2015']"
0,https://resumes.indeed.com/resume/dd59b11e3854ec57,"[u'Operations Research Analyst\nCSX Transportation Inc - Florida\nJanuary 2017 to Present\nWorking on railway track quality Index and combining various geometric channels into one single Track Quality Index.\nUsing SQL to pull out the data from Oracle(Toad) database and building data mining models.', u'Data Science Researcher\nAmazon - Buffalo, NY\nJanuary 2016 to December 2016\nPulled out the data using SQL from the data warehouse. Checked the relationships if there is anything between 2 categories. Procedure includes data cleaning, defining structures of samples, determining quality verification measures, computing statistics and making report on the analysis.\n\uf0b7 Data cleaning involved visualization methods to find the outliers using Box Plot. Also by differencing the data to find the trends and seasonality. Daily data was aggregated into biweekly format.\n\uf0b7 Used different forecasting models like ARIMA, ARMA, Single and Double Exponential Smoothening, Moving Average and ARIMA analysed the best method on the basis of the MAPE value.\n\uf0b7 Setting up an algorithm to recover sales from market share for whole Amazon and portion of the sales for that seller using Estimation Maximization Algorithm and Maximum Likelihood Estimator.', u'Data Analyst\nAngelitino Kasco Industries, India - India\nApril 2015 to July 2015\n-Identify & solve business challenges utilizing large structured, semi-structured, and unstructured data in a distributed processing environment and provide ongoing reports for continuous business enhancements.\n-Compile and validate data; reinforce and maintain compliance with Tableau.\n-Develop and initiate more efficient data collection procedures using SQL.\n-Work with management & Senior leadership to prioritize business and information requirements.', u'Data Engineer\nGodrej - Pune, Maharashtra\nJune 2014 to March 2015\n-Worked with other team members to complete special projects and achieve project deadlines like the Vehicle Routing Problems, Earliest Due Date, Silver Meal Algorithm, Branch and Bound and Knapsack Problems using Python and R.\n\uf0b7 Developed statistical models to forecast inventory and procurement cycles using the Stochastic Inventory Procedure using Python.\n\uf0b7 Conducted cost and benefit analysis on new ideas. Carried the cost v/s Quality Analysis. Scrutinized and tracked customer behaviour to identify trends and unmet needs.\nExtracted, compiled and tracked data, and analyzed data to generate reports.\n-Worked with other team members to complete special projects and achieve project deadlines.\n-Developed optimized data collection and qualifying procedures.\n-Develop statistical models to forecast inventory and procurement cycles\n-Conduct cost and benefit analysis on new ideas. Scrutinize and track customer behavior to identify trends and unmet needs.\n-Leveraged analytical tools to develop efficient system operations', u'Data Analyst\nAdor welding Pvt.Ltd - Pune, Maharashtra\nJune 2013 to June 2014\n-Developed numerous forecasting models to examine company\u2019s projected short- and long-term growth based on key indicators, sales projections, cash flow analysis, valuation, assets, liabilities, and credit risks.\n-Worked with CFO to create quarterly, annual, and long-term financial forecasts. Oversaw creation of annual budgets exceeding $35M annually. Played key role in renegotiation of company\u2019s warehousing costs, resulting in cost savings of $500,000 annually.\nImplemented the Sub Gradient Optimization Algorithm and solved it using the Lagrangean Relaxation and the KKT conditions.', u'Data Research Intern\nAutomotive Research Association Of India\nDecember 2012 to May 2013\nCollected data on diagonal and radial ply tires and their dynamic growth at 150 kmph.\n\uf0b7 Founded trends and seasonality in the data and categorised the data. Fitted the multinormal logit function using the logistic regression in R.\n\uf0b7 Designed a tyre test rig to measure the dynamic growth of tyres as per FMVSS and ISO standards.', u'Data Research Intern\nVolkswagen Pvt ltd\nJuly 2012 to December 2012\n-Collected data from different vendors on the defective parts and reduced 90 % of the defects in parts per million for front frame carrier and achieved 97% Direct OK seats for Polo, Vento, Fabia models and Used 7QC tools like Ishikawa , Why-Why Pareto Analysis.\n\uf0b7 Founded the covariance and autocovariance function. Differenced , detrended and fitted the Box Cox Transformation. Used ACF and PACF plots to find the best fit ARIMA model.\n\n.']","[u'Master of Science in Industrial Engineering', u'Bachelor of Technology in Mechanical Engineering']","[u'University at Buffalo, The State University of New York\nSeptember 2017', u'College of Engineering Pune, University of Pune Pune, Maharashtra\nMay 2013']"
0,https://resumes.indeed.com/resume/dd7c5c4b906d8826,"[u'Writer\nSLUG Magazine\nAugust 2015 to Present\nWrote album reviews and conducted interviews with professional musicians.', u'Software Development Intern\nEpic\nMay 2017 to August 2017\nUsed NLP and machine learning to develop tools for medical researchers to use during clinical trials.', u'Data Integration Engineer\nTheradoc\nJuly 2016 to May 2017\nBuilt and maintained interfaces and databases for hospitals using PL/SQL']","[u'Bachelor of Science in Computer Science', u'']","[u'Honors College\nMay 2018', u'University of Utah College of Engineering Salt Lake City, UT']"
0,https://resumes.indeed.com/resume/b50194b0efa1ed13,"[u'Data Analyst Intern\nRSNA - Oakbrook, Illinois - May, Illinois, US\nMay 2017 to December 2017\n\u2022 Programmed ETL SSIS data pipeline workflow to store the member usage details and set up an end to end data flow and migrated it to Redshift from S3 bucket and conserved the time needed to deploy code by 30min.\n\u2022 Developed a text mining model using python NLP that compared the social metrics and survey data from customers and identified the top customers with age groups and trend patterns based on the voting decisions.\n\u2022 Predicted the factors influencing the annual membership activity of RSNA by creating multivariate linear regression model using AWS machine learning and developed interactive dashboard in tableau for measuring the rate of activity.', u'Software Engineer Analyst\nAccenture - Chennai, Tamil Nadu\nDecember 2015 to July 2016\n\u2022 Discovered the key drivers of the customers like ""Customer service satisfaction"", ""satisfaction with price"" using SAS key driver analysis that lead to an amplification of 25% to the overall customer rating.\n\u2022 Validated data profiles in Teradata to resolve data gaps with focus on security and devised ETL rules to prevent the recurrence that lessened the average processing time from 15min to 8min.\n\u2022 Analyzed internal and external data sources to determine the factors affecting FX deals using R by integrating data from SQL server and expedited accurate forecasting and planning by 12%', u'Assistant Software Engineer\nAccenture - Chennai, Tamil Nadu\nJuly 2014 to December 2015\n\u2022 Created visually impactful dashboards in Excel and Tableau for data reporting by using pivot tables and VLOOKUP. Extracted, interpreted and analyzed data to identify key metrics and transform raw data into meaningful and actionable information.\n\u2022 Carried out trend analysis using Python Pandas by querying historical data through Postgres and derived the stock performance by boosting overall trends of the market by 15%\n\u2022 Forecasted 1st year loss rate ($) based on early delinquency rate at portfolio by computing moving average of historical roll rates using HIVE and PIG scripts that reconciled the account activity to $1M']","[u'Master in Information Technology and Management in Information Technology and Management', u'in Technology']","[u'Illinois Institute of Technology (IIT) Chicago, IL\nMay 2018', u'SASTRA University Thanjavur, Tamil Nadu\nApril 2014']"
0,https://resumes.indeed.com/resume/ea1785ec06249614,"[u'AI Engineer (Part-time)\nQuantum Mental Health\nJanuary 2018 to Present\nBuilding a virtual mental health therapist using IBM Watson platform.', u'Data Science Intern\nAntuit - New York, NY\nJune 2016 to August 2016\nUsed Keras library to build image-based product recommendation engine prototypes using Convolutional Neural\nNetworks to extract vectors of meaningful ""features"" from product images and assessing the similarity between feature vectors of different products']","[u'Master of Science in Computer Science', u'Bachelor of Science in Chemical Engineering']","[u'University of Southern California Los Angeles Los Angeles, CA', u'University of California Los Angeles, CA']"
0,https://resumes.indeed.com/resume/78625f7b0bd17758,"[u'Senior Data Engineer\nZenefits - San Francisco, CA\nAugust 2017 to Present\nSQL\n\nTechnical lead for internally-facing data engineering initiatives.\n* Diagnosed the problems with the failing Redshift and SQL ETL stack and designed a replacement Athena and Spark ETL stack.\n* Lead the implementation, saving USD$300K/year in operating expense,\nincreasing data quality and uptime, with zero end-user impact or migration.', u'Data Engineering Manager\nClara Lending - San Francisco, CA\nJuly 2016 to August 2017\nFounded and lead a small team of data engineers to establish analytics at the organization.\n* Identified short- and medium-term organization objectives around analytics, and initiated a project strategy to meet them on-time and under-budget.\n* Designed and lead the implementation of this initial architecture, using Scala and\nPostgreSQL. Selected Looker as the analytics platform vendor.\n* Aggressively tackled the highest-value, cheapest-to-implement analytics use\ncases, including email marketing and web application product usage analysis.', u'Senior Software Engineer\nClover Health - San Francisco, CA\nJuly 2015 to June 2016\nTechnical lead of design and implementation of data related services.\n* Scaled application engineers with the highest value libraries, designing and implementing functions such as service lifecycle management, centralized event\ntracking, and compiling complex state machines.\n* Greatly increased application engineer throughput with ORM-like extensions to\nSQLAlchemy on PostgreSQL, especially in the domain of temporal data.', u'Data Engineer\niMatchative - San Francisco, CA\nAugust 2014 to February 2015\nDeveloped the primary data ingestion pipeline in Python and Microsoft SQL Server T-\nSQL.', u'Platform Engineer\nNodePrime - San Francisco, CA\nMarch 2014 to August 2014\nResponsible for the data tier behind ingestion and analysis of IoT metrics. Designed the\nHTTP API for ingestion and time series analysis, compiling complex queries to\nOpenTSDB expressions.', u'Data Engineer\nNanigans - San Francisco, CA\nMay 2013 to January 2014\nResponsible for developing the data tier supporting real-time bidding and analytics for\nFacebook advertising optimization.\n* Engineered and built a data ingestion system on Flume meeting 99.999% uptime\nrequirements. The primary data being persisted and analyzed is 4,000 pixel\nevents per second 24/7 into a Cassandra cluster.\n* Responsible for performance and stability tuning (mostly GC).', u'Data Management Database Development Manager\nEze Castle - Boston, MA\nJanuary 2012 to May 2013\nLed a team of 4 database developers advancing the state of the Data Management\nproduct and providing 3rd level support to existing deployments of Data Management at a 30+ large institutional client base.\n* Designed and implemented a declarative materialized view framework to allow\nhighly customized, client-specific data integration workflows. These went from 1- 2 developer weeks on average of 2x per year per client to less than 10 minutes of\n1st level support time, saving an entire developer.\n* Designed and led the implementation of a dynamic mapping of Data Management\nposition data to SSAS. Previously, client-specific reports and feeds required 1-2\ndev weeks on average of 3x-4x/ year per client, as well as maintenance requiring\n2-3 dev weeks on average of 2x/year per client. This framework drastically cut\ntime required to 1-2 days of 2nd level support time, saving 3 devs.']",[u'B.S. in Management Studies'],"[u'Boston University Boston, MA\nJanuary 2001 to July 2007']"
0,https://resumes.indeed.com/resume/4eec8f7f4e38748a,"[u'QC Data\nATT - Denver, CO\nSeptember 2016 to Present\nInput various information into AT&T database via Auto-Cad software.', u'Mechanical Engineer, Sr\nSierra Nevada Corporation-ISR Division - Sierra, Nevada, US\nNovember 2007 to September 2014\nDenver, Colorado\n\u2022 Designed support structure using Solidworks and PDM software for avionic\nboxes, antennae, radio equipment and auxiliary components including equipment\nracks, consoles and carry-on equipment to meet the requirements of the System and Electrical Engineers.\n\u2022 Developed technical drawings in order to manufacture detailed components.\n\u2022 Developed redline drawings to support quick response for hangar personnel and vendor requests.\n\u2022 Modified existing aircraft to accept reconnaissance and surveillance equipment.\n\u2022 Re-design of existing assemblies and installations to improve performance and expedite manufacturing.\n\u2022 Worked liaison engineering, product integration, prototype development and vendor coordination in a production environment.\n\u2022 Supported floor technicians by solving problems, answering questions, and generating solutions via Non-Conforming Reports (NCR).\n\u2022 Employed Unigraphics NX to develop support structure for the main engines and main landing gear on the Dream Chaser program.\n\u2022 Refurbished a 60 year old aircraft to flight status. Responsibility included\nredesign of major bulkheads and aircraft airframe.\n\u2022 Redesigned interiors of medium sized aircraft to meet FAA aircraft smoke\nrequirements.', u'Design Engineer\nSikorsky Aircraft Company - Stratford, CT\nAugust 1981 to November 2007\nWorked liaison engineering in a production environment. Responsibility included\nresponding to aircraft problems due to initial design errors and/or aircraft\nmechanic errors involving the structural airframe of the aircraft. This included\nsheet metal and carbon fiber issues. Problems were resolved via engineering\norders and drawing revisions\n\u2022 Worked with peers in an integrated product development team environment.\nResponsibility included using Catia software to develop design concepts for an all carbon fiber keel beam, which lead to the production development. This task into the manufacturing/production phase of the program.\n\u2022 Lead Design Engineer for a major section of the aircraft. Responsibility included\nsupervision of 6 to 10 engineers to design the center box beam of an all carbon\nfiber airframe. This included interfacing with all major components of an aircraft.\nThis task into the manufacturing/production phase of the program.\n\u2022 Worked as a design engineer to reconfigure existing aircraft structure for prototype development and upgrade system requirements.', u'Design Engineer\nBoeing Aircraft Company\nNovember 1977 to July 1981\nSecurity Clearance\nSecret clearance based on NACLC completed on March 11th, 2011.']",[u'Bachelor of Science in Civil Engineering'],"[u'Roger Williams University Bristol, RI']"
0,https://resumes.indeed.com/resume/2877d17bb5e988fb,"[u'Data Analyst\nVERSCEND TECHNOLOGIES\nDecember 2013 to August 2016\n\u2022 Extract, transfer and load of large data-sets using SQL Scripting.\n\u2022 Create new PS/SQL script(Stored Procedures, functions, triggers) for every new healthcare data sets (Eligibility, Medical, Pharmacy and Others), analyze, develop, test, deploy and quality check.\n\u2022 Work on all phases of SDLC implementations including analysis, design, development, testing, maintenance and code-refactoring.\n\u2022 Perform cross-functional analysis on huge database in Oracle 10g, 11g.\n\u2022 Extract and analyze health information for our product Medical Intelligence (MI) - MI is an event based clinical web-based application that facilitates identification and stratification and pinpoints opportunities for clinically sound, financially effective interventions.\n\u2022 Provide ad-hoc reports to clients through SQL scripts.\n\u2022 Documentation and reporting through Microsoft Word, PowerPoint, MicroStrategy, Excel (performing analysis and displaying complex data into graphs using macros)\n\u2022 Successfully developed and optimized complex SQL queries to ensure optimal performance using joins, grouping, aggregation, sub-queries, and views to retrieve data from relational databases. (This drastically decreased the script running time and used space)\n\u2022 Creating various Data Visualization with ggplot2, lattice packages in R Programming Language. Also, Performed Data Cleaning using R.\n\u2022 Integrate business and clients specific requirements.\n\u2022 Follow the ETL process (Extract, Transform and Load) to access and manipulate source data and load it into target database.\n\u2022 Develop test cases, test scripts, test metrics and documentation including data research.\n\u2022 Prepare scripts to automate manual tasks.\n\u2022 Prepare reports and tools for Higher Management for decision making.\n\u2022 Mentor new recruitments, interns, trainee and associates by providing technical guidance.\n\u2022 Quality Control through automated in-house application.', u'Associate Software Engineer\nVERSCEND TECHNOLOGIES\nMarch 2013 to December 2013\n\u2022 Data import of large data files into Oracle database using SSH, PL/SQL\n\u2022 Understanding US health care business logics \u2013 related to Medicaid, Medicare\n\u2022 Graphical representation of the imported data through Excel and macros.\n\u2022 Used advanced Microsoft Excel to create pivot tables and charts, Quick Analysis, Power view, Flash fill, VLOOKUP, Advanced Formulas, Managing Data with Tables and other Excel functions.\n\u2022 Development of in-house web based canteen ordering system using Java swing.\n\u2022 Optimizing and managing database (Oracle 10g) \u2013 cleaning data and scripts.', u'Junior Data Analyst/Internship\nJasper IT\nSeptember 2012 to March 2013\n\u2022 Import and analyze the data in SQL Server.\n\u2022 Report the data trend to higher management.\n\u2022 Create DTS of the logics mapped.\n\u2022 Develop test cases to check the data.\n\u2022 Maintenance on tables, stored procedures, job schedules and users in SQL database.']","[u""Master's in Computer Science""]","[u'Western Illinois University Macomb, IL\nAugust 2016 to December 2017']"
0,https://resumes.indeed.com/resume/40e1334ddffd5f25,"[u'Data Center Engineer ll\nAOL/Yahoo - Dulles, VA\nJune 2017 to February 2018\n\u2022 Managed Internal Electrical Load Management Tool across three CoLo Data Centers (2 Local, 1 remote) totaling 3.3MW\n\u2022 Assigned all Infrastructure Servers and Network Gear throughout Data Centers\n\u2022 Break Fix drives/network cards in AOL & Brightroll Data Centers\n\u2022 Managed Internal Contractors for daily assignments', u'Sr. Data Center Project Engineer\nAOL/Yahoo - Dulles, VA\nMarch 2011 to June 2017\n\u2022 Managed Internal Electrical Load Management Tool for AOL owned 6MW Data Center and three CoLo Data Centers totally 9.3MW\n\u2022 Successfully sustained a 6MW facility at 94% electrical capacity\n\u2022 Designed Layout and electrical power for Verizon VDMS service Data Center transfer into AOL Data Center\n\u2022 Managed Vanderbelt SMS for all Data Center Restricted Access Request\n\u2022 Successfully reduced operational cost for three AOL CoLo Data Centers with a cost savings of 100K per month by reducing our CFMs and decreasing our PUE\n\u2022 Supported System Integration and Network teams with disk replacement/ network card swaps and new server installs\n\u2022 Managed all Internal Data Center Contractors', u'Data Center Project Engineer\nAOL/Yahoo - Dulles, VA\nMarch 2009 to January 2011\n\u2022 Managed Internal Electrical Load Management Tool for AOL Owned Data Center 6MW facility\n\u2022 Effectively planned all project installs and forecasted infrastructure growth\n\u2022 Streamlined all procedures for increased network and hardware installations\n\u2022 Managed Contractors and Data Center Technicians']",[u'Associate Degree in General Studies in General Studies'],"[u'Brevard College Brevard, NC']"
0,https://resumes.indeed.com/resume/51cf4f1978c6740d,"[u'Data Analytics Intern\nENSONO - Downers Grove, IL\nJanuary 2018 to Present\nProject: Marketing: Data Modeling using R Sales: Salesforce Reporting Jan 2018 - Present\n\u2022 Purchase Model: Investigated data from different sources like Marketo, Salesforce, RainKing, CASD to come with dependent and independent variable and performed Exploratory Data Analysis with visualizations.\n\u2022 Built Model using Decision Tree & Random Forest algorithms in R to help campaign manager identify if upcoming client will purchase technology service we provide.\n\u2022 Salesforce Reporting: Loaded Accounts, Opportunities, Contacts and Leads table data from Salesforce to SQL-Server database with the help of SQL Server\nIntegration Services(SSIS) on a weekly basis.\n\u2022 Introduced Trends Dashboard to increase visibility of the Sales Pipeline and improve Forecasting Accuracy with weekly snapshot.\nENSONO, Downers Grove, IL, US Data Analytics Intern Project: Sales / Marketing Social Media Analytics', u""Data Analyst Intern\nENSONO\nAugust 2017 to December 2017\nProduced Metrics like Fan Reach, Organic Reach, Click-Through Rate, Engagements, Reactions, Top Status, Negative Feedback, etc. to understand\nFacebook page performance using Ensono's and competitor's data extracted via Python and Social API.\n\u2022 Twitter: Analyzed Twitter data with the help of web data connector to have reports based on Influencers, Followers, Top Tweets, Engagements/Impressions.\n\u2022 Twitter Followers List: Pulled our followers list report with the help of R language and Social API to know and see whether Follower Base is growing (or shrinking).\n\u2022 Sales: Generated Client Health Dashboard for the Sales management team to view Overall Client Health, Client Engagement, Sales/Financials, Service Performance and Client's Account view using Tableau's advanced features like Groups, Sets, Calculated fields, Level of Detail(LOD), Clustering, etc."", u""Data Analytics Intern\nENSONO - Downers Grove, IL\nJune 2017 to August 2017\nIdentified ideal client's attributes / behaviors and potential match to new targets. Using Alteryx integrated data from different data sources like Marketo, Salesforce,\nWorkday, RainKing, ISG, etc.\n\u2022 Established different analysis based on Rapid Response, Web Visits, Page Performance and Campaign statistics to help understand 360-degree view of overall\nintegrated data.\n\u2022 Formulated interactive dashboards with trends, forecast and basic analysis of existing clients, web activity, account planning and sales territory."", u'ETL-Datamart-Sales\nIllinois Institute of Technology\nMarch 2017 to March 2017\n\u2022 Using Pentaho performed ETL task (Extraction, Transformation and Loading) to load data from Excel source to MySQL database with major transformation of data.\n\u2022 Created dashboards to visualize sales data using Tableau by connecting MySQL database and provided the findings and business decisions for the same.', u'Software Engineer\nSYNTEL Ltd - Mumbai, Maharashtra\nJuly 2015 to April 2016\n\u2022 Worked on SQL scripts and front-end with enhancement of the banking application using .Net which included agile requirement.\n\u2022 Analyzed data for reports and migrated application to Azure cloud with worker role task which included scheduling of email and reports.', u'Software Engineer\nSYNTEL Ltd - Mumbai, Maharashtra\nSeptember 2014 to June 2015\nAdded cloud services to existing .Net application also studied and applied changes reflected in internal CCAT (Cloud Code Automation Tool) tool to migrate in Azure\nplatform, worked on SQL scripts, etc. Extracted weekly reports from the database to analyze data of different applications.\n\nRELEVANT PROJECTS']","[u""Master's in Information Technology and Management and Specialization in Data Management"", u""Bachelor's in Information Technology""]","[u'Illinois Institute of Technology Chicago, IL\nAugust 2016 to May 2018', u'PVPP College of Engineering, University of Mumbai Mumbai, Maharashtra\nMay 2014']"
0,https://resumes.indeed.com/resume/9dc49fb77142798e,"[u'Data Scientist\nVerizon - Dallas, TX\nJanuary 2016 to Present\nDescription:\nThe Project was mainly focused on reducing customer churn by understanding the customer behavior using Statistical Modelling, Machine Learning techniques and take necessary steps to reduce customer churn as much as possible.\nResponsibilities:\n\u2022 Working closely with marketing team to deliver actionable insights from huge volume of data, coming from different marketing campaigns and customer interaction matrices such as web portal usage, email campaign responses, public site interaction, and other customer specific parameters.\n\u2022 Characterizing false positives and false negatives to improve a model for predicting customer churn rate.\n\u2022 Consumer segmentation and characterization to predict behavior. Analyzing promoters and detractors (defined using Net Promoter Score).\n\u2022 Outlier detection using high-dimensional historical data. Acquiring, cleaning and structuring data from multiple sources and maintain databases/data systems. Identifying, analyzing, and interpreting trends or patterns in complex data sets.\n\u2022 Developing, prototype and test predictive algorithms. Filtering and ""cleaning"" data and review computer reports, printouts, and performance indicators to locate and correct code problems.\n\u2022 Developing and implementing data collection systems and other strategies that optimize statistical efficiency and data quality.\n\u2022 Used different statistical models like regression and classification models to create contact scoring models. Also used clustering to the customer data profiles to do customer segmentation and analysis.\n\u2022 Interpreting data, analyze results using statistical techniques and provide ongoing reports.\n\u2022 Building a recommender system based on client\'s past renewal history to upsell and cross-sell them other related products or services. Created a recommendations engine that finds related customers, products.\nEnvironment: Python- Pandas, Numpy, Scikit-Learn, TensorFlow - ANN, SciPy, Seaborn, Matplotlib, SQL, Machine Learning, Deep Learning. R-Foreign, ggplot, igraph, lattice, MASS, mice and logit.', u'Data Science Pilot Project\nPitt Plastic - Pittsburg, KS\nMay 2015 to December 2015\nDescription:\nThe project was focused on sales prediction by using Sales training data into supervised classification algorithm to predict customer churn.\nResponsibilities:\n\u2022 Supported sales forecasting & planning team by improving time series & principal component analysis.\n\u2022 Utilized machine learning techniques for predictions & forecasting based on the Sales training data.\n\u2022 Executed overall data aggregation/alignment & process improvement reporting within the sales dept.\n\u2022 Managed Data quality & integrity using skills in Data Warehousing, Databases & ETL.\n\u2022 Monitored and maintained elevated levels of data analytic quality, accuracy, and process consistency.\n\u2022 Assisted sales management in data modeling.\n\u2022 Ensured on-time execution and implementation of sales planning analysis and reporting objectives.\n\u2022 Worked with sales management team to refine predictive methods & sales planning analytical process.\n\u2022 Executed and monitored the accuracy and efficiency for sales forecasts & reporting.\n\u2022 Prepared Dashboards using calculations, parameters in QlikView.\n\u2022 Supported consistent implementation of company reporting and sales process initiatives.\n\u2022 Used Python to identify customer classification, tree map, and regression models.\n\u2022 Performed forecasting and time series analysis of customer likes and dislikes.\n\nEnvironment: ETL, QlikView, Python, Machine Learning, SQL', u'Data Analyst / Data Engineer\nCigna - Hyderabad, Telangana\nMay 2013 to December 2014\nDescription:\nDealing with scenarios related to Healthcare fraud, waste and abuse detection; denial claims management; clinical pathways optimization; and health plan member profitability- Understanding and managing clinical variation across a hospital system, for patients undergoing specific types of surgery.\nResponsibilities:\n\u2022 Characterizing false positives and false negatives to improve a model for predicting overpaid claims, Consumer segmentation and characterization to predict behavior.\n\u2022 Outlier detection using high-dimensional historical data.\n\u2022 Analyzing promoters and detractors (defined using Net Promoter Score).\n\u2022 Installed and configured Hadoop MapReduce, HDFS, Developed multiple MapReduce jobs in Python for data cleaning and preprocessing.\n\u2022 Supported Map Reduce Programs those are running on the cluster. Involved in loading data from UNIX file system to HDFS.\n\u2022 Experienced in managing and reviewing Hadoop log files.\n\u2022 Involved in writing Hive queries to load and process data in Hadoop File System.\n\u2022 Exported data from Impala to Tableau reporting tool, created dashboards on live connection.\n\u2022 Gained very good business knowledge on health insurance, claim processing, fraud suspect identification, appeals process etc.\n\u2022 Maintained integrity of the database by implementing different validation techniques to the data uploading procedures.\n\u2022 Defended conjunction between ERP system and database using SQL reporting services.\n\u2022 Developed dashboards in Excel with SQL connections to ensure smooth procedural flow and analysis.\n\u2022 Identified and proposed process enhancements.\n\u2022 Quantified multiple stocking proposals and contracts with customers which increased business by approximately 20% per customer.\n\u2022 Developed ad-hoc reports upon requests using MS Access and MS Excel.\nEnvironment: Hadoop, MapReduce, Hive, Impala, Python (pandas, NumPy, scikit-learn), SQL Server, Excel, Tableau', u'Data Analyst\nHexaware Technologies Hyderabad - Hyderabad, Telangana\nJune 2011 to April 2013\nProject 1: Customer Satisfaction Analysis\nCustomer Satisfaction Analysis Using Regression Models\nResponsibilities:\n\u2022 Identifying what factors could influence the overall satisfaction of consumers. Range (1-5).\n\u2022 Considered the SMG (Service Management Group) database survey results in analyzing the impact on overall customer satisfaction.\n\u2022 Analyzed business process workflows and assisted in the development of ETL procedures for mapping data from source to target systems\n\u2022 We used Ordinal logistic regression methodology in explaining the importance of features.\n\u2022 The analysis involved predicting the overall satisfaction - ordinal rating, by analyzing the impact of each independent factors in explaining the output.\n\u2022 Packages used: MASS package, plot function for Ordinal logistics regression model.\nEnvironment: R Studio, SQL Server, Dplyr, Tidyr, ggplot2, Tableau, MASS package - plot function.\nProject 2: Annual Marketing Budget Allocation.\nAnnual Marketing Budget Allocation.\nResponsibilities:\n\u2022 Responsible for Data collection and data preparation and normalizing the data.\n\u2022 Used SQL, ETL tool, R Studio, and Python for data preparation.\n\u2022 Supported data consultants in the data modeling phase.\n\u2022 Used Constraint optimization algorithms (USED EXCEL) to optimize the marketing budget.\n\u2022 Used Time Series Models - decomposition of time series, trend, and seasonality detection, forecasting and exponential smoothing in predicting the market share and brand share to allocate the Marketing budget\nEnvironment: R , Python (pandas, NumPy, scikit-learn), SQL Server, Excel , ETL.']",[],[]
0,https://resumes.indeed.com/resume/3be831a8d8534069,"[u""Data Migration Engineer\nCerner Corporation - Kansas City, MO\nJuly 2015 to Present\n\u2022 Organize multiple projects at once including data migrations, building of interfaces, and supporting live clients\n\u2022 Automate data migrations with terabytes of data into client data bases and health systems\n\u2022 Building added functionality such as adding custom code sets, or additional feeds\n\u2022 Execute outstanding customer supports skills and communication with clients\n\u2022 Custom scripting based on client's data such as table lookups, IF statements, and creating custom code sets\n\u2022 Perform code reviews internally for any changes made to master scripts affecting clients\n\u2022 Manipulate data via scripting creating an automated process ensuring no data is manually edited.\n\u2022 Consult with my team or client(s) on a weekly basis to discuss project timeframes & correctly allocate resources\n\u2022 Use Linux/Putty to cycle servers, run data extracts, edit files, connect to sftp, and trouble shoot data loads\n\u2022 Build custom inbound/outbound servers to upload files or messages\n\u2022 Lead calls with clients for updates and progress statuses as well as answer any questions about solution""]",[u'BSc. in Management Information Systems'],"[u'Kansas State University Manhattan, KS\nAugust 2010 to May 2015']"
0,https://resumes.indeed.com/resume/32535092b3774c09,"[u'Data Analyst III\nHealth Integrity, LLC - Dallas, TX\nMarch 2016 to Present\n\u2022 Built dozens of Ad-hoc data reports for law enforcement and internal clients, created the SAS macro templates to increase task efficiency by 50%, and improved the process time for Fraud Prevention System reports by 75%\n\u2022 Conducted and presented proactive data analysis to identify hundreds of investigation for Medicare and Medicaid fraud, waste, and abuse investigation using statistical model (e.g. Risk Score)\n\u2022 Created and reviewed statistically valid random samples generated by SAS EG and completed more than $1 million of Medicare data-driven overpayment requests', u'Business Intelligence Engineer Intern\nMedeAnalytics - Dallas, TX\nJuly 2015 to December 2015\n\u2022 Constructed relational database and stored procedures to achieve daily data loading for summary and detail report in SQL Server 2012 and built real-time online data reports for hospital clients\n\u2022 Created complex queries, data transformation, aggregation, stored procedures, and triggers using T-SQL, and generated customized reports including charts and table reports using MDX and SSRS\n\u2022 Troubleshoot and validated the ETL process daily to guarantee the data load successfully']","[u'M.S. in Information Technology and Management', u'M.A. in English Language and Literature', u'B.S in Computer Science']","[u'University of Texas at Dallas Dallas, TX\nDecember 2015', u'Shanghai Normal University\nJuly 2014', u'Shandong Agricultural University\nJuly 2011']"
0,https://resumes.indeed.com/resume/862eadd0c464eba2,"[u'Data Science Intern\nViral MD\nAugust 2017 to December 2017\n\u2022 Converted the PDF containing the scanned document to XML using the Abbyy OCR technology.\n\u2022 Development of scripts in Python to query the XML by converting it into element tree in to query the required information\nusing MySQL server using Etree and SQLAlchemy libraries.', u'Software Engineer\nAccenture Services Pvt. Ltd\nJuly 2015 to June 2016\n\u2022 Development of Oracle based utilities application using Java and oracle database system.\n\u2022 Developed scripts and plugins for Oracle Fusion Middleware and Oracle Master Data Management Systems.\n\u2022 Debugging and code fix of the utilities application using JUnit and HPE LoadRunner.']","[u'Masters of Computer Science in Computer Science', u'Bachelor of Engineering in Information Technology in Information Technology']","[u'The University of Texas at Dallas Richardson, TX\nMay 2018', u'University of Mumbai Mumbai, Maharashtra\nJune 2015']"
0,https://resumes.indeed.com/resume/9887f125ebcec937,"[u'Data Scientist, Consultant\nConstant Contact - Waltham, MA\nSeptember 2017 to Present\n\u2022 Performed the root-cause analysis on large amount of spam emails data using NLP and Text mining and then built a classification model to reduce improve the overall false negative rate.\n\u2022 Collaborated with BI and Analytics teams to leverage clickstream data with various other sources to build predictive modeling of visitor quality, conversion, retention.\n\u2022 Integrated Tableau with Spark SQL; loaded data in Tableau Server, and created and deployed BI dashboards.\n\nEnvironment: Amazon S3, Hadoop Hive Spark, Python, R, Java, SQL, NLP, Google Analytics', u'Data Scientist, Consultant\nDassault Systems - Waltham, MA\nFebruary 2017 to August 2017\nProject: Digitization and automation of all kinds of customers, marketing and pipeline data in order to provide a full customer 360 and events 360 view. Apply machine learning techniques to get actionable customer insight, improve forecasting, create affinity models personalize advocacy marketing, and increase ROMI.\nResponsibilities:\n\u2022 Extracted and transformed customers and marketing datasets obtained from multiple sources (Oracle, PostgreSQL), loading them into EXALEAD for indexing, and developed a web based front end/search bases applications using a combination of Grails/Java + JQuery + d3.js.\n\n\u2022 Consolidated CRM and events data and applied data mining techniques to perform CLV, segmentation, profiling and other scoring models.\n\n\u2022 Built and tested Time Series model in Spark R for trends and predictive sales pipeline analytics, used various smoothing methods to improve the model, and integrated the algorithm with the sales dashboard using D3.js, resulting 98.5% accuracy in FY17Q1.\n\u2022 Created a k-mean clustering model using Python to facilitate product product marketing campaigns.\n\u2022 Built the response model for email marketing using Logistic Regression resulting in a 350% increase in response rate.\n\u2022 Analyzed a vast amount of text data from various sources for NLP use cases, used Apache Spark for Massively Parallel NLP and Logistic Regression to predict the probability of the survey questions.\n\u2022 Developed a few dynamic BI dashboard and produced regular reporting to track key KPIs, sales and performance matrixes across multiple channels, used Tableau and D3.js.\n\u2022 Generated hypothesis for A/B testing for Banner Ads on Google, additionally, for multiple alternatives, choose multi-armed bandit algorithm to create more value in spending - saving 17% on budget with 750% ROI on PPC channel.\n\u2022 Involved with the R&D team in conceptualizing of a new Deep Learning framework with 3D simulation, revolutionizing the Data-Driven Design for PLM/CAD solutions.\n\nEnvironment: Azure, Hadoop Hive Spark, Python, R, Java, SQL, EXALEAD Cloudview, SQL, HTML, D3.js, JQuery', u""Data Mining Scientist\nDassault Systems - Waltham, MA\nSeptember 2016 to January 2017\nProject: Assisting product marketing team with segmentation, positioning, and overall go-to-market strategy for 2017 product launch. Building appropriate analytical models for the classification and text-mining problems, reporting services using Azure data warehouse and data visualization per business requirement.\nResponsibilities:\n\u2022 Created a product portfolio application using previous years of revenue data, using EXALEAD for indexing and HTML CSS JavaScript and Java to create an application.\n\u2022 Finding outliers, errors, trends, missing values, and distribution in the data. Utilized techniques like Histogram, Bar plot, pie-chart, scatter chart, box plots to determine the initial conditions of the data.\n\u2022 Loaded data from Azure HDInsight to Spark RDD and created a K-mean clustering model using Spark MLlib in order to segmented customers for advocacy marketing, used D3.js for visualization.\n\u2022 Built a regression model for revenue and channel analytics with detailed correlation and causation analysis, using Spark MLlib, integrated with global marketing dashboard using JavaScript.\n\u2022 Analyzed usage trends for content with Adobe analytics and CRM Data Extracts with Siebel Analytics.\n\u2022 Analyzed customer's feedback data using text mining package spark-ts in Spark, helping the team to know the sentiments of the customers about the product, ENOVIA.\n\u2022 Improved UX|UI of ENOVIA's webpage by collaborating with the design and engineering team.\n\n\u2022 Developed and maintained the BI dashboard and produced regular reporting to track key KPIs, sales and performance matrixes across multiple channels, used Tableau and D3.js.\n\nEnvironment: Azure, Hadoop Hive Spark, Python, JavaScript, Tableau, Adobe Analytics, EXALEAD"", u""Data Analyst\nUniversity of Massachusetts Boston - Boston, MA\nMarch 2014 to August 2016\nThe project was to enable the purchasing/finance department with periodic (weekly/monthly/quartly/yearly) reporting, data modeling, regression, and visualization solutions. Maintain and manage Contract & Compliance and asset management database. Also, Optimization of $70 million budget for two consecutive FYs.\nResponsibilities:\n\u2022 Analyzed business requirements, system requirements, data mapping requirement specifications, and responsible for documenting functional requirements and supplementary requirements.\n\u2022 Initial pattern recognition and data cleansing using dpylr package in R. Worked on data manipulation on raw purchasing data in different formats from multiple sources (oracle, byways) and prepared the data for further analysis using ggvis and ggplot2 packages.\n\u2022 Prepared annual/semi-annual/quarterly/monthly reports for the department using SQL server reporting service (SSRS) and advanced excel functions like Vlookups, PivotTables, Merging, Sorting.\n\u2022 Analyzed reports of data duplicates or other errors based on monthly or daily data reports.\n\u2022 Collected commodities data from multiple websites such as Target, Amazon and Walmart by using web scrapping with Python for price comparison of products, reducing annual expenditures by 7%.\n\u2022 Provided visual spend analytics using Tableau to identify opportunities to reduce cost, track contract compliances, and measure supplier's performance - 11% cost reduction.\n\u2022 Strategic contributions to finance and contract and compliance board meetings regarding optimization of university's budget and new projects.\nEnvironment: Excel (Vlookups, PivotTables etc), Tableau, PeopleSoft, SQL Server 2012, R Shiny, R Studio, Python\nVisual IQ, Needham MA\nData Scientist, Marketing\nProject: Building a recommendation system for a media streaming and retail clients. Further, develop web-apps and dashboards to showcase the results for the stakeholders for creating the appropriate strategies.\nResponsibilities\n\u2022 Built a real-time streaming model using Spark Streaming - Loaded streaming data from Amazon S3 to Spark RDDs, wrote a program to take the live streaming of Tweets, built a machine learning model using Spark MLlib TF-IDF in R, invoked it from SQL to build a dashboard with Apache Zeppelin that uses a live uses this machine learning model and filter out things for users.\n\u2022 Spatial data analysis and geo clustering using spatial libraries (sp, rgdal, maptools, spatstat, dbscan) - combining geo location data and search history to build recommendation systems.\n\u2022 Designed custom reports, charts, tables and dashboards in Shiny Apps to help the marketing and operations teams in their decision making process.\n\u2022 Created focused reports and dashboards on content/channel performance, lead generation and conversion rates using Google Analytics.\n\nEnvironment: Amazon S3, Apache Zeppelin, R, Python, Shiny, Google Analytics.\nBoston Children Hospital\nData Scientist/Project Manager\nProject: To create a project plan to develop a digital platform called KidsMD for Boston Children's Hospital to be a centralized technology platform of tools that parents/caregivers and patients can confidently refer to for interactive healthcare content.\nResponsibilities:\n\u2022 Worked with different teams (survey, pharmaceutical and research and development, claims etc.) to gain insights about the data concepts behind their health symptoms and business.\n\u2022 Designed ETL packages dealing with different data sources (SQL Server, Flat Files, and XMLs etc.) and loaded the data into target data sources by performing different kinds of transformations using USQL in Azure data lake analytics, and then created DataViz reports using Power BI.\n\n\u2022 Built the predictive model using multiple logistic regression using Spark MLlib in order to classify EMR and Non-EMR cases, analyzed the accuracy of the model using Receiver Operating Characteristic (ROC curve) and Confusion Matrix.\n\u2022 Provided recommendations regarding Voice Artificial Intelligence (voice-assistant) and collaborated with Amazon Alexa team for to partner with BCH platform.\n\u2022 Designed custom reports, charts, tables and dashboards using Tableau and for marketing and operations teams for their decision-making process.\n\u2022 Implement and customize web analytics and website optimization tools based on business needs, specifically Google Analytics.\n\u2022 Analyzed generic search data from GoogleAdwords to identify the trends in child health care.\nEnvironment: MS-Project, Microsoft Azure, Apache Spark, Python, SQL, Tableau, Google Analytics."", u'Software Engineer/Product Manager\nBeing Newton Group - New Delhi, Delhi\nMay 2012 to December 2013\nProject: Development of a computer-adaptive-testing (CAT) software.\n\u2022 Worked in a team of a few software developers to build Computer Adaptive Testing (CAT) application in C and Java using Agile SDLC methodology. Also, conducted JAD sessions and brainstorming.\n\u2022 Performed Unit Testing, Load Testing and Performance Testing in Java.\n\u2022 Designed a real-time intelligent chat bot in Python to improve the customer service and staff efficiency by 60%.\n\u2022 Strategic involvement in pricing strategy and overall go-to-market plan for Indian and global market.\nEnvironment: Agile, C, Java, Python, JIRA, PowerPoint', u'Data Analyst Intern\nBSNL - New Delhi, Delhi\nJanuary 2011 to April 2012\n\u2022 Worked with different teams to gain insights about the data concepts behind their business.\n\u2022 Performed data analysis primarily Identifying data sets, source data, source meta data, data definitions and data formats. Also, performed the root cause analysis for unexpected billings scores.\n\u2022 Gathered all data, cleaning and Analysis the data from different Districts inside of cities using time series analysis using R.\n\u2022 Used R Programing language to simple and complicate data analysis. Mainly, implement a wide variety of statistical and graphical techniques, including linear and nonlinear modeling, statistical tests, simple time-series analysis using IDE- R studio.\nEnvironment: R-Studio, SQL Server 2008, MS Excel, MS Access, PowerPoint, Statistics']","[u'MBA', u'BS']","[u'University of Massachusetts Boston Boston, MA', u'Information Technology Uttar Pradesh Technical University']"
0,https://resumes.indeed.com/resume/2d83fc791aac111f,"[u'Data Analyst\nAegis Data Intelligence Services, AegisMobile - Columbia, MD\nSeptember 2015 to Present\n\u2022Perform SQL query for data extraction, analysis, and reporting tasks on unrestricted data from hundreds of sources.\n\u2022Review stage data to ensure data quality, integrity, and validity.\n\u2022Identify access and extract data sources after research in various markets.\n\u2022Validate the data files from sources to make sure correct data has been captured to be loaded to target tables.\n\u2022Perform Data mining for the identification of unusual data records, nulls or data errors that require further investigation and present them to the senior management.\n\u2022Create source / target mapping documentation that included the transformation logics and business rules for complex datasets from various authorities and supportive sources.\n\u2022Validate the load process of ETL to make sure the target tables are populated according to the data mapping provided that satisfies the transformation rules.\n\u2022Knowledgeable in developing stored procedures, views, functions, triggers etc.\n\u2022Assist in the development of business requirements, synthesizing and standardizing data across various sources, and support the development of scalable, efficient, automated processes for large scale data analyses, model development, model validation and model implementation.\n\u2022Prepare necessary test plan and test scripts and executed them based on the functional requirements.\n\u2022Involve in Test Data creation and management.\n\u2022Perform the regression testing after the bugs have been fixed.\n\u2022Perform root cause analysis in finding the source of defects and implement corrective actions.\n\u2022Validate the reports generated by Cognos against the query in the database.\n\u2022Verify correctness of data after the transformation rules were applied on the source data.\n\u2022Help identifying reports for decision support system and interact with database designers and architect for gathering the requirements for reporting.\n\u2022Design and maintained Tableau reports used to graphically analyze business data.\n\u2022Strong knowledge of metadata organization, data cleansing, data profiling, and data modeling techniques including Star Schema and Snowflake Schema.\n\u2022Use advanced Excel functions like Pivot Tables, macros and vlookups, hlookups to manage, update and manipulate report orientation, structures and presentation.\n\u2022Use joins and sub queries for complex queries involving multiple tables from different databases.\n\u2022Deliver routine and ad-hoc reports in several formats.', u'Mechanical Engineer\nGeostrut - Lindon, UT\nJune 2012 to July 2014\n\u2022Prepared engineering calculations, designs, and documents for more than 100 projects per year on average in accordance to Telecommunication and engineering standards.\n\u2022Assisted Senior Scientist and Senior Engineer in testing, and evaluation of GeoStrut (carbon fiber tower) technology.\n\u2022Worked with sales department to review and analyze client specifications and requirements.\n\u2022Worked in collaboration with the engineers from Zoltek Carbon Fiber, Utah to study, simulate and analyze the wind loading on different GeoStrut structures which possess anisotropic property unlike metal structures.\n\u2022Investigate Quality issues for root cause and recommend/implement corrective actions.\n\u2022Developed an in-house Matlab application allowing to calculate the wind-loading in the tower structures per TIA standards.\n\u2022Ensure proper and continuous operation of the manufacturing equipment and systems.\n\u2022Prepared pricing and cost analyst, quotes and proposals for the clients specification.\n\u2022Enhanced the production capacity of the facility by 60 % with the optimization of CNC programming that allowed faster winding of carbon fiber tow and reduction in man-hours.\n\u2022Trained 6 production employees in operating CNC machines for cutting silicon pads and winding carbon fiber tow.']","[u'Master of Science in Information Technology', u""Bachelor's in Mechanical Engineering""]","[u'Towson University Towson, MD\nSeptember 2014 to December 2016', u'Brigham Young University-Provo Provo, UT\nSeptember 2007 to August 2012']"
0,https://resumes.indeed.com/resume/8fa24583ef383fd0,"[u'Big Data developer\nCGI - Fairfax, VA\nDecember 2015 to October 2017\nFairfax, VA, USA 12/2015 - 10/2017\nProject Type\nThe project involved developing application services with agility, transformation and cost efficiencies for their esteemed Red Roof Inn hotel client.\n\nRole \u2022 Big Data developer\nResponsibilities\n\u2022 Gathered information and requirements from the users, then documented in BRD, FSD.\n\u2022 Using Kafka to build pipelines from different sources to HDFS.\n\u2022 Written java Map-Reduce programs in AWS EMR to get the semi & un-structured data to structured data and to incorporate all the business transformations.\n\u2022 Developing the process to move the output of map-reduce data to MarkLogic and HBase for analytics\n\u2022 Used Informatica tool for cleaning, enhancing and protecting the data.\n\u2022 Performed Hive Queries to analyze the data in HDFS and to identify issues.\n\u2022 Worked on shell scripting to automate jobs.\n\u2022 Involved in building Hadoop Cluster.\n\u2022 Configured Hive MetaStore to use Oracle/MySQL database for establishing multiple connections.\n\u2022 Experience in retrieving data from MySQL and Oracle databases into HDFS using Sqoop and ingesting them into HBase for data processing.\n\u2022 Responsible for deriving the new requirements based on business data driven method for ETL applications.\n\u2022 Created Search server using Solr, indexed the files and then queried using HTTP GET calls\n\u2022 Used Oozie & Airflow to schedule automatic workflows in Hadoop Ecosystem.\n\u2022 Writing web scraping programs using python, java\n\u2022 Using PySpark SQL and Streaming for querying and analyzing real time data.\n\u2022 Writing Spark Programs in Python, Scala.\n\u2022 Building PySpark Models by using Machine Learning Algorithms.\n\nEnvironment: HDFS, MapReduce, Hive, Kafka, Spark 2.1.0, Java, Python, Scala, MySQL, HBase, Oracle, Sqoop, MarkLogic, Informatica', u""Software Engineer\nOracle - Bengaluru, Karnataka\nJanuary 2013 to July 2015\nBengaluru India 01/2013 - 07/2015\nProject Type\nThe Oracle Advanced Analytics team is responsible for developing scalable analytics on a variety of platforms, including the Oracle database, Oracle BigData Appliance, Apache Hadoop, Apache Hive, and Apache Spark. The team is responsible for developing the underlying framework, infrastructure, as well as the algorithms.\n\nRole \u2022 Software Engineer- Oracle Data Cloud\nResponsibilities\n\u2022 Obtain requirements from business SME's, documentation of system requirements, create data models, reporting specifications and test plans/cases.\n\u2022 Develop the custom map-reduce programs to get the semi-structured data to structured data and to incorporate all the business transformations.\n\u2022 Developing the process to move the output of map-reduce data to MarkLogic and HBase for analytics.\n\u2022 Used Informatica tool for cleaning, enhancing and protecting the data.\n\u2022 Design the new modules to support the market share project\n\u2022 Develop the market share project\n\u2022 Modify the existing process as part of change request or fixing the identified issues.\n\u2022 Participating the re-processing in case of any major changes.\n\u2022 Participated and implemented the security to anonymous the sensitive customer data.\n\u2022 Building data pipelines using Kafka and loading the data into HDFS.\n\u2022 Creating the Hbase tables and design HDFS data models to optimize the store.\n\u2022 Data enrichment project data by integrating with registrations or other telemetry data.\n\u2022 Building Spark Applications using Spark SQL, Streaming libraries.\n\u2022 Writing Scala and Python programs to build Spark Models.\n\u2022 Developing web scraping (python) programs to pull the data from retail stores.\n\u2022 Processing the scrapped data to perform the sentiment analysis of a printer and cartridge.\n\nEnvironment: Hadoop, Hive, Kafka, Spark 1.6.0, Spark SQL, Spark Streaming, Java, Python, Scala, MySQL, HBase, Oracle, Informatica"", u""BigData Developer, BI Data Modeler\nTravelClick - Hyderabad, Telangana\nNovember 2010 to December 2013\nHyderabad, India 11/2010 - 12/2013\nProject Type\nTravelClick provides innovative solutions for hotels around the globe that increase\nrevenue, reduce cost, and improve performance. TravelClick collects terabytes of raw data including 10 billion hotel rates. Project involved migrating data from\nPostgresSQL, SQL Server Database and various other sources to Hadoop to perform analysis to drive hotel marketing, revenue management, and business strategy.\n\nRole \u2022 BigData Developer, BI Data Modeler\nResponsibilities\n\u2022 Obtain requirements from business SME's, documentation of system requirements, create data models, reporting specifications and test plans/cases.\n\u2022 Work with off shore and near shore developers (India, Brazil) to communicate requirements in the form of design documents.\n\u2022 Develop project schedule for the reporting track, estimates and work break down structures.\n\u2022 Develop the project framework consisting of the ad-hoc environment and the objects that are leveraged for reports.\n\u2022 Develop the ETL framework and mapping sheets.\n\u2022 Perform testing of the deliverables to ensure conformance to requirements and design.\n\u2022 Perform status of deliverables to team, business and management.\n\u2022 Developing all the architecture documents like Application design document, System Design Document\n\u2022 Creating the tables, Index, sequences, Procedures, Packages, Views, Materialized views, Partitions and Performance tuning, AWR report, analyzing the tables, collecting the statistics\n\u2022 Establish the HADOOP cluster to archive the historical data to Hadoop Cluster and helping the analyst to SQL through Hive. Writing the java map-reduce programs to get the aggregated data for data warehouse.\n\u2022 POC on the Vehicle testing video data to the MongoDB and catalog data to MongoDB for web services.\n\nEnvironment: Hadoop, Java, MongoDB, Hive, SQL, MapReduce.""]","[u'M.S in Computer Engineering', u'B.S in Electronics Engineering']","[u'San Jose State University\nAugust 2015 to May 2017', u'KL University\nJuly 2006 to May 2010']"
0,https://resumes.indeed.com/resume/e7cd0e8efd24c1a5,"[u'Data Analyst/Developer\nMediachoice\nFebruary 2017 to Present\n\u2022 Extensive Data Analysis using Advanced Excel and SQL.\n\u2022 Data Analysis for Billboards related to traffic count on Google Earth/ Google Maps.\n\u2022 Excel vlookup, pivot tables, formulas along with filtering of large data sets and cleansing the data.\n\u2022 Report Dashboards, Tableau Visualization of data trends from past years.\n\u2022 Design/Develop Wordpress websites for clients with advanced HTML and CSS.', u'Web Developer\nVihik Cabs\nSeptember 2017 to December 2017\n\u2022 Worked on a website built in AngularJS and HTML5..\n\u2022 Designed, built and enhanced website main elements.\n\u2022 Experience working with integration of code with Bitbucket.\n\u2022 Experience working remotely for a fastest growing startup.', u'Software Engineer\nCDK Global\nJanuary 2016 to August 2017\n\u2022 Development and Testing of internal enterprise application tools to deliver to client facing products.\n\u2022 Experience working on build and deployment process of various applications using Java, Javascript, SQL, Bamboo.\n\u2022 Hands on experience with full software development life cycle process from dev, staging to production environments.\n\u2022 Automation Testing for internal applications using Java, Ruby.\n\u2022 Writing Test plans, test strategies and executing test cases.\n\u2022 Enhancing applications and fixing bugs continuously for the existing tools.\n\u2022 Involvement in several different projects with experience in pair programming.\n\u2022 Experience working in Agile Development.\n\u2022 Experience with version control systems like Git and Subversion.\n\u2022 Experience working in DevOps environment, Business Analysis and Requirement Gathering, Quality Control and Quality Assurance.\n\u2022 Experience working as a scrum master.', u'Software Engineer Intern\nCDK Global\nJune 2015 to December 2015\n\u2022 Internal application bug fixes, enhancements, reporting and documenting in a Agile environment.\n\u2022 Worked with team through entire process of Software Development Life Cycle including analysis and design, business and functional requirements analysis, data analysis, system design, development, testing and implementation.\n\u2022 Designed, built and tested the core components of the platform and applications.']","[u""Master's in Computer Science""]","[u'Texas State University San Marcos, TX\nJune 2014 to December 2015']"
0,https://resumes.indeed.com/resume/d9496f2896a9f280,"[u'Data Center Operating Engineer\nDigital Realty Data Center - Phoenix, AZ\nMay 2012 to Present', u'E-Bay Data Center - Phoenix, AZ\nDecember 2010 to January 2012', u""Data Center Engineer\nJohnson Controls at Charles Schwab Data Centers - Phoenix, AZ\nJuly 1998 to December 2010\n- Responsible for the daily operation of the data center facility which include electrical and mechanical systems, emergency generator backup systems, chillers, cooling towers, life safety systems and BMS controls.\n- Responsible for the operation, monitoring and maintenance of emergency generators, electrical switchgears, UPS systems, ASTSs, PDUs, VFDs and BMS.\n- Monitoring and maintenance of HVAC systems such as chillers, pumps, cooling towers CRAC's, and CRAH's.\n- Monitoring and PM of Life Safety Fire Alarm and Pre-Action Systems.\n- Worked on and experience with Andover BMS System and ALC BMS system.\n- Experience with Trane and York centrifugal chillers, air cooled chillers, DX cooling and complete central plant operations.\n- Responsible for planning, implementing, coordinating and supervising electrical operations, maintenance and energy management programs for critical environment data centers.\n- Strong understanding of electrical engineering design and operations of electrical systems.\n- Working experience with Outlook, Excel, Word and Power Popint."", u'Data Center Engineer\nFebruary 1994 to July 1998\nat Vanguard facility in Pennsylvania.', u'Electrical Field Engineer\nBechtel Engineering Inc\nJanuary 1983 to January 1994\nWorked as electrical field engineer during the construction of nuclear power plants Palo Verde Nuclear Power Plant in Phoenix, AZ and Limerick Nuclear Plant in Pottstown, PA.']",[],[]
0,https://resumes.indeed.com/resume/c711f380567f6cab,"[u'Full Stack Engineer / Data Engineer\nYouland\nSeptember 2016 to Present\nDevelop a real estate loan application platform using MEAN stack (MongoDB, Express, Angular, Node) with REST API\n\nbackend and Dockerize the web service on AWS both in China and US.\n\u2022 Deploy server containers under Nginx, achieving load balancing and reverse proxy features.\n\n\u2022 Use Redis and Dropbox as cache layer and CDN to improve the server performance and Dockerize the server app for\n\nscaling up server to multiple.\n\u2022 Develop an online underwriting system to help underwriting team to distribute tasks and complete the loan process.\n\n\u2022 Develop the legal files system, reducing the work load of legal files from hours to minutes.\n\n\u2022 Use Flask as the back-end to support trading service, AI consultant service and auto-investing service with REST API.\n\n\u2022 Build big data pipeline and machine learning system to deliver property valuation service by using SMACK stack (Spark,\n\nMesos, Akka, Cassandra, Kafka ) and crawler.\n\u2022 Build a micro CRM system with function of automatic customer service and campaign to support sales team. Get several\n\nleads per day and convert to 3-4 loans every week.\n\nTECHNICAL EXPERIENCE', u'MongoDB, Express, Angular, Node\nAugust 2016 to September 2016\nDevelop a full stack single page web app using MEAN stack (MongoDB, Express, Angular, Node) to deliver URL shorten\n\nservice with REST API backend.\n\u2022 Use Redis as cache layer over MongoDB to improve the server performance and Dockerize the server app for scaling\n\nup server to multiple.\n\u2022 Deploy server containers under Nginx, achieving load balancing and reverse proxy features.\n\n\u2022 Implemented LRU and Preload module for system speeding up.\n\n\u2022 Implemented Synchronize Statistic module for short URL statistic analyze based on Web Socket.\n\n\u2022 Develop emoji URL module to support service of generating emoji version of short URL.', u'Personal Heart-care\nMay 2016 to June 2016\nDevelop a full stack web service using MEAN stack (MongoDB, Express, Angular, Node) to provide Personal Heart-care\n\nInformation Recommendation service with REST API backend.\n\u2022 Use Kafka as the data streaming pipeline to process the data from web crawler.\n\n\u2022 Use Spark streaming to train the Collaborative Filtering model and generate recommendations for users based on their\n\npersonal health information.\n\u2022 Use Mesos for resource management and computation scheduling across AWS platform.\n\n\u2022 Use Tensorflow to train pre-gathered data and classify information into different categories for cold start.\n\nElastic distributed database May 2016 - July 2016\n\u2022 Develop an elastic distributed database using MySQL replication and used it to serve as he backend for a multi-tier web\n\napplication running TPC-W benchmark.\n\u2022 Develop a load balancer to route the browsing queries to master and route the ordering queries to slaves in a round-\n\nrobin manner.\n\u2022 Guarantee the high availability (HA) of the database in the real-time using backup salves.\n\n\u2022 Scale out/in the database server according to service level agreement when real-time workload changes.\n\n\u2022 Monitor the workload/database/operating system and demonstrate metrics using CanvasJS.\n\nLANGUAGES AND TECHNOLOGIES\nProgramming: Python; C++; Java; JavaScript; Scala; PHP; HTML; SQL; XML Schema; JSON; Android; Tensorflow\nWeb Development: Express.js; Angular.js; React; Node.js; Flask; Bootstrap; REST API;\nCloud Computing and Database: MongoDB; Cassandra; Redits; Solr; MySQL; Spark; Kafka; Mesos; Docker; AWS']","[u'M.S.E. in Computer Science & Engineering in Computer Science & Engineering', u'Master of Engineering in Engineering', u'Bachelor of Engineering in Engineering']","[u'University at Buffalo, SUNY\nSeptember 2015 to February 2017', u'JiangSu University\nSeptember 2012 to May 2015', u'JiangSu University of science and technology\nSeptember 2008 to May 2012']"
0,https://resumes.indeed.com/resume/766d77f50c2db464,"[u'Research Moderator\nInteractive Text-Mining Suite (ITMS)\nJanuary 2017 to Present\nCreated a visually stimulating and user friendly web-application to help researchers analyze content and find research trends from scholarly articles and digital collections.\n\xa7 Applied text analysis theories like topic modelling, linear discriminant analysis (LDA), frequency analysis, diachronic\nanalysis and sentiment analysis using R software and Shiny web application.\n\xa7 Utilized Plotly, Google Vis, Tensor flow, N-gram, spatial & temporal visualization for various visualization theories.', u'Sales Data Analyst\nCardinal Spirits - Bloomington, IN\nJune 2017 to August 2017\nDeveloped a comprehensive customer relationship management (CRM) tool capable of managing customer relationships,\ninventory and supporting sales analytics.\n\xa7 Built a NoSQL database using MongoDB to digitalize the database system that improved document retrieval.\n\xa7 Used R plotting technique and Tableau for generating sales insights. Recommended new growth opportunities in a couple of\nMidwestern states by harnessing economy of scale and other proprietary insights.', u""Software Engineer\nTATA Consultancy & Services (TCS)\nSeptember 2011 to May 2015\nProvided maintenance to a high performance and highly-available website capable of handling near real-time transactions,\nleading to an enriched banking experience for over 12 million Citibank customers all over US.\n\xa7 Acted as the liaison between the client (business customers) and the technology team to communicate business problems,\nsolutions strategy and regular status updates.\n\xa7 Explored Tera-bytes of test data to recognize flaw patterns and suggested solutions to improve performance. Implementation\nyielded success rate of 80% or higher at all times.\n\xa7 Managed and presented team's analysis results for major project milestones to upper management and clients.""]","[u'MS', u'B. Tech in Electrical Engineering']","[u'Indiana University\nAugust 2016 to May 2018', u""Sikhsha O' Anusandhan University Odisha, IN\nAugust 2007 to June 2011""]"
0,https://resumes.indeed.com/resume/21dcb442b46417e4,"[u'Data Science Capstone Intern\nFoursquare - New York, NY\nFebruary 2018 to Present\nBuilding a real-time recommendation app based on live foot traffic, off-trending analysis, and Natural Language Processing', u'Solutions Engineer Intern\nTransPerfect - New York, NY\nJune 2017 to December 2017\n\u2022 Implemented interactive and web-responsive dashboards based on client requirement analysis\n\u2022 Defined KPIs from a selection of important business metrics using Jaspersoft Studio\n\u2022 Applied text mining on historical Help Desk data to understand the polarity of user issues for process improvement\n\u2022 Designed and maintained SQL scripts for Data Extraction and Analysis on client demand\n\u2022 Evaluated Business Requirements Documents to generate Audit Trails and business status reports using MS SQL Server', u'Senior Systems Engineer\nInfosys Ltd - Bengaluru, Karnataka\nSeptember 2013 to April 2016\n\u2022 Developed interactive dashboards using Oracle BI tool (OBIEE) by effective selection of metrics for business understanding\n\u2022 Automated the weekly status reports of Oracle Apex tickets to track team performance and analyze workflow glitches\n\u2022 Administered the Build Management team of seven within the project to conduct environment builds\n\u2022 Constructed build plans, runbooks, and Schedule of Events(SOEs) to effectively translate client requirements']","[u'Master of Science in Information Systems', u'Bachelor of Technology in Electronics and Communications Engineering']","[u'New York University, Stern School of Business and Courant Institute of Mathematical Sciences New York, NY\nMay 2018', u'Jaypee University of Information Technology Solan, Himachal Pradesh\nMay 2013']"
0,https://resumes.indeed.com/resume/dcc92839c21d9055,"[u'Software Engineer\nApplied Physics Laboratory - Laurel, MD\nJanuary 2001 to January 2010\nDesigner and coder for the updated Cabrilla Power and Polarization Collection system. This project involves the conversion of legacy Ada code to object oriented C++, Analysis and design using UML, and a strong network programming foundation.\n\nPerformed research on the Linux 2.6 kernel and LynxOS 4 operating system to determine how well each OS handles real-time applications.\n\nProvided Unix Lab account and technical assistance to Johns Hopkins University students.\n\nCurrent member of the following discussion groups at APL: A2E UML Users Group, and the Personal Software Process Study Group.\n\nSelf-employed computer contractor.', u'Computer Troubleshooter and Data Entry Clerk\nThe Riverside Psychotherapy Associates - Metairie, LA\nJanuary 2000 to January 2001\nMetairie, LA, 2000-2001 Wrote a Visual Basic 6 program for a psychologist to speed up creating analysis reports for patients.\n\nComputer Troubleshooter and Data Entry Clerk', u'Data Entry\nGreen Marine - Metairie, LA\nJanuary 1997 to January 1998\nMade sure the software and peripherals on the computers were up-to- date and working properly. Entered information in the company database.']",[u'B.S. in Computer Science'],"[u'Loyola University New Orleans, LA\nJanuary 1997 to January 2001']"
0,https://resumes.indeed.com/resume/4ec8b39da42da4c8,"[u'Senior Data Modeler\nULINE - Pleasant Prairie, WI\nJanuary 2018 to Present\nProject Description: ULINE EDW\nData Modeler for ULINE Enterprise Data Warehouse. Designed Proof of Concept for Sales/Invoice and Quote datamart.\n\nResponsibilities:\n\u2022 Requirement Gathering, Understanding existing reporting system from ULINE OLTP systems.\n\u2022 Dimensional Modeling (Conceptual, Logical & Physical) for new Enterprise Data Warehouse.\n\u2022 Plan Project road map.\n\u2022 Involved in ETL and BI tool selection.\n\u2022 Defining Functional and Nonfunctional requirements.\n\u2022 Work with Data Steward for data standardization.\n\u2022 Agile Methodology work environment.\n\nEnvironment: SQL Server, DB2 LUW, ERWin, Netezza, Aqua data studio, TFS, Confluence.', u'Data Warehouse Architect\nHealth and Human Services Commission - Austin, TX\nAugust 2017 to January 2018\nProject Description: (MBOW) Mental and Behavioral Health Outpatient Warehouse\n\nMBOW is an Enterprise Data Warehouse for Special Ability Clients for the State of Texas. MBOW allows for extensive monitoring of data for decision support group for the Consumer Analysis Data for State and Local Authorities. MBOW data warehouse offers Consumer Analysis for Quality of services, Analysis of Cost information, Auditing Encounters, General Warehouse Information, Performance Measures and Continuity of Care.\n\nResponsibilities:\n\u2022 Dimensional Modeling (Logical & Physical) for new proposed changes.\n\u2022 Maintenance and Support for MBOW.\n\u2022 Health and Human Services Informatica 10.x Server Administration.\n\u2022 Health and Human Services Business Objects XI 4.2 Server Administration.\n\u2022 Analyze new requirements for Changing Performance Measures for existing Performance Measures.\n\u2022 Maintenance and updating Informatica ETL for MBOW and CMBHS Data Warehouse.\n\u2022 Maintenance and updating Business Objects reports for MBOW.\n\u2022 Manage, Maintain and Support MBOW database and Unix Scripts.\n\nEnvironment: Informatica 10.1.1, Business Objects 4.2, Oracle 11-12c, SQL Developer, Unix, Shell Scripts.', u'Database Administrator\nTexas Department of State Health Services - Austin, TX\nMarch 2017 to August 2017\nProject Description: (MHI) Mental Health Integration\nMHI Project is an effort to modernize Client Assignment and Registration (CARE) system by upgrading and transforming functionality on Modern System called Client Management for Behavioral Health Services (CMBHS). State Hospital IT department follows Agile Methodology.\n\nResponsibilities:\n\u2022 Analysis, Design & Deployment of new OLTP functionality for CMBHS in Agile Methodology.\n\u2022 OLTP Logical and Physical Data modelling.\n\u2022 Populate Database deployment scripts.\n\u2022 Database Administration activities.\n\u2022 Analysis and designing of new Dimensional model for MBOW data warehouse.\n\u2022 Coaching developers for Datamodel and technologies for Development effort.\n\nEnvironment: Oracle 11, SQL Developer, SQL Developer Data Modeler, Microsoft TFS.', u""Data Warehouse Architect 3\nDepartment of Aging & Disability Services - Austin, TX\nJanuary 2006 to March 2017\nSME for EWS Datamart, CDR, MDS 3.0 QM, QRS, Empdx systems in DADS IT.\n\nProject Description: (QRS) Quality Reporting System Jan 14 - Mar 17\n\nQRS is a public facing website which provides information for all types of long term care facilities of Texas. Besides basic information, QRS provides critical quality information for each facility that can help residents of Texas to choose and meet family member's need.\n\nResponsibilities:\n\u2022 Maintenance and Support.\n\u2022 Research and communication with Program customers.\n\u2022 Incorporate new changes and enhancements.\n\u2022 Communicate fixes to Development team.\n\nEnvironment: Unix server, Unix, Perl, Oracle 10g, Sybase 15.x, Rapid SQL, Informatica 9.x, Power Designer, Opcon/Cron."", u'Data Warehouse Architect\nDepartment of Aging & Disability Services - Austin, TX\nJanuary 2006 to March 2017\nSME for EWS Datamart, CDR, MDS 3.0 QM, QRS, Empdx systems in DADS IT.\n\nProject Description: (QRS) Quality Reporting System Jan 14 \u2013 Mar 17\n\nQRS is a public facing website which provides information for all types of long term care facilities of Texas. Besides basic information, QRS provides critical quality information for each facility that can help residents of Texas to choose and meet family member\u2019s need.\n\nResponsibilities:\n\u2022 Maintenance and Support.\n\u2022 Research and communication with Program customers.\n\u2022 Incorporate new changes and enhancements.\n\u2022 Communicate fixes to Development team.\n\nEnvironment: Unix server, Unix, Perl, Oracle 10g, Sybase 15.x, Rapid SQL, Informatica 9.x, Power Designer, Opcon/Cron.\n\n\nProject Description: (MDS QM) Minimum Data Quality Measures Aug 12 \u2013 Mar 17\n\nMDS QM is a critical system to rate health facilities. Based on QM computation Quality Reporting System website rates Texas facilities.\n\nResponsibilities:\n\n\u2022 Requirement gathering & analysis.\n\u2022 Prepared project plan and documentation.\n\u2022 Dimensional model & deployment. Created database deployment scripts.\n\u2022 Developed perl programs, Database Package and Informatica process for MDS QM computation.\n\u2022 Changes on CDR for new QM computation.\n\u2022 Communications with QRS developers.\n\u2022 Designed and developed deployment scripts.\n\u2022 Communication effort to team for the process Life Cycle.\n\nEnvironment: Windows server, Unix, Perl 5.x, Oracle 10g, SQL Server, Rapid SQL, Informatica 9.x, Sybase 12.5, Power Designer, Opcon, MS Visio, IBM VPN client.\n\n\nProject Description: Employment data exchange Sept 13 \u2013 Mar 17\n\nAn automated system to coordinate a secure exchange of employment data between DADS, DARS and TWC.\n\nResponsibilities:\n\u2022 Requirement gathering & analysis.\n\u2022 Prepared project plan and documentation.\n\u2022 Design Dimensional model & deployment. Created database deployment scripts.\n\u2022 Developed Informatica mapping to combine MBOW, TWC, DARS and DADS data to populate employment data.\n\u2022 Communications with DARS and TWC teams in regards to data.\n\u2022 Designed and developed deployment scripts.\n\u2022 Communication effort to team for the process Life Cycle.\n\nEnvironment: Unix, Perl 5.x, Oracle 10g, Rapid SQL, Informatica 9.x, Power Designer, Opcon, MS Visio.\n\nProject Description: (CDR) Central Data Repository Jan 10 \u2013 Mar 17\n\nCentral Data Repository is a warehouse which provides all types of Long Term Care Providers data. CDR mainly used by DADS Regulatory for various purposes. It also feeds all LTC data into public facing QRS website which displays information of all facilities in Texas including rating.\n\nResponsibilities:\n\u2022 Requirement Gathering.\n\u2022 Dimensional logical and physical Modeling.\n\u2022 Informatica ETL Development, Test & Deployment.\n\u2022 Maintenance and Support.\n\u2022 Research and communication with Program customers.\n\u2022 Incorporate new changes and enhancements.\n\u2022 Communicate fixes to Development team.\n\u2022 Involved with HHSC EDW Data Governance Project and provided client information.\n\nEnvironment: Unix server, Unix, Perl, Sybase 9.x-15.x, Rapid SQL, Informatica 9.x, Power Designer, Opcon/Cron.\n\n\nProject Description: (MDS) Minimum Data Set migration Aug 10 \u2013 Mar 17\n\nMDS is part of the U.S. federally mandated process for clinical assessment of all residents in Medicare of Medicaid certified nursing homes.\n\nResponsibilities:\n\u2022 Involved into migration from MDS 2.0 to MDS 3.0.\n\u2022 Review and Analyze MDS 2.0 environment to identify the needs to create MDS 3.0 environment.\n\u2022 Proposed a new extract flow for daily and monthly extract.\n\u2022 Developed the specification documents, crosswalk document and new extract file structure for MDS 3.0.\n\u2022 Designed new database for MDS 3.0.\n\u2022 Developed new process for MDS 3.0 extract.\n\u2022 Review, analyzed and provided solution to change required for existing dependent processes for other systems.\n\u2022 Designed and developed deployment scripts.\n\nEnvironment: Windows server, Perl scripts, Unix Shell Scripts, Oracle 10g, SQL Server, Rapid SQL 7, MS Visio, IBM VPN client.\n\n\n\nProject Description: (EWS) Early Warning System Jan 06 \u2013 Mar 17\n\nThe Project is to migrate the existing Data Warehouse on Early Warning System from Sybase to Oracle, the future enhancements, maintenance & support. EWS datamart stores the long term care data for Nursing Facility, ICFMR & Home health facility from Texas State.\n\nResponsibilities:\n\u2022 Involved into full SDLC.\n\u2022 Review Detailed High Level Design Documents and knowledge sharing sessions with the team.\n\u2022 Understood Business Users requirements. Involved in Requirement Gathering for migration.\n\u2022 Designed data models and implemented star and snowflake schemas using Erwin & Power Designer. Reverser engineering of Sybase model and forward logical and physical model for Oracle database.\n\u2022 Migrated the data models required for the data warehouse using Erwin and transferred from Sybase to Oracle.\n\u2022 Used Informatica as the ETL tool.\n\u2022 Migration of Cognos catalog on sybase to BOXI universe on Oracle.\n\u2022 Developed the solutions for ongoing enhancements using stored procedures, Informatica & BOXI.\n\u2022 Testing for BOXI universe and reports with the customer.\n\u2022 Maintained the existing system with monitoring and supporting weekly/monthly loads.\n\u2022 HIPPA training with DADS.\n\u2022 Developed & maintained in house fast paced projects as HCSSA load & CDR loads using Informatica.\n\u2022 Designed and developed deployment scripts.\n\u2022 Communication effort to team for the process Life Cycle.\n\nEnvironment: Informatica Power Center 7-10, Oracle 9i-10g-12c, Sybase 12c, ERWIN 4, Cognos 6 , BOXI 3.1, Power designer 12, Rapid SQL 7, MS Visio, Windows XP, UNIX, Shell scripts, Opcon.', u'Data Warehouse Consultant\nDepartment of Public Welfare - Harrisburg, PA\nMarch 2005 to September 2005\nProject Description:\nThe Project is to develop Datamart on ChildLine system. ChildLine System is the central register for all investigated reports of child abuse in Pennsylvania.\n\nResponsibilities:\n\u2022 Involved into full SDLC.\n\u2022 Review Detailed High Level Design Documents and knowledge sharing sessions with the team.\n\u2022 Involved in Requirement Gathering sessions with the business users.\n\u2022 Involved in the Meta Data Specs creation.\n\u2022 Test loads from SQL Server Operational Data Source into Oracle sample tables using Informatica Power Center 6.2.1.\n\u2022 Unit testing of ETL Work.\n\u2022 Cognos report team work with developers to support the user requirements.\n\u2022 HIPPA training by DPW.\n\nEnvironment: Informatica Power Center 6.2, Oracle 9i, SQL Server 2000, ERWIN 4, Cognos 6 , MS Visio, Toad 6, Windows 2000 Server, UNIX.', u'Sr. Consultant\nSBC Communications Inc - Schaumburg, IL\nJuly 2003 to January 2005\nProject Description:\nThe Project is AADM (Affiliate Adjustment of Data Management). Developed the Data Warehouse application to analyze region wise sales compensation against product and customer. In this project, data extraction from various legacy systems and loaded into the warehouse tables after data enrichment.\n\nResponsibilities:\n\u2022 Involved into full SDLC.\n\u2022 Involved to design Drafted schema and participated in Designing logical and physical data model using ERWIN.\n\u2022 Test of workflows for - AMR 2 (Affiliate Metrics Report), which is a part of AADM.\n\u2022 ETL Development of Product Ref Load and Daily Load to Oracle DB.\n\u2022 Performance Tuning to improve your mapping performance to get efficient results in time.\n\u2022 Business Object Universe, Business Objects Reports and Business Objects repository extensively used, and new universe were designed and developed to support the user requirements.\n\u2022 Created complex views to overcome loops and contexts.\n\u2022 WebI Reports were created and deployed.\n\nEnvironment: Informatica Power Center 7.1, db2 UDB EEE 7.1, Oracle 9i, Business Object 6.5, ERWIN 4.1, Unix.', u'Informatica Developer\nGenuity Inc - Woburn, MA\nMarch 2003 to June 2003\nProject Description:\nThe data mart was built using Star Schema data model to look at the invoice amounts from various dimensions. Genuity had all sales based data migrated from legacy system to the data mart into Data warehouse using Informatica.\n\nResponsibilities:\n\u2022 ETL using Informatica to load data from Flat file, MS SQL, Oracle to ORACLE\n\u2022 Performed complex queries involving large data volumes.\n\u2022 Modified the Informatica components, parameters, utilize data parallelism and thereby improve the overall performance to fine-tune the execution times.\n\u2022 Tested all the business application rules and monitored sessions using Informatica Server Manager\n\u2022 Developed and tested ETL applications for the warehouse application according to functional specifications using Informatica.\n\nEnvironment: Informatica Power Mart 5.1, Oracle 8i, SQL Server, Erwin 4.0, Cognos', u'Software Engineer\nKIT Solutions Pvt. Ltd\nOctober 2002 to January 2003\nProject Description:\nCCSM is a Customer Care Service module for online shopping website for after sales support. Primary responsibility to develop front-end application for customer care service using J2EE: JSP/Servlets, Java Scripts with MySQL.\n\nEnvironment:\nOperating System: Linux 7.2.\nLanguages & technology: Java Servlet, JSP, JavaScript, HTML, XML.\nDatabase: mySQL.\nTools: Apache-Tomcat Web server, Macromedia Dreamweaver.', u'Software Engineer\nKIT Solutions Pvt. Ltd\nAugust 2002 to November 2002\nProject Description:\nSwitched over the windows 98 Network to Linux 7. Managed 24 computers on 3 hubs. Mail Server, Chat Server Installation and configuration.', u'Software Engineer\nKIT Solutions Pvt. Ltd\nJanuary 2002 to July 2002\nProject Description:\nE-MailAddress Sync was developed for Zydus-Cadila Healthcare Ltd. Development environment under Linux with Java Servlet, JNDI, and LDAP & SENDMAIL Linux Mail Server.\nEnvironment:\nOperating System: Linux 6.\nLanguages & technology: Java Servlet, JSP, JavaScript, HTML.\nTools: Sendmail Mail Server, Apache-Tomcat Web server, LDAP Server and Macromedia Dreamweaver, XML.']",[u'Master of Computer Applications in Computer Applications'],"[u'University of Pune Pune, Maharashtra']"
0,https://resumes.indeed.com/resume/8797952a427d5bbb,"[u'Software Engineer Associate (Data)\nKannact - Portland, OR\nAugust 2016 to Present\nSpearheaded 90% of the work related to creating specialized reports using reportlab\n\u25cf Performed Data Cleaning to remove unwanted data and reduced the data 10 times\n\u25cf Performed Data Wrangling and converted 67% of the data to favour analysis\n\u25cf Created several visualizations using Diabetes and BloodPressure data\n\u25cf Helped in creating REST API using java and swagger', u'Software Engineer\nJayabheri Properties - Hyderabad, Telangana\nJune 2012 to July 2014\nHyderabad, India\n\u25cf Helped in digitizing their records 90%\n\u25cf Designed & Developed code in Java to extract and load data from MySQL database\n\u25cf Used Test driven development (TDD) for developing services required for the application\n\u25cf Used GIT for version control']",[u'M.S. in Computer Science'],"[u'Rochester Institute of technology Rochester, NY\nAugust 2014 to December 2016']"
0,https://resumes.indeed.com/resume/bd2a4cd8c8e882bf,"[u'Data Analyst Intern\nIntel Corporation - Chandler, AZ\nJanuary 2018 to Present\n\u2022 Currently working on Intel assembly testing manufacturing capstone project with an objective to optimize the production scheduling of the bottleneck stations and minimize the machine conversion frequency and the total conversion time', u'Business Analyst\nFishbowl India Private Ltd - Noida, Uttar Pradesh\nAugust 2016 to July 2017\n\u2022 Performed Guest Analytics to evaluate the dining tendencies of different demographic groups of customers leading to 2.7% redemption rate compared to the average 1.18% for company-wide promotions, and a full $5 higher average check\n\u2022 Built a tiered pricing system for a QSR chain client that resulted in a profitable and lower-risk approach to pricing and allowed for additional flexibility for future price initiatives resulting in 100% flow through of price increase to gross profit\n\u2022 Resolved the issue of fraudulent redemptions of coupons at point of sales in three weeks by analyzing customer\u2019s historical data and resulting in an estimated 2 million of fraud avoidance\n\u2022 Designed Pig scripts to check customer churn and built a smarter coupon strategy leading to 366% return on the discount that drove the promotion without cannibalizing existing business\n\u2022 Delivered client ready presentation to senior leadership and statistical/data driven insight to non-quantitative audiences', u'System Engineer\nTata Consultancy Services Ltd - Pune, Maharashtra\nMarch 2013 to August 2016\n\u2022 Successfully built prediction model using turbine and steam sensor data for GE Power & Water client to predict future maintenance and to provide better operating efficiency thus saving 20% of the operational cost\n\u2022 Designed data warehouse(OLAP) solutions for enterprise data across multiple source systems and flat files\n\u2022 Wrote and deployed many cloud based applications on GE Predix platform\n\u2022 Developed code fixes and enhancements for inclusion in future code releases and patches\n\u2022 Discussed and analyzed business requirements with end users to understand their critical ask/scenario, reporting necessities and user interface filters']","[u'Master of Science in Business Analytics', u'Bachelor of Technology in Electronics and Communication']","[u'W. P. Carey School of Business at Arizona State University Tempe, AZ\nMay 2018', u'Punjab Technical University Jalandhar, Punjab\nJune 2012']"
0,https://resumes.indeed.com/resume/40ad74d55a427922,"[u""Big Data Engineer\nAT&T - Richardson, TX\nFebruary 2017 to Present\nResponsibilities:\n\u2022 Extensively involved in Design phase and delivered Design documents in Hadoop eco system with HDFS, HIVE, PIG, SQOOP and SPARK with SCALA.\n\u2022 Collected the logs from the physical machines and the Open Stack controller and integrated into HDFS using Kafka.\n\u2022 Involved in the high-level design of the Hadoop architecture for the existing data structure and Business process\n\u2022 Part of Configuring & deployment of Hadoop Cluster in the AWS cloud.\n\u2022 Worked with clients to better understand their reporting and dash boarding needs and present solutions using structured Agile project methodology approach.\n\u2022 Worked on analyzing Hadoop cluster and different Big Data Components including Pig, Hive, Spark, HBase, Kafka, Elastic Search, database and SQOOP.\n\u2022 Involved in loading disparate datasets into Hadoop Data Lake, this would be available to the data science team to predict the future.\n\u2022 Implemented Partitioning, Dynamic Partitions and Buckets in HIVE for increasing performance benefit and helping in organizing data in a logical fashion.\n\u2022 Installed Hadoop, Map Reduce, HDFS, and developed multiple Map-Reduce jobs in PIG and Hive for data cleaning and pre-processing.\n\u2022 Created tables in HBase to store variable data formats of PII data coming from different portfolios\n\u2022 Worked on Sequence files, RC files, Map side joins, bucketing, partitioning forHive performance enhancement and storage improvement.\n\u2022 Experienced in pulling the data from Amazon S3 bucket to Data Lake and builtHive tables on top of it and created data frames in Spark to perform further analysis.\n\u2022 Used cloud computing on the multi-node cluster and deployed Hadoop application on cloud S3 and used Elastic Map Reduce (EMR) to run aMapReduce.\n\u2022 Explored MLlib algorithms in Spark to understand the possible Machine Learning functionalities that can be used for use case.\n\u2022 In preprocessing phase of data extraction, we used Spark to remove all the missing data for transforming of data to create new features.\n\u2022 Developed data pipeline using Flume, Sqoop, Pig and Java map reduce to ingest customer behavioral data and financial histories into HDFS for analysis.\n\u2022 Involved in loading data from UNIX file system to HDFS using Flume and HDFSAPI.\n\u2022 Configured Spark Streaming to receive real time data from the Kafka and storethe stream data to HDFS.\n\u2022 Developed RDD's/Data Frames in Spark using Scala and Python and applied several transformation logics to load data from Hadoop Data Lake to Cassandra DB.\n\u2022 Exported the analyzed data to the NoSQL Database using HBase for visualization and to generate reports for the Business Intelligence team using SAS.\n\u2022 Used various HBase commands and generated different Datasets as per requirements and provided access to the data when required using grant and Revoke\n\u2022 Created Hive tables as per requirement as internal or external tables, intended for efficiency.\n\u2022 Developed MapReduce programs for the files generated by hive query processing to generate key, value pairs and upload the data to NoSQL database HBase.\n\u2022 Implemented installation and configuration of multi-node cluster on the cloud using Amazon Web Services (AWS) on EC2.\n\u2022 Created and maintained Technical documentation for launching Hadoop Clusters and for executing Hive queries and Pig Scripts\n\u2022 Worked in tuning Hive & Pig to improve performance and solved performance issues in both scripts\n\u2022 Worked with Elastic MapReduce (EMR) and setting up environments on Amazon AWS EC2 instances.\n\u2022 Developed various data connections from data source to SSIS, Tableau Server for report and dashboard development\n\u2022 Involved unit testing, interface testing, system testing and user acceptance testing of the workflow tool.\n\u2022 Used JIRA for bug tracking and GIT for version control.\n\u2022 Involved in story-driven agile development methodology and actively participated in daily scrum meetings.\n\nEnvironment: Apache Hadoop 3.0, AWS, MLlib, MYSQL, Kafka, HDFS 1.2, Hive 2.3, Pig0.17, MapReduce, Flume 1.8, Cloudera, Oozie, UNIX, Oracle 12c, Tableau 7, GIT, UNIX."", u""Big Data Engineer\nCross Commerce Media - Pleasanton, CA\nFebruary 2016 to January 2017\nResponsibilities:\n\u2022 Implemented POC by comparing SPARK with Hive on big data sets by performing aggregations and observing time responses\n\u2022 Worked with Business Analyst and helped representing the business domain details and prepared low level and high level documentation\n\u2022 Created Hive tables and created Sqoop jobs to import the data from Oracle to HDFS and scheduled them in Autosys by creating Oozie workflows\n\u2022 Designed and developed applications that work on AngularJS based UI and RestfulAPIs, Cassandra DB, AWS environment, security,\n\u2022 Import the data from different sources like HBASE into Spark RDD developed a data pipeline using Kafka and Storm to store data into HDFS.\n\u2022 Developed script which will Load the data into Spark RDD and do in memory data computation to generate the output response.\n\u2022 Involved in converting Hive into Spark transformations using Spark RDDs, Scala and have a good experience in using Spark-Shell and Spark Streaming.\n\u2022 Developed Spark streaming script which consumes topics from distributed messaging source Kafka and periodically pushes batch of data to Spark for real time processing.\n\u2022 Developed Spark programs, scripts and UDF's using Spark SQL for aggregative operations as per the requirement.\n\u2022 Used Spark Data Frame API to perform analytics on hive data and implemented various checkpoints on RDD's to disk to handle job failures and debugging.\n\u2022 Involved in executing various Oozie workflows and automating parallel Hadoop MapReduce jobs.\n\u2022 Enabled speedy reviews and first mover advantages by using Oozie to automate data loading into the HDFS and to run multiple Hive and Pig jobs.\n\u2022 Involved in converting Hive into Spark transformations using Spark SQL and Scala.\n\u2022 Automated workflows using shell scripts and Control-M jobs to pull data from various databases into Hadoop Data Lake.\n\u2022 Involved in story-driven agile development methodology and actively participated in daily scrum meetings.\n\u2022 Leveraging DevOps techniques and practices like Continuous Integration, Continuous Deployment, Test Automation, Build Automation and Test Driven Development to enable the rapid delivery of end user capabilities\n\nEnvironment: Map Reduce, HBase, HDFS, Hive, Pig, Java, Kafka, SQL, Hortonworks, Spark, Sqoop, Storm, Flume, AWS, Tableau, YARN, Oozie, Eclipse, Cloudera, Cassandra, Python, Scala, Shell Scripting, Hadoop, Oracle, UNIX, NoSQL."", u""Hadoop Developer\nCon Edison - New York, NY\nNovember 2014 to December 2015\nResponsibilities:\n\u2022 Involved in importing and exporting data between Hadoop data lake and Relational Systems like Oracle, Mysql, DB2, Informix and Teradata using Sqoop.\n\u2022 Involved in ingesting data into Cassandra and consuming the ingested data from Cassandra to Hadoop Data lake.\n\u2022 Involved in the process of Cassandra data modelling and building efficient data structures.\n\u2022 Worked on Creating Kafka topics, partitions, writing custom partitioner classes.\n\u2022 Imported Avro files using Apache Kafka and did some analytics using Spark in Scala.\n\u2022 Developed script which will Load the data into Spark RDD and do in memory data computation to generate the output response.\n\u2022 Involved in migrating map reduce jobs into RDD (Resilient data distributions) and create Spark jobs for better performance.\n\u2022 Developed Spark code using Scala and Spark-SQL/Streaming for faster testing and processing of data\n\u2022 Involved in collecting and aggregating large amounts of streaming data into HDFS using Flume.\n\u2022 Involved in executing various Oozie workflows and automating parallel Hadoop Mapreduce jobs.\n\u2022 Enhanced HIVE queries performance using TEZ for Customer Attribution datasets.\n\u2022 Involved in loading data from UNIX file system to HDFS and also responsible for writing generic scripts in UNIX.\n\u2022 Transferred and loaded datasets from Hive tables to Greenplum using Yaml.\n\u2022 Hands on experience in creating analytic, attribute and calculation views in SAP Hana.\n\u2022 Involved in using Solr Cloud implementation to provide real time search capabilities on the repository with tera bytes of data.\n\u2022 Involved in bug fixing and 24-7 production support running processes.\n\u2022 Working with vendor and client support teams to assist critical production issues based on SLA's.\n\u2022 Involved in restarting failed Hadoop jobs in production environment.\n\u2022 Created Talend Mappings to populate the data into dimensions and fact tables\n\u2022 Developed jobs to move inbound files to vendor server location based on monthly, weekly and daily frequency in Talend.\n\u2022 Involved in running queries using Impala and used BI tools to run ad-hoc queries directly on Hadoop.\n\u2022 Generated various marketing reports using Tableau with Hadoop as a source for data.\n\u2022 Also took active part as a Release Engineer in releasing the code to CERT and Production Environment.\n\nEnvironment: Pivotal, MapReduce, HDFS, Hive, Pig, Oozie, Cassandra, Sqoop, Apache Kafka, Storm, Impala, Linux, Talend, Tableau, Splunk, Solr, Jira, Confluence, GitHub, Bitbucket, Source tree, Jenkins."", u'Build and Release Engineer\nHID Pvt. Ltd\nJanuary 2014 to October 2014\nResponsibilities:\n\u2022 Release Engineer for a team that involved different development teams and multiple simultaneous software releases.\n\u2022 Designing and implementing for fully automated server build management, monitoring and deployment by Using DevOps Technologies like Chef.\n\u2022 Responsible for design and maintenance of the Subversion/GIT, Stash Repositories, views, and the access control strategies.\n\u2022 Used ANT and Python scripts to automate the Build and deployment process. Used maven for few modules.\n\u2022 DevOps for load balanced environments & Multi-regional server environments\n\u2022 Monitoring each service deployment, and validating the services across all environments.\n\u2022 Deployed J2EE applications to Application servers in an Agile continuous integration environment and also automated the whole process. Build scripts using ANT and MAVEN build tools in Jenkins, Sonar to move from one environment to other environments.\n\u2022 Involved in build and deploying SCA modules in IBM WebSphere Process server.\n\u2022 Worked on Java/J2ee deployments in web sphere.\n\u2022 Prepared Migration logs for every release and maintained the data accuracy.\n\u2022 Maintained Defect Fix Deployments and documented the deployed files in the appropriate Environment Migration log.\n\u2022 Working with Change Order with current release and implement them in the Production.\n\u2022 Created Branches, Tags for each release and particular environments.\n\u2022 Merged the branches after the Code Freeze.\n\u2022 Created the Deployment notes along with the Local SCM team and released the Deployment instructions to Application Support.\nEnvironment: Java/J2ee, Eclipse, Chef, Ant, Maven, Jenkins, GIT, Subversion, WebSphere Application Server (WAS), Apache, PERL, BASH, UNIX, Python.', u'Build and Release Engineer\nVays Infotech Pvt Ltd\nJune 2012 to December 2013\nResponsibilities:\n\u2022 Setting up continuous integration and formal builds using Bamboo with Artifactory repository\n\u2022 Involved in setting up JIRA as defect tracking system and configured various workflows, customizations and plugins for the JIRA bug/issue tracker\n\u2022 Integrated Maven with SVN to manage and deploy project related tags\n\u2022 Installed and administered Artifactory repository to deploy the artifacts generated by Maven and to store the dependent jars which are used during the build\n\u2022 Mentor Business Areas on Subversion Branching, Merging strategies concepts\n\u2022 Resolved update, merge and password authentication issues in Bamboo and JIRA\n\u2022 Involved partially in deploying WARs/EARs (backend) through WebLogic Application Server console\n\u2022 Performed setup of Clustered environment with WebLogic Application Server\n\u2022 Written WLST scripts to deploy the WAR/WAR files to the target WebLogic Server\n\u2022 Support Lead developers with Configuration Management issues\n\u2022 Worked for Creating the Software Configuration Management Plan\n\u2022 Managed all the bugs and changes into a production environment using the Jira tracking tool\n\u2022 Managed the entire release communication and Release co-ordination during the Project roll-out Involved in estimating the resources required for the project based on the requirements\n\nEnvironment: Java, Maven, Bamboo, Linux, WebLogic, Subversion, Shell scripting, WLST Scripting', u'Linux Administrator\nIDrive Software Private Limited\nSeptember 2011 to April 2012\nResponsibilities:\n\u2022 Primarily responsible for maintenance and support of Linux environments in DEV/QA, Staging and Production.\n\u2022 Eliminated shared administrative privileges among all members of IT and replaced with model using permissions delegated to additional administrative accounts\n\u2022 Developed strategic plan to reduce material inventories and create a manageable IT infrastructure.\n\u2022 Monitor and tune appropriate systems to ensure optimum levels of performance.\n\u2022 Implemented a Norton Ghost program that resulted in significant time savings for new system builds\n\u2022 Initiated and created an automated self-healing data monitoring and retrieval system for archived data from multiple remote site\n\u2022 Work on installation, configuration and maintenance Redhat, CentOS and Servers at multiple Data Centers.\n\u2022 Building the host using the Kick start.\n\u2022 Scanning the newly assigned LUNs to the serves and assigning them to volume group and increasing the file system using Red Hat volume manager\n\u2022 Perform scripting on Perl and shell for monitoring and scheduling the Jobs.\n\u2022 Installed and managed VERITAS Volume Manager 3.5 (VxVM) on Solaris 9.\n\u2022 Understanding of SAN and NAS storage and load balancing.\n\u2022 Troubleshooting of Logical Volume manager. Configuration of Red Hat Clustering.\n\u2022 Installation, Configuration and administration of DNS, LDAP, NFS, NIS, NIS+ and Sendmail on Redhat.\n\u2022 Administration and Configuration of Apache Web Server and SSL.\n\u2022 Created and maintained network users, user environment, directories, and security.\n\nEnvironment: RHEL5&6 Solaris 9, 10 &11, IBM AIX 4 & 5, DNS, DHCP, LDAP NFS, FTP, GIT, Sendmail, JIRA, Puppet, Apache Tomcat, Shell, Veritas Volume Manager']","[u""Master's""]",[u'']
0,https://resumes.indeed.com/resume/4d9f8af03b0a4b51,"[u'Data Engineer Intern\nThird Eye Data - Santa Clara, CA\nJune 2017 to August 2017\nJoined on the team of an ongoing project. Discussed with my mentor for some better optimal implementation of Java,\nSpark, AWS and Cassandra\n\u25cf Implemented the idea where spark ML libraries were used to run various models on the No-SQL data and do that using\nAWS and Cassandra to see if there is any change in performance.\n\u25cf Assisted in some problem solving for the ongoing project.', u'Software Engineer\nDesign Innova - New Delhi, Delhi\nAugust 2014 to November 2015\nDeveloped a queue management system that would fetch queue numbers and display them on the screen.\n\u25cf I worked on developing a front-end that would get that data in JSON format using Java and display that data on the screens\nusing HTML, Javascript and Jquery\n\u25cf Contributed to the architecture of software applications or systems.\n\u25cf Assisted the team in gathering and analyzing business and functional requirements, and translate requirements into technical specifications for robust, scalable, supportable solutions that work well within the overall system architecture.\n\u25cf Participated in the full development cycle, end-to-end, from conception, design, implementation and testing to documentation, delivery and maintenance.\n\u25cf Worked on various Data modeling techniques.', u'Software Engineer Intern\nAT&T - Noida, Uttar Pradesh\nMay 2013 to August 2013\nProject based on technologies such as Raspberry Pi, Java, Pi4J api and PHP/Javascript.\n\u25cf Creating a LAMP server in order to have the server record values from the raspberry pi and upload them on a website using\nPHP/Javascript and those values can be accessed from any device having connection to internet.']","[u'Master of Computer Science in Computer Science And Engineering', u'Bachelor of Computer Science in Computer Science', u'', u'in communication']","[u'University of Texas at Arlington Arlington, TX\nDecember 2017', u'Amity University Noida, Uttar Pradesh\nDecember 2014', u'Secondary sorting', u'fetch school data and session handling']"
0,https://resumes.indeed.com/resume/f8af9dac9552207b,"[u""Environmental/Chemical Engineer\nJM Smucker Company\nMay 2016 to Present\nMaintain water and air quality: both at the water and wastewater treatment plants, and flue gas\nfrom waste coffee combustion.\n\u2022 Ensure compliance with all regulatory requirements and record retention in the EMS\n(Environmental Management System).\n\u2022 Confirms adherence to the same water quality standards as any public water supply within the state as our finished product serves both our production operations and our work force's\ndomestic needs while on address.\n\u2022 Responsible for potable water quality at the wells and wastewater treatment plant, ensuring\nwater discharged to the river meets regulatory compliance.\n\u2022 Confirm that physical, chemical and biological treatments are conducted to ensure the effluent\nwater has been adequately treated and is of acceptable quality per State & Federal EPA\nregulations in order to be discharged to the city or stream.\n\u2022 Responsible for maintaining activities for wastewater treatment plant, supervise confined space\nentries and manage projects with contractors for work on the wastewater treatment plant.\n\u2022 Responsible for the Environmental Management Systems which ensures that ISO 14001\ncertification is maintained and the facility remains in compliance. Performs internal EMS\nAudits.\n\u2022 Support Facility Environmental Coordinators with environmental compliance issues including\nwastewater permitting, air permitting and storm water permitting, which entails report\npreparation and permit applications.\n\u2022 Perform facility water, energy and waste management reviews, project scope development, and project management\n\u2022 Review all capital improvement projects on address to ensure that there will not be detrimental\nenvironmental impacts. Works with Plant Engineer and General Manager when necessary for the development of any expansion plans.\n\u2022 Analyze projects to determine if there are potential impacts to the wastewater treatment facility\nor permits, as well as any long-term environmental concerns.\n\u2022 Represent J.M. Smucker in a positive and professional manner with internal and external\ncontacts. Examples of outside contacts include MDNR, EPA, Engineers and Contractors.\n\u2022 Maintains a strong working relationship with vendors and external contacts while accomplishing department goals.\n\u2022 Report EPCRA (Tier I & Tier II forms) and land application of wastewater bio-solids annually.\n\u2022 Projects Executed:\no Destabilization of coffee oil/gums in the wastewater stream.\no Phenols treatment/adsorption design.\no Vacuum filter design & replacement."", u'Data Analyst\nLaboratory Corporation of America - Chesapeake, VA\nJune 2010 to August 2015\nLaboratory Support\n\u2022 Prepared laboratory specimen for analysis and ensuring compliance with environmental and safety standards.\n\u2022 Performed cytological analysis on samples.\n\u2022 Mixed and prepared chemicals as needed.\n\u2022 Analyzed specimen using optical microscopy, electron microscopy, and x-ray diffraction to uncover and evaluate the evidence.\n\u2022 Photo document and compared results to applicable standards utilizing reference materials and other.\n\u2022 Executed statistical design and experimental analysis. Write technical report to the Lab Manager.', u""Plant Metallurgist\nSian Goldfields Ltd - Ghana\nAugust 2002 to August 2009\nNkawkaw, Ghana Aug 2002- Aug 2009\nPlant Metallurgist\n\u2022 Supervised and directed metallurgical team at the gold-processing plant, ensuring compliance\nwith environmental and safety standards by training and developing local staff on regular basis.\n\u2022 Integrated geological data from grade control process and metallurgical data from laboratory\ntest work and process operation.\n\u2022 Maintained quality improvement of products based on the company's standards.\n\u2022 Assessed and managed quality of final concentrate ensuring that deviations were studied and noted. Provided information to processing team which enabled them to understand the\nsignificance of product quality.\n\u2022 Participated in research and development. Ensured that projects are executed on time and within budget.\n\u2022 Implemented control strategies to decrease downtime, increase productivity, and reduce costs.\nOptimized existing processes through simulations and trials; performed statistical analysis.\n\u2022 Reviewed projections for capital and operating expenditures and managed annual plant\nbudget.\n\u2022 Provide technical support to the other departments for safety and quality operations.\n\u2022 Wrote plant performance report to the Senior Metallurgist.\n\u2022 Specific achievements include the following process design:\no by expanding the company's ore reserve be designing a $10 million heap leaching plant, which boosted the company's profit margin by 50%.\no an in-line leach reactor, which reduced production time by 20% and increased gold\nrecovery by 35%.\no cost effective electrowining plant which improved the current efficiency by 15%, and saved the company about $400,000 per year.\no cost savings of approximately $150,000 per year by designing a steam generator\nemploying process economics.\no a new milling and classifying circuit which improved gold recovery by 30% and saved the company $600,000 per year.\no an in-line leach reactor which reduced the cost of cyanide by 40%.\no a plant to recycle sodium cyanide which reduced the cost of cyanide by 35%.""]","[u'MSc. in Environmental Engineering', u'MSc. in Chemical Engineering']","[u'Old Dominion University Norfolk, VA\nAugust 2014 to December 2015', u'Kwame Nkrumah University of Science and Technology (KNUST)\nSeptember 2005 to August 2008']"
0,https://resumes.indeed.com/resume/820fa24eab045c71,"[u""Data engineer\nAkraya Inc\nJanuary 2014 to Present\n\u2022 Designed data pipeline using Python and PySpark to extract StubHub's native app traffic data from TUNE and load into Hadoop\n\u2022 Developed a data extraction framework to pull ORGANIC and PAID search data of StubHub Competitors from Similarweb\n\u2022 Architected new reusable dedupe framework using Apache Spark to bring flexibility for scheduling dedupe jobs\n\u2022 Built StubHub's buyer funnel data pipeline for computing key metrics such as Drop out rate, check out rate"", u'ETL Engineer\nTech Mahindra\nJanuary 2006 to January 2014\n\u2022 Architected new near real-time data processing system to report key booking metrics, with new solution the refresh time was reduced to 15mins from 8hrs\n\u2022 Tuned Performance of Complex Teradata SQL queries and reduced the CPU usage from 40000 CPU cycles to 3000 CPU cycles\n\u2022 Reduced the extracted time of Data stage Extraction jobs by 3hrs by implementing new architecture in existing data extraction process']",[u'Bachelor of Technology in Electronics and Communication Engineering'],"[u'Jawaharlal Nehru Technological University Hyderabad, ANDHRA PRADESH, IN\nJanuary 2001 to January 2005']"
0,https://resumes.indeed.com/resume/d27191f78e984ef1,"[u'Data Engineer\nPRO Unlimited @ Sanofi\nFebruary 2018 to Present\n\u2022 The Aim of the project is Data Mapping and Ontology merging of Pharmaceutical data from multiple sources\n\u2022 Wrote python code to map synonym data from different ontologies and built a pipeline using the mapped data', u""Analyst Intern\nWerum IT Solutions\nJuly 2017 to December 2017\n\u2022 Worked with Seeq (a data analytics application) to analyze large pharmaceutical data and developed use cases\n\u2022 Wrote queries in PL/SQL for pre-processing data from Oracle 12c database, before importing to Seeq\n\u2022 Worked on creating SDKs specific to Werum's requirements using Python."", u'Big Data Intern\nInvolgix\nJanuary 2017 to May 2017\n\u2022 Worked on Horton works Hadoop Clusters with HDP 2.4 with Ambari 2.2\n\u2022 Configured and Maintained Apache Hadoop cluster for application development and Hadoop tools\n\u2022 Extracted, transformed and cleansed data sets for the Data Analysis using Hive Query Language(HQL)\n\u2022 Created Dashboards in QlikView and Tableau after analyzing the data', u'Hadoop Engineer\nKruxonomy/Grapal\nMay 2015 to November 2015\n\u2022 Involved in setting up the Hadoop Ecosystem alongside Hadoop Administrator\n\u2022 Developed a data pipeline utilizing Kafka to store information into HDFS\n\u2022 Worked on Multiple AWS EC2 instances using EMR with S3 as a Data Layer\n\u2022 Designed and Developed Hive Databases to capture different data streams relating the business requirements']","[u'M.S., Business in Analytics', u'B.Tech. in Electronics & Communication Engineering']","[u'The University of Texas at Dallas Dallas, TX\nDecember 2017', u'IIT\nMay 2015']"
0,https://resumes.indeed.com/resume/85f4132e4eb2a4d3,"[u'Data Architect\nCompuData Products Inc - Lewisville, TX\nOctober 2014 to Present\nWorked as database administrator and designer for mid-size office products company (~ 50M annual sales) supporting business reporting, business intelligence, SQL Server with SSRS, SSAS and SSIS systems, SharePoint, ERP system, web-site sales, and systems support as needed. Maintained six Microsoft SQL Server instances (versions 2012/2014) containing 30 to 40 databases, nine tabular Analysis services databases and a Reporting Services instance holding both ad-hoc and subscribed reports and a SharePoint portal. Responsible for design enhancement, updates/upgrades of all database, instances, and server. Built ETL process to pull and process over one million rows nightly updating a data warehouse with over 12 million records and 65 gigabytes of data. Built custom reports and dashboards using SSRS, ASP.NET/C# and JavaScript/Angular. Supported custom report design and ad hoc query analyses.', u""Data Manager/Data Analyst\nSears Logistic Services/Sears Holding Corporation - Hoffman Estates, IL\nOctober 2000 to October 2014\nWorked in Logistics Services groups aligned with Logistics Network Design and Strategy team providing logistics optimization. Helped set up, build, maintain and administer departmental data warehouse containing over 100 million rows of sales and transportation data and disparate data sources, including Oracle, Informix and DB2. Performed ETL, query optimization, report generation and ad hoc data support and analyses and custom data analysis tools to internal SHC customers. Designed and built a desktop tool integrated with our business intelligence data warehouse to help analyze 'what-if' type questions for inventory flows which was credited with approximately one million dollars in inventory savings in its first quarter of limited use. Designed and presented a course on Microsoft Access for Data Analysis to approximately 60 logistics analysts, engineers and managers"", u""Consultant\nNASA\nJune 1999 to October 2000\ncontract with Lockheed - Martin providing business and data analysis support to track customer usage of NASA's institutional infrastructure and scientific space services, including coordination and communication of problem and question resolution with Business and Financial Management, Production Operations, and Customer Services. Created database to store, track, and price customer utilization of NASA services under the contract. Provided Access/Visual Basic and Microsoft Office expertise to Metrics and Business Forecasting groups. Performed ad hoc data analysis, report generation and spreadsheet/database management tasks."", u""Engineer\nJanuary 1998 to June 1999\nWorked on integrating databases with AutoCAD to help automate the design and pricing of BOMs for customer plant designs using Visual Basic, Auto List, and AutoCAD R13. Built and analyzed simulation models of client's production facilities, and provided in-house IT support for the Burlingame office (about a dozen engineers/staff)."", u'Engineer Intern\nAdvanced Micro Devices - Austin, TX\nMay 1997 to August 1997\nWorked in the Operational Modeling Group to develop simulation tools and models to help evaluate production performance. Developed software to automate updating and running of production simulation models by linking proprietary database on HPUX to commercial simulation tools on Windows NT.']",[u'Masters of Science in Operations Research'],"[u'University of Texas at Austin Austin, TX\nJanuary 1995']"
0,https://resumes.indeed.com/resume/c61238f1c28d8439,"[u'Sr. Big Data Engineer\nSpark, Azure - Milwaukee, WI\nOctober 2017 to Present\nThe IoT Platform team in the Data Enabled Business (DEB) at JCI builds, delivers, maintains and supports a suite of micro-services that enable sending data from sensors on JCI equipment at customer sites to the Cloud, storing the data securely on the cloud, analyzing this data in real-time as it streams in and providing metrics based on these analysis. These micro-services are used by various JCI Connected Applications to deliver ""Big Data And cloud-enabled"" features such as Advanced Analytics, Data Virtualization and Command & Control within their applications.\n\nResponsibilities:\n\u2022 Hands on experience in developing Batch Processing pipeline from end to end by using spark data frames and Scala\n\u2022 Migrated Billions of Historic Data into Cosmos DB by developing applications using Spark\n\u2022 Populating the Rest API Data into many fields as per the requirement and sending the batches of data to the Event Hub and storing the result into Azure Cosmos DB through Stream Analytic Job(SAJ)\n\u2022 Developed Spark application for Generating UUIDs for the Chillers, Registering them and setting up the permissions through Rest API Services by using spark\n\u2022 Hands on experience on creating Docker images and running it on Kubernetes Cluster\n\u2022 Load the data into Spark RDD and performed in-memory data computation to generate the output response.\n\u2022 Performed different types of transformations and actions on the RDD to meet the business requirements.\n\u2022 Developed and configured Event Hub to pipeline server logs data into spark streaming for real time processing\n\u2022 Experience in Elastic Search in Hadoop that bridges that gap, letting us the leverage for the best of Hadoop\'s big data analytics and the real-time analytics\n\u2022 Enabled speedy reviews and first mover advantages by using Apache Oozie workflow engine for managing and scheduling Hadoop jobs\n\u2022 Developed MapReduce programs to parse the raw data, populate staging tables and store the refined data in partitioned tables\n\u2022 Familiar with data architecture including data ingestion pipeline design, Hadoop information architecture, data modeling and data mining, machine learning and advanced data processing\nEnvironment: Scala, Java 1.8, Spark SQL, Spark Streaming, Azure Cosmos DB, Event Hub, Oozie, Rest API, Redis Cache, Kafka, Maven, Jenkins, Kubernetes', u""Sr. Big Data Engineer\nSpark, Kafka - Edison, NJ\nFebruary 2016 to September 2017\nData Lake is a centralized repository for storing and managing the data across the enterprise which is being developed for a client who has sponsored the project. The framework is primarily used to acquire, ingest and provision the data. The stored data is used for further analytics, which are specific to CRO/Pharma/Biotech business using the analytical frameworks.\n\nResponsibilities:\n\u2022 Actively participated from design phase of the data lake starting with performing POC's using multiple Big Data (Cassandra, HBase, Kafka, DynamoDB, AWS) tools to identify the best tools that solve the business problem at hand.\n\u2022 Shared responsibility for the complete life cycle of migrating data from the existing in-house infrastructure to the cloud built on top of AWS-S3.\n\u2022 Developed Impala to define the tables and mapping them to equivalent tables in HBase.\n\u2022 Involved in the development of Spark Streaming Application for one of the data source using Scala, Spark by applying the transformations\n\u2022 Involved in converting Hive/SQL queries into Spark transformations using Spark RDD, Scala and Python\n\u2022 Developed a Script in Scala to read all the Parquet tables in a Database and parse them as Json files another script to parse them as a structured tables in Hive.\n\u2022 Extensively involved in writing Spark Applications with Spark-SQL/Streaming for faster processing of Data.\n\u2022 Analyzed schedules in Control-M to check for dependencies, modified to add dependencies on new jobs using xmls\n\u2022 Responsible for design and creation of Hive tables, partitioning, bucketing, loading data and writing hive queries. Implemented, migrated existing Hive Script in SparkSQL for better performance.\n\u2022 Created RESTful services, converted data formats to make it consumable by services.\n\u2022 Developed Spark streaming jobs in JAVA 8 to receive real time data from Kafka, process and store the data to HDFS.\n\u2022 Experience in validating and cleansing the data using Pig statements and hands-on experience in developing Pig MACROS.\n\u2022 Used Agile(SCRUM) methodologies for Software Development.\nEnvironment: JAVA8, Sqoop, Impala, Kerberos, Spark SQL, Spark Streaming, Kafka, Scala, Amazon Web Services(AWS-EMR), Hive, Pig, REST, Oozie, Control-M, Maven, Jenkins."", u'Big Data Engineer\nVAQYA, (HowUSayIt.Com)\nApril 2015 to November 2015\nNew Jersey\nVaqya is a voice-enabled social networking platform that will enable users to create, share and follow users and content a multi-modal interface including voice, touch and text.\n\nResponsibilities:\n\u2022 Actively participated in complete software development lifecycle (Scope, Design, Implement, Deploy, Test) including design and code reviews.\n\u2022 Moving Bulk amount of data into HBase with Map Reduce Integration.\n\u2022 Developed Pig Programs for loading and filtering the streaming data into HDFS using Kafka.\n\u2022 Developed Scala Scripts, UDFs using both Data frames/SQL and RDD/MapReduce in Spark 1.6 for Data Aggregation, Queries and writing data back into OLTP System through Sqoop.\n\u2022 Performed advanced procedures like text analytics and processing, using in-memory computing capabilities of Spark using Scala\n\u2022 Developed HBase data model on top of HDFS data to perform real time analytics.\n\u2022 Used tools like MapReduce and Spark with Scala for performing operations like Clickstream Analysis and to perform Analysis on batch Data.\n\u2022 Experienced in handling Avro data files by passing schema into HDFS using Avro tools and Map Reduce.\n\u2022 Optimizing the Hive queries using Partitioning and Bucketing techniques, for controlling the data distribution.\n\u2022 Developed visualizations and dashboards using multiple BI tools like Tableau, Platfora.\n\u2022 Used Oozie scheduler to automate the pipeline workflow and orchestrate the Map Reduce, Sqoop, hive and pig jobs that extract the data on a timely manner.\n\u2022 Involved in story-driven agile development methodology and actively participated in daily scrum meetings.\nEnvironment: MapReduce, Spark with Scala, Hive, Pig, Sqoop, Oozie, HBase, Platfora, Redis, REST Services, Linux, Maven, Jenkins, HDFS.', u'Java/J2EE Developer\nPeritus Technologies - Hyderabad, Telangana\nNovember 2010 to July 2014\nHyderabad, India\nPeritus Technologies is developing a product that supports their executives in processing and maintaining billing details of their customers.\n\nResponsibilities:\n\u2022 Designed, developed and validated User Interface using HTML, Java Script, XML.\n\u2022 Handled the database access by implementing Controller Servlet.\n\u2022 Implemented PL/SQL stored procedures and triggers.\n\u2022 Extensively worked with Struts, Hibernate, Spring(Spring Core, Spring MVC) application design, development.\n\u2022 Experience in using various design patterns like Business Delegate, Session Facade, Service Locator, Singleton and Model-View-Controller\n\u2022 Extensively worked with Web Services including SOAP over JMS & HTTP, REST(JAX-RS)\n\u2022 Experience with multithreading, enterprise java beans(EJB), worked with session, entity and message driven bean\n\u2022 Experience in preparation of Test procedures, Test Scenarios, Test Cases and Test Data.\n\u2022 Created Test Cases using Element Locators and Selenium WebDriver Methods.\n\u2022 Execution of Selenium Test Cases and Reporting Defects.\n\u2022 Involved in Regression Testing and Automation Infrastructure Development using Selenium.\n\u2022 Expertise in implementation of Automation FrameWork using Selenium, LoadRunner, UFT.\n\u2022 Experience in developing applications in WebSphere 8.5 & 7, JBoss, Tomcat and BEA web logic. Developed server running script for automation using the JBoss 6.3 application server.\n\u2022 Strong experience in SOA(Service Oriented Architecture), EAI (Enterprise Application Integration) and ESB (Enterprise Service Bus)\n\u2022 Extensive experience in developing web page quickly and effectively using JSF, Ajax JQuery, JavaScript, HTML, CSS and also in making web pages cross browser compatible\n\u2022 Good in unit testing skills using Junit framework and functional Junit capturing user entered data and mapping it back to database to provide accurate test results.\n\u2022 Interacted with application architect to design the workflow and service integration on top of spring MVC, Ajax and web services layers.\n\u2022 Set up Web sphere Application server and used Ant tool to build the application and deploy the application in Web sphere\n\u2022 Used Spring Framework for Dependency Injection and integrated with Hibernate.\n\u2022 Involved in writing JUnit Test Cases.\n\u2022 Used Log4J for any errors in the application\nEnvironment: Java, Spring Framework, J2EE, HTML, JUnit, XML, JavaScript, Eclipse, WebLogic, PL/SQL, Maven, Oracle.']",[u'MS in Computer Science Engineering'],"[u'Texas A&M University Kakinada, Andhra Pradesh']"
0,https://resumes.indeed.com/resume/24dd2f9389ccaa81,"[u'Corporate Researcher and Data Analyst\nOffice of Corporate and Global Partnership, Purdue Research Park - West Lafayette, IN\nMay 2017 to August 2017\ncontinuing as Graduate Research Assistant) May - Aug 2017\n\u2022 Developed Tableau visualizations of Purdue India engagements to aid in identifying strategic partnerships\n\u2022 Performed corporate research on partners using LexisNexis, Dun and Bradstreet to validate corporate hierarchies\n\u2022 Created Salesforce accounts for corporate partners, incorporated corporate hierarchies, partner contacts, awards and gifts to track completed and upcoming tasks, events, meeting and integrated Tableau dashboards to Salesforce\n\u2022 Proposed and designed a new data model for gifts and awards management from industry partners and implemented using Microsoft Visual Studio and T SQL', u'Senior Applications Engineer\nOracle Corporation\nAugust 2014 to July 2016\n\u2022 Spearheaded weekly virtual conferences and email communication with partners/stakeholders\n\u2022 Analyzed and implemented a new channel to extract data from Taleo entities using Fusion HCM Extract\n\u2022 Trained and mentored new team members and leveraged expertise to improve productivity by 50 %\n\u2022 Acquired remarkable functional and technical expertise on Talent Management Suite - Taleo and fixed 20 product issues using JavaScript and Java in two months after induction\n\u2022 Involved in functional design document discussions for new product features and developed technical design documents for the Oracle Fusion ERP', u'Applications Engineer\nJune 2011 to July 2014\n\u2022 Developed proof of concept and implemented an automated process using ADF, ESS, SQL and Java to facilitate easier interactions with Oracle security framework saving 8000 man-hours\n\u2022 Orchestrated and augmented a modular program to report user authentication information, saving $80K per report\n\u2022 Inducted into a time critical project in Oracle Fusion HCM module to devise a novel solution using SOA, BPEL and BI Publisher for a platinum customer to assist onboarding and performance appraisal\n\u2022 Handled direct interactions with consultants and customers as a subject matter expert during production and ""go-live"" of one off project with atypical implementation and release instructions']","[u'Master of Business Administration in Business Analytics and Management Information Systems', u'Bachelor of Technology in Electronics & Communication']","[u'Purdue University West Lafayette, IN\nJanuary 2018', u'Kerala University\nJanuary 2011']"
0,https://resumes.indeed.com/resume/eb3cf3144fa6b638,"[u""Software Engineer\nPatriot Properties - Marblehead, MA\nFebruary 2016 to Present\n\u2022 Development of new stored procedures in TSQL\n\u2022 Construction of Reports using Crystal Reports\n\u2022 Development of jobs through SQL server agent to control daily routines\n\u2022 Import / export databases into an out of SQL\n\u2022 Use of debug scripts along with SQL profiler to debug stored procedure errors\n\u2022 Set up users and securities throughout databases\n\u2022 Work in SVN to maintain that our file share system was complementary to our client's\n\u2022 Convert raw data into SQL for customers\n\u2022 Weekly engineer meetings to discuss ongoing projects and future ideas"", u'Data entry\nPartners Health Care - Salem, MA\nJuly 2015 to February 2016\nSalem, MA July 2015 - February 2016\nInformation Services\n\u2022 Reimaging of old desktops and installation of new software\n\u2022 Scheduling and Installation of Monitors and Desktops throughout Salem Hospital\n\u2022 Data entry of devices being replaced\n\u2022 Maintain Database of devices installed and disposed of using Excel\n\u2022 Use of CMD shell to fix unimaged computers']","[u""Bachelor's Degree in Computer Science in Information Systems""]",[u'Framingham State University\nSeptember 2010 to May 2015']
0,https://resumes.indeed.com/resume/bedddc667bc7a083,"[u'Data Center Project Manager/Engineer\nOutsource - San Francisco, CA\nJanuary 2014 to Present\nBuild, upgrade, configure and decommission custom build data center environments spanning several different disciplines from the ground up or using legacy equipment to meet contract specification. Install, troubleshoot and repair upgrade and decommission infrastructure, fiber optic, copper, twin ax, telco rack, server cabinet, seismic bracing, raised floor, etc. Monitor and report all data center, electrical, mechanical and emergency system issues. Create and read, hot aisle/cold aisle floor plans, project plans and data center inventories.\nJob Sites\nApple Structure Cable & Audio Visual Contractor\nAvaya Data Center Project Manager\nSalesforce Data Center Engineer/Sr. Technician\nNASA Data Center Asset Manager\nTesla Structure Cable & Audio Visual Contractor', u'Data Center Engineer/Technical Lead\nHorizon Communications\nJanuary 2013 to January 2014\nContractor for Horizon Communications - Manage, train and schedule technical team for all data center project activity for Salesforce.com - San Francisco and San Jose. Working lead with daily projects to install production and lab environments and decommission large and small detailed data center environments. Create and read data center inventories, rack elevations, cross connect patch plans, hot aisle/cold aisle floor plans and project plans. In depth knowledge of fiber and copper trunk and cable application for Salesforce data centers. Collaborate with various contractors, manufacturers and Salesforce datacenter engineering team to develop and maintain an SOP for ongoing training and adherence of technical contractors at SalesForce.com San Francisco and SalesForce.com San Jose data centers. Create excellent communication networks with internal and external customers for seamless daily operations of data center projects.']",[u'in Electrical Engineering Technology'],"[u'ITT Technical Institute Buena Park, CA']"
0,https://resumes.indeed.com/resume/63596d992f8a33ff,"[u'Principal Data Scientist\nEdison Energy - Boston, MA\nMarch 2016 to Present\nLead team of six data scientists & developers responsible for research, design, and implementation of derivatives pricing and price simulation algorithms, focused on renewable energy, power, and natural gas markets.\nDesigned, developed, and deployed portfolio management analytical product offering, booking $1.7 million in sales to Fortune 100 clients in first two years of product rollout.\nCoinventor on filed patent \u201dSystems and methods for energy management\u201d', u'Data Engineer\nAltenex - Boston, MA\nApril 2015 to March 2016\nDeveloped methodology and software implementation of long-term stochastic power system scenario simulation.\nDeveloped and deployed highly parallel and distributed implementation of price risk engine to AWS\nLead analytics for over 1GW of constructed on-site and off-site renewable energy projects, yielding millions of dollars in savings to clients.\nResearched and implemented copula-based methodology for risk-adverse renewable energy system portfolio optimization.', u'Senior Consultant\nNavigant - Washington, DC\nFebruary 2014 to March 2015\nWorked with the Global Energy Division in the Market Intelligence group. Focused on Transmission Modeling and Planning, Energy Forecasting, proprietary model development, and Environmental Policy. Sample engagements include:\n\nResearch and preparation of transmission components of present and forecasted North American energy market reference case.\n\nQuantified the Transmission System effects of distributed energy resource integration for IOU in FRCC and WECC.\n\nConducted a deliverability analysis using PSSE for an IOU in PJM to determine thermal overloads under N-1 contingency scenarios.\n\nCreation of a web data extraction tool to monitor updates to critical power systems infrastructure and market frameworks.\n\nDevelopment of algorithms to check integrity and validity of Bulk Electric System models.\n\nAuthored portions of environmental policy section in market forecast report.\n\nConducted transmission expansion planning analyses for an IOU in SPP through single and double contingency simulations to identify most at-risk corridors, recommended options to satisfy NERC reliability requirements.\n\nFull development of web scraping platform and web application for sales leads generation using Openshift, Python, Flask, and Import.io']","[u""Master's in Power Systems Engineering & Optimization"", u'BS in Environmental Engineering']","[u'Cornell University Ithaca, NY\nJanuary 2014 to May 2014', u'Cornell University Ithaca, NY\nAugust 2009 to May 2013']"
0,https://resumes.indeed.com/resume/7d3eb67809954cda,"[u'Sr. Big Data Developer\nGE Power Digital - San Ramon, CA\nSeptember 2017 to Present\nOperations Optimization is a GE Power Services suite of software designed to facilitate volume, velocity, and variety of data in the power and utility industry. It is built on Predix platform and uses time series data generated by Predix and performs analytics on it and predicts a Power Plant performance for future.\n\nResponsibilities\n\u25cf Running analytics on power plant data using Pyspark API with Jupyter notebooks in on premise cluster for certain transforming needs.\n\u25cf Used Predix IO for data storage for writing the transformed data.\n\u25cf Worked with Apache Airflow and Genie to automate job on EMR.\n\u25cf Responsible in loading and transforming huge sets of structured, semi structured and unstructured data.\n\u25cf Implemented automated frameworks in python for reconciling data between source and target systems and unit test case creation, execution and reporting test status.\n\u25cf Developed UDFs in python for high level scientific calculations as suggested by performance engineers,\n\u25cf Parsing the source into Json and CSV formats through Python Script for further analysis.\n\u25cf Created pipelines to run production jobs in Amazon EMR.\n\u25cf Knowledge of Kafka and streaming.\n\u25cf Hands-on experience in data analysis using Elastic MapReduce on the Amazon Web Services (AWS) cloud.\n\u25cf Experience in Automation Testing using Pytest, Software Development Life Cycle (SDLC) and good understanding of Agile Methodology.\n\u25cf Monitoring the changes in the weekly runs and reporting the fatal as well as non-Fatal changes in the data.\n\u25cf Working directly with the business users in gathering, reviewing and analyzing data requirements.\n\u25cf Collaborated with Infrastructure, network, database application and Business Intelligence team to ensure data quality and availability.\n\u25cf Created Data lakes and provided data for continuously improving the efficiency and accuracy of existing predictive model for data science team.\n\nEnvironment: Apache Spark 2.1.0, Python2.7, Anaconda Jupyter Notebooks, Apache Kafka, Amazon AWS, Hive, HDFS, YARN, Oracle 10g, MySQL, PL/SQL, Mac OS, UNIX and Git.', u'Sr. Big Data Engineer\nReal Page Inc - San Francisco, CA\nApril 2015 to August 2017\nReal Page Inc provides property management software solutions for the multifamily, commercial, single-family and vacation rental housing industries. It provides an integrated, end-to-end, web-based solution, it helps streamline all day-to-day operations. Centralize accounting, reporting, and spend management. Automate service requests and manage asset conditions on the go. And organize and store electronic records and other digital compliance documents quickly and easily.\n\nResponsibilities\n\u25cf Building ETL jobs using Pyspark API with Jupyter notebooks in on premise cluster for certain transforming needs and HDFS as data storage system.\n\u25cf Have developed automated Pyspark job to extract data from third party APIs which contributes in improving the property maintenance.\n\u25cf Used Netezza DB for data storage for writing the transformed data for the decision sciences team to consume.\n\u25cf Developed shell script for Job Control Process for Data Integration Layer which involves multiple dependency checks.\n\u25cf Responsible in loading and transforming huge sets of structured, semi structured and unstructured data.\n\u25cf Implemented automated frameworks in python for reconciling data between source and target systems and unit test case creation, execution and reporting test status.\n\u25cf Developed UDFs in python and Pyspark Jobs for a use case.\n\u25cf Have set up Spark Streaming jobs in pyspark to receive the data from Kafka on another host from the slame topic created.\n\u25cf Created a Hadoop Cluster on 4 nodes in the cloud using Cloudera Manager and have set up Zeppelin and ipython notebook to use spark interactively.\n\u25cf Have set up Spark Streaming jobs in pyspark to receive the data from Kafka on another host from the same topic created.\n\u25cf Performed data migration from Oracle to Hadoop environment using Sqoop.\n\u25cf Understanding in data analysis using Elastic MapReduce on the Amazon Web Services (AWS) cloud.\n\u25cf Experience in Automation Testing, Software Development Life Cycle (SDLC) using the Waterfall Model and good understanding of Agile Methodology.\n\nEnvironment: Apache Spark 1.5.0, Python2.7, Anaconda Jupyter Notebooks, Apache Kafka, Netezza, Amazon AWS, Hive, HDFS, YARN, Sqoop, Oracle 10g, MySQL, PL/SQL, Windows 7, UNIX and Git.', u'Data Integration Engineer\nBirch Communications - Atlanta, GA\nMay 2014 to March 2015\nBirch Communications is an Atlanta, Georgia-based provider of IP-based communications, network broadband, cloud computing, and information technology services to small, mid-sized, enterprise and wholesale business customers. The company began acquiring other telecom companies in an effort to increase its network size and service offerings. Involved in integrating all the systems.\n\nResponsibilities\n\u2022 Developed ETL solution to integrate the data from 5 legacy systems to develop a next generation CRM system.\n\u2022 Developed Data Model to get unified view for the internet, wire line telephone and television services.\n\u2022 Developed ETL jobs for key DI entities like (Accounts, Contacts, billing Addressees of the customer and Assets).\n\u2022 Developed and maintained ETL mapping to extract the data from multiple source systems that comprise databases like Oracle 10g, SQL Server 7.2, flat files to the Staging area, EDW and then to the Data Marts.\n\u2022 Analyzed the data after loading to production and written PL/SQL queries to test the data.\n\u2022 Performed Impact Analysis of the changes done to the existing mappings and provided the feedback.\n\u2022 Hands on experience on Star Schema Modeling, Snow-Flake Modeling, FACT and Dimensions Tables.\n\u2022 Requirement gathering and Business Analysis of the specifications provided by the clients.\n\u2022 Testing the ETL objects in all kinds of aspects and fixing the issues if exists.\n\u2022 Created and scheduled Worklets. Setup workflow and Tasks to schedule the loads at required frequency using Workflow Manager.\n\u2022 Used SQL tools TOAD to run SQL queries and validate the data in warehouse\n\u2022 Experience in Administration activities like Creating and Managing Repositories, Users, User Groups, Folders, Working with Administrator functions of Repository Manager.\n\u2022 Coordinated and monitored the project progress to ensure the timely flow and complete delivery of the project.\n\u2022 Worked on SQL and UNIX shell scripting.\n\u2022 Involved in client interaction sessions and project status meetings.\n\u2022 Delivery management and ownership.\n\nEnvironment: Oracle 10g, SQL Server 7.2, MYSQL, MS SQL 2008, TOAD 9.6.1, Visio, Flat files, Unix Shell Scripts, BASH, SQL Navigator, Windows XP/7,Putty, WinSCP, Jira, Mercury, MS Office.', u'ETL Engineer\nDirect Brands Inc - New York, NY\nJanuary 2012 to April 2014\nInvolved in design and development of three major analytic modules - Sales, Marketing and Services. Sales Data mart captures Opportunity data at the product, industry segment and customer level. Marketing Data mart is based on Campaign History and their Responses and provides ability to do analysis by Campaign, Offer, Segment and Region. Services Analytic module captures Orders, Activities, Contracts and Service Invoices.\n\nResponsibilities\n\u2022 Took part in building ETL jobs for multiple data reporting applications during this tenure.\n\u2022 Gained rich experience in Data Warehousing concepts, RDBMS systems, other file structures and importance of ETL process in an enterprise environment.\n\u2022 Analyzed and executed the test cases for various phases of testing - integration, regression and user.\n\u2022 Developed modules that integrate with web services that provide global information support such as customer and account information.\n\u2022 Created and monitored sessions using workflow manager and workflow monitor.\n\u2022 Assisted in deployment preparation and code deployment to Staging and Production databases.\n\u2022 Developed several complex queries in PL/SQL for implementing a tiered database model.\n\u2022 Completed data migration of consumer data from different commerce systems to single user database.\n\u2022 Analyzed system issues and fixed bugs. Coordinated with release team to release builds to production.\n\u2022 Associated with Production support team in various performances related issues.\n\u2022 Worked on Agile and Waterfall methodology for different projects.\n\u2022 Used advanced Excel functions to generate spreadsheets and pivot tables.\n\u2022 Compile and validate data; reinforce and maintain compliance with corporate standards.\n\u2022 Develop and initiate more efficient data collection procedures. Working with managing leadership to prioritize business and information requirements.\n\u2022 Develop and initiate more efficient data collection procedures. Working with managing leadership to prioritize business and information requirements.\n\u2022 Develop and initiate more efficient data collection procedures. Working with managing leadership to prioritize business and information requirements.\n\u2022 Rigorous unit testing and functional testing to reduce the defects before QA.\n\u2022 Prepare documentation on all aspects of ETL processes, definitions and mappings.\n\nEnvironment: Oracle 8i, DB2, SQL Server 2008, Windows XP/7, Unix Shell Scripts, BASH, LOTUS, SQL Navigator, Putty, WinSCP, Jira, Java, UNIX Scheduler, MS SQL Server 2008,Business Objects XI and Tortoise CVS.', u'Software Developer\nDirect Brands Inc - New York, NY\nJuly 2010 to December 2011\nDirect Brands, Inc. provides direct-to-consumer distribution services for media products. The company offers DVD s and Blu-ray discs via the negative option billing, marketing and promotion services. The company operates the DVD club brand under the name of Columbia House. The company serves its members through its club catalogs and online.\n\nResponsibilities:\n\u25cf Responsible for understanding user requirements, designing and developing the application.\n\u25cf Unit testing at Module Level.\n\u25cf Hands on experience with Z/OS, ISPF, COBOL, DB2, CICS, MQ Series, JCL, VSAM, SYNCSORT, File Aid, Endevor, DB2, IBM Utilities.\n\u25cf Preparing documentation on all modules in the existing system.\n\u25cf Experience in all facets of Software Development Life Cycle (SDLC), including requirements gathering, designing, coding, testing, and deployment.\n\u25cf Maintenance of existing computer system, including error resolution and enhancements to existing system utilizing COBOL programs.\n\u25cf Develop programs in COBOL and JCL, Decforms, CICS and Pro Cobol programs on Mainframes platform.\n\u25cf Knowledge in the business process and software development life cycle (SDLC), including requirements gathering, analysis, testing, implementation of software applications and application maintenance.\n\u25cf Assisted management, on a daily basis, in isolation, root-cause analysis, and resolution and provided recommendations to user-facing business and application issues.\n\u25cf Source code management as per the client policies.\n\u25cf Resolved CICS issues as assigned.\n\u25cf Created JCL for new clients and regions as needed.\n\u25cf Utilized FILE-AID to make global changes to JCL for new clients and environments.\n\u25cf Created a process using IDCAMS and COBOL programs to dynamically create GDG bases using other systems and clients as a model.\n\nEnvironment: IBM Mainframes, JCL, COBOL, CICS, Rexx, Tso/ISPF, IBM AIX, DB2, SQL, VSAM, IDCAMS, SYNCSORT, FILE-AID.']",[u'Bachelors in Computer Science Engineering in Computer Science Engineering'],"[u'Osmania University Hyderabad, Telangana']"
0,https://resumes.indeed.com/resume/842df9db96fc095f,"[u'Analytics Consultant\nNorthwestern Medicine - Chicago, IL\nDecember 2016 to Present\n\u25cf Define, extract, analyze and presents data cogently to answer the question being asked.\n\u25cf Work with business users to define requirements, formulate queries, validate results, and generate reports using Microsoft SQL Server Reporting Services (SSRS).\n\u25cf Analyze large datasets, evaluate data quality, and interpret results in a clear, concise manner.\n\u25cf Works collaboratively with and supports multi-departmental efforts and projects.\n\u25cf Wrote queries and stored procedures as a part of a huge remediation effort.\n\u25cf Used queries and stored procedures written to create different visualizations in tableau, or reports in SSRS.\n\u25cf All other duties as assigned.', u""Data Integration Engineer\nWageWorks - Mequon, WI\nJanuary 2014 to December 2016\n\u25cf Wrote scripts using the PL/SQL language, which are used to solve day to day data issues.\n\u25cf Wrote complex SQL queries for different report requests.\n\u25cf Responded to technical issues and provide third tier support for escalated non-repetitive data issues.\n\u25cf Analyzed, identified and resolved data issues by creating complex programs to resolve data conditions and anomalies in an efficient manner that is in accordance with established Service Level Agreements (SLA's).\n\u25cf Monitored job results, identified trends in data or process issues, diagnosed problems and provided fixes.\n\u25cf Maintained and performed operational back office processes that included but was not limited to executing batch jobs and reports.\n\u25cf Handled confidential information (including card data) and interacted professionally with internal and external clients, vendors and third-party administrators.\n\u25cf Assisted Product Engineering and IT Operations staff with routine and basic support issues that relate to software applications and utilities.\n\u25cf Performed other duties as assigned or apparent."", u'Data Analyst\nWageWorks - Mequon, WI\nSeptember 2013 to January 2014\n\u25cf Ran and monitored daily job processing including loading inbound data files, as well as processes and delivers outbound data files.\n\u25cf Evaluated and monitored data quality including exception and error handling. Researched inconsistent data loads and addressed issues.\n\u25cf Responded to basic and common technical issues to provide first tier support for data issues. Researched data inconsistencies by modifying and executing SQL queries to identify trends/issues in accordance with established Service Level Agreements (SLAs).']",[u'B.S. in Information Technology'],[u'Marian University\nAugust 2008 to May 2013']
0,https://resumes.indeed.com/resume/2609dfa322198830,"[u""Big Data Consultant\nPREDICTif Solutions - Houston, TX\nAugust 2014 to Present\nClient: PREDICTif Solutions\nEnvironment: Hadoop, HDFS, MFS, NFS, MapR Control System, HBase, MapR-DB (binary and document), Hive, Sqoop, Pig, Oracle Linux 6, CDH, Cloudera, Spark, Spark Streaming, Oracle Big Data Discovery\n\u2022 Experience in developing Spark programs in Scala to perform Data Transformations, creating DataFrames, and writing spark SQL queries, spark streaming, windowed streaming application in Scala.\n\u2022 Create a connector to migrate data from HBase/MapR-DB binary to MapR-DB document database in Spark using Scala.\n\u2022 Experience in handling large datasets using partitions, spark in-memory capabilities, broadcast variable, effective and efficient joins, transformations during ingestion process.\n\u2022 Experience in MLlib component of Spark for techniques like classification, regression, clustering, and creating recommendation engine based on machine learning workflow.\n\u2022 Import data from sources like HDFS/HBase into Spark RDD.\n\u2022 Experience in implementing Spark RDD transformations, and actions to implement business analysis.\n\u2022 Created Hive tables, loaded data, and wrote Hive queries that run within the map.\n\u2022 Used Oozie operational services for batch processing, and scheduling workflows dynamically.\n\u2022 Used Sqoop to import data into HDFS and Hive from Oracle database.\n\u2022 Developed Pig Latin scripts for the analysis of semi-structured data.\n\u2022 Manage jobs, streaming data, spark applications using Spark Web UI.\n\u2022 Converted Hive/SQL queries for processing and analyzing large volume of data.\n\u2022 Developed Spark scripts, UDF's using both Spark DSL and Spark SQL query for data aggregation, querying, and writing data back into RDBMS through Sqoop.\n\u2022 Extensively worked with MapR 5.x/6.x, CDH 5.x.\n\u2022 Worked on Flume for collecting logs from log collector into HDFS.\n\u2022 Installed and configured various components of Hadoop Ecosystems and maintained their integrity.\n\nClient: Galderma\nEnvironment: Eclipse 4.x, SpringBoot, Apache Tomcat 7.0, Oracle 11g, Java 7/8, Oracle SQL Developer, Maven, JavaScript, Hibernate, HTML5, CSS3, Git\n\u2022 Developed REST APIs using SpringBoot framework to expose data from the Oracle database.\n\u2022 Used Spring MVC to decouple business logic and view components.\n\u2022 Used Spring framework for Dependency Injection to inject objects when needed.\n\u2022 Used Hibernate as Object Relational Mapping (ORM) framework for mapping object-oriented domain model to Oracle database.\n\u2022 Interfaced with the Oracle back-end using Hibernate framework utilizing Annotations.\n\u2022 Used Spring MVC, Spring REST for building RESTful web services over JSON to provide services to the front end.\n\u2022 Developed DAO classes to implement CRUD (Create, Read, Update, Delete) operations.\n\u2022 Used Oracle SQL Developer to write queries for testing in the local environment.\n\u2022 Worked on PL/SQL stored procedures to develop backend logic.\n\u2022 Responsible for an entire workflow from front-end to back-end for multiple modules in the application.\n\u2022 Used agile methodology.\n\u2022 Deployed application on Apache Tomcatv7.\n\u2022 Used Maven as build and dependency management tool.\n\u2022 Used Git for version control.\n\nClient: Nebraska Public Power District\nEnvironment: Oracle 11g/12c, Oracle Apex 5.0.1, HTML, CSS, JQuery, JavaScript, SQL, PL/SQL, Oracle SQL Developer\n\u2022 Created PL/SQL packages, procedures, materialized views, PL/SQL tables, cursors, user-defined object types, and exception handling.\n\u2022 Designed and developed an efficient reporting solution in APEX enabling the client to report on data with the ingestion rate of 5.5 million rows per day.\n\u2022 Migrated data from flat files into Oracle database tables using SQL, PL/SQL, and SQL Loader.\n\u2022 Developed application using APEX (Reports/Forms) to empower clients in better analyzing the data.\n\u2022 Developed several pages, regions, items, processes in several instances such as on load, before the header, after the header, and requested page process, etc.\n\u2022 Developed several classic reports, interactive reports, PL/SQL custom reports, parameterized reports for the business users to view the vital information.\n\u2022 Developed and implemented authorization and authentication rules for application administrators to manage the APEX applications, users, and their log activity details.\n\u2022 Created database objects such as tables, views, indexes, and sequences according to business requirements.\n\u2022 Implemented application-level processes to interact with the databases for fetching the data without page submissions.\n\u2022 Used the data upload functionality in APEX to upload first-time data into the database tables.\n\u2022 Involved in documenting the project.\n\nClient: Cox Automotive\nEnvironment: Oracle E-business Suite, Oracle Endeca Information Discovery 3.0, Oracle EBS Extensions for Endeca, Oracle SQL Developer, Oracle Integrator ETL 3.1.1\n\u2022 Assisted in the installation of EBS Extension for Endeca 3.0.\n\u2022 Developed and customized Clover ETL graphs using IntegratorETL as per clients reporting requirements.\n\u2022 Proficient in Endeca Integrator ETL, MDEX Server, Studio, & Guided and Navigation Search.\n\u2022 Scheduled and executed Clover ETL graph services as per functional requirements.\n\u2022 Created Integrator graphs to load Collections, Attributes, Attribute groups, Refinement rules, and Search rules using configuration sheet.\n\u2022 Defined transformations for source to target definitions in Integrator graph components.\n\u2022 Collaborated with customers on defining data visualizations on Endeca studio.\n\u2022 Created and deployed data transformation jobs to CloverETL Server.\n\u2022 Created processes to refresh source data, and reset Endeca data domains.\n\u2022 Designed Endeca studio UI for different loads (Accounts receivables, Floorplanning, Accounts payable, and Order management), which included alerts, charts, maps, tag cloud, compare, and result table.\n\u2022 Developed complex stored procedures in PL/SQL to prepare the data for ingesting into Endeca Server.\n\u2022 Provided production support and performed code migrations from DEV to UAT and UAT to PROD.\n\u2022 Supported the Endeca EBS application by answering user questions and handling failed jobs.\n\nClient: Jack in the Box\nEnvironment: Oracle Endeca Information Discovery 3.1, Oracle 11g, MS SQL Server 2012, Informatica 9.6.1, Oracle SQL Developer, Oracle IntegratorETL 3.1.1\n\u2022 Developed data discovery applications using Oracle Endeca Information Discovery to help business get insights and map out a long-term strategy.\n\u2022 Contributed in the end to end implementation of the Oracle Endeca Information Discovery application.\n\u2022 Installed and configured Endeca components in the lower environments and helped Infrastructure team to setup Endeca in production.\n\u2022 Developed complex Clover ETL graphs using IntegratorETL to fulfill the business\u2019 reporting requirements.\n\u2022 Designed and developed the process to crawl unstructured data from various sources (Online forums, Official websites, Twitter feeds) using IntegratorETL to feed into the Endeca Server for reporting requirements.\n\u2022 Created Clover ETL graphs with various data transformations and discovery scenarios to extract all possible information from structured data.\n\u2022 Built different UI pages using Endeca Studio components.\n\u2022 Developed Endeca queries/views specific to business requirements.\n\u2022 Created bookmarks and configured filtering components like breadcrumbs, guided navigation, search box, and range filters.\n\u2022 Accelerated the Integration of Endeca application with Microsoft Active Directory and creation of different user roles/groups.\n\u2022 Performed Endeca MDEX performance tuning.\n\u2022 Involved in business requirements gathering sessions and prepared design documents."", u'Software Engineer\nInfosys Technologies Ltd\nNovember 2008 to June 2011\nEnvironment: .NET Framework 3.5, ASP.NET, ADO.NET, MS SQL Server 2008, VSS, AJAX\n\u2022 Developed web applications for diverse programming scenarios using .NET Framework and all the related components.\n\u2022 Designed and developed Web UI and business logic for web applications.\n\u2022 Implemented LINQ queries for optimized database operations.\n\u2022 Designed and developed several SQL Server stored procedures, functions, and triggers for all the DML functionality.\n\u2022 Implemented stored procedures to perform complex ETL (Extract, Transform and Load) jobs to migrate data from mainframe exported CSV\u2019s to SQL server database.\n\u2022 Enhanced the performance of the application by continually monitoring queries by running Execution plan and query optimization techniques in SQL server.\n\u2022 Worked on ADO.NET components SQL connection object, SQL command object, Data reader, Data adapter to communicate with the database.\n\u2022 Handled queries from team members and resolved issues.\n\u2022 Mastered Visual Source Safe (VSS) for all the source code control jobs.']",[u'Master of Science in Computer Science in Computer Science'],"[u'University of Houston Main Campus Houston, TX']"
0,https://resumes.indeed.com/resume/fbfb18d76ba3c99e,"[u""Data Science Intern\nFulcrum Global Technologies\nMay 2017 to Present\n\xd8 Came up with the idea of creating a predictive model which helps Law Firms to innovate Client\u2019s Intake Process using Machine Learning and Deep Learning.\n\xd8 Processed large scale time series data given by the LexisNexis and Westlaw and applied statistical modelling for features detection in order to train the\nfinal predictive model using Artificial Neural Network.\n\xd8 Worked on a recommender system model using machine learning which recommends highest profitable clients internally to the firms.\n\xd8 Our current research indicates that our model has the capability to innovate the way Law firms do business and will result in 20-30% increase in profit by\npredicting the risk matrix and Client's Indicator Score(CIS) which help to identify the potential clients.\n\xd8 Our work and techniques believed to be novel and unique so now it\u2019s the subject of a pending patent application."", u'Machine Learning Software Engineer\nTATA Consultancy Services, Cisco System - Mumbai, Maharashtra\nOctober 2014 to July 2016\n\xd8 Worked as Machine learning engineer to generate insights and improve business operation for Cisco System.\n\xd8 Built Recommender system for Cisco Commerce Workspace(CCW) users to recommend products which resulted in 10% increase in revenue.\n\xd8 Helped to pipeline massive data-streams in distributed computing environments such as Hadoop and Spark.\n\xd8 Worked with senior engineers to develop high-quality software that is robust and reliable.\n\xd8 Analyzed and resolved source code bugs in Cisco System e-commerce application(CCW) used globally by 10000+ customers due to which clients reported\nprofit of $6 million postfix.']",[],[]
0,https://resumes.indeed.com/resume/015a217b40b3f07e,"[u'Sales Engineer\nMen and Mice - Herndon, VA\nSeptember 2017 to Present\n- Support Men and Mice DNS and DHCP software and virtual appliances.\n- Perform technical demonstrations and point of concept installations for potential clients.\n- Collaborate with the development team for custom requests and functionality for clients,\nperform beta testing before implementation in client networks.\n- Research and recreate bugs reported by clients, file reports for Development team.\n- Update documentation to reflect new practices and software versions.\n- Support and troubleshoot BIND issues with contract clients.\n- Research new network implementations and features for Men and Mice software.', u'Network Engineer\nThe Athene Group - Herndon, VA\nNovember 2016 to June 2017\nResearched vendors and platforms for a new ticketing system; made recommendations to\nCOO.\n- Researched vendors for new SIEM solution, attended demonstrations and meetings, and configured products in lab environments.\n- Updated VPN configuration and security management with Cisco ASA and upgraded\nremote site onto Fortinet Fortigate Firewalls.\n- Managed project to merge all company computers and servers onto domain, and configured user group policies on AD servers.\n- Managed hardware and software administration and inventory, with weekly updates and audits.\n- Arranged and led new-hire security orientations.\n- Wrote Standard Operation Procedures for the Operations team.', u'Network Engineer\nStable Solutions Group - Sterling, VA\nJuly 2016 to October 2016\nContractor - Clarabridge\n- Configured VPNs using Cisco ASA Firewalls and updated network diagrams with Visio.\n- Performed audit of networking equipment at remote office that was relocating,\nrefurbished equipment, and configured network for the new branch office.\n- Virtualized existing database using VMWare.', u'Network Engineer\nZQEdge - Sterling, VA\nJanuary 2016 to July 2016\nContractor - BT Federal\n- Troubleshot outages, jitter, and packet loss issues.\n- Created and updated topology maps with Google Draw and Microsoft Visio.\n- Performed acceptance testing for circuits and creating reports for clients.\n- Completed onsite Solarwinds training to support the NOC team.', u'Network Engineer\nKelly IT - Herndon, VA\nFebruary 2015 to September 2015\nContractor - Booz Allen Hamilton\n- Updated networking devices across infrastructure to support Cisco ISE (merged from\nCisco NAC manager).\n- Updated and replaced network devices at remote sites, scheduling down time during non- peak hours and configured the wireless network for a new Booz Allen office location.\n- Updated BGP configurations for upgraded or new MPLS connections.\n- Conducted investigations and audits with the Computer Incident Response Team to resolve suspicious network activity.', u""Network/Sales Engineer\nDHK Enterprises - Sterling, VA\nSeptember 2012 to January 2015\nInternal, client-facing and government contracted network engineer\n- Performed technical demonstrations as a sales engineer to new customers.\n- Configured and maintained: Cisco routers and switches, Juniper firewalls and switches\n(SSG, SRX, MAG), network security settings and ACLs, SSL and IPsec VPNs for cloud\nconnectivity, and remote access for clients to VMWare virtualized desktops.\n- Configured, tested, and demonstrated VMWare's 3D Cloud Server Solution.\n- Documented client network infrastructures using Visio."", u'Data Center Technician\nCorestaff - Ashburn, VA\nDecember 2011 to August 2012\nContractor - Amazon\n- Hardware and software repairs on Linux servers and databases; updated and maintained\nRed Hat servers\n- Troubleshot and corrected a routing loop and outage; resolved network IT tickets.\n- Rebuilt RAID databases after repairs.\n- Assisted with training new technicians.']","[u'in Engineering', u'']","[u'Northern Virginia Community College', u'George Marshall High School']"
0,https://resumes.indeed.com/resume/43fa1273955f5e6f,"[u""Data Mining Engineer\nShenzhen MasterCom - Shenzhen\nMay 2017 to August 2017\nAssisted Communication Company in building base stations and automatically checking the signal strength in every floor of the building via SQL Server, Python and R.\n\u2022 Programmed base station's Unstructured Data Collection and Data Clean via SQL Server.\n\u2022 Implemented Random Forest Regression by R to predict the data usage of a mobile base station.\n\u2022 Collaborated to develop a Hierarchical Clustering and Greedy Sort of WIFI routers in a building and achieved 90% accuracy."", u'Research Student\nTokyo Institute of Technology - Tokyo\nSeptember 2016 to March 2017\n\u2022 Assisted in conducting numerical experiment using C# to calculate the primal solution and dual solution of a non- convex optimization problem in both stochastic and non-stochastic condition.', u""Data Analyst Intern\nChina Unicom - Guangzhou\nAugust 2015 to January 2016\nPredicted mobile phone users' behavior to profit Communication Company via C++ and Oracle in Linux.\n\u2022 Simplified complex SQL and helped build UI by connecting C++ with Database via OCCI.\n\u2022 Cooperated to build C4.5 Decision Tree to predict customer loss.\n\u2022 Introduced Downsampling to increase precision and recall rate and saved computation time.""]","[u""Master's in Computer Science"", u""Bachelor's in Mathematics and Applied Mathematics""]","[u'Rice University Houston, TX\nAugust 2017 to December 2018', u'Sun Yat-Sen University Guangzhou, CN\nSeptember 2012 to June 2016']"
0,https://resumes.indeed.com/resume/a20e5ca99d4d2110,"[u'Data Scientist\nEnloe Medical Center - Chico, CA\nDecember 2016 to Present\n\u2022 Applied predictive modeling with Machine Learning algorithms (Linear Regression, Logistic Regression, Na\xefve Bayes, Decision Tree, Decision Forest etc.) on large clinical data sets (Hadoop based)\n\u2022 Designed and implemented KPIs (Key Performance Indicator)\n\u2022 Integrated into Epic EHR system including data integration and did analytical reporting in EpicCare Ambulatory, Professional Billing, etc', u'Co-Founder/Lead Data Scientist\nAutoRithm Inc - Irvine, CA\nSeptember 2014 to Present\n\u2022 Designed and developed in conjunction with two co-founders our own algorithms to implement ADAS (Advanced Driver-Assistance Systems)\n\u2022 Built Machine Learning models for ADAS to implement object detection, recognition Object classification, object localization and prediction of movement', u""Software Design Engineer\nSamsung Research America - Irvine, CA\nMarch 2012 to September 2014\n\u2022 Developed large application for Samsung's Smart TV to create universal platform for Samsung devices including smart phone, tablet, and Smart TV.\n\u2022 Gained extensive research experience on Samsung Smart TV and applied research project patents."", u'Software Engineer\nCiena Corporation - San Jose, CA\nJune 2011 to February 2012\n\u2022 Developed embedded software for Ethernet switches.\n\u2022 Developed, enhanced and maintained layer 2 protocols such as CFM, PBT, LLDP and Vlans as part of the Layer2 Protocols Team.\n\u2022 Gained deep experience with Linux development in C, focus on networking protocols.', u""Software Engineer Intern\nSiemens PLM Software Inc - Cypress, CA\nMay 2010 to January 2011\n\u2022 Developed and maintained NX, the world's leading Computer Aided Design (CAD) software.\n\u2022 Developed on both UI and logic infrastructure layer, using C++ language.\n\u2022 Worked for the Part Modeling Core Architecture team."", u'Research Engineer\nWuhan Ship Communication Institute - Wuhan, CN\nSeptember 2006 to July 2008\n\u2022 Worked on communication protocols, specializing on the MAC (Media Access Control) Layer.\n\u2022 Gained experience in the Navy High Frequency Communication System.']","[u'Masters of Science in Computer Science in Computer Science', u'Bachelor of Engineering in Electronic and Information Engineering in Electronic and Information Engineering']","[u'University of Southern California Los Angeles, CA\nAugust 2009 to May 2011', u'Huazhong University of Science and Technology Wuhan, CN\nSeptember 2002 to July 2006']"
0,https://resumes.indeed.com/resume/f3e5d7bbaf4b7afe,"[u'Data Analyst\nTech Mahindra - IN\nJune 2015 to January 2016\nRoles & Responsibilities:\n\u25cf Created test cases by using MS Excel.\n\u25cf Created test data using UNIX commands.\n\u25cf Worked on SQL scripts to validate the test data.', u'Civil Engineer\nJuly 2010 to December 2013\nRoles & Responsibilities:\n\u25cf Explained drawings to contractor Engineers.\n\u25cf To check the R.C.C. Steel & dimensions of various units.\n\u25cf To check the specifications and levels.\n\u25cf Took the decisions as and when needed on field if the detailed specification were not provided in architectural drawing.\n\u25cf To give the guidance for line out, bar binding & other activities on the field.']","[u""Bachelor's""]",[u'']
0,https://resumes.indeed.com/resume/0e6cafc3ebbba45f,"[u'Network Engineer/Instructor\nTechnical Control Consultants - Jacksonville, NC\nOctober 2012 to Present\n\u2022 Implemented and maintained firewalls, series switches and security appliances.\n\u2022 Ensured network, system and data availability and integrity through preventative maintenance and upgrades.\n\u2022 Recommended network security standards to management.\n\u2022 Monitored network performance and provided network performance statistical reports for both real-time and historical measurements.\n\u2022 Trained junior members of IT team regarding network security and troubleshooting of data circuits.\n\u2022 Managed firewall, network monitoring and server monitoring both on- and off-site.\n\u2022 Resolved virus and malware issues.\n\u2022 Set up equipment for employees, including installing cables and hardware.\n\u2022 Installed software and operating systems on company computers.\n\u2022 Set up and configured hardware and software on company equipment.\n\u2022 Resolved computer hardware and software, printing, installation, word processing, email and operating systems issues.\n\u2022 Communicated with vendors to resolve network outages and periods of reduced performance.\n\u2022 Connected and disconnected wiring, piping and tubing in support of both networking and audio/visual systems.\n\u2022 Upgraded and expanded network systems and their components.\n\u2022 Designed and implemented networks in collaboration with project engineers.\n\u2022 Provided clear, informative lectures in Cisco Networking, Network+, Solarwinds and REDCOM Voice over IP to classes of 20+ students.\n\u2022 Tested students on the materials that were presented in workshops and classes.\n\u2022 Wrote course materials such as syllabi, homework assignments and handouts.\n\u2022 Planned, evaluated and revised course content and course materials.', u'Data Chief\nUnited States Marine Corps - Jacksonville, NC\nJanuary 2001 to September 2012\n\u2022 Supervised the maintenance of three separate networks while in a deployed environment.\n\u2022 Created and maintained networking diagrams which allowed for network optimization, redundancy, and load balancing.\n\u2022 Set up, monitored, and maintained a Solarwinds environment for use in network management.\n\u2022 Designed hardening script for network devices which increased network security.\n\u2022 Developed training curriculum to educate personnel networking fundamentals, digital mathematics, and Cisco IOS navigation.\n\u2022 Trained personnel on basic networking, switching and routing protocols, network design, and wireless point to point links.\n\u2022 Proficient in configuring and troubleshooting both DMVPN and GRE Point to Point tunnels utilizing multiple means of transmission paths, to include FDMA satellite terminals, TDMA satellite terminals, and point to point wireless radio links.\n\u2022 Proficient in the configuration and troubleshooting of switching and routing technologies to include, but not limited to, STP, GLBP, VSS, RIP, EIGRP, OSPF and BGP.\n\u2022 Managed over 20 remote sites, utilizing three separate classification enclaves.\n\u2022 Assisted in the virtualization of deployed servers onto NetApp appliances.\n\u2022 Trained in basic Microsoft Active Directory and Exchange.\n\u2022 Supervised a Helpdesk consisting of 15+ support personnel during multiple simulated deployed exercises.\n\u2022 Supervised and evaluated the performance of multiple platoons consisting of 30+ personnel.\n\u2022 Managed the scheduling and shifts for a Local Area Network, Wide Area Network, and Helpdesk teams consisting of 20+ Marines each']",[u'High School Diploma'],[u'Warren G. Harding High School']
0,https://resumes.indeed.com/resume/f1980b867b213196,"[u'Data Analytics Intern\nSymphony Health Solutions - Philadelphia, PA\nMay 2017 to Present\n\u2022 Analyze claims data in R, SQL. Perform exploratory data analysis and statistical modeling techniques such as regression, decision trees, correlation analysis, ANOVA, clustering techniques and define Source of business metrics.\n\u2022 Perform data analysis using various SQL Analytical functions like LEAD, LAG, ROW_NUMBER, and GROUP BY in Oracle environment.\n\u2022 Develop dashboards and report automation process using VBA and MS Excel to improve turnaround time and reporting capabilities.\n\u2022 Define KPI\u2019s, Source of Business metrics and develop various dashboards in Tableau.', u""Software Engineer\nTATA CONSULTANCY SERVICES (TCS) - Mumbai\nDecember 2014 to April 2016\n- Mumbai, India\n2 years of IT experience in Business Analysis, System Analysis, Data analysis, Software Design, Development, Implementation and Maintenance of Business Intelligence and Data Warehousing Platforms across various domains\nDesigned an application (www.vfsglobal.com) using C# and Asp.net, VBA which provides information about Visa Facilitation Services and effectively managed the database in MS SQL 2008.\nHandled and submitted Change Request's independent, right from developing Understanding Document (UD), getting Client sign off, implementing the Change Request in .NET and SQL.\n\u2022 Designed, Developed and Deployed reports in MS SQL Server environment using SSRS.\n\nProjects:\n\u2022 PetBreedHub: Tools Used Tableau, Google Analytics\nDesigned a website (www.petbreedhub.com) which provides the complete information about pet breeds. The main aim of the project is to track the web data and analyze the user\u2019s behavior by connecting the website to Google Analytics. Developed various analytical reports, dashboards using Tableau features like Parameters, Filters, Sets, Groups, Actions, etc. to present users with various scenarios of predictive analysis.\n\u2022 House Sales Application: | Tools Used: SAS E MINER, R\nDeveloped a model that predicts the Sale price of the house based on 49 different features, performed exploratory data analysis and developed various models using the Random forest, Regularization, Gradient Boosting algorithms.\nR-pubs link: https://rpubs.com/neerajkumar990/272986.\n\u2022 Stubhub Ticket Sales:\nDeveloped a model that predicts conversion rate of users and improve ticket sales using advanced Data mining Random forest algorithm. Conducted data preparation, outlier detection and built the model using R.\n\u2022 Patients Propensity to Diabetes: | Tools Used: R, Tableau\nPredicted high risk of having diabetes and suggest them to take preventive measures. Developed classification models logistic regression, random forest, naive Bayes classifier to predict the probability of diabetes.""]","[u'Bachelor of Science in Electronics and Communication', u'Master of Science in Business Intelligence and Analytics']","[u'Jntu University Hyderabad, Andhra Pradesh\nApril 2014', u""Saint Joseph's University Philadelphia, PA""]"
0,https://resumes.indeed.com/resume/28bab7e69f0d6545,"[u'Graduate Teaching Assistant\nUniversity of South Florida\nDecember 2016 to December 2017\n\u2022 Teaching Assistant for an undergraduate Statistics class of 300 students\n\u2022 Assisted professor in grading papers and assessing projects', u'Data Analyst\nTata Consultancy Services\nSeptember 2012 to April 2015\n\u2022 Performed initial exploratory analysis of large sets of high net-worth and ultra high net-worth customer data using ggplot2 package in R\n\u2022 Prepared reports in Tableau to present the analysis to the business team and client\n\u2022 Cleaned and filtered the data using dplyr package in R.\n\u2022 Built regression models in R to find the significant factors to be considered in order to provide insights to the customers to assist them in making financial decisions.\n\u2022 Helped strengthen customer engagement and improve customer relationship.', u'Systems Engineer\nTata Consultancy Services\nSeptember 2012 to April 2015\n\u2022 Worked on the middle tier of the client application using ASP.Net, C#, WCF and Webservices\n\u2022 Created and enhanced webservices to fetch data from underlying databases']","[u'MS in Management Information systems in Management Information systems', u'BS in Information Technology in Information Technology']","[u'University of South Florida Tampa, FL\nDecember 2017', u'West Bengal University of Technology\nJune 2012']"
0,https://resumes.indeed.com/resume/8700081db3ee9a03,"[u""Data Engineer\nErnst & Young - Bengaluru, Karnataka\nJuly 2015 to May 2017\nBangalore, India\n\u2022 Cleaned, validated and loaded high-volume data from the legacy system to destination for 4 US-based insurance projects.\n\u2022 Collaborated with a team of 4 to extract 10 million raw data and cleared the SIT, UAT stages and went live successfully in just 11\nmonths which cut down the cost of the project by $150,000.\n\u2022 Initiated the automation of the company's internal employee data using BI tools by creating packages that sourced the data from\nMSSQL database hence saving 20 man-hours per week.\n\u2022 Led a team of 2 and upskilled their capabilities in database management and supported implementation which increased the team\nefficiency by 15%.\n\u2022 Contributed as a Business Analyst during resource attrition by working over time to gather data and build data model thereby sustaining the productivity of the project.\n\u2022 Increased knowledge retention by documenting business processes and technical specifications critical for project operations.\n\u2022 Achieved an 'Extra Miler' award for reaching the project milestone well within time and for exceptional client service."", u'Intern\nLarsen & Toubro Infotech Ltd - Mysore, Karnataka\nJune 2014 to July 2014\nMysore, India\n\u2022 Built a circuit which detected the current falling out of prescribed range with the help of comparators, amplifiers and LEDs.\n\u2022 Reduced power wastage by 18% on introducing this circuit in the voltmeters hence saving $5 per voltmeter.\n\u2022 Received a certification of appreciation for successful completion of the project in a month.']","[u'Master of Science in Business Analytics', u'Bachelor of Engineering in Electronics and Instrumentation']","[u'W. P. Carey School of Business at Arizona State University Tempe, AZ\nMay 2018', u'Sri Jayachamarajendra College of Engineering at Visvesvaraya Technological University Mysore, Karnataka\nMay 2015']"
0,https://resumes.indeed.com/resume/ccfd7271ea60c749,"[u""Data Engineer\nCapital One - Plano, TX\nAugust 2016 to Present\nFlagship is a data product built completely in-house that reimagines the way we run our Marketing Campaigns process via AWS Cloud, an easy to use GUI and integration with Salesforce Marketing Cloud while staying true to our well-managed agenda which includes locked down suppressions, reusable components, seamless auditability and approval workflows. It is true agile operating model and speed to market was reduced by 75% and the new campaigns build up time has been cut off by 80%. Hand-offs between multiple teams have been eliminated resulting in increased accuracy and faster execution time.\n\n\u2022 Worked with a team of 5 people in designing, implementing and delivering a complete CRM analytic solution in AWS\n\u2022 Experienced in communicating with business users, other technical teams, and management to collect requirements, describe software product features, and technical designs.\n\u2022 Involved in daily SCRUM meetings to discuss the development/progress and was active in making scrum meetings more productive.\n\u2022 Used Git version control for code sharing in order to save the committed changes that are made to the project files by using different commands.\n\u2022 Developed and designed schema for Job Config DB to maintain and store business rules for all campaigns and the data from the DB is used for generating workflow for spark execution framework.\n\u2022 Contributed for Quantum (Spark Execution Framework-Scala Classes) which is an inner-sourced big data framework that enables users to easily ingest, process, and act on batch and real-time data using open source technologies.\n\u2022 Developed Python based API (RESTful Web Service) to insert, delete and update campaign rules in JOB Config DB using Flask, SQLAlchemy and PostgreSQL. Deployed all the API's on AWS EC2\n\u2022 Automated Testing Framework has been built to comply with CICD pipeline using Nose Test and PyTest.\n\u2022 Python code has been tested for quality using PyLint.\n\u2022 Wrote several Korn Shell Scripts for Schedule, Monitoring and Connecting to Teradata database.\n\u2022 Worked on reading/writing Avro, Parquet and ORC data format files from AWS S3 to AWS EMR for processing using Quantum (Spark Execution Framework).\n\u2022 Created scripts on EMR to pull the scheme for JOB Config DB, executed various scripts as required in sequence with exception handling for processing the input data and stored the required outputs on to AWS S3.\n\u2022 Followed the best practice of Python such as PEP-8.\n\nHammer is an ETL tool that is used for extraction and standardizing the queries for data pull. People with data expertise would upload the queries to solve a business problem and make it available to all users for Hammer. User with a specific business problem can login, search the repository for the required queries and execute/schedule them as required. It has an easy to use interface developed in Angular JS and uses python in the background to design wrapper and execute/package them as required.\n\n\u2022 Performed data extraction and data wrangling using Pandas and Numpy modules in Python.\n\u2022 Used collections in Python for manipulating and looping through different user defined objects.\n\u2022 Possess hands on experience in using PyCharm editor for writing the python scripts which also helped in code analysis, graphical debugging, integrated unit testing etc.\n\u2022 Implemented different python libraries like Numpy, MatPlotLib, SciPy, Scikit-learn in various tasks.\n\u2022 Performed aggregation and missing value imputation using SQL and Python.\n\u2022 Built complex SQL queries, generated reports from different sources for various business needs.\n\u2022 Conducted high level analysis to identify patterns & reported findings to the upper management.\n\u2022 Queried Data from SQL data sources to build visualization and dashboards in Tableau.\n\u2022 Worked with the team in setting up Airflow in distributed mode using Rabbit MQ and Celery for executing the jobs on AWS EC2.\n\u2022 Used Python Jinja templates and PyPDF packages for producing PDF reports on AWS EC2.\n\u2022 Used Airflow scheduler to author workflows as directed acyclic graphs (DAGs) of tasks, executes tasks on an array of workers while following the specified dependencies."", u'Data Analyst/Data Consultant Intern\nARDENT Technologies\nSeptember 2013 to December 2014\nWorked as part of the ETL team that aids in the migration of data from existing systems and unstructured data from web activity/browsing. Worked in building a Hadoop Lake to store/retain the data and coordinated with reporting teams in building dash boards and optimization of HIVE queries.\n\n\u2022 Wrote Python scripts to parse XML/JSON documents and load the data in database.\n\u2022 Used Python to extract information from XML files and wrote Python scripts to clean the raw data.\n\u2022 Developed reports in a drill down mode to facilitate usability and enhance user interaction.\n\u2022 Created Hive queries that analyze the Data and Provide Insights to the Analysts.\n\u2022 Developed MapReduce programs to parse the raw data, populate Hive tables and creating partitioned, managed and external tables.\n\u2022 Executed ad-hoc data analysis using Hive queries to provide delightful insights for data analysts.\n\u2022 Worked on Pig Latin scripts to transform semi-structured data into structured data using Joins and Groups.\n\u2022 Visualized data using BI tools like Tableau creating interactive dashboards and stories.']","[u'Master of Science in Information Systems in Information Systems', u'Bachelor of Engineering in Electronics & Communication in Electronics & Communication']","[u'Harrisburg University', u'JNTU']"
0,https://resumes.indeed.com/resume/38a100061dc99fb8,"[u""Data Analytics Manager\nDeloitte Advisory - Dallas, TX\nJune 2010 to Present\nLed the analytics team on a lending fraud investigation to uncover inappropriate\nloan origination practices and unauthorized transfers of borrower funds. Applied\nadvanced analytics techniques to uncover high risk loans and transactions as well as associated users to markdown activity and focus the investigation on high risk\nareas.\n\u2022 Led the development of a loan review web application for the support of mortgage\nbacked securities litigation. The application used text analytics to categorize loan\ndocuments, a survey module to record responses and a case management. The\nanalysis process included comparing reviewed loans against plaintiff's allegations and executing further analysis at the request of counsel.\n\u2022 Led a group of 5 data analysts to incorporate claim changes received from reviewers and business owners on a large claims administration engagement for a\nmajor airline carrier. I helped develop/maintain claims administration solution that\nwas able to track claim adjudication process for 20K claims and reduce 200 billion in estimated liability for the client. Handled claims review process for claims\naffected by contract negotiations and stipulations.\n\u2022 I served as a key team member of the team that developed a spend analytics\nsolution for a major energy company. The solution offered investigative analytics, a reporting feature for procurement and security business functions to provide\nmulti-million dollar cost savings. The solution had a multi-domain MDM component\n\nwith built-in data quality capabilities to extract and process data from structured and unstructured data sources and present it to end-user via portals and reports. I\nhelped develop a custom built tool to identify duplicate/fraudulent invoices/sub- invoices and payments, which also had the ability to track the recovery process and trained and provided support to the client after solution was deployed.\n\u2022 Found data analysis inconsistencies in the plaintiff's expert report on a healthcare\nlitigation engagement whereby reducing opposing party's loss claims by 65%($ 30\nmillion).\n\u2022 I was member of a team in assisting large companies to proactively identify fraud and internal control weaknesses. My responsibilities included data acquisition, data\nvalidation, data mapping, technical review of tests to be run, and forensic review of test results.\n\u2022 I have performed analysis on client, claimant, policy and agency data points to uncover issues related to worker's compensation claims. Also responsible for\npresenting the findings and recommendations to the client."", u'Data Engineer\nNokia - Irving, TX\nFebruary 2010 to May 2010\nQuality tested upcoming mobile devices for North American market for software and hardware integrity which included call tracing and network operation. I\nperformed data analysis on software failure rates across multiple hardware\nplatforms. Collaborated with hardware/software engineers to carry out test sets through Mercury Quality Center. Maintained relationships between internal Nokia\ncustomers and external customers including AT&T Mobility and T-Mobile.\nInstall Developer | IBM Global Services, Bank of America | April 2007 - Dec 08\n\u2022 Responsible for gathering business requirements scripting packaged application into MSI, Transforms, Merge Modules, and Patches (MSP) using Admin Studio as required and added custom actions using Vbscripts and C++ as required. I\nprovided application owners with the cross-reference reports after deployment and provided 3rd Level Support when required']",[u'Bachelors in Information Systems'],"[u'UNIVERSITY OF TEXAS AT ARLINGTON Arlington, TX\nDecember 2009']"
0,https://resumes.indeed.com/resume/ce58b66457e45773,"[u""Consultant\nCitigroup\nJanuary 2016 to July 2017\n* Actively led the team for migration of recruitment reporting platform from SAP BO to Oracle Business Intelligence\n* Part of Citi's global business analyst team for Human resources migration project from SAP BO to MicroStrategy (MSTR)\n* Used Scrum for agile development and identified requirements resulting in decrease of gathering time by 20%\n* Investigated and managed technical issues post project migration which led to efficiency improvements of up to 30%\n* Performed ad-hoc data analysis using Python and Excel to drive business insights on trends related to compensation and appraisal for Senior HR Officials, resulting in stronger use of department's capital allocation"", u'Consultant\nDeloitte Consulting\nMay 2015 to December 2015\nPartnered with the pre-sales team by designing appropriate reporting model for a prospective client using MSTR and D3\npackage on Hortonworks Hadoop, ultimately contributing towards securing client for the organization\n* Assists in gathering client information to create business requirements and business process documents\n* Streamlined update process for a client by writing SQL procedures, functions and creating process documents', u'Senior Data Analyst\nThird I Inc\nJune 2014 to May 2015\n* Applied analytical skills using Tableau and SQL for data analysis and reporting on financial metrics related to accounting\n* Developed the working capital Dashboard on (MicroStrategy) and HTML to provide analysis of public over iPad\n* Upgraded and tuned the database model from star to snowflake schema resulting in faster ETL and query performance', u""Software Engineer\nGEP\nJuly 2012 to June 2014\n* Customized Business Intelligence tool to create reports and scorecards for comparative spend analysis using MSTR\n* Contributed towards the database design for higher scalability over Amazon cloud with over 244 Million transactions\n* Provided unique insights and developed dashboards utilizing SQL and MicroStrategy over GEP's product\n* Collected and analyzed complex data for reporting and/or performance (trend) analysis""]","[u'Master of Information Technology and Analytics in Business Analytics Programming', u'Bachelor of Engineering in Computer Engineering in Computer Engineering']","[u'Rutgers Business School New Brunswick, NJ\nDecember 2018', u'University of Mumbai Mumbai, Maharashtra\nJuly 2012']"
0,https://resumes.indeed.com/resume/fa15d99bf7cadafe,"[u""DATA ENGINEER\nENTERPRISE DATAWAREHOUSE - Sunnyvale, CA\nJuly 2017 to Present\n\u2022 Built multiple self-service ETL pipelines to transfer data between multiple data platforms in multiple data centers to facilitate data analysis and avoided recurring development effort.\n\u2022 Improved data quality and data availability in data reporting layer and staging area by designing and building ETL monitoring solutions in one of the biggest enterprise data warehouse in world with exceptionally large data volume.\n\u2022 Deployed metadata driven framework to meet immediate data need of Data Scientists to discover data patterns to combat sudden onset of fraud attacks and system abuses.\n\u2022 Optimized data ingestion of largest table in Apple Teradata EDW with Tera bytes of data feed flowing per day.\n\u2022 Worked with data scientists to improve accuracy of fraud detection data models by effective data profiling.\n\u2022 Optimized Ad-hoc computation of feature's and label's owned by analytic insight team and migrated into process oriented semantic layers with documentations with high scalability.\n\u2022 Working in a fast-paced agile development environment to quickly analyze, develop, and test potential use cases for the business.\n\u2022 Partnered with Apple Data Insight team and built multiple scalable semantic layers for data scientists to build data model for detection of suspicious transactions, training data models and for extracting SQL rules to feed fraud decisioning engine.\n\u2022 Provided round the clock support for Analytic Insight team during onset of sudden fraud attacks and system abuses.\n\u2022 Experience in data interpretation to draw conclusions for senior management and drive key strategies.\n\u2022 5+ years of experience in Fraud Analytics domain and knowledgeable on data science algorithms, predictive analytics, statistical computing etc."", u'System Analyst\nINTEGRATED FRAUD PLATFORM - San Francisco, CA\nNovember 2011 to January 2016\n\u2022 Involved in documenting business and functional requirements, communicating effectively with upper management, developers and QA engineers.\n\u2022 Understand and analyze business requirements.\n\u2022 Perform impact analysis on the present system and design the flow.\n\u2022 Played a crucial role as implementer for setting up Actimize (AIS and RCM)\n\u2022 Assist in the Performance testing of the new code in terms of environments preparation.\n\u2022 Work to achieve a more stable state for the Application in the Production Environment and automate independent components wherever possible to reduce human interface.\n\u2022 Prepare SIT and UAT Test plans and Test Scripts.\n\u2022 Coordinate with Business Users in understanding the requirements and assisting them understand the solution.\n\u2022 Extract data from multiple sources, manipulate data and perform data validations. Performed data analysis, and troubleshoot data and/or system issues within the Data Warehouse environment as well as upstream systems, as needed.\n\u2022 Performed the data validations and control checks to ensure the data integrity and consistency.\n\u2022 Wrote queries on Hive, Impala, Cassandra for analyzing data for any user Issues or any data discrepancy.\n\u2022 Have extensively worked in developing ETL program for supporting Data Extraction, transformations and loading using Informatica Power Center.\n\u2022 Created UNIX shell scripts to run the Informatica workflows and controlling the ETL flow.\n\u2022 Used BTEQ scripts, Teradata utilities fastload, multiload, tpump to load data.\n\u2022 Wrote, test and implement UNIX scripts\n\u2022 Wrote views based on user and/or reporting requirements.\n\u2022 Wrote Teradata Macros and used various Teradata analytic functions.\n\u2022 Performance tuned and optimized various complex SQL queries.', u'DATA ENGINEER\niTUNES CUSTOMER ANALYTICS - Noida, Uttar Pradesh\nNovember 2005 to October 2011\nAPPLE INC. Noida, INDIA and Cupertino, CA\n\n\u2022 Involved in the high level design of the EDW data model for iTunes Customer Analytics.\n\u2022 Actively involved in the development for migrating the application form Oracle to Teradata and was responsible in gathering requirements at onsite.\n\u2022 As part of EDW maintenance team worked on enhancements, deployments and closing critical issues.\n\u2022 As a team lead involved in reviewing of design, development of all enhancements.\n\u2022 Have been involved in the performance improvement, by analyzing the long running jobs and optimizing them by doing some functional modifications.\n\u2022 Participated in meetings with Users, Business analysts in understanding the business requirements and analyzed use cases to develop detailed Test Plan\n\u2022 Used Teradata utilities fastload, multiload, tpump to load data\n\u2022 Wrote BTEQ scripts to transform data\n\u2022 Wrote Fastexport scripts to export data\n\u2022 Wrote, tested and implemented Teradata Fastload, Multiload and Bteq scripts, DML and DDL.\n\u2022 Wrote, test and implement UNIX scripts\n\u2022 Wrote views based on user and/or reporting requirements.\n\u2022 Wrote Teradata Macros and used various Teradata analytic functions.\n\u2022 Performance tuned and optimized various complex SQL queries.\n\u2022 Worked on data warehouses with sizes from 30-50 Terabytes.\n\u2022 Coordinated with the business analysts and developers to discuss issues in interpreting the requirements.\n\u2022 Generated reports in BO XI for users for analysis.', u'SOFTWARE ENGINEER, DWH\nTATA TELE SERVICES - Hyderabad, Telangana\nDecember 2004 to October 2005\nHyderabad, INDIA\n\n\u2022 Involved in the low level design of the Data warehouse data model for the Tata Tele Services enterprise.\n\u2022 Actively involved in the development of ETL mappings as per specifications using OWB.\n\u2022 Played vital role in developing reports using Oracle Discoverer.\n\u2022 As part of DWH maintenance team working on enhancements of mappings, closing critical issues.\n\u2022 During the part of ETL development wrote complex SQL, PL/SQL procedures.']",[u'in Business'],"[u'C V Raman College Of Engineering (Biju Patnaik University) Raman, Punjab\nJanuary 2003']"
0,https://resumes.indeed.com/resume/5446644f818ff9bb,"[u'Data Scientist\nSAPIENTRAZORFISH - Boston, MA\nMarch 2017 to Present', u'Graduate Assistant\nColorado State University - Fort Collins, CO\nJanuary 2016 to December 2016\n\u2022 Implemented an ETL pipeline to ingest, parse, and intelligently load and classifying twitter real time Twitter data.\n\u2022 Sentiment analysis of social media streams, product reviews, etc. for customer feedback and market insights using\nnatural language processing.\n\u2022 Using Hive to do transformations, joins, filter and some aggregations before storing the data onto HDFS.\n\u2022 Creating end to end Spark applications using Scala to perform various data cleansing, validation, transformation and summarization activities according to the requirement.\nSkillset: MySQL, Python (Numpy, Pandas, Scikit), Storm, Amazon S3, EC2, Spark', u""Data Science Intern\nJune 2016 to August 2016\n\u2022 Developed a predictive model to determine the conversion rate for email campaign and came up with recommendations for the marketing team to improve conversion.\n\u2022 Build model for detecting fraudulent activities hidden in terabytes of real-time data.\n\u2022 Built personalized product recommendations for a financial institution based on customers' behavior data.\nSkillset: Python, machine learning, decision trees / random forest, Logistic Regression, statistical techniques"", u'Software Engineer\nTATA Consultancy Services - Hyderabad, Telangana\nJuly 2013 to December 2014\n\u2022 Developed full-fledged REST web services for the project.\n\u2022 Worked on internal data infrastructure projects using SQL and Hive\n\u2022 Worked extensively in creating MapReduce jobs to power data for search and calculate variance.\nSkillset: Java, REST, spring, Hadoop, MapReduce, Hive, data cleaning, SQL']","[u'Masters of Science in Computer Science', u'Bachelors of Engineering in Computer Science']","[u'COLORADO STATE UNIVERSITY Fort Collins, CO\nDecember 2016', u'OSMANIA UNIVERSITY Hyderabad, Telangana\nJune 2013']"
0,https://resumes.indeed.com/resume/910eb79be0959a10,"[u'Data Engineer\nBritish Petroleum - Chicago, IL\nJuly 2016 to Present\n\u2022 Developed end-to-end data pipelines which include data extraction, data ingestion, data blending, publishing data on tableau server, and automation of pipelines under fast paced Agile Scrum environment.\n\u2022 Extracted Master Data from RDBMS and injected to the Data Lake as required.\n\u2022 Wrote SQOOP scripts for moving data between Relational DBs, HDFS, and S3 storage and automated the workflow using Oozie.\n\u2022 Developed the XSLTs to handle the Translations as required by business and thus converting the XMLs to CSVs.\n\u2022 Developed an ETL algorithm using Java & pig scripts to handle all the noise to meet the business standards thus generating the flat files.\n\u2022 Developed Hive pipelines in Data lake by implementing Partitioning, and bucketing concepts for improving performance.\n\u2022 Developed and Improved performance of Spark Pipelines by implementing repartition to manage resources efficiently.\n\u2022 Developed the pig scrips for the Data Check quality\n\u2022 Developed Tableau dashboards by leveraging published data sources from tableau server.\n\u2022 Performed Tableau Server administrative tasks by creating AD groups, managed subscriptions for the views, adding users, managing security, and tracking performance of dashboards.\n\u2022 Designed architectural view for Tableau Server Cloud Migration and worked on migrating data sources, workbooks, subscriptions by following best practices.\n\u2022 Worked on gathering requirements from business and designing architectural view for the use cases.', u'Data Engineer\nANConnect - Dallas, TX\nOctober 2014 to July 2016\n\u2022 Developed the pipelines which handles all the meta data/Images for the music, movies and books to the all the retail giants (Walmart Labs) in the industry.\n\u2022 Extracted the Data from AS400 as required using the SQOOP and injected into HDFS.\n\u2022 Handled all the translations using the ETL Pentaho Kettle\n\u2022 Involved in developing the RESTFul Services for interacting with Alfresco documentation server thus retrieving the images as required.\n\u2022 Automated the jobs with feeds as required with the feeds sending to the clients.\n\u2022 Used Pig for data cleansing and Hive for developing various View as required by business.\n\u2022 Developed scrips & Batch Jobs to schedule various Jobs as required\n\u2022 Used Jenkins to deploy the jobs in to the cluster.\n\u2022 Involved in production support and enhancement development.', u'Data Engineer\nAnderson Merchandisers - Dallas, TX\nMay 2013 to October 2014\n\u2022 Developed sqoop import and export scripts to handle incremental loading.\n\u2022 Created tables, loaded values using table data in Hive.\n\u2022 Used spark with YARN for Map Reduce programs for better performance.\n\u2022 Created Pig scripts and ran it on Tez instead of YARN.\n\u2022 Created Oozie workflows to upload the data to HDFS and run Hive QL analysis.']","[u'M.S. in Computer Science', u'Bachelors in Electrical & Electronics Engineering']","[u'Texas A&M University Kingsville, TX\nSeptember 2011 to January 2013', u'Jawaharlal Nehru Technological University Hyderabad, Telangana\nSeptember 2007 to May 2011']"
0,https://resumes.indeed.com/resume/ce893ada69b2d2f9,"[u'Data Analyst\nUT Austin Libraries\nMay 2017 to December 2017\n\u2022 Help with data gathering and analysis for the continuing resource renewals as well as data entry and processing of ebook and serial data into the electronic resource management system, Intota\n\u2022 Main duties include automating the cost upload process of the Reconciliation project, creating visualizations and scraping of records from the online databases.', u'Senior Software Engineer\nCapgemini India Private Limited\nJuly 2014 to July 2016\n\u2022 Development and fexing defects of minor interfaces using Eclipse for integration.\n\u2022 Database Monitoring Activities, Database Cloning and backup operations.']","[u'Master of Science in Information Studies', u'Bachelor in Engineering']","[u'University of Texas at Austin Austin, TX\nAugust 2016 to May 2018', u'Dwarkadas J. Sanghvi College of Engineering, University of Mumbai Mumbai, Maharashtra\nSeptember 2010 to May 2014']"
0,https://resumes.indeed.com/resume/5953ea46840e9a1f,"[u'Customer Data Analyts\nState Bank of India - Tamil Nadu\nMay 2015 to August 2016\n\u2022 Compiled a list of customer complaints in the region from various sources, conducted a business needs analysis to address issues and generated useful conclusions and actionable recommendations\n\u2022 Analyzed customer records to facilitate contests and promoted banking schemes to various customer segments which increased the region\u2019s business by 47%. Extensively used advanced Excel such as pivot table/charts and V-lookups.\n\u2022 Administered change management during the implementation of Queue Management System (QMS) and decreased wait time of customers by 56%. Reported metrics and productivity on a month to month deviation\n\u2022 Led a team of six Junior Analyst Trainee for carrying out footfall analysis to reduce customer complaints with the focus on reducing customer wait times and increasing cross and up-selling business', u'System Engineer\nInfosys - Tamil Nadu, India\nAugust 2011 to February 2013\n\u2022 Performed custom-configuration of parameters in Finacle 10.x platform that define business rules pertaining to currencies, interest rate calculation, pricing, and penalties\n\u2022 Designed front-end user interface for increased usability and user-friendly experience\n\u2022 Executed SQL queries during Unit testing for data extraction and modification to perform database update operations\n\u2022 Conducted workshops for the bank\u2019s core team detailing features presented by the new version and implementation of Finacle', u'Customer Data Analyst\nAs a Customer Data Analyst, I performed the role of a business analyst, a data analyst and a customer representative. I acquired the experience of brain storming and interviewing customers and proposed strategies to solve their problems. Implementing queue management system is one of the projects I was involved and utilized the advantage of high customer footfall for increasing cross-selling and up-selling business.']","[u""Master's in Information Systems"", u""Bachelor's in Biomedical""]","[u'University of Texas at Arlington\nAugust 2016 to December 2018', u'Anna University Tamil Nadu\nAugust 2007 to May 2011']"
0,https://resumes.indeed.com/resume/fa40d5139954346a,"[u'Big Data Engineer\nAAA - Costa Mesa, CA\nFebruary 2018 to Present', u'Research Assistant\nCalifornia State University\nJune 2017 to September 2017\nFullerton\nFunded by Raytheon to develop proprietary SDK for LiDAR environment sensing hardware.\n\u2022 Lead team of 8 graduate and undergraduate students in twice-weekly meetings, utilizing Agile\nmethods for more structured team organization.\n\u2022 Imported MATLAB computer vision module to visualize spin images for surface representation.\n\u2022 Used C++, Bash, Python, and RobotOS technologies to build robust app with additional user\nscripts.\n\u2022 Distributed and maintained Ubuntu 14.04 images to team for workspace consistency.', u'Software Engineer II\nAmphenol TCS\nJune 2014 to December 2015\nWrote data cleaning software for large database of circuit simulation objects for use in Ansoft\nHFSS.\n\u2022 Refactored and translated Visual Basic code in real time with a Python app.\n\u2022 Translated 2,000 classes and their respective syntax for a 60% speedup in compilation time.\n\u2022 Embedded functionality into spreadsheets using Visual Basic for ease of use.\n\u2022 Worked remotely with team of 4 colleagues.\n\nProjects']",[u'Bachelor of Science in Computer Science'],[u'California State University\nAugust 2015 to December 2017']
0,https://resumes.indeed.com/resume/38a98b553e1a3f4a,"[u'ETL&BI Engineer\nGenomic Health Inc. - Redwood City, CA\nJanuary 2018 to Present\nTools/Environment: SSIS, SQL Server, ER Studio, Erwin, Tableau, Power BI, Microsoft Azure\n\nRelated Tasks, Responsibilities and Actions:\n\u2022 Responsible for BI Enterprise data warehousing ETL architecture, design and development\n\u2022 Studied in-house requirements for the Data warehouse to be developed\n\u2022 Conducted one-on-one sessions with business users to gather data warehouse requirements\n\u2022 Analyzed database requirements in detail with the project stakeholders by conducting Joint Requirements Development sessions\n\u2022 Create/ implement operational data store(ODS) and Enterprise data warehouse (EDW) in Microsoft SQL server, design and ETL Strategy to Build, Develop and Load Data from Salesforce, NetSuite, and other various OLTP Source Systems.\n\u2022 Generated ad-hoc SQL queries using joins, database connections and transformation rules to fetch data from legacy DB2 and SQL Server database systems\n\u2022 Extracted data from the databases using SSIS to load it into a single data warehouse repository.\n\u2022 Design and develop various application specific Tableau/SSRS/Power BI reports for multiple Business Units like Finance, Executive, Commercial operations, Lab Operations etc.\n\u2022 Created POC s to migrate data into Microsoft Azure cloud and build Tableau /Power BI reports on top of it.\n\u2022 Integrated the work tasks with relevant teams for smooth transition from testing to implementation', u""Technology Lead/Data Modeler/BI-Datawarehouse Engineer\nBank of America - Infosys Limited - Chennai, Tamil Nadu\nAugust 2016 to June 2017\nTools/Environment: SSIS, SQL Server, ER Studio, ER-Win, Oracle, Tableau, Power BI\n\nRelated Tasks, Responsibilities and Actions:\n\u2022 Played the role of a Program Technical Lead/ETL Architect and was responsible for Data Integration from source systems into Operational Data Store-ODS to Data warehouse.\n\u2022 Facilitated JAD sessions for requirements gathering, creating data mapping documents, writing functional specifications and acted as a liaison between the business and development teams.\n\u2022 Analyze Different Data Sources like SQL Server, Oracle and Flat files and involved in the data profiling activities for the column assessment and natural key study.\n\u2022 Developed Conceptual/Logical/ Physical Data Models, Data Dictionary using EMBARCADERO ER Studio tool for integration of business rules into database tables, normalization and de-normalization for optimization and established referential integrity of the system.\n\u2022 Worked in Agile Scrum methodology and optimized performance in relational and dimensional database environments by making proper use of indexes and partitioning techniques.\n\u2022 Involved in ETL code to meet requirements for Extract, transformation, cleansing and loading of data from source to target data structures.\n\u2022 Worked on POC for developing Tableau Dashboards for performance and KPI's of location.\n\u2022 Tuning complex queries for better performance/efficiency and to meet business requirements."", u'Technology Analyst /Data Modeler/BI Specialist\nNationwide Building Society - Swindon, Wiltshire,United Kingdom\nMarch 2015 to July 2016\nTools/Environment: T-SQL, PL/SQL, SQL Profiler, Report Builder, SSIS, SSRS, Rational Software Architect, ER Studio\n\nRelated Tasks, Responsibilities and Actions:\n\u2022 Gathered Business requirements by organizing and managing meetings with business stake holders, development teams and analysts on a scheduled basis.\n\u2022 Designed the data flow from source systems to SQL server tables, technical specification documents for ETL processing into the data warehouse using Rational Software Architect.\n\u2022 Normalized the incoming files to 3NF before loading into facts and dimension tables.\n\u2022 Conducted design reviews with BA/content developers to create POC for reports.\n\u2022 Created database objects - configuration tables, indexes, views, stored procedures and user defined functions in SQL Server Management Studio to support SSIS process workflow.\n\u2022 Involved in maintaining and improving ETL processing efficiency for improved data quality, impact analysis and data validation.\n\u2022 Configured with checkpoints, audits, package logging, error logging and event handling to redirect error rows and fix the errors in SSIS.\n\u2022 Worked closely with the application developers to explain the database design and the complex Data Transformation logic.\n4. Company: Infosys Limited', u'Senior Systems Engineer/Data Warehouse Engineer\nDepository Trust and Clearing Corporation - Chennai, Tamil Nadu\nJuly 2012 to February 2015\nTools/Environment: SSIS, SSAS, SSRS 2008R2, Informatica, PPS 2010, Cognos\n\nRelated Tasks, Responsibilities and Actions:\n\u2022 Gathered and analyzed business requirements through continuous discussions with the business analysts, data analysts, and performed GAP analysis of the business rules, user administration and requirements in agile scrum project methodology.\n\u2022 Designed the ETL framework from scratch with multiple data sources (Flat File, Excel, Oracle and SQL) and designed reporting solutions.\n\u2022 Designed and developed complex packages, mappings to move data from multiple sources into target area such as Data Marts and Data Warehouse in SSIS & Informatica.\n\u2022 Worked with Database Administrators, Business Analysts and Content Developers to conduct design reviews and validate the developed models\n\u2022 Involved in complete Software development life cycle SDLC of the project including Analysis, design, develop, deployment, test, implement and production.\n\u2022 Wrote SQL Queries, SQL Stored Procedures, Views, Functions as needed.\n\u2022 Experienced in writing Parameterized Queries for generating Tabular reports, Sub reports using Global Variables, Expressions, Functions and subtotals for the reports using SSRS.\n\u2022 Coordinated cross-functional strategic business teams focused on improving product quality between the team at New Jersey (headquarters) and the development team in Chennai.', u'Systems Engineer/Data Warehouse Engineer\nAmeriprise Financial Inc - Chennai, Tamil Nadu\nJune 2010 to June 2012\nTools/Environment: Informatica, SQL Server, UNIX platform, IBM TWS, MS-BI (SSIS & SSRS)\n\nRelated Tasks, Responsibilities and Actions:\n\u2022 Member of the warehouse design team, assisted in creating fact and dimension table implementation in Star Schema model based on requirements.\n\u2022 Performed Data analysis, data migration by transferring them using SSIS packages.\n\u2022 Developed mappings in Informatica and Extensively used Transformations like Router, Aggregator, Normalizer, Joiner, Expression, Update strategy, Sequence generator and SP.\n\u2022 Performance & Tuning of Existing Mappings and Workflows in Informatica.\n\u2022 Performed efficient tuning of SQL source queries for data load and used stored procedures.\n\u2022 Created Parameterized, Drill down and Drill through reports, cross tab reports, Sub Reports, Summary Reports and charts using variables and expressions.\n\u2022 Resolved data issues, answered user queries data issues and assisted the QA team in designing the test plan and test cases. Performed User Acceptance Testing.']","[u""Master's""]",[u'']
0,https://resumes.indeed.com/resume/8eda27a3cafe9ca2,"[u""Data Analyst (Consultant from Libsys)\nRedbox Automated Retail, LLC - Oakbrook Terrace, IL\nJune 2017 to Present\n\u2022 Extract, validate data from data sources like XML, CSV, Excel using SQL, T-SQL queries and generate ad-hoc reports in SSRS leading to decrease in inconsistent data received from third-party vendors by 70%\n\u2022 Perform SQL based analysis on the title data in order to derive insights to manage content on different platforms\n\u2022 Utilize tableau for generating visualizations, dashboards to identify weekly rentals, new KPI's and campaign efficiency\n\u2022 Create JIRA tickets for data issues, own its communication, issue resolution and management\n\u2022 Work with team to formulate data quality improvement strategies, change process flow and document it for team manager"", u'Research Associate\nUniversity of Illinois - Springfield, IL\nJanuary 2016 to January 2017\n\u2022 Created SQL scripts to analyze social-network dataset of students to predict student groups linked with each other\n\u2022 Analyzed datasets by implementing different classification techniques to predict the best classifier which retains data accuracy in spite of noisy instances. Leveraged data mining tool WEKA to perform analysis\n\u2022 Prepared student data and performed analysis on the data in python using Jupyter notebook. Helped to answer questions like which students are more likely to join entrepreneurship club or human resource management groups and accordingly design strategies for both the clubs to increase student involvement\n\u2022 Developed a prediction model in python 3.5 to determine whether a customer will retain the cable subscription service of the company. Implemented the random forest classifier on customer data with 91% accuracy\n\u2022 Calculated the most cited papers by finding the page rank from a citation dataset. Used map reduce program written in Java and hive scripts to rank the papers. Also, deployed the dataset on AWS (EMR) to generate similar results.', u'Software/ Data Engineer\nTech Merchant IT Solutions Pvt Ltd - Mumbai, Maharashtra\nDecember 2014 to July 2015\n\u2022 Extensively collaborated with team members for requirements gathering, gap analysis, data analysis and support tasks following scrum methodology\n\u2022 Designed and implemented database solutions for web based application in PHP along with creating/updating database objects like views and stored procedures\n\u2022 Involved in monitoring and tuning of SQL queries using SQL profiler, tuning advisor. Created indexes and stored procedures to optimize performance of TSQL queries raising query efficiency by 15%\n\u2022 Utilized Microsoft BI tool( SSIS packages) for data transformation, data mapping using conditional splits, pivots\n\u2022 With assistance, learnt and implemented backup, recovery of SQL server databases along with high availability strategies']","[u'Masters in Computer Science in Computer Science', u'Masters in Computer Applications in Computer Applications', u'Bachelor of Science in Computer Science in Computer Science']","[u'University of Illinois at Springfield Springfield, IL\nDecember 2016', u'University of Mumbai Mumbai, Maharashtra\nJune 2015', u'University of Mumbai Mumbai, Maharashtra\nApril 2013']"
0,https://resumes.indeed.com/resume/e629ceae724ac946,"[u'Data Analyst Intern\nAcuty LLc - San Francisco, CA\nSeptember 2017 to Present\nUse Excel and Rapidminer to perform data analysis while developing key data analyst skills, such as translating hard data into viable trends, reports, and summaries to aide business professionals in making strategic decisions. Perform key research and analysis for an enterprise software company focused on helping procurement professionals streamline the contract and vendor/customer agreement process.', u'Data Analyst Intern\nFang88.com, Inc. - Palo Alto, CA\nJune 2017 to September 2017\nPerformed research and analysis of financial and sociological data to accurately predict housing prices in volatile Northern California markets with complex and nuanced socioeconomic and financial factors; leveraged Excel and Python Tensorflow to predict the pricing data across multiple local geographic territories.', u'Senior Software Engineer\nTCSOFT - Shanghai\nAugust 2014 to December 2014\nPrimary engineer for validating and maintaining medical software applications, including SaaS and data analytics leveraging corporate medical data.', u'Systems Engineer\nNokia-Alcatel - Shanghai\nJuly 2009 to September 2013\nConceptualized, validated, maintained, and integrated terminals across multiple platforms with broad exposure to multiple switches and servers. Oversaw team members, assigned tasks, and directed corporate DECT GAP configuration and validation; SME regarding corporate terminals and OXE.\n\u2022 Quality and Efficiency Improvements: Validated all types of Alcatel-lucent terminal (wire or WiFi) and set up QA network environment, network trouble shooting about many kinds of switches, maintain the configuration.\n\u2022 Recognitions: Specially recruited for CCNA training and recipient of \u201cOutstanding Employee\u201d award.', u'Validation Engineer\nNokia-Alcatel - Shanghai\nJune 2006 to June 2009\nOversaw all aspects of OmniTouch Unified Communications Application Suite (OTUC) support and administration with significant oversight of specification validation, including installation, configuration, software validation, and testing.\n\u2022 Quality Assurance: Performed extensive QA throughout various versions of OTUC, set up the configuration of OXE and OTUC, performed regression/performance/auto/manual testing.', u'Software Engineer Intern\nNokia/Alcatel-Lucent - Shanghai\nOctober 2005 to June 2006\nBuilt corporate web site content regarding APAC Enterprise Engineering and Development Center, including coding for Holiday Management and oversight of PHP software and stationary.']","[u""Master's in Computer Science ,"", u'Bachelor of Science in Computer Science']","[u'Louisiana Tech University Ruston, LA\nSeptember 2015 to February 2017', u'UNIVERSITY OF SHANGHAI Shanghai\nSeptember 2002 to June 2006']"
0,https://resumes.indeed.com/resume/588711e9c063a606,"[u'Project Engineer / Data Analyst\nBlue Earth Solar - San Diego, CA\nMarch 2015 to March 2016\nResponsible for O&M activities including plant performance evaluation and reporting\n- Provided daily system monitoring and analysis\n- Negotiated annual O&M services including reactive and preventative maintenance\n- Coordinated third party O&M groups during preventative and reactive maintenance\nactivities\n- Evaluated system production vs. performance guarantee agreement. Successfully\nmitigated contract implications by detailed analysis of system underperformance.\n\u2022 Created and validated an O&M cost model\n\u2022 Supported Business Development ventures in US and Mexico (~450 MWdc)\n\u2022 Provided support and technical assistance to Project Management during project engineering\nphases (55 MWdc single axis tracking site layout optimization, production forecasting, elevation\nanalysis, pile optimization, etc.)\n\u2022 NRG Project: Lenape II, 4.7 MWdc rooftop and carport project in Indianapolis, IN\n- Provided on site engineering, quality assurance, construction, and commissioning oversight\n- Responsible for Capacity Test data analysis and reporting', u'O&M Data Analyst\nSoitec Solar - San Diego, CA\nJanuary 2014 to February 2015\n\u2022 Responsible for US Solar power plants performance and reporting\n\u2022 Responsible for daily monitoring of all US sites to identify, confirm and prioritize\npotential underperformance issues\n\u2022 Performed statistical analysis of all variables to establish a basis for a mathematical\nmodel to optimize site peak performance while keeping expenses under control\n\u2022 Carried out PVsyst calculations to validate measured energy against theoretical (simulated) energy\n\u2022 Created an in-house Alarm System to detect and validate unexpected anomalies in the field to guarantee a quick and cost effective response\n\u2022 Established San Diego Remote Operation Center business plan\n\u2022 Attended international coordination meetings in South Africa and Germany\n\u2022 3 months of field experience in solar power plant commissioning and troubleshooting', u'Project Engineer / Loads Engineer\nAREVA Wind - Bremerhaven\nJanuary 2012 to August 2013\n\u2022 Established and coordinated pre-project development plans, including definition of scopes of work, project schedule, cost assessment and risk analysis\n\u2022 Responsible for the execution and performance of a complete feasibility study of an innovative\nTransport and Installation concept, from harbor to site of deployment.\n\u2022 Provided daily engineering support and coordination between customer and internal\ndepartments, especially, Sales, Logistics and Marketing departments\n\u2022 Identified wind turbine substructure-foundation requirements for new areas of deployment, based on deep evaluation of wind, wave and soil conditions.\n\u2022 Performed calculations in GL-Bladed to guarantee proper static and dynamic structural responses for offshore wind structures based on detailed evaluation of wind, wave and soil conditions', u'Data Analyst / Project Engineer\nFloat Inc - San Diego, CA\nMarch 2010 to January 2012\nAssisted in design, analysis, and implementation of a new Offshore Ocean Energy System\ncombining Wave Energy Converters (WECs) and Offshore Wind Turbines in a very large\nfloating platform\n\u2022 Carried out hydrodynamic analysis, wave tank test data analysis, and economic optimization\n\u2022 Developed a MATLAB in-house calculation tool to optimize size and energy production\n\u2022 Performed statistical analysis from wave and wind resources.', u'Data Analyst / Project Engineer\nMontoliu & Associates Engineering - Valencia, Valencia\nOctober 2009 to March 2010\n\u2022 Collaborated with a project team to develop wind energy projects in Spain\n\u2022 Performed feasibility studies of high-speed wind areas. Carried out wind data analysis\n\u2022 Conducted wind turbine layout optimization, including electrical grid design\n\u2022 Became familiar with funding processes and construction permitting for wind energy projects', u'Master\'s Thesis\nWater Research Laboratory (WRL), UNSW - Sydney NSW\nSeptember 2008 to March 2009\nMaster\'s Thesis ""Attenuation of Waves""\n\u2022 Investigated the attenuation of surface waves. Performed a wave tank experiment analyzing\nattenuation of waves caused by rain turbulence\n\u2022 Gained experience in design, fabrication and testing of experimental models, laboratory\nmeasurement techniques, and data analysis\n\u2022 Co-authored ""Rain-induced attenuation of deep-water waves"" - accepted for publication in Journal of']","[u'M.S. in Mechanical Engineering', u'B.S. in Mechanical Engineering']","[u'Polytechnic University of Valencia Valencia, Valencia\nJune 2010', u'Polytechnic University of Valencia Valencia, Valencia\nJune 2007']"
0,https://resumes.indeed.com/resume/064a9adf1224dfc8,"[u""Data Engineer\nThe Philadelphia Inquirer\nAugust 2016 to Present\no Designed the data model and defined the 360-degree view of the customer\no Moved data from legacy platforms to the cloud using ETL. Built data pipelines to stream data from clickstream analytics,\nsubscription, circulation, newsletters, commenting platforms into the data warehouse\no Wrote efficient queries to retrieve reports from Oracle and BigQuery\no Performed Statistical Analysis on ad supply chain and came up with revised sales target\no Surfaced clickstream data from data warehouse to Tableau and created segments, dimensions, calculated metrics for analytics and insights\no Redesigned supply chain management of a leading media company by recommending Service Level Agreements (SLAs) from customer's perspective based on various KPIs - Regular readers, High performance reader\no Built automated decision boards and reports and for monitoring the performance of initiatives and campaigns using\npython, bokeh, seaborn and Flask\no Discussed the data analytics strategy with different Media Publishing Companies including The Washington Post, The\nChicago Tribune, The New York Times, The Hearst\no Supported Lenfest Institute for its initiatives in Subscriber Analytics and led the conversation with prospective research\ngroups and defined clear goals and outcomes\no Evaluated different BI tools - Looker, Tableau, Domo, Microsoft BI and identified features that excites the entire\nbusiness teams and narrowed the tool of choice to Tableau"", u'o Published paper\nJanuary 2016 to January 2016\no Published paper on visualizing public opinions and sentiment trends on the US higher education in ACM', u'Graduate Researcher\nPenn State University\nJanuary 2014 to May 2015\no Partnered with the Big Data and Software Engineering Research team for various research projects\no Analyzed Siemens project team effectiveness and success through email, communication, project metrics, sprint logs\no Analyzed public opinions and sentiments using Natural Language Processing', u'Performance Engineer\nCognizant Technology Solutions\nNovember 2009 to January 2012\no Led the performance analysis for Multiple Image Infrastructure at eBay Inc.\no Reduced response time to render the catalog page by implementing load tests by profiling and monitoring the CPU,\nDB, and Java response times for loading multiple Images for every item\no Improved the Database performance by using indexing and implementing high performance queries\no Implemented regression, load and performance test for checkout transactions using Bill me later and PayPal\ncheckout and identified performance bottlenecks\no Worked with Staples Inc. to analyze performance of ESB with component level tests\no Monitored the performance of multiple web servers, DB servers and made a graphical representation of the downfall of the server and logged the activity\no Performed several smoke tests and benchmarked the Enterprise Service Bus that is the medium of several\napplications\nOpen Source Projects\nImage Classification\nThe CIFAR-10 dataset consists of airplanes, dogs, cats, and other objects. Classified images into respective categories\nusing Convolution Neural Networks github.com/rramyr/image-classification-using-cnn\nBike Sharing Rides\nUsing the open source bike data set from the UCI Machine\nLearning Database, developed a Neural Network using\ngradient descent, backpropagation in Tensor Flow with\nNumpy to predict the demand and supply of bike sharing\ncompany. github.com/rramyr/Bike-Sharing-Rides-Neural-Network\nLanguage Translation\nDeveloped a translation system using Sequence to Sequence\nRNN to translate English sentences to French github.com/rramyr/ language_translation\nTV Script Generation\nGenerate own TV subtitles for Simpsons TV Show using\nRNN github.com/rramyr/ tv-script-generation-using-RNN\nWeather Pattern Prediction\nCollected data from NOAA. Identified precipitation patterns over years using K-means clustering. Predicted precipitation\nusing linear regression with an accuracy of 85%']","[u'Master of Software Engineering in Software Engineering', u'Bachelor of Computer Science in Computer Science and Engineering']","[u'Penn State University University Park, PA\nMay 2015', u'Anna University Chennai, Tamil Nadu\nMay 2009']"
0,https://resumes.indeed.com/resume/9c810ff783ab7e48,"[u'Software design engineer, machine learning data scientist\nTATA CONSULTANCY SERVICE - Redmond, WA\nApril 2015 to Present\nDuties and Technologies used:\n\u2022 Azure Subscription Automation VM CPU usage and auto scaling machine learning modeling.\nUsing algorisms:\nARIMA (auto regression integrated moving average),\nNNet (feed-forward neural networks with a single hidden layer, and for multinomial log-linear models)\nMulti-Classes Neural Networks classifications\nWelch two samples statistics test for model evaluations\nTime Series Anomaly Detection\nK-Means clustering\nSigmoid Activation\nPlatform:\nAzure Machine Learning Studio with R Scripts, SQLite, Storage Blob\nData analysis exploration visualization:\nMS Power BI\n\u2022 Cosmos Scope big data extracting and parsing in huge telemetry JSON format raw data stream to build search traffic & revenue analysis funnels. Providing simple and clear data visualization for search team. Post-sale monetization customer behave analysis based on customer purchase daily cosmos big data.', u'Software design engineer III\nTRYGSTAD TECHNICAL SERVICE - Bellevue, WA\nMay 2008 to August 2014\nDuties\n\u2022 Continue Integration Service Central Database & Reporting Site design / development\n\u2022 Continue Integration Service Automation System development and data base performance improvement.', u'Data engineer\nSHANGHAI HUAWEI MICROSYSTEM CO., LTD - SHANGHAI, CN\nJanuary 1999 to January 2006\nDuties / Technologies used:\nProject / product - civic information management application system\n\u2022 Leading an agile development team working closely with customers, area experts and data scientists to provide supervision system analysis, total solutions, data standards, distribution architectures for Shanghai Municipal & Civic Archiving Information Center.\nTechnologies used:\n\u2022 Front: HTML + CSS + ASP / JSP + OWC web report components\n\u2022 Middle part: Java class EJB, COM wrappers\n\u2022 Back end: T-SQL/PL SQL + SQL Server DTS, OLAP\n\u2022 Animation: Adobe Flash Scenes & action scripts']","[u'in Computer Software Engineering', u'Certification']","[u'Shanghai Computer Manufacturing College Shanghai, CN', u'UW PCE']"
0,https://resumes.indeed.com/resume/420f3ffcb12ac7fe,"[u""Database Developer\nAfterInc - New York, NY\nJuly 2017 to Present\n\u25cf Created Jobs (Python, SQL Functions) to extract client's raw data (around 10M) from server and sync to the corresponding master tables.\n\u25cf Developed procedures to decode JSON object from client's file and sync the related data to core tables.\n\u25cf Build deeply nested JSON objects using PostgreSQL to facilitate the applications team in using it and populate the data accordingly on UI."", u""Data Services Engineer\nCopart - Dallas, TX\nJune 2016 to May 2017\nDeveloped ETL Jobs and transformations for the Data Migration using Pentaho Data Integration tool.\n\u25cf Designing data model for Global Setup and Maintenance project to help our sales team to incorporate new contract details.\n\u25cf Analyze technical requirements and design new data objects (Procedures, Functions) to support our internal SOLR search engine.\n\u25cf Created a story line and advanced Interactive dashboards to depict the customer's segmentation using Tableau.\n\u25cf Prepared ad-hoc tableau reports by making use of filters and calculated fields for internal sales and marketing team.\n\u25cf Assist in the overall planning, implementation and day-to-day functions of SQL based systems."", u'System Engineer\nTCS - Mumbai, Maharashtra\nOctober 2012 to July 2015\nDevelop and build efficient data marts in relational model to produce operational data store specific to each business function.\n\u25cf Generate reports of component distribution and customer interest across different sectors using Tableau by connecting to Oracle.\n\u25cf Performed ETL process to load the data from third party ticketing application and produced interactive data visualization using Tableau.\n\u25cf Led Migration Project which involved data migration scripts for more than 50000 records per table using intensive SQL queries.\n\u25cf Designed and implemented a mechanism to fetch the data from the database and trigger an email of all the collected data to the users.\n\u25cf Perform data analysis to give a granular insight to the product sales and help business to identify growth opportunities using SQL.\n\u25cf Implemented Data Cleanup procedures and scripts on production tables in SQL to substantially improve performance.\n\u25cf Requirement Gathering, Analysis & Design, Finalizing the Scope Documents on high visibility Change Requests.']","[u'Masters in Data warehouse and Analytics', u'Bachelor of Engineering in Computer Science']","[u'Illinois Institute of Technology Chicago, IL\nAugust 2015 to May 2017', u'Gogte Institute of Technology, Visvesvaraya Technological University Belgaum, Karnataka\nSeptember 2008 to June 2012']"
0,https://resumes.indeed.com/resume/7cc38cfceaa44d5a,"[u'Cyber Security Analyst\nGlobal Wireless Solutions - Dulles, VA\nFebruary 2015 to Present\nGlobal Wireless Solutions\nCyber Security Analyst\nResponsibilities:\n\u2022 Support proactive detection and analysis of security incidents by Splunk Enterprise security\n\u2022 Periodic assessment on incident trending to guide the strategy\n\u2022 Conduct Digital Forensics research, Malware Analysis, Cyber Threat Intelligence.\n\u2022 Vulnerability Assessment, reach conclusions, and make recommendations by Splunk\n\u2022 Monitor networks for security events and alerts clients to potential (or active) threats, intrusions, and compromises by Splunk & Fire Eyes.\n\u2022 Performing application vulnerability assessments\n\u2022 Performing code review across a variety of programming languages\n\u2022 Work closely with our developers and management on security practices\n\u2022 Identified information security related events/incidence and document through to resolution.\n\u2022 Analyze data provided from other departments/ Business Unit customers to evaluate the severity/magnitudes of events to determine if an incident occurred and formulate an appropriate and calibrated response in the event of a confirmed incident.\n\u2022 Sound knowledge of TCP/IP networking, switches, routers, firewalls, VPNs, and encryption.\n\u2022 possess demonstrated analytical ability and the ability to handle a large, complex workload. Monitoring indicators and warnings of threats and potential threats to voice, video, and data networks and associated systems;\n\u2022 Reports and documents on business use cases, the creation and maintenance ArcSight rule sets, channels, and customized views;\n\u2022 Short and Long-term security event trend analysis performed on a regular basis using Splunk, FireEye, and a dozen other cyber security tools;', u'Data Security Engineer\nNetwork Monitoring, Network Security - Irvine, CA\nOctober 2013 to February 2015\nResponsibilities:\n\u2022 Performs event correlation using information gathered from a variety of sources (e.g., individual host logs, network traffic logs, firewall logs, and intrusion detection system IDS logs) within the enterprise to gain situational awareness and determine the effectiveness of an observed attack By using Splunk, Fire eyes, Snort, SIEM/ArcSight and Scanning tools& Network security tools (Nmap.Nessus, Wireshark, and tcp dump).\n\u2022 ToDevelop and execute security processes, policies, and procedures.\n\u2022 To Proactively identify, troubleshoot, and resolve vulnerabilities\n\u2022 Performing code review across a variety of programming languages\n\u2022 Work closely with our developers and management on security practices\n\u2022 Participate in incident response and management as required 24x7\n\u2022 Participate in multiple Projects and manage large projects as required\n\u2022 To defense-in-depth security for the organization to protect critical IT assets and data.\n\u2022 Design the people, processes, and technology systems to enable an effective security program including:\n\u2022 Security infrastructure device management\n\u2022 Security and critical IT monitoring\n\u2022 Threat management\n\u2022 Vulnerability management']","[u'in distance education', u'MBA in Information Technology', u'B.Sc. in Information and Communication Engineering']","[u'Virginia Polytechnic Institute and State University Blacksburg, VA\nAugust 2018', u'American International University\nJanuary 2011', u'East West University\nJanuary 2005']"
0,https://resumes.indeed.com/resume/045f2601185c084e,"[u'Software Engineer Intern\nAbbott\nJune 2017 to Present\n\u2022 Develop quality software for medical devices like pacemakers and implantable cardioverter defibrillators(ICDs).\n\u2022 Implement and debug automated & semi-automated test scripts.\n\u2022 Track and resolve defects using Rational ClearQuest and AccuRev.\n\u2022 Apply applicable Food and Drug Administration (FDA) regulations and company operating procedures, policies and rules.', u'Software Developer\nDreamTek Inc\nApril 2017 to August 2017\n\u2022 Worked on the backend for a credit card marketing project using Python Flask and MongoDB.\n\u2022 Developed APIs for information management using RESTPlus.\n\u2022 Implemented NLP algorithms for credit card offer extraction.', u'Software Engineer\nZTE(Python)\nJune 2015 to December 2015\n\u2022 Built and tested REST-API to model alarm data fetched from the network units.\n\u2022 Optimized queries and methods to classify alarm data in the database, based on priority.\n\u2022 Maintained and monitored the internal network, dealt with software and hardware failures if happened.\n\u2022 Built unit tests for NMS (network management system)', u'Data Analysis Intern\nECE Department\nMay 2014 to August 2014\n\u2022 Built tools to locate unusual signals and behaviors in Synchrophasors data from dynamic transmission systems (PMU).\n\u2022 Improve the accuracy of stability analysis by 25% by implementing the Principal Component Analysis(PCA).\n\u2022 Analyzed grid stability by generating power flow plots to show undulations in real time due to power swings.\n\u2022 Presented and reported to our industry partners like ComEd and Naperville every week.\n\nPROJECTS:']","[u'MS in Electrical and Electronic Engineering', u'BS in Electrical Engineering']","[u'University of Southern California Los Angeles Los Angeles, CA\nDecember 2017', u'Illinois Institute of Technology Chicago, IL\nMay 2015']"
0,https://resumes.indeed.com/resume/c98267b5f66df586,"[u'Data Engineer\nOppenheimerFunds, Inc\nJanuary 2014 to Present\n\u2022 Analysis of source systems and work with business analysts to identify study and understand requirements and translate them into ETL code\n\u2022 Design and develop the Mappings using various transformations to suit the business user requirements and business rules to load data from Oracle, SQL Server, DB2, flat file and XML file sources targeting the views (views on the target tables) in the target database (Oracle).Write UNIX Shell Scripting for Informatica Pre-Session, Post-Session Scripts and also to run the workflows\n\u2022 Involved in the creation of Oracle tables, table partitions, materialized views and indexes and PL/SQL stored procedures, functions, triggers and packages\n\u2022 Conducted peer design and code reviews and extensive documentation of standards, best practices, and ETL procedures\n\u2022 On-Call Production support, for every 4 weeks - 24/7', u""Software Application Engineer\nCharles Schwab\nAugust 2011 to January 2014\nDesigning and creating batch JAVA, Korn Scripts on a AIX and Linux server.\n\u2022 Designing and creating a Java, PL/SQL and SQL/PL Stored Procedure to maintain, update, insert and access Customer Account Technology's DB2 and Oracle tables.\n\u2022 Design, creating and maintaining COBOL, JCL, Java programs within Schwab's mainframe and distributed systems by using such tools as ISPF, TSO, Data Studio, RDz, and Eclipse,\n\u2022 Create Unit Test Script, Technical Specifications, Functional Specification and other documentation for developing applications.\n\u2022 Conduct code review with and for other engineers.\n\u2022 Use the RUP and agile methodology to conduct new development and maintaining software."", u'Senior Specialist\nClearing Mutual Fund\nFebruary 2008 to August 2011\nProcess trades, dividend payments and reconciliation for Clearing Mutual Fund department.\n\u2022 Designed, created, and implemented an automation system for the JPCC asset conversion process for historic conversion of 65 billion and 4.2 million customers by using VBA and MS Access.\n\u2022 Designed, created, implemented and maintained a departmental wide MS Access database based on a mutli-tied application to collected and calculate Statistical data.\n\u2022 Increased user efficiency and through automating processes by designing, creating and maintaining clearing VB/VBA programs.\n\u2022 Performed end-user testing for enhancements on Legacy and SURPAS system and GUI/Forms by developing test scripts, regression testing and integration testing\n\u2022 Mentoring and trained personnel on policies and procedures regarding how to maintain and troubleshoot departmental applications.']","[u'MBA in MBA', u'Associate in Computer Information Systems', u'Bachelor of Science in Business Administration Focusing on Marketing']","[u'Webster University\nMay 2017', u'Arapahoe Community College\nMay 2012', u'Colorado State University\nMay 1999']"
0,https://resumes.indeed.com/resume/5993e580c617a7f2,"[u'Data Center Engineer\nHewlett Packard Enterprise - Palo Alto, CA\nJanuary 2010 to January 2017\nManaged over 75 racks of servers, storage and networking equipment in a Proof of Concept lab/data center. Managed over 90 HP Demo Loans at any one time, worth about $3.8M in June 2017.\n\u2022 Recognized by management as a flexible, willing to take on new roles and responsibilities when needed.\n\u2022 Acquired a reputation amongst team engineers for detail oriented deliverables and managing / tracking loans with a strong sense of ownership.\n\u2022 Achieved recognition from solution architects as a ""go to person"" to resolve any hardware issues.\n\u2022 Adapted into a role beyond hardware and into network monitoring to back up team members to support department infrastructure.\n\u2022 Redesigned layout of SAN Director and storage arrays to facilitate shorter cable runs that enable more reliable connectivity and easier/faster reconfigurations.\n\u2022 Accomplished a 100% no loss rate for returns of all Demo Loans using detailed tracking, packaging and returning processes.', u'Data Center Engineer\nHewlett-Packard - Cupertino, CA\nJanuary 2006 to January 2010\nCo-managed 4,000 square foot dedicated Proof of Concept lab and data center with 50+ racks of servers, storage and networking equipment. Received shipments of HP Demo equipment, de-palleted and installed equipment into racks.\n\u2022 Created bundled fiber channel cables using split tubing to make installing/removing cables easier, faster and safer.\n\u2022 Coordinated with facilities to order correct power drops for racks and ensured proper cooling for maximum reliability.\n\u2022 Validated configurations for hardware quotes for proper NIC and SAN adapters and ensured proper port counts and cables were included.\n\u2022 Troubleshot and resolved hardware problems utilizing onsite spare parts decreasing downtime and lessening workload for field Technical Services.']",[u'Bachelor of Science in Engineering Technology'],"[u'California Polytechnic University San Luis Obispo, CA']"
0,https://resumes.indeed.com/resume/c40c0fe68557df59,"[u'Big Data Intern\n63MOONS TECHNOLOGY - Mumbai, Maharashtra\nMay 2017 to July 2017\n\u2022 Analysed website traffic to determine popularity of images on a website and drive traffic to less popular images\n\u2022 Gathered static data from MySQL to Hadoop Distributed File System (HDFS) using SQOOP and collected clickstream data from server to HDFS using Flume\n\u2022 Determined popularity of images by creating SQOOP and PIG jobs with data imported from external tables in Hive', u'Software Engineer\nSYNTEL\nNovember 2016 to April 2017\nAll State Encompass\n\u2022 Migrated existing insurance data from relational database (ORACLE/SQL SERVER) to HIVE to store data\n\u2022 Worked on optimization of existing system by eliminating a few tables from current system\n\u2022 Generated partitioned tables by year to enable storage of the incremented data load in HIVE\n\u2022 Supplanted EXCEL reports with TABLEAU to visualize data in better format and produced interactive data for the end users to interact with data in the form of bar charts, line charts, and maps', u'Fast Data\nMay 2016 to October 2016\nDeveloped a framework to analyse real time Electrocardiogram, Heartbeat and Stock data stored in MySQL and easily visualize\ndata in dashboards\n\u2022 Developed Comma Separated Value type data generation from JAVA program and pushed CSV to KAFKA topic\n\u2022 Migrated the generated CSV data from KAFKA topic to SPARK Consumer for converting the data into streams of data\n\u2022 Gathered the data from SPARK Consumer to MySQL and displayed this data into Dashboard\n\u2022 Deployed of JAVA WEB SERVICES on APACHE TOMCAT to read data from MySQL database and presenting the result in dashboard for end users', u'Facets\nNovember 2015 to April 2016\n\u2022 Generated Data Lake in HIVE where all data gets stored from CSV file to HIVE tables.\n\u2022 Generated Refined Data Store to store the filtered records as per the business rules given by client\n\u2022 Replicated Refined Data Store in CASSANDRA, created query layer by filtering certain columns from Refined Data Store and querying in Cassandra for getting the final results for visualisation in TABLEAU.']","[u'Master of Science in Information Systems', u'Bachelor of Engineering in Information Technology']","[u'Pace University, Seidenberg School of CSIS New York, NY\nMay 2019', u'University of Mumbai, PVPP College of Engineering Mumbai, Maharashtra\nJune 2014']"
0,https://resumes.indeed.com/resume/167a78b36053085a,"[u'Data Centre Analyst\nIT PEOPLE LLC - Dubai, AE\nJanuary 2017 to Present\nRoles and Responsibilities:\n\u25cf Taking care of EOD operation Finacle, Prime, Flexcube, RLS and various Banking applications.\n\u25cf Monitor networks, server, Storage, backup changes/updates, and work closely with support team\n\u25cf Taking backup on daily basis in application servers.\n\u25cf Monitoring the scheduled backups to ensure success or take remedial action for failures.\n\u25cf Monitoring and managing disk space. Responsible for monitoring and bouncing services on Application servers.\n\u25cf Providing L1 troubleshooting for server, network and backup related issues.\n\u25cf Scheduling, configuring and managing jobs in control-M application and checking the relevant log file and handling control-m alerts.\n\u25cf Coordinated with end user business requirements to ensure that the processing of their daily work was completed on time and error free.\n\u25cf Performing ATM monitoring and working closely with support team.\n\u25cf Performing cloning of application production servers.\n\u25cf Escalation of issues as per the SLA.\n\u25cf Responsible for all data centre operations & day to day daily activities.\n\u25cf Maintaining reports of day to day activities.', u'Data Centre Analyst\nHCL Technologies Ltd\nJuly 2016 to January 2017\nRoles and Responsibilities:\n\u25cf On 24/7 team support rotation to monitor networks, server, Storage, backup changes/updates, and work closely with support team.\n\u25cf Configure the backup for new client to be added in Data zone for backup.\n\u25cf Creating groups, pools and assign clients to them for backup.\n\u25cf Investigating the cause of backup failure and resolving them.\n\u25cf By analyzing the daemon logs to check for the backup failure cause.\n\u25cf EMC Avamar administration tasks like performing, monitoring restore activities, server monitoring, and Suspend-Resume restores.\n\u25cf To restore the data from the database server after checking the Retention Period for the data from Server.\n\u25cf Concepts of Avamar and Data Domain like de-duplication and others.\n\u25cf Responsible in providing assistance to level 2 support team in monitoring, troubleshooting and administration of the backup infrastructure for Avamar\n\u25cf Responsible in providing assistance to level 2 support team in monitoring, troubleshooting and administration of the backup infrastructure for Data Domain.\n\u25cf Checking the status of drives in ADIC Library.\n\u25cf Checking the ECC console for severity alerts and also restarting the agents in case of them being inactive.', u'Data Centre Engineer\nRoyal Bank of Scotland - Chennai, Tamil Nadu\nAugust 2014 to June 2016\nRoles and Responsibilities:\n\u25cf DATA CENTRE OPERATIONS (24X7) production support for INDIA. Responsible for all data centre operations and day to day activities.\n\u25cf Monitoring and managing disk space. Responsible for monitoring and bouncing services on Application servers.\n\u25cf Taking care of EOD operation Finacle, Flexcube, RTGS, CASHIN and various Banking applications.\n\u25cf Monitoring and support netcast server.\n\u25cf Performing ATM monitoring and working closely with support team.\n\u25cf Performing cloning of application production servers.\n\u25cf Taking backup on daily basis in application servers.\n\u25cf Monitoring the scheduled backups to ensure success or take remedial action for failures.\n\u25cf Maintaining Media movement activity.\n\u25cf Monitor daily system processes, ensuring 100% data processing on critical data.\n\u25cf Production and DR switchover activity support.\n\u25cf Running Test cases in UAT servers. Responsible for carrying out weekly BCP.']",[u'B.E in Computer Science Engineering'],[u'K.Ramakrishnan College of Technology\nApril 2014']
0,https://resumes.indeed.com/resume/1fc2984cc1c081d4,"[u""Big Data Application Developer\nAnthem Inc - Chicago, IL\nAugust 2016 to Present\nDescription: Anthem Inc. is an American health insurance company founded in the 1940s, prior to 2014 known as WellPoint, Inc. It is the largest for-profit managed health care company in the Blue Cross and Blue Shield Association. I am part of digital marketing group. We analyzed the data from all transactions to find out the customer and company profitable plan with services which assists in determining new strategic plans into market.\n\nResponsibilities:\n\u2022 Developed Spark Applications by using Spark, Java and Implemented Apache Spark data\nprocessing project to handle data from various RDBMS and Streaming sources.\n\u2022 Migrated existing map reduce jobs to Spark to perform transformations on the data.\n\u2022 Worked on compressing the data size in the warehouse from ~12.2 TB to ~5.4 TB using BZip2 codec.\n\u2022 Implemented Sqoop jobs including incremental loads to migrate data from Teradata into Hive Warehouse.\n\u2022 Involved in loading and transforming large sets of structured, semi structured and unstructured data from relational databases into HDFS using Sqoop imports.\n\u2022 Developed scripts to create workflows in bash (shell) to integrate the hive queries and spark jobs.\n\u2022 Tuned & optimized hive queries to improve performance and reduce the latency.\n\u2022 Developed Hive UDF's to implement various business rules.\n\u2022 Worked on Partitioning, Bucketing, Parallel execution, Map side Joins for optimizing hive queries.\n\u2022 Experienced in working with Spark ecosystem using Scala and HIVE Queries on different data formats like Text file and parquet.\n\u2022 Collected the logs data from web servers and integrated in to HDFS using Flume.\n\u2022 Developed Hive scripts in Hive QL to de-normalize and aggregate the data.\n\u2022 Experience in using Apache Kafka for collecting, aggregating, and moving large amounts of data from application servers.\n\u2022 Involved in writing live Real-time Processing and core jobs using Spark Streaming with Kafka as a data pipe-line system.\n\u2022 Extensively worked on creating End-End data pipeline orchestration using Oozie.\n\u2022 Involved in converting Map Reduce programs into Spark transformations using Spark RDD's and Scala.\n\u2022 Involved in moving all log files generated from various sources to HDFS for further processing through Flume.\n\u2022 Work experience with cloud infrastructure like Amazon Web Services (AWS).\n\u2022 Optimized MapReduce Jobs to use HDFS efficiently by using various compression mechanisms.\n\u2022 Developed web based applications.\n\u2022 Developed applications to individual client standards.\n\nEnvironment: Hadoop, Spark, Map Reduce, Hive, Sqoop, Hbase, Kafka, Unix and Scala"", u'Data Engineer\nChicago, IL\nJanuary 2014 to July 2016\nDescription: Discover holds huge data in its Data lake, as a Big Data Developer my role is to provide the insights for various clients by performing transformations on data lake data. Ingestion into Data lake is achieved by using Kafka and storm which stored data into Hive tables. To generate client reports and for processing the data worked on various Hadoop technologies like Pig, Hive, Oozie, Sqoop, and Spark SQL using Scala, for real time processing used Kafka and spark streaming.\n\nResponsibilities:\n\u2022 Implemented Pig script for cleansing millions of records of CSV data.\n\u2022 Developed Map reduce job in Java to process CSV data to perform aggregations and rollups.\n\u2022 Implemented Pig script to extract data from log files and store them on HDFS.\n\u2022 Implemented Oozie workflows for scheduling jobs for generating reports on a daily, weekly, and monthly basis.\n\u2022 Implemented an end-to-end Oozie workflow for extraction and transformation of the data.\n\u2022 Responsible for reviewing logs and troubleshooting issues in Map reduce jobs.\n\u2022 Implemented sqoop jobs to Import data from SQL Server into Partitioned Hive tables and export the back onto MySQL after performing data analysis in hive.\n\u2022 Used Pig UDFs from Piggy bank for pre-processing the data.\n\u2022 Developed workflows using Oozie to automate the tasks of loading the data into HDFS and pre-processing with Pig.\n\u2022 Developed interactive shell scripts for scheduling various data cleansing and data loading process.\n\u2022 Converting the existing relational database model to Hadoop ecosystem.\n\u2022 Worked on Hue interface for querying the data.\n\u2022 Created Hive tables to store the processed results in a tabular format.\n\u2022 Developed Hive Scripts for implementing dynamic Partitions.\n\nEnvironment: Cloudera Distribution, Hadoop, Map Reduce, Pig, Hive, Sqoop, Oozie, Java, MySQL, SQL Server and Unix', u""Hadoop Developer\nWholefoods Inc - Austin, TX\nNovember 2012 to December 2013\nDescription: The objective is delivering large-scale programs that integrate with technology for the users (external customers) as well internal Users (US Foods employees) to achieve high performance. Design, implement and deploy custom applications on Hadoop cluster. Providing issue based solutions using big data analytics. The technical stack includes Spark, Kafka, Scala, Pig, Hive and Oozie.\n\nResponsibilities:\n\u2022 Worked with the Teradata analysis team to gather the business requirements.\n\u2022 Worked extensively on importing data using scoop and flume.\n\u2022 Responsible for creating complex tables using hive.\n\u2022 Created partitioned tables in Hive.\n\u2022 Transportation of data to Hbase using pig.\n\u2022 Developed workflow in Oozie to automate the tasks of loading the data into HDFS and pre-processing with Pig.\n\u2022 Experience with professional software engineering practices and best practices for the full software development life cycle including coding standards, code reviews, source control management and build processes.\n\u2022 Worked collaboratively with all levels of business stakeholders to architect, implement and test Big Data based analytical solution from disparate sources.\n\u2022 Involved in source system analysis, data analysis, data modeling to ETL (Extract, Transform and Load).\n\u2022 Experience with HIVE DDLs and Hive Query language (HQLs).\n\u2022 Worked on dash boards that internally use Hive queries to perform analytics on structured data, Avro and Json data.\n\u2022 Involved in importing the real time data to Hadoop using Kafka and implemented the Oozie job for daily.\n\u2022 Written multiple MapReduce procedures to power data for extraction, transformation and aggregation from multiple file formats including XML, JSON, CSV & other compressed file formats.\n\u2022 Handling structured and unstructured data and applying ETL processes.\n\u2022 Developed Pig Latin scripts to extract the data from the web server output files to load into HDFS.\n\u2022 Developed the Pig UDF'S to pre-process the data for analysis.\n\u2022 Develop Hive queries for the analysts.\n\u2022 Prepare Developer (Unit) Test cases and execute Developer Testing.\n\u2022 Create/Modify shell scripts for scheduling various data cleansing scripts and ETL loading process.\n\u2022 Supports and assist QA Engineers in understanding, testing and troubleshooting.\n\u2022 Written build scripts using ant and participated in the deployment of one or more production systems.\n\u2022 Production Rollout Support which includes monitoring the solution post go-live and resolving any issues that are discovered by the client and client services teams.\n\u2022 Designed, documented operational problems by following standards and procedures using a software reporting tool JIRA.\n\nEnvironment: Apache Hadoop, Java (jdk1.6), Flat files, Oracle 11g/10g, MySQL, Windows NT, UNIX, Sqoop, Hive, Oozie."", u'Java Developer\nEricsson India Global Services Pvt LTD - Bengaluru, Karnataka\nMay 2011 to November 2012\nDescription: Ericsson offers software services and communications technology for telecomm operators and provides business support. The primary objective of this project is to integrate Big Data with the EEA (Ericsson Expert Analytic) Application to leverage the raw/processed data and provide an enriched customer experience by delivering customer insights, profile information and customer journey.\n\nResponsibilities:\n\u2022 Object Oriented Design and Analysis using UML, development of class diagrams, State diagrams, Sequence diagrams and implementing in Microsoft Visio.\n\u2022 Implemented Java/J2EE Design Patterns as Business Delegate and Data Access Object, Data Transfer Object (DTO) and service locator.\n\u2022 Involved in writing Spring Configuration XML files that contains declarations and other dependent objects declaration.\n\u2022 Involved in development and designing user interface, web components using JSP, HTML & JDBC.\n\u2022 Developed SQL scripts to create and maintain the database, roles, users, tables, views, procedures and triggers.\n\u2022 Involved in the analysis of Safe/Drawer Transactions, Loan deposit modules and development of Collection Letters.\n\u2022 Worked on database interaction layer for insertions, updating and retrieval operations on data.\n\u2022 Coordinated & Communicated with onsite resources regarding issues rose in production environment and used to fix day to day issues.\n\nEnvironment: Java, Web Logic, Eclipse, JSP, Windows XP, HTML, Maven and XML.']","[u""Bachelor's""]",[u'']
0,https://resumes.indeed.com/resume/5a62e43af27c5227,"[u'Data Application Engineer\nIOTAS, Inc\nSeptember 2017 to February 2018\n\u2022 Implemented in Agile methodology. Worked as a team member and involved in analyzing user stories, coming up with timeline to implement the same\n\u2022 Developed and implemented data warehouse, data marts and modified existing schema and queries to improve efficiency to meet new requirements\n\u2022 Migrated about 2 TB of data from postgres to AWS RDS, worked end-to-end for the data migration process through Data Modelling, Schema design\nand ETL processes\n\u2022 Used JIRA tool for bug tracking, issue tracking, project management functions and Bitbucket as source version control for the development\n\u2022 Developed python scripts to test API gateway endpoints and created API documentation using Swagger\n\u2022 Created lambda functions to deploy RESTful services to AWS API gateway via serverless framework', u'Data Science Intern\nBITWATER FARMS\nMarch 2017 to May 2017\n\u2022 Implemented python scripts using frameworks like pandas, numpy, scipy to analyze data in postgres database\n\u2022 Created reports in Excel spreadsheet with pivot tables and identified data issues and provided recommendations to ensure optimal performance\n\u2022 Devised customized SQL queries in MySQL database and created interactive reporting dashboard in Tableau\n\u2022 Implemented ETL job plan through Microsoft SSIS and Extracted, compiled, tracked, and analyzed data to generate reports', u'Data Engineer Intern\nIRI, THE COSORT COMPANY, Florida\nMay 2016 to November 2016\n\u2022 Developed in Waterfall SDLC and designed schema for the application to handle transactions of data\n\u2022 Implemented python scripts to publish and subscribe data on client and server side via Mosquito broker using MQTT protocol\n\u2022 Worked on all phases of ETL development lifecycle, from requirement gathering to testing, implementation, and support\n\u2022 Stored data in MySQL database and using ETL process and used GitHub as a source version control and published blogs', u'Data Engineer Intern\nTECHNO VERTEX - IN\nAugust 2014 to June 2015\n\u2022 Developed in Agile SDLC and contributed in the backend development of application in Java6 according to RESTful guidelines\n\u2022 Modeled large databases using ad-hoc SQL, performed data normalization and mapped the data to the user interface of the application\n\u2022 Implemented SAS, Tableau and SQL in data collection, data analysis and reporting to procure data from database structures to report and provide\nsolutions to project management in a timely manner\n\u2022 Developed ETL design, use-cases for loading data from OLTP system to ODS, Prepared test cases and performed Unit Testing']","[u'Masters in Computer Science in Computer Science', u'Bachelors in Computer Science in Computer Science']","[u'Florida Institute of Technology Melbourne, FL\nAugust 2015 to May 2017', u'Visvesvaraya Technological University\nSeptember 2011 to June 2015']"
0,https://resumes.indeed.com/resume/a21e676bb8a8e5d6,"[u'Data Engineer\nOctober 2017 to Present\n\u2022 Constructed data pre-processing pipeline by creating R functions for Municipality Budget Data, including data cleaning, validation and integration into Mongo DB.\n\u2022 Performed data visualization in Power BI, with map-based visualization and Time Series forecasting.', u'Market Data Analyst\nBosch Power Tool (China) Co., Ltd - Hangzhou\nNovember 2016 to March 2017\n\u2022 Developed Greater China sales analysis model in VBA which aggregated daily data from Axapta to weekly and monthly to compared Y/Y trends. Suggested online promotion products, resulting in 53% monthly sales increase.\n\u2022 Developed an inventory tracking and ordering strategy for 7 e-commerce warehouses and designed data validation rule based on average and quartile of recent 15-day data to predict future 45 days inventory.\n\u2022 Served as a liaison between 11 warehouse managers and 6 logistics service executives in Asia-Pacific Region to solve transshipment abnormal, delivered logistics KPI at weekly meeting that drove optimization strategy.', u'Credit Rating\nFinancial Risk Management Competition - Hangzhou\nJune 2015 to June 2015\n\u2022 Performed individual credit rating with 60k data entry from an online lending company using R. Conducted data cleansing, descriptive analysis and paired t-test to get more in-depth statistics features of the data.\n\u2022 Selected 16 out of 28 variables such as salary and marital status by BP-Neural Network and classified the results in binomial cases. Implemented Logistic Regression with stepwise variable selection to do the classification as a comparison.\n\u2022 Determined the cutoff of the logistic regression by threshold of ROC (Receiver Operating Characteristic) curve and evaluated the performance between two methods by AUC, which is 73.8% vs 65.3%.']","[u""Master's in Statistics"", u""Bachelor's in Finance""]","[u'Rutgers University New Brunswick, NJ\nSeptember 2017 to May 2019', u'Zhejiang Gongshang University Hangzhou\nSeptember 2013 to June 2017']"
0,https://resumes.indeed.com/resume/503a550542e025c2,"[u""Assistant Engineer\nHINDALCO INDUSTRIES LIMITED - Renukut, Uttar Pradesh\nJanuary 2014 to June 2016\nHindalco is the world's largest aluminum rolling company and one of Asia's biggest producers of primary aluminum.\n\nResponsibilities:\n\n\u2022 Employee of the Month, revised and administered delay database methodology via SQL to reduce and simplify\ndelay database management leading to an organized delay database handling.\n\u2022 Analyzing machines delay data report to provide competitive analysis and steps for inhibiting the delay of machines for business development.\n\u2022 Team Lead for installing and programming advanced protective system for Solution Furnace resulting the breakdown time of furnace from 64 hours per month to 8 hours per month leading to increasing in production by Solution Furnace from 384000 kilograms to 447994 kilograms of Aluminum product, prices of which varied as per the engineering of the product."", u'Data processor\nPATHFINDER MANAGEMENT CONSULTING (INDIA) LIMITED - Pune, Maharashtra\nJuly 2012 to October 2013\n\u2022 Formulated and engineered submission database methodology to resolve complicated database handling.\n\u2022 Guided and systematized team of Database Developers as per their job roles and specialization.']","[u'Master of Science in Information Systems & Technology', u'Bachelor of Technology in Electrical Engineering']","[u'THE GEORGE WASHINGTON UNIVERSITY, School of Business Washington, DC\nJune 2018', u'BHARATI VIDYAPEETH UNIVERSITY, College of Engineering Pune, Maharashtra\nJune 2012']"
