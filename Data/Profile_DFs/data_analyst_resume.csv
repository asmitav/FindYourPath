,resume,work_exp,edu,univ
0,https://resumes.indeed.com/resume/f20865fd9d349c38,"[u'Laboratory Analyst\nTestAmerica Laboratories, Inc - Corpus Christi, TX\nOctober 2017 to Present\nPerform various analytical procedures according to Standard Methods and EPA Methods.', u'Data Analyst\nPort Royal Ocean Resort - Corpus Christi, TX\nApril 2017 to August 2017\nResponsible for compiling and analyzing rate, occupancy, and market data to make recommendations on marketing strategies.', u'Teacher\nFusion Academy Dallas\nJanuary 2017 to April 2017\nPhysics Tutor - Employed by UTA; responsible for guiding students in undergraduate level Physics\nSeptember 2015 - December 2016']",[u'Bachelor of Science in Physics'],"[u'University of Texas at Arlington Arlington, TX\nAugust 2012 to December 2016']"
0,https://resumes.indeed.com/resume/7b54006bf88ff0fd,"[u'Data Analyst\nBroadridge Financial Solutions, Inc. - Long Island, NY\nJanuary 2016 to Present\n\u2022 Coordinate data, analytics, and business reporting activities to create business reviews for mutual fund clients.\n\u2022 Conduct research and analysis to supply the continuous improvement efforts of the Client Service team.\n\u2022 Review, analyze, and report on business and operating trends for associates and management.\n\u2022 Maintain department spreadsheets and databases, and build queries using macros to support recurring and ad-hoc reporting and analysis.\n\u2022 Monitor the effectiveness of systems, policies, and procedures and make appropriate recommendations to ensure daily operations are efficient.', u'Data Specialist\nBroadridge Financial Solutions, Inc. - Long Island, NY\nJanuary 2014 to January 2016\n\u2022 Sort and process proxy voting information from international investment firms via a high volume of advanced MS Excel work, including the creation and use of macros and frequent use of v-look ups.\n\u2022 Compile data and generate reports on a daily basis to meet strict deadlines.\n\u2022 Collaborate with other departments and communicate with clients to resolve queries and provide client services.\n\u2022 Maintain strong relationships with clients to ensure satisfaction and build positive relationships with new clients to facilitate business growth.\n\u2022 Self-audit teams work to ensure accuracy and that all daily deadlines are met.', u'Associate\nVanBourgondien Nursery - Long Island, NY\nJanuary 2008 to January 2013\n\u2022 Handled clerical tasks of data entry and tracking inventory via MS Excel.\n\u2022 Interact with customers by selling products, diagnosing problems, giving advice, and providing information.\n\u2022 Mostly worked in a group environment, requiring strong communication between members and management.\n\u2022 Assumed supervisor role in 2010, helped train and supervise new employees.\n\nReferences']","[u""Bachelor's in Business Management""]","[u'SUNY Farmingdale Farmingdale, NY\nJanuary 2006 to January 2010']"
0,https://resumes.indeed.com/resume/e5e72829213f1876,"[u""Data Scientist\nWELLSFARGO\nJuly 2017 to Present\nDescription: Wells Fargo: Provider of banking, mortgage, investing, credit card, insurance, and personal, small business, and commercial financial services. Wells Fargo became the world's largest bank by market capitalization, edging past ICBC, before slipping behind JP Morgan Chase in September 2016, in the wake of a scandal involving the creation of over 2 million fake bank accounts by Wells Fargo employees.\n\nResponsibilities:\n\u2022 Utilized Spark, Scala, Hadoop, HBase, Kafka, Spark Streaming, MLLib, R, a broad variety of machine learning methods including classifications, regressions, dimensionally reduction etc. and Utilized the engine to increase user lifetime by 45% and triple user conversations for target categories.\n\u2022 Participated in features engineering such as feature intersection generating, feature normalize and label encoding with Scikit-learn preprocessing.\n\u2022 Used Python 3.X (numpy, scipy, pandas, scikit-learn, seaborn) and Spark2.0 (PySpark, MLlib) to develop variety of models and algorithms for analytic purposes.\n\u2022 Developed and implemented predictive models using machine learning algorithms such as linear regression, classification, multivariate regression, Naive Bayes, Random Forests, K-means clustering, KNN, PCA and regularization for data analysis.\n\u2022 Ensure solutions architecture / technical architectures are documented & maintained, while setting standards and offering consultative advice to technical & management teams and involved in recommending the roadmap and an approach for implementing the data integration architecture (with Cost, Schedule & Effort Estimates)\n\u2022 Designed and developed NLP models for sentiment analysis.\n\u2022 Led discussions with users to gather business processes requirements and data requirements to develop a variety of Conceptual, Logical and Physical Data Models. Expert in Business Intelligence and Data Visualization tools: Tableau, Microstrategy.\n\u2022 Developed and evangelized best practices for statistical analysis of Big Data.\n\u2022 Designed and implemented system architecture for Amazon EC2 based cloud-hosted solution for client.\n\u2022 Designed the Enterprise Conceptual, Logical, and Physical Data Model for 'Bulk Data Storage System 'using Embarcadero ER Studio, the data models were designed in 3NF\n\u2022 Worked on machine learning on large size data using Spark and Map Reduce.\n\u2022 Collaborated with data engineers and operation team to implement ETL process, wrote and optimized SQL queries to perform data extraction to fit the analytical requirements.\n\u2022 Performed data analysis by using Hive to retrieve the data from Hadoop cluster, SQL to retrieve data from RedShift.\n\u2022 Explored and analyzed the customer specific features by using SparkSQL.\n\u2022 Performed data imputation using Scikit-learn package in Python.\n\u2022 Let the implementation of new statistical algorithms and operators on Hadoop and SQL platforms and utilized optimizations techniques, linear regressions, K-means clustering, Native Bayes and other approaches.\n\u2022 Developed Spark/Scala, SAS and R programs for regular expression (regex) project in the Hadoop/Hive environment with Linux/Windows for big data resources.\n\u2022 Conducted analysis on assessing customer consuming behaviours and discover value of customers with RMF analysis; applied customer segmentation with clustering algorithms such as K-Means Clustering and Hierarchical Clustering.\n\u2022 Built regression models include: Lasso, Ridge, SVR, XGboost to predict Customer Life Time Value.\n\u2022 Built classification models include: Logistic Regression, SVM, Decision Tree, Random Forest to predict Customer Churn Rate.\n\u2022 Used F-Score, AUC/ROC, Confusion Matrix, MAE, RMSE to evaluate different Model performance.\n\nEnvironments: AWS RedShift, EC2, EMR, Hadoop Framework, S3,HDFS, Spark(Pyspark, MLlib, Spark SQL), Python 3.x (Scikit-Learn/Scipy/Numpy/Pandas/Matplotlib/Seaborn),Tableau Desktop (9.x/10.x), Tableau Server (9.x/10.x), Machine Learning (Regressions, KNN, SVM, Decision Tree, Random Forest, XGboost, Light GBM, Collaborative filtering, Ensemble), Teradata, Git 2.x, Agile/SCRUM"", u'Data Scientist\nGENERAL DYNAMICS INFORMATION TECHNOLOGY\nApril 2016 to June 2017\nDescription: As a trusted systems integrator for more than 50 years, General Dynamics Information Technology provides information technology (IT), systems engineering, professional services and simulation and training to customers in the defense, federal civilian government, health, homeland security.\n\nResponsibilities:\n\u2022 Tackled highly imbalanced Fraud dataset using under sampling, oversampling with SMOTE and cost sensitive algorithms with Python Scikit-learn.\n\u2022 Wrote complex Spark SQL queries for data analysis to meet business requirement.\n\u2022 Developed Map Reduce/Spark Python modules for predictive analytics & machine learning in Hadoop on AWS.\n\u2022 Worked on data cleaning and ensured data quality, consistency, integrity using Pandas, Numpy.\n\u2022 Participated in feature engineering such as feature intersection generating, feature normalize and label encoding with Scikit-learn preprocessing.\n\u2022 Improved fraud prediction performance by using random forest and gradient boosting for feature selection with Python Scikit-learn.\n\u2022 Performed Na\xefve Bayes, KNN, Logistic Regression, Randomforest, SVMandXGboost to identify whether a loan will default or not.\n\u2022 Implemented Ensemble of Ridge, Lasso Regression and XGboost to predict the potential loan default loss.\n\u2022 Used various metrics(RMSE, MAE, F-Score, ROC and AUC) to evaluate the performance of each model.\n\u2022 Used big data tools Spark (Pyspark, SparkSQL, Mllib) to conduct real time analysis of loan default based on AWS.\n\u2022 Conducted Data blending, Data preparation using Alteryx and SQL for tableau consumption and publishing data sources to Tableau server.\n\u2022 Created multiple custom SQL queries in Teradata SQL Workbench to prepare the right data sets for Tableau dashboards. Queries involved retrieving data from multiple tables using various join conditions that enabled to utilize efficiently optimized data extracts for Tableau workbooks.\n\nEnvironment: MS SQL Server 2014, Teradata, ETL, SSIS, Alteryx, Tableau (Desktop 9.x/Server 9.x), Python3.x(Scikit-Learn/Scipy/Numpy/Pandas), Machine Learning (Na\xefve Bayes, KNN, Regressions, Random Forest, SVM, XGboost, Ensemble), AWS Redshift, Spark(Pyspark, MLlib, Spark SQL),Hadoop 2.x, Map Reduce, HDFS, SharePoint', u""Data Analyst/Data Scientist\nAdvance Health - Chantilly, VA\nDecember 2014 to March 2016\nDescription: Advancing Member Engagement. By engaging members when and how they want, we collect the most accurate and timely information to share with providers and health plans. Advance Health provides industry leading managed care prospective health solutions. Our combination of proprietary mobile workflow technology and highly experienced, dedicated care providers yields outstanding program results and better outcomes for our clients and their health plan members.\n\nResponsibilities:\n\u2022 Gathered, analyzed, documented and translated application requirements into data models and Supports standardization of documentation and the adoption of standards and practices related to data and applications.\n\u2022 Participated in Data Acquisition with Data Engineer team to extract historical and real-time data by using Sqoop, Pig, Flume, Hive, Map Reduce and HDFS.\n\u2022 Wrote user defined functions (UDFs) in Hive to manipulate strings, dates and other data.\n\u2022 Performed Data Cleaning, features scaling, features engineering using pandas and numpy packages in python.\n\u2022 Applied clustering algorithms i.e. Hierarchical, K-means using Scikit and Scipy.\n\u2022 Created logical data model from the conceptual model and it's conversion into the physical database design using ERWIN.\n\u2022 Mapped business needs/requirements to subject area model and to logical enterprise model.\n\u2022 Worked with DBA's to create a best fit physical data model from the logical data model\n\u2022 Redefined many attributes and relationships in the reverse engineered model and cleansed unwanted tables/ columns as part of data analysis responsibilities.\n\u2022 Enforced referential integrity in the OLTP data model for consistent relationship between tables and efficient database design.\n\u2022 Developed the data warehouse model (star schema) for the proposed central model for the project.\n\u2022 Created 3NF business area data modeling with de-normalized physical implementation data and information requirements analysis using ERWIN tool.\n\u2022 Worked on the Snow-flaking the Dimensions to remove redundancy.\n\u2022 Worked in using Teradata14 tools like Fast Load, Multi Load, T Pump, Fast Export, Teradata Parallel Transporter (TPT) and BTEQ.\n\u2022 Helped in migration and conversion of data from the Sybase database into Oracle database, preparing mapping documents and developing partial SQL scripts as required.\n\u2022 Generated ad-hoc SQL queries using joins, database connections and transformation rules to fetch data from legacy Oracle and SQL Server database systems\n\nEnvironment: Machine learning(KNN, Clustering, Regressions, Random Forest, SVM, Ensemble), Linux, Python 2.x (Scikit-Learn/Scipy/Numpy/Pandas), R, Tableau (Desktop 8.x/Server 8.x), Hadoop, Map Reduce, HDFS, Hive, Pig, HBase, Sqoop, Flume, Oracle 11g, SQL Server 2012"", u""BI Developer/Data Analyst\nSanofi - Atlanta, GA\nApril 2013 to November 2014\nDescription: Sanofi is a global life sciences company committed to improving access to healthcare and supporting the people we serve throughout the continuum of care. From prevention to treatment, Sanofi transforms scientific innovation into healthcare solutions, in human vaccines, rare diseases, multiple sclerosis, oncology, immunology, infectious diseases, diabetes and cardiovascular solutions, consumer healthcare, established prescription products and generics.\n\nResponsibilities:\n\u2022 Used SSIS to create ETL packages to Validate, Extract, Transform and Load data into Data Warehouse and Data Mart.\n\u2022 Maintained and developed complex SQL queries, stored procedures, views, functions and reports that meet customer requirements using Microsoft SQL Server 2008 R2.\n\u2022 Created Views and Table-valued Functions, Common Table Expression (CTE), joins, complex sub queries to provide the reporting solutions.\n\u2022 Optimized the performance of queries with modification in T-SQL queries, removed the unnecessary columns and redundant data, normalized tables, established joins and created index.\n\u2022 Created SSIS packages using Pivot Transformation, Fuzzy Lookup, Derived Columns, Condition Split, Aggregate, Execute SQL Task, Data Flow Task and Execute Package Task.\n\u2022 Migrated data from SAS environment to SQL Server 2008 via SQL Integration Services (SSIS).\n\u2022 Developed and implemented several types of Financial Reports (Income Statement, Profit& Loss Statement, EBIT, ROIC Reports) by using SSRS.\n\u2022 Developed parameterized dynamic performance Reports (Gross Margin, Revenue base on geographic regions, Profitability based on web sales and smart phone app sales) and ran the reports every month and distributed them to respective departments through mailing server subscriptions and SharePoint server.\n\u2022 Designed and developed new reports and maintained existing reports using Microsoft SQL Reporting Services (SSRS) and Microsoft Excel to support the firm's strategy and management.\n\u2022 Created sub-reports, drill down reports, summary reports, parameterized reports, and ad-hoc reports using SSRS.\n\u2022 Used SAS/SQL to pull data out from databases and aggregate to provide detailed reporting based on the user requirements.\nUsed SAS for pre-processing data, SQL queries, data analysis, generating reports, graphics, and statistical analyses.\n\u2022 Provided statistical research analyses and data modeling support for mortgage product.\n\u2022 Perform analyses such as regression analysis, logistic regression, discriminant analysis, cluster analysis using SAS programming.\nEnvironment: SQL Server 2008 R2, DB2,Oracle,SQL Server Management Studio, SAS/ BASE, SAS/SQL, SAS/Enterprise Guide, MS BI Suite(SSIS/SSRS), T-SQL, SharePoint 2010, Visual Studio 2010,Agile/SCRUM"", u""Data Analyst\nExceloid Soft Systems, India - IN\nNovember 2011 to March 2013\nResponsibilities:\n\u2022 Wrote SQL queries for data validation on the backend systems and used various tools like TOAD& DB Visualizer for DBMS(Oracle)\n\u2022 Perform Data analysis, Backend Database testing, Data Modeling and Developing SQL Queries to solve problems and meet user's need for Database management in Data Warehouse.\n\u2022 Utilize object-oriented languages, concepts, database design, star schemas and databases.\n\u2022 Create algorithms as needed to manage and implement proposed solutions.\n\u2022 Participate in test planning and test execution for functional, system, integration, regression, UAT (User Acceptance Testing), load and performance testing.\n\u2022 Work with test automation tools for recording/coding in Database, and execute in regression testing cycles.\n\u2022 Transferred data from various OLTP data sources, such as Oracle, MS Access, MS Excel, Flat files, CSV files into SQL Server.\n\u2022 Working with Databases DB2, Oracle DM, SQL Server for Database testing and maintenance.\n\u2022 Involved in writing and executing User Acceptance Testing (UAT) with end users.\n\u2022 Involved in Post- Implementation validations after the changes have been to the Data Marts.\n\u2022 Chart out Graphs, and Reports alike in QC to point out the percentage of Test Cases passed, and thereby to point out the percentage of Quality achieved and uploading the status daily to ART reports an in-house tool.\n\u2022 Performed extensive Data Validation, Data Verification against Data Warehouse.\n\u2022 Used UNIX to check the Data marts, Tables and Updates made to the tables.\n\u2022 Writing advanced SQL Queries to query the data from Data marts and Landings to verify the changes has been made.\n\u2022 Involved in Client requirement gathering, participated in discussion & brain storming sessions and documented requirements.\n\u2022 Validating and profiling Flat File Data into Teradata tables using UNIX Shell scripts.\n\u2022 Actively participated Functional, System and User Acceptance testing on all builds and supervised releases to ensure system / functionality integrity.\n\u2022 Closely interacted with designers and software developers to understand application functionality and navigational flow and keep them updated about Business user sentiments.\n\u2022 Interacted with developers to resolve different Quality Related Issues.\n\u2022 Wrote and executed manual test cases for functional, GUI, and regression testing of the application to make sure that new enhancements do not break working features\n\u2022 Writing and executing Manual test cases in HP Quality Center.\n\u2022 Wrote test plans for positive and negative scenarios for GUI and functional testing\n\u2022 Involved in writing SQL queries and stored procedures using Query Analyzer and matched the results retrieved from the batch log files\n\u2022 Created Project Charter documents & Detailed Requirement document and reviewed with Development & other stake holders.\nEnvironment: Subversion, Tortoise SVN, Jira, Agile-Scrum, Web Services, Mainframe, Oracle, Perl, UNIX, LINUX, Shell Scripts, UML, Quality Center, , RequisitePro, SQL, MS Visio, MS Project, Excel, Power Point, Word, SharePoint, Win XP/7 Enterprise."", u""Data Analyst/Data Modeler\nZEN3 Infosolutions, India - IN\nApril 2009 to October 2011\nResponsibilities:\n\u2022 Data analysis and reporting using MY SQL, MS Power Point, MS Access and SQL assistant.\n\u2022 Involved in MY SQL, MS Power Point, MS Access Database design and design new database on Netezza which will have optimized outcome.\n\u2022 Used DB2 Adapters to integrate between Oracle database and Microsoft SQL database in order to transfer data\n\u2022 Designed the data marts using the Ralph Kimball's Dimensional Data Mart modeling methodology using ER Studio.\n\u2022 Involved in writing T-SQL, working on SSIS, SSRS, SSAS, Data Cleansing, Data Scrubbing and Data Migration.\n\u2022 Used Normalization methods up to 3NF and De-normalization techniques for effective performance in OLTP systems.\n\u2022 Initiated and conducted JAD sessions inviting various teams to finalize the required data fields and their formats.\n\u2022 Involved in designing and implementing the data extraction (XML DATA stream) procedures.\n\u2022 Created base tables, views, and index. Built a complex Oracle procedure in PL/SQL for extract, loading, transforming the data into the warehouse via DBMS Scheduler from the internal data.\n\u2022 Involved in writing scripts for loading data to target data Warehouse using Bteq, Fast Load, MultiLoad\n\u2022 Create ETL scripts using Regular Expressions and custom tools (Informatica, Pentaho, and Sync Sort) to ETL data.\n\u2022 Developed SQL Service Broker to flow and sync of data from MS-I to Microsoft's master database management (MDM).\n\u2022 Extensively involved in Recovery process for capturing the incremental changes in the source systems for updating in the staging area and data warehouse respectively\n\u2022 Strong knowledge of Entity-Relationship concept, Facts and dimensions tables, slowly changing dimensions and Dimensional Modeling (Star Schema and Snow Flake Schema).\n\u2022 Involved in loading data between Netezza tables using NZSQL utility.\n\u2022 Worked on Data modeling using Dimensional Data Modeling, Star Schema/Snow Flake schema, and Fact & Dimensional, Physical & Logical data modeling.\n\u2022 Generated Stats pack/AWR reports from Oracle database and analyzed the reports for Oracle8.x wait events, time consuming SQL queries, table space growth, and database growth.\nEnvironment: ER Studio, MY SQL, MS Power Point, MS Access, MY SQL, MS Power Point, MS Access, Netezza, DB2, T-SQL, DTS, Informatica MDM, SSIS, SSRS, SSAS, ETL, MDM, 3NF and De-normalization, Teradata, Oracle8.x, (Star Schema and Snow Flake Schema) etc.""]","[u""Bachelor's""]",[u'']
0,https://resumes.indeed.com/resume/70d41134150b41f4,"[u'Data Analyst Intern\nAdroitent\nJanuary 2018 to Present\nWorking as a Data Analyst in a project which is designed to create and develop a Quiz Engine with multiple choice answers for healthcare companies.\n\u2022 Assisted in the process of developing Data Cleaning and Data Transformation pipelines in Python and SQL.\n\u2022 Utilized parts of speech tagging(POS) to identify various nouns, noun phrases, verbs, adjectives, conjunctions etc.\n\u2022 Implemented ""word embedding"", which is the vector representation of words which maps words and word phrases from the vocabulary to the vectors of real numbers.This representation is done with the help of ""word2vec"".\n\u2022 Utilized Keras - a deep learning library in python for optimized tensor manipulation on GPU and CPU.', u'Data Analyst\nOmni Med Solutions\nJune 2016 to December 2016\nWorked as a Data Analyst in a project which was designed to build a health care management system that focuses on revenue cycle\nmanagement solutions with a primary goal to optimize revenue and ensure compliance.\nResponsibilities\n\u2022 Defined the detailed application and database design.\n\u2022 Created Queries using SQL and PL/SQL. Implemented the database connectivity with Oracle database as backend.\n\u2022 Improved advanced excel functions to generate pivot tables and spreadsheets.\n\u2022 Manipulating, cleansing & processing data using Microsoft Excel and SQL.\n\u2022 Responsible for loading, extracting and validation of client data.\n\u2022 Compiled business reports from gathered information and participated in building presentations.\n\u2022 Contributed in data administration, security and design to ensure the quality of database.']","[u'Masters in Information Systems', u'Bachelors in Information Technology']","[u'Stevens Institute of Technology Hoboken, NJ\nMay 2018', u'VNR Vignana Jyothi Institute of Engineering and Technology Hyderabad, Telangana\nSeptember 2012 to May 2016']"
0,https://resumes.indeed.com/resume/1e1e2ab53d67fb32,"[u'Data Analyst Intern\nData Science Center at UC Davis - Davis, CA\nJuly 2017 to September 2017\n\u2022 Developed a database of zoonotic diseases (transferred from animals to humans) including information such as the animal species, family, genus, virus, region of study, diagnostic tests used\n\u2022 Programmatically read ~ 4,000 papers and filled out a metadata table using R-programming; opened, scanned, and sorted documents, cleaned and organized data, analyzed and reported data trends\n\u2022 Collaborated in a team of three using a Docker platform; discussed, reported, and solved programmatical and biological errors', u'Data Analyst Intern\nBioinformatics Facility of UC Davis - Davis, CA\nOctober 2016 to January 2017\n\u2022 Utilized Linux operating system and Github repository hosting service\n\u2022 Sequenced Quality Control and data analysis: dimension reduction, duplicate deletion, sequence trimming, quantifying abundance of genes, exons, and more\n\u2022 Distinguished effective and ineffective quality control applications\n\u2022 Studied multivariate analysis of ecological communities in the human microbiome\n\u2022 Interacted with software engineers as well as the associated lab manager']",[u'B.S. in Mathematical and Scientific Computation'],"[u'UC Davis Davis, CA\nSeptember 2013 to June 2018']"
0,https://resumes.indeed.com/resume/47bc331603bc2441,"[u""Operations Data Intern\nInteractive Advertising Bureau\nOctober 2017 to Present\n\u2022 Worked with Operations and Investment & Relations team to administer and regulate data quality and data Integrity\n\u2022 Supervised weekly membership data and ensure adherence to IAB's data governance entry policies in Salesforce\n\u2022 Built and maintained data driven excel files shared between vendors and operations team within the company\n\u2022 Evaluated current processes and developed then integrated automation improvements using python for reporting\n\u2022 Architected SQL queries in toad to fetch data and Tableau to design template grids to control and improve workflow\n\u2022 Analyzed the vendor's participations in different C&C's and targeted top 75 accounts with maximum participation\n\u2022 Designed and built various dashboards in tableau for executives to recognize top 10 membership participations in events\n\u2022 Provided key insights to the team to develop and modify the interface to provide best user experience to sales team"", u""Data Analyst\nOnshore Constructions Company Pvt. Ltd\nJuly 2014 to June 2015\n\u2022 Conducted data clean up and data preparation on clients' supplied data to monitor its completeness and accuracy\n\u2022 Worked with cross-functional departments to provide data driven analytics in form of ad-hoc reports using MS Excel\n\u2022 Built easy to understand dashboards and reports which aid decision making and organization strategy development\n\u2022 Performed market analysis to meet the objectives effectively, saving inventory by 2% supporting the overall business\n\u2022 Designed dashboards in Excel to analyse and provide insights for the ongoing projects based on historical patterns""]","[u'Master of Science in Information Systems in Information Systems', u'Bachelor of Engineering in Information Technology in Information Technology']","[u'Pace University, Seidenberg School of CSIS New York, NY\nAugust 2015 to May 2017', u'University of Mumbai, PVPP College of Engineering Mumbai, Maharashtra\nFebruary 2010 to June 2014']"
0,https://resumes.indeed.com/resume/6fdcaa6b2ddffecd,"[u'Data Scientist\nTEKsystems (Ford) - Dearborn, MI\nMay 2017 to Present\nLogistics Optimization\nDevelop, maintain, and optimize existing processes for ETL.\nCommunicate with business to gather suggestions for enhancements and addressing concerns.\nDesign and test different picking algorithms.\n\nQlikView\nDesign dashboards for comparing data quality of different data sources.\n\nSQL\nCreate tables for supporting workflows and dashboards.', u'Market Analyst\nBlueChip Talent (Latcha) - Farmington Hills, MI\nOctober 2016 to May 2017\nTableau\nAutomation and standardization of monthly marketing metrics report.\nData source structuring and construction.\nDevelop ad hoc reports.\n\nFile Validation\nDevelop Python script for verifying quality of mailing lists and standardized list quality descriptions.\n\nSQL\nOptimize stored procedures for readability and scalability.\nCombining target lists from different vendors to determine marketing audience.', u'Systems Architect\nCDI Contractor (Ford) - Dearborn, MI\nOctober 2014 to September 2016\n\u25cf Lead Qlikview and Tableau developer\n\u27a2 Ad Hoc visualizations combining disparate data sources across different servers and file\nformats\no Charting server ages and location\no Measuring discrepancies between different data sources\no Text parsing for data cleansing\no Implementing interval matching to connect ranged data sets\n\u27a2 Consolidate data sources to ease update of visualization\n\u25cf Qlikview Enterprise Technology Metric Dashboard\n\u27a2 Worked with business to determine standards needed for tracking IT software/application\ncomplexity\n\u27a2 Engineered solution and process for measuring status\no Systematically applying different criteria to define states and aggregate statuses\no Created workflow for repeatability\n\u27a2 Used best practices for applying aggregations in script and transforming data before loading\n\u27a2 Used Qlikview set analysis features for drill downs and different views\n\u25cf Tableau Technology Usage Dashboard\n\u27a2 Inherited dashboard for making updates and changes\n\u27a2 Made alterations to migrate data sources and include new metrics\n\u27a2 Added new parameters for increased visibility\n\u25cf Provide technical support and clarification on visualization design and purpose\n\u25cf Developed database and reporting output for complexity scheduling tool\n\u27a2 Worked with international engineering teams to devise method for keeping track of assembly\ninformation\n\u27a2 Using their specifications created a familiar and more robust method for planning and keeping\ntrack of changes', u'Data Analyst\nIPG Contractor (GM) - Warren, MI\nMarch 2014 to October 2014\n\u25cf Developed database and reporting for tracking rental vehicles associated with ignition switch recall\n\u27a2 Provided improved, practical method of providing status of rental vehicles during recall\n\u27a2 Designed database and wrote code for daily standardized reporting\no Performed QA testing and made changes to incorporate new requirements and\nrequests\n\u27a2 Database features:\no Combining data sources\no Cleansing data for joins\no Repeatable process for standardized reporting\no Writing and optimizing queries to track progress\n\u25cf Optimize and maintain Excel VBA macros and add ons\n\u27a2 Studied old macros and implemented best practices to improve performance and functionality of\nadd ons\n\u27a2 Added web scraping functionality\n\u25cf Spent 2 days standardizing and formatting information in order to automate report generation saving at\nleast 3 days of work a month\n\u27a2 Wrote macro for formatting Excel into standardized PowerPoint output\n\u25cf Quality assurance of Cognos reports after updates', u'Data Analyst\nMoeller Manufacturing\nFebruary 2013 to March 2014\n\u25cf Proposed and lead project harvesting historical data for production scheduling\n\u27a2 Writing batch files and Python scripts to consolidate network data\n\u27a2 Devised simple machine learning algorithm for schedule predictions\n\u25cf Developed plant layout production tracker\n\u27a2 Wrote updates to parts tracker to implement network crawler\n\u27a2 Performed routine checking and accounting of parts production\n\u25cf Designed and created Oracle database and web interface for tool tracking']",[u'BS in Mathematics'],[u'Michigan Technological Univ\nJanuary 2008 to January 2012']
0,https://resumes.indeed.com/resume/514bee632bc850db,"[u""RVU Analyst\nLexington Medical Center - West Columbia, SC\nMay 2015 to Present\nCalculate physicians' productivity of Retail Value Units (RVUs) by using advanced functions and formulas in Microsoft Excel 2010. Current Procedural Terminology (CPT) codes are used to calculate total RVUs. Proficient level of knowledge of current procedural terminology codes to include all CPT-4 codes.\n\u2022 Prepares assigned recurring and non-recurring journal entries for the month-end closing process.\n\u2022 Reviews data received from other departments and subsystems for completeness and accuracy.\n\u2022 Run data reports as needed from EPIC database to ensure quality of reporting by physicians, practice managers, and coding & billing associates\n\u2022 Analyzes ledger accounts quarterly and reports analysis to management\n\u2022 Performs reviews, analyses, research, and other related duties independently, or as requested by management"", u'Data Analyst\nPricewaterhouse Coopers, LLC - Columbia, SC\nJune 2012 to May 2015\nInterpreted data and analyzed results using statistical techniques and provide ongoing reports for clients in Advisory.\n\u2022 Technical expertise regarding data models, database design development, data mining and segmentation techniques\n\u2022 Knowledge of statistics and experience using statistical packages for analyzing datasets (Excel, Oracle, and Access)\n\u2022 Strong analytical skills with the ability to collect, organize, analyze, and disseminate significant amounts of information with attention to detail and accuracy\nDescription of Training and Certifications']","[u'Master of Science in Accounting in Controllership', u'Bachelor of Science in Business Administration in accounting']","[u'STRAYER UNIVERSITY Columbia, SC\nDecember 2013', u'CLAFLIN UNIVERSITY Orangeburg, SC\nMay 2012']"
0,https://resumes.indeed.com/resume/9fbcfe523ddfbebf,"[u'Data Analyst/ Business Analyst\nAnthem, Inc. - Cerritos, CA\nOctober 2017 to Present\n\u2022Acted as lead business analyst by working across business and technical teams to support enhancement projects/programs, lights-on activity, and issues/defects within the analytic environment.\n\u2022Established and maintained excellent knowledge of data warehouse data structure design, definitions, capabilities, programming languages, and data integrity issues.\n\u2022Analyzed more complex business user needs, documents information requirements, facilitates the identification of potential gaps in available data and collaborates with business areas to determine best means for acquiring the data needed.\n\u2022Ensured proper data definitions, ownership, use and integrity of data.\n\u2022 Collaborated with IT resources and data owners to define business objectives and definitions for databases including business rules, sources, purge archive criteria, and reload schedule.\n\u2022Supported the validation and testing of data structure design, definitions and mapping, including business rules.\n\u2022Collaborated with IT resources on the creation and maintenance of the content of the data warehouse, establishes and maintains strong knowledge of data warehouse data structure design, definitions, capabilities, SQL, and data integrity issues, completes metadata assessment and reviews feedback to determine changes, ensures that data definitions are clear and current, and communicates changes to data definitions to all users.\n\u2022Acted as lead business analyst by working across business and technical teams to support enhancement projects/programs, lights-on activity, and issues/defects within the analytic environment.\n\u2022Established and maintained excellent knowledge of data warehouse data structure design, definitions, capabilities, programming languages, and data integrity issues.\n\u2022Analyzed more complex business user needs, documents information requirements, facilitates the identification of potential gaps in available data and collaborates with business areas to determine best means for acquiring the data needed.\n\u2022Ensured proper data definitions, ownership, use and integrity of data.\n\u2022Collaborated with IT resources and data owners to define business objectives and definitions for databases including business rules, sources, purge archive criteria, and reload schedule.\n\u2022Supported the validation and testing of data structure design, definitions and mapping, including business rules.\n\u2022Collaborated with IT resources on the creation and maintenance of the content of the data warehouse, establishes and maintains strong knowledge of data warehouse data structure design, definitions, capabilities, SQL, and data integrity issues, completes metadata assessment and reviews feedback to determine changes, ensures that data definitions are clear and current, and communicates changes to data definitions to all users.', u'Business Analyst\nUnitedHealth Group - Cypress, CA\nJuly 2011 to September 2017\n\u2022Extract data from existing data warehouse and other data sources using SQL & other extraction tools.\n\u2022Expert in interpreting National Committee for Quality Assurance (NCQA) measure specification-logic, generating various reports, suggesting quality improvements and strategy formulation for the company in order to improve Healthcare Effectiveness Data and Information Set (HEDIS) score; thus improving CMS Medicare STAR rating.\n\u2022Support the development of data warehouse documentation, metrics catalogs, and consolidated analytic menus.\n\u2022Work with end users to understand data needs and create reports that cater to providers\u2019 process improvement.\n\u2022Work as business liaison to technology teams (IT) for project discussions and planning.\n\u2022Adheres to SDLC and change control procedures.\n\u2022Creating/managing various databases for Quality improvement team and implementing new processes.\n\u2022Led and managed customer readiness activities and project implementation teams in process improvement activities.\n\u2022Led implementation efforts. Developed implementation plans. Defined test conditions and scenarios and developed accurate and complete test plans to ensure issues are identified, tracked, reported on and resolved in a timely manner. Communicated changes to development team; if applicable.']","[u'Bachelor of Science in Health Care Administration', u'in Liberal Arts Science and Mathematics']","[u'California State University Long Beach Long Beach, CA\nAugust 2014', u'Cypress College Cypress, CA\nJune 2012']"
0,https://resumes.indeed.com/resume/cbd57004cd07cf51,"[u'Data Analyst /Data entry Clerk\nINTERNATIONAL TOWER LIGHTNING - La Vergne, TN\nJuly 2017 to December 2017\nAnalyzing data of daily Sales from different sources to create reports and validating\nmissing information. Creating reports from analysis. Maintaining and adding new\ninformation into databases. Validating data from customers in databases. Contacting\ncustomers by email and phone. Data entry of daily sales into databases.', u'Data Analyst /Data entry Clerk\nATCO - La Vergne, TN\nJanuary 2017 to June 2017\nPerforming Analysis of information from buyers and adding them to multiple\ndatabases.Creating Reports. Data Entry of budget spending into databases. Validating and fixing missing information in Excel and Access databases from customers and buyers sources.', u""Data Analyst /Data entry Clerk\nSAKS FIFTH AVENUE - La Vergne, TN\nMarch 2014 to November 2016\nPerforming data analysis from company sales and creating reports. Validating and adding new information from customer's sources to multiple databases and contacting\nvendors, buyers and customers. Creating weekly reports from databases. Daily data\nanalysis from multiple sources."", u""Data Analyst /Data entry Clerk\nPETREX - Lima, PE\nJanuary 2006 to December 2013\nData analysis of company money spending from databases. Daily Analysis of budget\nspending from Onshore and Offshore work operations. Adding new data from employer's\ninformation to databases. Validating data, creating reports from databases. Data\nentry of big quantity of information from different sources. Validating excel\nformulas and creating new ones when necessary. Reporting budget spending of multiple\noperations to Manager and Finance Area.""]","[u""Bachelor's Degree in Biology""]","[u'Cayetano Heredia University Lima, PE\nJanuary 1999 to December 2003']"
0,https://resumes.indeed.com/resume/1deebc003fa79789,"[u'Analyst Intern\nSawyer Studios - New York, NY\nFebruary 2017 to Present\n\u2022 Designed and maintained an ETL process to gather campaign performance data.\n\u2022 Improved data integrity by optimizing campaigns and results.\n\u2022 Reduced feedback time by 80% with dashboard to enable clients to track marketing\ncampaigns in real-time.\n\u2022 Designed an interactive standard reporting dashboard application using Shiny\npackage in R.\n\u2022 Worked with social platform (i.e. Facebook Business Manager), Google AdWords and TheTradedesk.', u""Data Analyst\nWebosphere - IN\nSeptember 2014 to March 2016\nPrepared concise data reports and data visualizations according to client's\nrequirements.\n\u2022 Produced daily, weekly and monthly reports to track KPIs.\n\u2022 Performed data analytics for several e-commerce processes.\n\u2022 Worked with Amazon S3 and AWS Redshift""]","[u'Master of Science in Computer Science', u'Bachelor of Technology in Information Technology']","[u'New York Institute of Technology\nDecember 2018', u'Gandhinagar Institute of Technology\nJanuary 2010 to January 2014']"
0,https://resumes.indeed.com/resume/5be8878791193eb2,"[u'Global Service Data Scientist\nAbbott Laboratories\nOctober 2016 to Present\n- Developed 2 new prognostic health monitoring (PHM) algorithms by analyzing real-time instrument data with\nR. The first algorithm used an ROC Analysis to set a failure threshold. The second algorithm used an ensemble of 4 different models to identify anomalous data points in unlabeled data.\n- Implemented 20+ PHM algorithms, for a series of diagnostic instruments, by translating algorithm parameters into SQL code.\n- Created an application to automatically monitor each algorithm daily, and distribute a report via email\ncontaining any algorithm violations.', u'Data Management Data Analyst\nAbbott Laboratories\nNovember 2014 to October 2016\n- Responsible for the development, improvement and maintenance of 20+ fully automated metric reporting applications for multiple diagnostic instrument platforms. Applications were developed using JSL (an object- oriented programming language) and SQL, and resulted in a significant reduction in man hours. For example, an approximate reduction of 4 headcount for 1 week each month for one application.\n- Contributed to the development of instrument performance metrics, and their visualizations, for weekly and monthly status reports. Example metrics included Mean Tests Between Failures, Calls Per Year and Instrument\nAvailability as measured by Uptime.', u""Prognostic Health Monitoring Data Analyst\nAbbott Laboratories\nOctober 2013 to November 2014\n- Identified instrument components/indicators suitable for PHM, as measured and prioritized by the frequency of related errors, service calls and part replacements, by completing a historical analysis on instrument and service data.\n- Developed PHM infrastructure for Abbott's next generation diagnostic instruments, as measured by successful\nPHM data collection from various instrument servo motors, through collaboration with internal and external\nsoftware and engineering groups.\n- Achieved 40% reduction in customer downtime (75% reduction in unplanned downtime), greater than $500k\nannual savings and increased customer satisfaction, as measured by current field service data and NPS surveys, by implementing three PHM algorithms for Abbott's on-market Architect i2000, i2000SR and i1000SR\nimmunoassay analyzers.\n- Advanced JSL and SQL scripting skills to a high intermediate level by creating complex, automated daily\napplications to collect, analyze and display instrument data in violation of PHM algorithms."", u""Reliability Data Analyst\nAbbott Laboratories\nNovember 2012 to March 2014\n- Improved the instrument reliability data entry application, as measured by internal customer satisfaction and feedback, by redesigning the way instrument part and error information is entered and structured for all of\nAbbott's next generation diagnostic instruments.\n- Increased metric reporting efficiency, as measured by a 50% reduction in time and headcount, by automating the generation of metric data and plots through JMP scripting for weekly and monthly reports.\n- Developed and forecasted an Instrument Availability metric, as measured by hours of planned and unplanned\ndowntime, by completing a historical data analysis on Abbott's on-market versus next generation\nimmunoassay analyzers.""]","[u'M.S. in Predictive Analytics', u'B.S. in Biomedical Engineering']","[u'DePaul University\nMarch 2015 to November 2017', u'University of Wisconsin Madison, WI\nAugust 2005 to December 2009']"
0,https://resumes.indeed.com/resume/a7301909ba513904,"[u""Data Analyst Intern\nRevenue Solutions Inc - Harrisburg, PA\nMay 2017 to December 2017\n\u2022 Analyzed various claims data in R, SQL. Performed exploratory data analysis and statistical modeling techniques such as regression, decision trees, correlation analysis, ANOVA, clustering techniques to understand the revenue of products and define Source of business metrics.\n\u2022 Performed data analysis using various SQL Analytical functions like LEAD, LAG, ROW_NUMBER, and GROUP BY in Oracle environment.\n\u2022 Developed dashboards and report automation process using Visual Basic (VBA)and MS Excel to improve turnaround time and reporting capabilities.\n\u2022 Defined KPI's, Source of Business metrics and develop various dashboards in Tableau."", u'Data Analyst\nTechLumi Solutions Pvt Ltd - Hyderabad, Telangana\nJanuary 2015 to December 2015\n\u2022 Analyzed readings collected by smart electric meters for efficient energy management using R & Python.\n\u2022 Performed data integrity checks, data cleaning, exploratory analysis and feature engineer using R and Python\n\u2022 Performed statistical analysis of smart meter data using Pandas, Numpy, scikit learn.\n\u2022 Identified, analyzed and interpreted trends or patterns in complex data sets using data mining tools.\n\u2022 Built regression models, did parameter tuning using caret package and did visualizations of data using matplotlib and ggplot2 and Documented code using Jupyter notebook.']",[u'Master of Sciences in Sciences'],"[u'University Of Illinois Computer Science Springfield, IL\nMay 2017']"
0,https://resumes.indeed.com/resume/1fdb1e90d197eb14,"[u'Sr. Data/Business Analyst/Data Modeler\nFannie Mae - Mae, WA\nJanuary 2017 to Present\nThe EAR (Earnings at Risk) Project was to build a centralized data mart, bringing together all the different databases to one data mart and implement that data on to cloud using Amazon Web Services.\n\nResponsibilities:\n\u2022 Perform the set of tasks and techniques, work as a liaison among stakeholders in order to understand the structure, policies, and operations of the Business Process and Operations, and to gather data requirements to achieve its goals.\n\u2022 Performed Data Analysis on generating Canned as well as Ad-Hoc reports and Data mapping activities for the reports using various tools like OBIEE and Tableau\n\u2022 Gathering, analyzing, and documenting business objectives and understanding the data warehouses and collaborated with Data modelers, ETL developers in creating the Data Functional Design documents.\n\u2022 Create various Data Mapping Repository documents as part of Metadata services (EMR).\n\u2022 Used ER Studio Data Modeler for data modeling (data requirements analysis, database design etc.) of custom developed information systems, including databases of transactional systems and data marts.\n\u2022 Document the complete process flow to describe program development, logic, testing, and implementation, application integration, coding.\n\u2022 Performed Data analysis and Data profiling using complex SQL on various sources systems including Oracle.\n\u2022 Interact with Business System Analysts and Software Developers to transform business requirements and application requirements into appropriate data model solutions.\n\u2022 Used ER Studio 9.5 to create Logical and Physical Data Models of the Customer ODS using best practices to ensure high data quality near to third normal form.\n\u2022 Created Source to Target Mapping document with all the transformation rules.\n\u2022 Produced Entity relationship diagrams, Function relationship diagrams, data flow diagrams and enforced all referential integrity constraints..\n\u2022 Implemented Referential Integrity using Primary key and Foreign key relationships.\n\u2022 Maintained metadata in ER Studio model and provided Data Dictionary to the team.\n\u2022 Created Data Dictionaries for all 10 applications used from the Single Family Credit Team and Multifamily Credit Team', u'Sr. Business Analyst\nCigna Healthcare - Stamford, CT\nFebruary 2014 to September 2016\nThe deliverable of the project was to build a robust system to record enrollment eligibility feed for Medicare and Medicaid plans which would consolidate different means of data transfer within the organization\n\nRESPONSIBILITIES:\n\u2022 Compiled the Vision Document and composed detailed Business Requirements Document (BRD), Use Case Specification Documents.\n\u2022 Used UML to perform Use-Case analysis to capture the dynamic aspect of the application.\n\u2022 Interacted with stakeholders and gathered requirements as per the business needs using Scrum-waterfall methodology.\n\u2022 Extensive knowledge in ANSI X12 EDI 834 file format and in Medicare & Medicaid eligibility requirements.\n\u2022 Worked extensively in Medicaid eligibility and Medicaid Management Information Systems.\n\u2022 Involved with Quality Assurance team to focus on Web based enterprise applications.\n\u2022 Worked on cloud based technologies including Saas/Paas/IaaS.', u'Sr. Business Analyst\nTractor Supply - Nashville, TN\nNovember 2012 to December 2013\nResponsibilities:\n\u2022 Acquire and interpret business requirements, create technical artifacts, and determine the most efficient/appropriate solution design, thinking from an enterprise-wide view.\n\u2022 Involved in complete Software Development Lifecycle Experience (SDLC) from Business Analysis to Development, Testing, Deployment and Documentation.\n\u2022 Managed Talend Migration/Upgrade, maintaining existing Talend Jobs, improving/template current jobs, migration of Major Talend Job to new process design, supporting data quality for auditing. Also, Rewritten SAP jobs into Talend Open Source Studio.\n\u2022 Explore prebuilt ETL metadata, mappings and DAC metadata and Develop and maintain SQL code as needed for SQL Server database.\n\u2022 Performed data manipulations using various Talend components like tMap, tJavarow, tjava, tOracleRow, tOracleInput, tOracleOutput, tMSSQLInputand many more.\n\u2022 Provided Teradata Data Warehouse Administration and maintain existing physical and logical model\n\u2022 Involved in Migration projects to migrate data from data warehouses on Oracle/DB2 and migrated those to Teradata, Netezza.\n\u2022 Involved in designing dimensional modeling and data modeling using Erwin tool.\n\u2022 Main deliverables consisting artifacts such as - BRDs, FRDs, and RTMs.', u'Business Analyst/Data Analyst\nHumana\nAugust 2010 to October 2012\nAs EIM Implementation Coordinator, directed implementation and change management activities for the Department of Developmental Services (DDS) and the Department of Mental Health (DMH), supporting the implementation of Enterprise Invoice Management (EIM).\nResponsibilities:\n\u2022 Experience with Medicare and Medicaid\n\u2022 Conducted interviews with the SME and gathered information about Health care Enrollment, Billing and Claims processing.\n\u2022 The user interface (UI) screens were mocked up in Microsoft Visio and reviewed with the end users prior to programming to solicit User Acceptance.\n\u2022 Ensured that project followed Business Rules and was in compliance with HIPAA rules to display minimum benefit information and others.\n\u2022 Worked with end users to define business design solutions (e.g. new workflows, systems, programs, reports).\n\u2022 Created Report Mockups and Sample Mapping documents for the Error Reporting and flowcharts on how to use the system.\n\u2022 Created UAT plans with several test cases for each project to ensure that the system runs smoothly after the proposed enhancements.\n\u2022 Facilitated and scribed JAD sessions, user meetings, and interviews to extract business requirements to confirm that those requirements are in alignment with the expected results.\n\u2022 Created T-SQL statements, stored procedures to query the custom claims OLAP & OLTP databases.']",[u'in Business Skills'],[u'HP Quality Center']
0,https://resumes.indeed.com/resume/016e89dc09618fb2,"[u'Data Scientist\nNew York Community Bank - New York, NY\nFebruary 2014 to July 2015\nDescription: New York Community Bank operates as a New York state-chartered savings bank that provides personal and business banking products and services to customers in Metro New York, New Jersey, Florida, Ohio, and Arizona. The project is to build a complex system that can analyze, calculate, and evaluate margin risks. The system should also report, monitors and alerts on margins, gain/loss, market value at client, and trade levels.\n\nResponsibilities:\n\u2022 Utilized Apache Spark with Python to develop and execute Big Data Analytics and Machine learning applications, executed machine Learning use cases under Spark ML and Mllib.\n\u2022 Identified areas of improvement in existing business by unearthing insights by analyzing vast amount of data using machine learning techniques.\n\u2022 Interpret problems and provides solutions to business problems using data analysis, data mining, optimization tools, and machine learning techniques and statistics.\n\u2022 Designed and developed NLP models for sentiment analysis.\n\u2022 Led discussions with users to gather business processes requirements and data requirements to develop a variety of Conceptual, Logical and Physical Data Models. Expert in Business Intelligence and Data Visualization tools: Tableau, Microstrategy.\n\u2022 Worked on machine learning on large size data using Spark and MapReduce.\n\u2022 Let the implementation of new statistical algorithms and operators on Hadoop and SQL platforms and utilized optimizations techniques, linear regressions, K-means clustering, Native Bayes and other approaches.\n\u2022 Developed Spark/Scala, Python for regular expression (regex) project in the Hadoop/Hive environment with Linux/Windows for big data resources.\n\u2022 Data sources are extracted, transformed and loaded to generate CSV data files with Python programming and SQL queries.\n\u2022 Stored and retrieved data from data-warehouses using Amazon Redshift.\n\u2022 Worked on TeradataSQL queries, Teradata Indexes, Utilities such as Mload, Tpump, Fast load and FastExport.\n\u2022 Used Data Warehousing Concepts like Ralph Kimball Methodology, Bill Inmon Methodology, OLAP, OLTP, Star Schema, Snow Flake Schema, Fact Table and Dimension Table.\n\u2022 Refined time-series data and validated mathematical models using analytical tools like R and SPSS to reduce forecasting errors.\n\u2022 Worked on data pre-processing and cleaning the data to perform feature engineering and performed data imputation techniques for the missing values in the dataset using Python.\n\u2022 Created Data Quality Scripts using SQL and Hive to validate successful dasta load and quality of the data. Created various types of data visualizations using Python and Tableau.\n\nEnvironment: Hadoop, Map Reduce, Spark, Spark MLLib, Tableau, SQL, Excel, VBA, SAS, Matlab, AWS, SPSS, Cassandra, Oracle, MongoDB, SQL Server 2012, DB2, T-SQL, PL/SQL, XML, Tableau.', u'Data Scientist\nFirst Atlantic Bank - Jacksonville, FL\nAugust 2012 to January 2014\nDescription: First Atlantic Bank gives subsidizes or advances to individuals with little business prerequisites. Candidates get their advances authorized taking into account their record of loan repayment. The candidate data is kept up in a database alongside the points of interest of the advance for reimbursement. This information is filtered into diverse classifications in light of parameters like kind of record, advance sum, due date. The filtered information is utilized for insights for producing report.\n\nResponsibilities:\n\u2022 Collaborates with cross-functional team in support of business case development and identifying modeling method (s) to provide business solutions. Determines the appropriate statistical and analytical methodologies to solve business problems within specific areas of expertise.\n\u2022 Generating Data Models using Erwin9.6 and developed relational database system and involved in Logical modeling using the Dimensional Modeling techniques such as Star Schema and Snow Flake Schema.\n\u2022 Guide the full lifecycle of a Hadoop solution, including requirements analysis, platform selection, technical architecture design, application design and development, testing, and deployment\n\u2022 Consult on broad areas including data science, spatial econometrics, machine learning, information technology and systems and economic policy with R\n\u2022 Performed Datamapping between source systems to Target systems, logicaldata modeling, created classdiagrams and ERdiagrams and used SQLqueries to filter data\n\u2022 Enabled speedy reviews and first mover advantages by using Oozie to automate data loading into the Hadoop Distributed File System and PIG to pre-process the data.\n\u2022 Used various techniques using R data structures to get the data in right format to be analyzed which is later used by other internal applications to calculate the thresholds.\n\u2022 Maintaining conceptual, logical and physical data models along with corresponding metadata.\n\u2022 Done data migration from an RDBMS to a NoSQL database, and gives the whole picture for data deployed in various data systems.\n\u2022 Developed triggers, stored procedures, functions and packages using cursors and ref cursor concepts associated with the project using PLSQL\n\u2022 Used Meta data tool for importing metadata from repository, new job categories and creating new data elements.\n\nEnvironment: R, Oracle 12c, MS-SQL Server, Hive, NoSQL, PL/SQL, MS- Visio, Informatica, T-SQL, SQL, Crystal Reports 2008, Java, SPSS, SAS, Tableau, Excel, HDFS, PIG, SSRS, SSIS, Metadata.', u'Data Scientist/Data Analyst\nAssurant Specialty Property - Santa Ana, CA\nMay 2008 to December 2010\nDescription: Assurant partners with leaders in mortgage lending, manufactured housing, multifamily housing and other industries to protect client and consumer property.\n\nResponsibilities:\n\u2022 Worked on data cleaning and reshaping, generated segmented subsets using Numpy and Pandas in Python\n\u2022 Wrote and optimized complex SQL queries involving multiple joins and advanced analytical functions to perform data extraction and merging from large volumes of historical data stored in Oracle 11g, validating the ETL processed data in target database\n\u2022 Identified the variables that significantly affect the target\n\u2022 Continuously collected business requirements during the whole project life cycle.\n\u2022 Conducted model optimization and comparison using stepwise function based on AIC value\n\u2022 Applied various machine learning algorithms and statistical modeling like decision tree, logistic regression, Gradient Boosting Machine to build predictive model using scikit-learn package in Python\n\u2022 Developed Python scripts to automate data sampling process. Ensured the data integrity by checking for completeness, duplication, accuracy, and consistency\n\u2022 Generated data analysis reports using Matplotlib, Tableau, successfully delivered and presented the results for C-level decision makers\n\u2022 Generated cost-benefit analysis to quantify the model implementation comparing with the former situation\n\u2022 Worked on model selection based on confusion matrices, minimized the Type II error\n\nEnvironment: Tableau 7, Python 2.6.8, Numpy, Pandas, Matplotlib, Scikit-Learn, MongoDB, Oracle 10g, SQL', u""Data Architect/Data Modeler\nPitney Bowes Inc - Stamford, CT\nMay 2008 to December 2010\nDescription: PITNEY BOWES (PB) is the manufacturer of copiers, faxes and other office automation. Pitney Bowes's operations are aligned under three lines of business, Global Mailing Systems (GMS) - It is the company's core mail automation and shipping business.\n\nResponsibilities:\n\u2022 Develop Integrations jobs to transfer data from source system to Hadoop.\n\u2022 Installation of Talend Studio.\n\u2022 Technical design documents for Transformation processes.\n\u2022 Application of business rules on the data being transferred.\n\u2022 Task allocation for the ETL and Reporting team.\n\u2022 Communicate effectively with client and their internal development team to deliver product functionality requirements.\n\u2022 Architecting and design of data warehouse ETL processes.\n\u2022 Demo of POC built for the prospective customer and provide guidance and gather the feedback to backend ETL testing on SQL Server 2008 using SSIS.\n\u2022 Create the Operational manual Document.\n\u2022 Create Integration Jobs to backup a copy of data in network file system.\n\u2022 Design and implement the ETL Data model and create staging, source and Target tables in SQL\n\u2022 server database.\n\u2022 Gathering and analysis requirements definition meetings with business users and document meeting outcomes.\n\nEnvironment: Hadoop, MS Office, Talend Studio, ETL, ODS, OLAP , SQL Server 2008."", u""Data Analyst/Data Modeler\nNestle - IN\nMay 2008 to December 2010\nDescription: The Nestle is a Swiss transnational food and drink company. Nestle's products include baby food, medical food, bottled water, breakfast cereals, coffee and tea, confectionery, dairy products, ice cream, frozen food, pet foods, and snacks.\n\nResponsibilities:\n\u2022 Created and maintained Logical and Physicalmodels for the data mart. Created partitions and indexes for the tables in the datamart.\n\u2022 Performed data profiling and analysis applied various data cleansing rules designed data standards and architecture/designed the relational models.\n\u2022 Developed SQLscripts for creating tables, Sequences, Triggers, views and materializedviews\n\u2022 Worked on query optimization and performance tuning using SQL Profiler and performance monitoring.\n\u2022 Developed mappings to load Fact and Dimension tables, SCD Type 1 and SCD Type 2 dimensions and Incremental loading and unit tested the mappings.\n\u2022 Utilized Erwin's forward/reverse engineering tools and target database schema conversion process.\n\u2022 Worked on creating enterprise wide Model EDM for products and services in Teradata Environment based on the data from PDM. Conceived, designed, developed and implemented this model from the scratch.\n\u2022 Involved in extensive DATA validation by writing several complex SQL queries and Involved in back-end testing and worked with data quality issues.\n\u2022 Developed and executed load scripts using Teradata client utilities MULTILOAD, FASTLOAD and BTEQ.\n\u2022 Exporting and importing the data between different platforms such as SAS, MS-Excel.\n\u2022 Generated periodic reports based on the statistical analysis of the data using SQL Server Reporting Services (SSRS)\n\u2022 Write SQLscripts to test the mappings and Developed Traceability Matrix of Business Requirements mapped to Test Scripts to ensure any Change Control in requirements leads to test case update.\n\nEnvironment: DB2, Oracle SQL Developer, PL/SQL, Business Objects, Erwin, MS office suite, Windows XP, TOAD, SQL*PLUS, SQL*LOADER."", u'Data Analyst\nAccenture - Bengaluru, Karnataka\nMay 2008 to December 2010\nDescription: Accenture is a global management consulting and professional services company that provides strategy, consulting, digital, technology and operations services.\n\nResponsibilities:\n\u2022 Designed different type of STARschemas for detailed data marts and plan data marts in the OLAP environment.\n\u2022 Developed and executed load scripts using Teradata client utilities MULTILOAD, FASTLOAD and BTEQ.\n\u2022 Responsible for development and testing of conversion programs for importing Data from text files into map Oracle Database utilizing PERL shell scripts & SQL*Loader.\n\u2022 Used Graphical Entity-Relationship Diagramming to create new database design via easy to use, graphical interface.\n\u2022 Responsible for development and testing of conversion programs for importing Data from text files into map Oracle Database utilizing PERL shell scripts &SQL*Loader.\n\u2022 Worked with the ETL team to document the Transformation Rules for Data Migration from OLTP to Warehouse Environment for reporting purposes.\n\u2022 Created SQLscripts to find dataquality issues and to identify keys, data anomalies, and data validation issues.\n\u2022 Formatting the data sets read into SAS by using Format statement in the data step as well as Proc Format.\n\u2022 Applied Business Objects best practices during development with a strong focus on reusability and better performance.\n\u2022 Developed Tableau visualizations and dashboards using Tableau Desktop.\n\u2022 Used Graphical Entity-Relationship Diagramming to create new database design via easy to use, graphical interface.\n\u2022 Co-ordinate with various business users, stakeholders and SME to get Functional expertise, design and business test scenarios review, UAT participation and validation of financial data.\n\u2022 Write SQL scripts to test the mappings and Developed Traceability Matrix of Business Requirements mapped to Test Scripts to ensure any Change Control in requirements leads to test case update.\n\nEnvironment: Oracle SQL Developer, PL/SQL, Business Objects, TOAD, Tableau, Informatica, MS SQL Server, SQL*PLUS, SQL*LOADER, XML.']","[u""Bachelor's""]",[u'']
0,https://resumes.indeed.com/resume/c3b428cc58c89552,[u'Data Analyst Intern\nICTACT\nMarch 2016 to April 2016\n>Got familiarized with Big Data environment.\n>Used R programming to uncover trends from twitter database.\n>Analyzed the frequency of occurrence of words in twitter.\n>Discovered hidden patterns of tweets and categorized them.'],"[u'M.S. in Business Analytics', u'B.E. in Computer Science and Engineering']","[u'The University of Texas at Dallas Dallas, TX\nMay 2019', u'Anna University\nMay 2017']"
0,https://resumes.indeed.com/resume/f4f0f78bae741232,"[u""Sr. Data Analyst/Data Modeler\nCaresource - Dayton, OH\nJanuary 2015 to Present\nResponsibilities:\n\n\u2022 Created logical data model from the conceptual model and it's conversion into the physical database design using ERWIN.\n\u2022 Mapped business needs/requirements to subject area model and to logical enterprise model.\n\u2022 Worked with DBA's to create a best fit physical data model from the logical data model\n\u2022 Redefined many attributes and relationships in the reverse engineered model and cleansed unwanted tables/ columns as part of data analysis responsibilities.\n\u2022 Enforced referential integrity in the OLTP data model for consistent relationship between tables and efficient database design.\n\u2022 Developed the data warehouse model (star schema) for the proposed central model for the project.\n\u2022 Created 3NF business area data modeling with de-normalized physical implementation data and information requirements analysis using ERWIN tool.\n\u2022 Worked on the Snow-flaking the Dimensions to remove redundancy.\n\u2022 Worked on Metadata exchange among various proprietary systems using XML.\n\u2022 Worked with Architecture team to get the metadata approved for the new data elements that are added for this project.\n\u2022 Created high level ETL design document and assisted ETL developers in the detail design and development of ETL maps using Informatica.\n\u2022 Responsible for creating the staging tables and source to target mapping documents for the ETL process.\n\u2022 Involved in Performance Tuning of the database which included creating indexes, optimizing SQL Statements & monitoring the server.\n\u2022 Identified and tracked the slowly changing dimensions, heterogeneous sources and determined the hierarchies in dimensions.\n\u2022 Helped in migration and conversion of data from the Sybase database into Oracle database, preparing mapping documents and developing partial SQL scripts as required.\n\u2022 Generated ad-hoc SQL queries using joins, database connections and transformation rules to fetch data from legacy Oracle and SQL Server database systems\n\u2022 Used ERWIN for reverse engineering to connect to existing database and ODS to create graphical representation in the form of Entity Relationships and elicit more information.\n\u2022 Used Model Mart of ERWIN for effective model management of sharing, dividing and reusing model information and design for productivity improvement.\n\u2022 Participated in performance management and tuning for stored procedures, tables and database servers.\nEnvironment: Windows 7, Teradata 14, ERWIN r9.1, SQL server 2005/2008, Business Objects XI, MS Excel 2010, Informatica 7.3, SQL Server 2000/2005, Rational Rose, Oracle 8i, TOAD for Data Analysis, Teradata SQL Assistant 13.0."", u""Sr. Data Analyst/Data modeler\nTrinity Health - Southfield, MI\nSeptember 2012 to October 2013\nResponsibilities:\n\n\u2022 Performed data analysis and profiling of source data to better understand the sources.\n\u2022 Created logical data model from the conceptual model and it's conversion into the physical database design using Erwin.\n\u2022 Worked with DBA's to create a best-fit Physical Data Model from the logical data model.\n\u2022 Redefined many attributes and relationships in the reverse engineered model and cleansed unwanted tables/columns as part of data analysis responsibilities.\n\u2022 Interacted with the database administrators and business analysts for data type and class words.\n\u2022 Conducted design sessions with business analysts and ETL developers\n\u2022 Used Model Mart of Erwin for effective model management of sharing, dividing and reusing model information and design for productivity improvement.\n\u2022 Created ER Diagrams, Data Flow Diagrams, grouped and created the tables, validated the data, identified PK/ FK for lookup tables.\n\u2022 Created 3NF business area data modeling with de-normalized physical implementation data and information requirements analysis using Erwin tool.\n\u2022 Developed Star Schema and Snowflake Schema in designing the Logical Model into Dimensional Model.\n\u2022 Ensured the quality, consistency, and accuracy of data in a timely, effective and reliable manner.\n\u2022 Involved in ETL mapping documents in data warehouse projects.\n\u2022 Created high level ETL design document.\n\u2022 Involved in extensive Data Analysis on the Teradata and Oracle Systems querying and writing in SQL and TOAD.\n\u2022 Assisted the ETL team to document the transformation rules for data migration from OLTP to Warehouse environment for reporting purposes.\n\u2022 Involved in extensive data analysis on the Teradata and Oracle systems querying and writing in SQL and TOAD.\n\u2022 Used SQL joins, aggregate functions, analytical functions, group by, order by clauses and interacted with DBA and developers for query optimization and tuning.\n\u2022 Conducted several Physical Data Model training sessions with the ETL Developers. Worked with them on day-to-day basis to resolve any questions on Physical Model.\nEnvironment: Oracle 11g, SQL Server 2005/2008, Teradata 14, MS Access, Teradata, Data stage, Erwin r8.0, Windows XP, MS Excel, Business Objects XI, SSIS, MS Access, Teradata, Data stage, Erwin8.0, Windows XP, Excel, Informatica."", u""Sr. Data Analyst/Data Modeler\nCapital On - Plano, TX\nJuly 2010 to August 2012\nResponsibilities:\n\n\u2022 Facilitated JAD sessions for project scoping, requirements gathering & identification of business subject areas.\n\u2022 Identified and documented detailed business rules and use cases based on requirements analysis.\n\u2022 Identified the entities and relationship between the entities to develop Conceptual Model using ERWIN.\n\u2022 Developed Logical Model from the conceptual model.\n\u2022 Worked with DBA's to create a best-fit Physical Data Model from the logical data model.\n\u2022 Used forward engineering to generate DDL from the Physical Data Model and handed it to the DBA.\n\u2022 Developed dimensional model for Data Warehouse/OLAP applications by identifying required facts and dimensions.\n\u2022 Designed STAR schema for the detailed data marts and plan data marts consisting of confirmed dimensions.\n\u2022 Used ETL methodology for data extraction, transformations and loading in a complex Enterprise Data Warehouse (EDW)\n\u2022 Identified and documented data sources and transformation rules required to populate and maintain data warehouse content.\n\u2022 Assisted in Data Stage Server jobs to load data from sequential files, flat files and MS Access.\n\u2022 Used Data Stage Manager for importing metadata from central repository.\n\u2022 Involved in extracting, cleansing, transforming, integrating and loading data into different Data Marts using Data Stage Designer.\n\u2022 Developed SQL Queries to fetch complex data from different tables in remote databases using joins, database links and bulk collects.\n\u2022 Used Erwin's reverse engineering to connect to existing database and ODS to graphically represent the Entity Relationships.\n\u2022 Involved in Performance tuning by leveraging oracle explain utility and SQL tuning.\nEnvironment: Oracle 9i, SQL Server 2005/2008, DB2, Microsoft Transaction Server, Erwin, Teradata 14, Crystal Reports, SSIS/SSRS, Informatica, MS-SQL server manager, Microsoft Access, MS Excel, MS Visio, Rational Rose, Requisite Pro"", u'Data Analyst/Data Modeler\nCVS Caremark - Lincolnshire, IL\nApril 2007 to June 2010\nResponsibilities:\n\n\u2022 Involved in dimensional modeling of the data warehouse to design the business process.\n\u2022 Designed star schemas and bridge tables to control slowly changing dimensions.\n\u2022 Developed ER and Dimensional Models using Erwin advanced features.\n\u2022 Created physical and logical data models using Erwin.\n\u2022 Done extensive bulk loading in to target using Oracle SQL Loader.\n\u2022 Used workflow manager for session management, database connection management and scheduling of jobs.\n\u2022 Involved in writing UNIX shell scripts for Informatica ETL tool to automate sessions.\n\u2022 Involved in M-LOAD, Fast-load and T-pump loading to migrate data from Oracle to Teradata.\n\u2022 Created BT queries and stored procedures in Teradata.\n\u2022 Assisted the team in the development of design standards and codes for effective ETL procedure development and implementation.\n\u2022 Involved in extensive performance tuning by determining bottlenecks at various points like targets, sources, mappings and sessions.\n\u2022 Involved in the process design documentation of the Data Warehouse Dimensional Upgrades.\nEnvironment: Erwin 4.0, Informatica PowerCenter 7.1/6.2 (Power center Repository Manager, Designer, Workflow Manager and Workflow Monitor), Oracle 9i, DB2 8.0, PL/SQL, Teradata V2R3, MS SQL Server 2000, Business Objects 5.1, MS Access, UNIX Shell scripting, Windows XP.']","[u""Bachelor's""]",[u'']
0,https://resumes.indeed.com/resume/85056bf4bbd83712,"[u'Project Analyst\nAvnet - Phoenix, AZ\nOctober 2016 to Present\nlead role on automation projects that effect the company on an enterprise level. Creation of reporting suites used by multiple business groups throughout Americas organization, with the intention of driving certain business practices. Creation of ad hoc reporting for high level executives within organization. Creation of process to feed/clean data daily to server so that daily reporting runs. Monitoring of daily reporting automations that provide existing reports to over 200 people. This position is high visibility, with meetings with executives to discuss project statuses happening frequently. Also facilitated the hiring and training of new team members.\n- Balancing workload of many projects at once with hard deadlines\n- Requirement gathering/Creation of BRDs\n- Work flow creation/timeline(Visio)\n- Data purge/cleaning for daily uploads of data\n- Backend design (Relational Database design, T-SQL, SSIS packages, MS Access)\n- Automation\n- VBA Macros\n- Front end design (PowerBi, PowerApps, MS Excel, MS Access)\n- Facilitation of UAT testing/AB Testing\n- Maintenance/Ownership of existing reports once created\n- High level communication to customers(internal) on ongoing statuses of projects', u'Data Analyst\nRhythmOne\nAugust 2016 to October 2016\nDaily analysis of data, with emphasis on revenue impact cause and effect\n\u2022 Interface between sales/accounting team and development team to facilitate internal product development\n\u2022 Manage client accounts/ deal with client concern over performance of campaign\n\u2022 Identify issue causing faulty data in reporting software']",[u'BS in Informatics'],[u'Arizona State University\nMay 2016']
0,https://resumes.indeed.com/resume/fb5b1d55f78c2f71,"[u""SENIOR DATA ANALYST\nENCORE CAPITAL GROUP\nMarch 2014 to Present\nProactively track and analyze key metrics of Encore's direct marketing mail campaigns, QA and validate that our strategies are\nin-line before sending mail\n\u2022 Execute and manage Encore's most complicated marketing tests that have given us significant insight into our consumer\nbehavior\n\u2022 Conducted test reads/analysis to measure performance based on key metrics and made business recommendations\n\u2022 Collaborated with multiple departments in the development of the new lettering infrastructure that has allowed Encore to execute more complex testing and minimized error rate within our lettering processes\n\u2022 SME for all Encore's lettering business rules, decisions and processes\n\u2022 Extensive experience working with multiple departments while managing complex projects that require crucial group inputs and effort before successful implementation\n\u2022 Served as the go-to-person for creating data analysis algorithms within SAS that ultimately allowed the team to analyze data"", u'DATA ANALYST\nENCORE CAPITAL GROUP\nJanuary 2012 to March 2014\n\u2022 Developed new marketing mail execution process, reducing the time it takes to send the mail out and minimizing error rate to an all-time low\n\u2022 Collaborated with multiple departments running processes to support core business and increase performance throughout the company\n\u2022 Developed the first QA phase and trend within the department that decreased error rate throughout multiple processes\n\u2022 Streamlined workload for CSS in development of automating redundant tasks by developing SAS code to segment inquiries to their proper party; I also developed reporting to measure inquiry performance\n\u2022 Created the first lettering infrastructure for PROPEL marketing letter process with very limited resources', u'PRESIDENT\nNU ALPHA KAPPA FRATERNITY INC\nJanuary 2010 to January 2011\n\u2022 Responsible for managing and supervising 20+ positions within the organization\n\u2022 Instrumental in maintaining contact between the organization, University, and National office\n\u2022 Accountable for motivating and inspiring group members to perform and excel in their duties\n\u2022 Delegated tasks to the organization, increasing productivity and efficiency.']",[u'Bachelor of Arts in Economics'],[u'San Diego State University\nAugust 2011']
0,https://resumes.indeed.com/resume/24f26e191bb3ded8,"[u'WOW Logistics - Menasha, WI\nMay 2017 to Present', u'Breadsmith - Appleton, WI\nJanuary 2017 to May 2017', u'Gatehouse Media - Freeport, IL\nJanuary 2015 to January 2016\nManagerial roles. Worked with newspaper carriers, resolving customer complaints, required ability to work quickly and efficiently under pressure, delivering of down routes and providing redelivery to missed customers. Used Word, Excel, and other programs extensively.', u""Joann's Fabrics - Freeport, IL\nJanuary 2015 to January 2016\nWorked with customers to assist in their craft projects of all stripes from first time crafters to experienced professionals. Stocked store, cut fabric, checked out customers, and kept the store tidy."", u'Data Analyst\nColony Brands Inc - Monroe, WI\nJanuary 2012 to January 2014\nCorrecting mail, phone, and internet orders, collating, scanning, UPS return labels\n\u2219 Microsoft Excel and Outlook']",[u'Associate Degree'],"[u'Highland Community College (HCC) Freeport, IL\nJanuary 2011']"
0,https://resumes.indeed.com/resume/f91577dabc22b4f4,"[u'DATA ANALYST\nKuber Casting PVT. LTD\nOctober 2014 to July 2017\n\u2022 Queried the data from MYSQL using: Join, Sub-queries, Analytical functions for routine reporting of employees data.\n\u2022 Analysed data from global market for optimizing operations by creating visualization which led to increase in profit by 10%.\n\u2022 Reduced demand-supply gap by 5%, by creating interactive dashboards and analysing inventory management system.\n\u2022 Conducted expenditure analysis of the organization in Tableau. Analyzed business metrics and customer segmentation using trend line, groups, sets, calculated fields and reduced expenditure by approx. 1.2 million annually.', u'SOFTWARE DEVELOPER\nWipro Technologies\nOctober 2012 to September 2014\n\u2022 Created scripts in CORE JAVA to extract meaningful data from processed information which was stored in the master directory.\n\u2022 Optimized data collection procedure and created reports which led to 20 % reduction in time for data analysis.\n\u2022 Maintained and updated code for the entitlement controlling application which was used for internal audit compliance.\n\u2022 Developed project through analysis of business requirements, design development, and implemented Object Oriented concepts in Core Java/J2EE.']",[u'Master of Science in Management Information Systems in Engineering'],"[u'SUNY, University at Buffalo']"
0,https://resumes.indeed.com/resume/7459295f73d112c4,"[u'Data Analyst\nAmazon Fulfillment - Patterson, CA\nSeptember 2013 to Present\n\u2022 Charting and graphing of data for reporting purposed\n\u2022 In depth research of defect trends\n\u2022 Data collection and entry as needed\n\u2022 Development of data collection processes and data management systems Maintenance of data integrity (0% error rate)\n\u2022 Designing of queries, compiling of data, and generation of reports in MS Access and Excel\n\u2022 Data mining and problem solving\n\u2022 Back up for Process Assistant duties\n\u2022 Provide quality feedback for associates']","[u'Bachelors in Business Administration-Management', u'Associates in Business Administration']","[u'California State University of Stanislaus\nMarch 2019', u'Modesto Junior College\nMarch 2017']"
0,https://resumes.indeed.com/resume/949ef3bd90f62678,"[u'Independent Consultant - Data Analyst\nIdeal Auto Inc - Brooklyn, NY\nMay 2017 to January 2018\n\u2022 Worked closely with executives to provide information through data analysis to support business decisions\n\u2022 Managed data clean-up projects for all eight brokerage stores including two collision shops\n\u2022 Streamlined data - improved operational efficiency and data quality\n\u2022 Collaborated with IT team overseas to assist with the development of a custom CRM/database system\n\u2022 Developed, designed, and maintained standardized reports and book keeping logs utilized across the company', u'Sales Operations Data Analyst\nNewsela Inc - New York, NY\nApril 2015 to May 2017\n\u2022 Summarized large volumes of data in analytical reports; manipulated and interpreted complex data to support key business initiatives\n\u2022 Worked closely with executives to provide information for timely decision making across the business\n\u2022 Monitored Key Performance Indicators and worked with respective managers to address potential risks\n\u2022 Identified areas for improvement and worked with individual groups to implement solutions\n\u2022 Assisted in the development of reports and dashboards, including documenting methodology, analysis, and design\n\u2022 Standardized common report requests and provided self-service capabilities\n\u2022 Collaborated with other departments to identify missing data, errors or other anomalies; monitor data integrity\n\u2022 Implemented ongoing process to monitor and maintain data standards', u""Sales Analyst\nRichline Group Inc - New York, NY\nMay 2012 to August 2013\n\u2022 Managed inventory for several national retail chain partners (Amazon, Sam's Club, Zales)\n\u2022 Performed in-depth review of product line performance analyzing sales trends and patterns\n\u2022 Created custom reports pooling data from multiple sources to forecast sales\n\u2022 Attended weekly flow meetings to discuss merchandise performance, allocation, and client matters\n\u2022 Collaborated with sales teams to investigate poor performing products and devise improvements\n\u2022 Discussed potential issues with team members from satellite offices to mitigate sales forecasting inaccuracies"", u'Data & Inventory Analyst\nWesco Integrated Supply - Wesco Distribution - Port Washington, NY\nAugust 2010 to January 2012\n\u2022 Managed the replenishment order process for Pratt and Whitney in Wesco\n\u2022 Prepared monthly and quarterly inventory reports for subsidiaries under United Technologies Corp. (Sikorsky, Otis)\n\u2022 Approved warehouse releases and transfers between accounts\n\u2022 Audited daily inventory reports and reconciled irregularities\n\u2022 Documented and ensured proper inventory, reporting, and freight compliance procedures\n\u2022 Collaborated with IT dept. to troubleshoot and streamline database systems']",[u'Bachelor of Arts in Economics in Independent Coursework'],"[u'New York University New York, NY\nJanuary 2018']"
0,https://resumes.indeed.com/resume/032123fcf902b0b8,"[u'Volunteer\nIndian Students Association\nSeptember 2017 to Present', u'Sotally Tober\nAugust 2017 to December 2017\n\u2022 A party planning app which has features like ""plan a party"", "" search, buy, compare"", "" no queue"".\n\u2022 The user gets information regarding the nearest wine shop which has the required drink. Google maps\nintegration helps provide user with navigation. Scan and generation of bill makes the user get rid of queues.\n\u2022 Context diagram, Use-case diagram, Process diagram, use case description, Data dictionary, Class diagram,', u'""Turo""\nSeptember 2017 to September 2017\nBuilt an Entity relationship model and a dimensional model depicting all possible outcomes and relationships between the different business processes in a car rental website. A universe was built for both the models\nusing Universe design tool. Web-Intelligence tool was used along with Tableau to generate reports.', u""Data Analyst Intern\nAttiitude Star Fashion Pvt\nMay 2017 to July 2017\nPerformed data analysis on key HR metrics of 30,000 employees using SQL. Developed and presented tableau\ndashboards on KPI to the Board of Directors and executives.\n\u2022 Reduced Employee turnover rate by 5% using logit model and provided actionable target list to the HR team.\n\u2022 Segregated requirements, design and developed solutions for business users' analytic needs.\n\u2022 Used Data warehousing techniques and performed data analysis and discovery using Market basket\nanalysis, Association rules and learnt usage of other data exploratory tool.Also performed report and dashboard testing on segregated data.\nACADEMIC PROJECT"", u'Smart Parking System, DSCE\nJanuary 2017 to July 2017\n\u2022 An automated parking system for smart cities using Internet-of-Things technology.\n\u2022 A Smart Parking System based on Smart-phones\' Embedded Sensors and Short-Range Communication\nTechnologies"" to ease searching for vacant parking. Successfully implemented the proposed system in the real world.', u'COMPETITIONS\nSeptember 2013 to January 2017', u'DSCE\nMarch 2016 to March 2016\n\u2022 Enhanced Security of Docker using Linux Hardening Techniques.\n\u2022 ISBN Part Number: CFP16D66-USB IEEE ISBN: 978-1-5090-2398-1.\n\u2022 DOI: 10.1109/ICATCCT.2016.7911971.', u'COMPETITIONS\nOctober 2013 to May 2015']","[u'M.S. in ITM', u'B.E. in Computer Science']","[u'The University of Texas at Dallas Dallas, TX\nMay 2019', u'Dayananda Sagar College of Engineering\nJuly 2017']"
0,https://resumes.indeed.com/resume/f9b7a1b3c5660292,"[u'Data Analyst\nFranciscan PHO Northern Indiana - Munster, IN\nJuly 2016 to Present\nDesigning monthly dashboards to keep track of trends and current activity\n\u25cf Generate strong visuals of data for ease of interpretation\n\u25cf Projecting budgets and preparing monthly, quarterly, and yearly financial expenses\n\u25cf Provide ad hoc reports ranging from utilization of services, quality metrics, & financial performance', u""Mathematics Teacher\nPlum Creek Christian Academy - Dyer, IN\nAugust 2015 to May 2016\nTaught 5th - 8th grade mathematics ranging from arithmetic, algebra, geometry, and statistics\n\u25cf Increased academic grades by 30% by collaborating with administration, fellow teachers, and parents to develop solutions for academic and behavioral struggles of individual students\n\u25cf Improved school's curricula and course materials by evaluating state and national standards""]","[u'M.S.A. in Analytics', u'B.S. in Mathematics', u'A.S. in Mathematics']","[u'Dakota State University Madison, SD\nAugust 2017 to December 2020', u'Purdue University Hammond, IN\nAugust 2013 to December 2015', u'Prairie State College Chicago, IL\nAugust 2011 to May 2013']"
0,https://resumes.indeed.com/resume/5895b21bdd0c62f4,"[u'Data Analyst\nAFFINITY SOLUTIONS - New York, NY\nNovember 2013 to June 2016\nCreate internal and client-facing reports using php and sql on a weekly/monthly basis;\nWrite automated php scripts to improve and fasten the existing process.\nMaintain statistics and perform data integrity checks on existing and newly acquired datasets\nMaintain Database warehouse ETL Process.']",[u'Bachelor of Computer Information System in Computer Information System'],[u'Stony Brook University\nMay 2002']
0,https://resumes.indeed.com/resume/cfaa8bf826467636,"[u'Junior Data Analyst\nNew Editions Consulting Inc. - Falls Church, VA\nDecember 2017 to Present\n\u25cf Utilizing SPSS to clean data, conduct analysis, and update syntax code for the 40th and 41st Annual Reports to Congress for the Office of Special Education Programs in the U.S. Department of Education.\n\u25cf Responsible for developing R code to generate tables of counts for the Administration of Community Living biweekly reports.\n\u25cf Updating SPSS syntax code to generate current statistics for end-of-contract report on technical assistance activities conducted for the Money follows the Persons program for the Center for Medicare and Medicaid Services.\n\u25cf Using SPSS to clean, sort, and perform analysis on several large data sets to produce the Annual Performance Report for the National Institute on Disability, Independent Living, and Rehabilitation Research.']","[u""Bachelor's in Economics""]","[u'University of Virginia Charlottesville, VA\nAugust 2015 to May 2017']"
0,https://resumes.indeed.com/resume/432bbae28b23129e,"[u""Data Analyst\nTwitter Sentiment Analysis Tool - Hoboken, NJ\nMarch 2018 to Present\nSpring 2018 - Present\nStevens Institute of Technology\nMicroblogging websites has evolved to become a source of varied kind of information. The goal of this project is to analyze the sentiment trend of president Trump's tweets for the past two years.\n\n1. Performed data cleaning and text mining using certain packages in RStudio.\n2. Designing a tool using RStudio to analyze the content of Twitter posts by using the categories identified as positive,\nnegative and neutral. The algorithm evaluates tweets based on the number of positive and negative words in the tweets.\n3. Also, using NLP to make word cloud and show node network analysis of the most frequent words used in tweets.\n4. This aids in observing trends using NLP and making graphical representation using ggplot.""]","[u'Master of Science in Information Systems', u""Master's""]","[u'Stevens Institute of Technology\nDecember 2018', u'IMS Engineering College\nJune 2010 to June 2014']"
0,https://resumes.indeed.com/resume/a551f4d162ee445b,"[u'Master Data Analyst\nPerrigo Pharmaceuticals - Bronx, NY\nFebruary 2012 to Present\nCollects, records, and maintains MRP data\nin SAP, serving schedulers, customer\nservice, accounting, and operations.\nUpdate procedures & assist with internal\ncustomer service support', u'QC Analyst\nIntercos America - Congers, NY\nSeptember 2010 to November 2012\nPerforms investigations and visual\ninspections to identify quality problems\nand recommend viable solutions\nWrites technical reports, testing\nprotocols, and trend analyses\nCoordinates testing with contract labs', u'Data Control Analyst\nIntercos America - Congers, NY\nSeptember 2009 to September 2010\nProvides clerical and administrative\nsupport\nGranted promotion to QC analyst due to\nexcellent performance']","[u""Bachelor's in Business Administration"", u'Associate in Liberal Arts Humanities and Social Science']","[u'SUNY Cobleskill\nJanuary 2017 to May 2020', u'Rockland Community College Suffern, NY\nSeptember 2009 to May 2016']"
0,https://resumes.indeed.com/resume/7b7997ba85dfc1f1,"[u'Specialist\nU.S. Army Reserves - El Paso, TX\nMarch 2009 to Present\nDuties Included: Driving military vehicles in field exercises and on military bases. Team leader experience managing up to 4-6 people at one time and managed my team\u2019s military equipment and kept track of it using Microsoft Office Suites. Helped the supervisor file and maintain personal files and information on a military computer database. Supervises & performs duties involving request, receipt, storage, issue, accountability, and preservation of individual, organizational, installation, and expendable supplies and equipment. Receives, inspects, inventories, loads, unloads, segregates, stores, issues, delivers and turns in organization and installation supplies and equipment. Issues and receives small arms. Secures and controls weapons and ammunition in security areas. Schedules and performs preventive and organizational maintenance on weapons.', u'Data Intel Analyst\nASTS - El Paso, TX\nJuly 2010 to February 2017\nDuties Included: Received phone calls, conduct investigations and background checks from law enforcement nationwide and federal government federal agencies to include CBP, Border Patrol, DEA, FBI, ATF, HIDTA, DOJ, HSI, INS/ICE. Analyzed data received by law enforcement by using LEIA (Law Enforcement Inquiry Alert) system, which uses multiple law enforcement databases to include CIS, NADDIS, and others. Trainer for new employees while working at the El Paso Intelligence Center.']","[u'MBA in Business Administration & Homeland Security', u'Bachelors of Science in Criminal Justice Administration']","[u'Park University El Paso, TX\nJanuary 2017 to May 2019', u'Park Univ Fort Bliss, TX\nJanuary 2007 to May 2010']"
0,https://resumes.indeed.com/resume/ad2036adf11855db,"[u'Data Analyst\nACG Prorelsys - Bengaluru, Karnataka\nAugust 2014 to Present\n3rd Year) 61%\nPUC PCMB S.V. Study centre National Institute of Opening School (Govt ofIndia) 2012 59.33%\nSSLC Sri Saraswathi Convent High Schol Karnataka Secondary Education Board 2003-2004 53.60%', u'Data Analyst\nACG Prorelsys Pvt Ltd - Bengaluru, Karnataka\nAugust 2014 to Present\nBranch Name of the institute University Year of Passing %\nGDCE Graduate Diploma in civil Engineering Indian Institute of Engineering Studies Indian council for Engineering Board 2010 (1st year) 58.5%\n2011 (2nd Year) 62.25%']",[],[]
0,https://resumes.indeed.com/resume/42606520d56eade2,"[u""Data Analyst\nAcuty LLC\nMarch 2017 to Present\nBuilt algorithms for CT Real Estate Market to improve the accuracy of the home price\nprediction via R program. Increased the ROI to 18% by optimizing the sale price when investing in real estate. Studied competitors' behaviors to develop self-advantage\n\u2022 Extracted, compiled and tracked data via SQL\n\u2022 Preprocessed data by data cleaning and one - hot encoding categorical feature\n\u2022 Applied Linear Algorithm to identify the top factor that influenced the house price and evaluated model via cross-validation"", u'Data Analyst Intern\nArecy LLC\nAugust 2016 to March 2017\nDeveloped algorithms for Real Estate Broker Service to predict customer churn\nprobability based on labeled data via python programming\n\u2022 Preprocessed data set by data cleaning, categorical feature transformation and standardization, etc.\n\u2022 Trained supervised machine learning models including Logistic Regression, Random\nForest, KNN and SVM and applied regularization with optimal parameters to overcome overfitting.\n\u2022 Evaluated model performance via cross-validation tech and confusion matrix and analyzed feature importance\nNatural Language Processing and Topic modeling\nOct 2017 - Dec 2017\n\u2022 Clustered unlabeled textual documents into groups and discovered latent semantic\nstructures using Python\n\u2022 Preprocessed text by tokenizing, stemming and stopwords removing, and extracted\nfeatures by TF-IDF approach\n\u2022 Trained unsupervised learning models of K-means Clustering and LDA\n\u2022 Identified latent topics and keywords of each document for clustering\n\u2022 Visualized model training results by dimensionality reduction using PCA', u""CFD Researcher\nWashington University in Saint\nJanuary 2015 to August 2016\nCompared two Blade Momentum Method and optimized Tip Loss Correction to improve the accuracy and stability of BEM algorithm via JAVA programming in master\nthesis 'A Comparative Study of Two Solution Methods for Blade Momentum Equations'\n\u2022 Applied different schemes to solve the linear advection equations by using MATLAB,\ncalculated and evaluated the advantages of different algorithm in CFD project\n\u2022 Wrote MATLAB programs to solve Laplace's Equation by using different methods, such as Second Order Upwind Scheme etc. Analyzed the convergences, stability and consistency of each algorithm and selected the most efficient model""]","[u'MS', u'BS in Hydropower and Water Conservancy Engineering']","[u'Mechanical Engineering Washington University St. Louis, MO\nAugust 2014 to August 2016', u'Hohai University\nSeptember 2009 to September 2013']"
0,https://resumes.indeed.com/resume/453e66d1d9db6c40,"[u'SENIOR DATA ANALYST\nSAPIENT CONSULTING\nMay 2009 to December 2017\n\u2022 Meeting with financial domain clients to learn about their business problems - Franklin Templeton, Massachusetts Financial Services (MFS), Royal Bank of Scotland (RBS), TD Bank\n\u2022 Analysing massive, complex data sets\n\u2022 Running workshops to gather information for a project\n\u2022 Extracting data from various systems for projects\n\u2022 Preparing a dashboard or data visualisation for client\n\u2022 Extensive data analysis skills with strengths in drilling into underlying databases for discovery and identifying opportunities\n\u2022 Perform extensive QA on data to ensure the quality of data. Follow the standard process for reporting any issues identified and work with Data team to close the data issues\n\u2022 Applying computer-aided statistical and data modeling (SQL) to solve business problems and perform Data Mining / Data Cleaning\n\u2022 Data visualisation using Tableau / Python - Matplotlib and Seaborn libraries\n\u2022 Build accurate predictive models driving decision making\n\u2022 Generate insights related to positions consolidation of securities & hedging suggestions in wealth management, using cluster analysis in dendograms & plotting heatmaps\n\u2022 Communicate insights to make data analysis actionable and understandable by business partners\n\u2022 Exposure to Agile development methodologies, Jira tool and Scrum processes', u""DATA ANALYST\nUNITED HEALTH GROUP\nNovember 2007 to February 2009\n\u2022 Make test predictions and data mocking on United Health Group's healthcare enterprise data warehouse Galaxy\n\u2022 Provide insights for medical trend analysis, provider profiling, care management, premium pricing, member demographics, fraud and abuse, customers, product penetration, and network adequacy"", u'IT ANALYST\nFISERV\nAugust 2006 to October 2007\n\u2022 Generate insights from Core Banking subsystems like Customer, Loan, Transaction, Time, General Ledger, Branch, Teller/ Journal and Common for credit bureau information and credit card processing.\n\u2022 Dashboard reporting using Scorecard solution for Actuals, Objectives and % Objectives achieved to stakeholders based on staff and/or business hierarchy.']","[u'PG DIPLOMA in BUSINESS ANALYTICS', u'BACHELOR OF ENGINEERING DEGREE, INFORMATION TECHNOLOGY in Computer Science']","[u'NARSEE MONJEE INSTITUTE OF MANAGEMENT STUDIES Mumbai, Maharashtra\nJanuary 2008 to January 2010', u'MAHARSHI DAYANAND UNIVERSITY Gurgaon, Haryana\nJanuary 2001 to January 2004']"
0,https://resumes.indeed.com/resume/10ac602b28c27150,"[u""Data Analyst Intern\nFIS Australasia Pty Ltd - Melbourne VIC\nJune 2017 to August 2017\nImproved the business processes by Scanning, analyzing and following up on the remediation efforts with 198\nworkstation owners to increase compliance measure and remove the risk of sensitive unencrypted PAN data\n\u2022 Reduced the security vulnerabilities from 16,000 to 11,000 by analyzing them on a per case basis\n\nPROJECTS\nDatabase System for Dark Springs Bottled Water company, CMU\n\u2022 Devised and constructed a fully scalable database system using SQL Data modeler and oracle 11g\n\u2022 Developed Data Dictionary, Logical Data model, Physical Data model and Database objects\n\nData warehouse System for analysis of liquor sales in Iowa, CMU\n\u2022 Engineered the Design and development of a data warehouse using SSIS and MS SQL Server, modelling POS retails sale\n\u2022 Determined the Brand popularity, consumption patterns and price variances across the state of Iowa\n\nDeterminants of School Performance, CMU\n\u2022 Built a model to classify schools in California as High/Low using Academic Performance Index (API) as the metric\n\u2022 Cleaned and segregated data into train and test sets to perform feature selection by to identify factors that are most\nhighly associated with good school performance\n\u2022 Ran Lasso, randomForest, and SVM for each data set and selected the best classifier with accuracy of more than 80%\n\nStudent Lab at A.T. Kearney, CMU\n\u2022 Pioneered a dynamic tool for reducing cost expended on pricing of million-dollar products and predict the winning price for competitive bids in the global market using stepwise regression and localization\n\u2022 Automated their entire process and reduced the process time by 2 months that led to productivity improvement saving of millions of dollars\n\nAnalysis of U.S photovoltaic installations Data Using R, CMU\n\u2022 Modelled relation between aspects of installation's capacity and various socio-economic, environmental factors with an accuracy of 74.7%\n\u2022 Utilized quantitative and qualitative analysis to identify the key predictors to produce statistical reports\n\nDistributed Systems Programming, CMU\n\u2022 Created a full-stack solution for finding 1000's of global locations on Android devices, with a native Android app frontend and an analytical backend based on Glassfish & MongoDB\n\u2022 Implemented MapReduce for calculating the suburb with the most amount of crimes in Pittsburgh\n\nDatabase Assistant - HDFC Standard Life Insurance Company LTD, Mumbai\n\u2022 Constructed SQL queries and examined the accuracy of retrieved data in DB2 environment\n\u2022 Generated 100's of reports extracting the data specified by the clients using IBM Cognos\n\nSenior Year Project: 'Textural analyzer of Retinal blood vessels', M.I.T COE\n\u2022 Revolutionized the system to identify people suffering from nascent Diabetic Retinopathy by using MATLAB\n\u2022 Analyzed 24 blood vessels features against healthy retinal images improving the efficiency of the current system by 5%""]","[u'Master of Information Systems Management in CMU', u'Bachelor of Engineering in Information Technology']","[u'Carnegie Mellon University Pittsburgh, PA\nMay 2018', u'M.I.T College of Engineering, University of Pune Pune, Maharashtra\nJuly 2015']"
0,https://resumes.indeed.com/resume/923a011afdd8f636,"[u""Business Analyst\nCharter Communications - Stamford, CT\nJune 2016 to Present\nTeam: Sales and Service Delivery Team\n\n\u2022 Provide detailed analysis on Completion Rates for SMB & Residential\n\u2022 Identify root cause for Completion Rate performance\n\u2022 Identify the Process gaps\n\u2022 Provide Business requirements for Reporting for the new product launch (Mobile)\n\u2022 Developed Test cases for the Mobile launch\n\u2022 Perform Business Testing in UAT using ALM\n\u2022 Define Process Flows & Job Aids for the Mobile launch\n\u2022 Provide L&D Training content for Mobile launch\n\u2022 Developed & maintained the Plan of Record for Mobile launch\n\u2022 Perform Tool Analysis and provided recommendation for Mobile Launch\n\u2022 Define call flows and provide business requirements for Call routing and Desktop IT\n\u2022 Set up Quality Process and voice of the customer for the Mobile launch\n\u2022 Provide feedback with sample data to field, Construction & Sales Support\n\u2022 Track performance of the new processes implemented\n\u2022 Provide Monthly/Daily reports on Completion Rates\n\u2022 Provide decks for the Monthly Executive Steering Committee Meetings\n\u2022 Provide Biweekly Decks for the Operations Steering Committee Meetings\n\u2022 Provide Adhoc Analysis for the Top Management\n\u2022 Provide customer journey maps\n\u2022 Built & maintained the Project Portfolio for the Sales & Service Delivery Team\n\u2022 Provide monthly ECAF report\n\u2022 Read every ECAF that comes into Service Delivery and identify the cause, Immediate/Near Term and Long Term Resolutions\n\u2022 Provide Top 3 Action Plans to the operation leaders of Direct Sales, Sales Support , Inbound, OTM, & Online/Chat that helps identify process gaps and improve customer service & Completion Rates\n\u2022 Monitor Closure of every ECAF so SLA's are not missed\n\u2022 Launched RTCSR across all Sales Channels\n\u2022 Provided weekly and monthly reports on RTCSR usage\n\u2022 Insource Profile Management. Build the Residential Sales Support profile for the agents\n\u2022 Performed Site visit assessments during merger\n\u2022 Focus group assessments and provided observations, insights & opportunities"", u'Operation Analyst\nCharter Communications - Stamford, CT\nMarch 2013 to June 2016\nTeam: Call Analytics Team\n\u2022 Analyze and report on the daily call volume and call rate\n\u2022 Create reports with in depth analysis for the digitalization\n\u2022 Extract reports from Microstrategy and use generate pivot tables and make detailed reports which provide basis for decision making.\n\u2022 Generate various trending graphs, heat graphs including bar and pie charts\n\u2022 Represent various statistical data to help with the call management\n\u2022 Create templates for daily reports which include many pivot tables and graphs\n\u2022 Develop Scorecards\n\u2022 Track all product initiatives call volume from Day-30 to Day+30\n\u2022 Report on the hourly updates on the day of deployment\n\u2022 Generate daily tracking and keep track of all the changes of the product\n\u2022 Listen to calls and provide the root causes and resolutions\n\u2022 Create Adhoc reports\n\u2022 Provide Weekly updates\n\u2022 Generate Monthly reports\n\u2022 Analyze and track all product launches and provide detailed reporting\n\u2022 Provide daily updates on Software changes\n\u2022 Create presentations using power point.\n\u2022 Build the call disposition codes.\n\u2022 Build Tracker & Compliance reports and update on a weekly basis.\n\u2022 Report on the Average Handling Time on a daily basis', u""Risk Data Analyst & Business Analyst\nGE Capital - Norwalk, CT\nJune 2010 to February 2013\nIt's a risk system where the automated batches will reject the customer based on invalid / insufficient info. We need to study the customer with the help of various internal data warehousing systems and correct the data and push it back to PRD before the next day batch starts.\n\n\u2022 Analyze and validate customer master information in UCM from Dun and Bradstreet\n\u2022 Analyze all the data in the Enterprise Data Warehouse and build credit standings for the Top Customers\n\u2022 Utilize OSIRIS to monitor company mergers/acquisitions\n\u2022 Validating the top customers subsidiaries using EDGAR EX-21 10-K filing\n\u2022 Monitor GE GUP movement periodically and report all latest updates\n\u2022 Analyze and build legal hierarchies of companies with exposure amounts of up to 100MM+\n\u2022 Monitor bankruptcy filings, including failed banks (via FDIC), and recorded new obligors\n\u2022 Worked on IRIS to validate the financial activities of Customers\n\u2022 Worked on Top Customers and ensured their mastering is correct\n\u2022 Goal is to make data as clean and accurate as possible and to aggregate risk and exposure from customers to GE across industry, credit rating, geography, sales and asset size, etc.\n\u2022 Designed SOPs and presented to Risk Manager\n\u2022 Perform UAT, validate the results before the final submission to PRD.\n\u2022 Analyze the BO reports/extracts and submit to the Risk manager after the proper validation.\n\u2022 Ran SQL queries to extract data and submit the report\n\u2022 Active participation in internal audits\n\u2022 Worked in a global delivery model and raise/discuss the issues / bugs with the Offshore Siebel team\n\u2022 Tracking the issues / bugs with the help of Support trackers\n\u2022 Validate the fixes and provide the feedback"", u""Business Analyst\nIcon Resources - Singapore\nFebruary 2007 to March 2010\nIcon resources is an infra (data storage) company where client's requirement on the data usage is studied and suggest the storage required for their performance. Interacting with the onshore engineers to study the test data / results and arrive at the decision to suggest the customer.\n\n\u2022 Worked in Incremental Agile methodology\n\u2022 Understand the requirement and confirm the same with PO\n\u2022 Translate the requirement to the scrum team to enhance user stories\n\u2022 Take part in daily standoff meetings with scrum team, SM and PO\n\u2022 Validate the user stories against the acceptance criteria mentioned\n\u2022 Analyzed the business requirement understood the needs and enhanced steps to create the user stories\n\u2022 Actively participated in acceptance and validation of the stories by validating the delivered stories.\n\u2022 Confirm the performance of a change in the acceptable range in a sprint cycle\n\u2022 Validate /confirm the performance across sprint cycles of a given release.\n\u2022 Provide the sign off for the features included in the release\n\u2022 Helped team and to create good product backlogs\n\u2022 Prepared the artifacts on training for new joiners"", u""Data Analyst\nIcon Resources - Singapore\nSeptember 2004 to January 2007\nIcon resources is an infra (data storage) company where client's requirement on the data usage is studied and suggest the storage required for their performance. My main responsibility was to work with Business analyst and evaluate the feasibility of the requirement and provide the input to the onshore engineers to conduct the performance / system testing.\n\n\u2022 Worked in Incremental Agile methodology\n\u2022 Understand the testing requirement and confirm the same with BA\n\u2022 Translate the user stories to test engineers along with the testing scenario\n\u2022 Validate the data at the end of sprint cycles against the user stories\n\u2022 Tracking performance of the test between different sprint cycles and make a release level performance document\n\u2022 Actively participated testing of stories by validating the end result.\n\u2022 Confirm the performance with BA on the standoff meeting\n\u2022 Approve the testing of a sprint cycle if it meets performance requirement\n\u2022 Take part in scrum meeting with BA and scrum team\n\u2022 Daily testing status update to SM and PO"", u'Support Analyst\nIT Solutions (India) Pvt. Ltd - Chennai, Tamil Nadu\nFebruary 2003 to August 2004\nIT Solutions is a service provides company (now known as NTT DATA) which provides services like System support , L2 support , consulting etc , where this project is to provide the L2 support for AMER region for Aerospace applications. Addressed medium complex applications and escalated high complex issues to L3 and tracked all the issues to the closure.\n\n\u2022 Handle customer Queries\n\u2022 Client Interaction Honeywell Customers from the US\n\u2022 Address the medium complex issues\n\u2022 Escalate the issues to L3\n\u2022 Incident management\n\u2022 Problem Management\n\u2022 Documented the customer requirement and explained DEV team\n\u2022 Tracking and validating customer requirements\n\u2022 Status updates to the clients\n\u2022 Job Monitoring\n\nClients Handled:\n\u2022 Honeywell (US)\n\u2022 IBM (US)', u'Quality Analyst\nDSQ Software Ltd. - Net vision cybertech Ltd - Chennai, Tamil Nadu\nJune 2002 to February 2003\nDSQ is an IT firm where BPO is one new wings of it where resources are trained to support the AMER customer. To train them and improve quality of the calls, they have deployed quality analyst. My main responsibility was to assess the quality of the outbound sales calls of the callers and provide the feedback for improvisation.\n\u2022 Quality checks of the calls\n\u2022 Customer interaction\n\u2022 Handled escalations\n\u2022 Handled Floor confirmation\n\u2022 Training the new agents\n\nClients Handled:\n\u2022 SPRINT\n\u2022 AT&T\n\u2022 DISH-HOME\n\u2022 SUNCOMM\n\u2022 VOICESTREAM\n\u2022 TMOBILE\n\u2022 CINGULAR WIRELESS\n\u2022 PROTECT AMERICA\n\u2022 EARTHLINK']",[],[]
0,https://resumes.indeed.com/resume/b776465705d23fa0,"[u""Data Scientist\nSykes Enterprises, FL\nDecember 2017 to Present\n\u2022 Planning strategic workforce scaling by implementing employee attrition (Microsoft Azure, SQL, and R) predictive model; recommend analytical strategies on retaining best employees\n\u2022 Built and deployed a recommender system on Microsoft Azure to reduce agent's average handle time of issue by 16%\n\u2022 Predicting type of customer service request by applying deep learning techniques on call center telemetric data to reduce an employee's average request handling time when assisting customers"", u""Data Science Consultant\nLimra International Insurance\nJuly 2017 to December 2017\n\u2022 Identified trends that affect length of service(LOS) of agents and estimated LOS using Linear regression; suggested business proposals to improve LOS of insurance agents\n\u2022 Drawn insights about agents that drive high premium collections and designed analytical models to predict premium collected by agents of various insurance organizations\n\u2022 Constructed employee churn model on insurance agents' data; Identified characteristics of good agents; proposed business ideas to alleviate effect of new rule from Department of Labor(DOL)."", u'Data Analyst\nIBM - IN\nJanuary 2015 to December 2016\n\u2022 Created classification model (Logistic Regression, SQL, and R) with 90% accuracy to analyze customer complaints on incorrect orders and performed Root Cause Analysis; reduced incorrect orders by 75%\n\u2022 Delivered Financial Solution to fix incorrect monthly bill cycle and correctly billed approximately $1 million towards client and won recognition from Subject Matter Expert\n\u2022 Designed interactive dashboards in Tableau to track Key Performance Indicators (KPIs) that assist in meeting service level agreement; proposed business strategies leading to increase of team performances by 15%\n\u2022 Extracted data(SQL), prepared reports and identified performance hindering metrics; fall in percent of aged incidents by 60% in 4 months']",[u'in Business'],[u'University of Connecticut School of Business\nMay 2018']
0,https://resumes.indeed.com/resume/cfa9627a4b73c08d,"[u'Data Scientist\nJornaya\nJanuary 2017 to Present\nPerform R&D for new products and product enhancements\nAutomate the building and management of predictive models\nAid the customer success and sales teams with ad-hoc client analysis', u'Data Analyst\nJornaya\nJanuary 2016 to January 2017\nPerform ad-hoc analysis for proof of concepts for new products as well as internal tools\nBuild predictive models combining multiple data sources in order to help clients better manage their marketing lead strategies']",[u'Bachelor of Science in Actuarial Mathematics'],"[u'The University of Pittsburgh Pittsburgh, PA\nApril 2015']"
0,https://resumes.indeed.com/resume/2c084462325f4989,"[u""Data Science Analyst\nRomp N' Roll - Wethersfield, CT\nSeptember 2017 to December 2017\nR, Excel, Tableau\n\u2022 Recommended the best pricing model by performing A/B testing for each franchise location by analyzing the historical sales data\n\u2022 Segmented the customers and built a survival model to decrease attrition through churn prevention\n\u2022 Text mined customer reviews to generate insights to understand member attrition\n\u2022 Created a tracking framework to see variances and trends in Sales and customer traffic across different locations"", u'Systems Engineer (Data Analyst)\nInfosys - Chennai, Tamil Nadu\nJune 2013 to August 2015\nMicroStrategy 9.X, MS SSIS, SSAS, IBM Cognos, Netezza\n\u2022 Created and executed contracts, cost, and risk reporting & analysis of vendor management systems\n\u2022 Translated simpler ad-hoc reports and translated them to dynamic dashboards; improved the user experience\n\u2022 Automated the daily vendor reports for business team using SQL and MicroStrategy\n\u2022 Built and implemented intelligent cubes that decreased the reporting time over 50%\n\u2022 Performed vendor segmentation to identify and analyze the high risk and high spending vendor profiles\n\u2022 Delivered artifacts for technology platforms by collaborating with stakeholders; documented and analyzed the technical and functional requirements\n\u2022 Designed and created proof of concept and prototypes for various project proposals to help acquire new\nprojects from prospective clients']","[u'Master in Business Analytics and Project Management', u'Bachelor of Engineering in Computer Science']","[u'University of Connecticut School of Business Hartford, CT\nMarch 1995 to March 2018', u'Anna University Chennai, Tamil Nadu\nJanuary 2010 to May 2012']"
0,https://resumes.indeed.com/resume/1b2f9b2a8f177f21,"[u""Data Scientist\nJohn Deere - Moline, IL\nMarch 2016 to Present\nDescription: John Deere is the brand name of Deere & Company, an American corporation that manufactures agricultural, construction, and forestry machinery, diesel engines, drivetrains (axles, transmissions, gearboxes) used in heavy equipment, and lawn care equipment. In 2017, it was listed as 105th in the Fortune 500 America's ranking and was ranked 407th in the global ranking.\n\nResponsibilities:\n\u2022 Developed a recommender system using Matchbox recommender in AzureML, to assign top 5 Senior Agents to Agents seeking help regarding a topic. Thus, facilitated effective query handling and increased operational efficiency by 40%\n\u2022 Performed statistical modeling and developed a model using hierarchal clustering and logistic regression, to identify employees who would need help. Thereby reduced average call handle time by 30% and enhanced customer satisfaction.\n\u2022 Achieved an accuracy of more than 90% for the predictive models for each of the projects and presented the results to the clients.Used ensemble across all models for client delivery on a monthly basis.\n\u2022 Built proficiency in the rare disease space and generated a revenue growth of at least7 million dollars for each of the clients.\n\u2022 Identified the different leading indicators/important variables based on claims,physician and demographic level data. Used dimension reduction based on mean decrease in accuracy, PCA and checked for co linearity to reduce the number of variables from 6500 variables to 20 variables.\n\u2022 Experienced in working with high dimensional claims and third party data sets (274 million rows and 6500 columns).\n\u2022 Built efficient SQL queries for data mining, data preparation and cleaning.\n\u2022 Built chi-square test to compare the values between different groups of severe heart attacks (STEMI) and non-severe (NSTEMI) based on age, gender, ethnicity, geographic location, insurance method, BMI index, in hospital procedures etc. Conducted ANOVA to compare the values between different groups and within levels. Used Wilcox test to compare the medians between different groups, calculated the risk ratio between different groups.\n\u2022 Managed a 6 member team to build predictive models, conduct statistical analysis and defined KPI'sfor patient journey (demographics, co morbidity, payer, physician, line of therapy analysis) to help clients make decisions.\n\u2022 Designed appropriate reports, visualization and written analyses for clients using R, MSExcel and PowerPoint.\n\u2022 Extracted meaningful analyses, interpreted raw data, conducted quality assurance and provided meaningful conclusions and recommendations to the clients based on the data results.\n\u2022 Conducted training and knowledge sharing session's forthe offshore and onsite team members, interns on various analytical, statistical testing, machine learning concepts and tools.\n\u2022 Performed social network analysis and topic modeling in R, on employee chat data, and develop Sankey plot to understand the communication paths, the strength of relations between Agents and the topics frequently discussed between them\n\u2022 Analyzed employee behavior and performance data, and developed Shiny dashboards to evaluate team preparedness through metrics, which helped evaluate leadership skills, agent experiences, agent behavior and customer sentiments\n\u2022 Developed SQL procedures to synchronize the dynamic data generated from GTID systems with the Azure SQL Server.\n\u2022 Creation of intelligent benchmarks for claims KPIs using machine learning to reduce the noise in the existing alert framework\n\u2022 Time series forecasting using combination of methodologies to forecast the future values of KPI with dynamic tolerance limits based on the historical pattern\n\u2022 Process automation using Python/R scripts with Oracle database to generate and write the results in the production environment on weekly basis\n\u2022 Intelligent matching of truck delay data & work order data\n\u2022 Root cause analysis using text mining of work order description to find reason behind machine breakdown and the failed part(s) involved\n\u2022 Sequence mining to identify pattern of machine breakdown\n\nEnvironment: R 9.0, Informatica 9.0, ODS, OLTP, Oracle 10g, Hive, OLAP, DB2, Metadata, MS Excel, Mainframes MS Vision, Rational Rose."", u""Data Scientist\nCVS Health - Dallas, TX\nFebruary 2015 to March 2016\nDescription:CVS Health Corporation (previously CVS Corporation and CVS Caremark Corporation) is an American retail pharmacy and health care company headquartered in Woonsocket, Rhode Island. The company began in 1964 with three partners who grew the venture from a parent company, Mark Steven, Inc., that helped retailers manage their health and beauty aid product lines.\n\nResponsibilities:\n\u2022 Machine Learning Projects based on Python, SQL, Spark and SAS advanced programming. Performed data exploratory, data visualizations, and feature selections\n\u2022 Applications of machine learning algorithms, including random forest and boosted tree, SVM, SGD, neural network, and deep learning using CNTK and Tensorflow.\n\u2022 Performed data analysis, natural language processing, statistical analysis, generated reports, listings and graphs.\n\u2022 Big data analytics with Hadoop, HiveQL, SparkRDD, and SparkSQL.\n\u2022 Tested Python/SAS on AWS cloud service and CNTK modeling on MS-Azure cloud service.\n\u2022 Built prediction models of major subsurface properties for underground image, geologic interpretation and drilling decisions. Utilized advanced methods of big data analytics, machine learning, artificial intelligence, wave equation modeling, and statistical analysis. Provided exclusive summary on oil/gas seismic data and well profiles, conduct predictive analyses and data mining to support interpretation and operations.\n\u2022 Cross-correlation based data analysis method through Python and Matlab on multi-offset-well to help predict the models and pore-pressure ahead a little for real time drilling. Big data modeling with incorporation of seismic, rock physics, statistical analysis, well logs and geological information into the 'beyond image'.\n\u2022 Using Python, Developing, Operationalizing, and Productionizing machine learning models to make significant impact on the geological pattern identification and subsurface model prediction. Analyzing seismic and log data with sub-group analysis (classification-clustering) and model prediction methods (regression, decision tree, generic programming etc.).\n\u2022 Use SAS statistical regression method and SAS/REG polynomial simulation in Excel to simulate the anisotropic trend as 1D depth functions. Validate the simulated function by image quality of depth migration.\n\u2022 Tested the migrated data processing system on Google Cloud with velocity model updating tasks.\n\u2022 ETL to convert unstructured data to structured data and import the data to HadoopHDFS. Utilized MapR as a low-risk big data solution to build a digital oilfield. Efficiently integrated and analyzed the data to increase drilling performance and interpretation quality. Analyzed sensors and well log data in HDFS with HiveQL and prepare for prediction learning models.\n\u2022 Constantly monitored the data and models to identify the scope of improvement in the processing and business. Manipulated and prepared the data for data visualization and report generation. Performed data analysis, statistical analysis, generated reports, listings and graphs.\n\u2022 Co-leader of mathematics community 2015, SchlumbergerEureca.\n\u2022 Accomplished customer segmentation using K-means algorithm in R, based on behavioral and demographic tendencies, for improving campaigning strategies. This helped reduce marketing expenses by 10% and helped boost client's revenue\n\u2022 Built customer lifetime value prediction model using historical telecom data in SAS to better serve high priority customers through loyalty bonus, personalized services and draft customer retention plans and strategies\n\u2022 Developed PLSQL procedures and functions to automate billing operations, customer barring and number generations\n\u2022 Redesigned the workflows of ServiceRequest, BulkService orders using UNIXCron jobs and PL/ SQL procedures, thereby reduced order processing time and average slippages per month dropped by 40%.\n\nEnvironment: SQL Server 2008R2/2005 Enterprise, SSRS, SSIS, Crystal Reports, Hadoop, Windows Enterprise Server 2000, DTS, SQL Profiler, and Query Analyzer."", u'Data Analyst\nLiquidity Services Inc - Washington, DC\nMarch 2014 to January 2015\nDescription: Liquidity Services, Inc. operates various online auction marketplaces for surplus and salvage assets in the United States. Its auction marketplaces include liquidation.com, which enables corporations and selected government agencies located in the United States to sell surplus and salvage consumer goods and capital assets.\nResponsibilities:\n\u2022 Responsible for performing Machine-learning techniques regression/classification to predict the outcomes.\n\u2022 Responsible for design and development of advanced R/Python programs to prepare transform and harmonize data sets in preparation for modeling.\n\u2022 Developed large data sets from structured and unstructured data. Perform data mining.\n\u2022 Partnered with modelers to develop data frame requirements for projects.\n\u2022 Performed Ad-hoc reporting/customer profiling, segmentation using R/Python.\n\u2022 Tracked various campaigns, generating customer profiling analysis and data manipulation.\n\u2022 Provided R/SQL programming, with detailed direction, in the execution of data analysis that contributed to the final project deliverables. Responsible for data mining.\n\u2022 Analyzed large datasets to answer business questions by generating reports and outcome.\n\u2022 Worked in a team of programmers and data analysts to develop insightful deliverables that support data-driven marketing strategies.\n\u2022 Executed SQL queries from R/Python on complex table configurations.\n\u2022 Retrieving data from database through SQL as per business requirements.\n\u2022 Create, maintain, modify and optimize SQL Server databases.\n\u2022 Manipulation of Data using BASESAS Programming.\n\u2022 Adhering to best practices for project support and documentation.\n\u2022 Understanding the business problem, build the hypothesis and validate the same using the data.\n\u2022 Managing the Reporting/Dash boarding for the Key metrics of the business.\n\u2022 Involved in data analysis with using different analytic techniques and modeling techniques.\n\nEnvironment: Erwin r9.0, Informatica 9.0, ODS, OLTP, Oracle 10g, Hive, OLAP, DB2, Metadata, MS Excel, Mainframes MS Visio, Rational Rose, Requisite Pro, Hadoop, PL/SQL, etc.', u'Data Analyst\nFarm Bureaus - Columbus, OH\nAugust 2013 to February 2014\nDescription: Over the last 85 years, Nationwide has grown from a small mutual auto insurer owned by Policyholders to one of the largest insurance and financial services companies in the world. Early growth came\nfrom working together with Farm Bureaus that sponsored the company. Nine Farm Bureaus continue to promote Nationwide.\n\nResponsibilities:\n\u2022 Statistical Modelling with ML to bring Insights in Data under guidance of PrincipalDataScientist\n\u2022 Datamodeling with Pig, Hive, Impala.\n\u2022 ame\n\u2022 Used SVN to commit the Changes into the main EMM application trunk.\n\u2022 Understanding and implementation of text mining concepts, graph processing and semi structured and unstructured data processing.\n\u2022 Worked with AjaxAPI calls to communicate with Hadoop through Impala Connection and SQL to render the required data through it .These API calls are similar to MicrosoftCognitiveAPI calls.\n\u2022 Good grip on Cloudera and HDP ecosystem components.\n\u2022 Used ElasticSearch (BigData) to retrieve data into application as required.\n\u2022 Performed MapReduce Programs those are running on the cluster.\n\u2022 Developed multiple MapReduce jobs in java for data cleaning and preprocessing.\n\u2022 Analyzed the partitioned and bucketed data and compute various metrics for reporting.\n\u2022 Involved in loading data from RDBMS and web logs into HDFS using Sqoop and Flume.\n\u2022 Worked on loading the data from MySQL to HBase where necessary using Sqoop.\n\u2022 Developed Hive queries for Analysis across different banners.\n\u2022 Extracted data from Twitter using Java and TwitterAPI. ParsedJSON formatted twitter data and uploaded to database.\n\u2022 Launching AmazonEC2 Cloud Instances using AmazonImages (Linux/ Ubuntu) and Configuring launched instances with respect to specific applications.\n\u2022 Exported the result set from Hive to MySQL using Sqoop after processing the data.\n\u2022 Analyzed the data by performing Hive queries and running Pig scripts to study customer behavior.\n\u2022 Have hands on experience working on Sequencefiles, AVRO, HAR file formats and compression.\n\u2022 Used Hive to partition and bucket data.\n\u2022 Experience in writing MapReduce programs with JavaAPI to cleanse Structured and unstructured data.\n\u2022 Wrote Pig Scripts to perform ETL procedures on the data in HDFS.\n\u2022 Created HBase tables to store various data formats of data coming from different portfolios.\n\u2022 Worked on improving performance of existing Pig and Hive Queries.\n\nEnvironment: SQL/Server, Oracle 9i, MS-Office, Teradata, Informatica, ER Studio, XML, Business Objects, HDFS, Teradata 14.1, JSON, HADOOP (HDFS), MapReduce, PIG, Spark, R Studio, MAHOUT, JAVA, HIVE, AWS..', u'Data Architect/Data Modeler\nInfotech Enterprises IT Services Pvt. Ltd - Hyderabad, Telangana\nJanuary 2012 to July 2013\nDescription: Infotech Enterprises IT Services Pvt. Ltd. (Infotech IT), part of the $ 260 million Infotech Enterprises group, leverages its business process knowledge, technological competence, strategic alliances and strong global presence to offer innovative IT solutions to the Retail Industry\n\nResponsibilities:\n\u2022 Worked with large amounts of structured and unstructured data.\n\u2022 Knowledge in MachineLearning concepts (GeneralizedLinearmodels, Regularization, RandomForest, TimeSeriesmodels, etc.)\n\u2022 Worked in Business Intelligence tools and visualization tools such as BusinessObjects, Tableau, ChartIO, etc.\n\u2022 Deployed GUI pages by using JSP, JSTL, HTML, DHTML, XHTML, CSS, JavaScript, and AJAX.\n\u2022 Configured the project on WebSphere 6.1 application servers.\n\u2022 Implemented the online application by using CoreJava, Jdbc, JSP, Servlets and EJB 1.1, WebServices, SOAP, WSDL.\n\u2022 Handled end-to-end project from data discovery to model deployment.\n\u2022 Monitoring the automated loading processes.\n\u2022 Communicated with other HealthCare info by using WebServices with the help of SOAP, WSDLJAX-RPC\n\u2022 Used Singleton, factory design pattern, DAO Design Patterns based on the application requirements\n\u2022 Used SAX and DOM parsers to parse the raw XML documents\n\u2022 Used RAD as Development IDE for web applications.\n\u2022 Preparing and executing Unit test cases\n\u2022 Used Log4J logging framework to write Log messages with various levels.\n\u2022 Involved in fixing bugs and minor enhancements for the front-end modules.\n\u2022 Implemented MicrosoftVisio and RationalRose for designing the UseCaseDiagrams, Classmodel, Sequence diagrams, and Activity diagrams for SDLC process of the application\n\u2022 Doing functional and technical reviews\n\u2022 Maintenance in the testing team for System testing/Integration/UAT.\n\u2022 Guaranteeing quality in the deliverables.\n\u2022 Conducted Design reviews and Technical reviews with other project stakeholders.\n\u2022 Was a part of the complete life cycle of the project from the requirements to the production support\n\u2022 Created test plan documents for all back-end database modules\n\u2022 Implemented the project in Linux environment.\n\nEnvironment: R 3.0, Erwin 9.5, Tableau 8.0, MDM, QlikView, MLLib, PL/SQL, HDFS, Teradata 14.1, JSON, HADOOP (HDFS), MapReduce, PIG, Spark, R Studio, MAHOUT, JAVA, HIVE, AWS.', u'Data Analyst/Data Modeler\nFirst Indian Corporation Private Limited - Hyderabad, Telangana\nMay 2009 to December 2011\nDescriptionFirst Indian Corporation Private Limited, a member of The First American family of companies, provides specialized offshore transaction services, technology services and analytics to the mortgage industry.\n\nResponsibilities:\n\u2022 DevelopedInternet traffic scoring platform for ad networks, advertisers and publishers (ruleengine, sitescoring, keywordscoring, liftmeasurement, linkageanalysis).\n\u2022 Responsible for defining the key identifiers for each mapping/interface.\n\u2022 Clients include eBay, ClickForensics, Cars.com, Turn.com, Microsoft, and Looksmart.\n\u2022 Implementation of MetadataRepository, MaintainingDataQuality, DataCleanupprocedures, Transformations, DataStandards, DataGovernance program, Scripts, StoredProcedures, triggers and execution of test plans.\n\u2022 Designed the architecture for one of the first analytics 3.0. Onlineplatforms: all-purpose scoring, with on-demand, SaaS, API services. Currently under implementation.\n\u2022 Web crawling and text mining techniques to score referral domains, generate keyword taxonomies, and assess commercial value of bid keywords.\n\u2022 Developed new hybrid statistical and data mining technique known as hidden decision trees and hidden forests.\n\u2022 Reverse engineering of keyword pricing algorithms in the context of pay-per-click arbitrage.\n\u2022 Performed data quality in TalendOpenStudio.\n\u2022 Coordinated meetings with vendors to define requirements and system interaction agreement documentation between client and vendor system.\n\u2022 Automated bidding for advertiser campaigns based either on keyword or category (run-of-site) bidding.\n\u2022 Creation of multimillion bid keyword lists using extensive web crawling. Identification of metrics to measure the quality of each list (yield or coverage, volume, and keyword average financial value).\n\u2022 Enterprise Metadata Library with any changes or updates.\n\u2022 Document data quality and traceability documents for each source interface.\n\u2022 Establish standards of procedures.\n\u2022 Generate weekly and monthly asset inventory reports.\n\nEnvironment: Erwin r7.0, SQL Server 2000/2005, Windows XP/NT/2000, Oracle 8i/9i, MS-DTS, UML, UAT, SQL Loader, OOD, OLTP, PL/SQL, MS Visio, Informatica.']",[],[]
0,https://resumes.indeed.com/resume/e213f851de825c42,"[u'Data Entry Analyst\nCommunication & Power Industry - Palo Alto, CA\nJune 1996 to February 2018\nData Entry Analyst']","[u'MS Office Software in Computer', u'Secretarial Graduate in Computer', u'General Education in ESL', u'High school or equivalent in General Education']","[u'CPI Palo Alto, CA\nAugust 2011 to May 2012', u'CET San Jose, CA\nOctober 1991 to June 1993', u'Mission College Santa Clara, CA\nMarch 1990 to June 1991', u'Institute National Centro America Guatemala Centro America\nJanuary 1976 to October 1981']"
0,https://resumes.indeed.com/resume/a5e42fb0d18e54d7,"[u'Data Clerk\nUganda PrisonsService - Kampala\nPresent\nData Clerk, Data entrance, Data analyst', u'Data Analyst\nMinistry of Internal affairs(Prisons Headquarter - kamapala\nPresent\ncapturing, Cleaning, analysing and reporting']","[u'Diploma in Computer Science', u'Diploma in Computer Science']","[u'YMCA Comprehensive Institute Kampala', u'YMCa Comprehensive Institute']"
0,https://resumes.indeed.com/resume/052e8b533ee1be46,"[u'Data Analyst\nLakewood, NJ\nOctober 2012 to Present\nCreate hundreds of annual implementation reports to increase sales and renewals (over 750\nreports in 2016)\n\u2022 Develop visually appealing ad hoc reports in Tableau and Excel using graphic tools, tables, and graphs, as per customer requests\n\u2022 Develop Tableau templates and processes for use in recurring reports, to automate processes\nformerly done manually, resulting in significant savings of time for each report run\n\u2022 Review 500 automated reports monthly to confirm accuracy of data and precision of process\n\u2022 Monitor a Salesforce queue to ensure customer requests for data reports are met within a short\ntimeframe (571 Salesforce tickets in 2016)\n\u2022 Work with other team members to complete special projects and meet project deadlines\n\u2022 Provide basic and higher-level training and support to other team members in Tableau\n\u2022 Use advanced Tableau features including calculated fields, filters, table calculations, and joins']","[u'in Computer Science', u'']","[u'Thomas Edison College, Zaidner Institute Division Jerusalem, IL', u'Duke University']"
0,https://resumes.indeed.com/resume/72c60a3e372c3aaf,"[u'Business Data Analyst\nClient -Prosight Specialty Insurance\nJanuary 2014 to Present\nProject: Premiere Duck Creek\nPremiere is the implementation of Duck Creek Policy Administration system across the Organization. Project involved design and development of an integration solution that enable Duck Creek Session XML data to flow to the downstream system. Multiple Lines (LOBs) are being implemented including Auto, WC, Crime, Business Owners and CPP.\n\n\u27a2 Gathered and documented Business Requirement Documents and the Technical Specification.\n\u27a2 Created Entity Relation diagrams to represent the relationship existing between the internal schema of the database, depicting the data tables, the data columns of those tables, and the relationships between the tables.\n\u27a2 Created Test Scenarios and scripts for Unit testing, Integration testing and User Acceptance testing (UAT) of the processes.\n\u27a2 Validated various ISO and Proprietary Forms.\n\u27a2 Interacted with Business users to analyze the business process and requirements. Worked with the Business Analyst and Data Modeler during the functional design and technical design phases.\n\u27a2 Developed Testing Strategies and managed development of detailed test plans and schedules, coordinated all the levels of tests for different test phases - planning, design, setup, execution, and defect management.\n\u27a2 Analyzing the XML for source data validation.\n\u27a2 Provided Production support and coordinated the activities of support team.\n\u27a2 Performed data mapping from Source Duck Creel XML to target SQL Tables\n\u27a2 Perform ETL Testing to ensure Informatica Workflow Source and Target Mapping are as per requirements.\n\u27a2 Partner with development team to ensure bugs resolution are prioritized, resolved and retested.\n\u27a2 Collaborate with offshore team on development and implementation.\n\u27a2 Develop and execute SQL Scripts for Data Analysis and Validations.\n\u27a2 Key member in Design and Development of Quality Metric Reports in Tableau.\nProject: MGA Premium Processing\n\nManaging General Agents (MGAs) serve as Brokers and sell insurance on behalf of the Insurance Company. MGAs send the Policy Transactions in the form of a monthly batch file (Bordereaux) that needs to be processed and booked in the downstream system. The project involved design and development of a custom solution that enabled Automated MGA Premium file processing. Data Quality and Compliance issues were addressed using a Rules Engine that validated, cleansed and enriched the data.\n\nResponsibilities:\n\u27a2 Gathered user and business requirements through open-ended discussions, prototyping and observation of the compliance department and work environment\n\u27a2 Documented the user requirements, analyzed & prioritized and converted them as system requirements that must be included while developing the software.\n\u27a2 Conducted iterations and communicated results and milestone achievements to the stakeholders while keeping the developers closely updated on all documented refinements.\n\u27a2 Conducted walkthroughs with the end users and stakeholders to gather the modification requests from the user to upgrade/change the business specifications for the product & ensured that the developers were updated.\n\u27a2 Involved in the development of new line of business - RCA and Truckers.\n\u27a2 Worked closely with internal and external teams to ensure the quality control of product deliverables to customers.\n\u27a2 Developed Testing Strategies and managed development of detailed test plans.\n\u27a2 Used SQL queries for analyzing the rejects.\n\u27a2 Prepared prototypes of UI screens and assisted Business User teams to design screen interfaces.\n\u27a2 Managed and resolved Jira tickets.\n\u27a2 Managed Support team in processing Monthly Bordereau files.\n\u27a2 Conducted end user training in using various data control tools.\n\u27a2 Key team member in designing and developing Data Quality tool.\nEnvironment: Windows, SQL Server, JIRA, MS Word, Excel, Informatica, Tableau', u'Business Data Analyst\nUSAS Technologies\nJanuary 2012 to December 2013\nNJ.\nPPMS is a Cloud-based Patient Physician Management System that is built on the Salesforce.com platform. It captures Physician, Patient, and Billing information. PPMS is designed to track and increase Patient Referrals. In addition to Referral Management, PRMS manages end-to-end operational information Appointments, Patient Communication and Billing.\n\nResponsibilities:\n\u27a2 Conducted end user training in using various data control tools.\n\u27a2 Developed Functional Specification Document (FSD) and Business Requirements Document (BRD) for the Patient Management lifecycle in Salesforce.\n\u27a2 Developed Use Cases and create Sequence Diagrams to map processes and workflows.\n\u27a2 Customized Salesforce Standard Objects including Contacts, Leads, Reports and Opportunities.\n\u27a2 Created Custom Field and Field Dependencies.\n\u27a2 Developed Validation Rules on Standard and Custom Objects to maintain data quality.\n\u27a2 Developed Informatica based mapping to load data into Staging Tables.\n\u27a2 Developed DupeBlocker Rules to eliminate duplicate data from PPMS.\n\u27a2 Designed Custom Salesforce Reports.\n\u27a2 Prioritized requirement deliverables documented in BRD to meet the project milestones.\n\u27a2 Review Test Cases and Test Scenarios and coordinate User Acceptance Testing (UAT).\n\u27a2 Prepared User Manual.\n\nEnvironment: Saleforce.com, Oracle, Informatica, Toad, SQL Navigator, Windows 7']","[u'Masters of Computer Management in MCM', u'Bachelor of Commerce in Commerce']","[u'Pune University Pune, Maharashtra', u'Bangalore University Bengaluru, Karnataka']"
0,https://resumes.indeed.com/resume/d3597d4544894896,"[u'Data Analyst\nTotal Health Care - Detroit, MI\nJanuary 2014 to December 2015\nProject description:\nICD 10 Project is a federal mandate to replace the current ICD-9 Diagnosis codes with the ICD 10 Diagnosis code set. The US Department of Health and Human Services (HHS) is requiring health plans, Physicians, hospitals, and other health care professionals to be ICD 10 compliant.\n\nResponsibilities:\n\u2022 Managed Project scope, milestones, risks and user acceptance tests with internal and external stakeholders.\n\u2022 Conducted meetings with users and other stakeholders to elicit, organize and document the requirements and also created project status reports and feasibility reports for stakeholders.\n\u2022 Gathered and analyzed the requirements and converted them into Business Requirement Specifications (BRS) and Functional Requirement Specifications (FRS) for the designers and developers and QA Team to understand them as per their perspective.\n\u2022 Designed and developed use cases, activity diagrams, and sequence diagrams using Net Beans.\n\u2022 Extensively facilitated meetings/conference calls with stakeholders, project managers, developers, QA Analysts and other end users based onsite/offshore to deal with business and technical issues and to resolve the issues.\n\u2022 Created ""As-Is Business Models"" to understand the existing claim settlement process flow through interacting with SMEs, underwriters and Finance department.\n\u2022 Shared knowledge by performing Impact Analysis on transition from ICD 9 - ICD 10, made suggestions on UI and other changes required to be met during transition.\n\u2022 Worked intensively with Medicare & Medicaid claims and claims processing for HIPAA 5010 X12 transactions.\n\u2022 Involved in requirements gathering and preparing the technical system design and System Design analysis of the flow of EDI transaction sets 834, 835, 997.\n\u2022 Performed Gap Analysis for new functionality requirements, as well as prioritized them based on actual business needs so as to align them with the product release roadmap.\n\u2022 Involved in testing the conversion of the application by generating test scenarios in test plan document.\n\u2022 Validated data at the back-end to ensure that all the Claims related data had been loaded to the corresponding Data Sets in the back-end and the pricing for these Claims was completed as per the Standards.\n\u2022 Developed HL7 messaging for bi-directional case and disease report exchange, in text and XML formats, in accordance with HL7 specifications\n\u2022 Analyzed the results, generated reports and tracked the defects using Quality Center.\n\u2022 Created and maintained the Test and Traceability Matrix.\n\u2022 Conducted System, Integrated and Regression testing to the application using Business, Functional, User Acceptance and Usability testing.\n\u2022 Prepared test data in HIPAA compliant X12N format for both inbound and outbound healthcare EDI transactions.\n\u2022 Trained implementation managers to work on the new application.\n\u2022 Generated SQL scripts for Development and testing purposes.\n\u2022 Actively participated in all phases of testing lifecycle (Design, Planning, Development and Results)', u'Data Analyst\nAssurant Health Foundation Inc - Milwaukee, WI\nNovember 2011 to December 2013\nLocation: Milwaukee, WI\n\nAssurant Health provides individual, short-term health insurance and small employer group health insurance.\nThe scope of the project includes the enhancement of the existing claims system from HIPAA X12 4010 to X12 5010.\nI have worked on EDI transaction 837- Health care Claims and electronic data interfaces (EDI) to accommodate the upcoming 5010 EDI changes.\n\nResponsibilities:\n\u2022 Interacted with Senior Executives and Stake Holders to understand needs and identify key challenges, constraints and risks; thereby define scope.\n\u2022 Gathered High-Level and Detailed-Level functional and non-functional requirements via daily JAD sessions to define the Business requirements and also created Requirement Traceability Matrix (RTM)\n\u2022 Created Use-Case documents, UML Diagrams using MS Visio for all functional requirements to help architects, Developers and testing team.\n\u2022 Coordinate with the development team in making them understand the requirements\n\u2022 Coordinated the upgrade of Transaction Sets 837 to HIPAA compliance.\n\u2022 Did gap analysis for HIPAA 4010 EDI 837 and HIPAA 5010 EDI 837.\n\u2022 Involved in impact analysis of HIPAA 5010 837 transaction sets on different systems in Assurant Health.\n\u2022 Conducted walkthroughs and participated in defect triage meetings periodically to assess the status of the testing process and discuss areas of disparaging\n\u2022 Identifying the requirements for accommodating HIPAA 4010 and 5010 standards for EDI X12 transactions\n\u2022 Collaborated with testing teams in reviewing test strategy, test plans and test scripts.\n\u2022 Performed UAT Testing with QA and Business Team.\n\u2022 Performed data validation using SQL queries.\n\u2022 Managed and controlled the project documentation using SharePoint\n\u2022 Ensured the timely delivery of Business Analyst team artifacts.\n\u2022 Participated and Facilitated Status meeting, Business Requirement Elicitation and Solution meetings', u'Data Analyst\nApollo - Columbus, OH\nOctober 2010 to October 2011\nProject involved Web-based application called EDR (Electronic Data Reserve) database for storing user files\n\nResponsibilities:\n\u2022 Worked on Hospital data processing for improving database for storing user files.\n\u2022 Analyzed business needs, created and developed new functionality to meet real time data integration that facilitated decision.\n\u2022 Active involvement in ETL design for data integration and master data management.\n\u2022 Worked with SQL queries for data manipulations.\n\u2022 Performed duties of a Scrum Master and led the team to produce quality software on schedule using Agile principles and practices. Facilitated daily stand up meetings.\n\u2022 Used INVEST model to write user stories &Planning Poker, Relative valuation to estimate user stories.\n\u2022 Prepared Test cases & Managed Requirement churn by incorporating changes to product backlog.\n\u2022 Created Entity relationship diagrams for the proposed database.\n\u2022 Participated in the development and creation of the data warehouse.\n\u2022 Assisted in building a business analysis process model using Rational Rose, Requisite Pro and Visio.\n\u2022 Designed and developed Use Cases, Activity Diagrams and Sequence Diagrams.\n\u2022 Involved in User Acceptance Testing along with Business User']",[u'Bachelor of Technology in Computer Science and Engineering'],[u'JNTU']
0,https://resumes.indeed.com/resume/bd37755f6af492cd,"[u'Capstone project-Prediction of Profitability of Hospitals\nOctober 2017 to Present\nAnalyze and predict financial performance via SVM&Random Forest algorithms.\n\u2022 Crawled 5000+ operations and financial reports in complicated Excel formats efficiently in 5 days by Python Selenium.\n\u2022 Improved the crawling and data processing speed by 8 times through Multithreads.\n\u2022 Standardized the naming and structure, normalized numeric data, and synchronized data into AWS RDS (MySQL).\n\u2022 Reduced data dimension from 800 to 60 and compared feature selection performances between PCA and Random Forest.\nCourse project-Video site playback QoS analysis dashboard: Oct- Dec, 2017\nAnalyzed and visualized user behaviors through building a QoS dashboard\n\u2022 Processed 10GB video player logs using Hadoop (information extraction, GeoIP lookup etc.) and stored in Hive.\n\u2022 Visualized data in dynamic Web UI with search and drill-down features to help with marketing and customer retention.\n\u2022 Analyzed user behaviors on this dashboard; for example, how rebuffing and video resolutions affect user satisfaction.\nIndustry Project-Personalized sales email generation: Sept- Dec, 2017\nGenerated human-like email response via neural network in Python\n\u2022 Extracted 50,000 raw dialogues and emails from Twitter and Enron; created a dataset of dialogues in Q& A format.\n\u2022 Cleaned special characters, tokenized words, and embed dialogues into vectors of the same dimension via Word2Vec.\n\u2022 Implemented Sequence2Sequence algorithm using TensorFlow with 2 LSTM layers to classify the model.\n\u2022 Decreased the loss function from 8 to 0.03 after training the full dataset for 160 epochs in a week by using GPU.', u'Field Data Analyst\nSchlumberger\nAugust 2014 to May 2017\n\u2022 Core member of the data analysis team for Schlumberger, responsible for $10M annual revenue.\n\u2022 Analyzed geological and geophysical data from well logs, to locate and estimate probable natural gas and oil resources.\n\u2022 Optimized oil discovery process by applying decision tree and regression data models in proprietary software.\n\u2022 Developed an visualization software to highlight potential reservoir zone and detect anomaly of data.\n\u2022 Awarded the ""Team of Best Service Quality"" by reducing the annual loss time from 10 hours to 0.5 hours.', u""Manufacturing Analyst\nHakaru+ - Osaka, JP\nAugust 2012 to February 2014\n\u2022 Initiated the development of a mechanical design database to archive projects and reduce repetitive work.\n\u2022 Standardized mechanical designs by eliminating redundant designs by 20% from 1000 parts to 800.\n\u2022 Communicated with engineers and subcontractors to gather process information for optimization analysis, maintain\nstable inventory and shortened assembling and delivery time.\n\u2022 Prioritized production of different parts based on clients' requirements and manufacturing time; delivered in time 100%\nPROJECTS""]","[u'M.Eng. in Industrial Engineering & Operations Research', u'Bachelor in Energy & Mechanical Engineering', u'']","[u'University of California, Berkeley Berkeley, CA\nAugust 2017 to May 2018', u'Zhejiang University Hangzhou, CN\nSeptember 2008 to June 2012', u'Zhejiang University & Honor']"
0,https://resumes.indeed.com/resume/96859d1caac5ba72,"[u'IT Data Analyst\nBenefit Transact Solutions - Houston, TX\nAugust 2017 to Present\nDevelops and uses data analytic tools to facilitate reconciliation of healthcare enrollment and premium data.\n\nResponsible for Development and maintenance of Third Party Oversight system using SharePoint and InfoPath. Also created and maintained reporting for this system using Access, Excel and Qlikview.\n\nPrepares daily eligibility and fulfillment reports of healthcare benefits.\n\nEnsure that all Eligibility and Fulfillment membership updates adhere to system requirements.\n\nPerforms daily data reports in efforts to identify and correct database coding errors (if any).\n\nAutomate processes, correct internal/external feeds among systems and reduce lag times between systems and processes.', u'Asset Integrity Analyst\nPinnacleArt - Pasadena, TX\nJanuary 2017 to August 2017\nCapture client data through documents such as Piping and Instrumentation Diagrams, Engineering drawings and Code forms.\n\nProduced data reports that ensure the accuracy and integrity of client assets.\n\nImproved the reporting accuracy to eliminate inaccurate data within our Microsoft Excel databases.\n\nEnsured integrity of sourced data by developing new processes and procedures to that identify risk factors that may affect client assets.\n\nTracked project progression and schedule for expected project completion using automated spreadsheets.\n\nSupports cross-functional teams by identifying missing data regarding pipeline and asset integrity.', u'Data Analyst\nThe Reach Group - Houston, TX\nJanuary 2013 to December 2015\nAnalyzed underlying data for potential discrepancies, investigated errors, and performed data cleanup.\n\nSet up wells and completion steps within Excel; monitored and performed QA/QC of daily WellView data for anomalies.\n\nCreated second line support training manual and successfully trained incoming data analyst to be assigned to company projects.\n\nCreated data queries and reports using WellView, Visual Studio, SQL Server Management Studio, SSRS, and Excel.\n\nCreated custom queries/reports designed for quality verification and information sharing.\n\nAchieved highest ROI (Return on Investment) in 2013 for client satisfactory upon project completion.\n\nProvided clear data visualization and a reporting discrepancies on failed units (Mud Motors and BHAs).\n\nModified well databases and performed Quality Assessments on documented well data to improve the reporting accuracy.']","[u'Bachelor of Business Administration in Management Information Systems', u'Masters of Science in Information Technology']","[u'Lamar University Beaumont, TX', u'Southern New Hampshire University']"
0,https://resumes.indeed.com/resume/9fe6d96c973c2222,"[u'Digital Data Analyst\nFidelity Investments - Boston, MA\nJuly 2017 to December 2017\nDeveloped regression and correlation model using web banner data to identify KPIs that drive click through rate. The model\nwas published in the fidelity business review.\n\u2022 Analyzed web traffic using Omniture Adobe Analytics and generated automated dashboards for leadership review to make\nstrategic decisions.\n\u2022 Developed an exclusive dashboard for management review using Qlik sense for the customers with highest trades per year\nto reduce the turnaround time and improve customer satisfaction.\n\u2022 Identified banners for the web marketing team to improve CTR using A/B test data with actionable insights. The report was\npublished in the fidelity business review.\n\u2022 Created data visualization on Adobe Analysis Workspace and analyzed customer behavior including customer journey.', u'Business Analyst\nEricsson India Global Services - Gurgaon, Haryana\nAugust 2012 to June 2016\nLiaised with various business and project stakeholders on an ongoing basis to understand their needs and priorities as well\npro-actively managed expectations, buy-in, feedback and provided resolution of issues.\n\u2022 Draft and maintain business requirements and align them with the technical and functional requirements\n\u2022 Led team of 11 members by organizing agile scrum practices to ensure active and punctual participation in all scrum protocols\nincluding daily stand-ups, sprint planning, demo, retrospective meetings and received accolades for on-time delivery of projects.\n\u2022 Designed workflows and process mapping to reduce the time in various stages of visa approval process in Ericsson using\nBPMN. The workflow ensured parallel approvals with no holds and reduced the onsite travel time from 2 months to one\nweek.\n\u2022 Created a dashboard to generate daily reports and displayed various graphs of the head counts leading to a reduction in the time and effort of managers by 95%.\n\u2022 Routinely performed manual testing/functional testing, exploratory testing, user acceptance testing, system testing,\nregression testing, negative testing, end to end testing and sanity check. Well acquainted with SDLC & STLC.']","[u'Master of Engineering in Management', u'Bachelor of Engineering in Information & Communication Technology']","[u'Northeastern University, College of Engineering Boston, MA\nSeptember 2016 to April 2018', u'Manipal Institute of Technology\nJuly 2008 to May 2012']"
0,https://resumes.indeed.com/resume/8e4ad690723d3ac3,"[u'Data Analyst\nEmergency Physicians of Tidewater\nJune 2005 to Present\n\u2022 Transformed 13 unstructured data sources into clear and concise financial reports and dashboard visualizations for board members using Excel, Access, SQL, and Tableau, consolidating tasks and saving the company approximately $70K per year\n\u2022 Improved financial reporting and forecasting by compiling years of raw financial data into user-accessible online database using Alpha Anywyere\n\u2022 Saved approximately 100 man-hours per year through continuous learning of new tools and processes such as Alteryx and Power BI to further automate data collection and reporting\n\u2022 Leveraged Access/SQL and organization skills to create medical chart tracking database and coordinated with hospital, billing coders, and providers to complete otherwise non-codeable charts for recovered revenue of $1M per year\n\u2022 Migrated, managed and improved data-driven web portal as a single point of reference for scheduling, HR, contact, and professional licenses for 200 clinical providers and staff\n\u2022 Developed productivity-based compensation system for providers and calculated physician productivity and payroll']",[u'Some college'],[u'']
0,https://resumes.indeed.com/resume/c7333364e23fef11,"[u'Marketing Data Analyst\nThermo fisher Scientific - Carlsbad, CA\nMay 2017 to Present\nResponsibilities:\n\u2022 Analyze data from various sources and creating reports weekly, monthly and quarterly reports using Excel, SQL, Tableau and IBM Cognos.\n\u2022 Extracting data from different CRM platforms like DataRoma and Siebel and Omniture and Adobe Analytics.\n\u2022 Experience in data blending across different data sources to the data requirement.\n\u2022 Creating User iterative Tableau Dashboards according to the requirement and publishing them on the Tableau Server.\n\u2022 As a Site Administrator, hands on experience in creating users, groups and assigning permissions, Maintaining Tableau Licenses.\n\u2022 Running the Extract schedules on daily, weekly, monthly basis and also refreshing all the published data sources.\n\u2022 Experience in Tableau Server Clean up in creating backups and archiving unused workbooks in Tableau Server.\n\u2022 Be able to assist with ad-hoc requests from regional/divisional marketing teams.\n\u2022 Help marketing automation developer to govern and validate the database, assist in ETL process and prepare documentations.\n\u2022 Work with different functional teams to connect different Oracle data sources and systems.\nEnvironment: Tableau Desktop 10.2, Tableau Server, EDW Cognos, Oracle.', u""Data Analyst\nJohnson & Johnson - Raritan, NJ\nMarch 2016 to February 2017\nResponsibilities\n\u2022 Involved in reviewing business requirements and analyzing data sources form Excel/MS SQL Server for design, development, testing of reporting and analysis projects within Tableau Desktop.\n\u2022 Development of ad-hoc data visualizations for analytical efforts for the internal team.\n\u2022 Working with data extracts of 4 GB in size, optimizing and managing these extracts by refreshing the schedules.\n\u2022 Working with Business users, Analysts, Team Lead and Functional Analysts to identify process improving opportunities in data analytics and data modeling.\n\u2022 Implementing new advancements and enhancements to translate data into visualizations.\n\u2022 Designing the mock ups, KPI model designs, Financial planning in support for PS operations, initiatives and strategies in new opportunities available.\n\u2022 Experience working as site administration and server administration of the Tableau server.\n\u2022 Experience working with Business Objects tool and also optimizing the performance of Business Objects.\n\u2022 Extensive knowledge working on Business Objects universe layer configuration parameters and design aspects loops, contexts, aliases and cardinality.\n\u2022 Data warehousing environment such as facts, dimensions, star schema, snowflake schema SCD etc.\n\u2022 Through understanding of logical and physical data modeling and business optimizing methods for business objects against physical model.\n\u2022 Involved in the SLA's licensing management with the Tableau support.\n\u2022 Created views in Tableau Desktop that were published to internal team for review and further data analysis and customization using filters and actions.\n\u2022 Preparing Dashboards using calculations, parameters, calculated fields, groups, sets and hierarchies in Tableau.\n\u2022 Reviewing the designed dashboards with the internal team and managers\n\u2022 Developed Metadata Models in Tableau and dashboards.\n\nEnvironment: Tableau 9,10., Oracle, Teradata, MS SQL Server, SAS, Javascript API, Tableau Server."", u""Data Analyst\nSolar Turbines - San Diego, CA\nAugust 2014 to February 2016\nResponsibilities:\n\u2022 Involved in reviewing business requirements and analyzing data sources form Excel/Oracle SQL Server for design, development, testing, and production rollover of reporting and analysis projects within Tableau Desktop..\n\u2022 Converted charts into Crosstabs for further underlying data analysis in MS Excel.\n\u2022 Blended data from multiple databases into one report by selecting primary key's from each database for data validation.\n\u2022 Combined views and reports into interactive dashboards in Tableau Desktop that were presented to Business Users, Program Managers, and End Users.\n\u2022 Scheduled data refresh on Tableau Server for weekly and monthly increments based on business change to ensure that the views and dashboards were displaying the changed data accurately.\n\u2022 Participated in meetings, reviews, and user group discussions as well as communicating with stakeholders and business groups.\n\u2022 Adjust views/dashboards after initial POC presentation.\n\nEnvironment: Tableau V9, (Desktop, Server, Reader), Oracle, MS SQL Server, MSTR, Cognos."", u'Data Analyst\nSkyward Inc - Bloomington, IL\nSeptember 2012 to July 2014\nResponsibilities\n\u2022 Involved in reviewing business requirements and analyzing data sources form Excel/MS SQL Server for design, development, testing of reporting and analysis projects within Tableau Desktop.\n\u2022 Created views in Tableau Desktop that were published to internal team for review and further data analysis and customization using filters and actions.\n\u2022 Preparing Dashboards using calculations, parameters, calculated fields, groups, sets and hierarchies in Tableau.\n\u2022 Developed Metadata Models in Tableau and dashboards.\n\u2022 Found efficient ways to make tables and graphs which were visually easy to understand and at the same time maintaining the accuracy of the core information content.\n\nEnvironment: Tableau 8/9, MS SQL Server 2012, Oracle, Teradata, Java, MS Excel, Windows Server 2012.', u'Cognos Developer\nThoughtwave Software & Solutions - Aurora, IL\nOctober 2010 to June 2012\nResponsibilities:\n\u2022 Involved in Creation of Cognos 8 Reports, Dashboard Reports.\n\u2022 Created Complex Reports using Report studio, Plugged Prompts, Drill through Master- Detail Reports.\n\u2022 Built Framework Manager Models and published the various packages to the Report net server.\n\u2022 Created DMR data models and Published Packages using Framework Manager according to needs.\n\u2022 Created Cognos Cubes and use it in Framework Packages.\n\u2022 Experienced with trouble shooting and optimizing Report performance.\n\u2022 Customized data by adding filters at both the Framework level and Report level.\n\u2022 Created organized and formatted reports, folders and other contents on Cognos Connection Portal.\n\u2022 Involved in part of ETL Process for loading the data and by designing catalogs and supporting ETL Process\n\u2022 Customized data by adding Filters Calculations, Prompts, Summaries and Functions.\n\u2022 Applied three levels of security to the model (Data, Object and Package).\n\u2022 Applying Row and Object level security to Reports.\n\u2022 Performed Admin task for user permissions, scheduling the reports.\n\u2022 Performed Report Administrator to give set of permissions to the end user', u'Oracle developer\nBentley Systems, Inc - Lisle, IL\nAugust 2008 to September 2010\nResponsibilities:\n\u2022 Developed database programs, reports, database structures and forms using SQL 9i, PL/SQL 9i, oracle Forms 6i, oracle Reports 6i, Unix, Toad, SQL*Plus, Microsoft SQL 5.0, Microsoft T-SQL 5.0, SQL Server Management Studio, SQLCMD and Pro*C.\n\u2022 Development performed for the Banner Customer Information System and the Interactive Voice Response (IVR) System for company clients.\n\u2022 Met project deadlines and responsibilities on time.\n\u2022 Initiated new technical solutions for process improvement and automation.\n\u2022 Performed the full application lifecycle including requirements analysis, documentation, database modeling, design, application development, testing and maintenance for the existing Banner Customer Information System.\n\u2022 Researched and evaluated new, efficient solutions for meeting client needs.']","[u""Master's in Computer Information Systems""]",[u'California university of Management & Sciences\nJanuary 2015']
0,https://resumes.indeed.com/resume/a83cb6d873d48588,"[u'Clinical Analyst\nHealthcare Informatics Systems\nNovember 2016 to Present\nProvide value-added analysis of population healthcare data. Demonstrate business and data understanding; provide advanced comprehensive reporting of healthcare data, statistical analysis, including predictive analytics, data manipulation, interpretation, presentation, and recommendations on specific courses of action. Responsible for research and analysis of clinical and financial data to identify opportunities and improve medical policies within Healthcare Services and Account Management departments.\n\u2022 Create and maintain detailed monthly performance reporting for Healthcare Services and Account Management departments to identify potential cost drivers and negative trends at the service and/or pharmacy level and make appropriate recommendations to senior management to support all financial and operational objectives\n\u2022 Envision, develop and create ad hoc financial analysis/ reporting as requested or to further support recommendations.\n\u2022 Use insight and business aptitude to bridge finance and health analytics and suggest the development of new reports to enhance available information/reporting on an ongoing basis\n\u2022 Identify and reengineer the process to automate reporting solutions to eliminate redundancy - Design and develop utilization management, case and disease management dashboards/models and reports to support on-going operations of Healthcare Services department and to evaluate population health management programs\n\u2022 Provide comprehensive analysis of account/group/provider performance through an in-depth knowledge of benefit design, trend analysis, utilization impacts to trend, and clinical drivers of cost and utilization\n\u2022 Create accurate and meaningful reports and acts in a consultative capacity to articulate complex clinical data or trends in meaningful, understandable presentations or report formats for all internal and external customers', u'Consultant\nHealthcare Informatics Systems\nFebruary 2013 to November 2016\nResponsible for the processes used for analysis of client health care data. In this position, I demonstrated proficiency in the business of health insurance and of data in support of analyses, Also responsible for oversight of the development and implementation of data preparation, statistical analysis, data modeling, interpretation, presentation, and making recommendations on specific courses of action\n\u2022 Created accurate and meaningful reports for employers using company and industry supported reporting tools to provide utilization and outcome analysis, trend and other reports involving claims, utilization and membership data\n\u2022 Demonstrated opportunities to employers to reduce medical costs and how the plan is managing cost and utilization on behalf of the account. Made recommendations on programs to manage cost and utilization\n\u2022 Worked with internal and external customers to set-up and maintain access to multiple reporting tools. Trained on the effective use of the reports and effectively communicate results of reports\n\u2022 Responsible for quality assurance of report production. Verified and analyzed report data and document processes\n\u2022 Researched issues to determine source of discrepancies. Identify, communicate and monitor areas for improvement', u'Data Analyst\nFact Finders, Inc\nJanuary 1990 to January 2013\n\u2022 Development of Reporting System: Programmed IBM SPSS statistical software for data processing, analysis, and reporting. Designed custom-reporting tailored to research audience using Microsoft Office suite of programs. Created and managed reporting to over 200 individual accounts using custom-designed ""report cards."" Provided design templates for client. Proficient in Microsoft Word, Excel, Access and PowerPoint. \u2022\n\u2022 Budget Management: Co-authored research budget for multimillion dollar contract. Developed efficiencies in the face of budget cuts to the research program. Redesigned budget to allow client to hit performance guarantees on contracts requiring research. Consulted on alternative project designs to fit client budgetary requirements. \u2022\n\u2022 Project Management: Collaborated with clients with agency regulation to produce research to meet requirements. Briefed, updated, and responded to individual client requests in a timely manner. Maintained client documentation and history. Negotiated and developed vendor and subcontractor contracts. Developed and maintained targets to achieve client timelines. Assisted company in developing and instituting Federal electroniccompliance procedures. \u2022\n\u2022 Design, Implementation, and Maintenance of Internal Network: Worked with contract vendor to design employer\'s network and install wiring for building. Supervised equipment purchase and installation. Managed the conversion from paper-based data collection to CATI software. Developed custom programs to perform backup process and redundancy. Managed the office upgrade from DOS to Windows. Managed Y2K compliance. Manage the office network hardware and software on minimal budget. \u2022\n\u2022 Personnel Trainer: Sawtooth Inc. Ci3 authoring software, Sawtooth Technologies CATI (computer-aided telephone interviewing) software for sample management, conversion from DOS to Windows, IBM SPSS statistical software for data processing and analysis, MS Office Suite (Word, Excel, PowerPoint, Access, Publisher).']","[u""Master's Degree in Cultural Studies"", u'in Sociology', u""Bachelor's degree in Sociology""]","[u'State University of New York Empire State College New York, NY\nJanuary 2009 to January 2014', u'SUNY Albany\nJanuary 1982 to January 1983', u'Niagara University\nJanuary 1977 to January 1981']"
0,https://resumes.indeed.com/resume/3ae1ebb3ea0f0839,"[u'Data Analyst\nSavvysherpa, Inc - Minneapolis, MN\nDecember 2013 to Present\nSavvysherpa is a Data and Research company that improves the cost- effectiveness of healthcare. We\nmostly work with companies--\n\u2022 To develop and test minimally-viable health-care products and programs\n\u2022 Create distribution and business models for new health-care products\n\u2022 Develop new risk-sharing models for payers, providers, and commercial organizations\n\nRoles and Responsibilities:\n\u25cf Collaborate with research teams to provide data and analysis for a wide variety of projects. Prepare\ndatasets supporting development of data products and advanced data analysis.\n\u25cf Prepare executive level reports, spreadsheets presentation for management and clients using\nTableau, MS Excel, R\n\u25cf Predictive modeling and exploring of data using R\n\u25cf Univariate/multivariate methods (regression, general linear models, logistic regression, LASSO),\ncategorical analysis, descriptive statistics\n\u25cf Provided multiple demonstrations of Tableau functionalities and efficient data visualizations\napproaches using Tableau to the senior management at the client as part of the business\nrequirement\n\u25cf Involved in data validation of the results in Tableau by validating the numbers against the data in the database tables by querying on the database.\n\u25cf Generated dashboards with quick filters, parameters and sets to handle views more efficiently.\n\u25cf Access and transform massive datasets through filtering, grouping, aggregation, and statistical\ncalculation.\n\u25cf Consult with clients and internal stakeholders to gather requirements and plan milestones in the research, development, and implementation phases of the project lifecycle.\n\u25cf Write SQL queries, procedures and scripts using SQL Server Management Studio\n\u25cf Support execution of prospective experimentation by providing insights to pilots\n\u25cf Quickly understand unfamiliar data by formulating and then testing data structure hypotheses based\non a sound understanding of the domain\n\u25cf Exhaustive experience working with health care-related data, including a sound understanding of medical claims/ EMR data and associated data processes\n\u25cf Exhaustive exposure on Google cloud platform and work on BigQuery, Datalab\n\u25cf Mentor junior analysts on the tools and techniques to support and extend research using unfamiliar\ndatasets\n\u25cf Work efficiently with management, researchers and writers at different stages of a project.\n\u25cf Communicate on daily basis with data team in Cebu, Philippines.\n\n1']","[u'Master of Science in Operations Research in Industrial & Systems Engineering', u'Master of Science in Computer Science in Computer Science', u'Bachelor of Technology in Computer Science and Engineering in Computer Science']","[u'SUNY Buffalo', u'University of Georgia Athens, GA', u'West Bengal University of Technology Kolkata, India']"
0,https://resumes.indeed.com/resume/b9feff7a5dbf8dc5,"[u'Tester / Data Analyst\nPioneer Smiles - Irving, TX\nJanuary 2018 to Present\nWylie Smiles, Minty Smiles, Budget Smiles, Pioneer Dental, Garland Dental and others are group of dental clinics under the Pioneer Smiles Umbrella. Mostly dealing with dental claims data for Medicaid, chip or commercial patients.\n\n\u2022 Created report in SQL for accuracy of correctly billed codes (ICD10 / CPT)\n\u2022 Analyzed data and created report for trends in patient spending.\n\u2022 Tested and Created report for missing notes data by dentist.\n\u2022 Reviewed and Tested Claims data for inconsistency before submission to Medicaid & Chip\n\u2022 Worked on various T-SQL statements\n\u2022 Tested for accuracy of financial data submitted to various Insurance for claims.', u'QA Testing Analyst\nAssurant Solutions - Lewisville, TX\nMay 2016 to January 2017\nA Fortune 500 company, Assurant focuses on the housing and lifestyle markets, and is among the market leaders in mobile device protection; extended service contracts; vehicle protection; pre-funded funeral insurance; renters insurance; lender-placed homeowners insurance; and mortgage valuation and field services\n\n\u2022 Reviewed and analyzed Business Requirement, System Requirements, and design documents to gain understanding of POS implementation.\n\u2022 Creation of Performance scenarios with HP ALM Performance Center or with HP LoadRunner Controller\n\u2022 Participated in meetings with BA and Developers to understand the scope of automated testing\n\u2022 Test Scenarios, Test Cases, and prepared data for assigned modules\nCreated and Executed Test Cases manually as well as verify the actual results based on expected results.\n\u2022 Performed Functional Testing, Regression Testing, User Acceptance Testing (UAT) and End to End Testing\n\u2022 Documented and reported all found defects in Quality Center\n\u2022 Track bug items\n\u2022 Test application as per requirements and stress testing.\n\u2022 Utilized Loadrunner for performance testing for the customer enrollment system.\n\u2022 Identify the functional scenarios to do performance testing.\n\u2022 Build the environment for performance testing.', u'Data Analyst\nNobel Medical College\nDecember 2008 to June 2010\nNoble Medical Teaching Hospital - Biratnagar Nepal\nNobel Medical College, recognized by the Nepal Medical Council (NMC), Ministry of Education, Nepal and Ministry of Health, Nepal, has constantly maintained its academic standards as per the requirements of other Medical councils and Universities across the globe. The hospital currently has 911 beds of its own at full function.\nResponsibilities:\n\u2022 Assists faculty and staff with queries, statistical analyses, reports and technical difficulties related to data retrieval.\n\u2022 Documented functionality of applications for future reference.\n\u2022 Checked the accuracy of diagnosis and procedural codes.\n\u2022 Developed different statistical analysis based on top diagnosis, top procedure codes\n\u2022 Developed statistical analysis based demographics and usage.\n\u2022 Gathered and documented Business User Requirement and translated into design.\n\u2022 Created workflow and design document to simplify the development process.\n\u2022 Wrote SQL queries for reports.\n\u2022 Tested application as per requirements and stress testing.']","[u'Certification', u'Bachelor in Management in Complete', u'Masters of Business Management in Business Management']","[u'Portland Community College\nDecember 2016', u'Sri Sathya Sai University Anantapur Campus Anantapur, Andhra Pradesh\nNovember 2008', u'Apex College']"
0,https://resumes.indeed.com/resume/96e86f603603798f,"[u'Data Scientist\nKELLOGG - Elmhurst, IL\nJuly 2017 to Present\nDescription:The Kellogg Company is an American multinational food manufacturing company headquartered in Battle Creek, Michigan, United States.\n\nResponsibilities:\n\u27a2 Developed MapReduce/Spark Python modules for predictive analytics & machine learning in Hadoop on AWS.\n\u27a2 Application of various machine learning algorithms and statistical modeling like decision trees, text analytics, natural language processing (NLP), supervised and unsupervised, regression models, social network analysis, neural networks, deep learning, SVM, clustering to identify Volume using scikit-learn.\n\u27a2 package in python, MATLAB.\n\u27a2 Experienced in working with NLP frameworks such as Apache NLP, Stanford Core NLP for \u27a2 processing natural language text.\n\u27a2 Implemented a Python-based distributed random forest via Python streaming.\n\u27a2 Built models using Statistical techniques like Bayesian HMM and Machine Learning classification models like SVM, XG, SVM, Boost and Random Forest.\n\u27a2 Worked with data governance team to maintain data models, data compliance teams, Metadata, data Dictionaries, define source fields and its definitions.\n\u27a2 Data analysis tools and Setup storage in Amazon Web Services cloud computing infrastructure.\n\u27a2 A highly immersive Data Science program involving Visualization & Data Manipulation, Web Scraping, Machine Learning, Python programming, SQL, Unix Commands, API Git, MongoDB, NoSQL, Hadoop.\n\u27a2 Transformed Logical Data Model to Erwin, Physical Data Model Foreign Key relationships in PDM and ensuring the Primary Key, Consistency of definitions of Data Attributes and Primary Index Considerations.\n\u27a2 Developed Oracle11g stored packages, functions, procedures and database triggers using PL/SQL for ETL process, data handling, logging, archiving and to perform Oracle back-end validations for batch processes.\n\u27a2 Documented logical, physical, relational and dimensional data models.\n\u27a2 Designed the Data Marts in dimensional data modeling using star and snowflake schemas.\n\u27a2 Worked as Data Architects and IT Architects to understand the movement of data and its storage and ER Studio 9.x\n\u27a2 Coordinate with the UNIX team and installed TIDAL job scheduler on QA and Production Netezza environment.\n\u27a2 Handled importing data from various data sources, performed transformations using MapReduce, Hive and loaded data into HDFS.\n\u27a2 Worked with BTEQ to submit SQL statements, generate reports in Teradata and import, export data\n\u27a2 Designed and documented Use Cases, Activity Diagrams, Sequence Diagrams, OOD (Object Oriented Design) using UML and Visio.\n\u27a2 Created Hive queries that helped analysts spot emerging trends by comparing fresh data with historical metrics and EDW reference tables and processed the data using HQL (like SQL) on top of Map-reduce.\n\u27a2 Hand on development and maintenance using Oracle SQL, SQL Loader, PL/SQL and Informatica Power Center9.1.\n\u27a2 Designed the ETL process to Extract translates and load data from OLTP Oracle database system to Teradata data warehouse.\n\u27a2 Created tables, sequences, synonyms, joins, functions and operators in Netezza database.\n\u27a2 Created and implemented MDM data model for Consumer/Provider for HealthCare MDM product from Variant.\n\u27a2 Hands on Oracle External Tables feature to read the data from flat files into Oracle staging tables.\n\u27a2 Analyzed the web log data using the HiveQL to extract number of unique visitors per day, visit duration, page views, and most purchased product on website and managed and reviewed Hadoop log files.\n\u27a2 Created SSIS Packages using Pivot Transformation, Execute SQL Task, Data Flow Task, etc to import data into the data warehouse.\n\u27a2 Developed and implemented SSRS, SPSS, SAS, SSIS and SSAS application solutions for various business units across the organization.\n\nEnvironment: Hadoop, HDFS, Pig, Hive, MapReduce, Erwin r 9.x, Teradata, Oracle11g, PL/SQL, UNIX, Informatica Power Center, MDM, SQL Server, Netezza, DB2, Tableau, Aginity, Architecture, SAS/Graph, SAS/SQL, Python, Tableau, SAS/Connect and SAS/Access.', u""Data Scientist\nAmazon, CA\nApril 2016 to June 2017\nDescription: Amazonis an American electronic commerce and cloud computing company based in Seattle, Washington that was founded by Jeff Bezos on July 5, 1994.\n\nResponsibilities:\n\u27a2 Involved in development of data warehouse environment and design, liaison to business users and technical teams gathering requirement specification documents and presenting and identifying data sources, targets and report generation.\n\u27a2 Designed an Industry standard data Model specific to the company with group insurance offerings, Translated the business requirements into detailed production level using Workflow Diagrams, Activity Diagrams, Sequence Diagrams and Use Case Modelling\n\u27a2 Conceptualized the most-used product module (Research Center) after building a business case for approval, gathering requirements and designing the User Interface\n\u27a2 Successfully managed projects using Agile development methodology\n\u27a2 Developed predictive models using Decision Tree, Random Forest, Vector Machines and Naive Bayes and collaborating with marketing and dev-ops teams for production deployment.\n\u27a2 A team member of analytical Group and assisted in designing and development of statistical models for the end clients.\n\u27a2 Handle with end users for designing and implementation of e-commerce analytics solutions as per project proposals.\n\u27a2 Conducted market research for client; designed sampling methodologies and developed, and analyzed the survey data for pricing and availability of clients' products. Investigated product feasibility by performing analyses that include market sizing, competitive analysis and positioning.\n\u27a2 Successfully optimized codes in Python to solve a variety of purposes in data mining and machine learning in Python.\n\u27a2 Created Data QualityScripts using SQL and Hive to validate successful data load and quality of the data. Created various types of data visualizations using Python and Tableau.\n\u27a2 Facilitated sprint reviews to drive project completion and stakeholder meetings.\n\u27a2 Project experience in Data mining, segmentation analysis, business forecasting and association rule mining using Large Data Sets with Machine Learning.\n\u27a2 Automated Diagnosis of Blood Loss during Accidents and Applied Machine Learning algorithms to diagnose blood loss from vital signs (HF, GSR, and ECG).\n\u27a2 Trained organization-wide employees for Financial domain certification & Business Analysis Certification exams\n\u27a2 Prepared graphs and reports using GGplot2 library for an overview of the analytical models and results.\n\u27a2 Developed Shiny and R application showcasing machine learning for improving business forecasting.\n\nEnvironment: R, SQL Server 2012/2014, SQL, Oracle 10g, Windows XP/NT/2000, DB2, MS Excel, Mainframes MS Visio, Rational Rose, Requisite Pro, Crystal Reports 9, Python, R Studio, Shiny, Excel 2013."", u'Data Scientist\nFermilab - Batavia, IL\nDecember 2014 to March 2016\nDescription: Fermi National Accelerator Laboratory (Fermilab), located just outside Batavia, Illinois, near Chicago, is a United States Department of Energy national laboratory specializing in high-energy particle physics.\n\nResponsibilities:\n\u27a2 Provided the architectural leadership in shaping strategic, business technology projects, with an emphasis on application architecture.\n\u27a2 Utilized domain knowledge and application portfolio knowledge to play a key role in defining the future state of large, business technology programs.\n\u27a2 Participated in all phases of data mining, data collection, data cleaning, developing models, validation, and visualization and performed Gap analysis.\n\u27a2 Performed Source System Analysis, database design, data modeling for the warehouse layer using MLDM concepts and package layer using Dimensional modeling.\n\u27a2 Used Pandas, SciPy, NumPy, Matplotlib, Scikit-learn, seaborn, NLTK in Python for developing various machine learning algorithms and utilized machine learning algorithms such as linear regression, multivariate regression, naive Bayes, Random Forests, K-means&KNN for data analysis.\n\u27a2 Demonstrated experience in design and implementation of Statistical models, Predictive models, enterprise data model, metadata solution and data life cycle management in both RDBMS, Big Data environments.\n\u27a2 Designed and implemented system architecture for Amazon EC2 based cloud-hosted solution for client.\n\u27a2 Tested Complex ETL Mappings and Sessions based on business user requirements and business rules to load data from source flat files and RDBMS tables to target tables.\n\u27a2 Analyzed large data sets apply machine learning techniques and develop predictive models, statistical models and developing and enhancing statistical models by leveraging best-in-class modeling techniques.\n\u27a2 Worked on customer segmentation using an unsupervised learning technique - clustering.\n\u27a2 Worked with various Teradata15 tools and utilities like Teradata Viewpoint, ARC, MultiLoad, Teradata Administrator, BTEQ and other Teradata Utilities.\n\u27a2 Utilized Spark, Scala, Hadoop, HBase, Matlab, MLlib, Spark Streaming, Python, Kafka a broad variety of machine learning methods including classifications, regressions, dimensionally reduction etc.\n\nEnvironment: Java, Hadoop, Hive, Teradata, random forest, OLAP, Azure, MariaDB, SSRS, Tableau, MLlib, regression, Cluster analysis, Scala NLP, Spark, Kafka, MongoDB, logistic regression, SAP CRM, HDFS, ODS, NLTK, SVM, JSON, Tableau, XML, Cassandra, MapReduce, AWS.', u""Data Scientist/ Data Analyst\nTrust Company\nApril 2013 to November 2014\nDescription: The TCW Group is an asset management firm with a broad range of products across fixed income, equities, emerging markets and alternative investments. With nearly five decades of investment experience, TCW today manages over $194 billion in client assets.\n\nResponsibilities:\n\u27a2 Responsible for technical data governance, enterprise wide data modeling and database design.\n\u27a2 Used Model Mart of ERwin for effective model management of sharing, dividing and reusing model information and design for productivity improvement.\n\u27a2 Conducted detailed and comprehensive Business Analysis by working with the IT staff, SME's, Business Staff, and other stakeholders to identify the system, operational requirements and process improvements.\n\u27a2 Designed an Industry standard data Model specific to the company with group insurance offerings, Translated the business requirements into detailed production level using Workflow Diagrams, Activity Diagrams, Sequence Diagramsand Use Case Modeling\n\u27a2 Involved in design and development of data warehouse environment, liaison to business users and technical teams gathering requirement specification documents and presenting and identifying data sources, targets and report generation.\n\u27a2 Worked with Development DBA to assist and support developers with SQL performance tuning, query tuning and code reviews.\n\u27a2 Developed Python programs for manipulating the data reading from various Teradata, update the Content in the database tables.\n\u27a2 Responsible for evaluating various RDBMS like OLTP modeling, documentation, and metadata reporting tools including Erwin, developed logical/ physical data models using Erwin tool across the subject areas based on the specifications and established referential integrity of the system.\n\u27a2 Extensively worked on Source to Target mapping for business need and documentation purpose.\n\u27a2 Created snapshots, views, and database indexes for improving the query performance.\n\u27a2 Created and maintained Logical Data Model (LDM) / Physical Data Modeling for the insurance system.\n\u27a2 Created dimensional model for the reporting system by identifying required dimensions and facts using Erwinr8.\n\u27a2 Perform updating data by weekly and and maintain and monthly, manipulating the data for database management. Used the SAS/MACROS for the monthly production.\n\u27a2 Worked with Comparison between Data Model Vs Database and generate difference Report.\n\nEnvironment: Windows XP/NT/2000, Erwin r8, Informatica, SQL Server 2005/2008, SQL, Oracle8i/10g, DB2, MS Excel, Mainframes MS Visio, Rational Rose, Requisite Pro."", u'Data Analyst\nDatawise - Hyderabad, Telangana\nNovember 2011 to March 2013\nDescription: The DATAWISE Group is a leading Research and Analytics services Company that leverages data to derive meaningful insights that help its global customers to make better decisions.\n\nResponsibilities:\n\u27a2 Developed ETL processes for data conversions and construction of data warehouse using IBM InfoSphere DataStage.\n\u27a2 Responsible for writing stored procedures, triggers etc., in MS SQL Server.\n\u27a2 participated in group technology reviews to critique work of self and others.\n\u27a2 Used Star Schema and designed Mappings between sources to operational staging targets.\n\u27a2 Provided On call Support for the project and manage knowledge transfer for the clients.\n\u27a2 Used Rational Application Developer (RAD) for version control.\n\u27a2 Handle performancetuning.\n\u27a2 Reused python scripts to fetch data from sandra\n\u27a2 Analyzing Problems in the system and making recommendations for system improvement.\n\u27a2 Developed custom reporting solutions using SQL Server.\n\u27a2 Developed transformations using jobs such as Filter, Aggregator, Lookup, Join, Transformer and Dataset.\n\u27a2 Develop components of database, data queries, data storage, data schema, data transformations, and data warehousing applications.\nEnvironment: Microsoft Excel Macros, Pivot Tables, vlookups, Match/Index, Pivot Tables and other advanced functions to leverage raw data', u'Data Analyst\nKarvy Analytics - Hyderabad, Telangana\nApril 2009 to October 2011\nDescription: Karvy Analytics Limited is a new age company and a modern arm of the leading Karvy Conglomerate. Led by visionary management, the young and forward thinking team is building world class solutions for the global analytics universe.\n\nResponsibilities:\n\u27a2 Worked with internal architects, assisting in the development of current and target state data architectures.\n\u27a2 Worked with project team representatives to ensure that logical and physical ER/Studio data models were developed in line with corporate standards and guidelines.\n\u27a2 Involved in defining the business/transformation rules applied for sales and service data.\n\u27a2 Implementation of Metadata Repository, Transformations, Maintaining Data Quality, Data Standards, Data Governance program, Scripts, Stored Procedures, triggers and execution of test plans\n\u27a2 Define the list codes and code conversions between the source systems and the data mart.\n\u27a2 Involved in defining the source to business rules, target data mappings, data definitions.\n\u27a2 Responsible for defining the key identifiers for each mapping/interface.\n\u27a2 Responsible for defining the functional requirement documents for each source to target interface.\n\u27a2 Responsible for defining the key identifiers for each mapping/interface.\n\u27a2 Performed data quality in Talend Open Studio.\n\u27a2 Enterprise Metadata Library with any changes or updates.\n\u27a2 Document data quality and traceability documents for each source interface.\n\u27a2 Establish standards of procedures.\n\u27a2 Coordinated meetings with vendors to define requirements and system interaction agreement documentation between client and vendor system.\n\nEnvironment: Windows Enterprise Server 2000, SSRS, SSIS, Crystal Reports, DTS, SQL Profiler, and Query Analyze.']","[u""Bachelor's""]",[u'']
0,https://resumes.indeed.com/resume/3af0a44e6c65b11c,"[u""Data Analyst\nThomson Reuters - Hanoi, VN\nMarch 2015 to July 2017\nManaged and maintained Thomson Reuters information database on Vietnam's Fundamentals using internal software\n\u2022 Identified and resolved data discrepancies in the database in order to answer queries from clients\n\u2022 Ensured timely and efficient processing of all financial data information\n\u2022 In charge of TRF Row Cleanse project - the most important project to date which involves clearing\nhistorical errors in the database""]","[u'Master of Science in Finance', u'Bachelor of Arts in International Business Economics']","[u'Syracuse University Syracuse, NY\nDecember 2018', u'Foreign Trade University Hanoi, VN\nJuly 2014']"
0,https://resumes.indeed.com/resume/953f8cbcb1114e19,"[u'Data Analyst\nHawes Financial - Vancouver, WA\nApril 2015 to Present\nDuties:\n- Develop meaningful reporting using data visualization tools for internal (Operations) and external (Client Side) use.\n- Extract information from our local database using the SQL language.\n- Create daily ad hoc SQL queries on a request basis for various departments.\n- Develop scheduled SQL procedures to facilitate the movement of consumer accounts to various departments.\n- Use the SQL language to create new tables and compile data sets.\n- Auditing internal processes using the SQL language when they ""break"", present solutions to developers.\n- Analyze and identify meaningful patterns in our data (regression analysis), present findings.\n- Data Importing (EDI, Vendor Information).\n- Self Starter, personal time management a must.']",[u'Bachelor of Science in Economics (Minor: Business Administration)'],"[u'University of Oregon Eugene, OR\nDecember 2014']"
0,https://resumes.indeed.com/resume/f82bef78b3fdccaa,"[u""Sr. Data Analyst\nPacific Western Bank - Glendale, CA\nMay 2016 to Present\nResponsibilities:\n\u2022 Experienced in data extraction, transforming and loading (ETL) using SQL Server Integration Services (SSIS), SQL Server Reporting Services (SSRS) & SQL Server Analysis Services (SSAS).\n\u2022 Worked as Data Analyst to turn data into information, information into insight and insight into business decision.\n\u2022 Improved visibility regarding trends within data by authoring and presenting executive dashboards and scoreboards using Excel, V-Lookup, and VBA-Macros.\n\u2022 Created dashboards, reports, visualizations and analytics using Qlik view followed by ETL to integrate different data subsets from different databases SQL server, MySQL, Postgres and Oracle.\n\u2022 Carried Data visualization and analysis on Data sources provided using Qlik view and accessed reports generated through Qlik sense cloud.\n\u2022 Created ETL scripts under Vertica and Oracle database platform, cleared dataset anomalies using SSIS and SSAS.\n\u2022 Link business processes to organizational objectives, perform critical path analysis, and identify opportunities for business process improvement.\n\u2022 Create Mapping documents, ETL technical specifications and various documents related to data migration.\n\u2022 Data Analysis of sources being used to create production environment reporting systems to determine if coding changes or data source changes would make difference in output as well and/or runtime.\n\u2022 Used knowledge of various businesses units to assist with the management of resources and how projects development using SAS and SQL were implemented.\n\u2022 Responsible for production environment changes to the commission system as per requests from stakeholders.\n\u2022 Developed complex SQL Queries to validate the data in the Cognos Custom reports against Safety Database.\n\u2022 Established a Data Quality program for Product Information Management ensuring maintenance data quality over time for the client's three major products: Insurance, Credit and savings.\n\u2022 Analyzed the existing legacy systems, data lineage, data transformations rules and documented Business and Functional requirements for implementing.\n\u2022 Gathering Requirements and design of technical specification documents. Document the business requirements for the Cognos BI reports. Derive Cognos report requirements from already existing report templates.\n\u2022 Used SAS to mine, alter, manage and retrieve data from a variety of sources and perform statistical analysis.\n\u2022 Data governance and defining processes concerning how data is stored, archived, backed up, and protected from mishaps, theft or attack.\n\u2022 Used Data warehousing for Data Profiling to examine the data available in an existing database and created Data Mart.\n\u2022 Developed and tested PL/SQL scripts and stored procedures designed and written to find specific data.\n\u2022 Written PL/SQL Stored Procedures and Functions for Stored Procedure Transformation in Informatica.\n\u2022 Implemented PL/SQL scripts in accordance with the necessary Business rules and procedures.\n\u2022 Generated SQL and PL/SQL scripts to create and drop database objects including: Tables, Views, and Primary keys, Indexes, Constraints, Packages, Sequences and Synonyms.\n\u2022 Conducted or participated in requirement gathering workshops and design sessions necessary to capture the business needs and develop models.\n\u2022 Interfaced with business and technology stakeholders to gather, analyze, and document business and data requirements.\n\u2022 Used HP Quality Center for UAT Test Case Management and defect tracking and resolution."", u'Sr. Data Analyst\nKeyCorp - Cleveland, OH\nFebruary 2014 to April 2016\nResponsibilities:\n\u2022 Worked in Data Warehouse and Business Intelligence Projects along with the team of Informatica, Talend (ETL), Cognos 10, Impromptu and Powerplay.\n\u2022 Created ETL solution that reads Product, vendor and order data from .csv, Excel and Text files on a network share into a SQL server database.\n\u2022 Created Data Warehouse by using Dimensional modeling with Star Schema and implemented Ralph Kimball Methodologies for effectively for creation of data marts and data warehouse.\n\u2022 Involved in mapping the data elements from the User Interface to the Database and help identify the gaps.\n\u2022 Created action filters, parameters and calculated sets for preparing dashboards and worksheets in Tableau.\n\u2022 Developed advanced SQL queries with multi-table joins, group functions, subqueries, set\noperations and T-SQL stored procedures, user defined functions (UDFs) for data analysis.\n\u2022 Created SQL reports, data extraction and data loading scripts for different databases and Schemas with also prepared a handbook of standards and Documented standards for Informatica code development.\n\u2022 Involved in Data Migration from MS SQL server, Oracle, My SQL and Progress SQL to Teradata.\n\u2022 Identify business rules for data migration and Perform data administration through data models and metadata.\n\u2022 Conducted Review sessions of Data Models with DBA must best fit physical Model Indexes for better DB performance.\n\u2022 Reviewed Entities and relationships in the engineered model and cleansed unwanted tables/ columns as part of data analysis responsibilities.\n\u2022 Used ERWIN tool to create Data model and performed reverse and forward engineering for performance tuning and restructuring of Data model.\n\u2022 Wrote PL/SQL module to migrate and reconcile data from third parties and Extracted, transformed, and load data from Oracle Warehouses into Postgres and MYSQL servers.\n\u2022 Created and manage Dashboards, Worksheets helping senior Management to have at-a-glance views of KPIs using Tableau and OBIEE Suite.\n\u2022 Developed intricate algorithms based on deep-dive statistical analysis and predictive data models used to deepen relationships, strengthen longevity and personalize interactions with customers.\n\u2022 Developed an interactive web page (HTML/JavaScript) for users to search restaurants, update preference and view recommended restaurants.\n\u2022 Developed a web service using (Java servlet, REST API) to fetch restaurant data from Yelp API.\n\u2022 Designed and developed a filtering and sorting algorithm to match similar restaurants based on categories.\n\u2022 Improved the precision by ordering restaurants based on stars, distance and matched categories.\n\u2022 Developed the data warehouse dimensional models &schema for the proposed central model for the project.\n\u2022 Created Logical/Physical Data models in 3NF in the Warehouse area of Enterprise Data Warehouse.\n\u2022 Created 3NF business area data modeling with de-normalized physical implementation data and information requirements analysis using Erwin tool.\n\u2022 Dashboard development using Excel to visualize big data sets and assist in the analysis of complex, high-volume, high-dimensionality data from varying sources.\n\u2022 Worked directly with Managers and Team Leads to create reports used for financial, expansion, hiring, and other purposes.\n\u2022 Performed statistical analysis and provided detailed reports to the admissions committee for decision-making.', u'Data Analyst\nCapital One - Dallas, TX\nJune 2011 to December 2013\nResponsibilities:\n\u2022 Demonstrable expertise in core IT processes, utilizing ETL tools to query, validate, and analyze data.\n\u2022 Expert business and technical requirements documentation skills employing contemporary tools for data mapping, diagramming, Use Cases, and business rules to produce concise functional specifications.\n\u2022 Created Tableau dashboards/reports for data visualization, Reporting and Analysis and presented it to Business\n\u2022 Developed and standardized the data structures to support the generation of business intelligence and strategy deliverables for executives.\n\u2022 Worked on Dimensional modeling, Data cleansing and Data Staging of operational sources using ETL processes.\n\u2022 Responsibilities include gathering business requirements, developing strategy for data cleansing and data migration, writing functional and technical specifications, creating source to target mapping, designing data profiling and data validation jobs in DataStage, and creating ETL jobs in DataStage.\n\u2022 Prepared test Data sets and performed data testing using the PL/ SQL scripts. Also, used MS excel for data mining, data cleansing, data mapping, data dictionary and data analysis.\n\u2022 Involved in Data Modeling of both Logical Design and Physical design of data warehouse and data marts in Star Schema and Snow Flake Schema methodology.\n\u2022 Collected business requirements to set rules for proper data transfer from Data Source to Data Target in Data Mapping.\n\u2022 Follow and assess the business process model defining metadata rules and critical data elements.\n\u2022 Conduct analysis, gather requirements, develop Use Cases, data mapping, and workflow diagrams.\n\u2022 Develop global incident management reporting dashboard for the DQM over multiple IM platforms.\n\u2022 Investigate unused modules of the DQM and report viability and feasibility for implementation.\n\u2022 Comparative cost/ benefit analysis between DQM modules and Inquiry Framework for DQ assessments.\n\u2022 Utilize multimedia office suite applications and conduct surveys for high level dashboard reporting.\n\u2022 Performing daily integration and ETL tasks by extracting, transforming and loading data to and from different RDBMS.\n\u2022 Creating complex SQL queries and scripts to extract and aggregate data to validate the accuracy of the data.\n\u2022 Business requirement gathering and translating them into clear and concise specifications and queries.\n\u2022 Prepare high level analysis reports with Excel and Tableau. Provides feedback on the quality of data including identification of billing patterns and outliers.\n\u2022 Identify and document limitations in data quality that jeopardize the ability of internal and external data analysts.\n\u2022 Wrote standard SQL Queries to perform data validation and created excel summary reports (Pivot tables and Charts).\n\u2022 Gather analytical data to develop functional requirements using data modeling and ETL tools.\n\u2022 Systems Documentation, change control/defect analysis and updates. Implementation testing.\n\u2022 Gathered data and documenting it for further reference and designed Database using Erwin DATA modeler.\n\u2022 Experienced in logical and Physical Database design & development, Normalization and Data modeling using Erwin.\n\u2022 Extensively used PL/SQL in writing database packages, stored procedures, functions and triggers in Oracle 10g.', u'Data Analyst\nHuntington Bank - Columbus, OH\nJuly 2009 to May 2011\nResponsibilities:\n\u2022 Developed Dashboard Reports using SQL Server Reporting Services (SSRS), Report Model using Report Builder.\n\u2022 Documented the source to target spread sheets which have all the information of source and target fields and all the necessary transformation rules which are in turn used for CCS reports development.\n\u2022 Developed detailed mapping documents that will provide the reporting and data warehouse team with source to target data mapping includes logical names, physical names, data types, corporate meta-data definitions, and translation rules.\n\u2022 Created Tableau dashboards using Side by side Bar graphs with a combination of Line charts on top of Bar graphs, Donut charts, Tree maps, Tabular Charts, Attributes, dill down reports using Hierarchy functionality.\n\u2022 Created Informatica Mappings to load data using transformations like Source Qualifier, Sorter, Aggregator, Expression, Joiner, and Connected and Unconnected lookups, Filters, Sequence, Router and Update Strategy\n\u2022 Participated in project planning and data warehouse modeling to set up an optimal system for quick and accurate data retrieval per business requirements, closely working with client teams.\n\u2022 Involved in the creation of database objects like Tables, Views, Stored Procedures, Functions, Packages, DB triggers, Indexes.\n\u2022 Created various views in Tableau-like Tree-maps, Heat Maps, Scatter plots, Geographic maps, Line chart, Pie charts etc.\n\u2022 Worked on Data warehousing applications using ETL and OLAP tools like Informatica, Cognos with Oracle, SQL Server.\n\u2022 Created logical data model from the conceptual model and its conversion into the physical database design using Erwin.\n\u2022 Used SAS to build models to identify definite patterns and suggest business with possible problems and feasible solutions. Identify requirements for new possible business flows.\n\u2022 Created reports using either Cognos and/or Tableau based client needs for dynamic interactions with the data produced.\n\u2022 Designed, developed and deployed databases using MS Access to assist with inventory, payment tracking and various other data control needs.\n\u2022 Use of Excel and PowerPoint on various projects as needed for presentations or summarization of data to provide insight on key business decisions.\n\u2022 Data consolidation for operational weekly and monthly business reports, effective and efficient solutions to deal with large data sets.\n\u2022 Closely involved in planning and execution of all the testing phases including UAT (onsite), proactively performed thorough reviews of System/UAT test cases and data-sets.\n\u2022 Involved with Tracking and Managing the Requirements using Requirement Traceability Matrix (RTM) that controls numerous artifacts produced by the teams across the deliverables for a project.\n\u2022 Generated periodic reports based on the statistical analysis of the data from various time frame and division using SQL Server Reporting Services (SSRS)\n\u2022 Created Data Base Design of Fact & Dimensions Tables, Conceptual, Physical and Logical Data Models using Erwin tool.\n\u2022 Gathered and documented the Audit trail and traceability of extracted information for data quality.\n\u2022 Worked with the Business Intelligence and ETL developers on the analysis and resolution of data related problem tickets and other defects and assist them to design of efficient processes to load and manage the data, including a Data Quality Assessment to ensure the quality of the source data will meet the information requirements.\n\u2022 Pulled out sales order data and generated ad hoc stock pulling list using advanced excel functions every day.\n\u2022 Created template and optimized the procedures which significantly decreased time spent in daily work.\n\u2022 Performed in-depth data analysis, successfully made more than 4,000 less-sold items competitive in market\n\u2022 Involved in Troubleshooting, resolving and escalating data related issues and validating data to improve data quality.\n\u2022 Business requirement gathering and translating them into clear and concise specifications and queries.\n\u2022 Provides feedback on the quality of data including identification of billing patterns and outliers.\n\u2022 Identify and document limitations in data quality that jeopardize the ability of internal and external data analysts.']",[],[]
0,https://resumes.indeed.com/resume/c6e40dce33fa356c,"[u'QA Test Analyst\nK Link Pos - Dallas, TX\nJanuary 2017 to Present\n\u2022 Preparation of the Test Strategy and guiding the team.\n\u2022 Involvement in the System Test Plan Preparation and Requirements Streamlining.\n\u2022 Involvement in Preparation of Test Procedures, Test Scenarios, Cases and Test Data.\n\u2022 Responsible for GUI and Functional Testing.\n\u2022 Involvement in Test Execution, Results Analyzing and Defect Reporting.\n\u2022 Created Test cases Using Element locators and Selenium Web-driver methods.\n\u2022 Enhanced Test cases using Java programming features and Test-NG Annotations.\n\u2022 Execution of Selenium Test cases and Reporting defects.\n\u2022 Conducting Data driven testing, cross browser testing and parallel test execution.\n\u2022 Involved in Regression Testing using Selenium.\n\u2022 Backend testing with SQL & Restful API that transports data from one part of the modula to another part of the application.\n\u2022 Preparation of weekly and monthly status reports.', u""Data Analyst\nWetanson Group - Manhattan, NY\nFebruary 2015 to November 2016\n\u2022 Prepares source data for entry by opening and sorting mail; verifying and logging receipt of data; obtaining missing data.\n\u2022 Records data by operating data entry equipment; coding information; resolving processing problems.\n\u2022 Protects organization's value by keeping information confidential.\n\u2022 Accomplishes department and organization mission by completing related results as needed.""]",[u'Associate'],[u'Queens college\nJanuary 2016']
0,https://resumes.indeed.com/resume/132a26c62a5cffd4,"[u'Data Management Analyst / Technology Analyst\nINFOSYS LIMITED\nJuly 2011 to Present\nOver 6.5 years of experience in the IT industry including 3 plus years at onshore (USA) with abundant experience in areas of Data Quality, Data Management, Data Warehousing & Business Intelligence projects.\n-Experience as a Data Management Analyst working on Data Management tools ensuring data quality and adherence to banking standards\n-Knowledge of Enterprise Data Management and standards\n-Maintenance and testing of data lineage and controls to comply with bank regulatory standards\n-Documentation and review of data lineage and controls\n-Data loading and maintenance of lineage/controls information using in house data management tools\n-Involved in data testing activities as part of Enterprise audit exercises\n-Involved in remediation efforts and risk assessment for potential data gaps, data quality and data standard risks\n-Experience in collaborating with SMEs and key stakeholders to ensure adherence to policies that ensure compliance with enterprise data standards enabling business and regulatory use cases\n-Documentation and review of lineage/controls for Volcker, MiFID, Liquidity, GFCC and other data flows.\n-Creation of High level/Low Level specifications, source-to-target mapping design documents\n-Exposure to COLLIBRA - data governance platform. Involved in evaluating Collibra tool based on business requirements\n-As a Technical Developer, extensive hands-on experience and good knowledge with ETL tool Informatica Power Center (8.x, 9.x)\n-Good knowledge of Informatica transformations, mappings, SCD types and optimization methods\n-Worked on projects with Teradata, Oracle database\n-Experienced in creating and debugging SQL queries\n-Experience working with UNIX shell scripts\n-Experience in working with the scheduling tool Autosys\n-Excellent verbal and written communication skills\n-Communicate and partner with associates and stakeholders effectively\n-Experienced in leading a large team from both offshore and onshore\n-Experienced to work and deliver challenging tasks in tight schedule.\n-Strong knowledge in all phases of SDLC project like Requirements Gathering, Estimation, Planning, Analysis, Design, Development, Testing, and Project Coordination, Leading team (From both onsite and offsite), Delivery and postproduction warranty support.']",[u'Bachelor of Engineering in Information Technology'],[u'Karpagam College of Engineering (Anna University)']
0,https://resumes.indeed.com/resume/ff5a816adf8cabde,"[u'Data Scientist\nAviall - Dallas, TX\nJanuary 2017 to Present\nResponsibilities:\n\u2022 Played key role in optimizing and benchmarking the Classification models in order to standardize the results across different departments.\n\u2022 Applied advanced statistical and predictive modeling techniques to build, maintain, and improve on multiple real-time decision systems. Conducted advanced data analysis and developed complex algorithms.\n\u2022 Built models using Statistical techniques and Machine Learning classification models like XG Boost, SVM, and Random Forest. Developed and design advanced predictive analysis models. Model and frame business scenarios that are meaningful and impact critical business processes and/or decisions.\n\u2022 Worked with Big Data Technologies such Hadoop, Hive, MapReduce. Extracted data from HDFS and prepared data for exploratory analysis using data munging. Designed experiments, tested hypothesis, and built models.\n\u2022 Developed MapReduce/Spark Python modules for machine learning & predictive analytics in Hadoop on AWS. Implemented a Python-based distributed random forest via Python streaming.\n\u2022 Worked extensively with AWS services like EC2, S3, VPC, ELB, AutoScalingGroups, Route 53, IAM, CloudTrail, CloudWatch, CloudFormation, CloudFront, SNS, and RDS.\n\u2022 Wrote Ansible Playbooks with Python SSH as the Wrapper to Manage Configurations of AWS nodes and Tested Playbooks on AWS instances using Python.\n\u2022 Perform data/systems analysis to determine best BI solution (reports, dashboards, scorecards, data cubes, etc) using Tableau.\n\u2022 Develop load scripts for extracting, transforming and loading data into Tableau applications.\n\u2022 Design and develop new interface elements and objects as required, developed Macros, SET ANALYSIS to provide custom functionality using Tableau\n\u2022 Wrote scripts in Python using Apache Spark and ElasticSearch engine for use in creating dashboards\n\u2022 Developed and presented clear concise recommendations outlining alternatives and key decision criteria. Prepared graphs using GGplot library and Tableau for an overview of the analytical models and results.\n\u2022 Handled importing data from various data sources, performed transformations using Hive, Map Reduce, and loaded data into HDFS.\nEnvironment: AWS (S3, EC2), Python (Scikit Learn), Tableau, Tensorflow, Linux Systems(Ubantu), Hive, MongoDB, SQL, Apache Spark, Apache Hadoop. Major Model tested: Neural Networks, SVM, Logistic Regression, k-Nearest Neighbor (kNN): Decision Tree. Ensemble Trees: Random Forest, GBMboost, XGboost', u'Data Scientist\nRare Genomics Institute - Hanover, MD\nJune 2014 to December 2016\nResponsibilities:\n\u2022 Developed computational and data science solutions for the storage, management, analysis, and visualization of genomic data.\n\u2022 Leveraged existing tools and publicly available genomics data to develop, test, or implement bioinformatics pipelines.\n\u2022 Extracted patent text and numerical features with python library Beautiful Soup, created Decision Tree algorithm to predict the patent classification on their Diseases.\n\u2022 Detected the near-duplicated news by applying NLP methods (e.g. word2vec) and developing machine learning models like label spreading, clustering\n\u2022 Provided expertise in statistical methods or machine learning with the goal of applying these techniques to health data.\n\u2022 Worked with Mobile Science 2.0, Mobile App teams to build a Classifier for Mobile App users that could be used by the digital marketing team to tailor specific messages to groups of users.\n\u2022 Used regulatory genomics/epigenetics & computational approaches in genetics and Patient data to perform clustering to group patients with similar diseases.\n\u2022 Algorithms implemented in Python, SQLite, Hadoop, MapReduce, MongoDB, R.\n\u2022 Worked with Big Data Technologies such Hadoop, Hive, MapReduce.\n\u2022 Handled importing data from various data sources, performed transformations using Hive, MapReduce, and loaded data into HDFS.\n\u2022 Created complex formulas and calculations within Tableau to meet the needs of complex business logic.\n\u2022 Combined Tableau visualizations into Interactive Dashboards using filter actions, highlight actions etc., and published them to the web.\n\u2022 Developed various data connections from data source to Tableau Desktop for report and dashboard development\nEnvironment: NLP, Python, Hadoop, MapReduce, Tableau, Spark, Hive, R. Major models tested: K-Means Clustering, SVM, Decision Tree based models: CART, CHAID, Information Gain, Random Forest', u'Data Scientist\nCeridian HCM Inc - Minneapolis, MN\nJuly 2012 to May 2014\nResponsibilities:\n\u2022 Worked on data cleaning and reshaping, generated segmented subsets using numpy and Pandas in Python. Developed Python scripts to automate data sampling process. Ensured the data integrity by checking for completeness, duplication, accuracy, and consistency.\n\u2022 Worked on model selection based on confusion matrices, minimized the Type II error. Generated cost-benefit analysis to quantify the model implementation comparing with the former situation.\n\u2022 Wrote and optimized complex SQL queries involving multiple joins and advanced analytical functions to perform data extraction and merging from large volumes of historical data stored in Oracle 11g, validating the ETL processed data in target database.\n\u2022 Conducted model optimization and comparison using stepwise function based on AIC value.\n\u2022 Applied various machine learning algorithms and statistical modeling like decision tree, logistic regression, Gradient Boosting Machine to build predictive model using scikit-learn package in Python.\n\u2022 Continuously collected business requirements during the whole project life cycle. Identified the variables that significantly affect the target\nEnvironment: Decision Tree, Logistic regression, Hadoop, Teradata, Python, MLLib, SAS, random forest, OLAP, HDFS, NLTK, SVM, JSON and XML', u'Data Analyst\nHornet InfoTech - Hyderabad, Telangana\nJuly 2010 to June 2012\nResponsibilities:\n\u2022 Ran SQL queries in Oracle database to analyze and manipulate data. Wrote SAS programs to performed ad-hoc analysis and data manipulation.\n\u2022 Created various SAS Reports, Tables, Graphs and Summary analysis on PMS systems being used in these properties.\n\u2022 Transferred data from Oracle database as well as MS Excel into SAS for analysis and used filters based on the analysis.\n\u2022 Used SAS Import/Export Wizard as well as SAS Programming techniques to extract data from Excel.Used SAS Base programming as well as SAS Enterprise Guide 4.0 to produce various reports, charts and graphs\n\u2022 Participated in the technology support team meeting to coordinate, review and determine appropriate hotel property software system for hotel property\nEnvironment: SAS, SQLite, Hadoop, MapReduce, SQL, MS Excel.', u'Java Developer\nSunGard Solutions - Hyderabad, Telangana\nMarch 2008 to June 2010\nResponsibilities:\n\u2022 Actively participated in all the phases of SDLC including Requirements Collection, Design & Analysis of the Customer Specifications, Development and Customization of the application.\n\u2022 Developed the application using Agile/Scrum methodology which involves daily stand ups. Test driven development, continuous integration, demos and test automations.\n\u2022 Strong hands-on knowledge of Core JAVA, Web-Based Application, and OOPS concepts.\n\u2022 Developed Client Side technologies using HTML, CSS, and Java Script. Developed Server Side technologies using spring, Hibernate, Servlets/JSP, Multithreading.\n\u2022 Extensively worked with the retrieval and manipulation of data from the Oracle database by writing queries using SQL and PL/SQL. Web application development by Setting up an environment, configuring an application and Web Logic Application Server.\n\u2022 Hands on Experience in coding, unit testing, Integration testing and Bug fixing.\nEnvironment: Oracle/SQL Server and PL/SQL, spring, Hibernate, Ant, Apache, Tomcat, JBOSS, Web logic, UNIX, RDBMS, HTML, CSS, Java Script, JDBC, Eclipse, Multithreading.']","[u'Master of Science in Computer Science', u'Bachelor of Technology in Computer Science and Engineering']","[u'Indiana State University', u'JNTU']"
0,https://resumes.indeed.com/resume/50a273367e3db1b8,"[u'Data Analyst\nGEICO - Washington, DC\nJune 2016 to March 2017\n\u2022 Enhanced GEICO digital platforms by guiding business decisions through data visualization and advanced analytics techniques.\n\u2022 Reduced online user abandonments on a payment % by adding a link to the FAQs page.\n\u2022 Delivered clear data insights and made actionable business recommendations for new user growth, mobile usage, and revenue based on quantitative analysis.\n\u2022 Designed experiments and formulated metrics to improve GEICO online business.\n\u2022 Built and maintained reports to monitor the web and mobile based behavior of customers.', u'Business Analyst\nGEICO - Washington, DC\nNovember 2014 to June 2016\n\u2022 Analyzed customer profiles and purchase patterns to develop innovative sales strategies.\n\u2022 Executed end-to-end analysis over 14 million policies to identify business opportunities in retention.\n\u2022 Saved over 400 cancellations ($1.5M) over 12 weeks of a pilot program by revamping an existing cancellation process.\n\u2022 Provided analytic support and analysis to various teams in cross-functional projects.\n\u2022 Trained and mentored new analysts on programing in SAS and SQL.']",[u'B.S. in Applied Mathematics'],"[u'Indiana University-Purdue University-Indianapolis Indianapolis, IN\nAugust 2010 to June 2014']"
0,https://resumes.indeed.com/resume/15779ec2cd51f6a5,"[u'Marketing Analyst\nNavy Federal Credit Union\nMarch 2017 to Present\n\u2022 Target prospects and members using different marketing channels\n\u2022 Test channels and analyze responsiveness for specific campaigns\n\u2022 Strategize with product analysts and business units about targeting for members and prospects\n\u2022 Prioritize campaigns and track number of campaign touches per member\n\u2022 Build and track multiple campaigns for a variety of products\n\u2022 Document standard work flow processes for future training of others', u""Data Analyst\nBuxton Company Analytics\nSeptember 2014 to March 2017\n\u2022 Downloaded, cleaned, validated and loaded data received from clients\n\u2022 Performed counts on prospects and customers, based on different criteria as requested by clients\n\u2022 Extracted data from the database and warehouse based on different criteria requested by clients\n\u2022 Appointed point-of-contact to client's internal data team to receive initial data transfer from client"", u'Junior Data Analyst\nTargetbase, Division of Omnicom Group\nJanuary 2014 to September 2014\n\u2022 Gathered requirements from Client Services and utilized in practical work\n\u2022 Investigated data integrity issues and fixed discovered problems\n\u2022 Quality-checked team members work to ensure accurate data delivered\n\u2022 Worked in fast-paced environment based on daily Client needs', u'Front End Lead\nKmart Stores\nOctober 2012 to December 2013']",[u'Bachelors'],[u'Business Administration IQRA University']
0,https://resumes.indeed.com/resume/b3f3f8a0c171510f,"[u'DATA ANALYST\nSelf-employed - Atlanta, GA\nFebruary 2016 to Present\nNEURONICA.US (Self-employed), Atlanta, GA\n\u2022 Develop neural networks software packages for spreadsheets (OpenOffice, MS Excel, etc.) using C++\n\u2022 Develop software tools using R in order to solve various business and client related needs.\n\u2022 Analyze business databases and create reports using Tableau and Excel.', u'DATA ANALYST\nTAPCO-M - Moscow, RU\nOctober 2014 to January 2016\nMoscow, Russia.\n\u2022 Performed data-driven decisions to increase sales and effectiveness of marketing efforts\n\u2022 Provided sales forecasting using predictive models and what-if scenarios\n\u2022 Created statistical models (Machine Learning algorithms, Regressions) and cluster groups (Kohonen maps, K-Means) for target marketplaces\n\u2022 Performed and visualized analysis of the export-import databases with Tableau, Pivot tables in Excel, etc.\n\u2022 Produced reports and/or data sets for ad hoc requests\n\u2022 Organized and aggregated data to obtain descriptive statistics for analyses', u'MARKETING DATA ANALYST\nFELIX - Moscow, RU\nJuly 2006 to September 2013\nMoscow, Russia\n\u2022 Led team to analyze marketing data, create reports and provide customer service\n\u2022 Trained new team members to understand marketing concepts and processes and to more effectively use MS Excel and other tools\n\u2022 Determined significant factors affecting successful sales\n\u2022 Investigated multiple data sources and variables for project use\n\u2022 Performed data-driven decisions to optimize warehouse assortment.\n\u2022 Analyzed sales data and trends using time-series forecasting algorithms\n\u2022 Performed quantitative and qualitative marketing surveys, consumer insights and analyzed results', u'MARKETING DATA ANALYST\nVORONEZH STATE AGRICULTURAL UNIVERSITY\nJanuary 2001 to January 2004\nVoronezh, Russia\n\u2022 Built predictive models using to urban land appraisal\n\u2022 Created tools combining neural networks and ArcGIS to visualize predictive models']","[u'PhD in Economics', u'Diploma in Chemistry', u'']","[u'Voronezh State Agricultural University', u'Voronezh State University', u'STANFORD']"
0,https://resumes.indeed.com/resume/2d1976e1bb3a23da,"[u'Analyst\nOn Target Maintenance - New York, NY\nAugust 2017 to Present\n\u2022 Maintaining databases across multiple business segments and manipulating it using MS Excel for analytical reporting.\n\u2022 Generating bi-weekly sales and ad-hoc reports by designing interactive Tableau dashboards for senior management.\n\u2022 Generated market analysis reports to identify target market, devise strategies using customer and market segmentation.\n\u2022 Analyzing competitor patterns, services and pricing to identify market and industry trends for business improvements.', u'Data Analyst Intern\nNew York City Transit - New York, NY\nSeptember 2016 to December 2016\n\u2022 Loaded employee data from multiple data sources into SQL databases received by the NYC HRA for 20 subway locations.\n\u2022 Created tables, joins and performed insert, update functions to populate tables in SQL databases as per requirements.\n\u2022 Participated in weekly meetings to provide status reports, communicated issues and provide recommendations.', u'Business Analyst\nCollegepond Consulting - Mumbai, Maharashtra\nOctober 2013 to July 2015\n\u2022 Designed a database management solution using SQL for faster data retrieval, migrated flat files into SQL databases.\n\u2022 Enhanced data extraction process by executing SQL queries, creating joins, views and stored procedures for data analysis.\n\u2022 Collaborated with managers, team members to research for and gather new data to populate schemas and tables.\n\u2022 Created Tableau workbooks and reports by connecting to various data sources for senior managers to pitch with clients.\n\u2022 Designed Tableau dashboards to identify trends and for monthly reporting of enrollment statistics across all business lines.\n\u2022 Supported a business initiative to develop a CRM system to streamline & optimize existing internal workflow by 40%.\n\u2022 Functioned as a liaison between the stakeholders, operations and development team to understand business processes.\n\u2022 Documented project scope, objectives, expectations, deliverables and gathered stakeholders & customer requirements.\n\u2022 Developed flowcharts, created data flow diagrams and contributed with the designing of the UI of the CRM system.']","[u'Master of Science in Technology Management in Business Analysis', u'Bachelor of Engineering in Information Technology in Core']","[u'New York University, Tandon School of Engineering New York, NY\nMay 2017', u'University of Mumbai Mumbai, Maharashtra\nMay 2013']"
0,https://resumes.indeed.com/resume/6fddebccec121b7a,"[u'Sr. Data Analyst/ Data Modeler\nNavy Federal Credit Union - Sterling, VA\nJanuary 2016 to August 2017\nResponsibilities:\n\u2022 Navy Federal Credit Union (or Navy Federal) is a US credit union, chartered and regulated under the authority of the National Credit Union Administration (NCUA). Navy Federal is the largest natural member (or retail) credit union in the United States, both in asset size and in membership.\n\u2022 A roving modeler, responsible forNavy Federal Student Loan (Private Student Loan and Consolidated Student Loan)project which integrates and standardizes data from multiple sources on the Teradata platform. This platform allows to present customized views of a single set of physical data to various parts of Private Student Loan (PSL) without the need to duplicate and distribute that data. A centralized warehouse environment where data from PSL sales, origination and servicing systems is integrated and standardized to provide a consistent and integrated view of the business.Partnered with business analysts, business leadership, IT management, and strategy teams to ensure that architectural directions align with and support both business and IT strategies.\n\u2022 Gathered Business requirements by organizing and managing meetings with Business stake holders, Application architects, Technical architects and IT analysts on a scheduled basisand created artifacts to support architecture governance, including standards, advisories, research papers, reference models and reference architectures and also Presented Enterprise Architecture-related information to various enterprise teams, including portfolio governance, business working groups, IT leadership or business leadership.\n\u2022 Analyzed the business requirements by dividing them into subject areas and understood the data flow within theorganization and supported and contributed to efforts to introduce new business and technical capabilities to Navy Federal, including participating in RFIs, RFPs, vendor reviews and product selection.\n\u2022 Extensively worked on early-stage business projects, discovery efforts, and engagements initiated by Business Relationship Managers to provide appropriate architecture deliverables, such as stakeholder analyses, capability analyses, risk/value analyses, or technical analyses.\n\u2022 Worked with Database Administrators, Business Analysts and Content Developers to conduct design reviewsand validate the developed models.\n\u2022 Used Model Mart of Erwin for effective model management of sharing, dividing and reusing modelinformation and design for productivity improvement.\n\u2022 Performed data analysis and data profiling using complex SQL on various sources systems and answeredcomplex business questions by providing data to business users.\n\u2022 Generated ad-hoc SQL queries using joins, database connections and transformation rules to fetch data fromlegacy DB2 and SQL Server database systems.\n\u2022 Analysis of functional and non-functional categorized data elements for data profiling and mapping fromsource to target data environment using Informatica Data Quality and developed working documents to supportfindings and assign specific tasks.\n\u2022 Developed Data Mapping, Data Governance, Transformation and Cleansing rules for the Master Data Management Architecture involving OLTP, ODS and OLAP.\n\u2022 Collaborated with ETL, BI and DBA teams to analyze and provide solutions to data issues and other challengeswhile implementing the OLAP model.\n\u2022 Worked with data source systems and Client systems to identifydata issues, data gaps andrecommended solutions.\n\u2022 Enforced Referential Integrity for consistent relationship between parent and child tables.\n\u2022 Facilitated in developing testing procedures, test cases and User Acceptance Testing (UAT).\n\u2022 Prepared analytical and status reports and updated the project plan as required.\n\u2022 Integrated the work tasks with relevant teams for smooth transition from testing to implementation.\n\u2022 Provide consultative approach with business users, asking questions to understand the business need and deriving the data flow based on those needs.\n\u2022 Perform the detail data analysis, Identify the key facts and dimensions necessary to support the business requirements.\n\u2022 Analyze business requirements and Design, Develop ETL processes to load data from varioussources like Flat files, XML files, Oracle Database.\n\u2022 Develop strategies for data extraction and loading from the source system across domain, usingInformatica Power Center.\n\u2022 Execution of test plans for loading the data successfully into the targets.Developed various Mappings and Transformations for migration of data from various existing systems to the new system using Informatica.\n\u2022 Perform unit testing and user acceptance testing to check whether the data being loaded intothe target, which was extracted from different source systems according to the userrequirementaccurately.\n\nEnvironment: Erwin r9.5, Informatica 8.6.1, Oracle SQL Developer, Oracle Data Modeler, TOAD, Oracle 9i/10g, Teradata 14, DB2, SSIS, Business Objects, SQL Server 2005/2008, SQL, PL/SQL, IBM DB2, VBA MS Excel, ER/Studio Windows XP, MS Excel', u'Sr. Data Analyst/ Data Modeler/Business Analyst\nNavy Federal Credit Union - Cleveland, OH\nApril 2016 to July 2016\nResponsibilities:\n\u2022 Responsible forKey Home Mortgageproject called MIDE (Mortgage Integrated Data Environment) which integrates and standardizes data from multiple sources on the Teradata platform. This platform allows to present customized views of a single set of physical data to various parts of KHM without the need to duplicate and distribute that data. A centralized warehouse environment where data from KHM sales, origination and servicing systems is integrated and standardized to provide a consistent and integrated view of the business.\n\u2022 Empowered the retention and acquisition of private banking customers, which boosted deposits by 8%. Increased productivity and reduced loan processing time 2% by facilitating the sharing of data among home loans, auto loans and credit cards.\n\u2022 Extracted data from Oracle, SQL Server and DB2 using Informatica to load it into a single data warehouse repository.\n\u2022 Created entity relationship diagrams and multidimensional data models, reports and diagrams for marketing.\n\u2022 Used Model Mart of Erwin for effective model management of sharing, dividing and reusing model information and design for productivity improvement.\n\u2022 Developed and implemented measurements for various marketing campaigns.\n\u2022 Created a net loss model which analyzes the impact of loan defaults and recoveries.\n\u2022 Developed complex SQL queries, and perform execution validation for remediation and Analysis.\n\u2022 Created a relational model and dimensional model for online services such as online banking and automated bill pay.\n\u2022 Applied data cleansing/data scrubbing techniques to ensure consistency amongst data sets.\n\u2022 Developed logical data models and physical data models using ER-Studio.\n\u2022 Adjusted and maintained SQL script and perform further data analysis and data aggregation.\n\u2022 Interact between technical groups and non-technical line of business partners and internal group subject matter experts.\n\u2022 SME (subject matter experts) for remediation exclusion and Data Dictionary.\n\u2022 Involved in process improvement, infrastructure, Business Support (Remediation Process), Support Structure and Analytics.\n\u2022 Analyzed requirements and data models, development of cases for complex scenarios.\n\u2022 Worked with data and results; gather data in reports; extensively analyze data and observations; express analysis and observation.\n\u2022 Obtained information from disparate sources, including production and ad hoc systems, linking and analyzing the information, performing data integrity checks and exploratory data analysis.\n\u2022 Gathered business and Technical requirements that would best suit the needs of the technical architectural development process.\n\u2022 Worked with internal architects and, assisting in the development of current and target state enterprisedata architectures.\n\u2022 Involved in defining the source to target data mappings, business rules and data definitions.\n\u2022 Comprehensively used data analysis techniques to validate business rules and identify low quality missing data.\n\u2022 Extensively used SQL for accessing and manipulating database systems\n\u2022 Worked closely with QA team and developers to clarify/understand functionality, resolve issues and provided feedback to nail down the bugs.\n\u2022 Acted as a liaison between the development team and the management team to resolve any dataconflicts in terms of requirements.\n\u2022 Interacted with all levels of the project team, from end users to Software Architects, data governance team, Database Administrators, and testers.\n\nEnvironment: Erwin r9.5, Informatica 8.6.1, Oracle SQL Developer, Oracle Data Modeler, TOAD, Oracle 9i/10g, Teradata 14, DB2, SSIS, Business Objects, SQL Server 2005/2008, SQL, PL/SQL, IBM DB2, VBA MS Excel, ER/Studio Windows XP, MS Excel, Sybase Power Designer.', u""Sr. Data Analyst/ Data Modeler\nTD Bank, Mount Laurel, New JerseyAug\nJanuary 2015 to March 2016\nResponsibilities:\n\u2022 Gathered and translated business requirements into detailed, production-level technical specifications detailing new features and enhancements to existing business functionality.\n\u2022 Involved in gathering user requirements along with Business Analyst.\n\u2022 Interacted with end users to identify key dimensions and measures that were relevant quantitative.\n\u2022 Created the Source System Analysis documents and Architectural Solution documents.\n\u2022 Developed the logical and physical data model for the proposed solution.\n\u2022 Part of team conducting logical data analysis and data modeling JAD sessions, communicated data-related standards.\n\u2022 Involved in the complete life cycle of the product from requirements gathering to end of system testing.\n\u2022 Created views to support the Business Objects.\n\u2022 Responsible for creating the staging tables and source to target mapping documents for the ETL process.\n\u2022 Identified and tracked the slowly changing dimensions, heterogeneous sources and determined the hierarchies in dimensions.\n\u2022 Extensively used the dimensional modelling for the data ware house design.\n\u2022 Involved in many of the integration projects.\n\u2022 Created Indexes and capacity planning and worked closely with the DBA's.\n\u2022 Created Summary and Aggregate tables for better performance of the query.\n\u2022 Worked with ETL teams and used Informatica Designer, Workflow Manager and Repository Manager to create source and target definition, design mappings, create repositories.\n\u2022 Developed and maintained data dictionary to create metadata reports for technical and business purpose.\n\u2022 Created action plans to track identified open issues and action items related to the project.\n\u2022 Prepared analytical and status reports and updated the project plan as required\n\nEnvironment:Erwin r9.5, Oracle9i/10g, Teradata 14, TOAD, ERWIN r7.3, Teradata SQL Assistant 13.0, DB2, SSIS, Business Objects, SQL Server 2005/2008, SQL, PL/SQL, IBM DB2, VBA MS Excel, ER/Studio Windows XP, MS Excel, Sybase Power Designer."", u'Sr. Data Analyst/ Data Modeler\nTD Bank, Mount Laurel, New JerseyAug - Township of Warren, NJ\nFebruary 2015 to July 2015\nResponsibilities:\n\u2022 Gathered and translated business requirements into detailed, production-level technical specifications, creating robust data models, data analysis features and enhancements for this major provider of home and auto insurance in property predictive modeling.\n\u2022 Participated with in a team of data management subject matter experts to identify and evolve key data management strategies and initiatives within the organization.\n\u2022 Refined and standardized data modeling practices to align with application paradigm (E.g. Business Intelligence, Transactional Processing, Big Data etc.)\n\u2022 Translated and transformed business requirements into Conceptual, Logical, and Physical data models.\n\u2022 Conducted extensive data analysis to evaluate data sources to determine the best source for business information.\n\u2022 Analysis of functional and non-functional categorized data elements for data profiling and mapping from source to target data environment. Developed working documents to support findings and assign specific tasks.\n\u2022 Involved with data profiling for multiple sources and answered complex business questions by providing data to business users.\n\u2022 Implemented business models consistent within organizations Information Architecture strategy.\n\u2022 Facilitated the interaction between the development teams and the DBA staff to design physical database structures that meet performance expectations and deliver business function.\n\u2022 Adhered to approved architectural standards; Recommendedarchitectural standard improvements.\n\u2022 Defined, developed and delivered consistent information and data standards, methodologies, guidelines, best practice and approved modeling techniques around data quality, data governance and data security.\n\u2022 Partnered with subject matter experts, architects and developers to capture and analyze business needs to complete all data modeling related artifacts including Conceptual, Logical and Physical Data Models.\n\u2022 Worked with data integration teams to ensure that the model design and development is properly communicated.\n\u2022 Used Informatica Designer, Workflow Manager and Repository Manager to create source and target definition, design mappings, create repositories and establish users, groups and their privileges.\n\u2022 Used Model Mart of Erwin for effective model management of sharing, dividing and reusing model information and design for productivity improvement.\n\u2022 Extracted data from the databases (Oracle and SQL Server, DB2, FLAT FILES) using Informatica to load it into a single data warehouse repository.\n\u2022 Participated in design reviews and ensure that model solutions meet database standards.\n\u2022 Involved in maintaining and updating Metadata Repository with details on the nature and use of applications/data transformations to facilitate impact analysis.\n\u2022 Developed and maintained Data Dictionary to create Metadata Reports for technical and business purpose using Erwin 9.5 report designer.\n\u2022 Experienced in development and design of RDBMS -OLTP, dimensional modeling using data modeling tool ERWIN, Sybase Power Designer.\n\u2022 Designed, documented and implemented common message models (XML).\n\u2022 Defined message models that are ideally scalable, reflective of industry standards.\n\u2022 Analyzed existing business processes and data flows and determined the most optimal level of normalization for message publication.\n\u2022 Reviewed data & message Models with Architecture Review Board.\n\u2022 Independently estimated work effort.\n\u2022 Mentoredjunior resources as designated by team leader.\n\nEnvironment: Erwin r9.5, Oracle SQL Developer, Oracle Data Modeler, Teradata 14, DB2, SSIS, Business Objects, SQL Server 2005/2008, ER/Studio Windows XP, MS Excel, Sybase Power Designer.', u""Sr. Data Modeler / Data Analyst\nAllstate Insurance - Northbrook, IL\nMay 2014 to January 2015\nResponsibilities:\n\u2022 Worked with Business Analyst for requirements gathering, business analysis and project coordination.\n\u2022 Developed a Conceptual Model and Logical Model using Erwin based on requirements analysis.\n\u2022 Created the best fit Physical Data Model based on discussions with DBAs and ETL developers.\n\u2022 Utilized Erwin's forward/reverse engineering tools and target database schema conversion process.\n\u2022 Reviewed and implemented the naming standards for the entities, attributes, alternate keys, and primary keys for the logical model.\n\u2022 Worked on data mapping process from source system to target system.\n\u2022 Created dimensional model for the reporting system by identifying required facts and dimensions using Erwin.\n\u2022 Extensively used Star and Snowflake Schema methodologies.\n\u2022 Worked on Slowly Changing Dimension tables.\n\u2022 Involved in maintaining and updating Metadata Repository with details on the nature and use of applications/data transformations to facilitate impact analysis.\n\u2022 Developed and maintained Data Dictionary to create Metadata Reports for technical and business purpose.\n\u2022 Experienced in data migration and cleansing rules for the integrated architecture (OLTP, ODS, DW).\n\u2022 Worked on Performance Tuning of the database which includes indexes, optimizing SQL Statements.\n\u2022 Experience with Oracle SQL and PL/SQL programming andused Database utility programs like TOAD and SQL Navigator.\n\u2022 Used Model Mart of Erwin for effective model management of sharing, dividing and reusing model information and design for productivity improvement.\n\nEnvironment: Erwin r8.2, Oracle SQL Developer, Oracle Data Modeler, Teradata 14, SSIS, Business Objects, SQL Server 2005/2008, ER/Studio Windows XP, MS Excel."", u""Sr. Data Modeler / Data Analyst\nCummins - Indianapolis, IN\nAugust 2013 to May 2014\nResponsibilities:\n\u2022 Coordinated in requirements gathering session with business users and sponsors to understand and document the business requirements as well as the goals of the project.\n\u2022 Developed Conceptual model using Erwin based on requirements analysis.\n\u2022 Worked with Business Analyst during requirements gathering and business analysis to prepare high level Logical Data Models and Physical Data Models.\n\u2022 Redefined many attributes and relationships in the reverse engineered model and cleansed unwanted tables/columns as part of data analysis responsibilities.\n\u2022 Created dimensional model for the reporting system by identifying required facts and dimensions using Erwin.\n\u2022 Implemented Forward engineering to create tables, views and SQL scripts and mapping documents.\n\u2022 Prepared data dictionaries and Source-Target Mapping documents to ease the ETL process and user's understanding of the data warehouse objects\n\u2022 Implemented the slowly changing dimension (Type II) and determined the hierarchies in dimensions.\n\u2022 Translated business concepts into XML vocabularies by designing XML Schemas with UML.\n\u2022 Exhaustively collected business and technical Metadata and maintained naming standards.\n\u2022 Extensively used reverse engineering feature of Erwin to save the data model with production.\n\u2022 Used Erwin data model mart for storing different versions of the model.\n\u2022 Involved in maintenance of models and loading models to Metadata Repository.\n\u2022 Involved in data analysis of the Teradata.\n\u2022 Experience in Oracle SQL and PL/SQL including all database objects: Stored procedures, Stored functions, Packages, Objects, Triggers, cursors, cursors, Parameterized cursors, Views, Materialized Views, PL/SQL collections.\n\u2022 Responsible for integrating the work tasks with relevant teams for smooth transition from testing to implementation phase.\n\nEnvironment: Erwin 8.0, SQL, SQL Server 2012, Teradata, SQL Assistant 12.0/13.11, Data stage 8.1, DB2."", u""Data Modeler / Data Analyst\nPark & Fly - Atlanta, GA\nJanuary 2012 to July 2013\nResponsibilities:\n\u2022 Demonstrated strong analytical skills in identifying and resolving data exchange issues.\n\u2022 Developed Data Mapping, Data Governance, Transformation and Cleansing rules for the Master Data Management.\n\u2022 Executed enterprise Data Governance strategies in line with the organization's business strategies and objectives.\n\u2022 Good experience in data analysis querying and writing in SQL and TOAD.\n\u2022 Extensively used Data Tools to maintain consistent data.\n\u2022 Performed Data Stewardship roles like implementing data standards, creating metadata and managing databases.\n\u2022 Performed data conversion from multiple sources and verified validation test on the converted data.\n\u2022 Good understanding and experience with the entire Data Migration process from analyzing the existing data, cleansing, validating, translating tables, converting and subsequent upload into new platform.\n\u2022 Used E.F Codd's Normalization (1NF, 2NF & 3NF) and Demoralizationtechniques for effective performance in OLTP and OLAP systems.\n\u2022 Designed data models in Dimensional Modeling (DM) environment.\n\u2022 Executed SQL queries to retrieve data from databases for analysis.\n\u2022 Created Data Model Reports and Data Dictionary (DD)using ERWIN.\n\u2022 Developed normalized Logical and Physical database models to design OLTP system for enterprise applications.\n\u2022 Performed forward engineering operations to create a physical data model with DDL that best suits requirements from the logical data model.\n\nEnvironment: CA Erwin 8, Oracle10g, SQL server 2012, IBM DB2, Informatica Power Center 8.6, SQL BI 2008, Oracle BI, Visual Studio, SSIS&SSRS, SQL server management studio 2012."", u'Data Analyst\nJefferies - New York, NY\nDecember 2010 to March 2012\nResponsibilities:\n\u2022 Gathered and translated business requirements into detailed, production-level technical specifications, new features, and enhancements to existing technical business functionality.\n\u2022 Involved in developing Conceptual Data Model and conducted controlled brainstorming sessions with project focus groups.\n\u2022 Identified the Objects and relationships between the objects to develop a logical model and translated the model into physical model using ForwardEngineering in ERWIN.\n\u2022 Conducted logical data model walkthroughs and validation.\n\u2022 Hands on experience in Financial Services data model (FSDM).\n\u2022 Used Model Marts to understand different versions of data models existing in the system.\n\u2022 Involved in dimensional modeling in identifying the Facts and Dimensions and different hierarchies.\n\u2022 Used Erwin tool for relational database and dimensional data warehouse designs.\n\u2022 Created entity-relationship diagrams, functional decomposition diagrams and data flow diagrams.\n\u2022 Worked extensively on SQL querying using Joins, Alias, Functions, Triggers and Indexes.\n\u2022 Managed all indexing, debuggingand query optimization techniques for performance tuning using T-SQL.\n\u2022 Created documentation and test cases, worked with users for new module enhancements and testing.\n\u2022 Worked with business analyst to design weekly reports using combination of Crystal Reports.\n\u2022 Understood existing data model and documented suspected design affecting the performance of the system.\n\u2022 Assisted in the Master Data Management strategies and process improvements.\n\u2022 Experienced in data cleansingand Data migration for accurate reporting. Thoroughly analyzed the data and integrated different data sources to process matching functions.\n\nEnvironment: Erwin 7.2, SQL Server 2008, SQL Server Reporting Services (SSRS), Business Intelligence Development Studio (BIDS), MS Excel, Windows XP, TOAD.', u""Data Modeler\nDr. Reddy's Laboratories Ltd\nJune 2009 to November 2010\nResponsibilities:\n\u2022 Interacted with the business users on regular basis to consolidate and analyze the requirements.\n\u2022 Identified, formulated and documented detailed business rules and use casesbased on requirements analysis.\n\u2022 Identified the Entities andthe relationships between the Entities to develop a logical model and later translated into physical model.\n\u2022 Developed Facts & Dimensions using ERWIN.\n\u2022 Used Normalization up to 3NF and De-normalization for effective performance.\n\u2022 Developed Star and Snowflake schemas using dimensional data models.\n\u2022 Involved in designing OLAP data models and extensively used slowly changing dimensions (SCD).\n\u2022 Enforced Referential Integrity for consistent relationship between parent and child tables.\n\u2022 Strong working experience in Extract/Transform/Load (ETL) design and implementation in areas related to Teradata utilities such as Fast Export and MLOAD for handling multiple tasks.\n\u2022 Extracted data from databases like Oracle, SQL server and DB2 using Informatica to load it into a single repository for data analysis.\n\u2022 Created source to target mapping specifications usingInformatica data quality tool.\n\u2022 Reverse Engineered the Data Models and identified the Data Elements in the source systems.\n\u2022 Compared data with original source documents and validated data accuracy.\n\u2022 Involved in development and implementation of SSIS, SSRS and SSAS application solutions for various business units across the organization.\n\u2022 Experienced in data migration and cleansing rules for the integrated architecture (OLTP, ODS, DW).\n\u2022 Worked on multiple data marts in Enterprise Data Warehouse (EDW).\n\u2022 Involved in generating reports andmaintaining the data base.\n\nEnvironment: Erwin r7.0, SQL/MS SQL Server, MS Analysis Services, Windows NT, MS Visio,\nXML, Informatica.""]",[u'Bachelors of Engineering in Engineering'],[u'Jawaharlal Nehru Technological University']
0,https://resumes.indeed.com/resume/c59cf6a491419a1b,"[u""Data Analyst\nBloomberg LP\nOctober 2017 to Present\nPromoted to a team of four people to track and analyze the Exchange-Traded Funds market in the U.S and Canada.\n\u25cf Monitor a universe of 2800+ ETFs to ensure accuracy and make sure data flows properly through Bloomberg Terminal\n\u25cf Automated several processes for the team including, ETF Daily U.S Error Report, FI ETF Indices, Vanguard's Daily\nAssets, Fund US/Canadian Asset Summation Tool, and other projects\n\u25cf Coordinated with Bloomberg Compliance/Tax Department to source data from large investment companies regarding their Exchange-Traded Products\n\u25cf Met with clients regarding the use of the Terminal and to demonstrate how ETF products/internal Python IDE Tools\ncould add value to their companies"", u'Bloomberg LP - Princeton, NJ\nJanuary 2017 to Present', u""Data Analyst\nBloomberg LP\nJanuary 2017 to October 2017\nUtilized the terminal across multiple asset-classes, data sets, functions, API's, and other client-driven applications\n\u25cf Communication with clients across global markets, fielding over 6,000 queries\n\u25cf Developed programs utilizing Python /Pandas to streamline efficiency as well as creating new key insights\n\u25cf Collaborated directly with cross-functional teams to create new processes\n\u25cf Participated in the Bloomberg recruitment process as well as mentoring the new hires on their transition to the data\nteams\n\u25cf Utilized the data from the terminal to learn about new trends within the funds market"", u'Cyber Security Intern\nCipherTechs, Inc - New York, NY\nAugust 2015 to December 2015\nProvided computer help desk support for employees via telephone or in person\n\u25cf Distributed RSA tokens to employees and resolved technical problems regarding logging in or forgotten keys\n\u25cf Assisted in setup and implementation of information technologies (servers, firewalls, and phones)\n\u25cf Provided technical support for the network including routers, firewalls, and wireless access points\n\u25cf Generated reports for clients on what their employees were doing throughout their work day as well as what IP\naddresses were hit on a daily basis\n\u25cf Used McAfee SIEM and Zscaler to monitor client workers and view reports generated from the software to advise\nclients regarding what was going wrong within their network']",[u'BA in Information Technology and Informatics'],"[u'Rutgers University New Brunswick, NJ\nDecember 2016']"
0,https://resumes.indeed.com/resume/7b023ec9cdd7e8a0,"[u'Business Data Analyst\nAkrimax Pharmaceuticals - Cranford, NJ\nJuly 2016 to Present\nAkrimax Pharmaceuticals manufactures and markets pharmaceutical products with meaningful clinical advantages that improve the life of patients. Using a combination of revitalized promotion, product reformulation, Akrimax makes new therapeutic options available for healthcare providers and patients.\n\nProject:\nAkrimax Pharmaceuticals has over 500+ distributors throughout USA. To ease the complete supply chain process, worked on Distributor portal. It was a self-served portal for distributors.\n\nRoles & Responsibilities:\n\u2022 Performed Gap analysis, defined the scope of the project, Statement of Work (SOW), gathered business requirements and documented them.\n\u2022 Worked closely with the business users in understanding the payment reconciliation process.\n\u2022 Understanding the business requirements from user interviews and converting it into technical specifications.\n\u2022 Described the Use Cases and designed UML Diagrams such as Activity Diagrams, Sequence Diagrams.\n\u2022 Conducted regular Joint Application Development (JAD) sessions for requirement gathering.\n\u2022 Facilitated regular meetings with the System Architects, Developers, Database Developers, Quality Analysts to discuss business requirements, test planning, resource utilization and defect tracking.\n\u2022 Responsible to resolve any issues which surfaced during the project lifecycle.\n\u2022 Designed prototypes/wireframes using Axure Pro which met user defined requirements and company standards for the UI team.\n\u2022 Extracting data from Oracle database using SQL queries to check data integrity.\n\u2022 Responsible for creating reports to understand the product sales.\n\u2022 Identified test execution method and defined the test cases.\n\u2022 Assisted the Release Manager in preparing release documentation, defining the scope and planning releases.\n\u2022 Collaborated with the team members in the use of HP ALM (HP Application Lifecycle Management /HPQC) for the management of UAT test cases.\n\u2022 Actively participated in UAT phase by assisting the business users and writing guidelines for formal UAT.\n\u2022 Participated in product release and deployment after successful development and testing of the product.\n\u2022 Experienced with ERP software (Enterprise Resource Planning) within the wholesale distribution environment.\n\u2022 Performed various AS-IS and TO-BE process on modules like Vendor Management/ clearinghouse management.\nTools: HP ALM, Axure Pro, MS Office, ERP software, Microsoft Visio\nServer: Oracle 11g\nLanguages: Java, J2EE, SQL', u""Business Data Analyst\nSpectra Laboratories - Rockleigh, NJ\nMay 2014 to June 2016\nSpectra Laboratories is a wholly-owned subsidiary of Fresenius Medical Care, the world's largest vertically integrated dialysis service provider. Spectra Labs is a leading provider of renal-specific testing services, performing more than 54 million tests each year. Dialysis Test Billing is a complex process based on certain rules and regulations set by Medicare/Medicaid and also Commercial Insurances. TRAC Billing System is a home grown application based on the complex needs of Dialysis Billing.\n\nProject Description:\nCharges posting in TRAC Billing application on daily basis with huge volume require complex yet efficient queries. Therefore the application had to be updated to meet the modern usage and standards. Extensive usage of Explain plan to design the queries, procedures and scripts in the right way. For comparison and auditing purposes data is also analyzed across various DB platforms, like Oracle and SQL Server.\n\nHL7 interfaces were also developed for receiving and sending HL7 messages. These messages included interface for receiving ADT's via TCP/IP and updating the TRAC application or receiving the charge files from HLAB on a nightly basis.\n\nRoles and Responsibilities:\n\u2022 Met with business users to analyze the requirements and performed in-depth analysis to those requirements.\n\u2022 Conducted Gap analysis and system analysis to determine if solutions to the business requirements.\n\u2022 Analyzed impact of proposed solution across the business. Developed proposed statement of work.\n\u2022 Actively participated in all the scrum ceremonies such as sprint planning meetings, sprint estimation meetings, daily scrum meetings, sprint demos and retrospective meetings.\n\u2022 Successfully used agile/scrum methodologies for project tracking and biweekly deliveries.\n\u2022 Responsible for Oracle data loading into oracle database from flat files using SQL loader.\n\u2022 Extracting data from TRAC DB and matching with data from excel sheets.\n\u2022 Responsible for all reporting needs of the Billing Users. Designed more that 200+ reports ranging from Daily, Weekly or Monthly reports. Client Report like Invoices, Order/Charge Summary Report, Client Aging, Patient Aging by Insurance or Date of Service.\n\u2022 Created flows, documents & Step by Step Procedures for the Support Team which would provide Support to the Application.\n\u2022 Coordinated and Monitored User Acceptance Testing (UAT) with the business users and interfaced with the development and test team with regards to defect status and fixes on a regular basis. Also parsed EDI files for User testing and analysis.\nTools: McKesson HLAB, TRAC Billing System, Crystal Reports 2013/2008/Xi R1/R2, SQL Developer 3.x, 4.x, Jira Agile, Balsamic, MS Visio.\nServer: Oracle 10g/11g\nLanguage: SQL, Java"", u""Business Analyst\nPharma Care Inc - Clark, NJ\nFebruary 2013 to April 2014\nProject:\nThe Electronic Pharmacist Information Consultant (EPIC) service is offered to Pharma care patients. The EPIC service allows physicians, nurses, and health care staff to quickly receive critical information, individually tailored to a resident's circumstances. With early detection and notification, appropriate changes in a resident's medication regimen can be implemented in order to avoid the risks of adverse drug interactions and potentially dangerous side effects.\n\nResponsibilities:\n\u2022 Performed JAD sessions with stakeholders and business users to gather and document requirements\n\u2022 Interfaced with business users to create Business Requirements Document (BRD) and Functional Specifications Document (FSD)\n\u2022 Was an integral and active team member during the sprint and scrum ceremonies\n\u2022 Analyzed requirements and did Use Case analysis designing Use case diagrams, Class diagrams, and Sequence diagrams using UML approach.\n\u2022 Worked on AS-IS and TO-BE models to enhance the existing system\n\u2022 Participated in the Change Management Control process for the projects by facilitating group meetings and/or one-on-one interviews/meetings with work stream owners to discuss change request impact on timelines and scope.\nTools: MS Office, MS Project, MS Visio, JIRA, Oracle\nSDLC: Agile (Scrum)\nServer: Oracle\nLanguage: Java, J2EE."", u""Project Coordinator\nHCL Technologies Ltd - IN\nFebruary 2012 to December 2012\nProject:\nHCL is one of India's leading global IT Services Multi-National Companies, providing software led IT solutions\nResponsibilities:\n\u2022 Performed Pre-Sales Activities like Proposal Writing and Bidding\n\u2022 Wrote Papers for various Go to Market Software Solutions\n\u2022 Facilitate weekly project status meeting with clients, prioritize backlog, and discuss any issues or risks\n\u2022 Prepared Protocols and reports\n\u2022 Coordinate timely acquisition of deliverables signoff approval documentation\n\u2022 Played active role in organizing events with focus on maintaining its Client relationship across the globe.\nTools: MS Office, MS Project, UML, MS Visio.""]","[u""Bachelor's""]",[u'']
0,https://resumes.indeed.com/resume/cba208ef7d2177f3,"[u'Data Analyst\nMathematical model building, data trending, statistics models (regression, correlation, etc)']","[u'Graduate certificate in Data Science', u'BA, MS in Mathematics and economics']","[u'Harvard University Extension School Cambridge, MA\nJanuary 2016 to December 2017', u'Moscow State University of Economics, Statistics and Informatics \u041c\u043e\u0441\u043a\u0432\u0430\nJanuary 2005 to January 2010']"
0,https://resumes.indeed.com/resume/97187f48c764e3f6,"[u'Data Analyst\nRoostify Inc. - San Francisco, CA\nJune 2017 to December 2017\n\u2022 Collaborated with a team of product managers to create new leads and opportunities to be implemented in Q1 2018.\n\u2022 Improved data quality for data mining from various Real estate public records, Salesforce, and other database repositories using Python scripting language and Excel based on Roostify\u2019s specifications.\n\u2022 Implemented Dynamic Regression forecasting model with an accuracy of 79% for product\u2019s sales for Q4 2017.\n\u2022 Provided analytical solutions using PowerBI which was successfully implemented during business expansion.', u'Software Engineer\nBhabha Atomic Research Centre - Mumbai, Maharashtra\nJuly 2014 to December 2015\n\uf0a7 Created an encrypted SSL Certificate for BARC to protect communication channel of their internal networks. This helped secure internal servers with an efficiency of over 95%\n\uf0a7 Maintained databases by providing access to different groups of users in the company using PostgreSQL.']","[u'Master of Science in Management Information Systems in Data science', u'Bachelor of Engineering in Information Science & Engineering']","[u'California State University\nJanuary 2016 to January 2018', u'New Horizon College of Engineering, Visvesvaraya Technological University\nJune 2015']"
0,https://resumes.indeed.com/resume/df25ea9e4032f1dd,"[u'Data Analyst\nSpecial Funds Conservation Committee - New York, NY\nMarch 2017 to Present\n\u2022 Perform daily, weekly, and monthly reports on company data, including transactions and bills processed\n\u2022 Manage database utilized by all employees in SFCC\n\u2022 Communicate and report to COO and managers from offices in Albany, Buffalo, Dewitt, and NYC\n\u2022 Streamline analysis of data into easily accessible spreadsheets and written reports\n\u2022 Troubleshoot technical computer issues.', u'Media Analyst Temp\nNews America Marketing - New York, NY\nOctober 2016 to March 2017\n\u2022 Streamline analysis by acquiring rates, costs, and formatting relevant data into useful, easily accessible spreadsheets\n\u2022 Consolidate financial data and information, including pricing, deals, and contracts\n\u2022 Locate and contact new clients for advertising']",[u'Bachelor of Arts in Economics'],"[u'Wesleyan University Middletown, CT\nMay 2016']"
0,https://resumes.indeed.com/resume/e67494656cd8616f,"[u'Teaching Assistant\nTemple University - Philadelphia, PA\nAugust 2015 to Present', u'Data Analyst\nDalian Broadcast and Television Center - Dalian\nAugust 2009 to July 2015']","[u'PhD in Applied Mathematics', u'BS in Applied Mathematics']","[u'Temple University Philadelphia, PA\nAugust 2015 to June 2020', u'Jilin University Changchun\nSeptember 2005 to June 2009']"
0,https://resumes.indeed.com/resume/3810f55d23c7e65c,"[u""Data Analyst\nCompany name upon request - Monterey Park, CA\nFebruary 2017 to Present\nPlease do not contact employers without prior consent\nData Analyst\nCompany name upon request\nFeb 2017 - Present\nAccomplishments:\n\n\u2022 Used the algorithm such as Logistic Regression, Random Forest to build the model that can predict and\ncalculate the risk of live loans, and identified the 20 most significant features of the charged off loans;\nsuccessfully increased the AUC value by 0.11 from baseline model\n\u2022 Built a model that successfully predicted the probability that a driver would initiate an auto insurance claim\nin the following year.\n\u2022 Acquired the live data of the users and their loan information by using API, trained the past 3-year data with\nlabels as fully paid or charged off, and conducted various feature engineering on data cleaning and data\ntransformation\n\u2022 Greatly improved efficiency of the BI process.\n\nDuties:\n\u2022 Conduct data analysis for clients and present results\n\u2022 Prepare and analyze historical data to identify the pattern, create custom Tableau dashboards to make informative\nvisualizations, and help explore deep data insights\n\u2022 Cooperate with team leader to build various data models by using Python or R, and write complex SQL queries\nto analyze data\n\u2022 Apply statistical methods (linear regression, logistic regression), machine learning techniques to solving\nbusiness problems, and work on large and complex data sets\n\u2022 Coordinate weekly meetings with cross-functional teams (i.e. engineers, marketing teams) and clearly\ncommunicated analysis results to both technical and non-technical stakeholders\n\nBusiness Analyst Assistant\nPro (Procal) Management Consulting\nLos Angeles, CA\nMar. 2015 - Feb 2017\n\n\u25cf Utilized SQL to retrieve, clean, and manipulate the raw data of business operation information for franchise\nrestaurants; filtered out the valuable data, analyzed the data and formulate key summaries for senior management\nuse\n\u25cf Evaluate comparative industry market research and provide recommendation to clients\nSelina Yang\nSan Jose, CA\nselinayang326@gmail.com\n\n\u25cf Responsible for the company's periodic operational analysis and budget forecast; presented reports to supervisors\ninsights of the company's; developed marketing forecasts to help the company with investment decisions.""]",[u'MS in Engineering'],[u'University of Florida']
0,https://resumes.indeed.com/resume/0d50e3bedd6af4fe,"[u'Data Analyst\nDept. of Healthcare Policy and Research - New York, NY\nSeptember 2017 to Present\n\u2022 Used unsupervised learning methods to explore complex gene expression data structures\n\u2022 Implemented the Bayesian model averaging approach for identifying differentially expressed genes in association with a single or multiple patient level factors using RNA-seq gene expression data', u'Leader\nMain Risk Factors\nJanuary 2017 to December 2017\n\u2022 Led the whole team and made basic analysis framework\n\u2022 Analyzed the dataset concerning about maternal factors and the risk of low birth weight neonates using SAS\n\u2022 Selected variables using backward selection by comparing deviance of each model\n\u2022 Built univariate and multivariate logistic regression models with selected variables and dummy variables', u'New York City Air Quality - New York, NY\nJanuary 2017 to December 2017\n\u2022 Cleaned and organized the raw data and then constructed models finding association between Asthma and Cardiac diseases and air quality using R\n\u2022 Used R to plot the average air quality per neighborhood overlaid on a map of New York City.', u'Derivative Analyst\nChina International Capital Corporation Limited (CICC) - Beijing, CN\nSeptember 2016 to October 2016\n\u2022 Drafted weekly reports concerning Asset-Backed Securities (ABS) and researched market situation through past cases\n\u2022 Searched and analyzed various types of asset securitization cases through the WIND terminal and learned the different yields of primary, secondary, and subordinated bonds and bond ratings', u'Fixed Income Data Analyst\nHarvest Global Investments Limited - Beijing, CN\nJuly 2016 to August 2016\n\u2022 Made valuation: Discounted Cash Flow Valuation/Relative Valuation/Contingent Claim Valuation\n\u2022 Collected and updated data to support the research on macro study, convertible bonds, and debentures\n\u2022 Assisted researchers and analysts in building data models and writing analysis reports\n\nRELEVANT PROJECTS\nCreating the R package lmsrmy Feb. 2018\n\u2022 The package lmsrmy, was designed to give users a nice convenient way to generate their linear regression, ANOVA, non- parametric analysis and post-hoc analysis, especially for the one factor experimental study designs.\n\u2022 The package included shiny document for users to produce their interactive plots or tables analysis report.\n\u2022 The tables and plots relating to model assumptions test were generated and organized as well, which could be way more\nefficient for users to analyze.', u'Data Analyst\nMathematical Models Study (Matlab/Excel)\nSeptember 2015 to September 2015\n\u2022 Utilized MATLAB to measure the shadow length of the sun through the curve fitting\n\u2022 Employed the least square method combined with the known equation to establish a function model']","[u'M.S. in Biostatistics and Data Science', u'B.Ec. in Actuarial Science and Risk Management', u'']","[u'Cornell University New York, NY\nSeptember 2017 to July 2018', u'University of International Business and Economics Beijing, CN\nSeptember 2013 to July 2017', u'National College']"
0,https://resumes.indeed.com/resume/589ff4b4769895d6,"[u'Operations Supervisor\nJetBlue Airways Corporation - Kenner, LA\nPresent\n\u2022Provide leadership and technical support to Airport and Ground Operations Crewmembers\n\u2022Exemplify exceptional customer service\n\u2022Resolves and handles Customer and Crewmember conflicts\n\u2022Provide technical/operational support to Airports Operations Leaders and Crewmembers\n\u2022Report and provide operational feedback and performance to leadership either through verbal or written means of communication\n\u2022Participate in airport authority activities as well as attend regular airport meetings\n\u2022Responsible for safety programs at the station to ensure a safe workplace and operation for Crewmembers and customers -- responsible for meeting OSHA and local and state requirements\n\u2022Work with business partners to ensure all operational metrics are being met\n\u2022Perform under pressure and within fixed time constraints\n\u25e6Debrief Crewmembers daily and reports all information about shifts to Crewleader on duty including any maintenance issues or cancellations\n\u25e6Evaluate daily manpower, assign teams and ensures maximum coverage\n\u25e6Plan and coordinate Crewmember shift schedules and work assignments during irregular operations\n\u25e6Plan and coordinate Customers accommodations during irregular operations\n\u25e6Identify and implement operational improvements\n\u25e6Assist customers and perform day-to-day Crewmember functions as necessary\n\u25e6Ensure adherence to airport and aircraft security regulations and report any anomalies\n\u2022Work with Leadership to ensure Crewmembers are adhering to JetBlue Uniform Policy Manual Standards\n\u2022Take a significant role in the development of crewmembers to support the engagement, growth, goal achievement\n\u2022Other duties as assigned\n\u2022Is accountable to senior management for ensuring the safety and security of ground handling operations\n\u2022Ensures that policies and procedures are conducted in accordance with applicable federal regulations and standards', u'Data Analyst\nUniversal Health Plans - Union, NJ\nDecember 2008 to January 2010\n\u2022 Provided support for all hardware/applications including telephony and mobile devices\n\u2022 Provided excellent onsite and remote Customer Service for employees\n\u2022 Coordinated resolution of incidents/requests from beginning to end with internal and external resources.\n\u2022 Monitored and supported systems\n\u2022 Maintained and supported user access\n\u2022 Facilitated software package rollouts and maintenance\n\u2022 Collaborated with all Information Technology (IT) disciplines\n\u2022 Worked to achieve first-touch resolution with all issues\n\u2022 Maintained documentation of requests per standards\n\u2022 Maintained and updates knowledge base documentation with most relevant information\n\u2022 Coordinated response for major incidents and outages\n\u2022 Other duties as assigned']",[u'Some College in Business'],"[u'Baton Rouge Community College Baton Rouge, LA\nJanuary 2003 to January 2005']"
0,https://resumes.indeed.com/resume/c6b58a88759cd477,"[u""Data Analyst\nDallas Baptist University - Dallas, TX\nJuly 2016 to January 2018\nCo-ordinated with the Business Office, Immigration Department and Housing Office to accurately evaluate, report and analyze data.\n* Developed and optimized SQL queries to extract and validate data for daily reporting using SSRS\n* Reduced redundancy in the Business Office's reimbursement process by incorporating Macros\n* Analyzed historical data using tableau and optimized space utilization for Housing by 25% in a span of 8 months\n* Used various excel tools including VLOOKUP, Pivot Table, Index/Match, to evaluate and analyze historical recruitment data for various projections"", u'Assistant Financial Analyst\nB.M Investments Private Limited - Surat, Gujarat\nNovember 2014 to December 2015\nExtracted relevant information using data mining methods for financial reports and presentation. Provided key insights using quantitative research methods to managers. Analyzed financial activities using Excel modeling (Vlookup, Pivot Tables, Formulas, etc.) including the monthly close process, balance sheet account valuations and reconciliations.\n* Conducted fraud risk analysis to determine the nature of the fund (growth or value) and performance attribution, increasing margin by $1 million\n* Computed Net Asset Value (NAV) to the tune of $100K to 500K to accurately reflect the closing market prices\n* Developed detailed forecast models against budgets based on key indicators, increasing accuracy by 25%\n* Prepared ad hoc analysis and performance reports using Tableau to assist management team in evaluating projects\n* Ensured the integrity of reported revenues and expenses through analysis of trading positions\n* Conducted quantitative research and analysis on risk & return drivers specific to alternative investments']","[u'Master of Science in Management', u'Bachelor of Financial Studies in Financial Studies']","[u'Dallas Baptist University\nMay 2018', u'University of Mumbai\nMay 2014']"
0,https://resumes.indeed.com/resume/54a5d2f4dd654dde,"[u'Software Engineer\nWells Fargo (Diversant)\nMay 2017 to February 2018\n\u2022 Wrote Java programs that automated applications (web, native, and hybrid) using Selenium, Appium, Perfecto, and Applitools\n\u2022 Wrote a JavaScript program that sent requests to the JIRA API and parsed the response JSON object for data\n\u2022 Wrote a Python program to verify that CSV files had been properly obfuscated\n\u2022 Performed API testing using Postman and internal frameworks\n\u2022 Planned and tracked issues using JIRA\n\u2022 Configured Jenkins to execute test suites and perform error checks\n\u2022 Reported defects to improve quality of Wells Fargo applications', u'Data Analyst\nSilver Spring Networks\nJanuary 2013 to May 2014\n\u2022 Manipulated databases using SQL\n\u2022 Worked in a team of 2 to process data files for electric meters, relays, and access points', u'Data Analyst\nCatapultWorks\nMay 2012 to November 2012\n\u2022 Used SQL to query data from a database containing hundreds of thousands of records and compiled reports for clients\n\u2022 Performed data scrubbing using Excel, Mail Manager, Apex Data Loader, and Map Designer']","[u'B.S. in Computer Science', u'B.A. in Business Economics']","[u'University of California Irvine, CA\nSeptember 2014 to September 2016', u'University of California Santa Barbara, CA\nSeptember 2009 to June 2011']"
0,https://resumes.indeed.com/resume/62f3bd2cdbd731c0,"[u'Data Analyst\nWells Fargo Mortgage - Minneapolis, MN\nJanuary 2017 to Present\nProject Description:\nProject upgraded and automated the loan life cycle process, an integrated system that addressed all aspects of loan origination system right from customer initiation to loan closure. System included a credit evaluation and pricing engine that minimized foreclosure and defaults. Upgraded loan origination system enhanced the loan processing cycle, coordinated with the borrowers, and helped internal users achieve higher efficacy.\n\nResponsibilities\n\u2022 Working closely with data mapping SME and QA team to understand the business rules for acceptable data quality standards.\n\u2022 Performed data profiling on datasets with millions of rows on Teradata environment, validating key gen elements, ensuring correctness of codes and identifiers, and recommending mapping changes.\n\u2022 Wrote complex SQL queries to identify granularity issues and relationships between data sets and created recommended solutions based on analysis of the query results.\n\u2022 Performed unit testing on transformation rules to ensure data moved correctly.\n\u2022 Wrote the SQL queries on data staging tables and data warehouse tables to validate the data results.\n\u2022 Delivered Enterprise Data Governance, Data Quality, Metadata, and ETL Informatica solution\n\u2022 Maintained Excel workbooks, such as development of pivot tables, exporting data from external SQL databases, producing reports and updating spreadsheet information.\n\u2022 Researched and fixed data issues pointed out by QA team during regression tests.\n\u2022 Interfaced with business users to verify business rules and communicated changes to ETL development team.\n\u2022 created Tableau views with complex calculations and hierarchies making it possible to analyze and obtain insights into large data sets\n\u2022 Creating and executing SQL queries to perform Data Integrity testing on a Teradata Database to validate and test data using TOAD.\n\u2022 Worked with data architects team to make appropriate changes to the data models.\n\u2022 Worked on the ETL Informatica mappings and other ETL Processes (Data Warehouse)\n\u2022 Worked with the data governance team to ensure the data quality of compliance reports for EDI transactions.\n\u2022 Utilized Tableau server to publish and share the reports with the business users.\n\u2022 Experience in designing complex Drill-Down & amp; Drill-Through Reports using Business Objects.\n\u2022 Experience in creating UNIX scripts for file transfer and file manipulation.\n\u2022 Generate ad-hoc or management specific reports using Tableau and Excel.\n\u2022 Analysed the subscriber, provider, members and claim data to continuously scan and create authoritative master data.\n\u2022 Proficient with Tableau Framework to create Filters, Histograms, Parameters, Quick Filters\n\u2022 Assisted with validating hierarchies for data combination process.\n\u2022 Prepare the data rules spreadsheet using MS Excel that will be used to update allowed values, findings, and profiling results\nEnvironment: SQL, PL/SQL, Oracle 11g/10g, ERWIN, Quality Center, ETL Informatica, Business Intelligence Tableau, SQL Server, Teradata SQL Assistant, Excel, Visio, Erwin, UNIX, MDM, and Hyperion 11.x', u""Data Analyst\nU.S. Bank - St. Louis, MO\nMarch 2015 to October 2016\nProject Description:\nAt US Bank, the project involved developing an application on Home Loan Management System which supported the loan application process. The application, an online workflow system, monitored loan portfolios. The application processed loan application from potential clients, evaluated their eligibility, approved them and maintained the loans until they are closed. Our application supported the workflow technology in controlling and monitoring various steps in loan origination, reducing delays and inefficiencies in handling paper documents using digital imaging technology.\n\nResponsibilities:\n\u2022 Performed Data mapping, logical data modeling, created class diagrams and ER diagrams and used SQL queries to filter data\n\u2022 Worked with Asset Class owners to validate the results and configuration\n\u2022 Mapped Data Lineage for various source systems, right from data origination to results area\n\u2022 Worked with Market Risk and Operational Risk team which were part of a different initiative\n\u2022 Worked with Internal modeling team for data analysis and profiling\n\u2022 Designed and implemented basic SQL queries for testing and report/data validation\n\u2022 Developed a Testing format to compare the data at different systems\n\u2022 Worked with Data governance and quality\n\u2022 Actively participated in gathering requirements from various SMEs, Financial Analysts, Risk Analysts, and Portfolio Management via JAD/JAR sessions\n\u2022 Identify business rules for data migration and Perform data administration through data models and metadata.\n\u2022 Mined data from complex perspectives and summarized it into Excel\n\u2022 Designed reports using Tableau, tabular forms, Pivot tables and Charts on Excel\n\u2022 Writing complex SQL queries for checking the counts and for validating the data at field level\n\u2022 Designed and developed Use Cases, Activity Diagrams, Sequence Diagrams, OOD using UML\n\u2022 Identified, researched, investigated, analyzed, defined, and documented business processes and Use Case Scenarios\n\u2022 Performed ETL Informatica development task's like creating jobs using different stages, debugging etc\n\u2022 Data mapping, logical data modeling, created class diagrams and ER diagrams and used SQL queries to filter data within the Oracle database\n\u2022 Gathered data from the legacy system (db2) to the present updated system using data warehouse tools such as Informatica\n\u2022 Involved in strategic projects\n\u2022 Performed extensive Requirement Analysis including data analysis and gap analysis\n\u2022 Worked on data modeling and produced data mapping and data definition documentation\n\u2022 Functioned as the primary liaison between the business line, operations, and the technical areas throughout the project cycle. Worked with ODS and OLAP system\n\u2022 Conducted JAD sessions to resolve critical issues\n\u2022 Played a key role in the planning, testing, and implementation of system enhancements and conversions\nEnvironment: Informatica 8.6.1, Cognos, WinSQL, QTP 9.2, Quality Center 9.2, TOAD, Oracle 9i/10g, PL/SQL, IBM DB2, VBA MS Excel."", u'Data Analyst\nThe Private Bank - Chicago, IL\nJanuary 2014 to February 2015\nProject Description:\nProject upgraded Online Banking for Retail and Small Business customers by designing and offering new products to increase customer satisfaction, brand loyalty and a reliable product. New system embedded new functionalities such as register aliases, payment alerts, wire transfer and security features.\n\nResponsibilities\n\u2022 Lead the development, maintenance, and replication of various Global, Local and Regional Lists of sanctioned people and entities, which help Private Bank regional offices, maintain strict standards in terms of financial transactions\n\u2022 Development and tuning of the Optimization Script in the database, to reduce the rate of the false positive hits generated by the system and streamlining the overall screening and filtering process and improve the overall performance\n\u2022 Guided the automation of Gap Analysis process and preparation of Lists using MS Excel, Macros, MS Access, SQL and SAS\n\u2022 Profiling of the input data, of the new businesses within Private Bank, scheduled to come on board NESS (Name and Entity Search System), to assess issues with data quality, using DataFlux\n\u2022 Designed the framework for the DataFlux Architect and Data Flux Profiler jobs for the extraction of data from various data sources flat files, MS SQL Server, Oracle, Excel and COBOL Copybook files\n\u2022 Development of a reporting solution for data profiling, using DataFlux, MS SQL Server, Business Objects\n\u2022 Designed the test cases, test scripts and coordination of testing activities for various NESS application releases.\n\u2022 Provided analysis reports using Excel & Cognos\n\u2022 Involved in development and maintenance of Hot Lists and SDN Lists and as requested by OFAC (Organization of Foreign Assets Control)\n\u2022 Documentation of procedures for creation, analysis, and maintenance of various Global, Local and Regional Lists\n\u2022 Provide Business as Usual (BAU) support and Production support for the NESS application and perform data analysis\n\nEnvironment- Oracle 10g/11, MS SQL Server 2005/2008, Sybase 12.5/15.x, IBM DB2, Toad, QlikView, Business Objects, SQL Navigator, SQL Profiler, Windows Vista, MS Excel, Unix Shells Scripts.', u'Data Analyst\nJPMorgan Chase - Chicago, IL\nMay 2012 to November 2013\nProject Description:\nProject involved developing a Commercial Lending product that provided end to end processes for covering four broad categories of activities such as: origination of primary products, marketing of secondary products, management of the loan portfolio and servicing of loans. The high-level processes in commercial lending are also woven around these four activity segments.\n\nResponsibilities:\n\u2022 Analysis of functional and non-functional categorized data elements for data profiling and mapping from source to target data environment. Developed working documents to support findings and assign specific tasks\n\u2022 Involved with data profiling for multiple sources and answered complex business questions by providing data to business users\n\u2022 Worked with data investigation, discovery, and mapping tools to scan every single data record from many sources\n\u2022 Performed data mining on claims data using very complex SQL queries and discovered claims pattern\n\u2022 Created DML code and statements for underlying & impacting databases\n\u2022 Extensively used ETL methodology for supporting data extraction, transformations and loading processing, in a complex EDW using Informatica\n\u2022 Perform data reconciliation between integrated systems\n\u2022 Metrics reporting, data mining and trends in helpdesk environment using Access\n\u2022 Written complex SQL queries for validating the data against different kinds of reports generated by Business Objects XIR2\n\u2022 Extensively used MS Access to pull the data from various data bases and integrate the data\n\u2022 Performed data analysis and data profiling using complex SQL on various sources systems including Oracle and Teradata\n\u2022 Responsible for different Data mapping activities from Source systems to Teradata\n\u2022 Assisted in the oversight for compliance to the Enterprise Data Standards\n\u2022 Worked in importing and cleansing of data from various sources like Teradata, Oracle, flat files, SQL Server 2005 with high volume data\n\u2022 Worked with Excel Pivot tables\n\u2022 Worked closely with organization Cognos developer and reporting team\n\u2022 Create and Monitor workflows using workflow designer and workflow monitor\n\u2022 Performing data management projects and fulfilling ad-hoc requests according to user specifications by utilizing data management software programs and tools like Perl, Toad, MS Access, Excel, and SQL\n\u2022 Written SQL scripts to test the mappings and Developed Traceability Matrix of Business Requirements mapped to Test Scripts to ensure any Change Control in requirements leads to test case update\n\u2022 Involved in extensive DATA validation by writing several complex SQL queries and Involved in back-end testing and worked with data quality issues\n\u2022 Developed regression test scripts for the application and Involved in metrics gathering, analysis and reporting to concerned team and Tested the testing programs\n\u2022 Identify & record defects with required information for issue to be reproduced by development team\n\u2022 Flexible to work late hours to coordinate with offshore team\n\nEnvironment: Quality Center, MS Excel, PL/SQL, Cognos, Informatica, Oracle 8i, Teradata, Teradata SQL Assistant.']","[u""Master's""]","[u'information systems Murray State University Murray, KY']"
0,https://resumes.indeed.com/resume/c627aadc67abfda3,"[u'Data Reporting Analyst\nHealth Media Network - Darien, CT\nJune 2017 to Present\nHealth Media Network is the fastest growing digital Point of Care media company in the U.S., reaching more than 60,000 physicians in over 13,000 medical offices and healthcare systems across the country. HMN offers 30 specialty health networks that educate and empower patients to facilitate better conversations with their doctors.\n\u2022 Maintained and developed SQL queries, stored procedures, views, functions and reports using Microsoft SQL Server.\n\u2022 Created, managed, and delivered interactive web-based reports to support daily operations.\n\u2022 Successfully deployed SSIS Package into Production environment and used Package configuration to export various package properties to make package environment independent.\n\u2022 Developed Scheduled Jobs from SQL Agent to automate the packages.\n\u2022 Implemented Event Handlers and Error Handling in SQL Server Integration Services (SSIS) packages and notified process results to various user communities.\n\u2022 Created Report through SQL Server Reporting Services (SSRS).\n\u2022 Played a vital role in cleansing and validating the data.\n\u2022 Managed, updated and manipulated report orientation and structures with the use of advanced Excel functions including Pivot Tables and VLOOKUP.', u'Data Analyst\nArun Pharma - Hyderabad, Telangana\nFebruary 2015 to December 2015\n\u2022 Responsible for maintaining the integrity of the SQL database and reporting any issues.\n\u2022 Assisted in mining data from the SQL database that was used in several significant presentations.\n\u2022 Managed, updated and manipulated report orientation and structures with the use of advanced Excel functions including Pivot Tables.\n\u2022 Develop and implemented standard operating procedures to bridge data gaps and resolve related issues.']","[u'Masters in Healthcare Informatics in Healthcare Informatics', u'Bachelor of Pharmacy in Pharmacy']","[u'Sacred Heart University\nMay 2017', u'JNTU Hyderabad, Telangana\nMay 2015']"
0,https://resumes.indeed.com/resume/317fd1becb3664a7,"[u'Data Analyst\nAccess Insurance - Dunwoody, GA\nJune 2017 to January 2018\nI learned SQL, Access, Excel, and R Studio. I helped put together reports under the senior Data Analyst. I would retrieve the data, manipulate the desired data, and then construct it in an easy to read format for the Vice President of Underwriting. I prepared a report on Mapping with R studio to train the other analysts before my internship ended in January. Mapping with R studio allows the other analysts to quickly retrieve auto insurance data based on any desired criteria for any state they operated in.']","[u'Bachelor of Science in Chemistry minor Mathematics', u'Diploma']","[u'Alabama State University Montgomery, AL\nMay 2017', u'Parkview High School Lilburn, GA\nMay 2013']"
0,https://resumes.indeed.com/resume/6c87918a185b8f2f,[u'Data Analyst\nSchneider Electric - Beijing\nMarch 2017 to June 2017'],"[u'M.S. in Statistics', u'B.S. in Mathematics']","[u'Boston University Boston, MA\nSeptember 2017 to December 2018', u'China Agricultural University BEIJING, CN\nSeptember 2013 to June 2017']"
0,https://resumes.indeed.com/resume/b10ee90a7206d9a1,"[u'Data Analyst\nSbase Technologies\nJanuary 2016 to Present\nResponsibility:\n\u2022 Involved in Requirements gathering\n\u2022 Involved in Analysis\n\u2022 User Interface Design\n\u2022 Involved in Development\n\u2022 Database mappings\n\u2022 Expertise In IT with exposure in Client Server and Web Development.\n\u2022 Creating Dynamic/Interactive web pages using Internet Tools\n\u2022 Proficient in RDBM S environment SQL Server.\n\u2022 Coded for reading Text files and storing into SQL Server as records.\n\u2022 Final Reports generation into Spread Sheet.\n\u2022 Storing images into the database as a binary format.\n\u2022 Developed the user interfaces of the application using ASP.NET Server Controls, HTML Controls, XHTML, Java Script and CSS.\n\u2022 Designed and developed Customer registration and login screens using HTML, and JavaScript.\n\u2022 Used Client Side Scripts using JavaScripts.\n\u2022 Involved in SDLC of an application developed using AGILE methodology\n\u2022 Used Cascading Style Sheets (CSS), HTML to attain uniformity of all web pages\n\u2022 Good Understanding of .NET security features such as Authentication & Authorization, Windows-based Authentication, Forms-based Authentication, Authorizing Users and Roles.\n\u2022 Good in-depth understanding and experience in building server applications using VB.NET, C#, ASP.Net, ADO.Net, XML, Web Services, HTTP modules and handlers based on .Net Framework4.0/3.5/3.0/2.0.\n\u2022 Excellent analytical and proven problem solving skills.\n\u2022 Capable of quickly learning and delivering solutions as an individual and as part of a team.\n\u2022 Strong team player, ability to work independently and in a team as well, ability to adapt to a rapidly changing environment, commitment towards learning.\n\u2022 Ability to work in challenging and versatile environments and Self-motivated, excellent written/verbal communication, Organizational skills combined with attention to detail and time management skills.\n\u2022 Team Player with good technical, analytical and communication skills\n\u2022 Hands on experience in design using Object Oriented Programming Principles like Inheritance, Polymorphism.']",[],[]
0,https://resumes.indeed.com/resume/97c2c50872f2e974,"[u""Reporting Data Analyst\nUnited Healthcare - Fort Worth, TX\nJanuary 2017 to January 2018\nContract)\n\u25cf Supported the company's healthcare software functions, implementation and training.\n\u25cf Make business recommendations based on data collected to improve business efficiency.\n\u25cf Collected data from all clients and made recommendations for improvement.\n\u25cf Ad-hoc data extraction through queries in SQL Server 2008, Excel or PDF.\n\u25cf Supported business lead for special assignment and to ensure production efficiency.\n\u25cf Interpreting data, analyzing results using statistical techniques and provide ongoing reports and feedback.\n\u25cf Providing timely, accurate and reliable management reports.\n\u25cf Data cleaning-identifying erroneous also determining appropriate resolution."", u""Data Analyst\nAids Arms, Inc - Dallas, TX\nJune 2015 to November 2016\nContract)\n\u25cf Supported, manage, and maintain organizations clinical data in databases in SQL Server.\n\u25cf Acted as an in-house Business Analyst to facilitate teams of Data and Business Analysts.\n\u25cf Initiated meeting as a lead to understand / gather requirements and document processes.\n\u25cf Maintained patients' information within in-house database systems (Clear Health & Aries).\n\u25cf Reported on periodical basis for federal funding and grants.\n\u25cf Ad-hoc reporting and querying for the staff data requests though SQL Server 2005, 2008 and 2012.\n\u25cf Prepared user guides and training manuals for the new users and clients\n\u25cf Weekly analysis of the new data entered in database."", u'Inventory Analyst\nElwood Staffing - Arlington, TX\nNovember 2013 to March 2015\nContract)\n\u25cf Identified, collected, and analyzed inventory data to determine correct inventory composition, level and location.\n\u25cf Audited materials management processes to ensure compliance with inventory policies and customer requirements.\n\u25cf Presented daily analysis of performance measurements to senior and executive managers, processed purchase orders, received product and maintained a daily balance with accuracy in our physical and virtual inventory levels.\n\u25cf Used SQL and Excel to pinpoint data discrepancies, investigate fraudulent orders, and solve crucial problems with orders.']","[u'Bachelors of science in information Technology', u'associates of Arts in CVS pharmacy']","[u'Stratford University, , Stratford University\nJanuary 2015', u""Prince George's community college""]"
0,https://resumes.indeed.com/resume/ca9f9727252a9554,"[u'Data Marketing Analyst Intern\nSummer Youth Employment Program - Bronx, NY\nJuly 2017 to Present\nat Roman & Roman Cleaning Services\nBilly Gene Marketing\nMicrosoft, Excel, PowerPoint experience\nLearned how to create a database on gmail\nCreated a website (HTML)\nLearned how to make a persuasive email\nObtained knowledge on how to make Facebook messenger ads and sponsored messages\nGained photoshop experience']",[u''],"[u'A.Phillip Randolph High School New York, NY\nJanuary 2014 to Present']"
0,https://resumes.indeed.com/resume/9af0b10c9364ef9c,"[u'Data Analyst Intern\nChemical Abstract Services - Columbus, OH\nMay 2017 to August 2017\nUsed Apache Hadoop to store and process customer communications data and performed exploratory data\nanalysis by using R, Python, Tableau, and Vos viewer.\n\u2022 Presented data visualizations, which highlighted key areas for cost savings up to 10%, to members of top\nmanagement.', u'Data Analyst Intern\nGeneral Electric - Bengaluru, Karnataka\nJune 2015 to August 2015\n\u2022 Optimized data collection methods and used Microsoft Excel software to analyze the data.\n\u2022 Interpreted data and used statistical techniques such as hypothesis testing to validate the interpretations.\n\u2022 Presented final report, which contained visualizations, recommendations and scope for future work to the head\nof the Indian team resulting in positive feedback.']","[u'Master of Science in Engineering & Technology Innovation Management in Engineering & Technology Innovation Management', u'in Technology']","[u'Carnegie Mellon University Pittsburgh, PA\nMay 2018', u'Sri Sivasubramaniya Nadar College of Engineering Chennai, Tamil Nadu\nMay 2016']"
0,https://resumes.indeed.com/resume/7cf775eb61012db1,"[u'Data Analyst\nLamsa - Amman, JO\nNovember 2016 to August 2017\nLamsa is a mobile elearning application. At Lamsa I have worked so far on:\n\u25cf Creating weekly report for marketing managers.\n\u25cf Update main data sheet on daily basis which requires going through different resources such as (Leanplum, Mix Panel, Google Developer Console, iTunes Reports, Local Database)\n\u25cf Conclude insights from data and recommend actions based on it.\n\u25cf Dig deep in data to identify reasons of drops and strikes.\n\u25cf create analytical reports including Churn Rates, Retention Rates, Customer LifeTime Value, Various Cohorts.\n\u25cf Cooperate with developer and QA to achieve best tracking on user behaviour on the application.\n\u25cf work closely with developers on integration testing and scenarios.', u""Financial Reports Developer\nMacy's Inc\nJanuary 2016 to June 2016\nAs a staff reports developer I was responsible of the following:\n\u25cf Modify/Create reports using Hyperion Financial Reports Studio.\n\u25cf Modify PL/SQL codes to fit Manager's' Requests.\n\u25cf Create Microsoft Access forms to help managers retrieve necessary data.\n\u25cf Communicate with DBA team to create tickets to migrate reports from Dev. environment to production."", u'Data Analyst\nAugust 2015 to January 2016\nI took care of the following:\n\u25cf Create scorecards for the supervisors to help coaching their agents. This is done using advanced Excel formulas and VBA code.\n\u25cf Automate reports using VBA code that made report update data and refresh pivot tables automatically.\n\u25cf Create simple forecasting reports to help supervisor expect when they will achieve the goal using Excel add-in (QM4).', u'Data Analyst\nSouq Group\nApril 2012 to July 2015\nI had the following responsibilities as a data analyst:\n\u25cf Generate ad hoc reports for all the departments (finance, Operations, marketing, etc )\n\u25cf Send Reports on Quarterly, Monthly, Weekly and Daily basis to the Management.\n\u25cf Work with the developers to fix bugs and issues found during Data Reporting to maintain data accuracy.\n\u25cf Create automated reports using MS Excel macros that read from database.\n\u25cf Create reports on Pentaho BI user console.\n\u25cf Worked on Reconciliation Projects (with Finance Team)\n\u25cf Pull data from the MySQL Database directly and from various resources such as Google analytics, Adobe Omniture, Silverpop mailing Service, Mailchimp Mailing Service, and other\n\u25cf Maintain Reports Data Accuracy through comparison, test cases and sample testing.\n\u25cf Work Closely with Finance Department to Generate Monthly closing Reports.\n\u25cf Conduct workarounds to come up with reports on time and in the required formats.\n\u25cf Create ETLs using Pentaho Data Integration kettle to create reports that require various steps to be prepared, and get them automated.']","[u""Master's in Management Information Systems"", u""Bachelor's in Computer science""]","[u'Amman Arab University\nOctober 2012 to February 2015', u'Balqa Applied University\nSeptember 2008 to January 2012']"
0,https://resumes.indeed.com/resume/19052f641b3e7317,"[u""Data Scientist/ Machine Learning\nFIS - Jacksonville, FL\nJanuary 2017 to Present\nDescription: FIS provides financial software, world-class services and global business solutions. Let us help you compete and win in today's chaotic marketplace. Fidelity National Information Services Inc., better known by the abbreviation FIS, is an international provider of financial services technology and outsourcing services. FIS is the world's largest global provider dedicated to financial technology solutions. FIS empowers the financial world with software, services, consulting and outsourcing solutions focused on retail and institutional banking, payments, asset and wealth management, risk and compliance, trade enablement, transaction processing and record-keeping.\n\nResponsibilities:\n\u2022 Extracted data from HDFS and prepared data for exploratory analysis using data munging\n\u2022 Built models using Statistical techniques like Bayesian HMM and Machine Learning classification models like XGBoost, SVM, and Random Forest.\n\u2022 Participated in all phases of data mining, data cleaning, data collection, developing models, validation, visualization and performed Gap analysis.\n\u2022 A highly immersive Data Science program involving Data Manipulation&Visualization, Web Scraping, Machine Learning, Python programming, SQL, GIT, MongoDB, Hadoop.\n\u2022 Setup storage and data analysis tools in AWS cloud computing infrastructure.\n\u2022 Installed and used Caffe Deep Learning Framework\n\u2022 Worked on different data formats such as JSON, XML and performed machine learning algorithms in Python.\n\u2022 Worked as Data Architects and IT Architects to understand the movement of data and its storage and ER Studio 9.7\n\u2022 Used pandas, numpy, seaborn, matplotlib, scikit-learn, scipy, NLTK in Python for developing various machine learning algorithms.\n\u2022 Data Manipulation and Aggregation from different source using Nexus, Business Objects, Toad, Power BI and Smart View.\n\u2022 Implemented Agile Methodology for building an internal application.\n\u2022 Focus on integration overlap and Informatica newer commitment to MDM with the acquisition of Identity Systems.\n\u2022 Coded proprietary packages to analyze and visualize SPCfile data to identify bad spectra and samples to reduce unnecessary procedures and costs.\n\u2022 Programmed a utility in Python that used multiple packages (numpy, scipy, pandas)\n\u2022 Implemented Classification using supervised algorithms like Logistic Regression, Decision trees, Naive Bayes, KNN.\n\u2022 As Architect delivered various complex OLAPdatabases/cubes, scorecards, dashboards and reports.\n\u2022 Updated Python scripts to match training data with our database stored in AWS Cloud Search, so that we would be able to assign each document a response label for further classification.\n\u2022 Used Teradata utilities such as Fast Export, MLOAD for handling various tasks data migration/ETL from OLTP Source Systems to OLAP Target Systems\n\u2022 Data transformation from various resources, data organization, features extraction from raw and stored.\n\u2022 Validated the machine learning classifiers using ROC Curves and Lift Charts.\n\nEnvironment:Unix, Python 3.5.2, MLLib, SAS, regression, logistic regression, Hadoop 2.7.4, NoSQL, Teradata, OLTP, random forest, OLAP, HDFS, ODS, NLTK, SVM, JSON, XML and MapReduce."", u""Data Scientist\nCBRE - Dallas, TX\nOctober 2015 to November 2016\nDescription: CBRE Group, Inc. is the largest commercial real estate services and investment firm in the world. It is based in Los Angeles, California and operates more than 450 offices worldwide and has clients in more than 100 countries.\n\nResponsibilities:\n\u2022 Utilized Spark, Scala, Hadoop, HQL, VQL, oozie, pySpark, Data Lake, TensorFlow, HBase, Cassandra, Redshift, MongoDB, Kafka, Kinesis, Spark Streaming, Edward, CUDA, MLLib, AWS, Python, a broad variety of machine learning methods including classifications, regressions, dimensionally reduction etc.\n\u2022 Utilized the engine to increase user lifetime by 45% and triple user conversations for target categories.\n\u2022 Application of various machine learning algorithms and statistical modeling like decision trees, text analytics, natural language processing (NLP), supervised and unsupervised, regression models, social network analysis, neural networks, deep learning, SVM, clustering to identify Volume using scikit-learn package in python, Matlab.\n\u2022 Worked onanalyzing data from Google Analytics, AdWords, Facebook etc.\n\u2022 Evaluated models using Cross Validation, Log loss function, ROC curves and used AUC for feature selection and elastic technologies like ElasticSearch, Kibana.\n\u2022 Performed Data Profiling to learn about behavior with various features such as traffic pattern, location, Date and Time etc.\n\u2022 Categorized comments into positive and negative clusters from different social networking sites using Sentiment Analysis and Text Analytics\n\u2022 Performed Multinomial Logistic Regression, Decision Tree, Random forest, SVM to classify package is going to deliver on time for the new route.\n\u2022 Performed data analysis by using Hive to retrieve the data from Hadoop cluster, Sql to retrieve datafrom Oracle database and used ETL for data transformation.\n\u2022 Performed Data Cleaning, features scaling, features engineering using pandas and numpy packages in python.\n\u2022 Exploring DAG's, their dependencies and logs using AirFlow pipelines for automation\n\u2022 Performed data cleaning and feature selection using MLlib package in PySpark and working with deep learning frameworks such as Caffe, Neon.\n\u2022 Developed Spark/Scala, R Python for regular expression (regex) project in the Hadoop/Hive environment with Linux/Windows for big data resources.\n\u2022 Used clustering technique K-Means to identify outliers and to classify unlabeled data.\n\u2022 Tracking operations using sensors until certain criteria is met using AirFlowtechnology.\n\u2022 Responsible for different Data mapping activities from Source systems to Teradata using utilities like TPump, FEXP, BTEQ, MLOAD, FLOADetc\n\u2022 Analyze traffic patterns by calculating autocorrelation with different time lags.\n\u2022 Ensured that the model has low False Positive Rate and Text classification and sentiment analysis for unstructured and semi-structured data.\n\u2022 Addressed over fitting by implementing of the algorithm regularization methods like L1 and L2.\n\u2022 Used Principal Component Analysis in feature engineering to analyze high dimensional data.\n\u2022 Used MLlib, Spark's Machine learning library to build and evaluate different models.\n\u2022 Implemented rule based expertise system from the results of exploratory analysis and information gathered from the people from different departments.\n\u2022 Created and designed reports that will use gathered metrics to infer and draw logical conclusions of past and future behavior.\n\u2022 Developed MapReduce pipeline for feature extraction using Hive and Pig.\n\u2022 Created Data Quality Scripts using SQL and Hive to validate successful data load and quality of the data. Created various types of data visualizations using Python and Tableau.\n\u2022 Communicated the results with operations team for taking best decisions.\n\u2022 Collected data needs and requirements by Interacting with the other departments.\n\nEnvironment:Python 2.x, CDH5, HDFS, Hadoop 2.3, Hive, Impala, AWS, Linux, Spark, Tableau Desktop, SQL Server 2014, Microsoft Excel, Matlab, Spark SQL, Pyspark."", u'Data Analyst\nWalgreens - Deerfield, IL\nDecember 2013 to September 2015\nDescription:The Walgreens Companyis an American company that operatesas the second-largest pharmacy store chain in the United States. It specializes in filling prescriptions, health and wellness products, health information, and photo services\n\nResponsibilities:\n\u2022 Worked with BI team in gathering the report requirements and also Sqoop to export data into HDFS and Hive\n\u2022 Involved in the below phases of Analytics using R, Python and Jupyter notebook.\n\u2022 a. Data collection and treatment: Analysed existing internal data and external data, worked on entry errors, classification errors and defined criteria for missing values\nb. Data Mining: Used cluster analysis for identifying customer segments, Decision trees used for profitable and non-profitable customers, Market Basket Analysis used for customer purchasing behaviour and part/product association.\n\u2022 Developed multiple Map Reduce jobs in Java for data cleaning and preprocessing.\n\u2022 Assisted with data capacity planning and node forecasting.\n\u2022 Installed, Configured and managed Flume Infrastructure\n\u2022 Administrator for Pig, Hive and HBase installing updates patches and upgrades.\n\u2022 Worked closely with the claims processing team to obtain patterns in filing of fraudulent claims.\n\u2022 Worked on performing major upgrade of cluster from CDH3u6 to CDH4.4.0\n\u2022 Developed Map Reduce programs to extract and transform the data sets and results were exported back to RDBMS using Sqoop.\n\u2022 Patterns were observed in fraudulent claims using text mining in R and Hive.\n\u2022 Exported the data required information to RDBMS using Sqoop to make the data available for the claims processing team to assist in processing a claim based on the data.\n\u2022 Developed Map Reduce programs to parse the raw data, populate staging tables and store the refined data in partitioned tables in the EDW.\n\u2022 Created tables in Hive and loaded the structured (resulted from Map Reduce jobs) data\n\u2022 Using HiveQL developed many queries and extracted the required information.\n\u2022 Created Hive queries that helped market analysts spot emerging trends by comparing fresh data with EDW reference tables and historical metrics.\n\u2022 Was responsible for importing the data (mostly log files) from various sources into HDFS using Flume\n\u2022 Enabled speedy reviews and first mover advantages by using Oozie to automate data loading into the Hadoop Distributed File System and PIG to pre-process the data.\n\u2022 Provided design recommendations and thought leadership to sponsors/stakeholders that improved review processes and resolved technical problems.\n\u2022 Managed and reviewed Hadoop log files.\n\u2022 Tested raw data and executed performance scripts.\n\nEnvironment:HDFS, PIG, HIVE, Map Reduce, Linux, HBase, Flume, Sqoop, R, VMware, Eclipse, Cloudera, Python.', u""Data Scientist\nJohnson and Johnson - Raritan, NJ\nSeptember 2012 to November 2013\nDescription:Transamerica is the US-based brand of Aegon, a Dutch financial services firm. Aegon is one of the world's leading providers of life insurance, pensions and asset management and is helping approximately 30 million customers globally to achieve a lifetime of financial security.\n\nResponsibilities:\n\u2022 Manipulating, cleansing & processing data using Excel, Access and SQL.\n\u2022 Responsible for loading, extracting and validation of client data.\n\u2022 Modelled clean data into the Kafka servers for use over the spark engine.\n\u2022 Zookeeper along with Kafka was used to stream data and end-to-end client communication.\n\u2022 Performed transformations over the warehoused data using Scala& Python and modelled the data back into the servers for iterative transformations into KAFKA.\n\u2022 Modelled data using Machine learning libraries(Sci-kit learn) apart from SVN and KNN based classificationto create a training dataset for use in a predictive model.\n\u2022 Liaising with end-users and 3rd party suppliers.\n\u2022 Analyzing raw data, drawing conclusions & developing recommendations\n\u2022 Writing T-SQL scripts to manipulate data for data loads and extracts.\n\u2022 Developing data analytical databases from complex financial source data.\n\u2022 Performing daily system checks.\n\u2022 Data entry, data auditing, creating data reports & monitoring all data for accuracy.\n\u2022 Designing, developing and implementing new functionality.\n\u2022 Monitoring the automated loading processes.\n\u2022 Advising on the suitability of methodologies and suggesting improvements.\n\u2022 Carrying out specified data processing and statistical techniques.\n\u2022 Supplying qualitative and quantitative data to colleagues & clients.\n\u2022 Using Informatica& SAS to extract, transform & load source data from transaction\nsystems.\n\u2022 Performed sequential analytics using SAS Enterprise miner using jobs fed by the SAS Grid Manager.\n\u2022 Loaded packages and stored procedures using Base SAS and integrated functional and business requirements using the EBI suite.\n\u2022 Creating data pipelines using big data technologies like Hadoop, spark etc.\n\u2022 Creating statistical models using distributed and standalone models to build various diagnostics, predictive and prescriptive solution.\n\u2022 Utilize a broad variety of statistical packages like SAS, R , MLIB, Graphs, Hadoop, Spark , MapReduce, Pig\n\u2022 Performed a check using quality parameters fed using the SAS QC engine.\n\u2022 Created a UI dashboard for end users and performed prototype testing using Tableau.\n\u2022 Refine and train models based on domain knowledge and customer business objectives\n\u2022 Deliver or collaborate on delivering effective visualizations to support the client business objectives\n\u2022 Communicate to your peers and managers promptly as and when required.\n\u2022 Produce solid and effective strategies based on accurate and meaningful data reports and analysis and/or keen observations.\n\u2022 Establish and maintain communication with clients and/or team members; understand needs, resolve issues, and meet expectations\n\u2022 Developed web applications using .net technologies; work on bug fixes/issues that arise in the production environment and resolve them at the earliest\n\nEnvironment:Cloudera, HDFS, Pig, Hive, Map Reduce, python, Sqoop, Storm, Kafka, LINUX, Hbase, Impala, Java, SQL, Cassandra, MongoDB, SVN."", u'Data Architect/Data Modeler\nInfotech Enterprises IT Services Pvt. Ltd - Hyderabad, Telangana\nDecember 2010 to August 2012\nDescription: Infotech Enterprises IT Services Pvt. Ltd. (Infotech IT), part of the $ 260 million Infotech Enterprises group, leverages its business process knowledge, technological competence, strategic alliances and strong global presence to offer innovative IT solutions to the Retail Industry.\n\nResponsibilities:\n\u2022 Worked with large amounts of structured and unstructured data.\n\u2022 Knowledge in Machine Learning concepts (Generalized Linear models, Regularization, Random Forest, Time Series models, etc.)\n\u2022 Worked in Business Intelligence tools and visualization tools such as Business Objects, Tableau, ChartIO, etc.\n\u2022 Deployed GUI pages by using JSP, JSTL, HTML, DHTML, XHTML, CSS, JavaScript, AJAX.\n\u2022 Configured the project on WebSphere 6.1 application servers\n\u2022 Implemented the online application by using Core Java, Jdbc, JSP, Servlets and EJB 1.1, Web Services, SOAP, WSDL.\n\u2022 Handled end-to-end project from data discovery to model deployment.\n\u2022 Monitoring the automated loading processes.\n\u2022 Communicated with other Health Care info by using Web Services with the help of SOAP, WSDLJAX-RPC\n\u2022 Used Singleton, factory design pattern, DAO Design Patterns based on the application requirements\n\u2022 Used SAX and DOM parsers to parse the raw XML documents\n\u2022 Used RAD as Development IDE for web applications.\n\u2022 Preparing and executing Unit test cases\n\u2022 Used Log4J logging framework to write Log messages with various levels.\n\u2022 Involved in fixing bugs and minor enhancements for the front-end modules.\n\u2022 Implemented Microsoft Visio and Rational Rose for designing the Use Case Diagrams, Class model, Sequence diagrams, and Activity diagrams for SDLC process of the application\n\u2022 Doing functional and technical reviews\n\u2022 Maintenance in the testing team for System testing/Integration/UAT.\n\u2022 Guaranteeing quality in the deliverables.\n\u2022 Conducted Design reviews and Technical reviews with other project stakeholders.\n\u2022 Was a part of the complete life cycle of the project from the requirements to the production support\n\u2022 Created test plan documents for all back-end database modules\n\u2022 Implemented the project in Linux environment.\n\nEnvironment: R 3.0, Erwin 9.5, Tableau 8.0, MDM, QlikView, MLLib, PL/SQL, HDFS, Teradata 14.1, JSON, HADOOP (HDFS), MapReduce, PIG, Spark, R Studio, MAHOUT, JAVA, HIVE, AWS.', u""Data Analyst/ Data Modeler\nGlobalLogic Technologies - Hyderabad, Telangana\nApril 2009 to November 2010\nDescription:GlobalLogic provides experience design, Digital Product Engineering Services, and Agile Software Development to the world's top brands by leveraging UX UI Design, next-gen technologies, and cloud software, with end-to-end solution by the best Software Development Company.\n\nResponsibilities:\n\u2022 Developed Internet traffic scoring platform for ad networks, advertisers and publishers (rule engine, site scoring, keyword scoring, lift measurement, linkage analysis).\n\u2022 Responsible for defining the key identifiers for each mapping/interface.\n\u2022 Clients include eBay, Click Forensics, Cars.com, Turn.com, Microsoft, and Looksmart.\n\u2022 Designed the architecture for one of the first analytics 3.0. online platforms: all-purpose scoring, with on-demand, SaaS, API services. Currently under implementation.\n\u2022 Web crawling and text mining techniques to score referral domains, generate keyword taxonomies, and assess commercial value of bid keywords.\n\u2022 Developed new hybrid statistical and data mining technique known as hidden decision trees and hidden forests.\n\u2022 Reverse engineering of keyword pricing algorithms in the context of pay-per-click arbitrage.\n\u2022 Implementation of Metadata Repository, Maintaining Data Quality, Data Cleanup procedures, Transformations, Data Standards, Data Governance program, Scripts, Stored Procedures, triggers and execution of test plans\n\u2022 Performed data quality in Talend Open Studio.\n\u2022 Coordinated meetings with vendors to define requirements and system interaction agreement documentation between client and vendor system.\n\u2022 Automated bidding for advertiser campaigns based either on keyword or category (run-of-site) bidding.\n\u2022 Creation of multimillion bid keyword lists using extensive web crawling. Identification of metrics to measure the quality of each list (yield or coverage, volume, and keyword average financial value).\n\u2022 Enterprise Metadata Library with any changes or updates.\n\u2022 Document data quality and traceability documents for each source interface.\n\u2022 Establish standards of procedures.\n\u2022 Generate weekly and monthly asset inventory reports.\n\nEnvironment: Erwin r7.0, SQL Server 2000/2005, Windows XP/NT/2000, Oracle 8i/9i, MS-DTS, UML, UAT, SQL Loader, OOD, OLTP, PL/SQL, MS Visio, Informatica.""]","[u""Bachelor's""]",[u'']
0,https://resumes.indeed.com/resume/9ab4e5772375099c,"[u'Rating Analyst\nS&P Global/McGraw Hill Financial - Centennial, CO\nMay 2017 to Present\nCollateralized Loan Obligation (CLO) team\n\u2022 Appointed as the Problem Solving Coordinator who facilitates the team\u2019s problem solving and improvement initiatives\no Plan and run weekly problem solving meeting with team\no Coordinate with outside groups and upper management regarding initiatives\no Track all phases of project throughout lifespan using KaiNexus (continuous improvement software)\n\u2022 Perform quantitative analysis using financial models and Excel based tools\no Cash Flow Evaluator- model expected investment cash flows while considering defaults and economic movements\no CDO Evaluator (Monte Carlo simulation)- used to model the probability of different outcomes in a process that cannot easily be predicted due to the intervention of random variables\n\u2022 Cross collaborate with numerous teams both internally and externally including domestic and overseas\n\u2022 Work in a changing environment with the new regulatory rollouts for Nationally Recognized Statistical Rating Organizations (NRSROs) as part of the Dodd-Frank Wall Street Reform and Consumer Protection Act', u""Rating Analyst\nS&P Global/McGraw Hill Financial - Centennial, CO\nJanuary 2015 to May 2017\nAccomplishments\n\u2022 Received Structured Finance Recognition Award for exceptional contributions to the team within 3 months of starting my role\n\u2022 Built two financial models for the Servicer Evaluations team\n\u2022 Appointed as team's Problem Solving Champion/Coordinator\n\nTasks\nMortgage Servicer Evaluation team\n\u2022 Audit, assess, and rank commercial and residential mortgage servicers as part of the Servicer Evaluations team within a highly regulated rating agency\no Conduct management meetings and on-site audits with top level management\no Evaluate management and operational data to reach a ranking conclusion to be published\no Analyze administrative documentation surrounding policies/procedures, audits, insurance information, and the Servicer Evaluation Analytical Methodology (SEAM) questionnaire\no Prepare and present post-analysis ranking recommendation to the team for a consensus vote\no Write and publish press releases (2-5 pages) and full reports (12-30 pages)\n\u2022 Appointed as the Problem Solving Champion for the team\n\u2022 Suggested and implemented process and system improvements\no Built a financial model for both the Residential and Commercial Servicer Evaluations teams\no Outsourced tasks to our India team to help improve our efficiency\no Created new user-friendly document templates and procedure manuals"", u'Data Manager\nS&P Global/McGraw Hill Financial - Centennial, CO\nMay 2012 to January 2015\nAccomplishments:\n\u2022 Appointed as the Global Enhancement Coordinator to manage all process improvements\n\u2022 Received a ACE awards for projects and teamwork\n\u2022 Suggested & helped implement several procedural improvements\n\u2022 Trainer and Peer Mentor for new staff members\n\nTasks\n\u2022 Manage all process improvements for group\no Prioritize and recommend system improvements based on anticipated value added to both efficiency and data quality\no Manage enhancements and requirements gathering with management in the following departments: Corporate and Governance (C&G), Structured Finance, RPM IT, and CORE IT\no Coordinate process improvements in RPM (user interface for the CORE database) while considering its direct effect on CORE (our ratings database which houses over 150,000 rated entities and over 1 million data fields), Standardandpoors.com, GlobalCreditPortal.com, government regulations & business requirements\n\u2022 Work on a number of database cleanup projects involving retroactive reports to improve data quality\n\u2022 Actively work on UAT (User Acceptance Testing) for new upgrades and features to detect bugs and problems with the system for the following applications:\no CORE (ratings database)\no RPM (user interface for CORE)\no Standardandpoors.com\n\u2022 Write SOP\u2019s (standard operating procedures) & create presentations for team members to follow\no Assisting in writing a training guide for all new hires to follow\no New issuer creation SOP\no Presentation on my role as the Global Enhancement Coordinator\no Presentation on the value of Six Sigma\no Presentation on how to use outlook effectively for the group and individual inboxes\n\u2022 Manage high pressure ratings actions with strict time deadlines\no Ensure high data quality of outgoing ratings with high attention to detail and data knowledge\no Confirm all regulations have been followed appropriately by the rating analyst\no Review confidential information and press releases for accuracy prior to being released to the public', u""Senior Data Research Analyst\nS&P Global/McGraw Hill Financial - Centennial, CO\nJuly 2008 to May 2012\nAccomplishments\n\u2022 Promoted 4 times in just over 2 years due to high quality work\n\u2022 Helped train and quality check new staff's work\n\u2022 Suggested many process improvement initiatives\n\u2022 Helped develop a new financial application for S&P Market Intelligence which is now fully integrated into their platform\n\nTasks\n\u2022 Involved in mentoring and assisting both new and experienced colleagues to help improve their data quality\n\u2022 Performed advanced financial statement analysis for a product called Compustat in which we standardized companies\u2019 financial statements into a database for distribution to clients\n\u2022 Proficient with numerous accounting standards (US, Canadian, IFRS, & other foreign GAAPs)\n\u2022 Identified areas for streamlining business processes that were implemented by management\n\u2022 Improved research quality by suggesting procedural & definitional enhancements\n\u2022 Helped develop and integrate a new business product called FundamenTool which loads data into CapitalIQ\u2019s platform very quickly using macros\no Selected by management based on high performance and analytical skills\no Project was made a permanent product due to its success""]",[u'Bachelor of Science in Business Administration'],"[u'Colorado State University Fort Collins, CO\nJanuary 2005 to January 2008']"
0,https://resumes.indeed.com/resume/a72cc45ece36a418,"[u'Data Analyst\nCigna, CT\nJanuary 2017 to Present\nHealthcare Management System: Cigna is well renowned as a healthcare service provider across the globe. The project entails responsibility for administering, developing and maintaining the BI environments and providing day to day support within the wider IT delivery team. As a Data Analyst, the Master Data Management (MDM) project involves supporting and coaching to other members in relation to BI tools such as OBIE.\nResponsibilities:\n\u2022 Administered Business Objects environments including user security, folder structures and publishing of content\n\u2022 Created ETL pipelines for data flow from various sources into raw and summary data using OBIE Edition\n\u2022 Analyzed the pattern on data and found the best suited algorithm for predictive analytics\n\u2022 Hands-on experience with installing, configuring, developing on, and tuning HDFS and Hadoop\n\u2022 Created pipelines to transfer data flow from MongoDB into AWS Amazon S3 clusters\n\u2022 Monitored the MDM environment for issues and resolve as needed.\n\u2022 Created application in MS Excel to track home sites and sales.\n\u2022 Created and manipulated various management reports in MS Excel for sales metrics using VLOOKUP and Pivot tables.\n\u2022 Coordinated planned upgrades, enhancements and maintenance outage windows including monitoring for potential system upgrades, service packs and Fix packs.\n\u2022 Collaborated with IT Staff, Systems Analysts, and business subject matter experts, to support the MDM environment.\n\u2022 Converted the legacy SSRS Reports into the interactive Tableau dashboards.\n\u2022 Created the Tableau reports on ""Tableau Server Group/User Permissions"" for auditing purposes.\n\u2022 Identified and implemented best practice models for different aspects of Business Intelligence developments and implementations.\n\u2022 Actively participated in quarterly Tableau user group meetings within the organization.\n\u2022 Implemented BI solutions by: monitoring and tuning queries and data loads, addressing user questions concerning data integrity, monitoring performance and communicating functional and technical issues.\n\u2022 Gathered requirement and proposed possible solutions to satisfy business needs and designed/ created star, snowflake schema models.\nEnvironment: Tableau, OBIE, MDM, Mongo DB, SAS, PostgreSQL, SSRS, AWS Redshift, MS Visual Studio, XML, Agile, SQL Server, MS Word, MS Excel (VLOOKUP and Pivot Tables), MS Powerpoint, Hadoop', u""Business / Data Analyst\nCrowdfire Inc - New York, NY\nMarch 2016 to January 2017\nSocial Media Marketing tool: The project entails the development of an enhanced product named Crowdfire app. The main objective of the project is to implement Data Migration process to support the new functionality that requires an upgraded data formats and increased data volume. This transition process facilitates all the data by providing a consolidated view of the data associated with customer information, contracts, products, risk metrics and KPI's and KYC details. This data is eventually drilled by the business users through dashboard - reports for investment decisions.\nResponsibilities:\n\u2022 Created dashboards and charts in Periscope Data by writing complex SQL queries from AWS Amazon Redshift data source\n\u2022 Performed analysis using Mixpanel &Google Analytics to ensure the timely adjustment/optimization of campaigns, along with other client-specific analysis such as customer segmentation, profiling & competitive market research\n\u2022 Allocated AWS S3 with different clusters and objects to include data flow from AWS Glacier and into Redshift Spectrum\n\u2022 Manipulated large data sets & evaluated IT/IS deliverables by collaborating with multiple teams & reviewing technical system requirements to extrapolate significant results from various performance metrics\n\u2022 Involved in defining the source to target data mappings, business rules and data definitions\n\u2022 Performed data profiling in the source systems to avoid data anomalies and ensure data quality\n\u2022 Coordinated Phased Data Migration process to handle large and complex volumes of data and address dashboard reporting capabilities.\n\u2022 Supervised and worked closely with data coordinators, and other team members to provide technical support and end user training on the developed applications.\n\u2022 Authored progress and completion reports, which were then submitted to project management on a weekly basis.\n\u2022 Performed end to end data integrity testing by creating & executing positive/negative test scenarios using JIRA\n\u2022 Active participant of research settings in our organization.\n\u2022 Created scripts using Python language and libraries such as numpy and panda\n\u2022 Utilized SQL queries to perform data cleansing through imposing data constraints\n\u2022 Published functional requirements, Screen prototypes on portal for access by all project participants.\n\u2022 Produced Requirements Traceability Matrices to support the application development that included Business Requirements, Actor Survey, Use Case Survey, Supplementary Specification Survey, Analysis Model, and Design Test Case Survey.\n\u2022 Assisted QA department to write and execute Test plans and Test Cases; and worked on UAT\nEnvironment: Postgre SQL, MS Office, MS Visio, Jira, AWS Redshift, UAT, Periscope Data, Python platform"", u""Business Data/Analyst\nKS&R - New York, NY\nFebruary 2014 to December 2015\nCustomer Segmentation Market Research: The main objective of the project is to manage the client's customer data, segment them and conduct surveys. The data is then transformed into valuable reports from master data warehouse as a part of MDM. The data which is collected from various sources are then stagged and business transformation is applied before being loaded into data warehouse.\nResponsibilities:\n\u2022 The project followed the Agile methodology and required involvement in all phases of the SDLC\n\u2022 Coordinated and participated in brainstorming sessions with the Product Owner and SMEs in understanding the requirements pertaining to the Customer Segmentation in order to design the Business Requirements Document (BRD)\n\u2022 Used JIRA to log and track the defects from various testing phases.\n\u2022 Created Process Work flows for the end-to-end solution, Functional Specifications, and Project Initiation documents along with Review of Existing Scope document.\n\u2022 Analyzed Product Scope and future phase requirements and prepared the Functional Requirement Document (FRD).\n\u2022 Responsible for Documenting Work flows and results of business analysis and obtaining sign-off from client on specifications.\n\u2022 Responsible for documenting functional specification documents based on user requirements using Use Case Modeling.\n\u2022 Created the meta-data to identify raw and summary data in the data warehouse as a part of MDM.\n\u2022 Facilitated Business Process Modeling of As-Is and To-Be internal process as per BPM notations common to stakeholders and developers using Power Designer.\n\u2022 Skillful in creating Tables, Views, Indexes and other SQL joins\n\u2022 Coordinated in developing optimized SQL Server stored procedures, functions, and database views\n\u2022 Distributed Trading Reports in multiple formats using SQL Server Reporting Services (SSRS)\n\u2022 Responsible for Data Analysis using Business Intelligence tools to Extract, Transform and Load critical data.\n\u2022 Designed & coded standalone application in .NET & HTML through MS Visual Studio TFS that parses query strings & fetches data from SQL Server database & generates batch of Email templates with unique User IDs.\n\u2022 Identified data patterns by designing corresponding machine learning algorithms using Java & Weka\n\u2022 Designed & implemented ETL strategies for integrating data into transactional systems & data warehouses\n\u2022 Developed Visual Studio scripts to automate, systemize SSRS report delivery thereby reducing error proneness\n\n\u2022 Cleansed source data by identifying data inconsistencies & produced Tableau reports in SQL Server environment\n\u2022 Participated in Tableau user group meetings within the organization.\n\u2022 Worked with the Business Objects developers to provide input for generating automated and ad-hoc Business reports.\n\u2022 Involved in Data Warehouse design for the Asset Management module.\n\u2022 Assisted system testers in QA activities like designing, creating and reviewing the test scripts and test cases.\nEnvironment: Agile Methodology, SQL Server, Jira, ETL, Tableau, MDM, Microsoft Office Suite, Microsoft Visio, Microsoft Project, SSRS"", u'Business Intelligence Analyst\nCisco Systems\nJanuary 2013 to January 2014\nEDW- Large Scale Services (LSS): EDW captures all the data movement and transformation occurring within Sales systems like Sales Crediting, Claiming & Adjustment, Hierarchy, Territory Management Planning, and Reporting across Cisco. As part of large scale services project, the principle objective was data migration from Oracle apps 11 legacy platform to R12.inorder to ensure business continuity with source and facilitate migration process for seamless BI reports. This process not only addressed the issues of handling huge data loads across the enterprise, but also supported Ad-hoc reports for OLTP/OLAP systems. As a HealthCare Business Intelligence Analyst, my prime responsibilities were to gather the business requirements from the data warehouse analysis team, develop ETL mappings for data migration and support change requests management\nResponsibilities:\n\u2022 Identified stakeholders of the project; prepared interview questionnaire for requirement elicitation on benefits, claims submission and CWB enhancements; interviewed SMEs and documented requirements. Gathered user and business requirements through interviews, surveys, prototyping and observing from subject matter experts.\n\u2022 Analyzed and prioritized user and business requirements as system requirements that must be included while developing the software. Documented both Business Requirement Document (BRD) and Functional Requirement Document (FRD).\n\u2022 Extensively involved in implementing the Software Development Life Cycle phases from identifying requirements to conducting UAT.\n\u2022 Gained exposure to OLTP/OLAP systems to develop ETL mappings using SSIS\n\u2022 Documented all custom and system modification.\n\u2022 Authored progress and completion reports and submitted them to the project manager on a weekly basis\n\u2022 Conducted User Acceptance Testing (UAT) and verified performance, reliability and fault tolerance issues\n\u2022 Performed Gap analysis by identifying existing processes and technologies and documented the enhancements needed to meet the end state requirements.\n\u2022 Used Excel Sheets for preparation of reports and modifications.\n\u2022 Suggested measures and recommendations to improve the current application performance.\n\u2022 Involved in preparing a simple and detailed user manual for the application, intended for a novice user.\nEnvironment: Oracle, SQL Server 2010, ETL, UAT, XML, Visio, MS Project, Quality Center, MS Excel, MS Word.', u""Business/ Data Analyst\nTata Consultancy Services - Chennai, Tamil Nadu\nDecember 2010 to January 2013\nClaims Management System: USAA is one among the top 10 providers of home and automobile insurance across the United States. The purpose of this project was to enhance its Claims Work Bench (CWB) application module, which handles all auto and home insurance claims, from inception of claim until settlement and archiving. The project was entailed to migrate from existing legacy systems to web-based interactive claims processing systems associated with CWB module in order to help insurers improve efficiency, reduce the cost of service and support, and improve the speed of response and customer satisfaction studying important metrics (KPI)\nResponsibilities:\n\u2022 Gathered user and business requirements through interviews, surveys, prototyping and observing from subject matter experts.\n\u2022 Analyzed and prioritized user and business requirements as system requirements that must be included while developing the software considering all important KPI's. Documented both Business Requirement Document (BRD) and Functional Requirement Document (FRD).\n\u2022 Interviewed key stakeholders to prepare Requirements Specification document & project artifacts for project kick-off\n\u2022 Acted as a liaison between clients & technical developers to elicit requirements, document system design\n\u2022 Coded part of auto loan application module in PS customized framework using Java/J2EE & SOAP XML Web services with Object Oriented Programming concepts (OOPS)\n\u2022 Worked for an Agile Database team with 8 members providing guidance on data architecture & design documentation\n\u2022 Launched Pilot Phase of the loan application process to gather results regarding maximum profit yielding component\n\u2022 Planned and defined system requirements to use case, use case scenario and use case narrative using the UML methodologies.\n\u2022 Scheduled meetings with developers, system analyst and testers to collaborate resource allocation and project completion.\n\u2022 Used SQL queries for organizing and abstracting data from MS Access databases as well as created reports using SSRS\n\u2022 Created mock-up forms in MS Access for better visualization and understanding of the software solution\nEnvironment: Java, OOPS, MS Visual Studio, XML, Waterfall, SQL Server 2008, MS Word, MS Excel, MS Access.""]","[u""Master's""]",[u'']
0,https://resumes.indeed.com/resume/fcaa22786d565041,"[u'Data Analyst\nHewlett Packard - Bengaluru, Karnataka\nJune 2015 to December 2015\nBangalore, Karnataka\nDeveloped complex database objects like Stored Procedures, Functions, Packages, Triggers using SQL\nCreated indexes on the tables for faster retrieval of the data to enhance database performance\nCreated PL/SQL scripts to extract the data from the operational database into simple flat text files\nHandled errors using Exception Handling extensively for the ease of debugging and displaying the error\nmessages in the application\nExperienced in data extraction, transforming and loading (ETL) using SQL Server Integration Services\n(SSIS), SQL Server Reporting Services (SSRS) & SQL Server Analysis Services (SSAS)\nWorked on Tableau server to publish dashboards to a central location for portal integration\nConnected Tableau server with SharePoint portal and setup auto refresh feature\nCreated visualization for Sales calculation and department spend analysis for each medical product\nGenerated KPI for customer satisfaction survey results and disease management\nEngaged in reporting to the Program Manager regarding the updates and progress of the project by submitting weekly reports', u'Data Analyst\nTRREC - Hyderabad, Telangana\nJune 2014 to May 2015\n06/2014 to 05/2015\nTRREC Hyderabad, Telangana\nWorked as Data Analyst to turn data into information, information into insight and insight into business\ndecision\nCreated star schemas and wrote SQL statements for joins, user defined functions, triggers and views\nTrained under the supervision of team lead in creating effective tables\nTroubleshot the minor glitches occured while creating the application\nInterpreted data analyzed results using statistical techniques and provided ongoing reports\nInterpreted patterns in complex data sets\nPerformed ""clean"" data and reviewed reports and performance indicator\nCreated SQL scripts to find data quality issues and identify keys, data anomalies, data validation issues\nAssisted Developers in Data Modeling using Entity Relationship Diagrams\nDeveloped Activity Diagrams, Sequence Diagrams and USE Case Diagrams with GUI components along with the screen designs using MS Visio\nDeveloped Test Plans, Test Cases and maintained Master Test Plan']","[u'Master of Science in Computer Science', u'Master of Science in electronics and communication engineering']","[u'Kent State University Kent, OH\nJanuary 2017', u'Jawaharlal Nehru Technological University Hyderabad Hyderabad, Telangana\nJanuary 2015']"
0,https://resumes.indeed.com/resume/1086d19901a856ac,"[u'Data Analyst\nRMU Career and Professional Development Center\nJanuary 2017 to Present\nRobert Morris University, Moon Township, PA\n\u2022 Compile and analyze post-graduate employment data.\n\u2022 Compile and analyze work-study payroll data.\n\u2022 Create comprehensive employment and payroll reports using Excel, Access, Word, and R Studio for the RMU Dean of Students']",[u'Bachelor of Science in Actuarial Science'],[u'Robert Morris University\nMay 2019']
0,https://resumes.indeed.com/resume/2dcaf9e146f87c0c,"[u'Data Analyst Market Intelligence\nAT&T - Hanover, MD\nSeptember 2016 to March 2018\nUtilized statistical algorithms in R to analyze historical data and creative predictive models by detecting and profiling patterns in massive marketing and financial data sets.\nCreated data visualizations using R packages and generated presentations that helped key stakeholders in their decision-making process.\nExtracted data from various financial data warehouse data stores and identified and collected appropriate customer, competitor, market, financial, and operational data and consolidated and reshaped the data.\nDeveloped User Interface in R shiny for marketing and financial analysis to replace existing adhoc reports and graphics to enhance functionality and user acceptance.\nUtilized financial data from various SQL databases to summarize financial and budget information for custom reports with R tools and packages.\nProvided market and competitive insights through presentations and reports to various internal management teams to allow for effective strategic and tactical decision-making and implementation.\nDesigned and presented statistical results with R packages such as ggplot2, sqldf, shiny, and shiny dashboard, etc. to perform statistical data analysis of markets and competitors.\nSupplied market and competitive intelligence component of strategic planning, and proactively made recommendations to management concerning key intelligence items that require further consideration.\nUsed R to create and update comprehensive industry, competitor, and consumer models including, but not limited to, market share, growth projections and trends, and market forces.\nBuilt R statistical models for analyzing, predicting, and visualizing the patterns, associations, and relationships among consumer purchases.', u""Data Analyst Intern\nESS - Gaithersburg, MD\nAugust 2015 to August 2016\nUsed statistical algorithms in R to analyze and interpret trends in datasets to communicate analysis results to the management with ongoing reports.\nCompiled and extracted data from various sources to present to internal and external clientele.\nGenerated and distributed various operational reports on a monthly and quarterly basis.\nSupported existing programs developed in various languages running on AWS cloud computing environments and Data Warehouse by customer's request.\nUtilized R packages to extract various data sources and merge them to interpret analytical results for financial data analysis.\nConducted ongoing user education and provided management consultation on the use and interpretation of analyses, reports, methodologies and source data and systems.\nAnalyzed financial data in relational databases to describe structures and movement of data in a system and presented data in graphics using R and SQL.\nSelected financial data from various SQL databases, summarized financial and budget information for custom reports with R tools and packages.\nConstructed visual data analysis using R packages such as ggplot2, lattice, etc. to interpret analytical results of financial information.\nApplied statistical methods in working with various data types and a range of data sources.\nProvided consultative and advisory recommendations to service area leaders to support data requirements for strategic operational and financial decisions.\n\n306 Picea View Ct,\nDerwood, MD\n(240) 899-9993\nnicholaslkomarov@gmail.com""]",[u'A.S. in Information Technology'],"[u'Montgomery College Rockville, MD\nJanuary 2015']"
0,https://resumes.indeed.com/resume/6c85298c22f13e08,"[u'HRIS Analyst II\nPrincess Cruises - Santa Clarita, CA\nDecember 2006 to Present', u'Data Analyst\nCa Walker and Associates - Burbank, CA\nMarch 1991 to September 2003']",[u'BS in Secondary Education Mathematics'],"[u'University of the Philippines Manila, Philippines']"
0,https://resumes.indeed.com/resume/e9f8badfd49c5c47,"[u""Data Analyst\nCapitis solutions - Clarksville, MD\nAugust 2017 to Present\n\u2022 I'm responsible for the development and support of reporting and dashboards in AWS Quicksight; Design and implementation of proof of concept solutions and creation of near real-time dashboards.\n\u2022 Created various plots like treemap, heat map etc. Used joins, custom SQL queries to improve and optimize performance.\n\u2022 Worked with large datasets, Used MySQL analytic functions and aggregate functions. I've also exhibited the ability to write efficient queries with complex joins.\n\u2022 Built and published interactive reports and dashboards using Tableau Desktop and Tableau Server.\n\u2022 Designed and developed visualizations using tableau. Used advanced navigation techniques and Level of Detail expressions.\n\u2022 Created action filters, parameters and calculated sets for preparing dashboards and worksheets in Tableau.\n\u2022 Performed data analysis using R statistical software, used statistical analysis and modeling techniques such as regression, ANOVA, correlation analysis, decision trees and including descriptive statistics.\n\u2022 Demonstrated use and working with agile methodologies; e.g. Jira."", u'Data Analyst Intern\nAramark\nMarch 2016 to March 2017\n\u2022 Used advanced Excel functions, VLOOKUP, and pivot tables to analyse sales data.\n\u2022 Generated reports and created dashboards in excel.', u'Data Analyst\nInnovisitors\nApril 2014 to December 2015\n\u2022 Performed data manipulation and analysis using MS SQL Server. Define, execute and interpret complex SQL queries. Knowledge of Microsoft SQL Server Reporting Services (SSRS)\n\u2022 Developed parameter and dimension based reports, drill-down reports, matrix reports, charts, and Tabular reports using Tableau Desktop. Implemented row-level security for embedded reporting in tableau.\n\u2022 Interpreted data to recognize patterns and analyze trends, drawing business inferences and clearly articulating findings to a target audience.\n\u2022 Proficient in data visualization, dashboard design, and workflow. Ability to translate requirements into sustainable and efficient visualizations and reporting. Experience in creating complex formulas and calculations.']","[u'Master of Science in Business Intelligence and Analytics', u""Master's in Electronics & Communication Engineering"", u""Bachelor's in Electronics & Communication Engineering""]","[u""Saint Joseph's University Philadelphia, PA\nMay 2017"", u'JNTUH College of Engineering Hyderabad (University Campus) Hyderabad, Telangana\nJune 2015', u'JNTUH College of Engineering Hyderabad (University Campus) Hyderabad, Telangana\nApril 2014']"
0,https://resumes.indeed.com/resume/a515f7d46c7241ac,[u'Data Analyst'],"[u""Master's in Mathematics"", u'Bachelor of Science in Financial Math']","[u'Rutgers University-New Brunswick\nSeptember 2014 to May 2016', u'The Ohio State University, College of Arts and Sciences Columbus, OH\nJanuary 2010 to January 2013']"
0,https://resumes.indeed.com/resume/85853e2fec99431e,"[u'Data Analyst\nByron Center Family Medicine - Byron Center, MI\nMay 2017 to Present\n\u2022 Analyze financial data for Byron Center Family Medicine physicians and employees\n\u2022 Types of analysis include: Multivariate analysis for high dimensional data, regression of many sorts, chi-square, MANOVA, principal components, canonical correlation analysis, Hypothesis Testing (using both classical statistics and Bayesian statistics), etc.\n\u2022 Use analysis for guiding financial decisions', u'Intern\nLansing, MI\nMay 2016 to July 2016\n\u2022 Created documents, charts, and tables to summarize files and legal procedures/protocols\n\u2022 Conducted data analysis on case-type proportions within a large data set\n\u2022 Conducted predictive analysis on case frequencies']",[u'Bachelor of Science in Statistics and Probability'],"[u'MICHIGAN STATE UNIVERSITY East Lansing, MI\nJanuary 2014 to May 2018']"
0,https://resumes.indeed.com/resume/8a3449c4aebfb703,"[u'SAS/Data Analyst\nUnited Network for Organ Sharing (UNOS) - Richmond, VA\nJanuary 1992 to April 2017']","[u'', u'BA in Anthropology and English']","[u'SAS Institute Cary, NC\nJanuary 1995 to January 2000', u'Virginia Commonwealth University Richmond, VA\nJanuary 1990']"
0,https://resumes.indeed.com/resume/9b92fb4f520442e1,[u'Data Analyst\nAccomplished four years analyzing construction contracts. Set up database of contracts. Applied data from contracts to spreadsheet.\n\nWork well with a d support attorneys who are also willing to advise their assistant on the wording and concept of the contractural provisions. Strong ability to support with strong ability to focus and learn.'],[u'BS in Legal'],"[u'Hilbert College Hamburg, NY\nAugust 1999 to May 2001']"
0,https://resumes.indeed.com/resume/16fb396c64f58587,"[u'ERP Analyst\nTurkana Food Inc - Kenilworth, NJ\nDecember 2017 to Present\n\u2022 Integrated ERP system into the company.\n\u2022 Established entire network of the firm under one common system which imposed reduction in data redundancy.\n\u2022 Implemented centralized server and data is backed up at regular intervals.\n\u2022 Designing a common framework for the company i.e. .Net .\n\u2022 Developed and updated documents of an organization.\n\u2022 Products data was altered using MySQL.\n\u2022 Ensure performance of databases, security, and availability.\n\u2022 Managing database procedures, such as upgrade, backup, recovery, migration, etc.', u'DATA ANALYST\nSAUDI ALUMINUM INDUSTRIES CO - JEDDAH, SA\nJuly 2015 to November 2016\n\u2022 Constitute data visualizations to current trends, analysis, or to simply display a data-story.\n\u2022 Change over client requirements into technical configurations through cooperation with account management department.\n\u2022 Improvement in current practices.\n\u2022 Created SQL queries from scratch to query database for the aluminum products coming in and going out of the company, new samples and the new manufactured materials.\n\u2022 Assisting on-line support to clients under the guidance of team leader.\n\u2022 Generating reports and graphs using statistical methods to analyze data.\n\u2022 Build database table, tableau extracts and write SQL queries to extract data.', u'DATA ANALYST INTERN\nTECHSOWARE PVT LTD - Hyderabad, Telangana\nJune 2014 to May 2015\n\u2022 Procuring information from primary or secondary data sources and looking after databases and data frameworks.\n\u2022 Translating data, analyzing end results using statistical techniques and preparing ongoing reports.\n\u2022 Support managers on the system essentials, various tasks and process effectiveness.\n\u2022 Recognizing, analyzing, and translating patterns or trends in complex data collections.\n\u2022 Perform weekly meetings to coordinate and assist managers on daily basis.\n\u2022 Accumulate data on reporting needs and objectives as new trade features are launched, document necessities and making reports for business clients utilizing Tableau and SSRS.\n\u2022 Creating and executing databases, data gathering frameworks, data analytics and different strategies that advance factual proficiency and quality.\n\u2022 Conduct, run, and refresh schedules for dashboards and reports.']","[u'MASTER OF SCIENCE in Info Systems Technologies', u'BACHELOR OF SCIENCE in Information Technology']","[u'WILMINGTON UNIVERSITY\nApril 2018', u'OSMANIA UNIVERSITY\nMay 2015']"
0,https://resumes.indeed.com/resume/bbb91fba557f76a6,"[u'Field Data Analyst\nSchlumberger\nAugust 2014 to May 2017\n\u2022 Core member of the data analysis team, responsible for $10M annual revenue.\n\u2022 Analyzed geological and geophysical data from well logs, to locate and estimate probable natural gas and oil resources.\n\u2022 Applying decision tree and regression data models in proprietary software to optimize oil discovery process.\n\u2022 Developed a visualization software to highlight potential reservoir zone and detect anomaly of data.\n\u2022 Awarded the ""Team of Best Service Quality"" by reducing the annual lost time from 10 hours to 0.5 hours.', u'Manufacturing Analyst\nHakaru+ - Osaka, JP\nAugust 2012 to February 2014\n\u2022 Initiated the development of database for mechanical design to archive projects and reduce repetitive work.\n\u2022 Standardized mechanical designs by reducing redundant designs by 20%.\n\u2022 Communicated with engineers and subcontractors to gather process information for optimization analysis, maintain\nstable inventory and shortened assembling and delivery time.\n\u2022 Achieved 100% on-time delivery by prioritizing production of various parts.']","[u'M.Eng. in Industrial Engineering & Operations Research in Industrial Engineering & Operations Research', u'Bachelor in Energy & Mechanical Engineering in Energy & Mechanical Engineering']","[u'University of California, Berkeley Berkeley, CA\nAugust 2017 to May 2018', u'Zhejiang University & Honor Hangzhou, CN\nSeptember 2008 to June 2012']"
0,https://resumes.indeed.com/resume/0e76ceaf61104d4a,"[u""Data Scientist/ Machine Learning\nFIS - Jacksonville, FL\nJanuary 2017 to Present\nDescription: FIS provides financial software, world-class services and global business solutions. Let us help you compete and win in today's chaotic marketplace. Fidelity National Information Services Inc., better known by the abbreviation FIS, is an international provider of financial services technology and outsourcing services. FIS is the world's largest global provider dedicated to financial technology solutions. FIS empowers the financial world with software, services, consulting and outsourcing solutions focused on retail and institutional banking, payments, asset and wealth management, risk and compliance, trade enablement, transaction processing and record-keeping.\n\nResponsibilities:\n\u2022 Extracted data from HDFS and prepared data for exploratory analysis using data munging\n\u2022 Built models using Statistical techniques like Bayesian HMM and Machine Learning classification models like XGBoost, SVM, and Random Forest.\n\u2022 Participated in all phases of data mining, data cleaning, data collection, developing models, validation, visualization and performed Gap analysis.\n\u2022 A highly immersive Data Science program involving Data Manipulation&Visualization, Web Scraping, Machine Learning, Python programming, SQL, GIT, MongoDB, Hadoop.\n\u2022 Setup storage and data analysis tools in AWS cloud computing infrastructure.\n\u2022 Installed and used Caffe Deep Learning Framework\n\u2022 Worked on different data formats such as JSON, XML and performed machine learning algorithms in Python.\n\u2022 Worked as Data Architects and IT Architects to understand the movement of data and its storage and ER Studio 9.7\n\u2022 Used pandas, numpy, seaborn, matplotlib, scikit-learn, scipy, NLTK in Python for developing various machine learning algorithms.\n\u2022 Data Manipulation and Aggregation from different source using Nexus, Business Objects, Toad, Power BI and Smart View.\n\u2022 Implemented Agile Methodology for building an internal application.\n\u2022 Focus on integration overlap and Informatica newer commitment to MDM with the acquisition of Identity Systems.\n\u2022 Coded proprietary packages to analyze and visualize SPCfile data to identify bad spectra and samples to reduce unnecessary procedures and costs.\n\u2022 Programmed a utility in Python that used multiple packages (numpy, scipy, pandas)\n\u2022 Implemented Classification using supervised algorithms like Logistic Regression, Decision trees, Naive Bayes, KNN.\n\u2022 As Architect delivered various complex OLAPdatabases/cubes, scorecards, dashboards and reports.\n\u2022 Updated Python scripts to match training data with our database stored in AWS Cloud Search, so that we would be able to assign each document a response label for further classification.\n\u2022 Used Teradata utilities such as Fast Export, MLOAD for handling various tasks data migration/ETL from OLTP Source Systems to OLAP Target Systems\n\u2022 Data transformation from various resources, data organization, features extraction from raw and stored.\n\u2022 Validated the machine learning classifiers using ROC Curves and Lift Charts.\n\nEnvironment:Unix, Python 3.5.2, MLLib, SAS, regression, logistic regression, Hadoop 2.7.4, NoSQL, Teradata, OLTP, random forest, OLAP, HDFS, ODS, NLTK, SVM, JSON, XML and MapReduce."", u""Data Scientist\nCBRE - Dallas, TX\nOctober 2015 to November 2016\nDescription: CBRE Group, Inc. is the largest commercial real estate services and investment firm in the world. It is based in Los Angeles, California and operates more than 450 offices worldwide and has clients in more than 100 countries.\n\nResponsibilities:\n\u2022 Utilized Spark, Scala, Hadoop, HQL, VQL, oozie, pySpark, Data Lake, TensorFlow, HBase, Cassandra, Redshift, MongoDB, Kafka, Kinesis, Spark Streaming, Edward, CUDA, MLLib, AWS, Python, a broad variety of machine learning methods including classifications, regressions, dimensionally reduction etc.\n\u2022 Utilized the engine to increase user lifetime by 45% and triple user conversations for target categories.\n\u2022 Application of various machine learning algorithms and statistical modeling like decision trees, text analytics, natural language processing (NLP), supervised and unsupervised, regression models, social network analysis, neural networks, deep learning, SVM, clustering to identify Volume using scikit-learn package in python, Matlab.\n\u2022 Worked onanalyzing data from Google Analytics, AdWords, Facebook etc.\n\u2022 Evaluated models using Cross Validation, Log loss function, ROC curves and used AUC for feature selection and elastic technologies like ElasticSearch, Kibana.\n\u2022 Performed Data Profiling to learn about behavior with various features such as traffic pattern, location, Date and Time etc.\n\u2022 Categorized comments into positive and negative clusters from different social networking sites using Sentiment Analysis and Text Analytics\n\u2022 Performed Multinomial Logistic Regression, Decision Tree, Random forest, SVM to classify package is going to deliver on time for the new route.\n\u2022 Performed data analysis by using Hive to retrieve the data from Hadoop cluster, Sql to retrieve datafrom Oracle database and used ETL for data transformation.\n\u2022 Performed Data Cleaning, features scaling, features engineering using pandas and numpy packages in python.\n\u2022 Exploring DAG's, their dependencies and logs using AirFlow pipelines for automation\n\u2022 Performed data cleaning and feature selection using MLlib package in PySpark and working with deep learning frameworks such as Caffe, Neon.\n\u2022 Developed Spark/Scala,R Python for regular expression (regex) project in the Hadoop/Hive environment with Linux/Windows for big data resources.\n\u2022 Used clustering technique K-Means to identify outliers and to classify unlabeled data.\n\u2022 Tracking operations using sensors until certain criteria is met using AirFlowtechnology.\n\u2022 Responsible for different Data mapping activities from Source systems to Teradata using utilities like TPump, FEXP,BTEQ, MLOAD, FLOADetc\n\u2022 Analyze traffic patterns by calculating autocorrelation with different time lags.\n\u2022 Ensured that the model has low False Positive Rate and Text classification and sentiment analysis for unstructured and semi-structured data.\n\u2022 Addressed over fitting by implementing of the algorithm regularization methods like L1 and L2.\n\u2022 Used Principal Component Analysis in feature engineering to analyze high dimensional data.\n\u2022 Used MLlib, Spark's Machine learning library to build and evaluate different models.\n\u2022 Implemented rule based expertise system from the results of exploratory analysis and information gathered from the people from different departments.\n\u2022 Created and designed reports that will use gathered metrics to infer and draw logical conclusions of past and future behavior.\n\u2022 Developed MapReduce pipeline for feature extraction using Hive and Pig.\n\u2022 Created Data Quality Scripts using SQL and Hive to validate successful data load and quality of the data. Created various types of data visualizations using Python and Tableau.\n\u2022 Communicated the results with operations team for taking best decisions.\n\u2022 Collected data needs and requirements by Interacting with the other departments.\n\nEnvironment:Python 2.x, CDH5, HDFS, Hadoop 2.3, Hive, Impala, AWS, Linux, Spark, Tableau Desktop, SQL Server 2014, Microsoft Excel, Matlab, Spark SQL, Pyspark."", u'Data Analyst\nWalgreens - Deerfield, IL\nDecember 2013 to September 2015\nDescription:The Walgreens Companyis an American company that operatesas the second-largest pharmacy store chain in the United States. It specializes in filling prescriptions, health and wellness products, health information, and photo services\n\nResponsibilities:\n\u2022 Worked with BI team in gathering the report requirements and also Sqoop to export data into HDFS and Hive\n\u2022 Involved in the below phases of Analytics using R, Python and Jupyter notebook.\n\u2022 a. Data collection and treatment: Analysed existing internal data and external data, worked on entry errors,classification errors and defined criteria for missing values\nb. Data Mining: Used cluster analysis for identifying customer segments, Decision trees used for profitable and non-profitable customers, Market Basket Analysis used for customer purchasing behaviour and part/product association.\n\u2022 Developed multiple Map Reduce jobs in Java for data cleaning and preprocessing.\n\u2022 Assisted with data capacity planning and node forecasting.\n\u2022 Installed, Configured and managed Flume Infrastructure\n\u2022 Administrator for Pig, Hive and HBase installing updates patches and upgrades.\n\u2022 Worked closely with the claims processing team to obtain patterns in filing of fraudulent claims.\n\u2022 Worked on performing major upgrade of cluster from CDH3u6 to CDH4.4.0\n\u2022 Developed Map Reduce programs to extract and transform the data sets and results were exported back to RDBMS using Sqoop.\n\u2022 Patterns were observed in fraudulent claims using text mining in R and Hive.\n\u2022 Exported the data required information to RDBMS using Sqoop to make the data available for the claims processing team to assist in processing a claim based on the data.\n\u2022 Developed Map Reduce programs to parse the raw data, populate staging tables and store the refined data in partitioned tables in the EDW.\n\u2022 Created tables in Hive and loaded the structured (resulted from Map Reduce jobs) data\n\u2022 Using HiveQL developed many queries and extracted the required information.\n\u2022 Created Hive queries that helped market analysts spot emerging trends by comparing fresh data with EDW reference tables and historical metrics.\n\u2022 Was responsible for importing the data (mostly log files) from various sources into HDFS using Flume\n\u2022 Enabled speedy reviews and first mover advantages by using Oozie to automate data loading into the Hadoop Distributed File System and PIG to pre-process the data.\n\u2022 Provided design recommendations and thought leadership to sponsors/stakeholders that improved review processes and resolved technical problems.\n\u2022 Managed and reviewed Hadoop log files.\n\u2022 Tested raw data and executed performance scripts.\n\nEnvironment:HDFS, PIG, HIVE, Map Reduce, Linux, HBase, Flume, Sqoop, R, VMware, Eclipse, Cloudera, Python.', u""Data Scientist\nJohnson and Johnson - Raritan, NJ\nSeptember 2012 to November 2013\nDescription:Transamerica is the US-based brand of Aegon, a Dutch financial services firm. Aegon is one of the world's leading providers of life insurance, pensions and asset management and is helping approximately 30 million customers globally to achieve a lifetime of financial security.\n\nResponsibilities:\n\u2022 Manipulating, cleansing & processing data using Excel, Access and SQL.\n\u2022 Responsible for loading, extracting and validation of client data.\n\u2022 Modelled clean data into the Kafka servers for use over the spark engine.\n\u2022 Zookeeper along with Kafka was used to stream data and end-to-end client communication.\n\u2022 Performed transformations over the warehoused data using Scala& Python and modelled the data back into the servers for iterative transformations into KAFKA.\n\u2022 Modelled data using Machine learning libraries(Sci-kit learn) apart from SVN and KNN based classificationto create a training dataset for use in a predictive model.\n\u2022 Liaising with end-users and 3rd party suppliers.\n\u2022 Analyzing raw data, drawing conclusions & developing recommendations\n\u2022 Writing T-SQL scripts to manipulate data for data loads and extracts.\n\u2022 Developing data analytical databases from complex financial source data.\n\u2022 Performing daily system checks.\n\u2022 Data entry, data auditing, creating data reports & monitoring all data for accuracy.\n\u2022 Designing, developing and implementing new functionality.\n\u2022 Monitoring the automated loading processes.\n\u2022 Advising on the suitability of methodologies and suggesting improvements.\n\u2022 Carrying out specified data processing and statistical techniques.\n\u2022 Supplying qualitative and quantitative data to colleagues & clients.\n\u2022 Using Informatica& SAS to extract, transform & load source data from transaction\nsystems.\n\u2022 Performed sequential analytics using SAS Enterprise miner using jobs fed by the SAS Grid Manager.\n\u2022 Loaded packages and stored procedures using Base SAS and integrated functional and business requirements using the EBI suite.\n\u2022 Creating data pipelines using big data technologies like Hadoop, spark etc.\n\u2022 Creating statistical models using distributed and standalone models to build various diagnostics, predictive and prescriptive solution.\n\u2022 Utilize a broad variety of statistical packages like SAS, R , MLIB, Graphs,Hadoop, Spark , MapReduce, Pig\n\u2022 Performed a check using quality parameters fed using the SAS QC engine.\n\u2022 Created a UI dashboard for end users and performed prototype testing using Tableau.\n\u2022 Refine and train models based on domain knowledge and customer business objectives\n\u2022 Deliver or collaborate on delivering effective visualizations to support the client business objectives\n\u2022 Communicate to your peers and managers promptly as and when required.\n\u2022 Produce solid and effective strategies based on accurate and meaningful data reports and analysis and/or keen observations.\n\u2022 Establish and maintain communication with clients and/or team members; understand needs, resolve issues, and meet expectations\n\u2022 Developed web applications using .net technologies; work on bug fixes/issues that arise in the production environment and resolve them at the earliest\n\nEnvironment:Cloudera, HDFS, Pig, Hive, Map Reduce, python, Sqoop, Storm, Kafka, LINUX, Hbase, Impala, Java, SQL, Cassandra, MongoDB, SVN."", u'Data Architect/Data Modeler\nInfotech Enterprises IT Services Pvt. Ltd - Hyderabad, Telangana\nDecember 2010 to August 2012\nDescription: Infotech Enterprises IT Services Pvt. Ltd. (Infotech IT), part of the $ 260 million Infotech Enterprises group, leverages its business process knowledge, technological competence, strategic alliances and strong global presence to offer innovative IT solutions to the Retail Industry.\n\nResponsibilities:\n\u2022 Worked with large amounts of structured and unstructured data.\n\u2022 Knowledge in Machine Learning concepts (Generalized Linear models, Regularization, Random Forest, Time Series models, etc.)\n\u2022 Worked in Business Intelligence tools and visualization tools such as Business Objects, Tableau, ChartIO, etc.\n\u2022 Deployed GUI pages by using JSP, JSTL, HTML, DHTML, XHTML, CSS, JavaScript, AJAX.\n\u2022 Configured the project on WebSphere 6.1 application servers\n\u2022 Implemented the online application by using Core Java, Jdbc, JSP, Servlets and EJB 1.1, Web Services, SOAP, WSDL.\n\u2022 Handled end-to-end project from data discovery to model deployment.\n\u2022 Monitoring the automated loading processes.\n\u2022 Communicated with other Health Care info by using Web Services with the help of SOAP, WSDLJAX-RPC\n\u2022 Used Singleton, factory design pattern, DAO Design Patterns based on the application requirements\n\u2022 Used SAX and DOM parsers to parse the raw XML documents\n\u2022 Used RAD as Development IDE for web applications.\n\u2022 Preparing and executing Unit test cases\n\u2022 Used Log4J logging framework to write Log messages with various levels.\n\u2022 Involved in fixing bugs and minor enhancements for the front-end modules.\n\u2022 Implemented Microsoft Visio and Rational Rose for designing the Use Case Diagrams, Class model, Sequence diagrams, and Activity diagrams for SDLC process of the application\n\u2022 Doing functional and technical reviews\n\u2022 Maintenance in the testing team for System testing/Integration/UAT.\n\u2022 Guaranteeing quality in the deliverables.\n\u2022 Conducted Design reviews and Technical reviews with other project stakeholders.\n\u2022 Was a part of the complete life cycle of the project from the requirements to the production support\n\u2022 Created test plan documents for all back-end database modules\n\u2022 Implemented the project in Linux environment.\n\nEnvironment: R 3.0, Erwin 9.5, Tableau 8.0, MDM, QlikView, MLLib, PL/SQL, HDFS, Teradata 14.1, JSON, HADOOP (HDFS), MapReduce, PIG, Spark, R Studio, MAHOUT, JAVA, HIVE, AWS.', u""Data Analyst/ Data Modeler\nGlobalLogic Technologies - Hyderabad, Telangana\nApril 2009 to November 2010\nDescription:GlobalLogic provides experience design, Digital Product Engineering Services, and Agile Software Development to the world's top brands by leveraging UX UI Design, next-gen technologies, and cloud software, with end-to-end solution by the best Software Development Company.\n\nResponsibilities:\n\u2022 Developed Internet traffic scoring platform for ad networks, advertisers and publishers (rule engine, site scoring, keyword scoring, lift measurement, linkage analysis).\n\u2022 Responsible for defining the key identifiers for each mapping/interface.\n\u2022 Clients include eBay, Click Forensics, Cars.com, Turn.com, Microsoft, and Looksmart.\n\u2022 Designed the architecture for one of the first analytics 3.0. online platforms: all-purpose scoring, with on-demand, SaaS, API services. Currently under implementation.\n\u2022 Web crawling and text mining techniques to score referral domains, generate keyword taxonomies, and assess commercial value of bid keywords.\n\u2022 Developed new hybrid statistical and data mining technique known as hidden decision trees and hidden forests.\n\u2022 Reverse engineering of keyword pricing algorithms in the context of pay-per-click arbitrage.\n\u2022 Implementation of Metadata Repository, Maintaining Data Quality, Data Cleanup procedures, Transformations, Data Standards, Data Governance program, Scripts, Stored Procedures, triggers and execution of test plans\n\u2022 Performed data quality in Talend Open Studio.\n\u2022 Coordinated meetings with vendors to define requirements and system interaction agreement documentation between client and vendor system.\n\u2022 Automated bidding for advertiser campaigns based either on keyword or category (run-of-site) bidding.\n\u2022 Creation of multimillion bid keyword lists using extensive web crawling. Identification of metrics to measure the quality of each list (yield or coverage, volume, and keyword average financial value).\n\u2022 Enterprise Metadata Library with any changes or updates.\n\u2022 Document data quality and traceability documents for each source interface.\n\u2022 Establish standards of procedures.\n\u2022 Generate weekly and monthly asset inventory reports.\n\nEnvironment: Erwin r7.0, SQL Server 2000/2005, Windows XP/NT/2000, Oracle 8i/9i, MS-DTS, UML, UAT, SQL Loader, OOD, OLTP, PL/SQL, MS Visio, Informatica.""]",[],[]
0,https://resumes.indeed.com/resume/b1ac494ec1275bbe,"[u'Financial Data Analyst\nTF-Securities Co., Ltd - Shanghai, CN\nOctober 2016 to December 2016\n\u2022 Engaged in the valuation of credit spread options using Monte Carlo Simulation\n\u2022 Conducted end-to-end industry data analysis that includes data gathering, processing, analysis, and visualizations in support of client deliverables by SQL, Python and Excel.\n\n\u2022 Implemented regression models to forecast yield to maturity of 10-year Treasury Bond.', u'Data Science Analyst\nLine0 E-Commerce Co., Ltd - Shanghai, CN\nJuly 2015 to September 2015\n\u2022 Manipulated data with MySQL, conducted exploratory data analysis\n\u2022 Identified actionable insights from analytical results. Collaborated with marketing team to make business decisions. Presented to department manager. Increased total number of orders by 7 percent.\n\nProjects\nStock Price Prediction Based on News for AAPL in Python\n\u2022 Scraped the news from news websites of AAPL and converted the HTML to TXT documents\n\u2022 Employed text mining techniques to do sentiment analysis and implemented feature selection by Lasso regression and LDA model\n\u2022 Utilized linear regression to predict stock price based on features of news and decreased MSE by 10% compared to only numerical\nanalysis.\nStatistical Analysis of the Recent Transformation in the NBA in R\n\u2022 Analyzed the changes of stats in NBA for teams, players and positions and visualized their performance by R shiny report engine\n\u2022 Set up a logistic model and a linear model to investigate how a team\'s success and predict the success of a team based on models\n\u2022 Reclassified the roles of NBA players in the ""new age"" by unsupervised machine learning methods\nAnalysis of a two-echelon inventory system with Dual Sourcing Strategy\n\u2022 Studied two-echelon inventory system with two supply modes\n\u2022 Simulated the model established by Matlab and analyze results of simulation\nTime Series Analysis in R\n\u2022 Fit an ARMA model to the differenced series of quarterly US GDP from the first quarter of 1947 to the first quarter of 2007\n\u2022 Fit GARCH(1,1) model to the monthly data of log returns on S&P500 index from Jan 1950 to June 2007']","[u'Master of Science in Operations Research', u'BBA in Electronic Business', u'']","[u'Columbia University New York, NY\nAugust 2017 to December 2018', u'School of Information Management and Engineering\nSeptember 2013 to June 2017', u'Shanghai University of Finance and Economics Shanghai, CN']"
0,https://resumes.indeed.com/resume/2bea6003a4175506,"[u'Data Analyst\nChevron - Cody, WY\nJune 2017 to August 2017\nConducted reservoir surveillance of a private operator based in Wyoming for analysis of Chemical EOR using descriptive statistics and predictive modelling.']",[u'B.S in Petroleum Engineering'],"[u'Colorado School of Mines Golden, CO\nAugust 2014 to May 2018']"
0,https://resumes.indeed.com/resume/5e7519b1ad38109a,"[u'Analytics Consultant\nMinacs\nOctober 2015 to January 2017\n\u2022 Consult with clients to identify relevant KPIs and performance metrics that effectively aligned with business and marketing objectives\n\u2022 Present campaign analyses to senior leadership and clients, inclusive of attrition model, text mining initiative and lapsed recapture predictive model\n\u2022 Collaborate with third party vendors to integrate data sources to support cross channel performance measurement\n\u2022 Support program managers with campaign planning, learning agendas, roadmaps and reporting metrics\n\u2022 Guide strategy development for new marketing offer based on insights discovered through text mining customer service tech comments, resulting in higher customer response rates\n\u2022 Responsible for managing analytical deliverables by ensuring proper prioritization and allocation of resources for timely delivery\n\u2022 Developed and managed curriculum detailing current clients marketing programs and statistical methodologies used for onboarding newly hired analytic consultants', u'Data Analyst\nTargetMarkeTeam\nOctober 2014 to October 2015\n\u2022 Work with internal clients to determine relevant KPIs to measure campaign effectiveness against primary marketing objectives and strategies\n\u2022 Improved reporting efficiency through automated procedures and developed standard QC procedure to improve quality of clients data\n\u2022 Responsible for performing various data analyses to provide insights about clients customer demographics, trends and key predictive indicators in customer donating behavior\n\u2022 Organized and trained statistical team responsible for using SAS to perform exploratory data analysis, data cleansing, clustering and building predictive models across multiple clients\n\u2022 Responsible for the design, development and presentation of predictive model results and recommendations to internal clients', u'Analyst\nDigitasLBI\nJuly 2011 to October 2014\n\u2022 Responsible for implementing A/B and Multivariate Testing and site optimization through Adobe Test and Target\n\u2022 Analyze and optimize digital media marketing campaigns in order to drive value and insightful marketing strategies\n\u2022 Analyze and interpret customer databases to develop actionable insights and understanding of various customer segments\n\u2022 Serve as an internal resource for SAS programming and statistical problems\n\u2022 Develop, manage and execute advanced analytic projects that include: Multiple Attribution Analysis, High Value Behavioral Analysis, Advanced Customer Segmentation, Response Modeling\n\u2022 Designing and develop ad hoc reports using various digital suites including DoubleClick, Site Catalyst, Google Analytics, Omniture']",[u'Bachelors of Science in Mathematics in Mathematics'],"[u'Kennesaw State University Kennesaw, GA\nMay 2011']"
0,https://resumes.indeed.com/resume/cd20466dcb2b094a,"[u""Data Analyst\nGoogle\nJanuary 2015 to January 2017\nUsing R and two dialects of SQL, developed custom metrics reporting and visualization.\n\u25cf Built spreadsheet-to-SQL script pipeline using App Script, to create easily managed source of truth for complicated data access pattern across four different dashboards.\n\u25cf Extended legacy data model reporting through Google's LMS transition by ensuring clean and consistent BigQuery data, while integrating new features\n\u25cf Synced ETL with upstream operations, reducing data staleness by 50%."", u'Data Analyst\nToca Boca\nJanuary 2012 to January 2015\nIncreased company revenue by 12% through systematic pricing experimentation.\n\u25cf Analyzed user LTV to shift marketing budget away from ROI-negative acquisition\ncampaigns.\n\u25cf Oversaw the creation of AWS-hosted MySQL database of atomic in-app user data, and built analysis pipeline in R, uncovering new insights on user behavior.\n\nProjects Notflix \u2022 Recommendation engine microservice for app inspired by Netflix\n\u25cf Crafted cosine-similarity machine learning model in R to generate 100,000,000\npersonalized movie recommendations to users.\n\u25cf Developed fast and scalable communication between microservices using AWS Simple\nQueue Service, leading teammates to adopt the technology throughout the app.\n\nVenmoo \u2022 React/Node.js/Postgres \u2022 Payments app prototype inspired by Venmo (team project)\n\u25cf Designed and deployed database, and developed the Pending Transactions feature, to ensure strict data consistency across multiple records in multiple tables.\n\nRe:slack \u2022 React/Node.js/Postgres \u2022 Workplace messaging app inspired by Slack (team project)\n\u25cf Built out full stack features around private chat and direct messaging.\n\nIs This A Sandwich? \u2022 React/Node.js/MongoDB\n\u25cf Built toy app to determine whether Oreos and hot dogs are sandwiches.\n\nshinyBog \u2022 R Shiny app (with some javascript)\n\u25cf Playable word game inspired by Boggle, with customization options for board size and language-specific letter distribution.']","[u'PhD in Linguistics', u'BS in Mathematics']","[u'The Ohio State University\nJanuary 2010 to January 2012', u'Haverford College\nJanuary 2004 to January 2008']"
0,https://resumes.indeed.com/resume/478588b53727f900,"[u'QC Analyst\nDatamatics Global Services\nAugust 2014 to Present', u'Data Analyst\nCivil Supply and Consumer Production Department\nSeptember 2010 to June 2014']","[u'B.B.A in Manimangalam', u'H.S.C in Pallavaram', u'S.S.L.C in Pallavaram']","[u'TMG College Of Arts and Science', u'M.M.A.Govt Higher Secondary School', u'M.M.A.Govt Higher Secondary School']"
0,https://resumes.indeed.com/resume/b05b9074d5ce491f,"[u""Analyst\nGro Solutions - Atlanta, GA\nJune 2017 to August 2017\n\u2022 Set up branding and workflow configuration of the mobile apps based on requirements using development tools like CSS and Javascript for the onboarding of new clients\n\u2022 Interacted with clients to improve upon and modify their custom app for mobile digital account opening customers\n\u2022 Collaborated with product management team to push changes to each client's native app\n\u2022 Worked alongside the Quality Assurance team to track and fix errors for existing clients that were using the app\n\u2022 Suggested and developed design and workflow improvements to better efficiency and performance of the mobile app"", u'Data Analyst Intern\nMerck Sharp & Dohme (MSD) - Mumbai, Maharashtra\nJune 2015 to August 2015\n\u2022 Executed the automation of monthly sales reporting processes to generate consolidated reports; Potential time saved due to automation would be 111 hours monthly\n\u2022 Documented existing processes and proposed changes with respect to data mapping which culminated in a presentation to regional leaders\n\u2022 Improved data mapping in Excel and used Spotfire to generate reports for upper management\n\u2022 Collaborated with external vendors to gain perspective and develop model for sales process automation']","[u'Master of Science in Quantitative and Computational Finance', u'Bachelor of Science in Engineering']","[u'GEORGIA INSTITUTE OF TECHNOLOGY, School of Industrial and Systems Engineering Atlanta, GA\nMay 2018', u'UNIVERSITY OF MICHIGAN, College of Engineering Ann Arbor, MI\nApril 2016']"
0,https://resumes.indeed.com/resume/d30054609149b0c6,"[u'Data Analyst\nARP Logistics Inc. - Delaware Water Gap, PA\nJanuary 2017 to Present\n\u2022 Troubleshoot issues as they occur within the data capturing process and coordinate with the other members of the staff\n\u2022 Update and process data regularly and efficiently\n\u2022 Analyze program data for accuracy and reporting\n\u2022 Troubleshoot statement data and reference data from various document types\n\u2022 Perform quality assurance measures and check points ensuring accuracy\n\u2022 Assist with invoicing, recording, including data maintenance, payment schedules and vendor correspondence', u'Data Analyst\ncQuest Research & Consulting - Sofia, Bulgaria\nJanuary 2015 to September 2016\n\xb7 Participated in all phases of data mining, data collection, data cleaning, developing models, validation, visualization and performed analysis. Identified and analyzed trends and patterns in complex data sets\n\xb7 Interpreted multiple types of data, analyzed results using statistical techniques and provided ongoing reports for variety of concepts: Customer purchase experience, Product testing, Brand awareness, Market segmentation, and Concept testing\n\xb7 Built predictive models and performed descriptive analytics using statistical methods such as clustering, segmentation, regression and classification models to support the marketing efforts and business needs of the clients\n\xb7 Provided the necessary data for the in depth-analysis to the clients (aggregated data \u2013 ex: tables or transaction-level data, depending on the need). Examples: Pre-campaign data for projections, Mid-campaign data partial results, Post-Campaign data results, Client QBR data results.\n\xb7 Edited existing scripts and created new ones to improve accuracy and efficiency of our team job (automation)\n\xb7 Advised on the suitability of methodologies and suggested improvements\n\xb7 Independently managed all end-to-end project level details to ensure that projects met client specifications, on time and on budget', u""Data Analyst\nGFK - Sofia, Bulgaria\nApril 2012 to December 2014\n\xb7 Analyzed customer and marketing data to measure marketing performance. Provided actionable overall market and customer insights to answer key strategic questions\n\xb7 Utilized survey data to evaluate brand health, understand customer path-to-purchase life cycle and sharpen the product positioning with statistical methodologies such as regression models, hypothesis testing and confidence intervals using IBM SPSS\n\xb7 Provided the data pulls for Marketing Campaigns for multiple clients. Carried heavy SQL and excellent business understanding to help improve client's marketing efforts and ROI for both tactical and strategic initiatives\n\xb7 Conducted statistical modeling, data mining, customer segmentation and other quantitative analyses to assess customer behavior, identify sales opportunities, and recommend tailored marketing approaches\n\xb7 Communicated and collaborated with project managers and operations staff throughout the timeline of a project, including start-up meetings, consultation, and processed reports\n\xb7 Multi-tasked on multiple continuous trackers and ad-hoc projects simultaneously\n\xb7 Processed international projects conducted in Asia, Europe and North America""]","[u'Master of Marketing Research', u'Bachelor of Economics']","[u'University of National and World Economy Sofia, BG\nSeptember 2012 to June 2013', u'University of National and World Economy Sofia, BG\nSeptember 2008 to June 2012']"
0,https://resumes.indeed.com/resume/00991fb58ef71f11,"[u""Data Analyst\nChurch Pension Group - New York, NY\nJune 2017 to Present\nGenerating business related strategic data reports for the SME's from SQL Server using SSMS.\n\u2756 Integrate data from multiple sources to produce requested or required data elements.\n\u2756 Experience in Administering, Managing and Troubleshooting Reporting services.\n\u2756 Worked on administration tasks such as data loading, batch jobs, data unloading, backup & Recovery, user and application table management, upgrades, creating databases/File groups/files/Transaction logs.\n\u2756 Created and modified tables, views and indexes; PL/SQL stored procedures, functions and triggers according to business requirement.\n\u2756 Created business reports by joining multiple tables from multiples database using complex SQL queries.\n\u2756 Responsible for fixing hundreds of critical defects addressed during data conversion.\n\u2756 Used SSIS transformation for Data Cleansing to remove unnecessary data.\n\u2756 Updated mapping tables related to assigned functional areas as directed by the Business Analysts.\n\u2756 Executed SQL conversion pre-validation and post-validation scripts to avoid data loss.\n\u2756 Created Power BI Dashboards (Power View, Power Query, Power Pivot, and Power Maps) and integrated them to share point.\n\u2756 Experienced in maintaining and supporting operational reports, dashboards, and scorecards using Microsoft Power BI (Power Pivot, T-SQL, Excel pivot tables)\nEnvironment/Technologies: MS Excel, Tableau, MYSQL, Power Pivot, VLOOKUP, PL/SQL, T-SQL, Teradata, Oracle 10g."", u""Business Data Analyst\nLenovo - Morrisville, NC\nMarch 2017 to April 2017\nThe Project involves implementation of a Predictive Business Modelling, growth improvement measures. This is done by Automating the company's Business data for the bridging the revenue gap by identifying the KPI's and building a cross-platform for the Data Management. The Automation of data is done through a third-party company which is a pioneer in Big Data Analytics.\n\u2756 Running the data analysis by modifying the existing query models and reporting based on the requirement of Cost metrics team.\n\u2756 Creating new query model and processing it using Hyperion, SSMS and MySQL TestBench.\n\u2756 Data navigation from the multiple databases hosting servers such as MSSQL Server, DB2 and MySQL Server.\n\u2756 Source the CRM data from Salesforce.com for the capturing the business opportunities.\n\u2756 Rolling out the product management training services through Salesforce.com and chatter feeds.\n\u2756 Responsible of the transfer of files over SFTP in coordination with respective IT teams.\n\u2756 Maintaining the data integrity over the SFTP process following the guidelines effectively.\n\u2756 Gathering the requirements on a day to day basis and performing the associative documentation.\n\u2756 Creating the Weekly and Monthly dashboards (Static & Dynamic) using Tableau and Excel.\n\u2756 Carrying out all the work in a timely and task driven environment\nEnvironment/Technologies: MYSQL, CRM, Tableau, MS Excel, Power Pivot."", u'Data Analyst\nNew York Institute of Technology - New York, NY\nJanuary 2016 to December 2016\nCollecting, collating and carrying out complex data analysis in support of management requests. Also, involved in reporting statistical findings to work colleagues and management.\n\u2756 Designed new queries and utilized existing query models to draw relevant customer information for development of financial reports utilized in forecasting, trending, and result analysis.\n\u2756 Manipulating, cleansing & processing data using Excel, Access and SQL.\n\u2756 Responsible for loading, extracting and validation of client data.\n\u2756 Creating dashboards, reports and trend analysis using Power Pivot and Tableau on Agile methodology.\n\u2756 Developing data analytical databases from complex financial source data.\n\u2756 Performing daily system checks. Data entry, data auditing, creating data reports & monitoring all data for accuracy.\n\u2756 Designing, developing and implementing new functionality.\n\u2756 Monitoring the automated loading processes.\n\u2756 Advising on the suitability of methodologies and suggesting improvements.\n\u2756 Carrying out specified data processing and statistical techniques.\nEnvironment/Technologies: MS Excel, MS Office Suite, MY SQL, Power Pivot, VLOOKUP', u'Oracle Admin\nPilog Technologies, Vsoft Technologies - Hyderabad, Telangana\nJanuary 2014 to December 2014\nWorking in 24*7 Environment and providing on-call and day-to-day support.\n\u2756 Maintenance activities in Non-Production hours (Statistics, Rebuilding Indexes etc)\n\u2756 Configured adding automatic datafile to a tablespace when it reaches Threshold Value.\n\u2756 Monitoring the production Oracle alert logs for database errors through Mail Alerts.\n\u2756 Applying Patches & Upgradations from 10g to 11g & 11g to 12c.\n\u2756 Monitoring Disk Space, Flash Recovery Usage, Table, events.\n\u2756 Installation of Cloud Control Setup and added 9 agents for better management.\n\u2756 Migration of Oracle Standard edition to Enterprise edition on Amazon RDS instance.\n\u2756 Maintaining Archiving data in secondary servers.\n\u2756 Hands on Data Guard Installation.\n\u2756 Monitoring Database with Mail Alerts and configured OEM Cloud control.\n\u2756 Refreshing (cloning) Database from Prod to Dev.\n\u2756 Cross Platform Database Migrations from Windows to Linux.\n\u2756 Moving the data folder directory and open the database.\n\u2756 Enabled fill factor for large tables.']","[u'Masters in Computer Science', u'Bachelors in Computers Science Engineering']","[u'New York Institute of Technology', u'Jawaharlal technological University']"
0,https://resumes.indeed.com/resume/47001ae331495193,"[u'Data Analyst\nCardinal Health - Columbus, OH\nMay 2017 to February 2018\n\u2022 Supported the customer integration of recently acquired Harvard Drug Group. Primary responsibility was to ensure the accurate and timely transition of customer attributes such as regulatory, pricing and credit settings from legacy systems into Salesforce and SAP.\n\u2022 Managed large and complex data sets within Excel to extract and set-up customer attributes\n\u2022 Navigated legacy data, SAP (ERP and MDG), ECM and SalesForce to review and assign attributes at a customer level\n\u2022 Prioritized work streams to deliver accurate and timely changes to support customer transitions\n\u2022 Collaborated with cross-functional teams to troubleshoot unique customer requirements\n\u2022 Achieved assigned goals and deadlines set by senior leadership while managing multiple tasks to keep project duration within expected timeline.\n\u2022 Directed team on multiple time sensitive ad-hoc requests from project director in absence of team-lead. Engaged necessary teams for information on completing tasks, formulated action-plans, distributed work between team, successfully executed action-plans and fulfilled those requests.\n\u2022 Negotiated elevated SAP access with project leadership that led to a decreased duration of the customer onboarding process while increasing the accuracy of customer data.', u""Assistant-Manager\nUnited Dairy Farmers\nJanuary 2014 to January 2018\n\u2022 Worked across departments to implement a logistic methodology to order and dispense supplies; Reduced product waste\n\u2022 Prioritized shift workers efforts to exceed companies and customer's needs\n\u2022 Project Management lead for facility upgrades; On budget and on time\nSupervise other clerical staff and provide training and orientation to ne\nw staff; exceeded company standards""]",[u'Bachelor of Science in Physics in Physics'],[u'The Ohio State University\nMay 2017']
0,https://resumes.indeed.com/resume/8b7eee647d162aa6,"[u'Data Analyst Intern; Corporate Finance Department\nSonac (China) Biotechnology Co., Ltd\nDecember 2016 to February 2017\n\u2022 Calculated the daily quantity of raw materials using JDE software and adjusting formula on other indexes\n\u2022 Predicted product prices and sales, monitored prices and sales fluctuations and analyzed reasons for the changes', u'Financial Analyst Intern\nIndustrial and Commercial Bank of China Co., Ltd - Wuhan, CN\nJuly 2016 to September 2016\n\u2022 Collected credit and financial information regarding companies and individuals and built a database module\n\u2022 Reviewed loan applications, analyzed credit information to determine loan eligibility, and made recommendations to loan officers\n\u2022 Assisted in post-lending management to reduce credit risk and verified loading criteria was met', u'Part-time Data Analyst\nHubei Provincial Center for Disease Control and Prevention\nJanuary 2015 to June 2016\nJan. 2015 - June. 2016\n\u2022 Collected and compiled data, analyzed and updated data and monitored database\n\u2022 Collected 3510 surveillance cases about epidemic cerebrospinal meningitis and epidemic encephalitis B and other diseases\n\u2022 Used EpiData software to create a database and Epi Info to analyze data', u'Data Analyst Intern\nHubei Provincial Center for Disease Control and Prevention - Wuhan, CN\nDecember 2015 to February 2016\n\u2022 Used Excel to collect and compile data and to process large datasets for 39 infectious diseases\n\u2022 Calculated time, geographical and population distributions of each infectious disease using ArcGIS\n\u2022 Reported the prevalence of each epidemic disease and compared past data to predict future epidemics']","[u'Master of Arts in Economics in Economics', u'Bachelor of Economics in Finance']","[u'The University of Texas at Austin Austin, TX\nDecember 2018', u'Hubei University\nJune 2017']"
0,https://resumes.indeed.com/resume/7ea0a5b525202020,"[u'Data Analyst\nPipefish - Atlanta, GA\nMay 2014 to August 2014\nCompiled data and created database to compare social media applications and their usefulness based on the ratings and interests of their user base for Midtown based internet start-up company.', u'Wait Staff\nPasta Vino\nOctober 2012 to March 2013\nDeveloped people skills and work/school balance while working in fast paced restaurant environment.']","[u'in Physics', u'']","[u'Georgia State University\nAugust 2015 to Present', u'Johns Creek High School\nJanuary 2011 to January 2015']"
0,https://resumes.indeed.com/resume/dc74057993f67377,"[u'Data Analyst\nInfosys Limited\nDecember 2014 to July 2016\n\u2022 Managed the operations of Marketing and sales team of Australian based telecommunication client by gathering and analyzing data on daily basis\n\u2022 Worked with architects and marketing team to understand patterns, performed A|B testing, analyzed improvement opportunities to retain the current customers and acquire more customers thus adding 280K more customer in a year\n\u2022 Coordinated with marketing and sales team to improve various customer plans leading to 60% increase in the data load experienced on the network\n\u2022 Used SPSS to understand the statistical stability of the models; Developed and maintained extensive reports, generated dashboards and KPI to give the insights of data collected\n\u2022 Provided data-driven insights to optimize and monitor marketing campaigns, also built predictive models to predict the efficiency of the campaign']","[u'Master of Science in Computer Systems Engineering in Database Management Systems', u'Bachelor of Engineering in Information Technology in Information Technology']","[u'Northeastern University Boston, MA\nSeptember 2016 to May 2018', u'Gujarat University Gujarat, IN\nAugust 2010 to July 2014']"
0,https://resumes.indeed.com/resume/c6741a93b5ae92c9,"[u'MIS Analyst\nJ.P Morgan Chase - Westmont, IL\nMay 2013 to October 2013\n\u2022 Worked as a liaison between the Business team and the Technical team.\n\u2022 Documented technical and functional specifications of the projects.\n\u2022 Successfully led key projects which resulted in efficient production of BAU work.\n\u2022 Reviewed and provided comments on the adequacy of documents and took necessary steps to cure any deficiencies.\n\u2022 Conducted analysis to address duplicity of reporting which led to redundancy free reporting in all Groups.', u'Analyst\nAlpha Metrix Group, LLC - Chicago, IL\nJanuary 2013 to May 2013\n\u2022 Analyzed performance of different Hedge Funds based on their ROR (Rate of Return) which led investors to invest money accordingly.\n\u2022 Prepared correspondence, accounting and financial documents for analysis.\n\u2022 Conducted Performance analysis to create alternative investment opportunities for the investors.', u'Data Analyst Intern\nJSC Global Solutions - Chicago, IL\nJune 2012 to August 2012\n\u2022 Planned and executed projects for targeting and segmentation of the prospective leads.\n\u2022 Effectively worked with the Clients and the Technical team to deliver the product to the client on stipulated time.\n\u2022 Organized and coded all documents related to due diligence for acquisitions.', u'Technology Analyst\nJSC Global Solutions - Chicago, IL\nApril 2009 to March 2010\n\u2022 Designed and documented technical and functional specifications of the software product.\n\u2022 Increased the revenue of the firm by Re-implementing the legacy (irMAR) mass e-mailing tool.\n\u2022 Managed cross-functional team on daily basis to meet the requirements of client.', u'Analyst Intern\nNational Hydro Power Corporation - New Delhi, Delhi\nMay 2006 to July 2006\n\u2022 Conducted analysis on potential projects for acceptable rates of return.']","[u'Master of Science in Management Information Systems', u'Master of Business Administration', u'Bachelors of Computer Applications']","[u'University of Illinois Chicago - Liautaud Graduate School of Business Chicago, IL\nJanuary 2012', u'Institute of Marketing & Management New Delhi, Delhi\nJuly 2007', u'Indraprastha (IP) University New Delhi, Delhi\nJuly 2005']"
0,https://resumes.indeed.com/resume/194764b57b858d82,"[u'Data Analyst\nFIRST MEDICAL HEALTH PLAN - Arecibo, PR\nApril 2017 to Present\nData analyst for Medical Affairs Department. Position duties include reports that are compmeted daily, weekly, monthly and each 3 months(or completed otherwise as per decision of suoervisors).', u'Manufacturing Technician\nSt Jude Medical - Arecibo, PR\nFebruary 2014 to May 2015']","[u""Bachelor's in Computer Science""]","[u'National University College-Arecibo Arecibo, PR\nAugust 2015 to June 2017']"
0,https://resumes.indeed.com/resume/01cd8e5c95694ae9,"[u""Sr. Data Analyst\nSan Francisco, CA\nJuly 2016 to February 2018\nMotivate is the most experienced team in bike share, committed to innovation, collaboration and providing outstanding service to clients, sponsors and the customers who use bike share.\nWorked with Motivate to analyze data for Bike Sharing programs in USA like Bay Area Bike Sharing (BABS), Capital Bikeshare, and Hubway.\n\u2022 Translated business needs into data analysis, business intelligence data sources and reporting solution for different types of Clients.\n\u2022 Develop SQL queries for extracting tables.\n\u2022 Analyzed different types of data to derive insights about relationships between locations, statistical measurements and qualitatively assess the data using R/R Studio\n\u2022 Performed analysis on implementing Spark using Scala and wrote spark sample programs using PySpark.\n\u2022 Designed algorithm to tokenize words from a sentence and retrieve its parts of speech.\n\u2022 Created External and Managed tables in Hive and used them appropriately for reporting.\n\u2022 Involved in assimilating different structured and unstructured data and using Pig/Hive queries to clean aggregate and transform data required for reporting.\n\u2022 Used SQL for Querying the database in UNIX environment\n\u2022 Used twitter APIs to analyze the demographics of customer base using Tableau.\n\u2022 Conducted a price elasticity test on the products of Motivate using R.\n\u2022 Developed data visualization reports and online dashboards with Tableau, and D3.js\n\u2022 Created word cloud of top words used by people when describing Motivate on social media.\n\u2022 Developed models to analyze data using R, MATLAB, and Excel\n\u2022 Utilized Python libraries of Pandas, Numpy, Matplotlib and D3JS technologies.\n\u2022 Developed Python scripts for data collection/data analysis\n\u2022 Developed functions for extracting valuable information from large datasets utilizing Excel and R.\n\u2022 Performed Text Mining for Customer Complaints using R.\n\u2022 Implemented action filters, parameters, calculated filed and set for preparing dashboards and worksheets in Tableau.\n\u2022 Extended algorithm to retrieve tweets from different users around the globe using the tweepy library in python to access the twitter API.\n\u2022 Built SQL\\T-SQL scripts, indexes, and complex queries for data analysis and extraction\n\u2022 Given the user's keyword, different tweets were pulled and stored and the sentiment attribute of the analysis variable of each string was calculated using the TextBlob natural language library.\n\u2022 Initiated ETL process to transfer raw data from Flat files into SQL Server\n\u2022 Performed sentiment analysis to get an understanding of what people think about\n\u2022 Analyzed customers' behavior using Linear Regression, Logistic Regression in gretl and Stata by implementing various predictive analysis algorithms.\n\u2022 Used Python coding to create data processing workflows\n\u2022 Filtered and Loaded data from different formats of data sources into Database Tables.\n\u2022 Created Tableau scorecards, dashboards using Stack bars, bar graphs, scattered plots,\n\u2022 Performed troubleshooting, fixed and deployed many Python bug fixes of the two main applications that were a main source of data for both customers and internal customer service team.\n\u2022 Developed dashboards in Tableau Desktop and published them on to Tableau Server that allowed end users to understand the data on the fly with the usage of quick filters for on demand-needed\n\u2022 Worked with large data sets, created Dataevaluation/Visualization reports and KPI reports using Tableau to ensure data accuracy, integrity and consistency.\nTechnology used: SQL, R, Python, Tableau, Hive and Excel."", u'Data Analyst\nBank of America, NJ\nFebruary 2014 to June 2016\nExtracted data from multiple sources, automated data load from those sources into the data warehouse.\n\u2022 Manipulated and analyzed data in a variety formats (SQL, Access, Excel) and from various sources.\n\u2022 Used Excel techniques like Pivot tables, vlookup to slice and dice data from various angles\n\u2022 Partnered with business users to understand their data needs and data use cases.\n\u2022 Involved in migration of various objects like stored procedures, tables, and views from various data source to SQL Server.\n\u2022 Used the Boruta feature selection to identify the resources that contribute the most to the output in R using the Caret package.\n\u2022 Web scraping using regular expressions and the XML library\n\u2022 Implemented several models to predict the loss value (loans) based on hundreds of categorical and continuous features.\n\u2022 Imported the customer data into Python using Pandas libraries and performed various data analysis - found patterns in data which helped in key decisions for the company\n\u2022 Performed data analysis and data profiling using complex SQL on various sources systems\n\u2022 The XGBoost had the best accuracy among logistic regression, random forest, and a neural net.\n\u2022 Updated and manipulated content and files by using python scripts.\n\u2022 Generated comprehensive analytical reports by running SQL queries against current databases to conduct data analysis.\n\u2022 Generated queries using SQL to check for consistency of the data in the tables and to update the tables as per the Business requirements.\n\u2022 Worked on the SQL query development for the testing of the different Dashboards and Analysis\n\u2022 Created Unit test cases for Analytics reports and Analytics dashboards and executed manually\n\u2022 Created Databases, Schemas, Tables, Cluster/Non-Cluster Indexes and other database objects.\n\u2022 Created and Modified T-SQL stored procedures/triggers for validating the integrity of the data.\n\u2022 Provided ad-hoc reporting and data analysis to the financial advisors using Tableau 10.x.\n\u2022 Modified database schema to optimize performance and data integrity.\nTechnology used: Python, SQL, R, Tableau, Excel.', u'Data Analyst\nAllstate Insurance\nNovember 2012 to January 2014\nNY\nAnalyzed Data of Claims for Allstate to analyze and Predict and control Loss based on various categories.\n\u2022 Used Agile Methodologies, Scrum stories and sprints experience in a Python based environment, along with data analytics and Excel data extracts.\n\u2022 Performed data analysis and provided data integrity verification.\n\u2022 Provided various reports on inventory and forecasting using complicated SQL queries.\n\u2022 Created dashboards, charts and presented Adhoc requests to support stakeholders.\n\u2022 Built dashboards for monitoring economic KPIs using Excel and Marco.\n\u2022 Designed and developed weekly, monthly reports by using MS Excel Techniques (Charts, Graphs, Pivot tables) and Power point presentations.\n\u2022 Strong Excel skills, including pivots, Vlookup, conditional formatting, large record sets. Including data manipulation and cleaning.\n\u2022 Utilized KPIs like AR turnover rate to show areas of improvement, which initiated actions to bring aging AR from 65% to 98% current, and to collect 2% interest income according to the contract.\n\u2022 Automated monthly reports generation process to integrate data, and to produce results.\n\u2022 Exported data to spreadsheets for reporting, summarizing, created pivot tables, bar graphs and charts.\n\u2022 Produced automated reports to track inventory of development and production equipment.\nEnvironment: SQL, Python, Excel, R and Python.', u""Database Analyst, Data Analyst\nTATA Consulting - Bengaluru, Karnataka\nMay 2010 to July 2012\nIndia\nWorked on various Data Analyst and Database Analyst Projects for TATA Consulting\n\u2022 Researched and analyzed data related high-profile clients 'background, positions, demographics,\n\u2022 Document the complete process flow to describe program development, logic, testing, and implementation, application integration, coding.\n\u2022 Used R to develop algorithms to analyze large data sets\n\u2022 Analyzed data to find patters of consumer behavior.\n\u2022 Used Tableau for Ad hoc Reporting.\n\u2022 Used Excel to present and analyze data charts etc.\n\u2022 Gathered, analyzed, formatted and keyed mission critical information.\n\u2022 Worked with developers on database design, test and automation tasks.\n\u2022 Trained organization's junior staff to do data research and input.""]",[u'Bachelor in Electronics and Communication in Electronics and Communication'],[u'Manipal University\nMay 2010']
0,https://resumes.indeed.com/resume/295b52057ec949b1,"[u'Data Scientist\nHarley Davidson - Milwaukee, WI\nJanuary 2017 to Present\nResponsibilities:\n\u2022 As an Architect design conceptual, logical and physical models using Erwin and build datamarts using hybrid Inmon and Kimball DW methodologies.\n\u2022 Worked closely with business, datagovernance, SMEs and vendors to define data requirements.\n\u2022 Worked with data investigation, discovery and mapping tools to scan every single data record from many sources.\n\u2022 Designed the prototype of the Data mart and documented possible outcome from it for end-user.\n\u2022 Involved in business process modeling using UML\n\u2022 Developed and maintained data dictionary to create metadata reports for technical and business purpose.\n\u2022 Created SQLtables with referential integrity and developed queries using SQL, SQL*PLUS and PL/SQL\n\u2022 Experience in maintaining database architecture and metadata that support the Enterprise Datawarehouse.\n\u2022 Design, coding, unit testing of ETL package source marts and subject marts using Informatica ETL processes for Oracledatabase.\n\u2022 Developed various QlikView Data Models by extracting and using the data from various sources files, DB2, Excel, Flat Files and Bigdata.\n\u2022 Handled importing data from various data sources, performed transformations using Hive, Map Reduce, and loaded data into HDFS.\n\u2022 Interaction with Business Analyst, SMEs and other Data Architects to understand Business needs and functionality for various project solutions\n\u2022 Researched, evaluated, architected, and deployed new tools, frameworks, and patterns to built sustainable Big Data platforms for the clients\n\u2022 Identifying and executing process improvements, hands-on in various technologies such as Oracle, Informatica, BusinessObjects.\n\u2022 Designed both 3NF data models for ODS, OLTP systems and dimensionaldatamodels using Star and SnowflakeSchemas.\nEnvironment: r9.0, Informatica 9.0, ODS, OLTP, Oracle 10g, Hive, OLAP, DB2, Metadata, MS Excel, Mainframes MS Visio, Rational Rose, Requisite Pro. Hadoop, PL/SQL, etc.', u'Data Scientist\nNBT Bank - Norwich, NY\nJanuary 2015 to December 2016\nResponsibilities:\n\u2022 Worked as a Data Modeler/Analyst to generate Data Models using Erwin and developed relational database system.\n\u2022 Analyzed the business requirements of the project by studying the Business Requirement Specification document.\n\u2022 Extensively worked on DataModeling tools ErwinDataModeler to design the datamodels.\n\u2022 Designedmapping to process the incremental changes that exists in the source table. Whenever source data elements were missing in source tables, these were modified/added in consistency with third normal form based OLTP source database.\n\u2022 Designed tables and implemented the naming conventions for Logical and PhysicalData Models in Erwin 7.0.\n\u2022 Provide expertise and recommendations for physicaldatabasedesign, architecture, testing, performance tuning and implementation.\n\u2022 Designedlogical and physical data models for multiple OLTP and Analytic applications.\n\u2022 Extensively used the Erwin design tool &Erwin model manager to create and maintain the DataMart.\n\u2022 Designed the physical model for implementing the model into oracle9i physical data base.\n\u2022 Involved with DataAnalysis primarily Identifying DataSets, SourceData, Source Meta Data, Data Definitions and Data Formats\n\u2022 Performance tuning of the database, which includes indexes, and optimizing SQL statements, monitoring the server.\n\u2022 Wrote simple and advanced SQLqueries and scripts to create standard and adhoc reports for senior managers.\n\u2022 Collaborated the data mapping document from source to target and the data quality assessments for the source data.\n\u2022 Used Expert level understanding of different databases in combinations for Data extraction and loading, joiningdata extracted from different databases and loading to a specific database.\n\u2022 Co-ordinate with various business users, stakeholders and SME to get Functional expertise, design and business test scenarios review, UAT participation and validation of financial data.\n\u2022 Worked very close with Data Architects and DBA team to implement data model changes in database in all environments.\n\u2022 Created PL/SQL packages and DatabaseTriggers and developed user procedures and prepared user manuals for the new programs.\n\u2022 Performed performance improvement of the existing Data warehouse applications to increase efficiency of the existing system.\n\u2022 Designed and developed UseCase, Activity Diagrams, Sequence Diagrams, OOD (Object oriented Design) using UML and Visio.\n\nEnvironment: SQL Server 2008R2/2005 Enterprise, SSRS, SSIS, Crystal Reports, Windows Enterprise Server 2000, DTS, SQL Profiler, and Query Analyzer.', u""Data Scientist\nConAgra Foods - Omaha, NE\nMay 2013 to December 2014\nResponsibilities:\n\u2022 Coded R functions to interface with CaffeDeepLearning Framework\n\u2022 Working in AmazonWebServices cloud computing environment\n\u2022 Worked with several R packages including knitr, dplyr, SparkR, CausalInfer, spacetime.\n\u2022 Implemented end-to-end systems for DataAnalytics, DataAutomation and integrated with custom visualization tools using R, Mahout, Hadoop and MongoDB.\n\u2022 Gathering all the data that is required from multiple data sources and creating datasets that will be used in analysis.\n\u2022 Performed Exploratory DataAnalysis and DataVisualizations using R, andTableau.\n\u2022 Perform a proper EDA, Univariate and bi-variate analysis to understand the intrinsic effect/combined effects.\n\u2022 Worked with Data governance, Data quality, data lineage, Data architect to design various models and processes.\n\u2022 Independently coded new programs and designed Tables to load and test the program effectively for the given POC's using with Big Data/Hadoop.\n\u2022 Designed data models and data flow diagrams using Erwin and MSVisio.\n\u2022 As an Architect implemented MDM hub to provide clean, consistent data for a SOA implementation.\n\u2022 Developed, Implemented & Maintained the Conceptual, Logical&Physical Data Models using Erwin for Forward/ReverseEngineered Databases.\n\u2022 Established Data architecture strategy, best practices, standards, and roadmaps.\n\u2022 Lead the development and presentation of a dataanalytics data-hub prototype with the help of the other members of the emerging solutions team\n\u2022 Performed datacleaning and imputation of missing values using R.\n\u2022 Worked with Hadoop eco system covering HDFS, HBase, YARN and MapReduce\n\u2022 Take up ad-hoc requests based on different departments and locations\n\u2022 Used Hive to store the data and perform datacleaning steps for huge datasets.\n\u2022 Created dash boards and visualization on regular basis using ggplot2 and Tableau\n\u2022 Creating customized business reports and sharing insights to the management\n\u2022 Worked with BTEQ to submit SQL statements, import and export data, and generate reports in Teradata.\n\u2022 Interacted with the other departments to understand and identify dataneeds and requirements and work with other members of the ITorganization to deliver data visualization and reportingsolutions to address those needs.\nEnvironment: Erwin r9.0, Informatica 9.0, ODS, OLTP, Oracle 10g, Hive, OLAP, DB2, Metadata, MS Excel, Mainframes MS Visio, Rational Rose, Requisite Pro. Hadoop, PL/SQL, etc.."", u'Data Scientist\nDPSG - Plano, TX\nAugust 2012 to April 2013\nResponsibilities:\n\u2022 Supported MapReduce Programs running on the cluster.\n\u2022 Evaluated business requirements and prepared detailed specifications that follow project guidelines required to develop written programs.\n\u2022 Configured Hadoop cluster with Namenode and slaves and formatted HDFS.\n\u2022 Used Oozie workflow engine to run multiple Hive and Pig jobs.\n\u2022 Performed Map Reduce Programs those are running on the cluster.\n\u2022 Developed multiple MapReduce jobs in java for data cleaning and preprocessing.\n\u2022 Analyzed the partitioned and bucketed data and compute various metrics for reporting.\n\u2022 Involved in loading data from RDBMS and web logs into HDFS using Sqoop and Flume.\n\u2022 Worked on loading the data from MySQL to HBase where necessary using Sqoop.\n\u2022 Developed Hive queries for Analysis across different banners.\n\u2022 Extracted data from Twitter using Java and Twitter API. Parsed JSON formatted twitter data and uploaded to database.\n\u2022 Launching Amazon EC2 Cloud Instances using Amazon Images (Linux/ Ubuntu) and Configuring launched instances with respect to specific applications.\n\u2022 Exported the result set from Hive to MySQL using Sqoop after processing the data.\n\u2022 Analyzed the data by performing Hive queries and running Pig scripts to study customer behavior.\n\u2022 Have hands on experience working on Sequence files, AVRO, HAR file formats and compression.\n\u2022 Used Hive to partition and bucket data.\n\u2022 Experience in writing MapReduce programs with Java API to cleanse Structured and unstructured data.\n\u2022 Wrote Pig Scripts to perform ETL procedures on the data in HDFS.\n\u2022 Created HBase tables to store various data formats of data coming from different portfolios.\n\u2022 Worked on improving performance of existing Pig and Hive Queries.\n\nEnvironment: SQL/Server, Oracle 9i, MS-Office, Teradata, Informatica, ER Studio, XML, Business Objects.', u'Data Architect/Data Modeler\nPeople Group - IN\nNovember 2010 to July 2012\nResponsibilities:\n\u2022 Deployed GUI pages by using JSP, JSTL, HTML, DHTML, XHTML, CSS, JavaScript, AJAX\n\u2022 Configured the project on WebSphere 6.1 application servers\n\u2022 Implemented the online application by using Core Java, Jdbc, JSP, Servlets and EJB 1.1, Web Services, SOAP, WSDL\n\u2022 Communicated with other Health Care info by using Web Services with the help of SOAP, WSDL JAX-RPC\n\u2022 Used Singleton, factory design pattern, DAO Design Patterns based on the application requirements\n\u2022 Used SAX and DOM parsers to parse the raw XML documents\n\u2022 Used RAD as Development IDE for web applications.\n\u2022 Preparing and executing Unit test cases\n\u2022 Used Log4J logging framework to write Log messages with various levels.\n\u2022 Involved in fixing bugs and minor enhancements for the front-end modules.\n\u2022 Implemented Microsoft Visio and Rational Rose for designing the Use Case Diagrams, Class model, Sequence diagrams, and Activity diagrams for SDLC process of the application\n\u2022 Doing functional and technical reviews\n\u2022 Maintenance in the testing team for System testing/Integration/UAT\n\u2022 Guaranteeing quality in the deliverables.\n\u2022 Conducted Design reviews and Technical reviews with other project stakeholders.\n\u2022 Was a part of the complete life cycle of the project from the requirements to the production support\n\u2022 Created test plan documents for all back-end database modules\n\u2022 Implemented the project in Linux environment.\n\nEnvironment: R 3.0, Erwin 9.5, Tableau 8.0, MDM, QlikView, MLLib, PL/SQL, HDFS, Teradata 14.1, JSON, HADOOP (HDFS), MapReduce, PIG, Spark, R Studio, MAHOUT, JAVA, HIVE, AWS.', u'Data Analyst/Data Modeler\nInfotech Info - Hyd\nJanuary 2009 to October 2010\nResponsibilities:\n\u2022 Worked with project team representatives to ensure that logical and physical ER/Studio data models were developed in line with corporate standards and guidelines.\n\u2022 Involved in defining the source to target data mappings, business rules, data definitions.\n\u2022 Worked with BTEQ to submit SQL statements, import and export data, and generate reports in Teradata.\n\u2022 Responsible for defining the key identifiers for each mapping/interface.\n\u2022 Responsible for defining the functional requirement documents for each source to target interface.\n\u2022 Document, clarify, and communicate requests for change requests with the requestor and coordinate with the development and testing team.\n\u2022 Work with users to identify the most appropriate source of record and profile the data required for sales and service.\n\u2022 Document the complete process flow to describe program development, logic, testing, and implementation, application integration, coding.\n\u2022 Involved in defining the business/transformation rules applied for sales and service data.\n\u2022 Define the list codes and code conversions between the source systems and the data mart.\n\u2022 Worked with internal architects and, assisting in the development of current and target state data architectures.\n\u2022 Coordinate with the business users in providing appropriate, effective and efficient way to design the new reporting needs based on the user with the existing functionality.\n\u2022 Remain knowledgeable in all areas of business operations in order to identify systems needs and requirements.\n\u2022 Responsible for defining the key identifiers for each mapping/interface.\n\u2022 Implementation of Metadata Repository, Maintaining Data Quality, Data Cleanup procedures, Transformations, Data Standards, Data Governance program, Scripts, Stored Procedures, triggers and execution of test plans\n\u2022 Performed data quality in Talend Open Studio.\n\u2022 Coordinated meetings with vendors to define requirements and system interaction agreement documentation between client and vendor system.\n\u2022 Enterprise Metadata Library with any changes or updates.\n\u2022 Document data quality and traceability documents for each source interface.\n\u2022 Establish standards of procedures.\n\u2022 Generate weekly and monthly asset inventory reports.\n\nEnvironment: Erwin r7.0, SQL Server 2000/2005, Windows XP/NT/2000, Oracle 8i/9i, MS-DTS, UML, UAT, SQL Loader, OOD, OLTP, PL/SQL, MS Visio, Informatica.']","[u""Master's""]",[u'']
0,https://resumes.indeed.com/resume/318caa00cdc0d61b,"[u'Data Analyst\nCharles Schwab - Richfield, OH\nFebruary 2017 to Present\nResponsibilities:\n\u2022 Monitor the Database for duplicate records.\n\u2022 Merge the duplicate records and ensure that the information is associated with company records.\n\u2022 Standardize company names, addresses, and ensure that necessary data fields are populated.\n\u2022 Review the database proactively t3 identify inconsistencies in the data, conduct research using internal and external sources to determine information is accurate.\n\u2022 Resolve the data issues by following up with the end user.\n\u2022 Coordinate activities and workflow with other data Stewards in the firm to ensure data changes are done effectively and efficiently\n\u2022 Review the database to identify and recommend adjustments and enhancements, including external systems and types of data that could add value to the system.\n\u2022 Extract the data from database and provide data analysis using SQL to the business user based on the requirements. Create pivots and charts in excel sheet to report data in the format requested\n\u2022 Used VBA for excel to automate the data entry forms to help standardize data\n\u2022 Assist CRM Analyst with Email Marketing Campaigns, including Client Publications, Newsletters, and announcements.\n\u2022 Assist other data Stewards with data Change Management (DCM) Inbox in resolving various tickets created by the User Change Request in Interaction Database.\n\u2022 Developed and Created Logical and Physical Database Architecture using ERWIN.\n\u2022 Designed STAR Schemas for the detailed data Marts and plan Data Marts involving Shared Dimensions.\n\u2022 Coordinated with different users in UAT process.\n\u2022 Conduct Design reviews with the business analysts and content developers to create a proof of concept for the reports.\n\u2022 Ensured the feasibility of the logical and physical design models.\n\u2022 Conducted the required GAP analysis between their AS-IS submission process and TO-BE Encounter Data Submission Process. Professional experience of most advanced SQL Server Database server installation, login and user creation, role and permission configuration\n\u2022 Experience on high performance data integration solutions- including extraction, transformation, and load (ETL) packages for data warehousing\n\u2022 Professional experience on load variety of large volume data files into SQL server database tables\n\u2022 Robust MS Access and Excel skills with Pivot tables\n\u2022 Strong team player with great verbal, written communication and interpersonal skills\n\u2022 Proven ability to approach problems effectively, prioritize and complete multiple tasks\n\u2022 Quick learner, take initiator, and hard worker. Willing to take responsibilities\n\nEnvironment: MS Outlook, MS Project, MS Word, MS Excel, MS Visio, MS Access, Power MHS, Citrix, Clarity, MS SharePoint.', u'Data Analyst\nFirst Financial Bank - Worthington, OH\nSeptember 2015 to November 2016\nResponsibilities:\n\u2022 Created new reports based on requirements. Responsible in Generating Weekly ad-hoc Reports\n\u2022 Planned, coordinated, and monitored project levels of performance and activities to ensure project completion in time.\n\u2022 Automated and scheduled recurring reporting processes using UNIX shell scripting and Teradata utilities such as MLOAD, BTEQ and Fast Load and Experience with Perl\n\u2022 Worked in a Scrum Agile process & Writing Stories with two week iterations delivering product for each iteration\n\u2022 Worked on transferring the Data files to vendor through sftp &Ftp process\n\u2022 Involved in defining and Constructing the customer to customer relationships based on Association to an account & customer\n\u2022 Created action filters, parameters and calculated sets for preparing dashboards and worksheets in Tableau.\n\u2022 Experience in performing Tableau administering by using tableau admin commands.\n\u2022 Worked with architects and, assisting in the development of current and target state enterprise level Data architectures\n\u2022 Worked with project team representatives to ensure that logical and physical Data models were developed in line with corporate standards and guidelines.\n\u2022 Involved in defining the source to target Data mappings, business rules and Data definitions.\n\u2022 Responsible for defining the key identifiers for each mapping/interface.\n\u2022 Performed Data analysis and Data profiling using complex SQL on various sources systems including Oracle and Teradata.\n\u2022 migrated three critical reporting systems to Business Objects and Web Intelligence on a Teradata platform\n\u2022 Created Excel charts and pivot tables for the Adhoc Data pull.\n\u2022 Update accounts using MS SQL in an effort to correct isolated issues in that could not be corrected in mass production.\n\u2022 Lead huddles with the team providing reminders, process updates, and current team statuses.\n\u2022 Work closely with various departments to assist with customer needs- DPSS, DVS, Advocacy, and Trust.\n\u2022 Provide callback reports for both the client specialist and client manager for our large market teams.\n\u2022 Project Management\n\u2022 Reduce cases that negatively impact our SLA by conducting a thorough analysis of all projects currently residing with other team members of our client support group and isolating trends of errors.\n\u2022 Use MS SQL to pull 401k related share request records in progress with other team members and reviewed the transaction accuracy of these request.\n\u2022 Through these efforts was able to reduce pending workflows by 90% and currently maintain this figure.\nEnvironment: Teradata 13.1, Informatica 6.2.1, Ab Initio, Business Objects, Oracle 9i, PL/SQL, Microsoft Office Suite (Excel, Vlookup, Pivot, Access, Power Point), Visio, VBA, Micro Strategy, Tableau, UNIX Shell Scripting ERWIN.', u'Data Analyst\nTakeda Pharmaceuticals - Chillicothe, OH\nApril 2014 to July 2015\nResponsibilities:\n\u2022 Involved in Business and Data analysis during requirements gathering.\n\u2022 Assisted in creating fact and dimension table implementation in Star Schema model based on requirements.\n\u2022 Performed segmentation to extract Data and create lists to support direct marketing mailings and marketing mailing campaigns.\n\u2022 Defined Data requirements and elements used in XML transactions.\n\u2022 Reviewed and recommended database modifications\n\u2022 Analyzed and rectified d Data in source systems and Financial Data Warehouse databases.\n\u2022 Generated and reviewed reports to analyze Data using different excel formats\n\u2022 Documented requirements for numerous Adhoc reporting efforts\n\u2022 Troubleshooting, resolving and escalating Data related issues and validating Data to improve Data quality.\n\u2022 Involved in Regression, UAT and Integration testing\n\u2022 Designed developed and implemented 2 professionally finished systems for tracking IT requests, and providing a Data repository about reports. Documented all system functionality\n\u2022 Participated in testing of procedures and Data, utilizing PL/SQL, to ensure integrity and quality of Data in Data warehouse.\n\u2022 Metrics reporting, Data mining and trends in helpdesk environment using Access\n\u2022 Gather Data from Help Desk Ticketing System and write adhoc reports and, charts and graphs for analysis.\n\u2022 Compiled Data analysis, sampling, frequencies and stats using SAS.\n\u2022 Identify and report on various computer problems within the company to upper management\n\u2022 Report on trends that come up as to identify changes or trouble within the systems using Access and Crystal Reports.\n\u2022 Performed User Acceptance Testing (UAT) to ensure that proper functionality is implemented.\n\u2022 Guide, train and support teammates in testing processes, procedures, analysis and quality control of Data, utilizing past experience and training in Oracle, SQL, Unix and relational databases.\n\u2022 Maintained Excel workbooks, such as development of pivot tables, exporting Data from external SQL databases, producing reports and updating spreadsheet information.\n\u2022 Modified user profiles, which included changing users cost center location, changed users authority to grant monetary amounts to certain departments - monetary amounts were part of the overall budget amount granted per department\n\u2022 Extracted Data from DB2, COBOL Files and converted to Analytic SAS Datasets.\n\u2022 Deleted users from cost centers, deleted users authority to grant certain monetary amounts to certain departments, deleted certain cost centers and profit centers from database\n\u2022 Created a report, using SAP reporting feature that showed which users have not performed scanning of journal voucher documents into the system.\n\u2022 Created Excel pivot tables, which showed a table of users that, have not performed scanning of journal voucher documents. Users were able to find documents by double-clicking on his/her name within the pivot table\n\u2022 Load new or modified Data into back-end Oracle database.\n\u2022 Optimizing/Tuning several complex SQL queries for better performance and efficiency.\n\u2022 Created various PL/SQL stored procedures for dropping and recreating indexes on target tables.\n\u2022 Worked on issues with migration from development to testing.\n\u2022 Designed and developed UNIX shell scripts as part of the ETL process, automate the process of loading, pulling the Data\n\u2022 Validated cube and query Data from the reporting system back to the source system.\n\u2022 Tested analytical reports using Analysis Studio\nEnvironment: SAS/BASE, SAS/Access, SAS/Connect, Informatica Power Center (Power Center Designer, workflow manager, workflow monitor), SQL *Loader, Congas, Oracle, SQL Server 2000, Erwin, Windows 2000, TOAD', u'Data Analyst\nJCPenney - Akron, OH\nJanuary 2012 to March 2014\nResponsibilities:\n\u2022 Analyze the client Data and business terms from a Data quality and integrity perspective.\n\u2022 Perform root cause analysis on smaller self-contained Data analysis tasks that are related to assign Data processes.\n\u2022 Worked to ensure high levels of Data consistency between diverse source systems including flat files, XML and SQL Database.\n\u2022 Develop and run ad hoc Data queries from multiple database types to identify system of records, Data inconsistencies, and Data quality issues.\n\u2022 Involved in translating the business requirements into Data requirements across different systems.\n\u2022 Involved in understanding the customer needs with regards to Data, documenting requirements, developing complex SQL statements to extract the Data and packaging/encrypting Data for delivery to customers.\n\u2022 Participated in the development of Enhancement for the current Commercial and Mortgage Securities Association (CMSA) Application\n\u2022 Wrote SQL Stored Procedures and Views, and coordinate and perform in-depth testing of new and existing systems.\n\u2022 Manipulate and prepare Data, extract Data from database for business Analyst using SAS.\n\u2022 Provided support to Data Architect and Data Modeler in Designing and Implementing Databases for MDM using ERWIN Data Modeler Tool and MS Access.\n\u2022 Worked with Data Modeling team to create Logical/Physical models for Enterprise Data Warehouse.\n\u2022 Reviewed normalized/Renormalization schemas for effective and optimum performance tuning queries and Data validations in OLTP and OLAP environments.\n\u2022 Exploited power of Teradata to solve complex business problems by Data analysis on a large set of Data.', u'Data Analyst\nSprint - Akron, OH\nMay 2007 to December 2012']",[u''],[u'processes the Data using Informatica Power Center Metadata Exchange']
0,https://resumes.indeed.com/resume/34fab03ec3875564,"[u'Data Analyst\nGrant Thornton LLP\nAugust 2017 to January 2018\nResponsibilities:\n\n\u2022 Developed actions, parameters, Filter (Local, Global) and calculated sets for preparing dashboards and worksheets using Tableau.\n\u2022 Scheduled data refresh on Tableau Server for weekly and monthly increments based on the new changes to ensure that the views and dashboards were displaying the changed data accurately.\n\u2022 Experience in creating different Tableau workbooks, dashboards including Bars, Histograms, Pie charts, Maps, Scatter plots, Bubbles and Highlight tables.\n\u2022 Implemented various data connections from data source to Tableau Server for report and dashboard development.\n\u2022 Used Tableau to connect data from SQL Server, build up visualized reports and analysis.\n\u2022 Pipelined (ingest/clean/munge/transform) data for feature extraction toward downstream classification.\n\u2022 Research and analyse user generated queries and tickets.\n\u2022 Implemented Root cause analysis of software issues; develop and deploy fix\n\u2022 Combined visualizations into Interactive Tableau Dashboards and published them to the web portal.\n\u2022 Build homepage to navigate to all reports.\n\u2022 Developed Tableau workbooks from multiple data sources using Data Blending.\n\u2022 Developed various dashboards using different components like Image, Webpage, Horizontal and Vertical in Tableau Desktop and published them on to Tableau Server that allowed end users to understand data in an easy and efficient manner.\n\u2022 Sophisticated stored procedures and triggers to provide complete technical solutions.\n\u2022 Involved in creating calculated fields, parameters, sets and hierarchies.\n\u2022 Involved in creating a dual-axis bar chart with multiple measures.\n\u2022 Generated Database Objects like Stored Procedures, Triggers, Functions, Indexes and Views by using T-SQL, Hadoop environment to create complex scripts and batches.\n\u2022 Created and updated stored procedures and triggers to meet the requirements and changes specified by the users.\n\nEnvironment: SQL Server, MS Office, Tableau 8.X, Tableau Server 8, T-SQL.', u'BI Data Analyst\nICICI Bank - Hyderabad, Telangana\nNovember 2012 to December 2015\nResponsibilities:\n\n\u2022 Retrieved and reported Statistical Data from MS SQL Server and represented into Excel Bar Charts and Histograms;\n\u2022 Worked with Lists, Pivot Tables, VLookups and HLookups, Matrix, Charts, Maps, Calculated Fields and Parameters to generate reports using SQL Server Reporting and Integration Services;\n\u2022 Developed different UML Diagrams based on existing Business and Functional Documentation using MS Visio\n\u2022 Performed Data Analysis and Data validation by writing complex SQL queries.\n\u2022 Analyzed the existing data model and incorporated new additions for the advancement data into the data model identifying the cardinality of the new tables to the existing tables and ensure proper referential integrity of the system\n\u2022 Work with users to identify the most appropriate source of record and profile the data required for sales and service.\n\u2022 Define the list codes and code conversions between the source systems and the data mart.\n\u2022 Involved in defining the source to target data mappings, business rules, business and data definitions\n\u2022 Responsible for defining the key identifiers for each mapping/interface\n\u2022 Responsible for defining the functional requirement documents for each source to target interface.\n\u2022 Involved in configuration management in the process of creating and maintaining an up-to-date record of all the components of the development efforts in coding and designing schemas\n\u2022 Developed the financing reporting requirements by analysing the existing business objects reports\n\u2022 Utilized Informatica toolset (Informatica Data Explorer, and Informatica Data Quality) to analyse legacy data for data profiling.\n\u2022 Establish standards of procedures.\n\u2022 Evaluated data profiling, cleansing, integration and extraction tools (e.g. Informatica)\n\nEnvironment: SQL Server 2012, SSAS, SSIS, SSRS, PL/SQL, SQL Server Agent, T-SQL, Informatica.', u'Data Analyst\nXpert solutions - Vijayawada, Andhra Pradesh\nApril 2011 to November 2012\nResponsibilities:\n\u2022 Conducted data cleaning and data preparation of the data provided by the client to monitor its completeness and accuracy.\n\u2022 Built complex SQL queries and procedures for data loading and extraction.\n\u2022 Created visually impactful dashboards using SSRS\n\u2022 Use data analytics tools to cross reference budget and cost allocation data provided by the client.\n\u2022 Interpret performance analytics data to evaluate trends and make forecasts.\n\u2022 Responsible for the daily management and performance of all databases (SQL and MS Access), their file structure, associated tables, views, and queries.\n\u2022 Interact with computer systems end-users and project business sponsors to determine, document, and obtain signoff on business requirements.\n\u2022 Generate weekly and monthly asset inventory reports.\n\u2022 Document data quality and traceability documents for each source interface\n\u2022 Remain knowledgeable in all areas of business operations in order to identify systems needs and requirements.\n\u2022 Created a conceptual design based on the interaction with the functional and technical team.\n\nEnvironment: MS SQL Server 2005, SharePoint, SSIS.']","[u""Master's in computer science"", u'Bachelors in Electronics and Communication Engineering']","[u'University of Bridgeport Bridgeport, CT\nMay 2017', u'PVP Siddhartha Institute of Technology Vijayawada, Andhra Pradesh\nApril 2011']"
0,https://resumes.indeed.com/resume/4a8c42bce12703d8,"[u'Data Management Specialist\nRoyal Bank of Scotland - Delhi, Delhi\nApril 2015 to June 2017\npart of CDO (Chief data office) team in Informatica which is technical team provide supports to various franchise and functions like C&IB, PBB, CBB, HR, Risk to fulfill their data quality requirement to meet their regulatory & compliance activity.\n\n\u25cf Works with business & functions to complete the DQMS output specification template for each KDE rule, ensuring the DQ rule output meets the business requirement.\n\u25cf Used Informatica Power Center 9.6 for extraction, transformation and load (ETL) of data in the data warehouse.\n\u25cf Extensively used Transformations like Router, Aggregator, Normalizer, Joiner, Expression and Lookup, Update strategy and Sequence generator and Stored Procedure.\n\u25cf Developed complex mappings in Informatica Data Quality 9.6 to load the data from various sources.\n\u25cf Implemented performance tuning logic on targets, sources, mappings, sessions to provide maximum efficiency and performance.\n\u25cf Parameterized the mappings and increased the re-usability.\n\u25cf Used Informatica Power Center Workflow manager to create sessions, workflows and batches to run with the logic embedded in the mappings.\n\u25cf Extensively used Informatica debugger to figure out the problems in mapping. Also involved in troubleshooting existing ETL bugs.\n\u25cf Design the DQMS output to include both standard CDO features and bespoke dashboard capability for each business area.\n\u25cf Implement each DQ dashboard in the DQMS application.\n\u25cf Functionality include ""slice & dice"" capabilities to allow for viewing DQ results by Business, country, source system\n\u25cf Trend analysis is available to monitor DQ progress over time.\n\u25cf Designed Mapping templates by using PowerCenter Mapping Architect for Visio to create multiple mappings for recurring business logic.\n\u25cf Responsible for standards when creating mapplets and code migration from development thru test.\n\u25cf Understanding of out of the box scorecards, dashboards, and data profiling.\n\u25cf Co-ordination during the Requirements Analysis, Development & Testing Phase.\n\u25cf Fixing problems in code during development, maintenance phases and modifying corresponding documentation.\n\u25cf Design the Mapping logic for performing the Business Validations on Inbound and Outbound files.\n\u25cf Implemented Informatica Concurrent Workflow Execution process for improving the \u25cf Developed physical data models and created DDL scripts to create database schema and database objects\n\u25cf Developed functions, views and triggers for automation.\n\u25cf Extensively used Joins and sub-Queries to simplify complex queries involving multiple tables.\n\u25cf Applied business intelligence solution by using Microsoft Power BI.\n\u25cf Built profound Microsoft PowerPivot models with varied data sources such as Database, Flat Files, Access and Excel Files.\n\u25cf Responsible for Developing Dashboard and preparing ad-Hoc Reporting in Tableau.\n\u25cf Transformed and automated data sources by Power Query; applied activities, such as integrate, duplicate, de-duplicate, add primary key and foreign keys, and index to merge and sharp data sources for further analysis.\n\nFinance & Risk Team: Provide supports to Finance & Risk franchise and functions to fulfill their data quality requirement to meet their regulatory & compliance activity.\n\n\u25cf Involved in complete SDLC (System Development Life Cycle).\n\u25cf Developed performance utilization charts, optimized and tuned SQL and designed physical databases. Assisted developers with Teradata load utilities and SQL.\n\u25cf Gathered the required information from the franchise.\n\u25cf Created tables, views in Teradata, according to the requirements.\n\u25cf Used Teradata utilities like MultiLoad, Tpump, and Fast Load to load data into Teradata data warehouse.\n\u25cf Wrote appropriate code in the conversions as per the Business Logic using BTeq scripts.\n\u25cf Loaded the data into the Teradata database using Load utilities like (Fast Export, Fast Load, MultiLoad, and Tpump).\n\u25cf Expertise in performance tuning the user queries. Execution of frequently used SQL operations and improve the performance.\n\u25cf Designing the ETLs and conducting review meets Involvement in implementation of BTEQ and Bulk load jobs.\n\u25cf Preparation of Production support document.\n\u25cf Maintained SQL scripts indexes and complex queries for analysis and extraction.\n\u25cf Prepare Dashboards using calculations, parameters in Tableau.', u'Data Analyst\nHCL Technologies - Noida, Uttar Pradesh\nJune 2011 to April 2015\nThe Restructured Accelerated Power Development and Reforms Programme (RAPDRP) initiated for reducing the Aggregate Technical and Commercial (AT&C) losses in the power sector and improving the quality and reliability of power supply. This achieved by strengthening and upgrading the sub-transmission and distribution system of high density load centers like state, towns and industrial centers.\n\u25cf Design the Mapping logic for performing the Business Validations on Inbound and Outbound files.\n\u25cf Adapt ETL code to accommodate changes in source data and new business requirements.\n\u25cf Created Informatica Mappings to build business rules to load data using transformations like Source Qualifier, Aggregator, Expression, Joiner, Connected and Unconnected lookups, Filters and Sequence, External Procedure, Router and Update strategy.\n\u25cf Stored reformatted data from relational, flat file, XML files using Informatica (ETL).\n\u25cf Perform extensive data analysis and develop data mapping and functional solution design documents.\n\u25cf Ensure data quality is maintained throughout data warehouse.\n\u25cf Design and Implementation of new requirements.\n\u25cf Perform extensive data analysis and develop data mapping and functional solution design documents\n\u25cf Design and creation of SQL tables, views, triggers and other objects required for web statistics & analysis.\n\u25cf Developed SQL Stored Procedures, Tuned SQL Queries (using Indexes and Execution Plan) and User Defined Functions.\n\u25cf Develop ad hoc reports writing advance query concepts (e.g. group by, having clause, union so on).\n\u25cf Reviewed existing business procedures and recommended and implemented changes.\n\u25cf Generating various reports depends on business requirements and data visualization dashboards trend analysis by using Power BI.\n\u25cf Maintained SQL scripts indexes and complex queries for analysis and extraction.\n\u25cf Resolve production issue.\n\u25cf Root cause analysis of Production issue and fixing them.']",[u'B Tech in Computer Science and Engineering'],[u'Amity University\nJanuary 2007 to January 2011']
0,https://resumes.indeed.com/resume/7e8842e475319382,"[u""Senior Data Analyst\nQC Bindery and Mailing - Marietta, GA\nJanuary 2013 to January 2018\nResponsible for daily data intake from multiple clients, performing analysis, managing data imports, and data extraction.\n\u2022 Data analysis and support for vendor invoices from the internal database system.\n\u2022 Work closely with internal and external billing systems to analyze and validate vendor invoices as well as provide trend analysis.\n\u2022 Involved in various projects ranging from ad-hoc reporting and data analysis for 50+ clients.\n\u2022 Design, develop and deploy ad-hoc SQL queries solutions for Operations, Finance and Marketing Department.\n\u2022 Resolves product or service problems by clarifying the customer's complaint; determining the cause of the problem; selecting and explaining the best solution to solve the problem; expediting correction or adjustment; following up to ensure resolution.\n\u2022 Performs other related duties as assigned by management.\n\u2022 Answers customer inquiries/communications as required.\n\u2022 Provides back-up support to other group members in the performance of job duties as required."", u""Senior Data Analyst\nNCO Financial Group - Norcross, GA\nJanuary 2005 to January 2013\nResponsible for daily data file management, resolving data issues, and manage client facing communications.\n\u2022 Managed 400+ files at a given day and managed approximately 45+ clients at a time from across the country.\n\u2022 Lead point of contact for client service related activities and worked proactively as a liaison between employer & clients'.\n\u2022 Troubleshoot and communicated processing status with both internal and external clients.\n\u2022 Handled new business batches, transactions, notes, and reconciliation import jobs and uncover errors between our organization and client's files.\n\u2022 Worked with clients on their files & system issues, and correcting variations to make it compliant to be able to process.\n\u2022 Collected necessary business requirements to accomplish analysis from multiple sources, accumulate it together in the prearranged format and entered the data in several data analysis software""]",[u'Bachelor of Computer Information System in Computer Information System'],[u'South Gujarat University']
0,https://resumes.indeed.com/resume/e2000f872a070dd2,"[u""Data Analyst\nBDM Innovative Solutions - IN\nNovember 2014 to August 2015\nProject Description: The scope of the project was to extract and analyze the KPI's (Click Rate, Response Time and Revenue) of the Shopper's Stop mobile app. Also, evaluating the user feedback and generate reports to help them focus on\ntheir products, services, and customer satisfaction.\nResponsibilities:\n\u2022 Collaborated with cross-functional teams to gather the data requirements.\n\u2022 Created detailed report specification document that included KPI descriptions and data mappings.\n\u2022 Worked closely with the database development teams to help them transform the data from flat files, excel sheets,\npdfs and other databases using ETL (Extract, Transform and Load) transformations in SSIS.\n\u2022 Interviewed the stakeholders and project sponsors for mapping requirements and analyzing structured and normalized data sets.\n\u2022 Written SQL queries using Joins, Triggers, Stored Procedures, Sub-queries, Union, CTE's, Temporary Tables to ensure that the data is being correctly mapped.\n\u2022 Built effective queries for high-performance reporting and rendered them to HTML, XML, PDF and Excel\nformats using SSRS.\n\u2022 Defined and implement routine reports (weekly/monthly) of key performance metrics and action items and alerts.\n\u2022 Generated analysis and develop key insights from various data sources to come up with actionable\nrecommendations.\n\u2022 Worked closely with the business intelligence and Tech team to define, automate, and validate the extraction of new metrics from various data sources for use in future analysis and report.\n\u2022 Presented findings to the team by designing dashboards using multiple visualizations in Tableau so as to improve\ntheir customer service."", u'Programmer Analyst Trainee\nCognizant Technology Solutions - IN\nJuly 2014 to October 2014\n\u2022 Trained on fundamentals of programming languages like C, SQL, and C#.\n\u2022 Learnt about Software Development Life Cycle (SDLC) and Object-Oriented Programming (OOPs) concepts.']","[u'Master of Science in Management Information Systems in Management Information Systems', u'Bachelor of Engineering in Computer Science and Systems Engineering in Computer Science and Systems Engineering']","[u'California State University Los Angeles, CA\nSeptember 2015 to May 2017', u'Sree Vidyanikethan Engineering College\nOctober 2010 to May 2014']"
0,https://resumes.indeed.com/resume/0fb5506b17e8576a,"[u'ANALYST INTERN\nHARVARD T.H. CHAN SCHOOL\nJuly 2016 to December 2016\nLiaised with various business groups in the organization to facilitate cross-functional implementation of new or improved\nbusiness process requirements for all IT related business.\n\u2022 Implemented new applications and enhancement requests for existing applications.\n\u2022 Work with end-users to improve their ability to utilize system capabilities, recognize problems and develop solutions.\n\u2022 Performed end-to-end user acceptance testing and provided training and support for business systems.', u'BUSINESS DATA ANALYST\nALLSCRIPTS HEALTHCARE SOLUTIONS\nJuly 2012 to July 2015\n\u2022 Engaged with end-users for performing and documenting business requirement for implementation of new application, and enhancement requests for existing applications.\n\u2022 Worked with data from multiple sources and in variety of formats (Excel, csv, databases, etc.) to build and deliver executive\ndashboards and reports in a range of formats including Tableau, PowerBI, SSRS.\n\u2022 Developed business performance, forecasting and cash flow dashboards and reports in Tableau by leveraging advance\nfeatures (data blending, filters, sets, calculated fields, level of detail expressions) enabling data-driven decision making.\n\u2022 Extracted and modified data from database using SQL queries and wrote SQL codes, stored procedures, views, functions, etc.\nto answer business questions.\n\u2022 Effectively developed and calculated new KPIs and summary metrics used to communicate trends.\n\u2022 Provided ad-hoc analysis to support client requests, including data summaries and conducting statistical analysis as needed.\n\u2022 Analyzed high volume patient data, generated ad-hoc reports for business decisions and detect anomalies in Excel (using\nfunctions, pivot tables and charts) and SQL.\n\u2022 Updated and cleaned customer data by using Excel, transformed and loaded data in MS SQL, identified key factor that might\naffect the satisfaction of customer.']","[u'MASTER OF SCIENCE IN INFORMATION SYSTEMS in COURSE WORK', u'BACHELOR OF SCIENCE in ELECTRONICS ENGINEERING']","[u'NORTHEASTERN UNIVERSITY Boston, MA\nAugust 2015 to December 2017', u'BAPURAO DESHMUKH COLLEGE OF ENGINEERING\nAugust 2007 to May 2011']"
0,https://resumes.indeed.com/resume/372f092580c3478d,"[u'Data Analyst\nFujitsu - Richardson, TX\nMay 2017 to October 2017\nPricing analysis for new product to improve price points.\nVisualization and reporting to aid in quick business decision making.\nEngineered the analysis framework from scratch thus helping improve the pre sales team with better data analysis\nWorked with product owners to improve product offering', u""Test Engineer\nInfosys Ltd - Pune, Maharashtra\nMay 2014 to June 2016\nSpearheaded testing of custom invoice application on SAP platform with 100% defect identification\n\u25cf Validated business reports and business objects in SAP BI/BW environment handling data from 3 different sources, facilitating 10+ critical business applications\n\u25cf Developed sales orders to cover a span of retail buying scenarios enhancing customer experience\n\u25cf Designed and enhanced test cases based on functional & technical documents, increasing reusability by 30%\n\u25cf Recognized among top 10% performers and awarded the 'Tech Domain Champion' based on high quarterly achievements and client appreciations (US based)""]","[u'M.S. in Business Analytics', u'Bachelors of Technology in Electronics and Communication Engineering']","[u'The University of Texas at Dallas Dallas, TX\nMay 2018', u'Indian Institute of Information Technology\nMay 2014']"
0,https://resumes.indeed.com/resume/0d6e758f745647f7,[],[],[]
0,https://resumes.indeed.com/resume/af6b5b709c491b0b,"[u'Pricing Data Analyst\nEssilor of America - Dallas, TX\nJanuary 2017 to Present\n\u2022 Integrated various data sources (SAS, OBIEE, Salesforce, Optifacts) to analyze the customer data.\n\u2022 Created GRI (General Revenue Increase) price model in SAS VA to analyze pricing increase affect strategy groups revenue in SAS\n\u2022 Developed price sensitively simulation model to analyze impact of pricing based on product features, price elasticity of demand in SAS VA.\n\u2022 Analyzed the ECP sales data to support sales team making competitive price strategy. Designed price bundles, rebates optimization and promotions for customers.\n\u2022 Wrote and ran SAS query to calculate more than 20millon dollar customer quarterly rebates in SAS EG. Worked closely with Finance to generate customer rebates/credits report.\n\u2022 Created SQL query in Optifacts to collect price data. Modified and maintained SQL query to update the price database.', u'Data Analyst\nUTA - Arlington, TX\nJanuary 2016 to January 2017\nAssisted in Lung Cancer cell detection project\n\u2022 Performed intensive data search, data collection, data entry, data cleaning (Nominal to Numeric, Discretization) and data validation to ensure data clinical meaningful and analyzable\n\u2022 Converted the data format to the required format such as PDF, CSV, HTML, TXT\n\u2022 Wrote and edited queries using Oracle SQL Server (select, join) to facilitate data analysis\n\u2022 Conducted the ANOVA analysis of the clinical data using SAS to test new methodology']","[u'Master of Science in Information System in Information System', u'MBA', u'Bachelor of Economics in Economics']","[u'University of Texas at Arlington Arlington, TX\nJanuary 2016', u'University of Texas at Arlington Arlington, TX\nJanuary 2011', u'China Youth University for Political Science Beijing, CN\nJanuary 2005']"
0,https://resumes.indeed.com/resume/a120e8d27cd1098f,"[u'Data Analyst\nWCCDA - Seattle, WA\nJanuary 2015 to January 2016\n\u2022 Collected, organized, and analyzed King county housing and foreclosure data using Microsoft SQL server.\n\u2022 Developed reports in Tableau summarizing housing data by zip code.', u'BI Analyst\nLucintel - Irving, TX\nJanuary 2011 to January 2014\n\u2022 Developed regression models that forecasted GDP growth of various countries. Performed factor analysis, time series analysis, and discriminant analysis that provided insights on investment opportunities.\n\u2022 Analyzed economic, social, political, legal, and technological datasets for multiple countries. Created visualizations that explained the assets to invest in for each country.\n\u2022 Analyzed pricing trends to determine the factors affecting interest and inflation of various economies.\n\u2022 Managed a team of 3 junior data analysts.\n\u2022 Produced risk analysis considering various risk elements like debt position and current account balance of countries. Performed data cleaning and organized unstructured and structured data using Excel pivot tables, add-ins, and formulas.']","[u'Bachelors in Applied Science in Data Analytics', u'Certificate in Mathematical Methods for Economics', u'Masters in Philosophy in Economics in MA Economics']","[u'Bellevue College\nJanuary 2018', u'San Jose State University\nJanuary 2015', u'Pt.RSS University\nJanuary 2007 to January 2011']"
0,https://resumes.indeed.com/resume/96decea3f6039cd8,"[u'Quality Engineer\nDrive DeVilbiss Healthcare - Port Washington, NY\nOctober 2014 to Present\n\u2022 Perform root cause analysis on product failure and present potential solutions in accordance with ISO 13485 quality management system (i.e.: nerve stimulation units, pulse oximeter, blood pressure monitors, nebulizers, and power scooters)\n\u2022 Develop test protocols for new prototypes and analyze\n\u2022 Help trouble shoot with customers as to why their devices are no longer working and provide them with options of replacing', u'Data Analyst\nSouth Shore Neurologic Associates - Patchogue, NY\nJune 2012 to August 2013\n\u2022 Helped analyze data in GAIT program for patients with MS as well as different patients with neurological problems\n\u2022 Helped eliminate any inconclusive data for further research and conclusions to be made\n\u2022 Analyzed the progress of the experimental drug on various patients']","[u'High School Diploma', u'Bachelors of Engineering in Biomedical Engineering in Biomedical Engineering']","[u'Sayville High School West Sayville, NY\nMay 2010', u'Stevens Institute of Technology Hoboken, NJ']"
0,https://resumes.indeed.com/resume/7863c495bcf8b05d,"[u'Quality Data Coordinator\nMusashi Auto Parts - Battle Creek, MI\nJanuary 2016 to January 2018\nWorked with ERP software to accurately track rejected parts. Created new reports in order to communicate information on reject trends more efficiently. Responsible for charging external suppliers for bad parts that were received. Ensured that rejected parts were charged to the correct internal department. Created new charts and reporting methods so reject trends could be more easily visualized and understood. Put together return merchandise authorizations so that suppliers could be charged accurately for poor quality supplied parts.', u'Data Entry Analyst\nTurfgrass Information Center - East Lansing, MI\nJanuary 2011 to January 2015\nWrote abstracts for and assigned indexing keywords to turfgrass-related materials within the MSU Library. Promoted to proof the work of other employees. Assisted parts in finding materials and information when necessary.']","[u""Bachelor's in Advertising""]",[u'Michigan State\nJanuary 2010 to January 2015']
0,https://resumes.indeed.com/resume/1bd179c9ca3b1729,"[u'Student\nOnline\nSeptember 2017 to Present\nCisco System CCNP, AWS Cloud', u'Data Analyst\nFlex - Austin, TX\nMarch 2011 to September 2017\n\u2022 Analyze inventory usage and transactions & Resolved problems between shippers, receivers, and clients\n\u2022 Manage the planning and development of design and procedures for metrics reports\n\u2022 Develop new reports and delegated tasks to team members\n\u2022 Identified unnecessary shipping cost, and reduced them\n\u2022 Assisted in the creation of a novel method of new account information input, using SAP & BAAN systems\n\u2022 Used advanced Microsoft Excel to create pivot tables, used VLOOKUP, and other Excel functions\n\u2022 Proposed solutions to improve system efficiencies and reduce total expenses', u'Network Administrator - Internship\nAL Neelain University - Khartoum, SD\nJune 2009 to December 2009\n\u2022 Configured Cisco Devices and Windows Servers\n\u2022 In-depth training in TCP/IP and Wireless communications\n\u2022 Configuration and maintenance of Router and Switch installation\n\u2022 Installation and maintenance of LAN to accommodate 100+ users\n\u2022 Configuration of switched network with VLANs & trunks\n\u2022 Troubleshooting of LAN, Hardware and Software problems']","[u'Cisco CCNA in Cisco System', u'Bachelor in Computer Engineering']","[u'Austin Community College Austin, TX\nDecember 2017', u'AL Neelain University Khartoum, SD\nDecember 2009']"
0,https://resumes.indeed.com/resume/0f9791cb31f4ebd4,"[u""Sr. Data Analyst\nApria Healthcare - Lake Forest, CA\nMay 2017 to Present\nThe main goal of this project is to refactor the legacy reports from different platforms to SQL server and deploy them on SSRS server, Perform data modeling to create new tables in the data model.\n\nRoles and Responsibilities:\n\n\u2022 Conducting meetings to understand business requirements and liaison between software systems' developers and business management.\n\u2022 Create complex SQL Stored Procedures to generate Reports.\n\u2022 Work with Relational Database (RDBMS) to query the data for validation and analysis.\n\u2022 Writing complex SQL queries to perform data analysis.\n\u2022 Perform Predictive Analysis of Data and develop Predictive Models using R.\n\u2022 Analyzing the data, identifying source of data and data mapping.\n\u2022 Develop source to target mapping document to ensure data flows correctly from source system to target system using business transformation logic.\n\u2022 Write T-SQL queries to analyze claim processing data, Intake Equipment data, and Payor data.\n\u2022 Analyzed HIPAA specific EDI 855, 856 transaction sets for medical equipment purchase order, shipment notification and medical equipment claims data.\n\u2022 Develop SSRS Reports and deploy to production SSRS server.\n\u2022 Develop SSIS packages to perform load data from Source to SQL server.\n\u2022 Develop T-SQL queries to ensure business requirements are implemented correctly.\n\u2022 Develop logical and physical data model in ER Studio.\n\u2022 Perform reverse engineering on the database using ER Studio.\n\u2022 Collaborate with Replication team to replicate data from source system to target SQL database tables.\n\u2022 Analyze the ad hoc queries and provide recommendations for the performance improvement.\n\nTechnical Environment: Microsoft SQL Server 2012, SSRS, SSIS, T-SQL, ER Studio, Version 1, SVN Repository, R 3.4.2, R Studio, Power BI, RDBMS."", u'Sr. Data Analyst\nProspect Medical Systems - Orange, CA\nDecember 2016 to May 2017\nThe main goal of this project is to understand the current healthcare claims process and integrate the new health plans medical claims and pharmacy claims data into Operational Data Source.\n\nRoles and Responsibilities:\n\u2022 Conduct meetings with business to understand the health plan data.\n\u2022 Perform Data profiling using SSIS data profiler task in order to identify the different patterns and values, frequencies of the data in the files.\n\u2022 Create System Specification document to include all the information of Health Plan.\n\u2022 Perform Source to Target mapping document to map health plan data fields to Operational Data Source.\n\u2022 Worked on RDBMS to perform analysis of source data.\n\u2022 Involved in the full HIPAA compliance lifecycle from GAP analysis, mapping, implementation, and testing for processing of Medicaid Claims.\n\u2022 Analyzed and worked with HIPAA specific EDI 835, 837 transactions for claims adjudication, member enrollment, billing transactions\n\u2022 Used SQL queries and utilities to transfer EDI data and databases between different Test environments that are located on different Servers.\n\u2022 Tested HIPAA specific EDI transactions for claims, member enrollment, billing transactions.\n\u2022 Analyzed HIPAA 4010 and 5010 standards for 837P EDI X12 transactions related to providers, payers, subscribers and other related entities.\n\u2022 Create the complete Data Flow Diagram of the new Health Plans to Operational Data Source.\n\u2022 Write T-SQL queries to analyze the Claims data and Pharmacy Data.\n\u2022 Write T-SQL queries to perform the data lineage on the claims data.\n\u2022 Write T-SQL queries to ensure the business requirements are implemented properly.\n\u2022 Develop Logical and Physical data models using Erwin.\n\u2022 Conduct meetings with the business to gather the business requirements and document them and give to developers.\n\u2022 Develop queries to validate and test the SSRS reports.\n\u2022 Analyze the Pharmacy claims data and come up with the business rules to perform the lineage of the claims.\n\nTechnical Environment: Microsoft SQL server 2008, Microsoft SQL Server 2012, T-SQL, ERWIN 9.7, MS Excel, SSRS, SSIS , Windows 7, RDBMS.', u'Sr. Data Analyst\nM&T Bank - Buffalo, NY\nNovember 2015 to October 2016\nThe main goal of this project is to bring the Foreign Exchange System data into the data warehouse and integrating the data with Financial Services Data Model.\n\nRoles and Responsibilities:\n\u2022 Working on Teradata, MS SQL Server 2008/2012, Informatica.\n\u2022 Conducting meetings to understand business requirements and translate those in technical language.\n\u2022 Conducted meetings with business to understand data warehouse and data migration of a newly acquired bank.\n\u2022 Coordinated meetings with vendors to define requirements and system interaction agreement documentation between client and vendor system.\n\u2022 Responsible for defining the key identifiers for each mapping/interface.\n\u2022 Performing data profiling using Informatica Analyst.\n\u2022 Develop T-SQL queries to ensure business requirements are implemented correctly.\n\u2022 Wrote complex queries in T-SQL to manipulate the data as per the mapping specifications.\n\u2022 Writing complex SQL queries to perform data analysis.\n\u2022 Analyzing the data, identifying source of data and data mapping.\n\u2022 Worked on Commercial Construction Loan (CLC) system upgrade.\n\u2022 Writing test scripts to perform RI checks, SCD (Slowly Changing Dimension), CDC (Change Data Capture), Data De-duplication.\n\u2022 Worked with business team to make sure test scripts are run with no defect.\n\u2022 Performing impact analysis of system upgrade on data warehouse and downstream application.\n\u2022 Involved in checking data lineage from source to target.\n\u2022 Develop source to target mapping document to ensure data flows correctly from source system to target system using business transformation logic.\n\u2022 Understanding business requirements and developing trust rank, match, and merge rules.\n\u2022 Review business rules with data governance team and provided mock-up for data simulation and business review.\n\u2022 Collaborate with data modelers, ETL developers in creating the Data Functional Design Document.\n\u2022 Work with business people and ETL developers in the analysis and resolution of data related problems.\n\u2022 Document the complete process flow to describe program development, logic, testing, and implementation, application integration.\n\u2022 Worked with data modelers to create logical and physical data models using ERWIN.\n\u2022 Worked with modeling and development team for inbound mapping to get the data needed and move from stage to base.\n\nTechnical Environment: Teradata, Microsoft SQL server 2008, Microsoft SQL Server 2012, T-SQL, Informatica MDM, MS Excel, Informatica 9.5.0, Windows 7.', u""Sr. Data Analyst\nBMC Health Net Plan - Boston, MA\nSeptember 2014 to October 2015\nManaged Care Organizations (MCOs) & Pharmacy Benefit Managers (PBMs):\n\nThis role was involved in creating MCOs and PBMs Data Marts to maintain all coverage information of health insurances of employers and seniors on Medicare and individuals. Also captures all processed claims & members' medicine delivery data and clinical services data for people with complex medical conditions. Different teams also use MCOs & PBMs data across the organization for Adhoc, daily and weekly reporting purposes.\n\nRoles and Responsibilities:\n\u2022 Led requirements gathering activities and JAD sessions to determine, analyze, and document business processes and fundamentals, and strategic data needs.\n\u2022 Scheduled meetings with developers, system analyst and testers to collaborate resource allocation and project completion using MS Project.\n\u2022 Analyzed the reverse engineered Enterprise Originations (EO) physical data model to understand the relationships between already existing tables and cleansed unwanted tables and columns as part of Data Analysis responsibilities\n\u2022 Designed and implemented basic PL/SQL queries for testing and report/data validation.\n\u2022 Used Data warehousing for Data Profiling to examine the data available in an existing database and also created Data Mart.\n\u2022 Developed and tested PL/SQL scripts and stored procedures designed and written to find specific data.\n\u2022 Participated in the requirements analysis and the data definition for the Oracle based system.\n\u2022 Worked with business users to translate requirements, process flows, and business rules into data models and GUI designs.\n\u2022 Responsible for understanding the design and structure of different databases and writing ad-hoc queries for data mining using different data mining tools\n\u2022 Extensive PL/SQL querying on Staging, Data warehouse and Data Marts.\n\u2022 Identified and documented data sources and transformation rules required populating and maintaining data warehouse content.\n\u2022 Performed data analysis using SQL queries on source systems to identify data discrepancies and determine data quality.\n\u2022 Assisted in migration of data models from Oracle Designer to Erwin and updating the data models to correspond to the existing database structures.\n\u2022 Ensured Error logs and audit tables are generated and populated properly.\n\u2022 Analyzed and designed the business rules for data cleansing that are required by the staging and OLAP & OLTP database.\n\u2022 Was responsible for indexing the tables in the data warehouse.\n\u2022 Implemented the Data Cleansing using various transformations.\n\u2022 Worked with the Business Intelligence and ETL developers on the analysis and resolution of data related problem tickets and other defects and assist them to design of efficient processes to load and manage the data, including a Data Quality Assessment to ensure the quality of the source data will meet the information requirements.\n\u2022 Gathered and documented the Audit trail and traceability of extracted information for data quality.\n\nEnvironment: SQL/Server, Toad Oracle, AQT (Advanced Query Tool), MS-Office, Informatica, ERWIN, XML, Business Objects."", u'Data Analyst\nForcht Bank - Lexington, KY\nNovember 2013 to August 2014\nBuild and analyze a data warehouse to identify inactive, active and newly added investors, track total investments, insert or update data in specified tables. Data from different sources were being integrated and loaded into data warehouse.\n\nRoles and Responsibilities:\n\u2022 Interacted with end client and business analysts to gather requirements.\n\u2022 Worked on designing star schema for detailed data marts and plan data marts involving conformed dimensions.\n\u2022 Conducted logical data analysis and data modeling JAD sessions, communicated data standards.\n\u2022 Involved in capturing all business entities, logical and physical data model designs to demonstrate business process flow, DDL scripts for database objects using Erwin.\n\u2022 Establish and maintain data standards.\n\u2022 Identified opportunities for standardizing data description, integration and archiving and elimination of unnecessary redundancy.\n\u2022 Creation of enterprise metadata that can be used across all line of businesses in Forcht bank.\n\u2022 Created technical documentation for data architecture team, ETL mappings, Source-to-Target mapping, Data model design and capture change artifacts.\n\u2022 Analyzed data results from use of business processing software and provides conceptual solutions to system design work.\n\u2022 Created data mapping documents to capture source of data, business rule to be applied to meet customer needs and target for data.\n\u2022 Performed data analysis at source level and determined key attributes fro designing Fact and Dimension tables using star schema for an effective data warehouse.\n\u2022 Created stored procedures, functions and packages as per business needs for developing ad hoc and robust reports.\n\u2022 Handled the entire testing process including test data creation, developing test cases, execution, and data validation.\n\u2022 Prepared test data and compared the expected results with the actual results.\n\u2022 Documented the test requirements and set the traceability between different requirements.\n\u2022 Worked with project team representatives to ensure that logical and physical ER/Studio data models were developed in line with corporate standards and guidelines\n\nEnvironment: SQL/Server, Oracle 9i, MS-Office, Teradata, Informatica, ERWIN, XML, Tableau', u'Data Analyst\nDirect TV\nJune 2012 to October 2013\nTelecom Expense Management (TEM)\n\nWorked as a Sr. Data analyst, to create a Telecom Expense Management (TEM) data mart required for Business Intelligence and reporting purposes. The Existing Telecom Expense Management (TEM) system which allows the agents to enter, modify new sales/service data is used as one of major source systems.\n\nTelecom Expense Management (TEM) Data Quality Integrity\nWorked as Sr. Data Quality analyst to perform Data Quality Integrity for the claim Telecom Expense Management (ATT-EM) DWH which contains billing information, contract rate analysis , Vendor relationship management information and ensuring expenditures match logical inventory and are in compliance with Carrier contract rates and commitments.\n\nResponsibilities:\n\u2022 Work with users to identify the most appropriate source of record and profile the data required for sales and service.\n\u2022 Involved in defining the business/transformation rules applied for sales and service data.\n\u2022 Define the list codes and code conversions between the source systems and the data mart.\n\u2022 Worked with internal architects and, assisting in the development of current and target state data architectures\n\u2022 Involved in defining the source to target data mappings, business rules, business and data definitions\n\u2022 Responsible for defining the functional requirement documents for each source to target interface.\n\u2022 Document, clarify, and communicate requests for change requests with the requestor and coordinate with the development and testing team.\n\u2022 Document data quality and traceability documents for each source interface\n\u2022 Establish standards of procedures.\n\u2022 Generate weekly and monthly asset inventory reports.\n\u2022 Evaluated data profiling, cleansing, integration and extraction tools (e.g. Informatica)\n\u2022 Coordinate with the business users in providing appropriate, effective and efficient way to design the new reporting needs based on the user with the existing functionality.\n\u2022 Remain knowledgeable in all areas of business operations in order to identify systems needs and requirements.\n\u2022 Implementation of Metadata Repository, Maintaining Data Quality, Data Cleanup procedures, Transformations, Data Standards, Data Governance program, Scripts, Stored Procedures, triggers and execution of test plans.\n\nEnvironment: SQL/Server, Oracle 9i, MS-Office, Teradata, Informatica, ER Studio, XML, Business Objects', u'Data Analyst\nL&T Infotech - Bengaluru, Karnataka\nMay 2011 to May 2012\nProject: HDFC Housing Finance\n\nResponsibilities:\n\u2022 Plan, design, and implement application database code objects, such as stored procedures and views.\n\u2022 Build and maintain SQL scripts, indexes, and complex queries for data analysis and extraction.\n\u2022 Created ad-hoc reports for the upper level management using Stored Procedures and MS SQL Server 2005 Reporting Services (SSRS) following the business requirements.\n\u2022 Created reports by dragging data from cube and wrote mdx scripts.\n\u2022 Created reports by extracting data from cube.\n\u2022 Generated reports using SQL Server Reporting Services 2005/2008 from OLTP and OLAP data sources.\n\u2022 Provide database coding to support business applications using Sybase T-SQL.\n\u2022 Perform quality assurance and testing of SQL server environment.\n\u2022 Develop new processes to facilitate import and normalization, including data file for counterparties.\n\u2022 Work with business stakeholders, application developers, and production teams and across functional units to identify business needs and discuss solution options.\n\u2022 Ensure best practices are applied and integrity of data is maintained through security, documentation, and change management.\n\nEnvironment: SQL Server 2005 Enterprise Edition, T-SQL, Enterprise manager, VBS, Qlikview']",[],[]
0,https://resumes.indeed.com/resume/02928cae4fd4d1e2,"[u""Data Integration Manager\nIntegrated Direct Marketing, Inc - Little Rock, AR\nDecember 2015 to Present\n* Lead all daily customer suppressions across different programs with the decision-making support help Program manager identity potential opportunities\n* Independent develop all source data cleanse, hygiene process then translates into an actionable worksheet for business manager across multi programs\n* Managing, compile and analyze the report, consisted provide first-hand, accurate weekly report to Account Directors\n* Gather and analyze large amounts of dataset quickly and effectively and develop compelling, insightful recommendations with our client's data science team."", u'Data Integration Analyst\nIntegrated Direct Marketing, Inc - Little Rock, AR\nJune 2014 to December 2015\n* Support the analysis of multi-campaign across the company\n* Contribute database build to help account manager make better decisions']","[u'Master of Science in Management Information System', u'Bachelor of Science in System Engineering']","[u'University of Arkansas at Little Rock Little Rock, AR\nMay 2016', u'University of Arkansas at Little Rock Little Rock, AR\nMay 2013']"
0,https://resumes.indeed.com/resume/a1129437b6d6e5a8,"[u'Crime/Data Analyst\nCity of Dallas, Dallas Police Department - Dallas, TX\nMay 2015 to Present\n\u2022 Contributes to the significant reduction in crime rates in Dallas through predictive analysis\n\u2022 Develops databases based on established business rules\n\u2022 Extracts, Transforms, and Loads (ETL) datasets for use by the Police Department\n\u2022 Administers SQL Server Services including Installation, configuration, creating user logins, and assigning permissions\n\u2022 Generates reports, dashboards, and stories using MS Excel, SSRS, and Tableau for easy monitoring of crime and arrests data as well as aid in effective decision making\n\u2022 Generates maps and models to predict crime trends thus helping with allocation of personnel to crime hotspots within the city\n\u2022 Optimizes SSIS packages and stored procedures that improve the processing speed of large files\n\u2022 Creates tables, figures, and reports needed by both internal and external clients using SAS/BASE, SAS/STAT, PROC CHART and GPLOT\n\u2022 Creates data dictionary for managerial references.\n\u2022 Utilizes Excel to provide comprehensive analytical and reporting support to identify opportunities to impact operational results, changes to processes, and support strategic initiatives.\n\u2022 Creates hotspot and density maps to help visualize the spatial variations in Crime within the city thus aiding with the allocation of scarce resources.', u""Data Analyst\nUniversity of North Texas, Toulouse Graduate School - Denton, TX\nDecember 2013 to July 2014\n\u2022 Advanced Toulouse Graduate School's knowledge of which students need help with their PhD program based on outcome of research\n\u2022 Integrated and analyzed data from multiple sources using SSIS and SSAS\n\u2022 Built reports using MS Excel, SPSS Base to show milestone status of PhD students at the University.\n\u2022 Presented monthly and quarterly projects reports to management team using MS-PowerPoint. This allowed the Graduate School to identify which colleges and students need the most help regarding staying on track"", u'Research Analyst\nUniversity of North Texas, Geography Department - Denton, TX\nJanuary 2011 to August 2013\n\u2022 Conducted extensive scientific research on HIV and Diabetes in Texas; Slum Health in Ghana, using SPSS, ArcGIS, Excel, and SAS\n\u2022 Visualized research data using Excel for easy presentation\n\u2022 Applied advanced SPSS skills such as ANOVA, Multiple Regression, and T-tests to convey research results\n\u2022 Created shapefiles and metadata for use by other members of research team', u'Data Analyst\nEnvironmental Protection Agency - Accra, GH\nAugust 2009 to November 2010\n\u2022 Built and maintained accurate database of all mining permits in Ghana using MS Access and SQL 2008\n\u2022 Assisted in compliance monitoring and inspection of mining concessions\n\u2022 Developed monthly mining reports using SSRS, MS Access, and MS Excel for internal and external clients\n\u2022 Reviewed Environmental Management Plans and Environmental Impact Assessment reports prior to providing companies with permits\n\u2022 Used ArcMap to create choropleth maps and density maps about mining areas in Ghana']","[u'Master of Business Administration in Business Analytics and Finance in Business Analytics and Finance', u'Master of Science in Applied Geography in Applied Geography', u'Bachelor of Arts in Geography in Geography']","[u'University of North Texas\nAugust 2018', u'University of North Texas\nAugust 2013', u'University of Ghana Accra, GH\nFebruary 2009']"
0,https://resumes.indeed.com/resume/e68c425ae5510fc8,"[u""Senior Data Analyst\nBOLDSEAS TECHNOLOGIES, CO., LTD - Shanghai, CN\nMay 2016 to July 2017\n\u2022 Provided guidance and consult over database, analysis, and CRM for Fiat Chrysler Automobiles China to improve sales and marketing outcomes of JEEP brand; contributed to 20 consecutive months of 2-digit sales improvement.\n\u2022 Extracted data and prepared regular data reports on customer service for JEEP brand and local dealers to monitor the daily sales and marketing performance; built a successful data-driven system for dealer management.\n\u2022 Analyzed data and drew conclusions for improving customers' experience and assisted customer service department to perfect the standardized customer service processes; improved customer satisfaction by ten percentage points.\n\u2022 Built customer models by data mining methodologies - decision trees and association rules - with R programming to find out the high-potential customers in sales and after-sales and merge the results into a web application used by sales assistant from local dealer; ameliorated the dealer's efficiency on sales and after-sales processes.\n\u2022 Developed marketing and sales data visualization system, which helped leaders with decision-making and to manage clients (Fiat Chrysler Automobiles China)."", u""Data Analyst\nLOUIS VUITTON CHINA - Shanghai, CN\nJune 2012 to May 2016\n\u2022 Organized, cleansed, and processed customers' data for the brand by using Excel, Access, and SPSS Modeler.\n\u2022 Built customer models and analyzed patterns by data mining methodologies, including decision trees, association rules, and clustering, with SPSS Modelers to classify potential customers for certain events or new products.\n\u2022 Established customer communications strategies, including mailing, e-mailing, and MMS campaigns, according to customer models; improves customer retention by 2%.\n\u2022 Advised on the customer service processes to sales colleagues based on customer patterns; improved ten percentage points for customer satisfaction.""]","[u'Master of Science in Programming for Analytics', u'in Leadership', u'Bachelor of Science in Statistics']","[u'THE GEORGE WASHINGTON UNIVERSITY, School of Business Washington, DC\nDecember 2018', u'School of Management\nJune 2012 to July 2013', u'FUDAN UNIVERSITY, School of Management Shanghai, CN\nJuly 2013']"
0,https://resumes.indeed.com/resume/841eef6715ce70fd,"[u""Operational Analyst\nBarclays Investment Bank - New York, NY\nJune 2015 to December 2016\nworked in same position as a contractor from APRIL 2012-APRIL 2014\n\u2022 Designed, documented, and maintained daily processes to reconcile fees charged from all North\nAmerican exchanges and to Barclays' clients, including loading data from Excel spreadsheets into an Access database using macros and using queries to process that data and create reports\n\u2022 Collaborated with Finance, Business, and Operational departments worldwide to arrange\npayments to and from clients, resulting in $850k accrual of previously uncharged fees\n\u2022 Diagnosed root causes of issues, including issues with technology and employee habits, in multiple fee and commission processes\n\u2022 Orchestrated reports and presentations for senior global managers both quarterly and as requested"", u'Data Analyst\nHSBC - New York, NY\nSeptember 2014 to June 2015\nSupported tax withholding and reporting team, corrected errors created by new reporting\nsoftware and confirming client data in use was correct']","[u""Bachelor's""]",[u'']
0,https://resumes.indeed.com/resume/dc18d97d4b9ab70f,"[u""Data analyst intern\nCAF SIGNALLING - Madrid, Madrid\nJuly 2015 to August 2015\nDesign and programming of a tool to analyze the company's projects Key\nPerformance Indicators, by linking the ERP with Excel using Visual Basic""]","[u'MS in Industrial Technology and Operations', u'MS in Industrial Engineering']","[u'Illinois Institute of Technology Chicago, IL\nJanuary 2017 to January 2018', u'Comillas Pontifical University\nJanuary 2016 to January 2018']"
0,https://resumes.indeed.com/resume/010ee073373caf7f,"[u'Business Data Analyst\nIndianic Infotech\nDecember 2016 to Present\nComplete a number of monthly reports and inform and change reports on request.\n* Supported and maintained current Tableau Reports.\n* Report generation and design using SQL queries and Tableau.\n* Develop new reports where suitable.\n* Research and analyse user generated queries and tickets.\n* write ad hoc queries and develop to support business needs\n* Conduct root cause analysis for SQL performance issues and create preventative measures\n* Perform study on forecasts, demand, income, capital and expense for products.\n* Prepared income and demand presentations in PowerPoint and Excel\n* Performed market place analysis to attain product goals and strategies\n* Lead the planning, recognition, development and completion of design and * changes to keep products metrics reports', u'Data Analyst\nTELCOM & DATA - Chicago, IL\nNovember 2016 to November 2016\n\u2022 Retrieved and reported Statistical Data from MS SQL Server and represented into Excel Bar Charts and Histograms.\n\u2022 Worked with Lists, Pivot Tables, VLookups and HLookups, Matrix, Charts, Maps, Calculated Fields and Parameters to generate reports using SQL Server Reporting and Integration Services.\n\u2022 Performed Data Analysis and Data validation by writing complex SQL queries.\n\u2022 Analyzed the existing data model and incorporated new additions for the advancement data into the data model identifying the cardinality of the new tables to the existing tables and ensure proper referential integrity of the system\n\u2022 Work with users to identify the most appropriate source of record and profile the data required for sales and service.\n\u2022 Created custom reports from SQL and Oracle databases, using Access, Excel and Crystal Reports.\n\nSECURITY CLEARNACE: ELIGIBLE BUT NOT APPLIED']","[u'MASTER OF SCIENCE in Computer Science', u'BACHELOR OF SCIENCE', u'in Mathematics']","[u'GUJARAT UNIVERSITY Gujarat, IN', u'GUJARAT UNIVERSITY', u'C. U. Shah Science College Gujarat, IN']"
0,https://resumes.indeed.com/resume/b270d9c799547a8d,"[u'Data Analyst\nAbhudaya Multimedia PVT Ltd - Indore, MADHYA PRADESH, IN\nJuly 2015 to July 2016\n\u2022 Interpreted data from primary and secondary sources using statistical techniques.\n\u2022 Performed daily data queries and prepared reports on daily, weekly, monthly, and quarterly basis.\n\u2022 Used advanced Excel functions and VLOOKUP to generate spreadsheets and pivot tables.']","[u'Master of Science in Information Technology', u'in Business', u'Master of Science in Management Information System']","[u'University of North Carolina Charlotte, NC\nJanuary 2017 to December 2017', u'Center FOR Medicare & Medicaid\nJanuary 2017 to May 2017', u'Utah State University\nAugust 2016 to December 2016']"
0,https://resumes.indeed.com/resume/13439f4218d3987a,[u'Data Analyst/Developer\nSantanna Energy Services\nMarch 2012 to September 2013\nAs a Data Analyst/Developer I performed the following:\n1 Run daily processes to generate lists and files\n\n2 Create reports as needed\n\n3 Write SQL procedures and improve efficiency within existing procedures\n\n4 Write and debug programs in C#\n\n5 Troubleshoot issues as they would arise through the help desk\n6 Work with my team to plan and implement new projects'],[u'Associates in Computer Science'],[u'Texas State Technical College\nSeptember 2005 to May 2009']
0,https://resumes.indeed.com/resume/77ca991c513ab228,"[u'Data Analyst\nUniversity of Michigan School of Public Health \xd7 Medical School\nMay 2017 to Present\n\u2022 Extract and preprocess medical claim data of prostate cancer from insurance database using SAS.\n\u2022 Develop statistical models to detect the association between prescription patterns and patient features.\n\u2022 Study the theory of regression models for interaction analysis under misspecification of main effects.\n\u2022 Design and run simulations in high-performance computing (HPC) cluster.\n\u2022 Contribute to two papers as first author and brainstorm in weekly meetings with statisticians.', u'Marketing Intern\nGuotai Junan Securities Co. Ltd - Guangzhou, CN\nJuly 2015 to August 2015\n\u2022 Assisted research investigators in creating a neural network model to study the customer relationship network\nin China Unicom and gain historical 30% coincidence rate.\n\u2022 Investigated the effects of customer churn on the social network and improved marketing methods.']",[u'M.S. in Statistics'],"[u'University of Michigan Ann Arbor, MI\nSeptember 2016 to May 2018']"
0,https://resumes.indeed.com/resume/5a8c6cae85f7bc02,"[u'CATEGORY MANAGEMENT ANALYST\nWHITE WAVE FOODS\nNovember 2017 to Present\n\u2022 Created and managed reports including monthly region, market & customer trend reporting\n\u2022 Performed weekly and monthly updates for various reports\n\u2022 Performed data integrity and validation checks', u'DATA ANALYST\nVISA, INC\nApril 2016 to July 2017\n\u2022 Performed analysis and presented results using SQL, SSIS, MS Excel\n\u2022 Imported, exported and manipulated large data sets under tight deadlines\n\u2022 Created reports in Excel by using pivot table, v-lookups, formulas, conditional formatting', u'DATA ANALYST\nURBAN LENDING SOLUTIONS\nJanuary 2012 to September 2015\n\u2022 Performed and conducted complex reporting analytics\n\u2022 Populated tables, databases for collection, tracking and reporting of data\n\u2022 Built SSIS packages to extract, transform and load data to data warehouse', u""UNDERWRITER\nWELLS FARGO HOME LOANS\nDecember 2010 to January 2012\n\u2022 Underwrote 40+ loans monthly; FHA, VA and Conventional\n\u2022 Reviewed borrower's income, assets, liabilities and required funds to close\n\u2022 Received conditional loan approvals with list of conditions and met all requirements""]","[u'B.S. in Computer Science in Mathematics, Physics']","[u'MISSOURI STATE UNIVERSITY Springfield, MO']"
0,https://resumes.indeed.com/resume/450bd7991af9bb8a,"[u'Data Analyst\nCenter for Community and Business Research\nJanuary 2015 to Present\n\u2022 Responsibilities:\n\u2022 Developing advanced SQL and PL/SQL queries for querying extremely large socio-economic and health care\ndatasets from multiple sources.\n\u2022 Highly involved in developing predictive models using Python to analyze the data and also responsible for performing data mining on Climate datasets using R Programming and produced statistical reports for various\nbusiness needs.\n\u2022 Responsible for pulling the data from multiple sources into spreadsheet using Beautiful Soup Python Library\n\u2022 Highly involved in using Talend Open Studio for Big Data Integration 5.5 and Hortonworks sandbox 1.3 for translating ETL/ELT process to a Mapreduce process in Hadoop environment by seamlessly integrating\nHadoop applications into Data Architecture and also responsible for performing data profiling and data\nmasking using Talend Open Studio for Data Quality and Master Data Management respectively.\n\u2022 Designed and maintained ETL process of developing Tableau Data Extract files using unstructured data,\nfixed width positional files and loaded them into Tableau Desktop.\n\u2022 Worked with various stakeholders in gathering requirements and developing dynamic ad-hoc reports and publishing them into Tableau Server.\n\u2022 Created dashboards that displays key performance indicators for decision making purpose using Heat maps, Geo\nmaps, Tree maps, Pie charts, Bar charts, Circle views, Line charts, Area charts, Scatter plots, and Bullet\ngraphs.\n\u2022 Designed Data Visualizations using Data story telling in Tableau to provide dynamic and effective presentation of the actionable insights within the data that are responsible for creating a business impact.', u""Data Analyst\nDigital Delight Computer Software Trading LLC\nJanuary 2014 to December 2014\n\u2022 Responsibilities:\n\u2022 Provided product supply chain data and trends used to develop, monitor and evaluate performance of the organization by constantly tuning the data and making a positive business impact in the organization.\n\u2022 Highly involved in analyzing product supply chain data, sales and revenue data, customer's data and designed an ETL process for creating a data extract file to Tableau and generating data visualization that made a huge\nbusiness impact and helping the organization to identify key performance indicators in their product sales\ndata.\n\u2022 Performed data analysis, ETL, generated reports, listings and graphs using BI tools like SSRS and Tableau.\n\u2022 Designed and maintained optimized and automated reports using SQL server reporting services SSRS\n\u2022 Responsible for performing predictive analytics using R on product supply chain data and generated reports\nusing Forecasting analysis and Trend lines in Tableau.\n\u2022 Developed T-SQL queries for manipulating the data and created reports using Business Intelligence tools.\n\nPROJECTS\nReal time Sentiment Analysis and Data Visualization\n\u2022 Developed a sentiment analysis on real-time live twitter data to get actionable insights from it and used ETL to transform the data and summarized the large dataset using text classification, tf-idf techniques and constructed\ntopic modeling based analytical framework for real time monitoring.\n\nPredicting Significant Contributors per Candidate Name from recent elections\n\u2022 Developed a time-series analysis based on variables like occupation, employer, state from the election dataset to check the rate of contribution over time for all candidate names and generated automated reports using Tableau.\n\nPredicting Continuous Patterns in housing data and data visualization\n\u2022 Developed a regression model using random forest that predicts the housing prices from a set of explanatory\nvariables and created forecasting analysis for the housing prices to find the costs based on location for the coming\nyear.\n\nEmbedding a real time health care data into a web application\n\u2022 Developed a training model for a real-time health care data that uses a document classifier to classify the patient\ndata into two categories, say in-patient and out-patient and embedded this model into a web application using a\nSQLlite database.""]","[u'Master of Science in Computer Science', u""Bachelor's in Computer Science and engineering""]","[u'The University of Texas at San Antonio San Antonio, TX\nDecember 2016', u'Anna University\nMay 2014']"
0,https://resumes.indeed.com/resume/2a11aedf85b31774,"[u'Associate Data Analyst\nNestle Waters\nDecember 2010 to Present\nResponsibilities:\n\u2022 Apply computational methods to mine and model data generated from daily and hourly QC tests\n\u2022 Create statistical models on the factory generated data to improve production efficiency.\n\u2022 Trained new hires on GLP, GMP, Factory Hygiene, data entry at New Hire Orientation\n\u2022 Designed applications of Machine learning, Statistical Analysis and Data visualizations with challenging large data processing problems.\n\u2022 Implemented predictive models using machine learning algorithms linear regression and linear boosting algorithms and performed in- depth analysis on the structure of models, compared the performance of all the models and found tree boosting is the best for the prediction.\n\u2022 Extensively used Azure Machine Learning to set up the experiments and creating Web services for the predictive analytics\n\u2022 Worked on writing complex SQL queries in performing Data analysis using window functions, joins, improving performance by creating partitioned tables,\n\u2022 Prepared multiple dashboards using Tableau to reflect the data behavior over period of time Analyzed and worked with all aspects of regression models (OLS etc.)\n\u2022 Responsible for working with stakeholders to troubleshoot issues, communicate to team members, leadership and stakeholders on findings to ensure models are well understood and optimized.', u'Data Quality Analyst\nMarshfield Food Safety LLC\nJune 2008 to December 2010\nResponsibilities:\n\u2022 Worked on data to increase cross-& up-sell revenues, enhance customer value or reduce non-credit losses.\n\u2022 Contributed implementing models to identify, extract, summarize, and reduce or categorize the relevant qualitative financial input information like sentiment/feedback/news according to specific structures (templates) from a source text (digital news) to support decision making.\n\u2022 Analyze customer consuming behavior and discover value of customers.\n\u2022 Building, publishing& scheduling customized interactive reports& dashboards using Tableau Server.\n\u2022 Deliver Interactive visualizations/dashboards using Tableau to present analysis outcomes in terms of patterns and predictions.\n\u2022 Created multiple workbooks, dashboards, and charts using calculated fields in Tableau to meet business needs.\n\u2022 Prepare comprehensive documented observations, analyses and interpretations of results including technical reports, summaries, protocols and quantitative analyses. Working closely with marketing team to deliver actionable insights from huge volume of data, coming from different marketing campaigns and customer interaction matrices such as web portal usage, email campaign responses, public site interaction, and other customer specific parameters.']","[u'MBA in Project Management', u'B.S. in Biotechnology']","[u'Amberton University\nFebruary 2018', u'West Texas A&M University']"
0,https://resumes.indeed.com/resume/98e86371df11093d,"[u'Data analyst\nPROFFESIONAL EXPIERIENCE\nMay 2017 to December 2017\n\u2022 Developed GUI widgets for start-up real estate Company using JavaScript\n\u2022 Successfully interpreted data to draw conclusions for managerial action and strategy\n\u2022 Used advanced Excel functions to generate spreadsheets and pivot tables', u'Analyst Programmer\nPG&E - Concord, CA\nJanuary 2016 to April 2017\nContract)\n\n\u2022 Developed applications using Visual Basic and Excel VBA to automate reporting\n\u2022 Used SQL queries and Excel VBA subroutines to retrieve utility usage and finance data sets from various sources\n\u2022 Wrote original methodologies and converted to VBA code and Excel Formulas\n\u2022 Performed ad-hoc analytics for utility sustainability and finance departments\n\u2022 Provided feedback for decision making\n\u2022 Communicated effectively in collaboration with other members of various departments', u'Data Analyst\nPepsiCo - Pleasanton, CA\nJanuary 2014 to February 2015\nContract)\n\n\u2022 Analyzed daily reports, trends, and other pertinent data to trade promotions and finance using SQL\n\u2022 Worked with the sales & marketing teams by providing them with the necessary information and guidance for short & long-range forecasts for their Sales Planner\n\n\u2022 Built SAP ERP event structures based on sales team planner databases', u'Business Analyst Intern\nInmet\nJune 2011 to August 2011\n2012, 2013\n\n\u2022 Helped Carry out financial analysis at multiple levels including rate of return, depreciation, working capital, investments, budgeting, and cost analysis\n\u2022 Evaluated data, analyzed trends, and presented results to managers\n\u2022 Developed applications using Visual Basic and Excel VBA to automate reporting']",[u'Bachelor Degree in Economics'],[u'California State University\nJanuary 2013']
0,https://resumes.indeed.com/resume/0bf7fdd4c168a508,"[u""Data Analyst\nNorth Windsor Inc\nJuly 2015 to Present\nUSA\nCustomer Segmentation and Marketing Campaign\n\u2022 Checked data quality using SQL and SAS functions such as PROC MEANS, PROC FREQ, PROC UNIVARIATE, etc.\n\u2022 Transformed source data formats to target data formats based on transformation rules using FORMAT statement, SAS MARCO, etc.\n\u2022 Developed customer segments based on consumer profile and credit history data using Linear Model and Cluster Analysis\n\u2022 Assigned different products and offers for customers based on the customer segment and campaign matrix\n\u2022 Created 2 one-row-per-subject SAS datasets including Customer information(ID, SSN, Birthdate, Age, Fico Score, etc. 25 elements in total) for customer segment and customers' marketing channel for marketing team\n\u2022 Reviewed and modified SAS Programs to create customized ad-hoc reports, processed data for publishing business reports\nRisk Management\n\u2022 Created a multiple-rows-per-subject file containing customer and roll-rates\n\u2022 Performed 3-month moving average analysis of roll-rates using SUM, LAG and N functions\n\u2022 Converted the multiple-rows-per-subject dataset to a one-row-per-subject SAS dataset based on transformation rule\n\u2022 Created an Event Indicator for delinquency with value of 1 or 0 for every customer\n\u2022 Calculated the time period from the beginning of study to the time that they charged-off using INTCK function to evaluate delinquency possibilities\n\u2022 Forecasted a list of customers with different delinquency history to see the probability that they would default in the future based on logistic regression analysis\n\u2022 Predicted potential risks and customers' behavior for client company"", u'Data Analyst\nChina Life Insurance Company\nMay 2011 to June 2013\nChina\n\u2022 Compared the total insurance rates, sales channels among biggest 6 health insurance companies using Excel\n\u2022 Predicted the potential risk, estimate cost and reply rates for 5 different sales channels using Excel and SQL\n\u2022 Analyzed customer preference for health insurance based on age, education level, salary, dependent(s), etc. by using SQL\n\u2022 Improved insurance products based on customer preference']","[u'Master of Science in Information Technology Management in Information Technology Management', u'Master of Science in Finance in Finance']","[u'The University of Texas at Dallas Dallas, TX\nMay 2016', u""Xi'an International Studies University Xi'an, CN\nJuly 2012""]"
0,https://resumes.indeed.com/resume/e524d2497f2fb755,"[u'Data Scientist\nAmerican Express - Little Rock, AR\nJuly 2017 to Present\nResponsibilities:\nInteraction with Business Analyst, SMEs, and other Data Architects to understand Business needs and functionality for various project solutions, Researched, evaluated, architected, and deployed new tools, frameworks, and patterns to build sustainable Big Data platforms for the clients\n\nParticipated in all phases of data mining, data collection, data cleaning, developing models, validation, visualization, and performed Gap analysis, Data Manipulation and Aggregation from a different source using Nexus, Toad, Business Objects, Power BI and Smart View, Worked as Data Architects and IT Architects to understand the movement of data and its storage and ER Studio 9.7, Data transformation from various resources, data organization, features extraction from raw and stored.\n\nSetup storage and data analysis tools in Amazon Web Services cloud computing infrastructure, Used pandas, numpy, Seaborn, scipy, matplotlib, sci-kit-learn, NLTK in Python for developing various machine learning algorithms, Installed and used Caffe Deep Learning Framework, Implemented Classification using supervised algorithms like Logistic Regression, Decision trees, KNN, Naive Bayes.\n\nWorked on different data formats such as JSON, XML and performed machine learning algorithms in Python, Implemented Agile Methodology for building an internal application, Designed both 3NF data\nmodels for ODS, OLTP systems and dimensional data models using Star and Snowflake Schemas, Focus on integration overlap and Informatica newer commitment to MDM with the acquisition of Identity Systems.\n\nAs Architect delivered various complex OLAP databases/cubes, scorecards, dashboards and reports, Handled importing data from various data sources, performed transformations using Hive, Map Reduce, and loaded data into HDFS.\n\nProgrammed a utility in Python that used multiple packages (scipy, numpy, & pandas), Updated Python scripts to match training data with our database stored in AWS Cloud Search, so that we would be able to assign each document a response label for further classification.\n\nIdentifying and executing process improvements, hands-on in various technologies such as Oracle, Informatica, and Business Objects.\n\nEnvironment: R 9.0, Informatica 9.0, ODS, OLTP, Oracle 10g, Hive, OLAP, DB2, Metadata, MS Excel, Mainframes MS Vision, Rational Rose.', u""Data Scientist\nWalmart - Bentonville, AR\nApril 2016 to June 2017\nResponsibilities:\nAnalyzed the business requirements of the project by studying the Business Requirement Specification document.\n\nPerformed Exploratory Data Analysis and Data Visualizations using R, and Tableau, Extensively worked on Data Modeling tools Erwin Data Modeler to design the data models, Worked with Data Governance, Data quality, data lineage, Data architect to design various models and processes, Developed, Implemented & Maintained the Conceptual, Logical & Physical Data Models using Erwin for forwarding/Reverse Engineered Databases.\n\nDesigned tables and implemented the naming conventions for Logical and Physical Data Models in Erwin 7.0, Explored and Extracted data from source XML in HDFS, preparing data for exploratory analysis using data munging, Designed data models and data flow diagrams using Erwin and MS Visio.\n\nPerform a proper EDA, Uni-variate and bi-variate analysis to understand the intrinsic effect/combined effects, Independently coded new programs and designed Tables to load and test the program effectively for the given POC's using with Big Data/Hadoop, Designed a mapping to process the incremental changes that exist in the source table. Whenever source data elements were missing in source tables, these were modified/added inconsistency with third normal form based OLTP source database.\n\nCreated indexes, made query optimizations. Wrote stored procedures, triggers utilizing T-SQL, Explained the data model to the other members of the development team. Wrote XML parsing module that populates alerts from the XML file into the database tables utilizing JAVA, JDBC, BEA WEBLOGIC IDE,\nAnd Document Object Model, As an Architect implemented MDM hub to provide clean, consistent data for an SOA implementation.\n\nEnvironment: SQL Server 2008R2/2005 Enterprise, SSRS, SSIS, Crystal Reports, Hadoop, Windows Enterprise Server 2000, DTS, SQL Profiler, and Query Analyzer."", u""Data Scientist\nLSB Industries Inc - Oklahoma City, OK\nDecember 2014 to March 2016\nResponsibilities:\nGathering all the data that is required from multiple data sources and creating datasets that will be used in the analysis.\n\nImplemented end-to-end systems for Data Analytics, Data Automation and integrated with custom visualization tools using R, Mahout, Hadoop, and MongoDB, Performed Exploratory Data Analysis and Data Visualizations using R, and Tableau, Coded R functions to interface with Caffe Deep Learning Framework, Worked with several R packages including knitr, dplyr, SparkR, CausalInfer, space-time, Developed, Implemented & Maintained the Conceptual, Logical & Physical Data Models using Erwin for forwarding/Reverse Engineered Databases.\n\nWorked with Data Governance, Data quality, data lineage, Data architect to design various models and processes, Performed data cleaning and imputation of missing values using R, Used Hive to store the data and perform data cleaning steps for huge datasets.\n\nIndependently coded new programs and designed Tables to load and test the program effectively for the given POC's using with Big Data/Hadoop, Worked with Hadoop eco system covering HDFS, HBase, YARN, and MapReduce, Designed data models and data flow diagrams using Erwin and MSVisio, Created dash boards and visualization on regular basis using ggplot2 and Tableau, Established Data architecture strategy, best practices, standards, and roadmaps\n\nPerform a proper EDA, Uni-variate and bi-variate analysis to understand the intrinsic effect/combined effects, Working in Amazon Web Services cloud computing environment, As an Architect implemented MDM hub to provide clean, consistent data for an SOA implementation, Worked with BTEQ to submit SQL statements, import and export data, and generate reports in Teradata.\n\nLead the development and presentation of a data analytics data-hub prototype with the help of the other members of the emerging solutions team\n\nInteracted with the other departments to understand and identify data needs and requirements and work with other members of the IT organization to deliver data visualization and reporting solutions to address those needs.\n\nEnvironment: Erwin r, Informatica, ODS, OLTP, Oracle 10g, Hive, OLAP, DB2, Metadata, MS Excel, Mainframes MS Visio, Rational Rose, and Requisite Pro, Hadoop, PL/SQL, etc."", u'Data Analyst/Data Modeler\nAccenture - Bengaluru, Karnataka\nApril 2009 to October 2013\nResponsibilities:\nDeveloped Internet traffic scoring platform for ad networks, advertisers, and publishers (rule engine, site scoring, keyword scoring, lift measurement, linkage analysis), Implementation of Metadata Repository, Maintaining Data Quality, Data Cleanup procedures, Transformations, Data Standards, Data Governance program, Scripts, Stored Procedures, triggers and execution of test plans.\n\nDeveloped new hybrid statistical and data mining technique known as hidden decision trees and hidden forests, Designed the architecture for one of the first analytics 3.0. Online platforms: all-purpose scoring, with on-demand, SaaS, API services. Currently under implementation, Web crawling and text mining techniques to score referral domains, generate keyword taxonomies, and assess commercial value of bid keywords.\n\nReverse engineering of keyword pricing algorithms in the context of pay-per-click arbitrage, coordinated meetings with vendors to define requirements and system interaction agreement documentation between client and vendor system, Creation of multimillion bid keyword lists using extensive web crawling. Identification of metrics to measure the quality of each list (yield or coverage, volume, and keyword average financial value).\n\nAutomated bidding for advertiser campaigns based either on keyword or category (run-of-site) bidding, Responsible for defining the key identifiers for each mapping/interface, Enterprise Metadata Library with any changes or updates, Document data quality and traceability documents for each source interface, Establish standards of procedures, Generate weekly and monthly asset inventory reports.\n\nEnvironment: Erwin r, SQL Server 2000/2005, Windows XP/NT/2000, Oracle, MS-DTS, UML, UAT, SQL Loader, OOD, OLTP, PL/SQL, MS Visio, Informatica.']","[u""Bachelor's""]",[u'']
0,https://resumes.indeed.com/resume/5588fcc7f8f709e7,"[u'Data Analyst\nTata Consultancy Services - Mumbai, Maharashtra\nAugust 2015 to July 2017\nTealium Tag Management (Tealium, JavaScript, Jira)\n\u2022 Contributed to development of new e-commerce website for ToysRUs.com.\n\u2022 Set up and upgraded the tag management system using Tealium, across multiple geographies.\n\u2022 Resulted in enhancing web analytics, digital marketing and enriching customer experience.\n\u2022 Worked on all phases of SDLC using Agile methodologies.\n\u2022 Led the offshore team of three members.\n\u2022 Collaborated with clients and third-party tag vendors like Facebook, Adobe etc.']","[u'Masters in Information Technology and Analytics', u'Bachelor of Engineering in Computer Engineering']","[u'Rutgers Business School New Jersey\nSeptember 2017 to December 2018', u'St. Francis Mumbai, Maharashtra\nJuly 2011 to June 2015']"
0,https://resumes.indeed.com/resume/27f20553dde65b46,"[u'SAS Programmer\nNV Lifecare - Ahmedabad, Gujarat\nDecember 2014 to July 2016\nAs a SAS programmer, I have built sophisticated Proc reports using style attributes. I was involved in data cleaning, data manipulation, requirement gathering, generating reports, statistical analysis, and data analysis.\n\nResponsibilities:\n\u2022 Performed analysis on campaign response datasets and created reports with extensive use of BASE SAS, Macros, SAS reports and procedures to produce tables, listings and graphs.\n\u2022 Converted Excel files to SAS dataset using PROC IMPORT.\n\u2022 Extensively used macros and macro functions in generating TLFs.\n\u2022 Performed SAS Procedures such as SUMMARY, FREQ, MEANS, UNIVARIATE, SORT, PRINT, TABULATE, GPLOT, GCHART and REPORT, in order to compute elementary statistical measures, and produce data listings, tabulations & graphical displays.\n\u2022 Worked on Administration, Database Design, Performance Analysis, and Production Support for Large (VLDB) and Complex Databases.\n\u2022 Created and maintained various databases for Production, Development and Testing environments using MS SQL Server 2014 and 2012.\n\u2022 Creating, cleaning, validating and updating SAS data sets. Implementing SAS programs by SAS procedure and SAS Macros.\n\u2022 Developed load scripts to work with multiple data sources (SQL, Flat Files, Excel, and MS Access).\n\u2022 Migrated data from Flat Files to Oracle database (Exadata) by designing scripts in SAS for compatibility with Oracle.\n\u2022 Used SAS to develop various ad-hoc programs and reports as well as provide detailed monthly tracking analysis, executive summaries, and presentations to Senior Management on performance and execution of Marketing programs and trend analysis for Planning and Control issues.\n\u2022 Designing and developing dashboards according to application requirements and demands of the business process using Tableau software.\n\u2022 Responsible for interacting with business stake holders, gathering requirements and creating effective Business Intelligence (BI) solutions and analytics dashboards using Tableau software.\n\nEnvironment: BASE SAS, SAS/STAT, SAS/GRAPH, SAS ODS, SAS Macros, SAS SQL, MS SQL, ELT (SSIS), SSRS, Tableau', u""Data Analyst\nWest-coast Pharmaceuticals - Ahmedabad, Gujarat\nJanuary 2013 to November 2014\nWest-coast Pharmaceuticals is one of India's leading Pharmaceutical Company offering a wide range of Pharmaceutical Ingredients, New Chemical Entries. I was involved in validating summary tables, listings and Analysis datasets. Handled with the phase I and phase II clinical trial data for the Tables and Listings generation.\n\nResponsibilities:\n\u2022 Involved in the migration of the data across different databases and different servers using SSIS and Import/export tools.\n\u2022 Creating and Modifying Tables, T-SQL Stored Procedures, Views, Indexes, User-defined Functions, and Triggers as required.\n\u2022 Validated programs and processes to Extract data from clinical data management systems, to prepare data listings, summary tables and reports using SAS.\n\u2022 Became proficient in SAS/BASE, SAS/ODS, SAS/SQL and in producing external files and reports employing various SAS procedures like PROC PRINT, PROC REPORT, PROC SUMMARY.\n\u2022 Performed data validation and data cleaning on clinical data using procedures like PROC SORT, PROC TRANSPOSE, PROC FREQ, PROC REPORT and PROC MEANS.\n\u2022 Created HTML and RTF reports using SAS ODS.\n\u2022 Reviewed SAPs, protocols and generated tables, listing using PROC PRINT, and PROC REPORT.\n\u2022 Used SAS MACROS and modified the existing ones related to multiple studies.\n\u2022 Involved in documentation of developed SAS code and participated in Testing/Validating SAS code with specifications.\nEnvironment: BASE SAS, SAS/STAT, SAS/GRAPH, SAS ODS, SAS Macros, SAS SQL, SSIS, Tableau, MS SQL.""]","[u'MS in Management Information System', u'Bachelor of Pharmacy in Karnataka']","[u'National University San Diego, CA', u'Rajiv Gandhi University of Health Science']"
0,https://resumes.indeed.com/resume/6cdb0a8daff88c4f,"[u'Lead Business Analyst\nHarris Health System - Houston, TX\nOctober 2012 to October 2012\n\u2022 Prepare system business operations budget, actual, cost, revenue financial analysis trend and forecast\n\u2022 Build system Healthcare operations scorecard models, monitor and alert possible business operations risk\n\u2022 Use Pivot Tables/Charts, Access, MSBI, Tableau Power BI design healthcare indicator economic models\n\u2022 Design and maintain system business operations, quality outcome database in SQL Server, SAS, Access\n\u2022 Process data ETL from claim, utilization, EPIC, BI for sensitive issues, CMS appeal & grievance objects\n\u2022 Develop business performance improvement strategy, healthcare best practice process and quality audit\n\u2022 Plot staff quality evaluation system, Web survey automatic measures scores and mandatory training plan', u'Data Analyst Consultant\nAB Stuffing Solutions, LLC - Gilbert, AZ\nNovember 2010 to September 2012\nData Analyst Consultant contract to Houston, Michael E. DeBakey Veterans Affairs Medical Center\n\u2022 Collect, validate, analyze multiple sources claim data to create dataset or data table/view for ad-hoc report\n\u2022 Format VA patient demographic statistical report by healthcare type, insurance, age, gender and race etc.\n\u2022 Generate dashboard/chart reports using SAS, SQL, Excel, Access, exhibition financial/economics models\n\u2022 Sketch Unit base data forecast module, monitoring financial budget and expense, fiscal budget proposal', u'Data Analyst/Statistical Programmer\nSummit Polymers, Inc - Kalamazoo, MI\nFebruary 2008 to April 2010\n\u2022 Collect manufacturing products, quality, cost, inventory, sales, revenue information into data warehouse\n\u2022 Process CPG quality/quantity standard, accurate economic analysis and quality trend forecast report\n\u2022 Use SAS/SQL to monitor operations cost, profit, marketing and product line supply chain forecast', u'Senior manager\nGrand China International Consulting Co. Ltd - \u5317\u4eac\u5e02\nAugust 2004 to June 2007\n\u2022 China top 50 universities audit--2005 renovation and disaster 10 billion fund, estimate property damages\n\u2022 Projects of mining safety evaluation and disaster economic damages quantification consulting, arbitration\n\u2022 Medicine Immune h-R3 R&D, SAS clinical trial stage I, II III statistical report, intellectual property evaluation\n\u2022 Collaboration with KPMG finished Canada Celgar Mill International M&A plan, due diligence study report', u'Data Analyst and Manager\nMinistry of Finance P.R. of China - \u5317\u4eac\u5e02\nJuly 1992 to July 2004\nAs a data analyst of Information Center, manager of statistics and appraisal department, I was mainly responsible for China natural resource industry system governmental treasury information collection, management, evaluation and report, monitor budget risk and submit ongoing budget proposals.']","[u'Master of Applied Statistics', u'Master of Applied Economics', u'Bachelor of Engineering']","[u'Western Michigan University Kalamazoo, MI\nDecember 2001', u'China University of Geosciences \u5317\u4eac\u5e02\nJuly 1992', u'China University of Geosciences Wuhan, CN\nJuly 1989']"
0,https://resumes.indeed.com/resume/08d8ac49aa30a408,"[u'SEO Analyst\nVee Technologies - Bengaluru, Karnataka\nOctober 2017 to February 2018\nRoles and Responsibilities:\n\u2022 Enhancing the volume and quality of the traffic to the website by increasing the organic search\nresults by improving the search engine page results\n\u2022 Improving title tags, description of meta tags and highlighted and added relevant links to site\nfor improving the search engine page results\n\u2022 Worked in groups to plan ease of use and roll out improvements in sites and web properties as required\n\u2022 Communicating with various people in the team and finalize keywords which helped in increasing keyword ranking and thus traffic to the site\n\u2022 Establish appropriate cross links and do directory submissions and Geo Graphical target and Link building - Off Page\n\u2022 Brand Name promotion through Social Media, engage with timeline - increase followers and likes (SMO)\n\u2022 Branding through images, videos, email campaign & SMS Marketing additionally doing regular\ncompetitor analysis\n\u2022 Keeping up on new Google trends, making note of trending keywords and doing the promotions\nas needed\n\u2022 Maintaining the username and password database for all the accounts used for promotions\n(Mails, Social Profiles, Off page)\n\u2022 Website Analysis on a month to month basis and keep up a report, making a note of errors with screenshots\n\u2022 Keeping up with data relating to leads crawling to our site using Google Analytics into BDMIS\ntool and do quality check, in mean time getting feedback from Business Development team\n\u2022 Keeping a note of Social Responses (Positive and Negative as well) with screenshots for future\nimprovements in online network promotions', u'Data Analyst\nVee Technologies - Bengaluru, Karnataka\nJune 2016 to February 2018\nRoles and Responsibilities:\n\u2022 Performing Data mining, Data extraction and Data management reports on daily, weekly,\nmonthly, and quarterly basis subsequently helping in lead generation\n\u2022 Work concerning Data/Email Appending, Web Research, Secondary Research, Market\nResearch\n\u2022 Focused social media analysis, Email Campaigning\n\u2022 Utilizing excel to accumulate data and perform essential visualization, involvement in online\ncompany database tools, such as, one source, Hoovers\n\u2022 Evaluating competitor websites profiles, market shares, and product FAB (features,\nadvantages and benefits) to incorporate best practice and create marketing strategy to ensure\noptimal results\n\nTools Used:\nZoominfo, LeadFerret, hunter, LinkedIn, Mail Tester, Verify Email, MailChimp, Millionwishes,\nway2SMS, Google Ad words']","[u'MSC', u'BSC']","[u'Mangalore University\nJanuary 2013 to January 2015', u'Mangalore University\nJanuary 2010 to January 2013']"
0,https://resumes.indeed.com/resume/54bcd648227b33da,"[u'Analytics Specialist\nAstellas Pharmacy\nNovember 2016 to Present\nComparing oncology data between vendors to acquire data from the one that best represents the oncology market. Comparison based on millions of claims, drugs, patients and physicians. (Impala, SQL, Excel Charts)', u'Chicago, Data Analyst\nUniversity of Illinois - Chicago, IL\nMay 2016 to May 2017\n* Developed drill through and drill down reports for supervisors in SQL Server. Created VLOOKUPs, Pivot tables and Excel dashboards for excel reports. (SQL Server, SSRS, Advanced SQL, Advanced Excel Reporting)\n* Maintained Database, administered user accounts, troubleshooting, diagnosed problems and IT assistance.', u""Data Science Intern\nBlue Cross Blue Shield of Illinois\nJanuary 2016 to April 2016\nDeveloped an analytical rating system that evaluates a surgeon's performance from his surgery level data. Identified critical post-surgery complications that are raising insurance claim amounts. (Python, Tableau, Lasso Regression)"", u'Data Analyst\nWipro Technologies\nJuly 2012 to March 2015\n* Analysed data to know the time consumed by each activity in a web application for employees on day-to-day basis. Helped to restructure the inhouse website in positioning most used tabs on homepage. (SQL Server, Excel, R)\n* Automated a manual quality assurance process thereby decreasing TAT by 43%. Quality assurance on a website transitioning from ICD9 to ICD10 codes. Quality supervision on implementing CPT codes.']","[u'Master of Science in Business Analytics', u'Bachelor of Technology in Mechanical Engineering']","[u'University of Illinois at Chicago, Liautaud Graduate School of Business Chicago, IL\nMay 2017', u'Gitam Institute of Technology Visakhapatnam, Andhra Pradesh\nMay 2012']"
0,https://resumes.indeed.com/resume/56eb145488cd73cc,"[u""Data Scientist\nMorgan Stanley, TimesSquare, NY\nFebruary 2017 to Present\nDescription: Morgan Stanleyis a financial holding company. The Company is engaged in global financial services. The Company, through its subsidiaries and affiliates, advises, and originates, trades, manages and distributes capital for governments, institutions and individuals. The Company's segments include Institutional Securities, Wealth Management and Investment Management. Through its subsidiaries and affiliates, the Company provides a range of products and services to a group of clients and customers, including corporations, governments, financial institutions and individuals.\n\nResponsibilities:\n\u2022 Long Short-Term Memory Recurrent Neural Networks (LSTM RNNs) learnt using Deep Learning techniques applied to Problem X.\n\u2022 LSTM RNNs applied to Problem Y.\n\u2022 Improving Fraud Detection using Digital Links at Amazon, Seattle.\n\u2022 Scaled upto Machine Learning pipelines: 4600 processors, 35000 GB memory achieving 5-minute execution.\n\u2022 Deployed GUI pages by using JSP, JSTL, HTML, DHTML, XHTML, CSS, JavaScript, AJAX.\n\u2022 Configured the project on WebSphere 6.1 application servers\n\n\u2022 Designed a new Machine Learning pipeline to replace existing prod: AUC perf. increase from 83% to 90%.\n\u2022 Handled 2+ TB data with graphs upto130 GB (50M nodes, 100M edges) using single-node in-disk scaling.\n\u2022 Developed a Machine Learning test-bed with 24 different model learning and feature learning algorithms.\n\u2022 By thorough systematic search, demonstrated performance surpassing the state-of-the-art (deep learning).\n\n\u2022 Upto 10 times more accurate predictions over existing state-of-the-art algorithms.\n\u2022 Developed in-disk, huge (100GB+), highly complex Machine Learning models.\n\u2022 Used SAX and DOM parsers to parse the raw XML documents\n\u2022 Used RAD as Development IDE for web applications.\n\n\u2022 Demonstrated performances comparable to other state-of-the-art deep learning models.\n\u2022 Applied Machine Learning algorithms to diagnose blood loss from vital signs (ECG, HF, GSR, etc.)\n\u2022 Devised and implemented a Vehicle Speed Detector using low-power LEDs and field-tested for robustness.\n\u2022 National Highways Authority (Govt. of India) is evaluating the design for installations across the country.\n\u2022 IIT Madras has installed the speed detectors across the institute for permanent speed limit enforcement.\n\u2022 Developed & tested feature tracking algorithms for Intelligent Transportation Systems Computer Vision.\n\u2022 Analyzed SIFT feature descriptors and their resilience to changes in illumination.\n\u2022 Devised a novel machine learning algorithm for classification of ECG abnormalities.\n\nEnvironment: R 9.0, Informatic a 9.0, ODS, OLTP, Oracle 10g, Hive, OLAP, DB2, Metadata,MS Excel, Mainframes MS Vision, Rational Rose."", u""Data Scientist\nState Street Bank - Boston, MA\nDecember 2015 to February 2017\nDescription:State Street Corporation, is an American worldwide financial services company. State Street was founded in 1792 and is the second oldest financial institution in the United States of America. It is one of the largest asset management companies in the world with $2.45 trillion (USD) under management and $28 trillion (USD) under custody and administration, which represents 11% of the world's total financial assets. State Street is a Fortune 500 company with headquarters at One Lincoln Street in Boston and has offices in 30 countries around the world.\n\nResponsibilities:\n\u2022 Manipulating, cleansing & processing data using Excel, Access and SQL.\n\u2022 Responsible for loading, extracting and validation of client data.\n\u2022 Liaising with end-users and 3rd party suppliers.\n\u2022 Analyzing raw data, drawing conclusions & developing recommendations\n\u2022 Writing T-SQL scripts to manipulate data for data loads and extracts.\n\u2022 Developing data analytical databases from complex financial source data.\n\u2022 Performing daily system checks.\n\u2022 Data entry, data auditing, creating data reports & monitoring all data for accuracy.\n\u2022 Designing, developing and implementing new functionality.\n\u2022 Monitoring the automated loading processes.\n\u2022 Advising on the suitability of methodologies and suggesting improvements.\n\u2022 Carrying out specified data processing and statistical techniques.\n\u2022 Supplying qualitative and quantitative data to colleagues & clients.\n\u2022 Using Informatica& SAS to extract, transform & load source data from transaction\nsystems.\n\u2022 Creating data pipelines using big data technologies like Hadoop, spark etc.\n\u2022 Creating statistical models using distributed and standalone models to build various diagnostics, predictive and prescriptive solution.\n\u2022 Utilize a broad variety of statistical packages like SAS, R , MLIB, Graphs,Hadoop, Spark , MapReduce, Pig and others\n\u2022 Refine and train models based on domain knowledge and customer business objectives\n\u2022 Deliver or collaborate on delivering effective visualizations to support the client business objectives\n\u2022 Communicate to your peers and managers promptly as and when required.\n\u2022 Produce solid and effective strategies based on accurate and meaningful data reports and analysis and/or keen observations.\n\u2022 Establish and maintain communication with clients and/or team members; understand needs, resolve issues, and meet expectations\n\u2022 Developed web applications using .net technologies; work on bug fixes/issues that arise in the production environment and resolve them at the earliest\nEnvironment: SQL Server 2008R2/2005 Enterprise, SSRS, SSIS, Crystal Reports, Hadoop, Windows Enterprise Server 2000, DTS, SQL Profiler, and Query Analyzer."", u""Data Scientist\neBay Inc - San Jose, CA\nApril 2014 to November 2015\nDescription:eBayis a multinational e-commerce corporation, facilitating online consumer-to consumer and business-to-consumer sales. It is headquartered in San Jose, California. eBay was founded by Pierre Omidyar in 1995, and became a notable success story of the dot-com bubble. Today it is a multibillion-dollar business with operations in about 30 countries.\nResponsibilities:\n\u2022 Data mining using state-of-the-art methods\n\u2022 Extending company's data with third party sources of information when needed\n\u2022 Enhancing data collection procedures to include information that is relevant for building analytic systems\n\u2022 Processing, cleansing, and verifying the integrity of data used for analysis\n\u2022 Doing ad-hoc analysis and presenting results in a clear manner\n\u2022 Creating automated anomaly detection systems and constant tracking of its performance\n\u2022 Strong command of data architecture and data modelling techniques.\n\u2022 Hands on experience with commercial data mining tools such as Splunk, R, Map reduced, Yarn, Pig,Hive, Floop, Oozie, Scala, HBase, Master HDFS, Sqoop, Spark, Scala (Machine learning tool) or similar software required depending on seniority level in job field.\n\u2022 Developed scalable machine learning solutions within a distributed computation framework (e.g. Hadoop, Spark, Storm etc.).\n\u2022 Utilizing NLP applications such as topic models and sentiment analysis to identify trends and patterns within massive data sets.\n\u2022 Knowledge in ML& Statistical libraries (e.g. Scikit-learn, Pandas).\n\u2022 Having knowledge to build predict models to forecast risks for product launches and operations and help predict workflow and capacity requirements for TRMS operations\n\u2022 Having experience with visualization technologies such as Tableau\n\u2022 Draw inferences and conclusions, and create dashboards and visualizations of processed data, identify trends, anomalies\n\u2022 Generation of TLFs and summary reports, etc. ensuring on-time quality delivery.\n\u2022 Participated in client meetings, teleconferences and video conferences to keep track of project requirements, commitments made and the delivery thereof.\n\u2022 Solved analytical problems, and effectively communicate methodologies and results\n\u2022 Worked closely with internal stakeholders such as business teams, product managers, engineering teams, and partner teams.\n\u2022 Created automated metrics using complex databases.\n\u2022 Foster culture of continuous engineering improvement through mentoring, feedback, and metrics.\nEnvironment: Erwin r9.0, Informatica 9.0, ODS, OLTP, Oracle 10g, Hive, OLAP, DB2, Metadata, MS Excel, Mainframes MS Visio, Rational Rose, Requisite Pro. Hadoop, PL/SQL, etc."", u""Data Scientist\nEagle Trading Systems - Princeton, NJ\nMay 2013 to March 2014\nDescription:Eagle Trading Systems Inc. is a financial investment advisory firm headquartered in Princeton, New Jersey. The firm manages 5 accounts totaling an estimated $481 Million of assets under management. Eagle Trading Systems Inc.'s 17 employees help advice 1-10 clients.\n\nResponsibilities:\n\u2022 Statistical Modelling with ML to bring Insights in Data under guidance of Principal Data Scientist\n\u2022 Data modeling with Pig, Hive, Impala.\n\u2022 Ingestion with Sqoop, Flume.\n\u2022 Used SVN to commit the Changes into the main EMM application trunk.\n\u2022 Understanding and implementation of text mining concepts, graph processing and semi structured and unstructured data processing.\n\u2022 Worked with Ajax API calls to communicate with Hadoop through Impala Connection and SQL to render the required data through it. TheseAPI calls are similar to Microsoft Cognitive API calls.\n\u2022 Good grip on Cloudera and HDP ecosystem components.\n\u2022 Used Elasticsearch (Big Data) to retrieve data into application as required.\n\u2022 Performed Map Reduce Programs those are running on the cluster.\n\u2022 Developed multiple MapReduce jobs in java for data cleaning and preprocessing.\n\u2022 Analyzed the partitioned and bucketed data and compute various metrics for reporting.\n\u2022 Involved in loading data from RDBMS and web logs into HDFS using Sqoop and Flume.\n\u2022 Worked on loading the data from MySQL to HBase where necessary using Sqoop.\n\u2022 Developed Hive queries for Analysis across different banners.\n\u2022 Extracted data from Twitter using Java and Twitter API. Parsed JSON formatted twitter data and uploaded to database.\n\u2022 Launching Amazon EC2 Cloud Instances using Amazon Images (Linux/ Ubuntu) and Configuring launched instances with respect to specific applications.\n\u2022 Exported the result set from Hive to MySQL using Sqoop after processing the data.\n\u2022 Analyzed the data by performing Hive queries and running Pig scripts to study customer behavior.\n\u2022 Have hands on experience working on Sequence files, AVRO, HAR file formats and compression.\n\u2022 Used Hive to partition and bucket data.\n\u2022 Experience in writing MapReduce programs with Java API to cleanse Structured and unstructured data.\n\u2022 Wrote Pig Scripts to perform ETL procedures on the data in HDFS.\n\u2022 Created HBase tables to store various data formats of data coming from different portfolios.\n\u2022 Worked on improving performance of existing Pig and Hive Queries.\n\nEnvironment: SQL/Server, Oracle 9i, MS-Office, Teradata, Informatica, ER Studio, XML, Business Objects, HDFS, Teradata 14.1, JSON, HADOOP (HDFS), MapReduce, PIG, Spark, R Studio, MAHOUT, JAVA, HIVE, AWS."", u""Data Architect/Data Modeler\nFirst Indian Corporation Private Limited - Hyderabad, Telangana\nFebruary 2011 to April 2013\nDescription:First Indian Corporation Private Limited, a member of The First American family of companies, provides specialized offshore transaction services, technology services and analytics to the mortgage industry. First Indian Corporation's primary focus is on the Title Insurance, Property Tax, Flood Certification, Default Management Services, Credit and Real Estate Information segments.\n\nResponsibilities:\n\u2022 Worked with large amounts of structured and unstructured data.\n\u2022 Knowledge in Machine Learning concepts (Generalized Linear models, Regularization, Random Forest, Time Series models, etc.)\n\u2022 Worked in Business Intelligence tools and visualization tools such as Business Objects, Tableau, ChartIO, etc.\n\u2022 Deployed GUI pages by using JSP, JSTL, HTML, DHTML, XHTML, CSS, JavaScript, AJAX.\n\u2022 Configured the project on WebSphere 6.1 application servers\n\u2022 Implemented the online application by using Core Java, Jdbc, JSP, Servlets and EJB 1.1, Web Services, SOAP, WSDL.\n\u2022 Handled end-to-end project from data discovery to model deployment.\n\u2022 Monitoring the automated loading processes.\n\u2022 Communicated with other Health Care info by using Web Services with the help of SOAP, WSDLJAX-RPC\n\u2022 Used Singleton, factory design pattern, DAO Design Patterns based on the application requirements\n\u2022 Used SAX and DOM parsers to parse the raw XML documents\n\u2022 Used RAD as Development IDE for web applications.\n\u2022 Preparing and executing Unit test cases\n\u2022 Used Log4J logging framework to write Log messages with various levels.\n\u2022 Involved in fixing bugs and minor enhancements for the front-end modules.\n\u2022 Implemented Microsoft Visio and Rational Rose for designing the Use Case Diagrams, Class model, Sequence diagrams, and Activity diagrams for SDLC process of the application\n\u2022 Doing functional and technical reviews\n\u2022 Maintenance in the testing team for System testing/Integration/UAT.\n\u2022 Guaranteeing quality in the deliverables.\n\u2022 Conducted Design reviews and Technical reviews with other project stakeholders.\n\u2022 Was a part of the complete life cycle of the project from the requirements to the production support\n\u2022 Created test plan documents for all back-end database modules\n\u2022 Implemented the project in Linux environment.\n\nEnvironment: R 3.0, Erwin 9.5, Tableau 8.0, MDM, QlikView, MLLib, PL/SQL, HDFS, Teradata 14.1, JSON, HADOOP (HDFS), MapReduce, PIG, Spark, R Studio, MAHOUT, JAVA, HIVE, AWS."", u""Data Analyst/Data Modeler\nDELTA Technologies & Managements Services - Hyderabad, Telangana\nAugust 2009 to January 2011\nDescription: Delta Technology's vision is to be an organization of value, respect and transparency for its people to continuously innovate, improve and deliver efficient and effective business solutions.\n\nResponsibilities:\n\u2022 Developed Internet traffic scoring platform for ad networks, advertisers and publishers (rule engine, site scoring, keyword scoring, lift measurement, linkage analysis).\n\u2022 Responsible for defining the key identifiers for each mapping/interface.\n\u2022 Clients include eBay, Click Forensics, Cars.com, Turn.com, Microsoft, and Looksmart.\n\u2022 Designed the architecture for one of the first analytics 3.0. online platforms: all-purpose scoring, with on-demand, SaaS, API services. Currently under implementation.\n\u2022 Web crawling and text mining techniques to score referral domains, generate keyword taxonomies, and assess commercial value of bid keywords.\n\u2022 Developed new hybrid statistical and data mining technique known as hidden decision trees and hidden forests.\n\u2022 Reverse engineering of keyword pricing algorithms in the context of pay-per-click arbitrage.\n\u2022 Implementation of Metadata Repository, Maintaining Data Quality, Data Cleanup procedures, Transformations, Data Standards, Data Governance program, Scripts, Stored Procedures, triggers and execution of test plans\n\u2022 Performed data quality in Talend Open Studio.\n\u2022 Coordinated meetings with vendors to define requirements and system interaction agreement documentation between client and vendor system.\n\u2022 Automated bidding for advertiser campaigns based either on keyword or category (run-of-site) bidding.\n\u2022 Creation of multimillion bid keyword lists using extensive web crawling. Identification of metrics to measure the quality of each list (yield or coverage, volume, and keyword average financial value).\n\u2022 Enterprise Metadata Library with any changes or updates.\n\u2022 Document data quality and traceability documents for each source interface.\n\u2022 Establish standards of procedures.\n\u2022 Generate weekly and monthly asset inventory reports.\n\nEnvironment: Erwin r7.0, SQL Server 2000/2005, Windows XP/NT/2000, Oracle 8i/9i, MS-DTS, UML, UAT, SQL Loader, OOD, OLTP, PL/SQL, MS Visio, Informatica.""]",[],[]
0,https://resumes.indeed.com/resume/7ec70db3f387d329,[u'Data Analyst'],"[u""Bachelor's of Science""]",[u'Benedictine University']
0,https://resumes.indeed.com/resume/1528604c513e57d1,"[u'Senior Data Analyst\nFoundation Radiology Group - Pittsburgh, PA\nJuly 2015 to December 2017\nPrimary data analytics resource for CEO/C-Suite. Utilized SAS, SQL, Oracle, Access, Salesforce, and project management skills to ensure CEO has optimal data for critical decisions.\n\nUtilized wide skill set to assist CEO in achieving aggressive growth target (Doubled). Growth target was achieved using analytics to create focused market segmentation based on CEO priorities. A Microsoft Access switchboard was utilized to communicate key factors for potential clients to the Business Development team.\n\nUsed project management and multi dimensional data mart skills to normalize terminology and source data across organization, increasing efficiency of reporting and eliminating misleading financial reports.\n\nUsed well rounded skill set for increasing efficiency of Radiologist scheduling, worked with Revenue Center Management to improve billing write offs, and developed target models and ROI calculations for physician referral programs.', u'Data Analyst\nHighmark Health - Pittsburgh, PA\nMay 2007 to January 2014\nPrimary analytics source for Senior Markets product development and Revenue Management.\n\nUsed project management and multi dimensional data mart skills to analyze impact of potential health network disruption, including but not limited to; membership disruption, impacted specialties, transition of care, clients, and member demographics. The design of the cube allowed for the flexibility of adding/changing multiple parameters and producing timely results.\n\nTeamed with Medical Management in the promotion and evaluation of the Advanced Illness Program, recording needs of patients and communicating available supporting data to write reporting specifications, producing a ROI of $2.5M.\n\nLeveraged project management skill to coordinated submission of RFP for potential client with 38,000 members.\n\nProduced methodology to accurately forecast appropriate RADV and Elimination of Lag accruals based on current HCC and membership data.\n\nWork with Finance and Operations to resolve membership discrepancies between Highmark and CMS and\nset appropriate accrual.', u'Actuarial Analyst\nHighmark Life and Casualty (Highmark Subsidiary) - Pittsburgh, PA\nFebruary 2001 to May 2007\nHead Actuary for Workers Comp product with risk pool of fifty thousand members generating forty million in\nannualized premium.\n\nDesigned and coordinated implementation of Workers Comp sales incentive.\n\nIdentified the large claims variability in small group policies as an underwriting constraint and developed new\npricing model to incorporate additional group experience with similar demographics, producing the desired results of a predictive claims trend.\n\nAnticipated an average 8.5% decrease in Pennsylvania lost cost factors and proposed pricing adjustments to improve positioning of the Workers Comp product.\n\nProvide Actuarial insight and direction for rates, profitability, and claim trend forecasting.\n\nPerform quarterly evaluations on reserve amounts totaling 100 million dollars to ensure adequacy for\nprojected future claims paid over a twenty year period.\n\nDesigned and implemented profitability reports on quarterly bases for all of Senior Management across all\nproducts, providing a written analysis of key points and potential risk identified by year over year trends.\n\nEnsured Workers Comp was compliant with the Sarbanes-Oxley Act of 2002.\n\nPrepared and presented annual ""State of Union"" for Workers Comp as it pertained to current performance\nand future expectation.']","[u'BS in Mathematics', u'Actuarial Exam 1 & Actuarial Exam 2']","[u'Indiana State University Terre Haute, IN\nJanuary 1995 to January 2000', u'Pittsburgh, PA']"
0,https://resumes.indeed.com/resume/7a09f5ebdd78b79b,"[u'Business Analyst\nAtlas Systems\nJanuary 2018 to Present\n\u2022 Used Clustering model with 75% accuracy rate on confusion matrix to target the right customers & health plans for a service using SQL and R (K- means algorithm).\n\u2022 Identified appropriate Hospitals to partner with for Provider Data Validation using Vlookup & Pivot Table in Excel, thereby directly contributing to save 0.5 Million in Operational Costs.\n\u2022 Increased the efficiency of Provider Validation to 95% using a better validation strategy by performing Logistic regression and Bayesian inference.\n\u2022 Reported directly to the top management to provide valuable insights in the Marketing forefront by using Salesforce & Pardot.', u'Data Analyst\nLime Brokerage\nAugust 2017 to December 2017\n\u2022 Improving the database storage efficiency by 60% by scrutinized the daily reports in accordance with the viewing frequency using Java.\n\u2022 Examined large set of unstructured data dump to predict and parse of data into comprehensible format based on data pattern recognition using\nSQL, thereby reducing search time of 10 hours of a week.\n\u2022 Introduced Tableau in the Organization and created custom dashboards that enables live streaming of data, to monitor customer behavior,\nidentify trends & unmet needs.\n\u2022 Maintained and automated daily stock monitoring reports and provided insights, helping the management to make decisions increasing to 30% in customer acquisition.', u'Data Analyst Consultant\nJanuary 2016 to October 2016\n\u2022 Built deep behavioral models of active clients, which led to product improvements that increased customer activation rates by 30%.\n\u2022 Gathered requirements for re-design of production Access database, resolved reporting errors and technical issues.\n\u2022 Provided time sensitive analyses on various customer and business metrics questions for venture capital meetings in support of next round of start-up funding.\n\u2022 Used HIVE for gaining better insights for product development based on customer preference.', u'Data Analyst\nAppizant Technologies\nMay 2015 to December 2015\n\u2022 Minimized the cost function of a linear regression model by 20% that reduces supply risk to develop competitive sourcing strategies by implementing gradient descent algorithm.\n\u2022 Improved total revenue of the organization by 40% using Market Basket analysis to meet the demand effectively.\n\u2022 Created and monitored operational/sales dashboards to track key performance indicator metrics for sales, account management, and marketing\ninitiatives using Excel & Tableau.']","[u""Master's""]",[u'']
0,https://resumes.indeed.com/resume/e0d0777e61bff872,"[u""Sr. Data Architect/Modeler\nAccruent - Minneapolis, MN\nNovember 2016 to Present\nMajor Responsibilities:\n\u2022 Maintain repository and exchange of metadata using EDW-based tools (Teradata, Informatica, Cognos)\n\u2022 Managed ELDM Logical and Physical Data Models in ER Studio Repository based on the different subject area requests for integrated model.\n\u2022 SQL Server, Oracle, DB2/UDB, Sybase Database Administration.\n\u2022 Worked on developing training program, user guide for Spotfire Professional and Spotfire web player in terms of best developing and design practices.\n\u2022 Knowledge with multiple Hadoop clusters using Kerberos and Sentry.\n\u2022 Azure Tech support for existing and new environments\n\u2022 Responsible for different Data mapping activities from Source systems to Teradata.\n\u2022 Working with Data Architects to design New Data Mart to design the Google Analytics data reports.\n\u2022 Developed and maintained an Enterprise Data Model (EDM) to serve as both the strategic and tactical planning vehicles to manage the enterprise data warehouse. This effort involves working closely with the business.\n\u2022 Built initial Data Governance Roadmap/Framework, and defined process, System/Domain scope tool selection process and data quality issue intake for Data Quality/Data Governance Program.\n\u2022 Worked on Sending data from hdfs (Hive db) to greenplum using sqoop\n\u2022 Led technical implementation of advanced analytics projects, Defined the mathematical approaches, developer new and effective analytics algorithms and wrote the key pieces of mission-critical source code implementing advanced machine learning algorithms utilizing caffe, TensorFlow, Scala, Spark, MLLib, R and other tools and languages needed.\n\u2022 Statistical and quantitative analysis, rules-based methods, and explanatory and predictive modeling.\n\u2022 Involved in performance tuning for Siteminder and Ldap for better response time and high throughput.\n\u2022 Experience in integration of SalesForce and SQL server using Sql Server Integration Services\n\u2022 Implemented UPS, USPS and DHL integration (Python, SUDS)\n\u2022 Involved in Data Architecture, Data profiling, Data analysis, data mapping and Data architecture artifacts design.\n\u2022 Created Logical views in MicroStrategy for complex report creation scenarios. Used free form SQL editor for complex report creation.\n\u2022 Routinely deal in with large internal and vendor data and perform performance tuning, query optimizations and production support for SAS, Oracle 11g\n\u2022 Involved in end to end implementation of Big data design.\n\u2022 Involved in review of one analytics Azure SQL mart designs of each MCS & Premier features\n\u2022 Design of Big Data platform technology architecture. The scope includes data intake, data staging, data warehousing, and high performance analytics environment.\n\u2022 Skilled in Object Oriented Analysis and Design with experience in creating Use Cases, Class Diagrams, Activity Diagrams, State Diagrams, Sequence Diagrams, Unified Modeling Language (UML) using MS Visio, Bizagi, JIRA &Rational Rose to extract business process workflows.\n\u2022 Worked on NoSQL databases including HBase, Mongo DB, and Cassandra. Implemented multi-data center and multi-rack Cassandra cluster.\n\u2022 Developing Database Architecture using design standards and tools including Erwin, IBM InfoSphere DB2\n\u2022 Azure, development and management\n\u2022 Led exploratory and predictive and prescriptive data modeling to attain the revenue and profitability goals.\n\u2022 Led workshops to educate business in MDM, Data Governance concepts and techniques and in implementing change management process establishing data quality controls in source systems.\n\u2022 Create and communicate the strategy and use of Big Data.\n\u2022 Responsible for the data architecture design delivery, data model development, review, approval and Data warehouse implementation.\n\u2022 Installation and Configuration of other Open Source Software like Pig, Hive, HBase, Flume and Sqoop.\n\u2022 Designed logic, data and process models for ISO 9001 Quality Management System and predictive models.\n\u2022 Created/modified set of Informatica mappings to do performance improvement on Greenplum DB.\n\u2022 Deployed scripts for monitoring Golden Gate lag and process (Extract, Datapump and replication)\n\u2022 Worked in importing and cleansing of data from various sources like Teradata, Oracle, flat files, SQL Server 2005 with high volume data.\n\u2022 Extensive use of PROC SQL, SAS Macros, Unix and Windows Batch Shell for process automation\n\u2022 Used bunch of transformations in Pentaho transformations including Row Normalizer, Row Demoralizer, Database Lookup, Database Join, Calculator, Add Sequence, Add Constants and various types of inputs and outputs for various data sources including Tables, Access, Text File, Excel and CSV file.\n\u2022 Built a B2B integration infrastructure utilizing Python, suds SOAP, Node.js.\n\u2022 Design MOLAP/ROLAP cubes on Teradata Database using SSAS.\n\u2022 Part of team conducting logical data analysis and data modeling JAD sessions, communicated data-related standards.\n\u2022 Experience in importing and exporting the logs using Flume. Optimizing performance of Hbase/Hive/Pig jobs.\n\u2022 Submitted talend jobs for scheduling using Talend scheduler which is available in the Admin Console.\n\u2022 Specifies overall Data Architecture for all areas and domains of the enterprise, including Data Acquisition, ODS, MDM, Data Warehouse, Data Provisioning, ETL, and BI.\n\u2022 Installed and configured Sybase ASE 12.5 and 15 servers. Also migrated databases from ASE 12.5 to ASE 15 via bcp or dump/load methods.\n\u2022 Worked on designing a Star schema for the detailed data marts and plan data marts involving confirmed dimensions.\n\u2022 Trained junior data modelers on data architecture standards and data modeling standards.\n\u2022 Loaded and transformed large sets of structured, semi structured and unstructured data using Hadoop/Big Data concepts.\n\u2022 Authentication (LDAP, Kerberos, AD, Pass thru)\n\u2022 Develop Enterprise Data Warehouse Framework, database Architecture, Business Solution using design standards and tools including Erwin, ETL as Informatica, Oracle/Unix.\n\u2022 Designed ER diagrams (Physical and Logical using Erwin) and mapping the data into database objects.\n\u2022 Creating architecture of data processing pipelines. Used machine learning (regression) for getting business insights from data.\n\u2022 Developed analyses and predictive models on customer segmentation, acquisition, attrition, enhanced cross sell and profitability using SPSS, excel, SQL server 2015 and MS access.\n\u2022 Used SAS/DDE to call Excel VBA macros in reporting applications development\n\u2022 Produced Logical /Physical Data Models.\n\u2022 Participate in requirements definition, analysis, and design of EDW solutions.\n\u2022 Worked with stakeholders to capture existing process workflow and present options for improvements utilizing Visio and Bizagi Modeler.\n\u2022 Reverse Engineer using ER Studio to understand source systems' structure and report the anomalies in data structure to source system data owner.\n\u2022 Created, maintained, modified, and optimized Teradata databases, database security and auditing\n\u2022 Developed SSIS Templates which can be used to develop SSIS Packages such a way that they can be dynamically deployed into Development, Test and Production Environments.\n\u2022 Created SSIS packages for different data loading operations for many applications.\n\u2022 Worked with team to document the transformation rules for data migration from OLTP to Warehouse environment for reporting purposes.\n\u2022 Developed an EDI parser to split shipments from Oracle OTM to internal synapse system.\n\u2022 Configured the Spotfire Application Server and the Web Player Server for optimal use.\n\u2022 Used SAS Macros to produce drill down capability for the reports.\n\u2022 Generated ad-hoc SQLqueries using joins, database connections and transformation rules to fetch data from legacy Oracle and SQL Server database systems.\n\u2022 Spark, Hadoop, Scala, Impala, Tableau, Zeppelin, Machine learning.\n\u2022 Map & Gap Analysis of SAP SD and MM modules for Quote to Cash (QTC) processes."", u'Sr. Data Analyst/Modeler\nPrudential - Newark, NJ\nJanuary 2014 to October 2016\nMajor Responsibilities:\n\u2022 Worked in Data Analysis, data profiling and data governance identifying Data Sets, Source Data, Source Meta Data, Data Definitions and Data Formats.\n\u2022 Developed normalized Logical and Physical database models to design OLTP system for insurance applications.\n\u2022 Created dimensional model for the reporting system by identifying required dimensions and facts using Erwin.\n\u2022 Created and maintained Database Objects (Tables, Views, Indexes, Partitions, Synonyms, Database triggers, Stored Procedures) in the data model.\n\u2022 Presented the data scenarios via, Erwin logical models and excel mockups to visualize the data better.\n\u2022 Working with various data processing platforms and languages including Apache Spark (Scala), Apache Drill, Python, Oracle PL/SQL, and PostgreSQL PL/pgSQL\n\u2022 Experience in using BA tools like Lucid Charts, Bizagi and Visio.\n\u2022 Designed security framework of LDAP to provide the access management.\n\u2022 Worked with Data Steward Team for designing, documenting and configuring Informatica Data Director for supporting management of MDM data.\n\u2022 Developed complex SAS Macros to simplify SAS code and effectively reduce coding time.\n\u2022 Performed DataAnalysis tasks on warehouses from several sources like Oracle, DB2, XML etc and generated various reports and documents.\n\u2022 Generated comprehensive analytical reports by running SQL queries against current databases to conduct Data Analysis.\n\u2022 Worked with MicroStrategy Object Manager to move objects among the Development, Test, and Prod environments.\n\u2022 Assist with the day to day activities to support EDI interfaces for QNXT, EZCap, MHC and data warehouse for encounter data.\n\u2022 Providing expertise and guidance on the overall SAP Technology Infrastructure as it relates to IT and business strategies.\n\u2022 Worked on converting the existing UNIX scripts to handle the source file movement and data load into Greenplum DB.\n\u2022 Converted multiple databases from Sybase ASE 12.5 to SQL Server 2005 platform. Bulk copy (bcp), backup and restore, detach and attach data and log files, SQL Server Integration Services (SSIS) were applied in the conversion process.\n\u2022 Conducted and guided TIBCO Spotfire BI application global roll out, managed QA Testing, User Acceptance Testing, software update, migration deployment and parallel run, stabilizing the server platform and application to achieve the steady state for transitioning.\n\u2022 Helped customers with architecture (Solr Indexing, clustering, infrastructure setup and troubleshooting), authentication (Active Directory, Kerberos, and Pass thru) and database configuration (Mysql, Oracle and PostgreSQL) issues and questions.\n\u2022 Performed Data Analysis, Data Migration and data profiling using complex SQL on various sources systems including Oracle and Teradata.\n\u2022 Provide AWS-based cloud support, technical expertise and guidance with Azure integration and proposals/RFIs/RFPs to our sales team and executive contract management.\n\u2022 Experience in Data mining with querying and mining large datasets to discover transition patterns and examine financial reports.\n\u2022 performed log analysis using AWK, Python, Pandas, D3. Built models and performed dataset analysis in terms of those models.\n\u2022 Data Integrated, Extracted and transformed from mainframe and loaded to Hadoop using Talend\n\u2022 Using Big Data analytics discussions with software and hardware vendors.\n\u2022 Studied in-house requirements for the Data warehouse to be developed.\n\u2022 Worked on analyzing Hadoop cluster and different big data analytic tools including Pig, Hbase database and Sqoop.\n\u2022 Maintaining expertise in the area of SAP architecture including industry trends, strategies, and products to ensure corporate assets are optimized.\n\u2022 Develop data movement requirements/technical specifications across EDW layers.\n\u2022 Performed data analysis, statistical analysis, generated reports, listings and graphs using SAS.\n\u2022 Created Rich dashboards using Tableau Dashboard and prepared user stories to create compelling dashboards to deliver actionable insights.\n\u2022 Participate in daily conference calls, advising on EDI functions related to the business processes.\n\u2022 Build and maintain scalable data pipelines using the Hadoop ecosystem and other open source components like Hive and HBase.\n\u2022 Developed new tree structure on the Red Hat Directory Server (RHDS) 9.0 and designed the LDAP schema with custom attributes and object classes\n\u2022 Experienced in implementing co-production systems with advanced replication for business continuity and system availability (snap-shot replication & multi-master)\n\u2022 Performed data management projects and fulfilling ad-hoc requests according to user specifications by utilizing data management software programs and tools like Perl, TOAD, MS Access, Excel and SQL\n\u2022 Worked on Naming standards for Table/Column/Index/Constraints names thru Erwin Macros and Master Abbreviations file.\n\u2022 Written SQL scripts to test the mappings and Developed Traceability Matrix of Business Requirements mapped to Test Scripts to ensure any Change Control in requirements leads to test case update.\n\u2022 Performed Researching and deploying new tools, frameworks and patterns to build a sustainable Big data platform.\n\u2022 Supported QA team to run talend jobs manually in unix via crontab.\n\u2022 Extensively used agile methodology as the Organization Standard to implement the data Models.\n\u2022 Knowledge in preparing required project documentation and tracking and reporting regularly on the status of projects to all project stakeholders\n\u2022 Technologies Used: Greenplum 4.2, Informatica 9.1, HP Neoview, UNIX.\n\u2022 Developed multi-threaded standalone app in Python.\n\u2022 Knowledge and experience in producing tables, reports, graphs and listings using various procedures and handling large databases to perform complex data manipulations.\n\u2022 Performed Data Reconciliation between integrated systems, Involved in extensive Data Validation with SQL queries, Involved in Regression testing and Investigated Data Quality issues.\n\u2022 Worked on Physical design for both SMP and MPP RDBMS, with understanding of RDMBS scaling features.\n\nEnvironment: Oracle 10g, 11g RAC, SQL Server 2008/2012, , SQLPLUS, PL/SQL, Big Data, Teradata R13, Teradata SQL Assistant, Hadoop 1x, Hive, Pig, HBASE, Sqoop and Flume, CA ERWIN r8, Subversion, MS-Office.', u""Sr. Data Analyst/Modeler\nOptum Healthcare - Minneapolis, MN\nAugust 2011 to December 2013\nMajor Responsibilities:\n\u2022 Experienced working on the Affordable Care Act (ACA)\n\u2022 Responsible for all metadata relating to the EDW's overall data architecture, descriptions of data objects, access methods and security requirements\n\u2022 Perform logical and physical OLAP / OLTP schema design.\n\u2022 Worked with data investigation, discovery and mapping tools to scan every single data record from many sources.\n\u2022 Performed data analysis and data profiling using complex SQL on various sources systems including Oracle.\n\u2022 Complete maps for integration with Oracle EDI 834, 820 Gateway, JD Edwards, SAP and other ERP packages.\n\u2022 Worked on SQL queries in a dimensional data warehouse as well as a relational data warehouse.\n\u2022 Developed dimensional model for Data Warehouse/OLAP applications by identifying required facts and dimensions.\n\u2022 Supported the Facets Upgrade Project Team in the Facets 2.96 to the 4.21 upgrade from database development/administration perspective.\n\u2022 Designed STAR schema for the detailed data marts and plan data marts consisting of confirmed dimensions.\n\u2022 Generated comprehensive analytical reports by running SQL queries against current databases to conduct Data Analysis.\n\u2022 Performed Data Analysis, Data Migration and data profiling using complex SQL on various sources systems including Oracle and Teradata.\n\u2022 Used Erwin model mart for effective model management of sharing, dividing and reusing model information and design for productivity improvement.\n\u2022 Used Model Manager Option in Erwin to synchronize the data models in Model Mart approach.\n\u2022 Created views and dashboards on end client's data. Produced powerful dashboards telling story behind the data in an easy to understand format such as pie, bar, geo, and line charts that are viewed daily by senior Management.\n\u2022 Performed logical data model design including normalization/de-normalization referential integrity, data domains; primary and foreign key assignments and data element definitions as applied to both relational and dimensional modeling.\n\u2022 Performed Data Analysis and Data Modeling, Profiling and worked on data transformations and data quality rules.\n\u2022 Worked on Informartica Data Quality tool for monitoring Production systems for Data Anomalies and resolve issues.\n\u2022 Designed ER diagrams (Physical and Logical using Erwin) and mapping the data into database objects.\n\u2022 Involved in project planning, coordination and QA methodology in the implementation of the Facets in the EDI transaction of the claims module.\n\u2022 Produced Logical /Physical Data Models.\n\u2022 Created, maintained, modified, and optimized Teradata databases, database security and auditing.\n\u2022 Performed logical data modeling, physical data modeling (including reverse engineering) using the Erwin Data Modeling tool.\n\u2022 Basic knowledge of SAP HANA and AWS Cloud platform.\n\u2022 Did data modeling and involved with design and development of conceptual, logical and physical data models using All Fusion Data Modeler (Erwin)\n\u2022 Collaborate with the Enterprise Architecture teams (data integration, data architecture, business intelligence) to develop and deliver MDM solutions.\n\u2022 Create process models and enhance existing business process models using Bizagi Modeler.\n\u2022 Worked with the ETL team to document the transformation rules for data migration from OLTP to Warehouse environment for reporting purposes.\n\u2022 Worked in importing and cleansing of data from various sources like Teradata, Oracle, flat files, SQL Server 2005 with high volume data.\n\u2022 Used forward engineering to generate DDL from the Physical Data Model and handed it to the DBA.\n\u2022 Performance tuning for Web server and Siteminder along with LDAP for better response time, low latency and high throughput.\n\u2022 Created data masking mappings to mask the sensitive data between production and test environment.\n\nEnvironment: Windows XP, Informatica Power Center 6.1/7.1, QTP 9.2, Test Director 7.x, Load Runner 7.0, Oracle 10g, UNIX AIX 5.2, PERL, Shell Scripting."", u'Data Analyst/Modeler\nSAP Labs - Bengaluru, Karnataka\nAugust 2008 to October 2011\nBangalore, India\nMajor Responsibilities:\n\u2022 Develop Logical and Physical data models that capture current state/future state data elements and data flows using Erwin / StarSchema.\n\u2022 Analyzed database performance with SQL Profiler and Optimized indexes to significantly improve performance.\n\u2022 Automated load run on Informatica sessions through UNIX Corn, PL/SQL scripts and implemented pre and post-session scripts, also automated load failures with successful notification through email.\n\u2022 Coordinated with ETL team, DB administrators& BI teams to elevate the data model changes in the system\n\u2022 Completed High-Level design documents which included a BIZAGI diagram of several processes, macro processes, use cases, activities, interfaces, business rules, gaps and premises\n\u2022 Prepared scripts for model and data migration from DB2 to the new Appliance environments.\n\u2022 Transformed Logical Data Model to Physical Data Model ensuring the Primary Key and Foreign Key relationships in PDM, Consistency of definitions of Data Attributes and Primary Index Considerations.\n\u2022 Implemented Data Archiving strategies to handle the problems with large volumes of data by moving inactive data to another storage location that can be accessed easily.\n\u2022 Worked closely hand in hand with the Business Analytics manager, who was also a part of the design/data modeling team.\n\u2022 Experience in Project development and coordination with onshore-offshore ETL/BI developers & Business Analysts.\n\u2022 Led enterprise logical data modeling project (in third normal form) to gather data requirements for OLTP enhancements.\n\u2022 Modeled multiple new LOBs (Financial data) into existing Financial Services EDW, with modifications to existing and new entities.\n\u2022 Converted third normal form ERDs into dimensional ERDs for data warehouse effort.\n\u2022 Involved in mapping spreadsheets that will provide the Data Warehouse Development (ETL) team with source to target data mapping, inclusive of logical names, physical names, data types, domain definitions, and corporate meta-data definitions.\n\u2022 Converted physical database models from logical models, to build/generate DDLscripts.\n\u2022 Maintained warehouse metadata, naming standards and warehouse standards for future application development.\n\u2022 Extensively used ETL to load data from DB2, Oracle databases.\n\u2022 Involved with data profiling for multiple sources and answered complex business questions by providing data to business users.\n\u2022 Expertise and worked on Physical, logical and conceptual data model\n\u2022 Designed both 3NF data models for ODS, OLTP systems and dimensional data models using star and snow flake Schemas\n\u2022 Extensively used ETL methodology for supporting data extraction, transformations and loading processing, in a complex EDW using Informatica.\n\u2022 Worked and experienced on Star Schema, DB2 and IMS DB.\n\u2022 Worked on Optimization of the application and the designing of the database tables with the right partitioning keys using the DPF feature of hash partitioning and range partitioning.\n\u2022 Performed cross-platform database migration for development & Production databases running on Sun OS to LINUX using utilities such as db2move and db2look.\n\nEnvironment: ERWIN, UNIX, Oracle, PL/SQL, DB2, Teradata SQL assistant, DQ analyzer.']","[u""Bachelor's""]",[u'']
0,https://resumes.indeed.com/resume/ee723cdf12d20687,"[u'Planning Analyst\nCatholic Health System - Buffalo, NY\nMarch 2017 to Present\nMember of the business planning department, an internal consulting and business development team, which works\nclosely with senior leadership to assess the viability of high-profile strategic business initiatives\n\u2022 Conduct research, perform analyses, and generate reports to identify opportunities for growth of Catholic Health\'s\nservices in the eight counties of Western New York\n\u2022 Create ""Rapid Assessments"", one-page reports, to quickly summarize how national and regional healthcare news\nand developments impact Catholic Health and surrounding markets\n\u2022 Assist Vice President of Planning & Regional Development in preparing for and conducting focus groups, which are used to gauge public sentiment of existing service lines and proposed projects\n\u2022 Responsible for analyzing market data to determine physician recruitment needs and to validate recruitment\nincentives used by Catholic Health\'s Physician Enterprise team\n\u2022 Established a process for the Marketing Director to calculate the return-on-investment of various marketing\ninitiatives, including digital advertising campaigns and community outreach events', u'Data Analyst\nACV Auctions - Buffalo, NY\nMay 2015 to February 2017\n\u2022 Third employee at startup business-to-business software company; company has grown to 200+ employees and raised more than $50 million in venture capital funding\n\u2022 Generated daily and weekly reports utilizing MySQL and Excel to organize and maintain accurate data\n\u2022 Collaborated with internal teams (i.e. sales, operations, software development) to foster a better understanding of customer behavior, sales, and growth, which led to data-driven business decisions\n\u2022 Completed special projects under the guidance of CEO, CTO, CFO, and VP of Sales\n\u2022 Supported the operations team with customer relations and other administrative duties', u'Sisters of Charity Hospital - Buffalo, NY\nDecember 2009 to April 2014\n\u2022 Collected information from patients and clinicians to register all newborns in the New York State data system\n\u2022 Maintained sensitive personal information and verified legal documents within strict regulatory deadlines']","[u'Master of Science in Professional Applied and Computational Mathematics', u'Bachelors of Arts in Mathematics']","[u'State University College at Buffalo Buffalo, NY\nDecember 2015', u'State University College at Buffalo Buffalo, NY\nMay 2014']"
0,https://resumes.indeed.com/resume/5845301591038a1d,"[u""Senior software Engineer/Data Engineer\nBank of America - Pennington, NJ\nJanuary 2017 to Present\nRefactor the architecture of MongoDB to make the environment highly available and fault tolerant by a. adding multiple secondary nodes to the cluster in different data centers.\nb. adding delayed secondary to recover from accidental deletion of data\n\u2022 Lead the end-to-end implementation MongoDB upgrade project in order to ensure more stability in the environment.\n\u2022 Design and automate the deployment of the following in cloud platform.\n\u2022 NoSQL Database (MongoDB, Elasticsearch) cluster for the corporate cloud platform.\n\u2022 Version control repository (Bit bucket Datacenter) for the corporate cloud platform\n\u2022 Hadoop cluster to archive Elasticseach indexes.\n\u2022 Migrate and upgrade jFrog artifactory to ensure compliance with the standards at the Bank.\n\u2022 Design and Architect Machine Learning capabilities for the corporate cloud platform, which will be used to forecast the capacity.\n\u2022 Develop Dashboard on the system's metric and generate alert and take preventive action to avoid unplanned downtime. Reduced more than 95% percent unplanned downtime.\n\u2022 Automate the Deployment of new features to the cloud platform. Reduced the 60-70% manual deployment by automation"", u'Data Engineer/Analyst\nCisco Systems - Research Triangle Park, NC\nOctober 2015 to December 2016\n\u2022 Built a metrics gathering, storing and reporting framework for the Management team to consume and make data driven decision to plan the workload on the developers of different teams.\n\u2022 Automated the building and configuration (using ConcourseCI) of private repository for different kinds of artifactories such Yum, Docker etc. in Jfrog Artifactory.\n\u2022 Achieved 60% reduction of manual testing by developing automated functional tests to verify the functionality of Kafka, RabbitMQ, and Streamsets.\n\u2022 Achieved 95 % reduction on unplanned downtime by developing an automated predictive alert system to ensure on time preventive maintenance of servers.\n\u2022 Delivered a predictive model by using random forest classification model to predict possibility of denial of service attack. Improved the accuracy to 95% by using grid search technique', u'Data Scientist Intern\nLear Corporations - Southfield, MI\nMay 2015 to August 2015\n\u2022 Led an initiative to deliver an analytical data model to help improve the product development cycle, integration, data transparency and better collaboration tools to all the functional teams engaged and analyze the impact of system change to downstream systems.\n\u2022 Created a linear regression model with 95% accuracy to predict the demand for a given month to plan for the required inventory level.', u'Data Analyst\nMichigan State University - East Lansing, MI\nJanuary 2015 to May 2015\n\u2022 Achieved more than 95% accuracy on finding the appropriate criteria to hire a celebrity for product endorsements. This was achieved by extracting the celebrity posts and tweets from social media websites and training a classification model.\n\u2022 Attained 94% accuracy on a liner regression model by using residual plot analysis, box-cox transformation. The model predicted the expected popularity of a celebrity in the long run.\n\u2022 Recommended the business the combinations of products to cross-sale to increase (expected 15%) the average order value by using association analysis (apriori).', u'Data Engineer/Data Analyst\nJohnson and Johnson\nJuly 2013 to December 2014\n\u2022 Delivered Global Business Intelligence Framework that provides 360-degree view of the performance of the company across the Globe. Analyzed the average selling price(s) of top 20 products across US and EMEA to set the optimum prices for those products in those regions.\n\n\u2022 Transformed data into visually appealing stories for 500+ users using data visualization tools to facilitate business decision making. The reports and dashboards reduced manual workload.\n\n\u2022 Led a project to deliver more than 200 Data Integration (ETL) processes to build the Data Mart/Data Warehouse by extracting, transforming and loading the data into Data Mart/Data warehouse from different source systems.', u'Data Engineer/Data Analyst\nFebruary 2009 to July 2013\n\u2022 Built 150 + Data Integration processes to extract data from source systems and developed reports to display different key performance indicators that help business to improve operational efficiency.\n\u2022 Re-engineered and optimized existing Informatica Data Integration processes (100+) to reduce manual workload. Optimized the query performance by removing row-chaining and gathering statistics.\n\u2022 Reduced more than 30% of manual workload by delivering an automated process to refresh more than 100 cubes and publish those cubes to web portal for consumption.', u""Data Engineer/Data Analyst\nIBM Cognos\nAugust 2005 to February 2009\n\u2022 Developed and Designed an Enterprise Data Warehouse, which reduced the dependency of the business on the IT to run complex queries to generate reports.\n\u2022 Migrated existing legacy reporting solution to the IBM Cognos to ensure efficient and consistent reporting across the organization i.e. single version of truth.\n\u2022 Automated the execution of more than 200+ ETL processes by writing UNIX shell\n\nADDITIONAL PROJECTS\nKAGGLE COMPETITONS\nKernel Link: https://www.kaggle.com/iamchanchal/kernels\n\u2022 Data Science for Good: Kiva Crowdfunding: The objective is to pair Kiva's data with additional data sources to estimate the welfare level of borrowers in specific regions, based on shared economic and demographic characteristics.\n\u2022 House Prices: Advanced Regression Techniques: The aim of this project is to accurately predict the price of homes.\n\u2022 Predicting Red Hat Business Value: The aim of this project is to classify customer potential by analyzing the behavior of individual customer.""]","[u'Master of Science in Business Analytics', u'Bachelor of Technology in Computer Science and Engineering']","[u'Michigan State University East Lansing, MI\nJanuary 2015 to December 2015', u'Kalyani Government Engineering College Kalyani, West Bengal\nJuly 2001 to July 2005']"
0,https://resumes.indeed.com/resume/adf0bf5a02692dfd,"[u""Data Analyst\nCiti Group Inc - Buffalo, NY\nNovember 2017 to Present\n\u2022 Maintained integrity of Citi's Market Risk data used for risk limit reporting, VaR, Stress VaR calculations by implementing daily data quality scorecards and performing root cause analysis in case of data quality issues for Citi's various trading businesses.\n\u2022 Automated the daily reconciliation processes using VBA and enhanced SQL queries and macros for an improved data quality accuracy.\n\u2022 Maintained close working relationship with Market Risk Management, different Business Units and control groups by providing necessary information for audit, threshold reviews, stress testing and ad-hoc queries.\n\u2022 Worked with key partners to perform and deliver data tracing reviews between the various systems and data mappings to improve reporting on a weekly basis."", u'Business Intelligence Analyst\nBuffalo, NY\nDecember 2016 to January 2017\n\u2022 Performed a comparative study to evaluate the capabilities of a relatively new Business Intelligence tool, Pinpoint, and Tableau using health care insurance data related to charges and payments.\n\u2022 Translated the real world business requirements already developed in Pinpoint into reports and data visualizations in Tableau to help hospital staff improve revenue and receivable cycle management, delay reduction and trend/pattern mining.\n\u2022 Created reports and dashboards to optimize scheduling efficiency by projecting FTE needs by department, seasonality, function etc. and to evaluate efficiency/productivity by employee, equipment, department, etc.', u'Senior Systems Engineer, Data Analyst\nInfosys Limited\nJuly 2013 to July 2016\n\u2022 Developed solutions for Budgeting and Forecasting, integrated it with the other ancillary applications, and acted as the SPOC for a FTSE 100 client, while working in an agile scrum environment.\n\u2022 Designed and delivered reports based on important Trade Promotion Management related KPIs that helped business in planning and forecasting each promotion more accurately and efficiently.\n\u2022 Steered a team of 6 comprising of BI and CRM consultants, monitored the project progress, coordinated with clients, global team members and other stakeholders to prioritize the issues and ensure timely and accurate resolutions on daily basis.\n\u2022 Elicited, gathered and documented the design changes, developed crucial functional specification, business flow diagrams and high-level design documents. Provided knowledge transfer and mentored to the junior team members and end users.\n\u2022 Assisted business financial planners analyze the root cause for data inconsistencies during reconciliation process using MS Excel features like pivot tables and v-lookups.\n\u2022 Streamlined the month end reporting process by removing the dependency on the offshore team and enabling the business to complete their analysis and reconciliations 15 hours in advance.\n\u2022 Extensively worked on data modelling and complex transformations using SQL for data extraction from Finance & Sales data sources.']","[u'MS in Management Information Systems in Management Information Systems', u'B.Tech in Electronics in Electronics & Communication']","[u'SUNY, UNIVERSITY AT BUFFALO\nJune 2017', u'KERALA UNIVERSITY\nJune 2012']"
0,https://resumes.indeed.com/resume/11f7dcb802fcd418,"[u""SQL Developer / Data Analyst\nFirst Recovery Group - Southfield, MI\nJanuary 2017 to October 2017\n\u2022 Involved in the entire life cycle of the project starting from requirements gathering to end of system integration.\n\u2022 Interacted with the client for business requirements gathering.\n\u2022 Worked with SQL Server database.\n\u2022 Identified the Entities, attributes and designed a relational database system.\n\u2022 Built the application as per the requirements and Created Views and Indexes.\n\u2022 Interacted with DBA to discuss database design and modeling, index creations and SQL tuning issues.\n\u2022 Used SQL*Loader to load the database from flat files and Converted Data stored in flat files into SQL Server Databases.\n\u2022 Worked with DBAs to create a best fit Physical Data Model from the Logical Data Model.\n\u2022 Worked on Conceptual, Logical Modeling and Physical Database design for OLTP and OLAP systems.\n\u2022 Migrating data using MS SQL Server Analysis services (SSAS) creating packages, transferring huge data from internal software system to external data management systems using transformation like derived column, merge, conditional split, data conversions.\n\u2022 Analyzed existing work-flows of systems to identify and implement operational improvements.\n\u2022 Identify project requirements, define specifications, determine scope and document results.\n\u2022 Developed Project plan with Project Manager's assistance for the first two phases of the project."", u""Data Analyst\nGoogle India Ltd - Hyderabad, Telangana\nNovember 2013 to July 2015\n\u2022 Worked on Google's internal database performed DML functions and ETL to pull data from various resources (FF, Excel, CSV, DB-files).\n\u2022 Collaborate with IT and analytical community to drive unique trends and solutions using Data Analysis and Visualization in Tableau.\n\u2022 Working with Production and IT department to ensure Quality Data Management for Data Modeling purposes.\n\u2022 Involved in writing SQL queries to retrieve above information that would rather facilitate some of the campaigns\n\u2022 Tested all the SQL scripts for performance and follow up with the support team.\n\u2022 Performed research and provided valuable inputs in data management process.\n\n\u2022 Converting user requirements into process diagrams and data flow/data model diagrams.\n\n\u2022 Facilitate weekly meetings and documenting updates about the process flow.\n\n\u2022 Liaison between customers and development team.\n\n\u2022 Creating mutual understanding between user requirements and development team.\n\n\u2022 Documenting user feedback and providing valuable inputs to the development team.\n\u2022 Developing test cases to find problems in product performance.\n\u2022 Played a key role in UAT process by taking valuable feedback from customers and improving product performance which led to project's extension."", u'SQL Data Analyst-Intern\nCMC Limited - Hyderabad, Telangana\nJune 2012 to October 2013\n\u2022 Database designing, data optimization.\n\n\u2022 Transferring huge sets of data from MS Access to MS SQL.\n\n\u2022 Developed data migration tool to migrate data from one database to another.\n\n\u2022 Creating tables and altering tables using data manipulation functions.\n\n\u2022 Data migration toolkit sends the data in row manner then tables and then keys to establish a relation between tables in a particular database.\n\n\u2022 Conducted data optimization using MS Excel, data management, database design aspects.']","[u'Master of Science in Engineering Management', u""Bachelor's in Information Technology""]","[u'Trine University Fort Wayne, IN\nMay 2017', u'B.V Raju Institute of Technology Hyderabad, Telangana\nJune 2013']"
0,https://resumes.indeed.com/resume/d81fb7dc9b421470,"[u'Data Analyst\nAxiomSL - New York, NY\nJanuary 2016 to January 2018\n\u2022 Initiated, analyzed, and socialized data projects in conjunction with various cross-functional stakeholders\n\u2022 Developed complex SQL query to pull data from multiple data sources (Oracle, DB2) to deal with data issues and conducted trends and patterns analysis to figure out root cause of Data Issues to improve the Data Quality\n\u2022 Performed relationship analysis among multiple Databases, Tables and Views for ETL processing\n\u2022 Produced ad hoc analysis and data modeling to understand and predict behavior trends to drive decision-making\n\u2022 Built well-designed dashboard and report on core metrics and KPIs for ad-hoc analysis, accurately represented data and insights of various marketing campaigns with Excel and Tableau\n\u2022 Collaborated with cross-functional team members including product managers and software engineers to deliver compelling products/services\n\u2022 Designed new features for major replacements on axiom new software, and estimated opportunity size through simulation ($2M+ annualized profit lift)\n\u2022 Created a standardized A/B testing workflow for axiom software to simplify new feature launch, creative modulation and audience targeting (Improved adoption rate by 100%)', u'Data Analyst\nIBM - New York, NY\nJanuary 2015 to January 2016\n\u2022 Partnered with stakeholders to develop business plans and ensure processes are adopted across regions. Helped identify, structure and lead operational improvement initiatives\n\u2022 Developed key business metrics, user insights and perform market/industry due diligence\n\u2022 Built and maintained data driven optimization models, experiments, forecasting algorithms on time series data\n\u2022 Leveraged tools like Google Analytics, SQL, Python, R, and Tableau to drive efficient analytics, reporting, and visualizations (Predicted future price trend with 80% accuracy)', u""Marketing Analyst\nCCB - Nanjing, CN\nJanuary 2013 to January 2015\n\u2022 Collaborated closely with the sales, business analytics, product and other insights team members to define business objectives\n\u2022 Created customized KPI dashboard and visualized data into reader-friendly monthly report using python packages\n\u2022 Maintained data quality analysis with SQL, R and Python and provide insights to decision level which successfully made stakeholder decisions and increase product new features' accuracy by 50%\n\u2022 Leveraged metadata to gain actionable insight into trending, user experience, and behavior in order to create highly-targeted and strategic marketing solutions\n\u2022 Performed descriptive analytics and built compelling predictive model by applying different algorithm (decision tree or linear/logistic regression)\n\u2022 Conducted A/B tests through Google Analytics to optimize customer funnel and user engagement (Increased customer conversion rate by 50% with potential profit of $30M)""]","[u'MS in Computer Science', u'BA in Finance']","[u'FORDHAM UNIVERSITY New York, NY\nDecember 2016', u'SICHUAN UNIVERSITY Chengdu, CN\nJune 2014']"
0,https://resumes.indeed.com/resume/367cb002d0e3dc12,"[u'Data Analyst\nVapcon Manufacturing Engineers - Mumbai, Maharashtra\nOctober 2014 to May 2017\nDeveloped and implemented databases, data collection Methods, data analytics that optimized performance,\nefficiency, and quality of various business operations.\n\u2022 Played an important role in Interpreting manufacturing data, analyze trends or patterns in complex dataset results\nusing statistical techniques and data manipulation on platforms like R & Python.\n\u2022 Worked on customer segmentation and marketing campaigns for profiling, targeting, and acquisition of customers and categories to optimize marketing investments & increase the revenue.\n\u2022 Performed extraction & analysis of third-party database (DODGE) to forecast the future demand and market size.\n\u2022 Developed variety of reports and dashboards in Tableau to present insights and analysis to decision makers.', u""Business Analyst\nTech Cooling\nJuly 2014 to September 2014\n\u2022 Built & integrated various databases to enhance data related operations.\n\u2022 Developed the pricing and promotions strategies based on previous sales data using R.\n\u2022 Analyzed statistical reports on the company's web traffic using Google analytics and generated business reports using\nTableau.\n\u2022 Clustered customers based on RFM (Recency, Frequency and Monetary value) score, customer demographics, business\nsegments and products choices.\n\u2022 Collected pricing data from over 30 e-commerce websites using web scraping scripts in python to prepare MAP prices.""]","[u""Master's in Data Science in Engineering"", u'']","[u'School of Informatics and Computer Engineering\nAugust 2017 to December 2018', u'Indiana University']"
0,https://resumes.indeed.com/resume/7a8d0ae4f67b1d48,"[u'Business Analyst\nMorgan Stanley\nFebruary 2018 to Present\nResearched daily transactions in excess of 1.5 mm to compile and distribute a daily report for senior management with commentary o complex transactions across foreign exchange.\n\u2022 Completed comprehensive business development projects across the US, Europe, and Hong Kong.\n\u2022 Maintained clear communication with senior team and clients to remain coordinated on internal workstreams and external execution processes.\n\u2022 Ensured adherence to Federal Reserve regulations including Dodd Frank.', u'Data Analyst\nThe Economic Club - New York, NY\nNovember 2017 to January 2018\nReported directly to CEOs and presidents as a liaison to the club to collect, analyze, and distribute dues and outstanding charges.\n\u25cf Created visually impactful dashboards in Excel for data reporting by using pivot tables and VLOOKUP.\n\u25cf Extracted, interpreted and analyzed data to identify key metrics and transform raw data into meaningful, actionable information.', u'Banker\nTD BANK - New York, NY\nMarch 2014 to August 2017\nProformance based promotion from sales position to financial consulting, handling 54 accounts with a total net worth of $1.1mm.\n\u25cf Made significant contributions to the strategic planning for various calling campaigns and outreaches to further enhance the client experience, while maintaining relationships and uncovering new opportunities.\n\u25cf Gathered and analyzed data for studies and reports and made product recommendations based on findings.\n\u25cf Performed extensive ad hoc financial analysis for TD Bank using various reports like detailed operating statements and balance sheets to solve problems and develop alternative solutions.\n\u25cf Ensured money supply within the branch ($7.5mm annual deposits) was in compliance with bank and federal and state laws, policies and regulations.']",[u'BBA in Finance'],[u'Queens College\nMay 2017']
0,https://resumes.indeed.com/resume/5d25dba7cc889034,"[u'Data Analyst Intern\nGolden Gate University - San Francisco, CA\nOctober 2017 to Present\nLead Analyst for enrollment research\n\u2756 Analyzed and cleaned data sets up to 25,000 students\n\u2756 Build visuals that displays gross revenue, total student and units, race, gender, programs, school departments,\nmilitary, and international vs domestics students\n\u2756 Combined visuals with filter that allows users to choose specifically the year, program, school, race, gender,\nmilitary, international students, and/or domestic students to make a dashboard for university board\n\u2756 University board asked for more similar dashboards specifically built for each school departments\n\u2756 Used R to perform chi squared test on grade distribution where I test grade variables between courses in each\nprogram']",[u''],"[u'Golden Gate University Portfolio (https San Francisco, CA\nDecember 2018']"
0,https://resumes.indeed.com/resume/84ccc4a35ef78d20,"[u'data analyst\nGenentech\nJuly 2017 to Present\nBuilt the predictive analytics portfolio for my team consisting of Excel linear modeling templates, time series forecasting R scripts, and a Monte Carlo base predictive engine of my own design\n\u25e6 These tools enabled my team to detect issues in study plans early which potentially saved the company millions.\n- Created interactive dashboards with Spotfire to visualize study data for study management teams\n\u25e6 Before the development of some of these dashboards, gathering the same information could take days now it takes only seconds.\n- Daily use of Excel and R to fulfill ad-hoc reporting and analytic requests\n- Compiled and presented portfolio timeline data in order to inform important business decisions\nProgramming\n- Comfortable coding in C, C++, Java, Python, R, and MATLAB\n- Completed extensive coursework performing numerical analyses with MATLAB and building large object oriented programs in Java and C++\nE.g. Airport passenger and flight management system\n- Wrote several advanced statistical forecasting tools using R\nE.g. Box Jenkins, Holt Winters, and developed an original design for study enrollment predictive engine', u'Data Analyst\nGenentech - San Francisco, CA\nJuly 2017 to Present', u""Cook\nDe Vere's Irish Pub - Davis, CA\nSeptember 2015 to June 2017"", u'Tutor\nSelf-Employed\nSeptember 2011 to June 2014']","[u""bachelor's in mathematics""]","[u'University of California Davis Davis, CA\nJune 2017']"
0,https://resumes.indeed.com/resume/5be269879981e778,"[u""Data Analyst\nSurefire Local - McLean, VA\nSeptember 2016 to Present\nMcLean VA August 2017\n\u2022 Helped different departments read through excel sheets and transfer information into their new systems\n\u2022 Compiled data from 6 different websites for 30+ companies for executive reporting purposes\n\nTutor (specialization in Japanese) - Great Falls, VA September 2016 - present\n\u2022 Provided private tutoring up to three times a week to elementary through high school students in any subject\n\u2022 Reviewed class notes to ensure students understood lessons\n\u2022 Developed lessons and activities that helped students better understand what they were learning\n\u2022 For the elementary level students, created games that helped build a strong foundation for their Japanese language skills\n\u2022 Raised a middle school student's Japanese grade from a DtoaC after 2 weeks and a B+ by the midterm""]","[u""Bachelor's degree in Chemistry"", u'']","[u'Wake Forest University Winston-Salem, NC\nSeptember 2017 to May 2021', u'Langley High School McLean, VA\nJanuary 2013 to January 2017']"
0,https://resumes.indeed.com/resume/a7f1c3beea0aca2c,"[u'Business and Data Analyst\nGernentz & Associates - Beaumont, TX\nJanuary 2008 to Present\nDriving business, data, accounting, and statistical analysis including modeling, projections, trend analysis and additional deliverables using MS Excel, Power BI, Visio, Powerpoint, Minitab, SQL, SharePoint, and additional software required by clients including learning software on the fly.\n- Reporting of analyzed information through presentations, conference calls, videos, documents, and email to management, clients, and additional stakeholders.\n- Determining, developing, and analyzing KPIs.\n- Gathering client business and IT requirements and processes through communications and records.\n- Project management and task delegation for both internal and external departments in dozens of projects using agile methodology.\n- Full lifecycle requirements and implementation in more than 12 projects.\n- Creating and maintaining macros, pivot tables, plots, graphs, and visuals.\n- Organizing and data entering of 500 or more pages of tax, payroll, shipping, sales, and other documents to create accurate worksheets with thousands of rows.\n- Completing and submitting projects with 40% or more of deadline time remaining.', u'Data Analyst\nTerumo Cardiovascular Systems - Ann Arbor, MI\nAugust 2017 to January 2018\n- Continually trained coworkers in MS Excel, SharePoint, and additional software techniques which increased departmental efficiency by over 400%.\n- Created and maintained macros, data table templates, and reference charts which were used daily by coworkers and managers to improve efficiency and accuracy in preforming their roles.\n- Drove data analysis to identify and solve problems as they arose within testing and development in medical device manufacturing.\n- Established new processes which assisted in and improved data accuracy and validation.\n- Created and validated spreadsheets with up to tens of thousands of cells, verified thousands of pages of written records, and delivered accurate reports day to day.\n- Conformed to Good Documentation Practices across all records in adherence to FDA standards.\n- Conversed with several departments, including IT and engineering, in order gather requirements and specifications to formulate a proposal to overhaul existing data systems to store data efficiently and automate several jobs and tasks.']","[u'Bachelor of Science in Human Biology', u'Associate of Science in advertising']","[u'Michigan State University East Lansing, MI\nDecember 2014', u'Oakland Community College West Bloomfield Township, MI\nMay 2010']"
0,https://resumes.indeed.com/resume/3e62d95d64d9002c,"[u""Data Scientist\nActive Health Management - Alpharetta, GA\nSeptember 2016 to Present\nAlpharetta, GA September 2016- Present\nActive Health Management is a global leader in the creation of innovative prescription medicines for all health, mental health, and anesthesia - products that contribute to the health of people and their quality of life. Its objective is to be a reliable partner of health care professionals and patients by providing products and services for the worldwide improvement of human health and quality of life.\nThe project mostly consists of data processing activities and analyzing data to build patient and treatment models, capture patient and treatment behaviors and aggregate rules for customer treatment classification there by creating customer treatment recommendations.\nDesignation: Data Scientist\n\u2022 Experience working in Data Requirement analysis for transforming data according to business requirements.\n\u2022 Applied Forward Elimination and Backward Elimination for data sets to identify most statically significant variables and to remove insignificant variables for Data analysis and to get better predictive insights.\n\u2022 Utilized Label Encoders in Python to convert non-numerical significant variables to numerical significant variables to identify their impact on pre-acquisition and post acquisitions by using 2 sample paired t test.\n\u2022 Worked with ETL SQL Server Integration Services (SSIS) for data investigation and mapping to extract data and applied fast parsing and enhanced efficiency by 17%.\n\u2022 Developed Data Science content involving Data Manipulation and Visualization, Web Scraping, Machine Learning, Python programming, SQL, GIT and ETL for Data Extraction.\n\u2022 Created A/B testing and Multivariate testing to check the performance of Patient's applications.\n\u2022 Developed Analytical systems, data structures, gather and manipulate data, using statistical techniques.\n\u2022 Designing suite of Interactive dashboards, which provided an opportunity to scale and measure the statistics of the HR dept. which was not possible earlier and schedule and publish reports.\n\u2022 Provided and created data presentation to reduce biases and telling true story of people by pulling millions of rows of data using SQL and performed Exploratory Data Analysis.\n\u2022 Applied breadth of knowledge in programming (Python, R), Descriptive, Inferential, and Experimental Design statistics, advanced mathematics, and database functionality (SQL, Hadoop).\n\u2022 Migrated data from Heterogeneous Data Sources and legacy system (DB2, Access, Excel) to centralized SQL Server databases using SQL Server Integration Services (SSIS).\n\u2022 Applied Descriptive statistics and Inferential Statistics on varies data attributes using SPSS to draw better insights of data and to provide products and services for patients.\n\u2022 Developed Machine learning algorithms such as Collaborative filtering, Neural Network models, Hybrid recommendation model and NLP for analyzing most significant variables to get better predictive insights.\n\u2022 Rapidly evaluated Machine learning algorithms and Deep Learning frameworks, tools, techniques and approaches for deployment in AWS and consumption of data science teams.\n\u2022 Utilized NLP algorithms with help of NLTK and Genism libraries to recognize text from the patient's reviews.\n\u2022 Utilized data reduction techniques such as Factor analysis to identify most correlated values to underlying factors of the data and categorized the variable according to factors.\n\u2022 Applied Wilcoxon sign test to patient and treatment data for pre-acquisition and post-acquisition for different sectors to find the statistical significance in R programming.\n\u2022 Performance Tuning: Analyze the requirements and fine tune the stored procedures/queries to improve the performance of the application.\n\u2022 Designed and built scalable infrastructure to support real- time analytics with the help of Scala.\n\u2022 Used Spark Streaming to divide streaming data into batches for batch processing.\n\u2022 Developed various Tableau9.4 Data Models by extracting and using the data from various sources files, DB2, Excel, Flat Files and Bigdata.\n\u2022 Handled importing data from various data sources, performed transformations using Hive, Map Reduce, and loaded data into HDFS by using HQL queries in Hadoop.\n\u2022 Utilized Amazon Web Services (AWS) S3, EC2, EMR and RDS, Redshift to setup storage for deploying different developed machine learning models.\n\u2022 Setting up EC2 instances and deployment of patient applications and treatment records.\n\u2022 Interaction with Business Analyst, SMEs and other Data Architects to understand Business needs and functionality for various project solutions.\n\u2022 Identifying and executing process improvements, hands-on in various technologies such as Oracle, Informatica, and Business Objects.\n\u2022 Designed both 3NF data models for ODS, OLTP systems and dimensional data models using Star and Snowflake Schemas.\n\nEnvironment: Python, Jupyter, Tensor Flow, Keras, Theano, R Programming, SPSS, SQL Server 2014, SSRS, SSIS, SSAS, Spark, Scala, AWS, Microsoft office, SQL Server Management Studio, Business Intelligence Development Studio, MS Access, Informatica, SAP Business Objects and Business Intelligence."", u""Data Scientist\nMISO - Carmel, IN\nJune 2015 to August 2016\nCarmel, IN June 2015- August 2016\nMISO is a cost-effective delivery of electric power consumption in Carmel, IN that specializes in power service. The company wanted to enhance the business to find new ways of bringing more customers.\nThe project involves data extraction and applying data integrity and analytical techniques for story telling from the data. The major key performance indicators such as power reliability and improved interconnected transmission data etc. using supervised and unsupervised machine learning techniques.\nDesignation: Data Scientist\n\u2022 Involved in gathering, analyzing and translating business requirements into analytic approaches.\n\u2022 Worked with Machine learning algorithms like Neural network models, Linear Regressions (linear, logistic etc.), SVM's, Decision trees for classification of groups and analyzing most significant variables.\n\u2022 And utilized SAS for developing Pareto Chart for identifying highly impacting categories in modules to find the work force distribution and created various data visualization charts.\n\u2022 Performed univariate, bivariate and multiple analysis of approx. 4890 tuples using bar charts, box plots and histograms.\n\u2022 Participated in features engineering such as feature creating, feature scaling and One-Hot encoding with Scikit-learn.\n\u2022 Converted raw data to processed data by merging, finding outliers, errors, trends, missing values and distributions in the data.\n\u2022 Generated detailed report after validating the graphs using R and adjusting the variables to fit the model.\n\u2022 Worked on Clustering and factor analysis for classification of data using machine learning algorithms.\n\u2022 Developed Descriptive statistics and inferential statistics for Logistics optimization, Value throughput data to at 95% confidence interval.\n\u2022 Imported data by using Power Query in MS Excel from API's and Web API's then created relationship between data tables by using Power Pivot.\n\u2022 Used Power Map and Power View to represent data very effectively to explain and understand technical and non-technical users.\n\u2022 Written MapReduce code to process and parsing the data from various sources and storing parsed data into HBase and Hive using HBase - Hive Integration.\n\u2022 Created SQL tables with referential integrity and developed advanced queries using stored procedures and functions using SQL server management studio.\n\u2022 Used TensorFlow, Keras, Theano, Pandas, NumPy, SciPy, Scikit-learn, NLTK in Python for developing various machine learning algorithms such as Neural network models, Linear Regression, multivariate regression, na\xefve Bayes, random Forests, decision trees, SVMs, K-means and KNN for data analysis.\n\u2022 Responsible for developing data pipeline with AWS S3 to extract the data and store in HDFS and deploy implemented all machine learning models.\n\u2022 Used Spark and Spark-SQL/Streaming for faster testing and processing of data.\n\u2022 Used packages like dplyr, tidyr and ggplot2 in R Studio for data visualization and generated scatter plot and high low graph to identify relation between different variables.\n\u2022 Worked on Business forecasting, segmentation analysis and Data mining and prepared management reports defining the problem; documenting the analysis and recommending courses of action to determine the best outcomes.\n\u2022 Experience with risk analysis, root cause analysis, cluster analysis, correlation and optimization and K-means algorithm for clustering data into groups.\n\u2022 Coordinated with data scientists and senior technical staff to identify client's needs.\n\nEnvironment: SQL Server 2012, Python, Jupyter, R 3.1.2, MATLAB, SSRS, SSIS, SSAS, MongoDB, HBase, HDFS, Hive, Pig, SAS, Power Query, Power Pivot, Power Map, Power View, Microsoft office, SQL Server Management Studio, Business Intelligence Development Studio, MS Access."", u""Data Analyst\nEducational Testing Service - Princeton, NJ\nJanuary 2014 to May 2015\nPrinceton, NJ January 2014- May 2015\nEducational Testing Service is a leading testing service organization. It provided and maintained all testing information of test takers. The project consisted of data preprocessing activities from OLTP server using ETL and analyzing data to build testing report and test models, capture test taker behaviors and aggregate rules for test takers by their recommendations.\nDesignation: Data Analyst\n\u2022 Developed complex SQL queries using group by, join, where clause to answer tester questions.\n\u2022 Communicated and coordinated with other departments to collect Business Requirement Analysis and developed data flow mapping to load OLAP server for analysis of policies details.\n\u2022 Worked on missing value imputation, outlier's identification using Random Forest and Box Plots.\n\u2022 Tackled highly imbalanced dataset using under sampling with ensemble methods, oversampling and cost sensitive algorithms.\n\u2022 Improved prediction performance by using random forest and gradient boosting for feature selection with the help of Scikit-learn library in Python.\n\u2022 Utilized Parametric and Non-Parametric test in SPSS to draw insights from data for making business decisions.\n\u2022 Implemented machine learning models (logistic regression, XGboost) with the help of Scikit-learn in Python.\n\u2022 Validated and selected models using k-fold cross validation, confusion matrices and worked on optimizing models for high recall rate.\n\u2022 Involved with data profiling for multiple sources and answered complex business questions by providing data to business users.\n\u2022 Participated in Agile planning process and daily scrums, providing details and discuss with team lead, Data Scientist, Data Analyst, Data Engineer and others.\n\u2022 Experience with routine DBA activities like Query Optimization, Performance Tuning and Effective SQL Server configuration for better performance and cost reduction.\n\u2022 Developed Tabular Reports, Sub Reports, Matrix Reports, drill down Reports and Charts using SQL Server Reporting Services (SSRS).\n\u2022 Designed rich data visualizations with Tableau 9.4 and dynamic dashboards for business analysis.\n\nEnvironment: SQL Server 2012, R programming, Python, MATLAB, SSRS, SSIS, SSAS, SPSS, Tableau, Minitab, Microsoft office, SQL Server Management Studio, Business Intelligence Development Studio, MS Access."", u""Data Analyst\nDecision Craft - Bengaluru, Karnataka\nSeptember 2012 to December 2013\nBangalore, India September 2012- December 2013\nDecision Craft is a provider of information technology (IT) services. The Company delivers a range of IT services through globally integrated onsite and offshore delivery locations. It offers its services to customers through industry-focused practices, including insurance, manufacturing, financial services, healthcare and telecommunications, and through technology-focused practices.\nThe project consisted of extracting data from various sources and transforming the data according to the business requirements and loading data to target destination. The SQL Server Integration Services (SSIS) is used to create mapping for automating data pipelines from OLTP to OLAP. The Data cleansing and Data integrity checks are performed on data and customized business report are developed and deployed on to SQL Server Reporting Services (SSRS) server for making Business Decisions.\nDesignation: Data Analyst\n\u2022 Gathered requirements and documented requirements with Use cases in Requisite Pro and created different traceability views by MS Visio.\n\u2022 Worked with Data Compliance teams to identify the most suitable source of record and outlined the data.\n\u2022 Functioned with project team representatives to ensure that logical and physical ER/Studio data models were established in line with business standards and guidelines.\n\u2022 Deeply analyzed the clients' data by using SQL Server Analytic Services (SSAS).\n\u2022 Completed data cleaning process to discover like taking care of missing values by utilizing strategies, like, supplanting by mean, forward/backward fill, evacuating whole rows/columns/values, expelling outliers and errors, normalizing, and scaling information in data set.\n\u2022 Implemented metadata repository, maintained data quality, data cleanup procedures, transformations, data standards, data governance program, scripts, stored procedures, triggers and executed test plans.\n\u2022 Worked with team by Extracting Mainframe Flat Files (Fixed or CSV) onto UNIX Server and then converting them into Teradata Tables using SQL Server Integration Services (SSIS).\n\u2022 Responsible for report generation using SQL Server Reporting Services (SSRS) and Crystal Reports based on business requirements and connect with Teradata base for generating daily reports.\n\u2022 Developed visualizations and dashboards using Tableau to present analysis outcomes in terms of patterns, anomalies, and predictions use of bar charts, Scatter Plots, 3D plots, and histograms.\n\u2022 Documented the complete process flow to describe program development, logic, testing, implementation, application integration, and coding by using SQL Server Reporting Services (SSRS).\n\u2022 Created customized SQL Queries using SQL Server 2008/2008R2 Enterprise to pull specified data for analysis and report building in conjunction with Crystal Reports.\n\u2022 Designed & developed various Ad hoc reports for different teams in Business (Teradata and MS ACCESS, MS EXCEL).\n\nEnvironment: SSRS, SSIS, SSAS, SQL Server 2008/2008 R2 Enterprise, Tableau, MS Visio, MS Excel, MS Project, Teradata, Crystal Reports, ER Studio, Crystal reports, and Business Objects.""]",[u'Bachelor of Technology in Technology'],[u'Jawaharlal Nehru technological university']
0,https://resumes.indeed.com/resume/be5df73e2d9ceae3,"[u""Data Analyst\nSears Holdings - Chicago, IL\nAugust 2017 to Present\nDescription:\nSears Holdings. The Sears Holdings Corporation is an American holding company headquartered in Hoffman Estates, Illinois, a suburb of Chicago. It is the owner of retail store brands Sears and Kmart, and was founded after the latter purchased the former in 2005. Sears Holdings also owns the brands Kenmore and DieHard.\nResponsibilities:\n\n\u2022 Wrote several Teradata SQLQueries using TeradataSQLAssistant for AdHocDataPull request.\n\u2022 Developed Python programs for manipulating the data reading from various Teradata and convert them as one CSV Files.\n\u2022 Performing statistical data analysis and data visualization using Python and R.\n\u2022 Worked on creating filters, parameters and calculated sets for preparing dashboards and worksheets in Tableau.\n\u2022 Created data models in Splunk using pivot tables by analyzing vast amount of data and extracting key information to suit various business requirements.\n\u2022 Created new scripts for Splunk scripted input for collecting CPU, system and OSdata.\n\u2022 Interacting with other data scientists and architected custom solutions for data visualization using tools like tableau, Packages in R and R-Shiny.\n\u2022 Implemented data refreshes on Tableau Server for biweekly and monthly increments based on business change to ensure that the views and dashboards were displaying the changed data accurately.\n\u2022 Maintenance of large data sets, combining data from various sources by Excel, SASGrid, Enterprise, Access and SQLqueries.\n\u2022 Analyzed DataSet with SASprogramming, R and Excel.\n\u2022 Publish Interactive dashboards and schedule auto-data refreshes\n\u2022 Experience in performing Tableau administering by using tableau admin commands.\n\u2022 Created Hivequeries that helped market analysts spot emerging trends by comparing incremental data with Teradata reference tables and historical metrics.\n\u2022 Responsible for creating Hivetables, loading the structured data resulted from MapReduce jobs into the tables and writing hive queries to further analyze the logs to identify issues and behavioral patterns.\n\u2022 Developed normalized Logical and Physicaldatabase models for designing an OLTP application.\n\u2022 Knowledgeable in AWSEnvironment for loading data files from on prim to Redshiftcluster.\n\u2022 Performed SQL Testing on AWSRedshift databases.\n\u2022 Developed TeradataSQLscripts using OLAPfunctions like rank and rank over to improve the query performance while pulling the data from large tables.\n\u2022 Involved in running MapReduce jobs for processing millions of records.\n\u2022 Written complex SQLqueries using joins and OLAP functions like CSUM, Count and Rank etc.\n\u2022 Involved in extensive routine operational reporting, hoc reporting, and data manipulation to produce routine metrics and dashboards for management\n\u2022 Created actionfilters, parameters and calculated sets for preparing dashboards and worksheets in Tableau.\n\u2022 Building, publishing customized interactive reports and dashboards, reportscheduling using Tableauserver.\n\u2022 Experienced in migrating HiveQL into Impala to minimize query response time.\n\u2022 Responsible for DataModeling as per our requirement in HBase and for managing and schedulingJobs on a Hadoop cluster using Ooziejobs.\n\u2022 Worked on SparkSQL and Dataframes for faster execution of Hive queries using SparkSqlContext.\n\u2022 Design and development of ETLprocesses using InformaticaETLtool for dimension and fact file creation.\n\u2022 Develop and automate solutions for a new billing and membership EnterprisedataWarehouse including ETLroutines, tables, maps, materializedviews, and stored procedures incorporating Informatica and OraclePL/SQLtoolsets.\n\u2022 Performed analysis on implementing Spark using Scala and wrote spark sample programs using PySpark.\n\u2022 Created UDFs to calculate the pending payment for the given residential or small business customer's quotation data and used in Pig and HiveScripts.\n\u2022 Experienced in moving data from Hive tables into HBase for real time analytics on Hive tables.\n\u2022 Handled importing of data from various data sources, performed transformations using Hive. (External tables, partitioning).\n\nEnvironment: SQL/Server, Oracle 9i, MS-Office, Teradata, Informatica, ER Studio, XML, Hive, HDFS, Flume, Sqooq, R connector, Python, R, Tableau 9.2."", u'Data Analyst\nAmerican college of cardiology - Washington, DC\nMay 2016 to July 2017\nDescription:\nThe American College of Cardiology is a 54,000-member medical society that is the professional home for the entire cardiovascular care team. The mission of the College is to transform cardiovascular care and to improve heart health. The ACC leads in the formation of health policy, standards and guidelines.\nResponsibilities:\n\n\u2022 Work with users to identify the most appropriate source of record required to define the asset data for financing\n\u2022 Performed data profiling in TargetDWH\n\u2022 Experience in using OLAP function like Count, SUM and CSUM\n\u2022 Performed Data analysis and Dataprofiling using complexSQL on various sources systems including Oracle and Teradata.\n\u2022 Hands on Experience on Sqoop.\n\u2022 Developed normalized Logical and Physical database models for designing an OLTP application.\n\u2022 Developed new scripts for gathering network and storage inventory data and make Splunk ingest data.\n\u2022 Imported the customer data into Python using Pandas libraries and performed various data analysis - found patterns in data which helped in key decisions for the company\n\u2022 Created tables in Hive and loaded the structured (resulted from Map Reduce jobs) data\n\u2022 Using HiveQL developed many queries and extracted the required information.\n\u2022 Exported the data required information to RDBMS using Sqoop to make the data available for the claims processing team to assist in processing a claim based on the data.\n\u2022 Design and deploy rich Graphicvisualizations with DrillDown and Dropdown menu option and Parameterized using Tableau.\n\u2022 Extracted data from the database using SAS/Access, SASSQL procedures and create SAS data sets.\n\u2022 Created TeradataSQL scripts using OLAP functions like RANK to improve the query performance while pulling the data from large tables.\n\u2022 Worked on MongoDBdatabase concepts such as locking, transactions, indexes, Sharding, replication, schemadesign, etc.\n\u2022 Performed Data analysis using PythonPandas.\n\u2022 Good experience in AgileMethodologies, Scrum stories and sprints experience in a Pythonbasedenvironment, along with data analytics and Exceldata extracts.\n\u2022 Created Hive queries that helped market analysts spot emerging trends by comparing fresh data with EDW reference tables and historical metrics.\n\u2022 Involved in defining the source to target data mappings, business rules, business and data definitions\n\u2022 Responsible for defining the key identifiers for each mapping/interface\n\u2022 Responsible for defining the functional requirement documents for each source to target interface.\n\u2022 Hands on Experience on Pivottables, Graphs in MSExcel\n\u2022 Using advanced Excel features like Pivottables and Charts for generating Graphs.\n\u2022 Designed and developed weekly, monthly reports by using MSExcelTechniques (Charts, Graphs, Pivottables) and Powerpoint presentations.\n\u2022 Strong Excelskills, includingpivots, Vlookup, conditionalformatting, large record sets. Including data manipulation and cleaning.\n\nEnvironment: SQL/Server, Oracle 9i, MS-Office, Teradata, Informatica, ER Studio, XML, Hive, HDFS, Flume, Sqoop, R connector, Python, R, Tableau 9.2', u'Data Analyst\nFleetCor Technologies Inc - Norcross, GA\nJanuary 2015 to April 2016\nDescription:FleetCor Technologies, Inc. provides specialized payment products and services to Dataes, commercial fleets, oil companies, petroleum marketers, and government entities in North America, Europe, South Africa, and Asia.\nResponsibilities:\n\n\u2022 Created new reports based on requirements. Responsible in Generating Weekly ad-hoc Reports\n\u2022 Planned, coordinated, and monitored project levels of performance and activities to ensure project completion in time.\n\u2022 Automated and scheduled recurring reporting processes using UNIXshellscripting and Teradata utilities such as MLOAD, BTEQ and FastLoad.\n\u2022 Worked in a ScrumAgileprocess&WritingStories with two week iterations delivering product for eachiteration.\n\u2022 Worked on transferring the data files to vendor through SFTP&FTPprocess.\n\u2022 Involved in defining and Constructing the customer to customer relationships based on Association to an account & customer\n\u2022 Created action filters, parameters and calculatedsets for preparing dashboards and worksheets in Tableau.\n\u2022 Experience in performing Tableauadministering by using tableau admin commands.\n\u2022 Worked with architects and, assisting in the development of current and target state enterprise level data architectures\n\u2022 Worked with project team representatives to ensure that logical and physical data models were developed in line with corporate standards and guidelines.\n\u2022 Involved in defining the source to target data mappings, business rules and data definitions.\n\u2022 Responsible for defining the key identifiers for each mapping/interface.\n\u2022 Performed data analysis and data profiling using complex SQL on various sources systems including Oracle and Teradata.\n\u2022 Migrated three critical reporting systems to BusinessObjects and WebIntelligence on a Teradata platform\n\u2022 Created Excelcharts and pivot tables for the Adhocdatapull\n\nEnvironment: Teradata 13.1, Informatica 6.2.1, Ab Initio, Business Objects, Oracle 9i, PL/SQL, Microsoft Office Suite (Excel, Vlookup, Pivot, Access, Power Point), Visio, VBA, Micro Strategy, Tableau, UNIX Shell Scripting ERWIN.', u'Data Analyst\nCopartInc - Fairfield, CA\nMay 2013 to December 2014\nDescription:Copart, Inc. provides online auctions and vehicle remarketing services in the United States, Canada, and the United Kingdom.\nResponsibilities:\n\n\u2022 Experienced in developing business reports by writing complex SQLqueries using views, volatile tables\n\u2022 Experienced in Automating and Scheduling the TeradataSQLScripts in UNIX using KornShell scripting.\n\u2022 Wrote several TeradataSQLQueries using TeradataSQLAssistant for AdHocDataPullrequest.\n\u2022 Extensive experience in working with TableauDesktop, TableauServer, and TableauReader in various versions of Tableau7.0, 8.0, 8.3, 9.0 and 10 as a Developer and Analyst.\n\u2022 Analysis of functional and non-functional categorized data elements for data profiling and mapping from source to target data environment. Developed working documents to support findings and assign specific tasks.\n\u2022 Design and prototype of accurate and scalableprediction algorithms using R/RStudio.\n\u2022 Analyzed different types of data to derive insights about relationships between locations, statistical measurements and qualitatively assess the data using R/RStudio.\n\u2022 Data Profiling to help identify patterns in the source data using SQL and Informatica and thereby help improve quality of data and help business to understand the converted data better to come up with accurate business rules.\n\u2022 Involved with data profiling for multiple sources and answered complex business questions by providing data to business users.\n\u2022 Implemented Indexes, CollectingStatistics, and Constraints while creating table\n\u2022 Created action filters, parameters and calculated sets for preparing dashboards and worksheets in Tableau.\n\u2022 Building, publishing customized interactive reports and dashboards, report scheduling using Tableau server.\n\u2022 Design and deploy rich Graphicvisualizations with DrillDown and Dropdown menu option and Parameterized using Tableau.\n\u2022 Created sidebysidebars, ScatterPlots, StackedBars, HeatMaps, FilledMaps and SymbolMaps according to deliverable specifications.', u""Data Analyst\nFirst Indian Corporation Private Limited - Hyderabad, Telangana\nDecember 2011 to April 2013\nDescription:First Indian Corporation Private Limited, a member of The First American family of companies, provides specialized offshore transaction services, technology services, and analytics to the mortgage industry.\n\nResponsibilities:\n\u2022 Activating lease contracts to generate invoices for customer payments in a system, Pyramid.\n\u2022 Assigning work to the team members and handling in-house workflow system.\n\u2022 Handling review calls with front-end customers on process updates.\n\u2022 Being involved in the quality check process for peers\n\u2022 Work closely with the deal desk team and other business partners like corporate systems ops and finance, to ensure accurate quotes and contracts are quoted and booked.\n\u2022 Involved with various customers for the quick resolution of pending deals or orders; preparing operational metrics for the process.\n\u2022 Maintaining relationship with clients to achieve quality service norms by resolving their service related critical issues.\n\u2022 Handling escalation cases in SFDC (VmStarApplication of Vmware) with quality.\n\u2022 Preparing control reports for the team and self are accurate.\n\u2022 Looking after US&Domesticcustomers and following TAT.\n\u2022 Maintaining daily trackers of the team in processing orders.\n\u2022 Providing updates on University'sclosure and the action to be taken by CSR to understand the delivery time to reach customer.\n\u2022 Identify root cause analysis of all complaints and standardize processes aspect wise &recommend process improvements. Conduct or coordinate gaps in internal business process are verified and fixed. Identify critical processes, schedule and audit processes/practices for compliance and performance, generate report, follow-up for closing of gaps\n\u2022 Supporting backend processes and ensuring that the orders are dispatched in warehouse and finally to end customer.\n\u2022 Ensuring SLA and TAT for all the process, which includes Singapore and US customers as well.\n\nEnvironment: Oracle 10g, MS-Office, Teradata, Tableau."", u'MS-Office, Teradata, Tableau\nJanuary 2013 to January 2013', u'System Analyst\nAccenture - Bengaluru, Karnataka\nMay 2009 to November 2011\nDescription: Accenture PLC is a global management consulting and professional services company that provide strategy, consulting, digital, technology and operations.\n\nResponsibilities:\n\u2022 OABS Analytics team is mainly into reporting and creation of dashboards and Digitization of reports for all the Practices.\n\u2022 Design and develop the Headcountreports for worldwide regions and for all the practices.\n\u2022 Validation of data to check the accuracy of it.\n\u2022 Analytics team is mainly into reporting and creation of dashboards and Digitization of reports.\n\u2022 Automate report to pull data from database directly to excel and make the formatting and other changes to the report.\n\u2022 Design and develop the sales standard reports for APJ regions and area across the client locations.\n\u2022 Prepare the Weekly monthly and fortnight sales report.\n\u2022 Validate the report in order to check the accuracy of it.\n\u2022 Identify trends and present results in chart format.\n\u2022 Manage, organize & update relevant data to support business activities\n\u2022 Check source data in order to validate completeness & accuracy\n\u2022 Involved in automation of reports, was a go to person for all automation on all MSofficetools.\n\u2022 Worked on VBAprojects as integration of Excel-PowerPoint and Excel-Access etc.\n\u2022 Used charts, Pivots and other complex excel function to automate and improve the productivity of the team.\n\u2022 Frequent Adhoc request to support key business requirement.\n\u2022 Providing Content and Data Managementservices to Ford of Europe being in Marketing Sales & Service team.\n\u2022 Involved in report writing and business process development.\n\u2022 Modified reports and make proper evaluations.\n\u2022 Validated data to ensure the quality, validity and accuracy of content.\n\u2022 Claim processing for both Indian and European market (Retail Claims, Fleet Claims, and Warranty Claims etc.).\n\nEnvironment:Adhoc, VBA, SQL/Server, Oracle 9i, MS-Office, Teradata.']",[],[]
0,https://resumes.indeed.com/resume/183acdbbce3d0a7d,"[u'DATA ANALYST, HEATHCARE\nARMSRx\nSeptember 2015 to September 2016\nManage and oversee all aspects of data analytics, to include data modeling, audits, reconciliations, and ad-hoc analyses, while producing savings analyses on PBM data received in order for clients to determine the most cost effective choice in Rx benefits. Partner in PBM implementation pre and post go-live testing. Participate in data requirements gathering with partners and clients. Initiate savings projections for benefit changes, copay modeling, auditing and the reconciliation of audit outcomes, and custom analysis requests using T-SQL, Access, Excel, and ETL scripting. Monitor plan performance, identify problem areas and identify cost savings; assist consultants with data reviews, client/pricing renewals, and the sale of new accounts. Monitor performance guarantees as stipulated in the clients contracts; lead the testing of specific benefit plans and formularies.', u'DATA ANALYST\nJune 2015 to September 2015\nBrookdale Senior Living\nConducted aspects of data analysis, ad hoc reporting, testing, and data validation tasks related to both new and existing applications and business projects, while working directly with business users/leaders to ensure support of\nbusiness objectives.\nDeveloped analysis to answer specific questions from business leaders.\nAssisted in the preparation of testing and validation of data conversions and application integrations.', u'SENIOR DATA ANALYST\nTricast, Inc\nJune 2012 to June 2015\nManaged and led the review and interpretation of PBM contracts to determine appropriate audit methodology,\ninvestigated and identified audit outcome discrepancies by comparing output files with SQL queries, initiated queries, and produced and ad-hoc analyses to support management decisions.\nAnalyzed and identified potential loopholes in PBM contract pricing language that could be costing the client\nmoney.\nCommunicated data requirements through project kick-off calls.\nCreated/maintained ETL SOP documents and FTP user accounts, and managed automated tasks moving files between FTP and directories.\nTranslated data layout documents into SQL format files and adapted BCP and PowerShell scripts for loading text\nfiles; mapped raw file fields to standardized fields by adapting SQL insert scripts.\nLoaded data through star structures and data stores to production.\nProduced ad-hoc reporting as requested by Business Analyst team.', u'DATA ANALYST/DATA MANAGEMENT CONSULTANT\nTrivantage/Thomson Reuters\nApril 2008 to June 2012\nConducted the review, analysis and interpretation of PBM contracts to determine appropriate audit methodology,\napplied methodology to internal audit application for automated reporting, and created/developed ad-hoc analyses\nusing SQL queries.\nManaged data flow, investigated and identified audit outcome discrepancies by comparing output files with SQL\nqueries, and reviewed findings with internal clients.\nCommunicate data requirements through kick off calls, created/maintained ETL SOP documents, and translated\ndata layout documents into SQL Loader scripts for loading text files.\nMapped raw file fields to standardized fields using SQL transform scripts.\nPerformed all aspects of data QA, adapted existing automated reports.', u'CASE MANAGER\nMedical College of Wisconsin\nJanuary 2000 to June 2007\nCollaborated with health care delivery team to triage patient calls or unscheduled visitations, conducted needs\nassessment and intervention, identified patients at risk, and proactively provided support and access to referral\nservices.\nFacilitated a dual diagnosis support group.\nConnected patients with community resources and assisted patients in applying for assistance programs for free\nmedication from various pharmaceutical companies.\nCreated a database to track patient assistance applications; saved Milwaukee County millions in medication\ncosts.\nAudited the pharmacy contracted with the clinic to ensure all proper discounts were applied.']","[u'Certification in Data Science', u'Bachelor of Science in Psychology & Sociology']","[u'Microsoft\nJanuary 2016 to January 2017', u'University of Wisconsin Milwaukee, WI']"
0,https://resumes.indeed.com/resume/ae6d6f71c7c95efb,"[u""Data Scientist\nVerizon, TX\nFebruary 2017 to Present\nDescription: Verizon Wireless is an American telecommunications company, a wholly owned subsidiary of Verizon Communications, which offers wireless products and services. Verizon Wireless is the largest wireless telecommunications provider in the United States.\nThe company is headquartered in Basking Ridge, New Jersey. It was founded in 2000 as a joint venture of American telecommunications firm Bell Atlantic, which would soon become Verizon Communications, and British multinational telecommunications company Vodafone.\nResponsibilities:\n\n\u2022 Responsible for working with various teams on a project to develop analytics based solution to target roaming subscribers specifically.\n\u2022 Leading a team of 4 data analysts and created multi-dimensional segmentation to define specific cohorts of subscribers.\n\u2022 Preparing the travel prediction model that can predict subscribers' future travel behavior up to a month in advance.\n\u2022 Combination of these elements (travel prediction & multi-dimensional segmentation) would enable operators to conduct highly targeted and personalized roaming services campaigns leading to significant subscriber uptake.\n\u2022 Scaled up to Machine Learning pipelines: 4600 processors, 35000 GB memory achieving 5-minute execution.\n\u2022 Deployed GUI pages by using JSP, JSTL, HTML, DHTML, XHTML, CSS, JavaScript, AJAX.\n\u2022 Configured the project on WebSphere 6.1 application servers\n\n\u2022 Developed a Machine Learning test-bed with 24 different model learning and feature learning algorithms.\n\u2022 By thorough systematic search, demonstrated performance surpassing the state-of-the-art (deep learning).\n\n\u2022 Up to 10 times more accurate predictions over existing state-of-the-art algorithms.\n\u2022 Developed in-disk, huge (100GB+), highly complex Machine Learning models.\n\u2022 Used SAX and DOM parsers to parse the raw XML documents\n\u2022 Used RAD as Development IDE for web applications.\n\n\u2022 Develop and plan required analytic projects in response to business needs.\n\u2022 In conjunction with data owners and department managers, contribute to the development of data models and protocols for mining production databases.\n\u2022 Develop new analytical methods and/or tools as required.\n\u2022 Contribute to data mining architectures, modeling standards, reporting, and data analysis methodologies.\n\u2022 Conduct research and make recommendations on data mining products, services, protocols, and standards in support of procurement and development efforts.\n\u2022 Work with application developers to extract data relevant for analysis.\n\u2022 Collaborate with unit managers, end users, development staff, and other stakeholders to integrate data mining results with existing systems.\n\u2022 Provide and apply quality assurance best practices for data mining/analysis services.\n\nEnvironment: R 9.0, Informatica 9.0, ODS, OLTP, Oracle 10g, Hive, OLAP, DB2, Metadata, MS Excel, Mainframes MS Vision, Rational Rose."", u'Data Scientist\nMarvell Technology Group - Santa Clara, CA\nDecember 2015 to January 2017\nDescription: Marvell Technology Group Limited is a producer of storage, communications and consumer semiconductor products. Marvell\'s U.S. operating headquarters is located in Santa Clara, California, and the company operates design centers in places including Canada, Europe, Israel, India, Singapore and China. Marvell is a ""fabless"" manufacturer of semiconductors. Its market segments include the enterprise, cloud, automotive, industrial and consumer markets.\n\nResponsibilities:\n\u2022 Developed the prediction model for crop yield, based on different kinds of field, weather and imagery data.\n\u2022 Exploratory data analysis and Feature engineering to best fit the regression model.\n\u2022 Designed a static pipeline in MS Azure for data ingestion and dashboarding. Used MS ML Studio for modeling and MS Power BI for dash boarding.\n\u2022 Analyze large datasets to provide strategic direction to the company.\n\u2022 Perform quantitative analysis of product sales trends to recommend pricing decisions.\n\u2022 Conduct cost and benefits analysis on new ideas.\n\u2022 Scrutinize and track customer behavior to identify trends and unmet needs.\n\u2022 Develop statistical models to forecast inventory and procurement cycles.\n\u2022 Assist in developing internal tools for data analysis.\n\u2022 Advising on the suitability of methodologies and suggesting improvements.\n\u2022 Carrying out specified data processing and statistical techniques.\n\u2022 Supplying qualitative and quantitative data to colleagues & clients.\n\u2022 Using Informatica & SAS to extract transform & load source data from transaction systems.\n\u2022 Creating data pipelines using big data technologies like Hadoop, spark etc.\n\u2022 Creating statistical models using distributed and standalone models to build various diagnostics, predictive and prescriptive solution.\n\u2022 Utilize a broad variety of statistical packages like SAS, R , MLIB, Graphs, Hadoop, Spark , MapReduce, Pig and others\n\u2022 Refine and train models based on domain knowledge and customer business objectives\n\u2022 Deliver or collaborate on delivering effective visualizations to support the client business objectives\n\u2022 Communicate to your peers and managers promptly as and when required.\n\u2022 Produce solid and effective strategies based on accurate and meaningful data reports and analysis and/or keen observations.\n\u2022 Establish and maintain communication with clients and/or team members; understand needs, resolve issues, and meet expectations\n\u2022 Developed web applications using .net technologies; work on bug fixes/issues that arise in the production environment and resolve them at the earliest\nEnvironment: SQL Server 2008R2/2005 Enterprise, SSRS, SSIS, Crystal Reports, Hadoop, Windows Enterprise Server 2000, DTS, SQL Profiler, and Query Analyzer.', u""Data Scientist\nEbay Inc - San Jose, CA\nFebruary 2014 to November 2015\nDescription: Ebay is a multinational e-commerce corporation, facilitating online consumer-to consumer and business-to-consumer sales. It is headquartered in San Jose, California. eBay was founded by Pierre Omidyar in 1995, and became a notable success story of the dot-com bubble.\nResponsibilities:\n\u2022 Developed scalable machine learning solutions within a distributed computation framework (e.g. Hadoop, Spark, Storm etc.).\n\u2022 Utilizing NLP applications such as topic models and sentiment analysis to identify trends and patterns within massive data sets.\n\u2022 Creating automated anomaly detection systems and constant tracking of its performance\n\u2022 Strong command of data architecture and data modelling techniques.\n\u2022 Knowledge in ML & Statistical libraries (e.g. Scikit-learn, Pandas).\n\u2022 Having knowledge to build predict models to forecast risks for product launches and operations and help predict workflow and capacity requirements for TRMS operations\n\u2022 Having experience with visualization technologies such as Tableau\n\u2022 Draw inferences and conclusions, and create dashboards and visualizations of processed data, identify trends, anomalies\n\u2022 Generation of TLFs and summary reports, etc. ensuring on-time quality delivery.\n\u2022 Participated in client meetings, teleconferences and video conferences to keep track of project requirements, commitments made and the delivery thereof.\n\u2022 Solved analytical problems, and effectively communicate methodologies and results\n\u2022 Worked closely with internal stakeholders such as business teams, product managers, engineering teams, and partner teams.\n\u2022 Data mining using state-of-the-art methods\n\u2022 Extending company's data with third party sources of information when needed\n\u2022 Enhancing data collection procedures to include information that is relevant for building analytic systems\n\u2022 Processing, cleansing, and verifying the integrity of data used for analysis\n\u2022 Doing ad-hoc analysis and presenting results in a clear manner.\n\u2022 Created automated metrics using complex databases.\n\u2022 Foster culture of continuous engineering improvement through mentoring, feedback, and metrics.\nEnvironment: Erwin r9.0, Informatica 9.0, ODS, OLTP, Oracle 10g, Hive, OLAP, DB2, Metadata, MS Excel, Mainframes MS Visio, Rational Rose, Requisite Pro. Hadoop, PL/SQL, etc."", u'Data Scientist\nEagle Trading Systems - Princeton, NJ\nNovember 2012 to January 2014\nDescription: Eagle Trading Systems Inc. is a financial investment advisory firm headquartered in Princeton, New Jersey. The firm manages 5 accounts totaling an estimated $481 Million of assets under management.\n\nResponsibilities:\n\u2022 Worked with Ajax API calls to communicate with Hadoop through Impala Connection and SQL to render the required data through it .These API calls are similar to Microsoft Cognitive API calls.\n\u2022 Good grip on Cloudera and HDP ecosystem components.\n\u2022 Used ElasticSearch (Big Data) to retrieve data into application as required.\n\u2022 Performed Map Reduce Programs those are running on the cluster.\n\u2022 Developed multiple MapReduce jobs in java for data cleaning and preprocessing.\n\u2022 Statistical Modelling with ML to bring Insights in Data under guidance of Principal Data Scientist\n\u2022 Data modeling with Pig, Hive, Impala.\n\u2022 Ingestion with Sqoop, Flume.\n\u2022 Used SVN to commit the Changes into the main EMM application trunk.\n\u2022 Understanding and implementation of text mining concepts, graph processing and semi structured and unstructured data processing.\n\u2022 Analyzed the partitioned and bucketed data and compute various metrics for reporting.\n\u2022 Involved in loading data from RDBMS and web logs into HDFS using Sqoop and Flume.\n\u2022 Worked on loading the data from MySQL to HBase where necessary using Sqoop.\n\u2022 Developed Hive queries for Analysis across different banners.\n\u2022 Extracted data from Twitter using Java and Twitter API. Parsed JSON formatted twitter data and uploaded to database.\n\u2022 Launching Amazon EC2 Cloud Instances using Amazon Images (Linux/ Ubuntu) and Configuring launched instances with respect to specific applications.\n\u2022 Exported the result set from Hive to MySQL using Sqoop after processing the data.\n\u2022 Analyzed the data by performing Hive queries and running Pig scripts to study customer behavior.\n\u2022 Have hands on experience working on Sequence files, AVRO, HAR file formats and compression.\n\u2022 Used Hive to partition and bucket data.\n\u2022 Experience in writing MapReduce programs with Java API to cleanse Structured and unstructured data.\n\u2022 Wrote Pig Scripts to perform ETL procedures on the data in HDFS.\n\u2022 Created HBase tables to store various data formats of data coming from different portfolios.\n\u2022 Worked on improving performance of existing Pig and Hive Queries.\n\nEnvironment: SQL/Server, Oracle 9i, MS-Office, Teradata, Informatica, ER Studio, XML, Business Objects, HDFS, Teradata 14.1, JSON, HADOOP (HDFS), MapReduce, PIG, Spark, R Studio, MAHOUT, JAVA, HIVE, AWS..', u'Data Architect/Data Modeler\nFirst Indian Corporation Private Limited - Hyderabad, Telangana\nFebruary 2011 to October 2012\nDescription: First Indian Corporation Private Limited, a member of The First American family of companies, provides specialized offshore transaction services, technology services and analytics to the mortgage industry.\n\nResponsibilities:\n\u2022 Worked with large amounts of structured and unstructured data.\n\u2022 Knowledge in Machine Learning concepts (Generalized Linear models, Regularization, Random Forest, Time Series models, etc.)\n\u2022 Worked in Business Intelligence tools and visualization tools such as Business Objects, Tableau, ChartIO, etc.\n\u2022 Deployed GUI pages by using JSP, JSTL, HTML, DHTML, XHTML, CSS, JavaScript, AJAX.\n\u2022 Configured the project on WebSphere 6.1 application servers\n\u2022 Implemented the online application by using Core Java, Jdbc, JSP, Servlets and EJB 1.1, Web Services, SOAP, WSDL.\n\u2022 Handled end-to-end project from data discovery to model deployment.\n\u2022 Monitoring the automated loading processes.\n\u2022 Communicated with other Health Care info by using Web Services with the help of SOAP, WSDL JAX-RPC.\n\u2022 Used Singleton, factory design pattern, DAO Design Patterns based on the application requirements\n\u2022 Used SAX and DOM parsers to parse the raw XML documents\n\u2022 Used RAD as Development IDE for web applications.\n\u2022 Preparing and executing Unit test cases\n\u2022 Used Log4J logging framework to write Log messages with various levels.\n\u2022 Involved in fixing bugs and minor enhancements for the front-end modules.\n\u2022 Implemented Microsoft Visio and Rational Rose for designing the Use Case Diagrams, Class model, Sequence diagrams, and Activity diagrams for SDLC process of the application\n\u2022 Doing functional and technical reviews\n\u2022 Maintenance in the testing team for System testing/Integration/UAT.\n\u2022 Guaranteeing quality in the deliverables.\n\u2022 Conducted Design reviews and Technical reviews with other project stakeholders.\n\u2022 Was a part of the complete life cycle of the project from the requirements to the production support\nEnvironment: R 3.0, Erwin 9.5, Tableau 8.0, MDM, QlikView, MLLib, PL/SQL, HDFS, Teradata 14.1, JSON, HADOOP (HDFS), MapReduce, PIG, Spark, R Studio, MAHOUT, JAVA, HIVE, AWS.', u'Data Analyst/Data Modeler\nAccenture - Bengaluru, Karnataka\nAugust 2009 to January 2011\nDescription: Accenture PLC is a global management consulting and professional services company that provides strategy, consulting, digital, technology and operations.\n\nResponsibilities:\n\u2022 Developed Internet traffic scoring platform for ad networks, advertisers and publishers (rule engine, site scoring, keyword scoring, lift measurement, linkage analysis).\n\u2022 Responsible for defining the key identifiers for each mapping/interface.\n\u2022 Clients include eBay, Click Forensics, Cars.com, Turn.com, Microsoft, and Looksmart.\n\u2022 Designed the architecture for one of the first analytics 3.0. online platforms: all-purpose scoring, with on-demand, SaaS, API services. Currently under implementation.\n\u2022 Web crawling and text mining techniques to score referral domains, generate keyword taxonomies, and assess commercial value of bid keywords.\n\u2022 Developed new hybrid statistical and data mining technique known as hidden decision trees and hidden forests.\n\u2022 Reverse engineering of keyword pricing algorithms in the context of pay-per-click arbitrage.\n\u2022 Implementation of Metadata Repository, Maintaining Data Quality, Data Cleanup procedures, Transformations, Data Standards, Data Governance program, Scripts, Stored Procedures, triggers and execution of test plans\n\u2022 Performed data quality in Talend Open Studio.\n\u2022 Coordinated meetings with vendors to define requirements and system interaction agreement documentation between client and vendor system.\n\u2022 Automated bidding for advertiser campaigns based either on keyword or category (run-of-site) bidding.\n\u2022 Creation of multimillion bid keyword lists using extensive web crawling. Identification of metrics to measure the quality of each list (yield or coverage, volume, and keyword average financial value).\n\u2022 Enterprise Metadata Library with any changes or updates.\n\u2022 Document data quality and traceability documents for each source interface.\n\u2022 Establish standards of procedures.\n\u2022 Generate weekly and monthly asset inventory reports.\n\nEnvironment: Erwin r7.0, SQL Server 2000/2005, Windows XP/NT/2000, Oracle 8i/9i, MS-DTS, UML, UAT, SQL Loader, OOD, OLTP, PL/SQL, MS Visio, Informatica.']","[u""Bachelor's""]",[u'']
0,https://resumes.indeed.com/resume/f195717772b8a97b,"[u'Data Analyst (Summer Intern)\nU.S. Department of Transportation - Washington, DC\nJune 2015 to August 2017\nWashington, DC Date: June 2015 - August 2017\n\u2022 Developed new and improved information pages for the 2015 & 2016 NHTSA CAF\xc9 (Corporate Average Fuel Economy) data base public report']",[u'B.S. in Computer Science & Mathematics'],"[u'Towson University Towson, MD\nJanuary 2018']"
0,https://resumes.indeed.com/resume/7a7a07059fcea965,"[u""Data Analyst Intern\nCollabera pvt ltd - Vadodara, Gujarat\nAugust 2016 to June 2017\n\u2022 Analyze data gathered and develop solutions or alternative methods of proceeding.\n\u2022 Provided insights about the most profitable market segments by performing statistical analysis on Collabera's\nmarketing data.\n\u2022 Forecasted the maintenance project activities to help track and ensure effective resource utilization, task\nscheduling.\n\u2022 Streamlined the reporting process through tuning SQL and automated the daily reporting process."", u'Data Analyst Intern\nCollabera - Vadodara, Gujarat\nAugust 2016 to June 2017']","[u'MS in Information Technology', u'Master of Information Technology in Information Technology', u'A. D.']","[u'Sacred Heart University Fairfield, CT\nSeptember 2017 to December 2018', u'Sacred Heart University\nAugust 2017 to December 2018', u'Patel Institute of Science and Technology\nJune 2012 to June 2016']"
0,https://resumes.indeed.com/resume/82e33303bff911b6,"[u'Data Analyst, Client Data Services Department\nJOOR Inc - New York, NY\nNovember 2014 to September 2017\n\u2022 Delivered all aspects of daily support for data operations, while maintaining database accuracy/integrity using PostgreSQL, front-end tools and Excel.\n\n\u2022 Created visually impactful dashboards in Tableau and Excel for GMV data reporting.\n\n\u2022 Extracted, interpreted and analyzed large amounts of data to identify key metrics and transform raw data into meaningful, actionable information.\n\n\u2022 Partnered with CSM, RSM teams and technical leads to help translate business requirements into IT application functional specifications and business processes.', u'Associate Data Analyst, Intern\nMidea Group - Wuxi\nMay 2013 to August 2013\n\u2022 Participated in the research and collection of complex data from monthly terminal retail reports for the Laundry Appliances Business Division, while compiling all gathered data in multiple formats via Excel.\n\n\u2022 Partnered in the design/execution of statistical analysis via SAS, ANOVA, correlation analysis and multiple regression models to identify/analyze impact of indicators that affect total sales for several chain stores.\n\n\u2022 Generated reports with compiled tables, charts and interpreted data; presented to senior manager.']","[u'Master of Science in Applied Statistics', u'Bachelor of Science in Ecology']","[u'Syracuse University New York, NY\nMay 2014', u'University of Science and Technology Beijing (USTB) Beijing, CN\nJune 2011']"
0,https://resumes.indeed.com/resume/a29c3bfbfb7a4f58,"[u'Data Scientist\nCapital Group - CA\nFebruary 2017 to Present\nA private equity firm, Capital Group provides investment management services for long-term investors to deliver superior, long-term investment results and service.I am a Data scientist responsible to help the Digital Marketing and Sales team.\nResponsibilities\n\u2022 Developed an NLP model using NLTK library in Python to classify the content in the investment portfolio database with 75 percent accuracy.\n\u2022 Implemented Linear Regression Algorithm resulting to identify the most valuable customers.\n\u2022 Implement data mining and statistical machine learning solutions to various business problems.\n\u2022 Develop customer segmentation algorithm to score sales and lead to increase market share.\n\u2022 Implement and monitor the fraud detection models for the Nadia mobile Marketing and Sales.\n\u2022 Develop large scale machine learning models for real time fraud detection using big data.\n\u2022 Implement demand forecasting models which improved upon forecast accuracy successfully.\n\u2022 Manage and work on setting up the data platform for analytic through integrating data sources.\n\u2022 Create and present executive dashboards to show the trends and to operationalize churn model.\nTools and Techniques used:Linear Regression, Scikit Learn, Python, Numpy, Spark, Data Mining, FeatureExtraction, Hypothesis Testing, Normalization, PCA, SQL, NLP, TensorFlow, Reporting Results', u'Data Scientist\nExperian\nAugust 2015 to December 2016\nA fortune 500 company, Experian unlocks the power of data to create opportunities for consumers, businesses and society.I was a part of the Application Security team responsible to implement data mining techniques to help protect the customer data by identifying the fraudulent transactions.\nResponsibilities\n\u2022 Built the fraudulent forecasting classification model using Logistic Regression algorithm.\n\u2022 Provided technical leadership in a team that developed systems to analyze large scale data.\n\u2022 Used Python to analyze and identify the Security data lakes for the fraudulent transactions.\n\u2022 Used NLP to text mine the customer service data lake for analyze the intent.\n\u2022 Monitored and provided analytical insights and predictive modeling for mobile games: price\n\u2022 Modeling, LTV, churn prediction, experimental design.\n\u2022 Developed Regression algorithms using advanced data mining algorithms successfully.\nTools and Techniques used: Logistic Regression, NeuralNetwork, Python, NLP, Pandas, SQL, MatplotLib, Tableau, TensorFlow, Scikit Learn.', u'Data Scientist\nDisney\nJune 2014 to July 2015\nWalt Disney Imagineering is the unique, creative force behind Walt Disney Parks and Resorts that dreams up, designs and builds all Disney theme parks, resorts, attractions worldwide.I was a part of the data science team and my role was to leverage the history of park & resorts financial transaction data to build the insights for the efficient and secure transactions.\nResponsibilities\n\u2022 Data engineered and defined the scope, features for the data set.\n\u2022 Processed and analyzed large data sets to identify fraudulent transactions using trading patterns;\n\u2022 Build the Decision Tree classification model to track the fraudulent activity;\n\u2022 Developed clear and well-structured analytical plans and analyzed large data-sets.\n\u2022 Identified, evaluated, and documented potential data sources in support of project requirements.\n\u2022 Built recommender system to help users to choose rides based on their previous selections.\n\u2022 Predicted labels of articles using Neural Network and used machine learning algorithms.\n\u2022 Developed models and reports, shared observations and recommendations with senior Management, executives, and stakeholders for further consideration\nTools and Techniques used: Decision Tree, Neural Network Algorithms, Python, Numpy, Pandas, SQL, MatplotLib, Tableau, Regression Analysis.', u'Data Analyst\nExperian\nSeptember 2013 to May 2014\nA Fortune 500 company, Experian unlocks the power of data to create opportunities for consumers, businesses and society.My responsibilities here were to data mine the web analytics data to help the product and development teams reduce the customer churn rate in the sales transaction funnel.\nResponsibilities\n\u2022 Applied the Data Mining techniques using Pandas and Numpy.\n\u2022 Prototyped the Linear Regression model to track the user inactivity using Scikit-Learn;\n\u2022 Preprocessed complex financial/regulatory documents and conducted keyword analysis;\n\u2022 Managed data operations team and collaborated with data warehouse developers to meet business\n\u2022 Developed ETL pipelines in the production for reporting marketing / operations teams.\n\u2022 Actively expedited the incumbent Market Segmentation analytics by 30% over the previous\n\u2022 Model by optimizing the statistical model successfully.\n\u2022 Programmed and tested data ingestion from SQL, HDFS to an ETL workflow.\n\u2022 Created customized reports and revealed fraudulent patterns using SQL and Tableau.\nTools and Techniques used:Linear Regression, Python, Machine Learning, Numpy, Pandas, SQL, Spark, No SQL, MatplotLib, Tableau.', u'Data Analyst\nDKRIN, UK\nOctober 2010 to July 2012\nA private Data Consulting company that provides a variety of services including implementation, process improvement, and software customization solutions.Analyzed the social media datasets twitter to build the customer behavioral insights.\nResponsibilities\n\u2022 Built Behavior insights using Natural Language Processing(NLP) to find ideal customer intent, allowing clients to reach their ideal customers with an informed understanding of the demographic, market, and behavioral patterns;\n\u2022 Designed the technology architecture for the current state of business strategy;\n\u2022 Interpreted complex simulation data using statistical methods as per requirements.\n\u2022 Architected and implemented analytics and visualization components for data analysis.\n\u2022 Developed cash-flow analytics for raw transaction data and implemented targeted features\n\u2022 Performed text mining to understand the spending patterns of potential customers.\n\u2022 Analyzed and created dimensional data modeling to meet OLAP needs.\n\u2022 Extracted, transformed and loaded present and historical data in preparation for data mining.\n\u2022 Conducted data analysis and developed complex designs algorithm\nTools and techniques used: Data Mining, Machine Learning, Regression Analysis, Text Mining, NLP, Python, Numpy, Pandas, SQL, MatplotLib, Tableau, Visualization.', u'Data Analyst\nDataBot Systems - IN\nMay 2008 to October 2009\nDatabot system offers a data analytics technology platform to help the start-ups by providing insights to create data-driven personas.\nResponsibilities\n\u2022 Extracted, transformed and loaded present and historical data in preparation for data mining.\n\u2022 Conducted data analysis and developed complex designs algorithm\n\u2022 Performed Text mining to better understand the buying patterns of potential customers;\n\u2022 Gathered, learning present and historical data in preparation for data mining;\nTools and Techniques used: Data Analysis, Data Mining, Python, Numpy, Pandas, SQL, ETL, Visualization Tools.']","[u'Masters in Computer Science', u'Masters in Embedded Systems', u'Bachelors in Computer Science']","[u'Texas A & M University\nJanuary 2013', u'University of South Wales\nJanuary 2010', u'JNTU\nJanuary 2008']"
0,https://resumes.indeed.com/resume/95fb4574082413ed,"[u'Data Analyst\nMetLife - Bridgewater, NJ\nOctober 2017 to Present', u'Independent Consultant\nJune 2014 to Present\nVisualizations/Reporting\n\u2022 Developed shiny apps and tableau dashboards with histogram, scatter plots with smoothers, q-q plots, radar charts and more.\n\u2022 Produced drill down maps dashboard for different stakeholders that had different macro/micro interests\nData Science\n\u2022 Experience with univariate analysis, classification & regression modeling (Random Forrest, Linear / Gamma / Logistic Regression, Gradient Boosting, Extreme-Gradient Boosting, Neural Networks) as well as Natural Language Processing (tokenization, stemming, sentiment analysis)\nExtract, Transform, & Load\n\u2022 Developed complex SQL statements through exploring table schemas, searching for appropriate tables & columns for joins and unions to make tables and views\n\u2022 Web scraping structured and unstructured data into usable data\nLeadership / Passion / Teamwork\n\u2022 As a highly motivated technology evangelist taught data analytics, programming languages, tools, and data collection techniques that would assist in certain situations to team through process work flow charts, articles, and programming libraries\nProgramming Languages\n\u2022 R | Python | VBA\nDatabase Experience\n\u2022 MS SQL | DB2 | Oracle | Access\nBusiness Intelligence Tools\n\u2022 Tableau | Alteryx | SQL Server Management Studio', u'Data Analyst\nComcast - Englewood, CO\nJanuary 2017 to March 2017', u'Data Analyst\nCVS Health - Denver, CO\nMarch 2016 to November 2016', u'Staff Accountant\nCommVault Systems, Inc - Oceanport, NJ\nNovember 2011 to November 2013']",[u'BS in Accounting'],[u'Manhattan College\nJanuary 2010']
0,https://resumes.indeed.com/resume/85069c0929487365,"[u'Learning Assistant\nMiami, FL\nJanuary 2018 to Present\nInstructed introductory undergraduate physics labs for the Florida International University physica department.', u'Data Analyst\nMiami, FL\nJune 2017 to Present\nAnalyzed data for the Jin He research group of the Florida International University physics department.']","[u""Bachelor's""]",[u'']
0,https://resumes.indeed.com/resume/4969a9c4b1ede288,"[u""Data Analyst\nBest Buy (contract) - Minneapolis, MN\nWells Fargo Home Mortgage, Minneapolis, MN (Contractor)\nData Analyst/Learning and Development Consultant\n\nSupported business unit as data analyst and consultant for budget and staffing concerns within Wells Fargo Home Mortgage's Training and Development area.\n\nDevelop and maintain budget and expense forecasting model and reports.\nAnalyze internal training and company data to benchmark efficiency and productivity ratios for use in reporting to company senior management and to facilitate department decision making.\nBuild process to analyze department data to measure ROI of internal training initiatives.\n\nMetLife Insurance, Eden Prairie, MN\nBusiness Procedures Consultant\n\nSupported business unit as internal consultant for quality improvement projects, operational report analyst, and project manager for quarterly corporate scorecard.\n\nManaged corporate scorecard delivery, allowing quarterly performance evaluation by client and supporting internal business decisions.\nDelivered expertise in statistical analysis, enabling management to make strategic business decisions.\nLed project team for Six Sigma quality improvement projects, diagnosing operational inefficiencies and strengthening performance measures.\nGenerated ad-hoc reports for internal staff and external clients that allowed recipients to more efficiently manage workloads and support operational measures.\n\nQuestar Data Systems, Eagan, MN\nProject Manager/Research Analyst\n\nActed in a dual role as project manager and research analyst for several accounts, within both the Employee Satisfaction and Customer Satisfaction divisions.\n\nConsulted with clients to assess needs, goals, and current issues ensuring successful project completion.\nProduced ad-hoc reports for senior research analysts that enabled timely delivery of client reports.\nManipulated data files in a variety of formats (SPSS, Excel, Access) for quality assurance purposes, guaranteeing validity and accuracy of data prior to delivery to customer\n\nHumana/Employers Health Insurance, DePere, WI\nQuality Assurance Analyst\n\nDiagnosed operational inefficiencies internally, and analyzed customer satisfaction levels.\nConsulted with internal departments on operational strategies using surveys and/or statistical analyses, improving departmental operations.\nLed team aimed at revising and delivering corporate-wide employee satisfaction survey to 10,000 employees to improve employee retention.\nAdministered and analyzed external customer satisfaction survey, reporting results to management for potential procedural changes.""]",[u'M.S. in Organizational Development'],"[u'University of Wisconsin Oshkosh, WI']"
0,https://resumes.indeed.com/resume/896807b2063e5dca,"[u'Consultant\nRedwood City\nOctober 2017 to Present\n\u2022 Automated a weekly refresh of 3 critical reports that improved\nbusiness response time by 5 days.\n\u2022 Wri ng & Presenta on Great attention to detail when preparing\npresentations and writing.\nConsultant\nPita Hub\nr 10/2017 - Ongoing + San Bruno / Redwood City\nI UX Design\nRan Operations and Marketing for family owned business. Extensive experience designing reports\n\u2022 Built advanced Excel tables for financial analysis and bookkeeping. for customers.', u'Data Analyst\nTableau - Brisbane QLD\nAugust 2015 to August 2017\nAustralia Google Analytics JIRA Agile\nLarge Australian Bank\n\u2022 Iteration Manager for a 15-person team that completed both database Adobe Creative MS Office\nsupport projects and ad-hoc requests.\n\u2022 Migrated sensitive customer data into new servers and decommissioned legacy databases.\n\u2022 Provided weekly reporting using SQL, Excel, and Tableau Visualizations\nas well as Interactive Dashboards.']","[u'', u'in Media Studies Website Redesign', u'B.S. in Supply Chain in Management']","[u'Miami University\nAugust 2011 to May 2015', u'Miami University Oxford\nFebruary 2015 to April 2015', u'University website']"
0,https://resumes.indeed.com/resume/8c24062c4b8899d5,"[u'Data Analyst\nCross-byte Computers - Las Vegas, NV\nFebruary 2018 to March 2018\nCross-byte Computer Services\n\u2022 Data analysis\n\u2022 Probability statistics\n\nJimmy Johns\n\u2022 Customer Service\n\u2022 Quality control\n\u2022 Problem solving\n\u2022 Organized supplies\n\u2022 Food preparation\n\u2022 Store maintenance\nWord of Life Christian Academy, Las Vegas, NV\nOffice Assistant- Current Duties included:\n\u2022 Project management\n\u2022 Customer Service\n\u2022 Class co-management\n\u2022 Handling mail\n\u2022 Copying papers\n\u2022 Orienting documents\n\u2022 Dispersing objects/papers/etc. to designated receiver\n\u2022 Stapling packets\n\u2022 Answering phones\n\u2022 Filing\n\u2022 Other remedial tasks']",[u'High school diploma'],"[u'Word of Life Christian Academy Las Vegas, NV\nJuly 2017']"
0,https://resumes.indeed.com/resume/d2e8b878c3215eba,"[u'Sales Insights Analyst\nGOOGLE - Mountainview, CA, US\nOctober 2017 to Present\nDescription: GMS is a marketing team of google across the world. There are several programs in sales teams based on geo hierarchy, market etc. The project is designed to create a business intelligence tool to prove the marketing teams with easy to access tools to monitor their performance and aid in decision making. These insights are very critical and used by teams all over the world.\n\nResponsibilities:\n\u2022 Acted as a Business Analyst and gathered the requirements from the business and documented.\n\u2022 Prepared the Technical Specification document based on the requirement.\n\u2022 Pull out data from database and prepare customized analysis datasets for specific reporting needs.\n\u2022 Redesigned SQL queries to improve operational performance and meeting customer requirements.\n\u2022 Designed queries, macros for data analysis including numeric and text using regular expressions on millions of rows.\n\u2022 Resolved several issues in the data pipelines, ensured data integrity, data accuracy across databases and different departments.\n\u2022 Designed new dashboards and added new features to the existing dashboards to deliver better insights.\n\u2022 Designed queries, macros for data analysis including numeric and text using regular expressions to run efficiently on millions of rows.\n\nEnvironment: SQL, Tableau, Github, Cider, Critique, Discovery, Plx, Dremel SQL, Buganizer', u""Data Analyst\nKaiser Permanente - Pleasanton, CA\nJanuary 2017 to June 2017\nDescription: Worked as a Data Analyst in business intelligence development project for Kaiser Permanente. Project includes analysis of complex business requirements; create data models, creating complex stored procedures, data visualizations to use in their report.\n\nResponsibilities:\n\u2022 Collect, extract, compile and analyze data in support of assigned projects and resolve discrepancies\n\u2022 Provide analysis of extracted and compiled data to support assigned projects as applicable\n\u2022 Coordinated with stakeholders to gather user requirements using elicitation techniques.\n\u2022 Gather requirements to map processes using Visio and to create metrics.\n\u2022 Create and update metrics on monthly basis for Manager's dashboard report.\n\u2022 Developed new business intelligence product helps management and doctors in aiding decisions.\n\u2022 Data cleaning, Data Visualization, Detection of missing data and skewness, correlation analysis, principal component analysis and model development were performed on the data.\n\u2022 Optimized SQL queries on SQL server and Windows server to run efficiently on more than a billion rows.\n\u2022 Design reports and presentations as needed for Manager, SMEs and Management briefing using excel functions (pivot table, v-look up, if statements) for data driven results.\n\u2022 Develop and initiate more efficient data collection procedures.\n\u2022 Redesigned Tableau workbooks for better visibility, updated metrics and improved performance.\n\u2022 Utilize Microsoft Excel and access to manipulate, cleanse, and process large data sets.\n\u2022 Ensured data accuracy through the creation and implementation of data integrity queries.\n\u2022 Perform trend analysis, prepare Pre-Organizational Inspection Program reports and presentations by maneuvering raw data using Excel and PowerPoint.\n\u2022 Assist team members to accomplish tasks and projects in a timely manner.\n\nEnvironment: SQL, SSIS, MS office, MS Excel, MS Visio, Tableau."", u'Data Analyst\nSatellite Healthcare - San Jose, CA\nAugust 2015 to December 2016\nDescription: Worked as a Data Analyst in data analysis and reporting solutions project. Project includes analysis of complex business requirements; create database, customized reporting solutions, data visualizations to use in their analysis.\n\nResponsibilities:\n\u2022 Acted as a Business Analyst and gathered the requirements from the business and documented.\n\u2022 Prepared the Technical Specification document based on the requirement.\n\u2022 Pull out data from SQL database and prepare customized analysis datasets for specific reporting needs.\n\u2022 Created reports based on stake holder requirements and pulled for various procedures/surgeries and created a bundle for each procedure.\n\u2022 Collect, extract, compile and analyze data in support of assigned projects and resolve discrepancies.\n\u2022 Optimized existing code for efficiency and automation of SQL queries to improve reporting efficiency.\n\u2022 Responsible for the proper coding documentation and validation of programs/macros/procedures to produce the standardized display.\n\u2022 Provide analysis of extracted and compiled data to support assigned projects as applicable.\n\nEnvironment: SQL, MS office, MS Excel, MS Visio, Tableau.', u'Data Analyst\nUCO Bank - IN\nJanuary 2012 to September 2015\nDescription: UCO bank is one of the largest bank in India. It deals with both commercial and personal customers. My project deals with developing reports, analyzing customer data sets, procedures for better insights and developing data visualizations for management.\n\nResponsibilities:\n\u2022 Interacted with cross-functional teams to facilitate gathering of business requirements for the reports.\n\u2022 Documented and updated procedures/policies for various processes\n\u2022 Created ad-hoc reports on need basis using Excel\n\u2022 Performed ad-hoc problem solving and analysis to resolve data discrepancies\n\u2022 Extensively used excel and excel functions (V-lookup, Pivot tables) to create highly complicated reports that would suffice the needs of higher management and Project Managers\n\u2022 Retrieved data using SQL queries as needed to create reports for internal and external clients\n\u2022 Assisted in creating dashboard reports required to effectively manage large portfolio of projects\n\u2022 Efficiently generated weekly and monthly report distributions\n\u2022 Effectively prepared and presented data to senior management\n\u2022 Worked in multiple and changing priorities with different teams\n\nEnvironment: SQL, MS Excel, Tableau, MS access']","[u""Master's""]",[u'']
0,https://resumes.indeed.com/resume/59d15b58ccca5a64,"[u'Data Analyst\nThomas Cook\nDecember 2014 to September 2015\n\u2022Analyzed the customer buying patterns with the intent of designing a service strategy by applying the clustering algorithm and predictive analytics using R.\n\u2022Performed sentiment analysis on customer data using quanteda () package in R to study customer behavior and their purchasing habit to offer them a better pricing strategy.\n\u2022Developed project plan highlighting the detail description of requirement for change implementation.\n\u2022Developed Requirement Traceability Matrix to track requirement changes and perform Impact Analysis.\n\u2022Participated in testing of procedures and data, utilizing PL/SQL, to ensure integrity and quality of data in data warehouse', u'Data Analyst\nChild Rights and You (NGO) - IN\nSeptember 2013 to January 2014\n\u2022Provided IT support to both internal and external clients using Agile development methodologies.\n\u2022Worked with cross-functional teams to build and deploy automated workflow (ETL) solutions and integrate data with a wide range of systems.\n\u2022Executed the daily, weekly, or one-off programs requiring SSIS packages to load data from XML files to T-SQL tables using stored procedure.\n\u2022Fashioned 15 NGO email marketing campaigns for a 75% increase in click through rate', u'Data Analyst\nSAS-MA - Mumbai, Maharashtra\nMay 2010 to May 2013\n\u2022Organized the project into phases to impose management control using stage-gate reviews such as high-level user stories, test scenarios, dimension modelling, data mapping and ETL.\n\u2022Optimized SQL queries for heavy traffic databases using SQL Profiling tools.\n\u2022Designed an interactive dashboard in Tableau and built complex calculated field as per business requirements which helped in formulating a budget plan resulting 21% annual cost saving.\n\u2022Collaborated with the QA team for reviewing the various mutual fund policy and approving it before the production release.\n\u2022Designed user stories for web-based interaction portal within an Agile methodology (SCRUM).']","[u""Master's in Technology Management""]",[u'University of Bridgeport\nAugust 2016 to May 2018']
0,https://resumes.indeed.com/resume/979ea22d9a8c1cff,"[u'Senior Data Analyst\nRAKUTEN AIP Corporation\nAugust 2014 to February 2018\nDuties:\nCarry out analysis of the given data and draw correct inferences, in keeping the objectives of the analysis\nResponsible for loading, extracting and validation of client data.\nManipulating, cleansing & processing data using Excel, SPSS, Wincross and SQL.\nWorking on Tabulations, Charts.\nHandling special custom requests of data send by client.\nMention ring Team and taking care of the dashboard, managing tasks within the Team.', u'Data Analyst\nToluna India Pvt Ltd\nJanuary 2012 to July 2014\nDuties:\nResponsible for loading, extracting and validation of client data.\nWriting scripts to manipulate data for data loads and extracts.\nWorking on segmentation algorithms, max diff and complex quota.\nData checking of high complexity surveys in Ad-Hoc, Trackers, multi country\nSurveys etc.\nCommunicating directly with the client to understand the exact requirement etc.']","[u'M.C.A in technology', u'B.Sc']","[u'Uttrakhand Technical University Dehra Dun, Uttarakhand', u'Kumaon University Uttrakhand']"
0,https://resumes.indeed.com/resume/33e5cd6eb4a5f711,"[u'Analytics Intern\nAmerican Heart Association - Dallas, TX\nAugust 2017 to December 2017\n\u2022Predicted if the participants will donate in the AHA event or not and recommended an approach to turn non-donors into donors using R, SQL and Tableau\n\u2022Used Segmentation Analysis and Logistic Regression Analysis to study the behavior of the participants in the events conducted\n\u2022Recommended the financial implication and findings to AHA executives', u'DATA ANALYST\nTATA CONSULTANCY SERVICES\nOctober 2012 to July 2016\nClient - Bank of America\n\u2022 Designed and prepared interactive and intuitive year end dashboards and reports using Tableau to show the test case performance by the engine this year\n\u2022 Developed ""Automated Engine"" using R-programing to identify discrepancies in the production and development codes using quality, cost, human resources and Technical debt as metrics (KPI), reduce the production cost by 30%\n\u2022 Analyzed data before and after changes in the operating server environment, to critically determine weaknesses, including errors in processes; and make recommendations to improve performance.\n\u2022 Used SQL queries for data-analysis and data extraction from database landscape and comprehend the data to perform sensitivity analysis\n\u2022 Performed Data profiling to examine the pattern of data coming from existing source systems for collecting the statistics and information about the data\n\u2022 Promoted to team leader, supervised and cross-trained 11 members and made them ready for projects in pipeline', u'DATA ANALYST INTERN\nALTIMA SYSTEMS PRIVATE LIMITED\nJune 2011 to December 2011\n\u2022 Maintained inventory transactions in oracle and performed analysis to minimize payments defaults and reduced losses by 6%\n\u2022 Performed regression Analysis to see the factors causing payment defaults using Microsoft Excel\n\u2022 Created interactive dashboard and reports to analyze daily market sales and compared them with target sales\n\u2022 Assisted database team in revamping the database from scratch and created ETL workflows on the existed data']","[u'M.S. Business in Analytics', u'B.Tech. in Electronics Engineering']","[u'University of Texas at Dallas Dallas, TX\nAugust 2016 to December 2017', u'Maharshi Dayanand University\nAugust 2008 to May 2012']"
0,https://resumes.indeed.com/resume/463715044e24a766,"[u'Data Analyst\nCisco - Raleigh-Durham, NC\nNovember 2016 to Present\nAudit customer inventory through reporting and linking data sources.', u'Data Analyst\nBay Area Techworkers - Wisconsin\nOctober 2013 to November 2016']",[u'Associate'],[u'UW Fox Valley']
0,https://resumes.indeed.com/resume/bd3828a6ec8d5371,"[u""Data Analyst\nWellington Management Company - Boston, MA\nJanuary 2017 to Present\n\u2022 Coordinate with internal/external teams to build macros surveying dynamic portfolio data\n\u2022 Maintain Fixed Income data (ex. sovereign flags, ratings, factors, currencies, debt seasoning)\n\u2022 Ensure firm-wide benchmark data responds to market shifts and security changes\n\u2022 Develop and maintain SQL scripts for troubleshooting workflows and pinpointing weaknesses\n\u2022 Collaborate with teams across firm to consolidate and simplify workflows while reducing risk\n\u2022 Communicate with investors to understand each other's objectives and strategic vision"", u'Economic Research - Worcester, MA\nJune 2013 to Present\nExtensive writing and statistical work, culminating in peer-reviewed publication:', u'Investment Analyst\nBallentine Partners, LLC - Waltham, MA\nOctober 2015 to October 2016\n\u2022 Oversaw portfolio rebalancing for $1.4B in assets - executing trades upwards of $100M\n\u2022 Writing background utilized for firm-wide memos and sensitive client communications\n\u2022 Leveraged Computer Science background to improve portfolio management through VBA macros\nand Excel dashboards - enabled firm to better visualize granular security data for attribution analysis\n\u2022 Earned Series 65 license to become a Registered Investment Advisor', u""Economic Research\nJanuary 2016 to January 2016\n539-562.\n\u2022 Wrote SQL scripts and employed advanced Excel functions to manage wide-ranging databases\n\u2022 Curated disparate information into world's first comprehensive dataset on low-level civilian violence"", u'GE Capital HQ - Norwalk, CT\nJune 2014 to August 2014\n\u2022 Coordinated with IT and team leaders to automate certain FP&A procedures for quarterly reporting\n\u2022 Successfully pitched validation platform to corporate officers, leading to its adoption by other teams']",[u'B.A. in Economics and Intl. Studies'],"[u'College of the Holy Cross Worcester, MA\nAugust 2011 to May 2015']"
0,https://resumes.indeed.com/resume/be80fe2fcac53def,"[u'Account Manager\nConvergent Outsourcing Incorporated - Houston, TX\nJanuary 2017 to Present\nResponsibilities:\n\n\u2022 Providing predictive assessment based on data modelling.\n\u2022 Analyzing current trajectories and suggesting future measures to stay on course.\n\u2022 Providing quality and compliance support.', u'Data Analyst\nHabib Bank AG Zurich - Dubai, AE\nJanuary 2012 to January 2016\nResponsibilities:\n\n\u2022 Searching for transaction anomalies in transaction data.\n\u2022 Verifying validity of transactions with anomalies.\n\u2022 Ensuring compliance to local and international laws.', u'Medical Data Analyst\nOutsource R US - Karachi, PK\nJanuary 2007 to January 2011\nResponsibilities:\n\n\u2022 Bio-statistical Data Analysis.\n\u2022 Electronic health record data processing.\n\u2022 Medical Transcription\n\u2022 Report creation']","[u'MS in Bioinformatics', u'BS in Biology']","[u'Johns Hopkins University\nJanuary 2017', u'University of Karachi\nJanuary 2001 to January 2006']"
0,https://resumes.indeed.com/resume/22164db273ec3277,"[u'Lead Data Analyst\nErnst & Young - San Francisco, CA\nFebruary 2017 to Present\n\u2022 Led effort in analyzing revenue insights for a top US credit card company. Analyzed large scale transaction data across multiple\nsources, wrote complex SQL and Python scripts to perform data cleaning and ensure data quality before analysis. Discovered\nabnormal transaction patterns and identified potential reasons. Created report dashboard to present result to leaderships\n\u2022 Led effort in analyzing sales growth insights for a top US retailer. Managed and worked with engineering, sales and marketing teams,\nfetched data from multiple data sources to discover important sales drivers and helped senior leaderships and non-technical\nstakeholders to better understand, invest and scale their business\n\u2022 Led effort in analyzing user engagements of a product offered by a top 5 subscription-based firm. Defined performance metrics and feature specs to analyze user engagements data from multiple A/B testing results. Captured key insights from user behavior\ndata from different experiments and give suggestions to improve user experience. Increased the product usage by 7%\n\u2022 Led effort in developing a pricing model for a financial product offered by a top 5 US bank, predicted forward looking trends using\ntime series data, forecasted performance and identified key insights across multiple data sources using Python and SQL\n\u2022 Led effort in developing and enhancing a payment model for leading credit card firm using SAS and SQL. Coordinated with different teams to build hypothesis and created metrics to track ongoing performance and virtualized the result in Tableau', u'Senior Quantitative Analyst\nAccenture - New York, NY\nMay 2015 to February 2017\n\u2022 Led effort in designing and developing high level enterprise data management workflow and data quality matrix for a top US bank\n\u2022 Led effort in performing strategic cost reduction, wealth management, benchmarking analysis, data management reorganization and presented directly to CFO of a top 10 global insurance firm and drove sector revenue up to 5 percent\n\u2022 Examined all key business elements, identified data quality issues which may affect accuracy of business parameter estimates\n\u2022 Developed investment strategies including private equity funds, credit opportunities funds and co-investment vehicles for PE team\n\u2022 Performance Recognition Award from senior leadership in fiscal year 2016', u'Data Analyst\nAXA - New York, NY\nFebruary 2014 to May 2015\n\u2022 Setup and run data models to different business uses cases across multiple insurance entities. Worked with AXA Data Lab on key\nprojects to develop new use cases with users.\n\u2022 Performed forecasting analysis and rating methodology for financial plans, insurance products and investment portfolios\n\u2022 Developed investment strategy in the support of NA investment division and interfaced with global capital market\n\u2022 Synthesized economics info and evaluation of problem assets as critical inputs to parameters estimation process']","[u'M.S. in Industrial Engineering and Operations Research', u'B.A. in Mathematics']","[u'Columbia University', u'University at Buffalo']"
0,https://resumes.indeed.com/resume/0b0daa03ff6434e6,"[u'Business intelligence / Data Analyst\nADDICKSANDYORK - New York, NY\nApril 2017 to Present\n\u2022 Developed Power BI reports and dashboards from multiple data sources using data blending\n\u2022 Designed Power BI data visualization utilizing cross tabs, maps, scatter plots, pie, bar and density charts\n\u2022 Explored data in a variety of ways and across multiple visualizations using Power BI. Strategic expertise in design\nof experiments, data collection, analysis and visualization\n\u2022 Queried relational data using SQL to gain business insights for several projects', u'Business Analyst / Data Analyst\nJOADEM CORPORATION - New York, NY\nOctober 2016 to April 2017\nEvaluate Risks related to requirements implementation, testing processes, project communications and training\nsaving the company on average $5000 +\n\u2022 Identify business trends utilizing real data, compile analysis reports that are delivered to developers and then\nfollow-up on all results using Excel and MS Access\n\u2022 Carried out univariate and multivariate analyses using R, Rstudio to understand the underlining trend the dataset\n\u2022 Engage client to gather software requirements/business rules, and ensure alignment with development teams\n\u2022 Liaise between business and technical personnel to ensure a mutual understanding of processes and applications', u""Business Analyst\nPAUWELS INT'L N.V GIL RESOURCES\nJuly 2014 to August 2016\n\u2022 Draft and maintain business requirements and align them with functional and technical requirements\n\u2022 Facilitate monthly meetings with clients to document requirements and explore potential solutions\n\u2022 Communicate client's business requirements by constructing easy-to- understand data and process models\n\u2022 Develop comprehensive E2E test cases at the application and multi-application levels\n\u2022 Tracked and maintained project cycle using JIRA software\n\u2022 Created interactive dashboards for data visualization and reporting using Tableau and R""]","[u'Master of Business Administration in information technology', u'Bachelor of Technology in Management']","[u'Monroe College New Rochelle, NY\nApril 2018', u'Federal University of Technology\nFebruary 2014']"
0,https://resumes.indeed.com/resume/eee91302f1d9be36,"[u'Data Scientist\nCapital One - Richmond, VA\nMarch 2017 to Present\nResponsibilities:\n\u2022 As a member of the Analytics Team, support advanced analytics projects to meet the needs of business partners.\n\u2022 Wrote a python-based SQL generator that helped speed up a weekly reporting from several days effort to just running the automation and getting the report done in a few hours.\n\u2022 Investigate existing and emerging technologies to explore analytics solutions for business partners, and make recommendations for enterprise-wide implementation.\n\u2022 Maintained SQL scripts to create and populate tables in data warehouse for daily reporting across departments.\n\u2022 Worked on a data set that predicts the length of stay in dentist office based on claim records\n\u2022 Created, coded, tested, modified and installed programs for a host of application\n\u2022 Use SQL, cluster analysis, and text analytics methodologies to investigate account data, including client interaction, to anticipate client dissatisfaction and determine statistical patterns that could be precursors closing or diminishing accounts, and to discover a relationship between client interaction and fraud.\n\u2022 Perform data extraction, sampling, advance data mining and statistical analysis using linear and logistic regression, time series analysis and multivariate analysis within R and Python.\n\u2022 Discover and visualize patterns in Tableau.\n\u2022 Examine customer feedback and activity for use in detecting or confirming fraud, using a combination of text analytics, statistical modeling, and classification.\n\u2022 Used SQL language to write queries inside the SQL server database.\n\u2022 Developed predictive models using Decision Tree, Random Forest and Na\xefve Bayes.', u""Data Analyst\nWellPoint - Newbury Park, CA\nAugust 2016 to March 2017\nResponsibilities:\n\u2022 Worked on an application that took care of online account services which included Bill paying, checking the bill history, changing customer profile, ordering auto insurance ID cards Worked on Commercial lines Property and Casualty Insurance including both policy and claim processing and reinsurance.\n\u2022 Provide customer service and support for Property & Casualty risk management needs\n\u2022 Query optimization using SQL Profiler and performance monitors to enhance the performance of database servers.\n\u2022 Involved in creating automated Test Scripts representing various Transactions, Documenting the Load Testing Process and Methodology. Created meaningful reports for analysis and integrated the Performance Testing in the SDLC.\n\u2022 Converted various SQL statements into stored procedures thereby reducing the Number of database accesses (since Stored Procedures passes the whole block at one time.)\n\u2022 Used the DataStage Designer to develop processes for extracting, cleansing, transforms, integrating and loading data into data warehouse database.\n\u2022 Used DataStage Manager for importing metadata from repository, new job categories and creating new data elements.\n\u2022 Designed and developed Use Cases, Activity Diagrams, Sequence Diagrams, OOAD using UML and Business Process Modeling.\n\u2022 Created UML (Class, Sequence and Component Diagrams) diagrams using Visio.\n\u2022 Used the DataStage Director and its run-time engine to schedule running the solution, testing and debugging its components, and monitoring the resulting executable versions (on an ad hoc or scheduled basis).\n\u2022 Implemented local containers to simplify the job design.\n\u2022 Implemented shared containers to use in multiple jobs, which have same business logic.\n\u2022 Involved in writing Job Automation process i.e. writing shell scripts using Korn shell, as scheduled daily, weekly, monthly, and sending these jobs to client's email-ids.\n\n\u2022 Involved in DBA tasks such as table creation and maintenance, create indexes, optimize queries, and grant privileges.\n\u2022 Involved in creation of Database Tables, Global Temporary Tables, Cluster Tables, Partition Tables,\nAnd Index-by Tables using storage parameters.\n\u2022 Wrote many stored procedures, stored functions, Packages and used in many Forms and Reports.\n\u2022 Created indexes on tables and Optimizing Stored procedure queries.\n\u2022 Involved in interaction with the End Users and implemented reviews of the application."", u'Data Analyst\nXerox - Houston, TX\nDecember 2015 to August 2016\nResponsibilities:\n\u2022 Involved in defining the business/transformation rules applied for sales and service data.\n\u2022 Document, clarify, and communicate requests for change requests with the requestor and coordinate with the development and testing team.\n\u2022 Worked on metadata clean-up and processing at regular intervals for better quality of data.\n\u2022 Involved in data load/export utilities like Fast Load, Multi Load, Fast Export and UNIX/Mainframes environments.\n\u2022 Responsible to build and maintain well-managed data solutions and deliver capabilities to tackle business problems.\n\u2022 Partner with the business to provide consultancy and translate the business needs to design and develop tools, techniques, metrics, and dashboards for insights and data visualization.\n\u2022 Drive an understanding and adherence to the principles of data quality management including metadata, lineage, and business definitions.\n\u2022 Work collaboratively with appropriate Tech teams to manage security mechanisms and data access governance.\n\u2022 Build and execute tools to monitor and report on data quality.\n\u2022 Answering the technical queries, driving the product initiatives and metric collection and analysis.\n\u2022 Generate daily, weekly and monthly reports as per the specifications/requirements by business users.\n\u2022 Discuss business solutions with client business team, resolving existing problems and improving the report quality.\n\u2022 Performed Source to Target data analysis and data mapping.\n\u2022 Created SQL queries to validate Data transformation and ETL Loading.\n\u2022 Building, publishing customized interactive reports and dashboards, report scheduling using Tableau server.', u'Data Analyst\nStratagems Services Pvt Ltd\nJune 2014 to November 2015\nResponsibilities:\n\u2022 Created new reports based on requirements. Responsible in Generating Weekly ad-hoc Reports\n\u2022 Planned, coordinated, and monitored project levels of performance and activities to ensure project completion in time.\n\u2022 Automated and scheduled recurring reporting processes using UNIX shell scripting and Teradata utilities such as MLOAD, BTEQ and Fast Load\n\u2022 Experience with Perl\n\u2022 Worked in a Scrum Agile process & Writing Stories with two weeks iterations delivering product for each iteration\n\u2022 Worked on transferring the data files to vendor through sftp &Ftp process\n\u2022 Involved in defining and Constructing the customer to customer relationships based on Association to an account & customer\n\u2022 Created action filters, parameters and calculated sets for preparing dashboards and worksheets in Tableau.\n\u2022 Experience in performing Tableau administering by using tableau admin commands.\n\u2022 Worked with architects and, assisting in the development of current and target state enterprise level data architectures\n\u2022 Worked with project team representatives to ensure that logical and physical data models were developed in line with corporate standards and guidelines.\n\u2022 Involved in defining the source to target data mappings, business rules and data definitions.\n\u2022 Responsible for defining the key identifiers for each mapping/interface.\n\u2022 Performed data analysis and data profiling using complex SQL on various sources systems including Oracle and Teradata.\n\u2022 Migrated three critical reporting systems to Business Objects and Web Intelligence on a Teradata platform\n\u2022 Created Excel charts and pivot tables for the Adhoc data pull']",[u'Bachelor in Computer Engineering in Computer Engineering'],[u'Gujarat Technological University']
0,https://resumes.indeed.com/resume/f2f0fd40b790da81,"[u""Data Analyst\nHarlem Commonwealth Council - Harlem, NY\nJune 2017 to December 2017\nReviewed the minority/Women Business EnterpriseM/WBE, provided a comparative data analysis between public and private\nsector data visualizations and ad-hoc data reports on Tableau and Qlikview to extract actionable insights and recommendations\n\u2022 Define, measure and communicated data reports for Key performance indicators(KPI's) or score cards as directed\n\u2022 Mapped out data process flows, data flows, business and data requirements, documentation and reporting logic using tools like Microsoft Visio, UML, Excel\n\u2022 Developed use-cases, test-sscripts and documentation for User Acceptance Training(UAT) sessions for the M/WBE Cloud\nproject and forwarded QA analytics documentation to the internal teams and clients\n\u2022 Conducted meetings to gather functional and non-functional requirements from all the departments to create business models,\nbusiness process workflows and translate them into technical and operational specifications"", u'Business Analyst\nIdeaMagix - Mumbai, Maharashtra\nAugust 2015 to June 2016\nBuilt and refined ""What-if"" scenario data models using Microsoft Excel to understand the business and design requirements\nwith links to external relational database sources like SQL server and Microsoft Access to facilitate decision making\n\u2022 Conducted market analysis, assessment, survey, research and competitive analysis to make a business case for the project\n\u2022 Served as a liaison between business and technical personnel to ensure a mutual understanding of processes and applications\n\u2022 Developed financial models with statistical data modeling such as forecasting and regression analysis for procurement']","[u'', u'Bachelor of Engineering in Electronics and Telecommunication in Electronics and Telecommunication']","[u'Stevens Institute of Technology Hoboken, NJ\nMay 2018', u'Mumbai University, Don Bosco Institute of Technology Mumbai, Maharashtra\nMay 2015']"
0,https://resumes.indeed.com/resume/44c4443c70e3c917,"[u'Data Analyst\nLennox International - Dallas, TX\nAugust 2017 to February 2018\n\u2022 Used ML studio in Microsoft Azure and built machine learning models using Boosted decision tree Algorithm, and forecasted sales with an efficiency of 74.6%.\n\u2022 Created complex calculated fields, heat maps, and functions and utilized Tableau Server to publish and share reports.\n\u2022 Executed Ad-hoc data visualizations using ggplot2 in R to evaluate the existing models.\n\u2022 Performed in-depth statistical analysis and data mining methods using R, including Cluster analysis, Logistic Regression, and helped reducing variance by 35%.\n\u2022 Worked on writing complex SQL queries in performing Data analysis using window functions, joins, improving performance by creating partitioned tables.\n\u2022 Involved in Data preparation, exploratory analysis, Feature engineering using Supervised and unsupervised modeling.\n\u2022 Split the data in Azure, trained the neural network and improved the regression model efficiency.\n\u2022 Created Supply Chain Analysis Reports for transportation (OTD, Spend), SIOP score cards (Inventory & sales).', u'Business Data Analyst\nAccenture Services Pvt. Ltd - Bangalore, Karnataka\nJune 2011 to November 2015\n\u2022 Analyzed and integrated business opportunities with cross functional teams, leading to increase in sales by 7%.\n\u2022 Developed customer commissions reports and dashboards to have a better understanding of potential earnings and a better evaluation of focus areas. This helped achieve faster sales cycle by 17%.\n\u2022 Implemented Salesforce.com (SFDC) for a food distribution giant across 69 divisions to a user base of 6000 employees.\n\u2022 Responsible for collecting, cleansing and visualizing data and perform hypothesis testing and correlations.\n\u2022 Captured user behavior by performing Data Analysis and launched specific pricing & recommendations, thereby achieved customer retention by 23%.\n\u2022 Performed logistic regression and developed a churn strategy Model to get rid of churn problem in a cell company. Validation analysis was done to come up with insights. Success rate of churn prediction is 49%.\n\u2022 Lead six sigma value chain project to reduce cost (TCO) by 15% using logistics network optimization.\n\u2022 Worked on optimization models using Monte Carlo (static) simulation and helped making decision under un-certainty.\n\u2022 Developed complex SQL queries, Unions and Multiple table joins, Views and database programming using SQL.\n\u2022 Performed A/B testing and proposed new features for converting leads to prospects & Opportunities.\n\u2022 Created an internal release tracker application, to track at least 10 release level metrics in the delivery Unit.\n\u2022 Built a cloud (SaaS) based predictive intelligence (AI) focused IVR for an Oil & Gas giant, using Deep Neural networks and made the IVR system intuitive and conversational.\n\u2022 Hands on experience with Text Analytics; extracted N-grams and used LDA algorithm to classify words into categories.\n\u2022 Worked on Speech Analytics using Recurrent neural networks like LSTMS, encoder-decoder and increased the IVR efficiency by bringing context to conversation.\n\u2022 Increased team productivity by 20%, by taking training technical sessions on Social CRM in SFDC.\n\u2022 Executed a low cost, totally customizable CRM solution and optimized the savings by 10% ($5M) and allowed for flexible and reliable reporting.\n\u2022 Lead a 3 member team and successfully implemented partner portal (PRM).\n\u2022 Hands-on experience with both sales cloud & service cloud, worked on standard objects, custom fields and served as an SME on CRM process flow.\n\u2022 Using Apex Data loader, performed DML operations and exported records to CSV files.\n\u2022 Performed gap analysis, understood the As-Is system and developed new scenarios for the potential To-Be system.\n\u2022 Shadowed PM, assisted in defining project Scope and provided Level of efforts (LOE) during Agile sprint planning.\n\u2022 Responsible for analysis, design and writing user stories, technical requirements, use case diagrams, test conditions, scenarios, test plans.']","[u'M.S in INDUSTRIAL ENGINEERING', u'B.Tech in Electrical & Electronics Engineering']","[u'University of Houston Houston, TX\nJanuary 2016 to May 2017', u'Vellore Institute of Technology Vellore, Tamil Nadu\nJuly 2007 to June 2011']"
0,https://resumes.indeed.com/resume/b5fbff945ca1d02e,"[u'DATA ANALYST\nHopkins Logistics - Dallas, TX\nJanuary 2015 to March 2018\nResponsibilities:\n\u2022 Performed analysis and presented results using SQL, SSIS, MS Access, Excel, and Visual Basic scripts.\n\n\u2022 Wrote and automated tools and scripts to increase departmental efficiency and automate repeatable tasks.\n\n\u2022 Wrote complex SQL queries using complex joins, grouping, aggregation, nested subqueries, cursors, etc.\n\n\u2022 Manipulated files and their associated data for rapid delivery to clients or loading onto internal databases.\n\n\u2022 Performance Tuning in SQL Server using SQL Profiler and Data Loading.\n\n\u2022 Designed, coded, tested and debugged custom queries using Microsoft T-SQL and Crystal Reports per defined technical specifications.\n\n\u2022 Managed the extraction, transformation, and population of data files and databases (MS SQL Server 2008, Excel, and Access)\n\n\u2022 Worked directly with customers and team members to determine project scope and specifications. Provides research and analysis to support intelligence organizations.\n\n\u2022 Collaboratively developed and communicated project plans, work assignments, target dates and other aspects of assigned projects.\n\n\u2022 Ensured that projects follow prescribed development life cycle and met quality standards.\n\n\u2022 Reformatted the data and inserted the reformatted data into staging table, performed data cleansing and data type conversions and then loaded into destination table', u'Data Entry Specialist\nCollin Oak - Plano, TX\nJanuary 2011 to December 2013\nResponsibilities\n\u2022 Compiled, prioritized, and sorted, and processed customer orders into local database.\n\u2022 Adjusted settings for format, page layout, line spacing, and other style requirements.\n\u2022 Transmitted work electronically to other locations upon request.\n\u2022 Researched further information for incomplete documents.\n\u2022 Reviewed and checked applications and supporting documents\n\u2022 Code and process applications into required electronic format\n\u2022 Scanned documents into database\n\u2022 Audited on-line applications for accuracy and completeness\n\u2022 Loaded information onto prescribed databases\n\u2022 Maintained complete activity logs and prepare reports\n\u2022 Responded to requests for information and statistics\n\u2022 Retrieved and presented required information in various formats\n\u2022 Provided guidance and information on application requirements\n\u2022 Routed data to appropriate staff']","[u'in computer science', u'', u'in Bacheloors']","[u'Brookhaven college', u'Colaberry school of data analytics', u'University of Nairobi']"
0,https://resumes.indeed.com/resume/53fd56f728748985,"[u'Data Analyst Intern\nSiriusXM - New York, NY\nSeptember 2017 to Present\n\u2022 Conducted end-to-end analysis including data gathering, analysis, ongoing scaled deliverables and presentations\n\u2022 Focused on all 48 channels of SiriusXM including Music, Talk Show and Sports categories\n\u2022 Created and updated dummy attributes with Boolean values for listener\u2019s preference for music genres\n\u2022 Managed and Automated the process of Data cleansing, resulting in 20% reduction of erroneous data\n\u2022 Build Data Model to know the likelihood of customers to help Marketing team to focus on Mass marketing\n\u2022 Performed data-driven decisions for Marketing campaigns using JMP Pro for preference of customer\n\u2022 Ensured quality assurance (QA) using Teradata queries to identify data model performance\n\u2022 Generated visualizations using JMP Pro for the marketing team\n\u2022 Build a Data Model to suggest relevant Genre music to the new listeners (customers)\n\u2022 Used KNN Algorithm to predict the preference depending on the Geographical location\n\u2022 Targeted right people for marketing campaigns and increase engagement\n\u2022 Generated visuals using Heat Map for Promotions and Event Marketing team to focus on which location is more responsive according to Genres', u""Data Analyst Intern\nBoost Payment Solutions - New York, NY\nJune 2017 to August 2017\n\u2022 Automated daily and monthly reports. Effectively reduced time spent by other data analysts by 30%\n\u2022 Data Manipulation, analysis and reporting using MS Excel\n\u2022 Analyzed Account Payable/Account Receivable datasets to identify potential clients for the unique B2B payments solution developed by the firm\n\u2022 Implemented ad-hoc SQL based reports based to increase the vendor list for the Sales team\n\u2022 Updated spreadsheets with data from an internal database to track key performance indicators (KPI)\n\u2022 Managed the maintenance and development of performance reports to improve customers' satisfaction and company's growth\n\u2022 Enhanced the existing reconciliation operations by developing macros and utilizing advanced Excel functions\n\u2022 Implemented vlookup to automate the billing reports for payment merchants \u2013 VISA, MasterCard, and Amex"", u'Data Analyst Intern\nAlzerina Jewelry - New York, NY\nJanuary 2017 to March 2017\n\u2022 As an independent designer, worked to create and developed a marketing plan that improves brand identity and consumer recognition of Alzerina and Vipera\n\u2022 Performed Data Profiling in the source systems that are required for Data Marts store all transactional data related to each purchase and also to store historical data and help with forecasting of the sales\n\u2022 Involved in extensive Data Validation by writing several complex SQL queries and Involved in back-end testing and worked with data quality issues\n\u2022 Identified and solved the problems by coordinating with senior managers in emergency and normal issues to meet the goals\n\u2022 Written SQL scripts to test the mappings and Developed Traceability Matrix of Business\n\u2022 Developed website that has a consistent feel and looks throughout all web properties and maintaining front end functionality\n\u2022 Leveraged analytical tools like Tableau and Google Analytics to facilitate the management of proprietary database and document warehouse, which is utilized by all internal groups for customized reports such as client deliverables, sales pitches, market overviews, funds coming to market\n\u2022 Performed data-driven decisions for marketing campaigns using Google Analytics\n\u2022 Created Weekly Data Quality Reports which includes Data Extraction, Business Rules Generation, Compiling and Building Report and Trouble Shooting if any Issues using Tableau']","[u'Master of Science in Information Systems', u'Bachelor of Engineering in Computer Engineering']","[u'Pace University-New York New York, NY\nJanuary 2016 to December 2017', u'Sarvajanik College of Engineering and Technology Surat, Gujarat\nAugust 2011 to May 2015']"
0,https://resumes.indeed.com/resume/7e8a732c8ec61187,"[u'Data Analyst II\nENVOLVE DENTAL, INC - Tampa, FL\nDecember 2016 to February 2018\nProvide ad hoc reports and ongoing reports to end users and state entities regarding dental insurance activity.\n\n\u2022 Trained incoming personnel.\n\u2022 Automated 40% of monthly reports using Visual Studio.\n\u2022 Maintained Access database used to monitor various reporting duties by other personnel to ensure State reporting was accurate and timely.\n\u2022 Distributed work amongst various reporting analysts.', u'Reimbursement Analyst\nMadison, WI\nApril 2015 to August 2016\nAnalyze reimbursement scenarios for use in hospital contract negotiations, maintain hospital reimbursement per contracts, work closely with other functional areas, and document all internal processes.\n\n\u2022 Generated compensation and bonus reports for client agencies and internal departments such as Sales and Retention. Decreased report creation time by 20 man-hours per quarter.\n\u2022 Introduced a SharePoint database that interfaced with SAS and expanded access to web-based data by 80%.\n\u2022 Ensured State and Federal compliance through maintenance of Process and Procedure documentation.\n\u2022 Delivered a monthly reporting package that saved department 24 man-hours per month in processing time.\n\u2022 Produced a claims-inquiry tracking system on SharePoint, with associated reports, thus expanding ability of senior management to view metrics at their convenience.', u""Marketing Analyst\nHEALTHGRADES - Madison, WI\nJanuary 2009 to April 2015\nProvide ad hoc reports and Excel workbooks to multiple clients in healthcare industry, offering analysis of market trends and consumer behavior.\n\n\u2022 Co-managed national health-system account and decreased client's monthly reporting turnaround time by 50% through Excel automation.\n\u2022 Introduced analysis into the effectiveness of marketing strategies for General Surgery patients by comparing timing of patient utilization versus timeliness of marketing communication. Account managers adopted modified communication strategies based on findings of this research.\n\u2022 Empowered a call center client to accurately monitor KPIs for performance evaluations and review purposes via reporting that our team created. Decreased client's reliance on hand-written tracking by 90%.\n\u2022 Trained or mentored 30% of team members.""]",[u'Bachelor of Arts in Geological Sciences'],"[u'University of Wisconsin Milwaukee, WI\nJanuary 1986']"
0,https://resumes.indeed.com/resume/13a6a3beec8ccf12,"[u'Data Analyst\nConsultADD Inc - Texas\nJune 2017 to Present\n\u2022 Successfully migrated the reporting environment from SSRS to Power BI\n\u2022 Introduced multidimensional cubes technology at the client side and created, updated, deployed and processed complex OLAP cubes using SQL server Analysis services(SSAS) from distributed data sources with multiple fact measure groups and multiple dimension hierarchies, performs drill-through actions, Multidimensional MDX query, Partitioning, calculating members based on the OLAP Reporting needs which helped client to draw business decisions effectively and gained more revenue for the company.\n\u2022 Created informative data mapping spreadsheets for ETL team with source to target data mapping with physical naming standards, data types, volumetric, domain definitions, transformation Rules\n\u2022 Implemented effectual ETL solutions using SSIS packages, planned package development process, and design the control flow, data flow within the packages\n\u2022 Actively involved with business managers to produce technical requirement documents citing report design, performance standards, SQL Server object selections and hardware specification for new PowerBI report development.\n\u2022 Used SQL database as well as OLAP cubes as data sources for PowerBI to gained multi-perspective view with visualizations that represented different findings and insights from that dataset and visualized the data in Pie chart, Bar graph, drill-through, tabular and other visualizations']",[u'M.S in IT'],"[u'Loyola University Chicago Chicago, IL\nJune 2015 to June 2016']"
0,https://resumes.indeed.com/resume/e7c5aa7e02caab3a,"[u""Data Systems Coordinator\nEnvironmental Systems Design, Inc - Chicago, IL\nMarch 2015 to Present\nDeveloped automated system that continually updates and generates employee\nresumes to increase workforce efficiency and ease of proposal development\n\u2022 Integrated the company's accounting system with our CRM system to allow the tracking of employee hours for automating the resume building process\n\u2022 Brought financial data into CRM system for the first time to improve\nproject analysis capabilities\n\u2022 Supported lasting customer relationships by organizing biweekly mailing lists for customer satisfaction surveys and distributing results to senior management\n\u2022 Built a research project detailing information on company's competitors and new geographical markets used by senior management to expand the company's\nvision for 2021\n\u2022 Provided the historical data and building research needed to aid in the decision\nmaking of a business development team that grew their portfolio by eight\nChicago buildings within the last year\n\u2022 Utilized the tools provided by CRM system to establish processes and procedures that support ongoing use and optimization\n\u2022 Performed continual content entry to ensure the accuracy and integrity of information in CRM database\n\u2022 Conducted training sessions introducing our CRM system to new users\n\u2022 Maintained business continuity throughout the firm as a member of the ESD\nBusiness Continuity Board"", u'Data Research Analyst Intern\nKingland Systems - Ames, IA\nSeptember 2014 to March 2015\n\u2022 Worked in a team to research companies and funds\n\u2022 Processed data & information using Insight program']",[u'Bachelor of Science in Marketing'],"[u'College of Business - Iowa State University Ames, IA\nMay 2015']"
0,https://resumes.indeed.com/resume/23b4899bdf42ad3d,"[u'Consulting Data Scientist\nPROSPECT RESOURCES INC - Skokie, IL\nMay 2017 to August 2017\n\u2022 Analyzed existing predictive analytics tool for layered hedging in energy procurement market.\n\u2022 Analyzed and forecasted impact on returns because of delayed hedging client responses.\n\u2022 Implemented a buying strategy based on Moving Average (MACD) and Deep Learning.', u'Consultant\nDELOITTE CONSULTING - Bengaluru, Karnataka\nJuly 2013 to July 2016\n\u2022 Designed data warehouse to house the product/portfolio health benefits data and customer segmentation to report for Obama Care HIX, to help Anthem Insurance increase sales and determine pricing strategies.\n\u2022 Integrated drug sales information into Veeva CRM (salesforce.com) and created visualizations to help business users of\nBiogen Idec with decision making, and intuitive analysis of patients, products and health plans.\n\u2022 Led a mid-size team to build an authoritative system for managing master data for PayPal Inc.\n\u2022 Consulted as a subject matter expert in Teradata Parallel Transporter. Presented several use-cases on advanced\nanalytics to clients as a part of request for proposals.', u'Data Analyst\nTATA CONSULTANCY SERVICES (TCS) LIMITED - Kolkata, West Bengal\nNovember 2010 to July 2013\nAnalyzed, cleansed and transformed insurance data for Sunlife Financials for risk reporting and portfolio management\nsystem to facilitate management and operations of its asset management activities on ETL/RDBMS framework.\n\u2022 Involved in requirement gathering, technical solutions and process improvement for BlackRock Inc. Created analytical\nreports based on Portfolio which facilitated a highly automated, consistent investment process.']","[u'Master of Sciences in Data Science', u'Bachelor of Technology in Information Technology']","[u'ILLINOIS INSTITUTE OF TECHNOLOGY Chicago, IL\nSeptember 2016 to December 2017', u'WEST BENGAL UNIVERSITY OF TECHNOLOGY Kolkata, West Bengal\nAugust 2006 to June 2010']"
0,https://resumes.indeed.com/resume/fb9f6057c4624889,"[u'Program Assistant/Data Analyst\nKean University - Project Adelante (Pre-College Program) - Union, NJ\nAugust 2012 to Present\n\u2022 Analyze, evaluate, and interpret data for surveys/questionnaires and assessments\n\u2022 Assist in planning class schedules, reservations, and other events/activities\n\u2022 Compile all quantitative and qualitative data to prepare annual reports to be submitted to school districts\n\u2022 Coordinate job rotation and cross-training\n\u2022 Delegate responsibilities and tasks among team members\n\u2022 Design and create data reports and surveys/questionnaires\n\u2022 Design forms (e.g., activity forms, lesson plans, reports, rosters, etc.)\n\u2022 Maintain and update database\n\u2022 Organize and coordinate staff meetings and workshops\n\u2022 Supervise student workers']","[u'Master of Business Administration (MBA) in Management Information Systems', u""Bachelor's Degree in Business Management & Minor in Computer Science""]","[u'New Jersey Institute of Technology Newark, NJ\nSeptember 2016 to August 2018', u'Kean University Union, NJ\nSeptember 2011 to May 2016']"
0,https://resumes.indeed.com/resume/678c496d27cb4902,"[u'Data analyst Intern\nBank of China - Changzhou, CN\nJune 2017 to August 2017\n\u2022 Changzhou, China\n\u2022 Analyzed the repayment ability of lender\n\u2022 Mastered the business process of the L/C\n\u2022 Estimated the current price of the mortgaged property to make appropriate decisions.']",[u'M.S in Data Science'],"[u'University of Wisconsin Madison Madison, WI\nJanuary 2018']"
0,https://resumes.indeed.com/resume/0b20305ce1bcbb91,"[u""Data Analyst\nPathways by Molina Healthcare - Anaheim, CA\nOctober 2017 to Present\n\u2022 Attended all data- and outcome- related meetings and ensured that assigned program was proactive in all data collection requirements and changes at the local and state level.\n\u2022 Produced timely, consistent, relevant data reports in all domain areas and disseminated for program and county use.\n\u2022 Ensured data complied with HIPAA and reflected program's outcomes, strengths, and weaknesses."", u""Bank Teller\nWells Fargo - Santa Ana, CA\nSeptember 2016 to October 2017\n\u2022 Processed confidential information according to corporate policies and procedures.\n\u2022 Participated in bank audits of internal controls consistent with jurisdictional regulations and industry standards.\n\u2022 Educated customers on products and options to determine financial needs and address them accordingly.\n\nACCOLADES\nBlack Hat USA 2017 Student Scholarship in 2017 Segal AmeriCorps Education Award in 2015\nDean's List in Spring 2016 and 2017, Summer 2017 California Seal of Biliteracy in 2013""]",[u'Bachelor of Science in Business Administration in Computer Information Systems'],[u'California State Polytechnic University\nJune 2018']
0,https://resumes.indeed.com/resume/b25482cd850bfd60,"[u'Trauma registrar / data analyst\nGenesis HealthCare - Zanesville, OH\nAugust 2010 to Present\nI am the lead trauma registrar at a Level 3 adult trauma center. I oversee all trauma abstractions, coding and entering of data into Traumabase. I am responsible for quarterly downloads to region, state and national trauma databases. I am responsible for all monthy and quarterly reporting and data analysis. I ensure all data and submissions meet TQIP standards as well as NTBD standards.']","[u""Bachelor's in Allied Healthcare"", u'Associates in Allied health - medical assisting']","[u'Franklin University Columbus, OH\nJanuary 2014', u'Zane State College Zanesville, OH\nJanuary 2010']"
0,https://resumes.indeed.com/resume/5ba6e1ec58ad1f38,"[u'Data Analyst\nAIG - Bangalore Urban, Karnataka\nJuly 2014 to July 2017\n\uf0b7 Designed and developed statistical Visualization dashboards to enable Data Driven Decision making regarding Data Quality improvement, Organizational efficiency monitoring among others.\n\uf0b7 Queried data (10M+ rows) from IBM DB2 and used MapReduce and Hive to create aggregated data marts, used Qlikview and Tableau to create visualization dashboards.\n\uf0b7 The dashboards powered D3 and contributed in improvement in Data Quality indices, helped establish an effective organizational health monitoring system.']","[u'MS in Computer Science', u'BE in Computer Engineering']","[u'UC San Diego San Diego, CA\nJune 2019', u'BMS College of Engineering Bengaluru, Karnataka\nSeptember 2010 to May 2014']"
0,https://resumes.indeed.com/resume/4430164f77917c85,"[u'DATA ANALYST\nMV-TRANSPORTATION\nMarch 2017 to November 2017\nMV-Transportation a provider of paratransit services and the largest privately-owned transportation contracting firm. Contracted by Metro Access and managing the efficient flow in the transportation of seniors and ADA customers in the DC Metro area. Innovative transportation solutions that provide freedom of movement, safety, and a great customer experience.\n\n\u2022 Examine unique needs and concerns of the business to develop relevant practices and procedures for preparing business following Metro Access daily operating procedures.', u""DATA/REPORTING ANALYST\nMBI HEALTH SERVICES\nFebruary 2013 to December 2016\nMBI is a healthcare company dealing with the day to day running of patients' health care needs, health insurance, medication supply, housing, and transportation.\n\n\u2022 Use SQL to extract from relational databases, MSSQL (2012, 2014) and analyze data on insurance types for patients - some private and others government (Medicaid 1, 2, 3).\n\u2022 Extract data from the enterprise (EDW) data warehouse using micro-strategy to perform data quality check on patient insurance, qualification, and benefits to avoid overlaps.\n\u2022 Troubleshoot overlap and data quality issues by looking through data lineage, systems, and triggers for signs of bugs and malfunction.\n\u2022 Used Ca Erwin to design and create scripts used by database administrators to create test database (Oracle 10g) structures as needed.\n\u2022 Write source to target document for data migration. Mapping the source to the destination.\n\u2022 Update business and technical metadata as needed and in Tableau during story and dashboard creation.\n\u2022 Perform cost-benefit analyses on client's data based on their utilization of treatment plans.\n\u2022 Perform comparative cost analyses on clientele transportation contractors like Metro Access, Uber, MTM and Private transportation and give recommendations.\n\u2022 Conduct data quality analyses on clientele insurance types in relation to patient status and type of insurance coverage based on business rules\n\u2022 Analyze database structures, models and other database objects on its impact on business process data.\n\u2022 Extract and analyze current and historic medical equipment suppliers for MBI- their rates, contract terms from EDW and recommend possible changes to procurement."", u'Technical Business/Data Reporting Analyst\nCOMCAST HD\nJanuary 2011 to January 2013\nComcast is one of the leading telecommunication companies in Maryland with a great line of clientele needing constant and reliable network services. The purpose of this project was to improve on the network services provided by the company compared to other competitors.\n\n\u2022 Gathered of functional and system requirements for software development (Network Pro).\n\u2022 Extraction of information from relational and dimensional databases, MSSQL (2008, 2012) using SQL and present in Excel as needed.\n\u2022 Proposed software enhancement and changes to database table structures based on results from data extracted and analyzed.\n\u2022 Working with other team members in prioritizing project requirements. (Network Pro Development).\n\u2022 Organized join application and development (JAD) sessions to disseminate results from requirement gathering and to keep track of project progress.\n\u2022 Performed business modeling and analysis as required.\n\u2022 Performed data queries to present technical reports using data Visualization.']",[u'Certification in Interpersonal Communication'],[u'Healthcare and information Tech']
0,https://resumes.indeed.com/resume/eb816baef4d78345,"[u""Big Data Analyst Intern\nPurchasing Power - Atlanta, GA\nMay 2017 to July 2017\n\u2022 Developed Sqoop jobs to transfer data between enterprise data sources and BDA.\n\u2022 Developed Spark Jobs to serve the business users' requirements and administered Oracle Big Data Appliance\nincluding HDFS maintenance.\n\u2022 Created ER diagram for Schemas and created a documentation for a small part in PCI Audit.\nPROJECTS:\n\u2022 Server load balancing with path selection in virtualized SDN. Java, Python\nImplemented load balancing mechanisms in the virtual networks built from the underlying SDN\ninfrastructure and optimized the virtual network.\n\u2022 Cancer Diagnostic System. C++\nDeveloped an intelligent expert system using forward and backward chaining to detect the cancer type and suggest treatment based on the symptoms provided by the user.\n\u2022 CONNECT4 Game. C++\nDeveloped a two-player computer vs computer Artificial Intelligence Game to play CONNECT4 game using\nGame Theory Algorithm.\n\u2022 Collision Avoidance and Depth Detection System. C\nImplemented a bot using Arduino UNO which detects obstacle and uneven surface on the path and takes\nnecessary action.""]",[u'Masters of Science in Computer Science'],[u'Texas State University\nMay 2018']
0,https://resumes.indeed.com/resume/23cf1d881d91846a,"[u'Senior Data Analyst\nMercer - Shanghai, CN\nJanuary 2016 to February 2016\n\u2022 Retrieved and analyzed data from large database using SQL/Python/Microsoft Office software\n\u2022 Conducted research, benchmarking and analysis on market trends for clients and present using Tableau, experience with data visualization and building dashboards\n\u2022 Built models for clients to forecast future cost based on historical data, increased accuracy by 20%\n\u2022 Solved clients\u2019 problems by identifying the data needed and applying quantitative analysis, data mining, and A/B testing to present solutions\n\u2022 Assisted project managers with initiating and driving analytics projects and delivering actionable insights and recommendations for clients\n\u2022 Participated in a M&A harmonization project for an Internet company by working on due diligence and post-merger harmonization assignments, experience with Hadoop and Hive\n\u2022 Built and maintained relationships with clients, influenced long-term strategic direction', u'Analyst\nAllianz - Shanghai, CN\nJanuary 2013 to October 2013\n\u2022 Built a process using SQL to prepare reinsurance data in order to calculate actuarial reinsurance reserve; Increased efficiency by 50%\n\u2022 Designed actuarial models for financial reinsurance profitability projection, made impact study on different assumptions\n\u2022 Participated in annual assumption review. Calculated monthly allowable expense and overrun, analyzed unusual movements in figures\n\u2022 Project manager for \u201cgroup reinsurance system\u201d project, coordinated various functions\n\u2022 Maintained contact with various reinsurance companies; Managed reinsurance treaty signoff process and in responsible for quarterly accounts delivery and settlements\n\u2022 Prepared documents for internal and external audit and regular reporting to region\u2019s regulatory department']","[u'Graduate Certificate in Management Science and Engineering', u""Bachelor's in Mathematics/Actuarial Science""]","[u'Stanford University Palo Alto, CA\nJanuary 2018 to January 2018', u'Ohio State University-Main Campus Columbus, OH\nSeptember 2010 to May 2013']"
0,https://resumes.indeed.com/resume/81d97f1ad1fb3d86,"[u'Research Analyst\nInternational Centre\nAugust 2015 to January 2016\nfor Oil and Environment, Scotland, U.K', u'Data Analyst\nUnilog Content Solutions pvt. Ltd - Mysore, Karnataka\nAugust 2012 to April 2013\nIndia)']","[u'Masters in Environmental Resource Management in Environmental Resource Management', u'Bachelor in Environmental Engineering in Equivalent to Civil Engineering', u'', u'Bachelor in VERMICOMPOSTING', u""in Drafting Environmental Systems Using AutoCAD''"", u""in Environmental Saftey and Management''""]","[u'Brandenburg Technical University Cottbus Senftenberg, Cottbus, DE\nOctober 2013 to March 2016', u'International Centre for Oil and Environment\nAugust 2014 to December 2014', u'National Institute of Engineering Mysore, Karnataka\nMay 2012', u'Visvesvaraya Technological University Belgaum, Karnataka\nMarch 2012', u'Visveswaraya Technological University Mysore, Karnataka\nAugust 2011', u'Vidyavardhaka College of Engineering Mysore, Karnataka\nMarch 2009']"
0,https://resumes.indeed.com/resume/137030d55591b8b0,"[u'Data Analyst\nHuangPu Customs of China - GuangZhou, CN\nJune 1999 to December 2012\nManaged routine cargo import and export data, including collecting data, setting up databases,\ncleaning up, sorting and classifying data.\n\u2022 Conducted international market research on Chinese import and export cargos, determined major\nfactors which greatly influenced import and export cargo prices and quantities.\n\u2022 Analyzed these main factors using descriptive and inferential Statistics.\n\u2022 Set up predictive models to forecast fluctuations of import and export cargo prices and quantities.\n\u2022 Wrote official forecast reports based on the above statistical analysis.']","[u'Master of Science in Mathematics - Statistics', u'Master of Science in Economics', u'Bachelor of Science in Economics']","[u'University of Massachusetts Lowell, MA\nSeptember 2014 to May 2017', u'Harbin Institute of Technology Harbin, CN\nSeptember 1996 to June 1999', u'Central South University Changsha, CN\nJune 1995']"
0,https://resumes.indeed.com/resume/e615778e0e8cb385,"[u'Data Scientist\nCadence Design Systems\nFebruary 2014 to July 2015\nDescription: Cadence Design Systems, Inc. is an American multinational electronic design automation software and engineering services company, founded in 1988 by the merger of SDA Systems and ECAD, Inc.\n\nResponsibilities:\n\u2022 Responsible for performing Machine-learning techniques regression/classification to predict the outcomes.\n\u2022 Responsible for design and development of advanced R/Python programs to prepare transform and harmonize data sets in preparation for modeling.\n\u2022 Identifying and executing process improvements, hands-on in various technologies such as Oracle, Informatica and Business Objects.\n\u2022 Designed the prototype of the Data mart and documented possible outcome from it for end-user.\n\u2022 Involved in business process modeling using UML\n\u2022 Developed and maintained data dictionary to create metadata reports for technical and business purpose.\n\u2022 Handled importing data from various data sources, performed transformations using Hive, Map Reduce, and loaded data into HDFS.\n\u2022 Interaction with Business Analyst, SMEs and other Data Architects to understand Business needs and functionality for various project solutions.\n\u2022 Created SQL tables with referential integrity and developed queries using SQL, SQL*PLUS and PL/SQL.\n\u2022 Involved with Data Analysis primarily Identifying Data Sets, Source Data, Source Meta Data, Data Definitions and Data Formats\n\u2022 Performance tuning of the database, which includes indexes, and optimizing SQL statements, monitoring the server\n\u2022 Wrote simple and advanced SQL queries and scripts to create standard and adhoc reports for senior managers.\n\u2022 Collaborate the data mapping document from source to target and the data quality assessments for the source data.\n\u2022 Created PL/SQL packages and Database Triggers and developed user procedures and prepared user manuals for the new programs.\n\u2022 Participated in Business meetings to understand the business needs & requirements.\n\u2022 Prepare ETL architect& design document which covers ETL architect, SSIS design, extraction, transformation and loading of Duck Creek data into dimensional model.\n\u2022 Provide technical & requirement guidance to the team members for ETL -SSISdesign.\n\u2022 Participated in Business meetings to understand the business needs & requirements.\n\u2022 Design ETL framework and development.\n\u2022 Design Logical & Physical Data Model using MS Visio 2003 data modeler tool.\n\u2022 Participated in stake holders meetings to understand the business needs & requirements.\n\u2022 Participated in Architect solution meetings & guidance in Dimensional Data Modeling design.\n\u2022 Coordinate and communicate with technical teams for any data requirements.\n\nEnvironment: Python, Spark MLlib, TensorFlow, K- means, ANN, Regression, Oryx 2, Accord.NET, Flask, ORM, Jinja 2, Amazon Machine Learning (AML),Apache,Django, Mako, Naive Bayes, SVM.', u""Data Scientist\nCeridian HCM, Inc - Minneapolis, MN\nAugust 2012 to January 2014\nDescription: Ceridian HCM, Inc. is a provider of human resources software and services with employees in the USA, Canada, Europe and Mauritius.\n\nResponsibilities:\n\u2022 Data modeling and formulation of statistical equations using advanced statistical forecasting techniques.\n\u2022 Document the complete process flow to describe program development, testing, application integration, coding and implementation.\n\u2022 Scoring predictive models as per regulatory requirements & ensuring deliverables with PSI.\n\u2022 Built predictive scorecards for Life Insurance, TD, Cross-selling Car loan and RD.\n\u2022 Mentoring Provide guidance to team members.\n\u2022 Developing propensity models for Retail liability products to drive proactive campaigns.\n\u2022 Transformation, Data cleansing and creating new variables using R.\n\u2022 Responsible for defining the functional requirement documents for each source to target interface.\n\u2022 Identifying the Customer and account attributes required for MDM implementation from disparate sources and preparing detailed documentation.\n\u2022 Approve and Present designed Logical Data Model in Data Model Governance Committee (DMGC)\n\u2022 Tabulation and Extraction of data from multiple data sources using R, SAS.\n\u2022 Work with users to identify the most appropriate source of record and profile the data required for sales and service.\n\u2022 Extracted data from HDFS and prepared data for exploratory analysis using datamunging.\n\u2022 Responsible for defining the key identifiers for each mapping/interface.\n\u2022 Validated the machine learning classifiers using ROC Curves and Lift Charts.\n\u2022 Arrange and chair Data Workshops with SME's and related stake holders for requirement data catalogue understanding.\n\nEnvironment: regression, logistic regression, Hadoop, Teradata, OLTP, Unix, Python, MLLib, SAS, random forest, OLAP, HDFS, NLTK, SVM, JSON and XML."", u'Data Scientist/Data Analyst\nSalesForce - San Francisco, CA\nMay 2008 to December 2010\nDescription: Salesforce.com, inc. develops enterprise cloud computing solutions with a focus on customer relationship management. The company offers Sales Cloud to store data, monitor leads and progress, forecast opportunities, gain insights through relationship intelligence, and collaborate around sales on desktop and mobile devices, as well as solutions for partner relationship management. It also provides Service Cloud, which enables companies to deliver personalized customer service and support, as well as connects their service agents with customers on various devices.\n\nResponsibilities:\n\u2022 Worked on data cleaning and reshaping, generated segmented subsets using Numpy and Pandas in Python.\n\u2022 Developed Python scripts to automate data sampling process. Ensured the data integrity by checking for completeness, duplication, accuracy, and consistency.\n\u2022 Worked on model selection based on confusion matrices, minimized the Type II error\n\u2022 Generated cost-benefit analysis to quantify the model implementation comparing with the former situation.\n\u2022 Wrote and optimized complex SQL queries involving multiple joins and advanced analytical functions to perform data extraction and merging from large volumes of historical data stored in Oracle 11g, validating the ETL processed data in target database.\n\u2022 Conducted model optimization and comparison using stepwise function based on AIC value\n\u2022 Applied various machine learning algorithms and statistical modeling like decision tree, logistic regression, Gradient Boosting Machine to build predictive model using scikit-learn package in Python\n\u2022 Continuously collected business requirements during the whole project life cycle.\n\u2022 Identified the variables that significantly affect the target\n\u2022 Generated data analysis reports using Matplotlib, Tableau, successfully delivered and presented the results for C-level decision makers.\n\nEnvironment: Teradata 13, Erwin 8, SQL Server 2008, Oracle 9i, PL/SQL, OLAP, Informatica Power Center, SQL*Loader, ODS, OLTP, SSAS.', u'Data Modeler\nSalesForce - Brentwood, TN\nMay 2008 to December 2010\nDescription: Sterling Insurance Company is located in Cobleskill, New York. Since 1895 we have been providing quality insurance protection to New Yorkers with an ever expanding array of product offerings.\n\nResponsibilities:\n\u2022 Develop Integrations jobs to transfer data from source system to Hadoop.\n\u2022 Installation of Talend Studio.\n\u2022 Technical design documents for Transformation processes.\n\u2022 Application of business rules on the data being transferred.\n\u2022 Task allocation for the ETL and Reporting team.\n\u2022 Communicate effectively with client and their internal development team to deliver product functionality requirements.\n\u2022 Architecting and design of data warehouse ETL processes.\n\u2022 Demo of POC built for the prospective customer and provide guidance and gather the feedback to backend ETL testing on SQL Server 2008 using SSIS.\n\u2022 Create the Operational manual Document.\n\u2022 Create Integration Jobs to backup a copy of data in network file system.\n\u2022 Design and implement the ETL Data model and create staging, source and Target tables in SQL server database.\n\u2022 Gathering and analysis requirements definition meetings with business users and document meeting outcomes.\n\nEnvironment: ETL, ODS, Hadoop, MS Office, Talend Studio, OLAP , SQL Server 2008.', u'Data Modeler\nSalesForce - IN\nMay 2008 to December 2010\nDescription: Bharti Airtel Limited commonly known as Airtel is an Indian telecommunications company that operates in 20 countries across South Asia, Africa and the Channel Islands. It operates a GSM network in all countries, providing 2G or 3G services depending upon the country of operation. Airtel is the third largest telecom operator in the world with over 243.336 million customers across 20 countries.\n\nResponsibilities:\n\u2022 Developed and implemented predictive models using Natural Language Processing Techniques and machine learning algorithms such as linear regression, classification, multivariate regression, Naive Bayes, Random Forests, K-means clustering, KNN, PCA and regularization for data analysis.\n\u2022 Designed and developed Natural Language Processing models for sentiment analysis.\n\u2022 Applied clustering algorithms i.e. Hierarchical, K-means with help of Scikit and Scipy.\n\u2022 Developed visualizations and dashboards using ggplot, Tableau.\n\u2022 Worked on development of data warehouse, Data Lake and ETL systems using relational and non relationaltools like SQL, NoSQL.\n\u2022 Built and analyzed datasets using Matlab R, SAS and Python (in decreasing order of usage)\n\u2022 Participated in all phases of datamining; datacollection, datacleaning, developingmodels, validation, visualization and performed Gapanalysis.\n\u2022 DataManipulation and Aggregation from different source using Nexus, Toad, Business Objects, PowerBI and Smart View.\n\u2022 Implemented Agile Methodology for building an internal application.\n\u2022 Good knowledge of Hadoop Architecture and various components such as HDFS, JobTracker, Task Tracker, Name Node, Data Node, Secondary Name Node, and MapReduce concepts.\n\u2022 As Architect delivered various complex OLAP databases/cubes, scorecards, dashboards and reports.\n\u2022 Programmed a utility in Python that used multiple packages (scipy, numpy, pandas)\n\u2022 Implemented Classification using supervised algorithms like Logistic Regression, Decision trees, KNN, and Naive Bayes.\n\u2022 Used Teradata15 utilities such as Fast Export, MLOAD for handling various tasks data migration/ETL from OLTP Source Systems to OLAP Target Systems\n\u2022 Maintenance in the testing team for System testing/Integration/UAT.\n\u2022 Involved in preparation & design of technical documents like Bus Matrix Document, PPDM Model, and LDM & PDM.\n\u2022 Understanding the client business problems and analyzing the data by using appropriate Statistical models to generate insights.\n\nEnvironment: MDM, QlikView, MLLib, HADOOP, MapReduce, PIG, MAHOUT, R, Erwin, Tableau, PL/SQL, JAVA, HIVE, AWS, HDFS, Teradata, JSON, Spark, R Studio.', u""Data Analyst\nSalesForce - Bengaluru, Karnataka\nMay 2008 to December 2010\nDescription: Exide Life Insurance Company Limited, is a 100% Indian owned life insurance company, owned by the Exide Industries. Exide Life Insurance distributes its products through multi-channels.\n\nResponsibilities:\n\u2022 Developed Tableau visualizations and dashboards using Tableau Desktop.\n\u2022 Applied Business Objects best practices during development with a strong focus on reusability and better performance.\n\u2022 Developed and executed load scripts using Teradata client utilities FASTLOAD, MULTILOAD and BTEQ.\n\u2022 Generated periodic reports based on the statistical analysis of the data using SQL Server Reporting Services.\n\u2022 Used Graphical Entity-Relationship Diagramming to create new database design via easy to use, graphical interface.\n\u2022 Maintained metadata (data definitions of table structures) and version controlling for the data model.\n\u2022 Designed different type of STAR schemas for detailed data marts and plan data marts in the OLAP environment.\n\u2022 Write SQL scripts to test the mappings and Developed Traceability Matrix of Business Requirements mapped to Test Scripts to ensure any Change Control in requirements leads to test case update.\n\u2022 Responsible for development and testing of conversion programs for importing Data from text files into map Oracle Database utilizing PERL, shell scripts & SQL*Loader.\n\u2022 Utilized Erwin's forward/reverse engineering tools and target database schema conversion process.\n\u2022 Developed SQLscripts for creating tables, Sequences, Triggers, views and materialized views.\n\u2022 Co-ordinate with various business users, stakeholders and SME to get Functional expertise, design and business test scenarios review, UAT participation and validation of financial data.\n\u2022 Worked on creating enterprise wide Model EDM for products and services in Teradata Environment based on the data from PDM. Conceived, designed, developed and implemented this model from the scratch.\n\nEnvironment: Oracle SQL Developer, MS SQL Server, SQL*PLUS, PL/SQL, Business Objects, SQL*LOADER, Tableau, Informatica, XML, Windows XP, TOAD, Business Objects.""]",[],[]
0,https://resumes.indeed.com/resume/766587f6c9f30073,"[u'Category Data Analyst\nFacebook Inc - Menlo Park, CA\nJanuary 2015 to January 2017\nWorked closely with the Product Intelligence Team on multiple projects with a focus on improving the relevancy of Facebook Ads.\n\n\u2022 Improved the shopping classification models and expanded the taxonomy for 2 classifiers: Sports and Apparel. As part of a team of 4 analysts increased the number of categories from 36 to 1650 in Sports and from 380 to 970 in Apparel. Performed QA on training data to improve product quality.\n\n\u2022 Analyzed web page content from OBA and SPI data of top shopping and non-shopping domains visited by Facebook users. This helped to extract user intent and create a shopping profile for each user. It also helped in the creation of a Product Recommender Tool.\n\n\u2022 Generated regular expressions for thousands of user visited webpages to identify and classify shopping urls, and recognize product listings from category and search result pages. This significantly helped in optimizing crawl data and faster processing of millions of urls.\n\n\u2022 Performed QA Testing using SRT based Tool developed by the Product Intelligence Team to assess the quality of our Classification Models and Information Extraction Pipeline. Product Page Classification quality improved from 55% to 90% on a manual QA of 2000+ pages.', u'Category Data Analyst\nTheFind Inc - Mountain View, CA\nJanuary 2010 to January 2015\nAs a Senior Data Analyst, worked on multiple projects mentioned below with a focus on search quality.\n\n\u2022 Product Taxonomy: Researched, designed and implemented a comprehensive navigation structure to optimize product discovery when searching and browsing TheFind.com. Created detailed taxonomies for multiple shopping categories. Added training data to the product nodes and performed QA tests to ensure product quality.\n\n\u2022 Attributes Project: Created a database of attributes and attribute values for the thousands of product nouns in the knowledge base. This noun-attribute association has helped in the successful identification and extraction of product nouns from title and description of a product. It also helped to create a more robust, user centered, customizable, and interactive website with the addition of web search filters.\n\n\u2022 WordNet Project: As part of a team, created a noun dictionary with manual collection of approx. 25k product nouns from top merchant sites. Used these nouns to develop a noun knowledge base, establishing relationships between noun pairs with a focus on improving search quality and improved Product Noun Recognition.', u'Search Quality Analyst\nSunnyvale, CA\nJanuary 2008 to January 2009\nResponsible for Search Quality Ratings for US - English.\n\u2022 Responsible for assessment of the authoritativeness, topical pertinence, and overall quality of web pages.\n\u2022 Performed in-depth analysis of user sessions data to establish query intent.', u""Google - Mountain View, CA\nJanuary 2006 to January 2008\nResponsible for Search Quality Ratings for US - English.\n\u2022 Using Google's online tool was responsible for analyzing and providing feedback on text, web pages, images and other types of information returned by search query results.\n\u2022 Actively participated in relevance tests and spam detection efforts.""]","[u'M.S. in Organic Chemistry', u'B.Ed.', u'Diploma in Software Technology', u'']","[u'University of Mumbai Mumbai, Maharashtra', u'University of Mumbai Mumbai, Maharashtra', u'NIIT Mumbai, Maharashtra', u'DeAnza College Cupertino, CA']"
0,https://resumes.indeed.com/resume/99e9729bc92181f4,"[u'Data Analyst\nBetter Business Bureau - St. Louis, MO\nApril 2017 to Present\nDatabase manager, data quality control officer, training employees in best practices, statistical analysis, special\nprojects', u""General Manager\nLeft Bank Books - St. Louis, MO\nSeptember 2012 to February 2017\nCustomer service manager - Responsible for cultivating sustaining relationships with customers, recruiting and retaining long-term customers by providing unmatched customer service and communicating LBB's history and mission\n\u25cf Special projects/events coordinator - event logistics and planning for events, staff events, source special items and all printed materials for events or sale in the store; created and managed This Just In first editions book club; structure\nsystem of evaluating special projects\n\u25cf Scheduling weekly for 20+ employees and 300+ events a year, scheduled all vacation, travel, employee requests for time off, regular department and all staff meetings, implemented and managed universal strategy and planning calendar\n\u25cf Database management - personal/contact information for all customers, LBB Foundation donors, inventory\nmanagement; kept files up to date, organized\n\u25cf Personnel management: staff performance reviews, training, interviewing & hiring new employees\n\u25cf Ordering for events & office supplies; cut administrative supplies costs with efficient, timely ordering; manage\nadministrative expenses including petty cash, travel, event spending\n\u25cf Technology maintenance - maintain network+server+client computers+registers with regular updates, system and software upgrades, and troubleshooting issues""]",[u'B.A. in Politics & Government'],"[u'Pomona College Claremont, CA\nMay 2012']"
0,https://resumes.indeed.com/resume/067b1be5b6014d2a,"[u""Data Analyst\nWells Fargo - Charlotte, NC\nJanuary 2017 to Present\nResponsibilities:\n\u2022 Developed several scripts to gather all the required data from different databases.\n\u2022 Provided management support asset lifecycles, change management lifecycles, and IT inventory control.\n\u2022 Communicated with different Stakeholders, Business Group, and User Group to elicit and to analyze Business Requirements.\n\u2022 Developed numerous reports to capture the transactional data for the business analysis.\n\u2022 Developed complex SQL queries to bring data together from various systems.\n\u2022 Organized and conducted cross-functional meetings to ensure linearity of the phase approach.\n\u2022 Collaborated with a team of Business Analysts to ascertain capture of all requirements.\n\u2022 Translate the requirements into Business (Functional) and System (Technical) requirements and review with the customer to get their approval. Wrote complex SQL queries to analyze and understand data.\n\u2022 Assisted the team for standardization of reports using SAS macros and SQL\n\u2022 Created multiple reports on the daily transactional data which involves millions of records.\n\u2022 Used Joins like Inner Joins, Outer joins while creating tables from multiple tables.\n\u2022 Implemented Indexes, Collecting Statistics, and Constraints while creating tables.\n\u2022 Developed various ad hoc reports based on the requirements.\n\u2022 Conducted independent statistical analysis, descriptive analysis, hypothesis testing and logistic regression.\n\u2022 Provided assistance for projects through SAS statistical and analysis software tools.\n\u2022 Created macros and used existing macros to develop SAS programs for data analysis.\n\u2022 Assisted in development of data formatting and cleaning criteria from various data sources.\n\u2022 Generated database reports, presentation and documentation of analytical methods.\n\u2022 Performed ad hoc analysis of data sources for all external and internal customers.\n\u2022 Coordinated with clients for management of data mining and recovery projects.\n\u2022 Developed SQL Queries to fetch complex data from different tables in remote databases using joins, database links and formatted the results into reports and kept logs.\n\u2022 Involved in writing complex SQL queries using correlated sub queries, joins, and recursive queries.\n\u2022 Delivered the artifacts within the time lines and excelled in the quality of deliverables.\n\u2022 Validated the data during UAT testing.\n\u2022 Involved in defining the source to target data mappings, business rules and data definitions.\n\u2022 Coded python scripts, automating the data ingestion process between the client and external data servers, reducing the client's data ingestion time from a week to one day. Coded automation scripts to transact between the client's servers and external FTP servers, freeing up time for the client to work on other tasks. Languages used included Bash, MySQL, and Python.\n\u2022 Performing source to target Mapping\n\u2022 Built logics for automated data extraction and validation for EIT (Enterprise Independent Testing).\n\u2022 Acted as subject matter expert on data mining, code development, quantitative tools, and automated reporting.\n\u2022 Developed a macro to manipulate and message the dataset to arrive at an exception report for management.\n\u2022 Responsible for creating a Business Requirements Document to have an overall automated reconciliation approach from a go forward perspective.\n\nEnvironment: Google Tag Manager (GTM), Dynamic Tag Manager (DTM), Google Analytics - Google Analytics Premium, AdWords, Dynamic Tag Manager (DTM), HP Quality Center, HTML JavaScript, XML, Microsoft Excel, Microsoft Word, PowerPoint, Windows XP OS, PL/SQL, Test Director, TOAD, SQL*Plus, SQL*Loader, Autosys, Windows, UNIX."", u""Data Analyst\nBCBS - Chicago, IL\nJune 2016 to December 2016\nResponsibilities:\n\n\u2022 Provided research and root cause analysis for claim process remediation.\n\u2022 Experienced in development implementations, data modeling, or data mapping efforts.\n\u2022 Experienced creating clear and concise project documentation.\n\u2022 Created queries and reports using SQL or reporting tools\n\u2022 Worked with the internal business users to gather business requirements that require data warehousing and operational data store related work.\n\u2022 Operational Reporting Environment teams to develop data requirements, data mapping and data modeling.\n\u2022 Make certain the user stories are good, comply with acceptance criteria, have sufficient business value and selling points.\n\u2022 Guide the development teams to break down large and complex user story into simplified versions for execution. Developed and implemented process improvements for services that are performed by the Claims Department.\n\u2022 Experience in SAS programming for auditing data, developing data, performing data validation QA and improve efficiency of SAS programs.\n\u2022 Assist the Business users to define and refine the scope of the project.\n\u2022 Translate the requirements into Business (Functional) and System (Technical) requirements and review with the customer to get their approval. Wrote complex SQL queries to analyze and understand data.\n\u2022 Generate reports from SAS, Business Objects or Crystal reports depending on the customer requirement.\n\u2022 Write SQL queries to create mock up reports as per the business requirements and got user approval.\n\u2022 Collaborated with other BSA's for grooming business requirements related to reporting and/or source data manipulation.\n\u2022 Work with the business users to develop the User Acceptance Test (UAT) Cases documents.\n\u2022 Provided support to QA Team and was involved in SIT, Functional, Smoke, End-to-end and User acceptance testing.\n\u2022 Involved in preparing test cases and involved in reviewing test plans and test scripts developed by QA team to make sure that all requirements are covered in scripts and tested properly.\n\nEnvironment: SQL Server 2012, Agile, Power BI, ETL, Informatica9.0, MDM, SQL, XML, Oracle11g, MS Access"", u""Business Data Analyst\nAccenture - Hyderabad, Telangana\nAugust 2013 to January 2016\nHyderabad, India\n\nResponsibilities:\n\u2022 Delivered custom analytics for marketing insight team including customer segmentation, market structure analysis, market driver analysis, and promotion response modeling.\n\u2022 Gathered data required for conducting analysis from several sources, Performed data transformations and pre-process data in R and entered data into data analysis software like SPSS.\n\u2022 Performed data Extraction, Pre-processing & cleansing to remove redundant and unwanted data (ETL) to extract insights from unstructured data and Developed Tableau based Dashboards.\n\u2022 Created meaningful derived metrics from data assets to improve effectiveness of analytics.\n\u2022 Performed regression analysis, basket analysis, segmentation analysis, and correlation analysis.\n\u2022 Created models (knn, k-means and regression) & patterns with statistical tools (SPSS) to produce data and visualization solutions, using which the stakeholders can interpret the underlying insights.\n\u2022 Performed analysis of the given data by predictive modeling and draw accurate inferences by visualizing the data in Tableau and generating reports by using SSRS, in accord with the objectives of the analysis.\n\u2022 Formulated procedures for data extraction, transformation and integration of data.\n\u2022 Balanced differing and changing schedules for each market, communicating all variation to stakeholders.\n\u2022 Conducted assessment to ensure data quality and completeness with validation techniques.\n\u2022 Assisted in designing and development of analytical products for overall enterprise.\n\u2022 Designed and developed continuous outcomes reporting for multiple intervention programs.\n\u2022 Coordinated with technical teams for research and evaluation of operational problems.\n\u2022 Participated in critical review and revision of existing systems for updates and enhancements.\n\u2022 Supported technical teams for development and execution of supplemental analytical tools and methods.\n\u2022 Developed a user interface to help a client transition away from an Excel-based to a MySQL-based data management system. Reduced job time for the client by 60%, partially by automating data retrieval from IRS servers so that the client would not need to rely on external data stores.\n\u2022 Programmed a script to automatically condense 100+ page PDFs into 4 page PDFs so that the client would not have to spend time opening and scanning for information. Developed meta tag and ranked search systems to increase database search speed. Languages used include CSS, HTML, jQuery, JavaScript, PHP, Python, and SQL.\n\u2022 Coded python scripts, automating the data ingestion process between the client and external data servers, reducing the client's data ingestion time from a week to one day. Coded automation scripts to transact between the client's servers and external FTP servers, freeing up time for the client to work on other tasks. Languages used included Bash, MySQL, and Python.\n\u2022 Interacted with business analysts and technical leads from other platforms to gather the key elements of data which need to be monitored on a regular basis.\n\u2022 Perform impact analysis to assess impact of source changes to the warehouse environment.\n\u2022 Responsible for rewriting the existing data quality reports in SAS.\n\u2022 Developed new reports in SAS using SAS ODS, PROC REPORT, PROC TABULATE, and PROC SQL.\n\u2022 Imported Data from relational database into SAS files per detailed specifications.\n\u2022 Responsible for the development and maintenance of SAS Information Maps for Analytics and Business Forecasting Team.\n\u2022 Write complex SQL queries with sub-queries, analytical functions, pivot functions and inline views.\n\u2022 Provided many innovative solutions for streamlining the data quality check process and managing the requests/ queries from analysts regarding warehouse environment.\n\nENVIRONMENT: Oracle, XML, SQL, SAS, R, SPSS, SSRS Metadata Hub, Informatica Data Quality tool""]",[u'Masters of Information Technology in Information Technology'],"[u'Rutgers Business School Newark, NJ']"
0,https://resumes.indeed.com/resume/5019cef186fad3f7,"[u'Crash Data Analyst\nCurrently I am employed by Manpower Solutions at the Florida Department of Transportation, I started working there on January 22nd of 2018 and since that time I feel I have done very well, I was promoted from my training within 60 days while the required time is usually around 90 days. In summary my position as a crash Data analyst requires me to locate traffic accidents down to their exact latitude and longitude after the police report has been sent through the FDOT system.']",[u'High school or equivalent'],[u'']
0,https://resumes.indeed.com/resume/9fe39d7a79c3f52a,"[u'Bluetooth Battery Hub\nJanuary 2017 to January 2018\nC# (WPF .NET)\nBluetooth hub that monitors battery levels and displays device information in an interface.\nBattery information can also be displayed in the task bar with notifications.', u'Car Diagnostics Monitor\nJanuary 2017 to January 2017\nPython\nUtilizes car engine data (OBD-II) directed to a Raspberry Pi and then outputted to a display.\nVisualizes information such as engine RPM, mileage, intake temperature, etc.', u'Automated Blinds\nJanuary 2017 to January 2017\nC++\nBlinds that are automated by the time and local forecast. Can be manually operated by a local website. Operated by a Raspberry Pi and a modified servo.', u'Data Analyst\nRB Enterprises\nJanuary 2015 to January 2016\nPart-time\nFiled documents into system to track, organize, and maintain data. Sorted and organized\nstockroom. Analyzed, managed, maintained documents and inventory.\n\nLANGUAGES\nPrimary C++\n\nSecondary C# Java Python\nHTML CSS']",[u'in Computer Science'],[u'University of Washington\nJanuary 2017 to January 2019']
0,https://resumes.indeed.com/resume/e158807f8accf9c8,"[u'Data Analyst\nAberdeen Group - San Diego, CA\nApril 2017 to Present\n\u2022 Managed Data vendor partnerships\n\u2022 Fulfilled specialized custom data requests utilizing MSSQL\n\u2022 Worked closely with CTO to ensure data quality and completeness\n\u2022 Worked closely with Account Manager team to answer client questions and complete renewals', u'Manager, Data Operations\nBrightScope - San Diego, CA\nSeptember 2015 to April 2017\n\u2022 Led team responsible for data quality and custom client requests\n\u2022 Product owner for new DOL Fiduciary Rule Product in development\n\u2022 Managed and completed special analytic requests from clients\n\u2022 Managed operational plan to ensure all responsibilities of the organization completed efficiently\n\u2022 Worked closely with executive team to complete priority projects\n\u2022 Received recognition as Outstanding Performer within the Data Org for 2015', u""Data Analyst / Senior Data Analyst\nBrightScope - San Diego, CA\nOctober 2013 to September 2015\n\u2022 Fulfilled specialized data requests from clients by extracting data through custom SQL queries and Excel including projects worth more than $50k in value\n\u2022 Completed custom research projects for journalists and institutions such as the Wall Street Journal\n\u2022 Extracted key information from audit reports and 5500's to provide data points that help in the ranking of 401(k) plans""]","[u'Master of Business Administration in Business Administration', u'Bachelor of Science in Business Administration']","[u'California Polytechnic State University\nJune 2013', u'California Polytechnic State University\nJune 2011']"
0,https://resumes.indeed.com/resume/17fc58d7eb3fd63f,"[u""Data Analyst Intern\nChina Telecommunication Technology Labs - Beijing, CN\nJuly 2016 to September 2016\n\u2022 Extracted, interpreted and analyzed user behaviors data to identify key metrics.\n\u2022 Transform raw data into meaningful, actionable information that helped increase online sales.\n\u2022 Performed user case studies to provide references for making marketing strategies.\n\u2022 Gathered user feedback to improve the usability of the lab's website.""]","[u'Master of Information Management in Data Analytics', u'Bachelor of Management in Management', u'Bachelor of Science in E-Commerce Engineering']","[u'College Park College Park, MD\nMay 2019', u'Beijing University of Posts and Telecommunication Beijing, CN\nJune 2017', u'Queen Mary University of London\nJune 2017']"
0,https://resumes.indeed.com/resume/0cba10ff592d181f,"[u'Data Analyst\nCareFirst BlueCross BlueShield - Washington, DC\nJanuary 2017 to Present\nAs a Data Analyst, I worked on a Medicaid Management Information System (MMIS) project serving the internal business community (Claims, Billing, Membership, Customer Service, membership management, provider management, advanced Healthcare management) and the project team. I was involved in requirement gathering and working on data extraction from the database.\n\u2022 Build and support the transformation of various Data inputs into the enterprise Data Warehouse.\n\u2022 Analyze & design solutions, create technical specs for integration and consult with developers and QA personnel on their tasks related to the imports and extracts. Troubleshoot existing interface issues.\n\u2022 Responsible for final work product within assigned projects as related to Data, Data design, transformations and quality of results.\n\u2022 Lead successful project to stand up database for Individual Health Insurance Exchange applications and payments. Because of resource constraints, designed solution, created all database objects, wrote specs for integration, exports and reports, and handled Data issues.\n\u2022 Saved cost and hours on a project by revising how we report on certain Data\n\u2022 Conducted Data modeling review sessions for different user groups, participated in requirement sessions to identify requirement feasibility.\n\u2022 Participate in cross-functional task forces to identify and document functional or reporting', u""Data Analyst\nHumana - Washington, DC\nJune 2014 to December 2016\nWorked with Humana's data analytics team in a project that focused on management of finances in care and insurance strategies.\n\u2022 Collected the business requirements from the subject matter experts like data scientists and business partners.\n\u2022 Worked on streaming the analyzed data to Hive Tables using tableau for making it available for visualization and report generation by the BI team.\n\u2022 Extracted the data from RDBMS like Oracle, MySQL.\n\u2022 Used different file formats like Text files, Sequence Files, Avro.\n\u2022 Loaded data from various data sources into HDFS using Kafa.\n\u2022 Transferred data using Informatica tool from AWS.\n\u2022 Create queries in SQL Server Management Studio generate reports to verify all Device exceptions within the organization, making sure they are deployed or decommissioned.\n\u2022 Conduct meetings and email teams to discuss solutions to verify inventory records.\n\u2022 Experience with Centers for Medicare and Medicaid Services, health care provider Data and online Data submission process.\n\u2022 Use state-of-the-art decision support and neutral network tools to detect potential fraud and support investigations organizing case files, research violations and accurately and thoroughly document all steps taken in project development\n\u2022 Used SQL Assistant front-end tool to issue SQL commands matching the business requirements to run reports for Data on Providers.""]","[u'Bachelors of Social Science Linguistics in Social Science Linguistics', u'Bachelors of Science in Cyber Security']","[u'University of Buea', u'University of Maryland University College']"
0,https://resumes.indeed.com/resume/eeb8fd6c1ee8867f,"[u'Data Analyst\nMCLAB LLC - San Francisco, CA\nApril 2017 to March 2018\n\u2022 Identifying suitable laboratory techniques for the collection and analysis of data\n\u2022 Interpret data to draw conclusions for managerial action and strategy\n\u2022 Use statistical hypothesis testing to validate data\n\u2022 Attend meetings and present findings to team\n\u2022 Propose solutions to improve system efficiency', u'Research Assistant\nMCLAB LLC - San Francisco, CA\nJuly 2014 to July 2016\n\u2022 Assist research scientists in R&D department with lab work\n\u2022 Conduct lab experiments and interpret the corresponding results\n\u2022 Contribute to team decisions about research directions\n\u2022 Prepare reports and slides for group meetings\n\u2022 Responsible for some technical tasks, such as webpage updates']","[u'M.S. in Biostatistics', u'B.S. in Cell & Molecular Biology']","[u'California State University Hayward, CA\nSeptember 2016 to Present', u'San Francisco State University San Francisco, CA\nAugust 2010 to May 2014']"
0,https://resumes.indeed.com/resume/bf7fd98fa69449cf,"[u""Analyst\niConsult - Syracuse, NY\nOctober 2017 to Present\niConsult, a non-profit organization providing information management solutions for local businesses/NGO\n\u2022 Interacted with key cross-functional stakeholders to understand project dependencies and led the efforts of providing six sales reports\n\u2022 Developed Tableau reports by integrating multiple data sources like MYSQL, Google Analytics and created\nvisualization for various industry-specific KPI's, reducing reporting time by 15%"", u'Business Data Analyst\nSlangcode - Mumbai, Maharashtra\nDecember 2016 to June 2017\nSlangcode, IT services firm delivering web and analytics solutions for small medium enterprises locally\n\u2022 Developed functional specifications; reporting and data requirements and translated them into technical\nspecifications\n\u2022 Recommended and pioneered a client website re-design by analyzing consumer behavior data to increase\nwebsite visits by 27%\n\u2022 Used advanced MS Excel functions like pivot tables and Power BI to explore, clean and transform data\n\u2022 Delivered multiple analytics projects by utilizing advanced tableau features, got recognized by manager\n\u2022 Supported rapid turnaround for ad-hoc analysis, researched and resolved issues using SQL', u'Analyst Intern\nS&S Infotech - Mumbai, Maharashtra\nJuly 2016 to December 2016\nS&S provides customized software development services and specialized in Business Intelligence reporting\n\u2022 Worked closely with the project management team to analyze and consolidate request requirements to improve the decision planning process, create use-case diagrams, flowchart and perform UAT\n\u2022 Saved 20% of human hours by automating Tableau reports/dashboards which were updated via MS Excel\n\u2022 Built complex SQL reports for conducting survey analysis and analyzed Net Promoter Score']","[u'Master in Information Management', u'Bachelor of Computer Engineering in Computer Engineering']","[u'Syracuse University Syracuse, NY\nMay 2019', u'Mumbai University Mumbai, Maharashtra\nMay 2016']"
0,https://resumes.indeed.com/resume/1d4a310d88414829,"[u""Sr. Data Analyst & ETL Tester\nCapital One Finance - Richmond, VA\nDecember 2016 to Present\nResponsibilities:\n\u2022 Creating ETL processes like procedures, functionalities and packages to provide data for dashboarding and reporting purposes in Oracle Data Integrator\n\u2022 Worked with Business analysts for requirements gathering, business analysis, testing, metrics and project coordination.\n\u2022 Involved in gathering requirements from business users and created Multidimensional model using Erwin.\n\u2022 Understood OLTP Source tables and build Dimension and fact tables accordingly.\n\u2022 Identified all the conformed dimensions to be included in the target warehouse design and confirmed the granularity of the facts in the fact tables.\n\u2022 Created Python scripts to take client content documents and images as input and create web pages, including home page, table of contents and links.\n\u2022 Extracted the business requirements from the stakeholders keeping in mind their need for the application and documented them using Rational ReqPro.\n\u2022 Developed Mappings in Informatica to load the data from various sources into the Data Warehouse using different transformations like Source Qualifier, JAVA, Expression, Normalizer, Lookup, Aggregate, Update Strategy and Joiner.\n\u2022 Worked on advanced Transformations like XML Parser, XML Generator, Application Source Qualifier, Web service, HTTPS, JAVA and SQL\n\u2022 Experience with working on PL/SQL language to create procedures, triggers and functions.\n\u2022 Working in teams to develop and achieve business KPI's.\n\u2022 Designed dimensional models that can reduce development time for Data Warehouse and Tableau Reporting.\n\u2022 Created views in Tableau Desktop that were published to internal team for review and further data analysis and customization using filters and actions.\n\u2022 Developed number of Informatica mappings, mapplets and reusable transformations for the product line.\n\u2022 Configuration, maintenance, technical support and troubleshooting of Informatica ETL Server implementation.\n\u2022 Prepared dashboards using calculations, parameters in Tableau.\n\u2022 Supported users on SharePoint 2003, SharePoint 2007 and SharePoint 2010 issues and fixing them\n\u2022 Performed ETL and Database testing, wrote complex SQL queries, for Data Profiling, Data validation, Source-2-Target Mapping and BI Report Testing\n\u2022 Involved in writing SQL queries for Teradata, using Teradata SQL Assistant to write SQL queries, test cases to validate test results in both front end validation as well as back end data-validation.\n\u2022 Created Master Test Plan, define test strategy, monitor status report documenting change request and coordinating testing activity between QA team, project team and PMO in an Agile environment. Performed VLOOKUP, HLookup, and Range Lookup to extract and analyze data from large records in MS Excel.\n\u2022 Experience in scripting Unix Shell in the process of automation.\n\u2022 Integrated Informatica Data Quality (IDQ) with Informatica PowerCenter and Created POC data quality mappings in Informatica Data Quality tool and imported them into Informatica Powercenter as Mappings, Mapplet\n\u2022 Created Multiple Dimensions and Fact tables. Used concepts of Degenerated dimension, sub-dimension, Fact less fact table, Aggregate fact tables in Multidimensional model, Snowflake schema.\n\u2022 Conducted ETL design review meetings to get approval from Cross functional teams\n\u2022 Organizing and managing meetings between clients, third party vendors, development teams and analysts on a scheduled basis.\n\u2022 Creating API's for database systems for managing and interacting with the DBFS.\n\u2022 Performed complex SQL queries from multiple Tables to exhibit required data into reports.\n\u2022 Involved in the Extraction, Transformation and Load process (ETL) for a Data Warehouse from their legacy systems using Informatica.\n\u2022 Extensively worked on Cloud Adaptation (AWS/AZURE), Analytics (Big DATA, BI), SDLC, Global Data center migration, Audit-Security-Compliance, Mergers/Acquisitions & Third Party Platform.\n\u2022 Worked on multiple disparate Data sources ranging from flat files, XML Files, SQL server databases to load the data into Heterogeneous Target Data Warehouse.\n\u2022 Expensively worked on unstructured data on Big Data systems on JSON filesystem.\n\u2022 Created Reusable Transformations, Mapplets and used them in Mappings to develop the business logic for transforming source data into target.\n\u2022 Worked with Mapping Parameters and Mapping Variables in various mappings to pass the values between sessions\n\u2022 Created Test data and Unit test cases to ensure successful data loading process\n\u2022 Involved in transforming the knowledge about system to Production support team\n\u2022 Supported the System for a period of two months after it went live into Production and helped production support team to come up to the speed.\n\u2022 Helped in creating Production Support Manuals.\n\u2022 Used SAS for coding and data mining of scattered data the scattered data was then compiled and organized to be placed into the Big Data database.\n\u2022 Helped my team to finish project successfully when they lost a key member in final Phase of project\nInvolved in taking some vital decisions along with my team lead in crucial stage of project when one of team member left project."", u'ETL Tester & Data Analyst\nBest Buy - Minneapolis, MN\nOctober 2014 to November 2016\n\u2022 Demonstrated ability to move data between production systems and across multiple platforms.\n\u2022 Worked with reporting / monitoring leveraging different data sources\n\u2022 Worked on Object oriented programming and debugging experience\n\u2022 Worked on AWS platform.\n\u2022 Developed, documented and designed standards and governance for SharePoint sites in the collaboration and portal environments.\n\u2022 Was responsible to perform data analysis using excel functions and SQL queries.\n\u2022 Developed Macros, Pivot Tables etc. using MS Excel extensively.\n\u2022 Developed Dashboards and prepared ad-Hoc Reports in Tableau utilizing parameters, calculated fields and table calculations, user filters, action filters and sets to handle views more efficiently.\n\u2022 Build mappings to pull data from the source & validate the data with source system. Create reports using the loaded data using Tableau.\n\u2022 Tableau Visualizations like Dual Axis, Bar Graphs, Scatter Plots, Pie-Charts, Heat Maps, Bubble Charts, Tree Maps, Funnel Charts, Box Plots, Waterfall Charts, Geographic Visualization and other making use of actions, other local and global filters per the business/client requirements.\n\u2022 Experienced in Informatica data quality (IDQ), power center, data cleansing, data profiling, data quality measurement and data validation processing, Match, Merge, Weightage score, Deduplication process.\n\u2022 Experience in scripting Unix Shell in the process of automation.\n\u2022 Worked with Business analysts for requirements gathering, business analysis, testing, metrics and project coordination.\n\u2022 Worked with Mapping Parameters and Mapping Variables in various mappings to pass the values between sessions\n\u2022 Created Test data and Unit test cases to ensure successful data loading process\n\u2022 Performed Data profiling, data mining and identified the risks involved with data integration to avoid time delays in the project.\n\u2022 Experience in MS SQL Server Business Intelligence in MS SQL Server Integration Services (SSIS), MS SQL Server Reporting Services (SSRS), and MS SQL Server Analysis Services (SSAS).\n\u2022 Worked with other developers to match company deadlines, using Java and Ruby on Rails, MySQL, MongoDB, Redis, Jquery, Agile, Git, Ajax etc\n\u2022 Experienced in designing high level ETL architecture, created, configured, and fined tuned ETL workflows designed in DTS for overall data transfer from the OLTP to OLAP with the help of SSIS.\n\u2022 Expert in designing and scheduling complex SSIS Packages for transferring data manually from multiple data sources like SQL Server 2000, Oracle Database, Excel, Flat file, Oracle to Microsoft SQL Server by using SSIS and DTS utility to SQL server\n\u2022 Experience in UNIX shell scripting, CRON, FTP and file management in various UNIX environments.\n\u2022 Created Test data and Unit test cases to ensure successful data loading process\n\u2022 Extensive Knowledge on Source to Target Mapping , Data Warehousing , Data Profiling and Data Cleansing\n\u2022 Extensive knowledge on Unix Operating system.\n\u2022 Extensive knowledge in writing SQL Queries for Data Validation, Data Integration and testing.\n\u2022 Extensive knowledge in developing ETL for Data extraction, Data transformation and loading.', u'Data Analyst\nMinneapolis, MN\nJanuary 2012 to September 2013\n\u2022 Gathered and translated business requirements into detailed reporting requirements for Data warehousing.\n\u2022 Participated in JAD sessions involving the discussion of various reporting needs.\n\u2022 Worked with the Business Analyst, QA team in their testing and DBA for requirements gathering, business analysis, testing and project coordination.\n\u2022 Developed a Conceptual models using ERWIN based on requirements analysis.\n\u2022 Created ER diagrams (Entity Relationship diagrams) using ERWIN and created logical and physical models as well.\n\u2022 Created dimensional model for the reporting system by identifying required dimensions and facts using ERWIN.\n\u2022 Used forward engineering to generate DDL from the Physical Data Model and handed it to the DBA.\n\u2022 Executed DDL to create databases, tables and views.\n\u2022 Analyze the data based on requirements and wrote down techno functional documents and developed complex mappings using Informatica data quality (IDQ) 9.6.1 used to remove the noises of data using parser, labeler, standardization, merge, match, case conversion, consolidation, lookup etc. Transformations and performed unit testing of accuracy of data.\n\u2022 Designed a STAR schema for the detailed data marts and Plan data marts involving Conformed dimensions.\n\u2022 Identified and tracked slowly changing dimensions and determined the hierarchies in dimensions.\n\u2022 Conducted design discussions and meetings to come out with the appropriate Data Mart at the lowest level of grain for each of the Dimensions involved.\n\u2022 Reverse engineered the reports and identified the Data Elements (in the source systems), Dimensions, Facts and Measures required for reports.\n\u2022 Conducted updates and enhancements to the existing data models as required using ERWIN.\n\u2022 Experienced in generating and documenting Metadata while designing OLTP and OLAP database environments.\n\u2022 Created and maintained the Data Model repository as per company standards.\n\u2022 Maintained Referential Integrity by adding foreign keys and normalizing the existing data structure.\n\u2022 Collaborated with ETL, BI and DBA teams to analyze and provide solutions to data issues and other challenges while implementing the OLAP model.\n\u2022 Developed and maintained data dictionary to create metadata reports for technical and business purpose.\n\u2022 Created action plans to track and identify open issues and action items related to the project.\n\u2022 Prepared analytical and status reports and updated the project plan as required.', u""Data Analyst\nVerizon - Atlanta, GA\nJanuary 2011 to December 2011\n\u2022 Gathered and translated business requirements into detailed, production-level technical specifications detailing new features and enhancements to existing business functionality.\n\u2022 Developed project plan with project manager's assistance for the first two phases of the project.\n\u2022 Developed system flow and data flow diagrams for the proposed system.\n\u2022 Designed conceptual and logical data models.\n\u2022 Identified objects and relationships and how those all fit together as logical entities, these are then translated into physical design using the forward engineering in ERwin tool.\n\u2022 Involved in the critical design review of the finalized database model.\n\u2022 Worked with data compliance teams, Data governance team to maintain data models, Metadata, Data Dictionaries; define source fields and its definitions\n\u2022 Assisted in creating a High Level Request for Proposal (RFP)\n\u2022 Developed test plans and test cases for QA Unit Testing, System Testing and Enterprise testing.\n\u2022 Managed the project in breaking down the existing EDW to manageable data marts.\n\u2022 Involved in designing and implementing the security for the databases.\n\u2022 Conducted JAD sessions for communicating with Stakeholders and Project directors.\n\u2022 Generated SQL scripts and implemented the relevant databases with related properties from keys, constraints, indexes & sequences.\n\u2022 Played a key role in the planning, User Accepted testing, and Implementation of system enhancements and conversions\n\u2022 Assigned tasks among development team, monitored and tracked progress of project following Agile methodology.\n\u2022 Conducted user interviews, gathered requirements, analysed the requirements by using Rational Rose, Visio and Requisite pro -RTC Designed and developed Use Cases, Activity Diagrams, Sequence Diagrams, and Process Flows\n\u2022 Tracked and managed all Change Requests, using Rational Clear Quest\n\u2022 Was responsible for development and testing of conversion programs for importing data from text files into Oracle database utilizing PERL shell scripts & SQL*Loader.\n\u2022 Tested Performance & Accuracy related queries under SQL Server and Teradata.\n\u2022 Involved in the daily maintenance of the database that involved monitoring the daily run of the scripts as well as troubleshooting in the event of any errors in the entire process.""]","[u""Master's""]",[u'']
0,https://resumes.indeed.com/resume/f1f2e7d49bf906ad,"[u'Data Analyst\nAccenture\nDecember 2013 to July 2017\n\u2022 Analyzed financial and sales datasets to provide strategic direction to the clients and improved the revenue by 8 %\n\u2022 Built and constructed prototypes, proof of concepts and designed data models as per the business requirements\n\u2022 ETL Data cleansing, integration, and transformation using SAP BW: Responsible for managing data from disparate sources\n\u2022 Developed and designed reports and dashboards for supporting the business analysis using Business Objects, Crystal Reports, Xcelsius\n\u2022 Performed Data Profiling to learn about user behavior and merge data from multiple data sources\n\u2022 Designed and developed various machine learning frameworks using Python and R\n\u2022 Worked on different data formats such as JSON, XML and performed machine learning algorithms in Python.\n\u2022 Created customized reports and processes in SAS and Tableau Desktop']","[u'Master of Science in Business Analytics and Information Systems', u'Bachelor of Technology in Electronics and Communication Engineering']","[u'University of South Florida Tampa, FL\nDecember 2018', u'Institute of Technology\nMay 2013']"
0,https://resumes.indeed.com/resume/6e4725b0d4a26021,"[u""Financial Analyst /Data Analyst\nShenyang Private Lending Register Center - Shenyang, CN\nMarch 2017 to July 2017\nAnalyzed and researched 12 different financial products which are came from Fortune Global 500 Corps, such as China Construction Bank,\nChina Life Insurance, People's Insurance Co. of China, and CEFC China Energy. Recorded characteristics of each financial product, and advised to customers\n\u2022 Assisted 70+ private lenders to invest appropriate financial products; Assisted 100+ customers to evaluate lending risks & calculated the capital return\n\u2022 Updated client databases; Analyzed market data, and financial activities using Excel modeling (Pivot Tables, Formulas, etc)\n\u2022 Developed detailed spreadsheets to identify trends and develop forecasts through game theory\n\u2022 Analyzed and identified the accuracy of corporation's information include (Online reviews, Capital return, Market liquidity, etc)\n\u2022 Contributed to 10% growth trend by developing market strategic recommendations"", u'Data Analyst / Marketing Analyst\nChengtai Inc - Mission Viejo, CA\nFebruary 2013 to February 2014\nAnalyzed the supply & demand landscape of Chinese Hearing Aid industry and recommended M&A opportunities for international\nexpansion and subscription strategies for a joint-venture network.\n\u2022 Utilized Pivot Tables to perform detailed analysis on current customer territory to effectively penetrate new and existing growth\nopportunities\n\u2022 Researched and offered companies insights on market trends, competitors and international opportunities\n\u2022 Analyzed products sales performance data to identify and define potential issues, collaborated with other members of sales team to develop\npresentations on analysis for management team\n\u2022 Collaborated with internal partners to resolve international distribution issues; conducted competitor, customer segmentation & retention\nanalysis']","[u'Certification', u'Associate Degree in Statistic and Transfer in General Education', u'']","[u'Extension program\nJune 2017 to September 2018', u'Santa Monica College Santa Monica, CA\nSeptember 2012 to June 2015', u'University of California Los Angeles, CA']"
0,https://resumes.indeed.com/resume/62adeb46c963f0ed,"[u'Sr. Data Analyst\nCiti Bank - New York, NY\nFebruary 2017 to March 2018\nResponsibilities:\n\u2022 Interacted with different lines of business users for gathering requirements and prepared technical design documents for the business use cases. Provide thought leadership in specific subject matters with other financial team members to drive improvements to financial status and increase productivity.\n\u2022 Worked on importing data from various sources such as Oracle, SQL Server and performed transformations using HiveQL to load data into HDFS.\n\u2022 Load and transform large data sets of structured, semi -structured and unstructured data using Hadoop concepts based in real-time.\n\u2022 Involved in designing, building, and deploying Hadoop/Hive cluster utilizing AWS stack (Including EC2, S3 and EMR), focusing on high-availability, fault tolerance, and auto-scaling.\n\u2022 Conducted multi-dimensional analysis on revenue data based on multiple predefined metrics and worked extensively with data warehouses in HDFS and MongoDB clusters to create detailed summary report using interactive dashboards.\n\u2022 Experience with the technologies like MongoDB, SQL Server in writing custom SQL queries and working with large data sets and optimize performance within the database using Tableau API.\n\u2022 Experience with creating script for data modeling and data import and export. Extensive experience in deploying, managing, and developing MongoDB and HDFS clusters.\n\u2022 Carried out data quality improvements, data organization, metadata, and data profiling. Involved technical support on data warehouse teams. Demonstrated ability to move from one sequential assignment to the next.\n\u2022 Strong understanding of advanced Tableau features including calculated fields, parameters, table calculations, row-level security, joins, data blending, and dashboard actions.\n\u2022 Building, publishing customized interactive reports and dashboards, report scheduling using Tableau server.\n\u2022 Generated context filters and data source filters while handling huge volume of data and created multiple extracts to quickly access historical data across MongoDB clusters.\n\u2022 Generated Tableau dashboards for sales with forecast and reference lines and generated interactive dashboards with an implementation of data blending on databases.\n\u2022 Created data queries and reports using MongoDB, SQL Server Management Studio, SSRS, and Excel. Created custom queries/reports designed for quality verification and information sharing.\n\u2022 Designed and deployed rich graphic visualizations for financial data with drill-down and drop-down options and associated parameters using Tableau 9 and QlikView.\n\u2022 Developed complex SQL queries to create temporary table in staging databases to support various Tableau reports.\nEnvironment: Hadoop, Hive, MongoDB, AWS EC2, S3, EMR, Tableau 9.x, QlikView, Data Analytics, Oracle 11g, MS Excel, SQL Server, Advanced Database queries', u'Data Analyst\nKey Bank - Chicago, IL\nMarch 2015 to May 2016\nResponsibilities:\n\u2022 Experienced in analyzing source data, creating use case diagrams and documented user stories based on business specifications from users.\n\u2022 Developed various MongoDB database concepts such as locking, transactions, indexes, sharding, replication and schema design.\n\u2022 Collaborated with the development team in planning, defining, and designing functional requirements based on the specification given by Business.\n\u2022 Performed entire analytics cycle from data extraction/scraping, data integration with identifying errors and data inconsistencies and data integration and modelling from various data sources (RDBMS, CSV, Spreadsheets, etc.).\n\u2022 Carried out complex data integrations from various data sources and created complex data structures through modelling. Also, carried out thorough data cleansing and validation before creating any target mappings.\n\u2022 Designed and developed various analytical reports from multiple data sources by blending data on a single worksheet in Tableau.\n\u2022 Worked on building queries to retrieve data into Tableau from MongoDB and Hadoop clusters by developing various scripts for fetching/loading using API connectors.\n\u2022 Worked on creating different type of indexes based on different collections to get good performance in MongoDB and analyzed chunk migrations, splitting, and balancing across the cluster.\n\u2022 Experience in configuring reports from different data sources using data blending and creating users, groups, projects, Data connections, settings as a Tableau administrator.\n\u2022 Designed, developed, implemented, and supported Tableau and Power BI dashboards. Integrated data sources and databases with Tableau desktop and designed and developed data models and backend queries for presenting data.\n\u2022 Conducted exploratory data analysis using python NumPy and Seaborn to see the insights of data and validate each feature through different charts and graphs.\n\u2022 Built predictive models including Linear regression, Logistic regression with Moderation, Random Forest Regression and Support Vector Regression to predict the Customer churn by using Python scikit-learn.\n\u2022 Maintained existing data migration program with occasional upgrades and enhancements and performed data migration enforcement tasks.\n\u2022 Created visually impactful dashboards in Tableau and Power BI to develop an understanding of critical issues occurring in various groups and presented data that helped to track progress of issue and decide the next action plan.\nEnvironment: Hadoop, Tableau 8.x, Power BI, AWS EC2, MongoDB, Machine Learning, Python, Data Analytics, ETL, Oracle 10g, SQL Server, Advanced Database queries', u'Business Data Analyst\nAllstate Insurance - Chicago, IL\nSeptember 2013 to February 2015\nResponsibilities:\n\u2022 Coordinate with SMEs, business partners, technology group and others via JAD session to develop clear and accurate reporting and process maps.\n\u2022 Conducted reverse engineering based on demo reports to understand the data without documentation to connect to existing database and create graphical representation (ER diagram).\n\u2022 Generated different Data Marts for gathering the tables needed (Member info, Claim info, Transaction info, Appointment info, Diagnose info) from various databases such as Teradata, SQL Server.\n\u2022 Performed Data cleansing and Data profiling for detecting and correcting inaccurate data from the databases and to track data quality and to assess the risk involved in integrating data for new applications.\n\u2022 Designed and implemented Entity Relationship Diagrams, grouped and created the tables, validated the data, Primary Keys in lookup tables were identified.\n\u2022 Redefined many attributes and relationships in the reverse engineered model and cleansed unwanted tables/columns as part of data analysis responsibilities.\n\u2022 Data type inconsistencies between source systems and the target system were resolved using Mapping Documents.\n\u2022 Involved using ETL tool Informatica to populate the database, data transformation from the old database to the new database.\n\u2022 Experienced in generating and documenting Metadata while designing OLTP and OLAP system environment.\n\u2022 Worked on database features and objects such as partitioning, change data capture, indexes, views, indexed views to develop optimal physical data model.\n\u2022 Identified and tracked the slowly changing dimensions, heterogeneous sources and determined the hierarchies in dimensions.\n\u2022 Generated ad-hoc reports using SQL queries using joins, database links to fetch data from legacy Oracle and SQL Server database systems.\n\u2022 Conducted exploratory data analysis using R Studio and SAS packages to see the insights of data and validate each feature through different charts and graphs.\n\u2022 Created Use-Cases and Business Use-Case Model after accessing the status and scope of the project and thoroughly understanding the business processes.\nEnvironment: Erwin, OLTP, OLAP, Data Marts, Informatica Data Quality 8.x, SQL Server, Teradata, Data Analytics, T-SQL, Advanced Database queries.', u""Business Data Analyst\nAnthem Health Insurance - Indianapolis, IN\nMarch 2012 to August 2013\nResponsibilities:\n\u2022 Conducted JAD sessions to clear understanding of the business requirements and gather the scope and limitations on the scope of the project.\n\u2022 Interacted with SME's, asking detailed questions and carefully recording the requirements in a format that can be reviewed and understood by both business people and technical people.\n\u2022 Designed and Developed Oracle PL/SQL Procedures and UNIX Shell Scripts for Data Import/Export and data Conversions.\n\u2022 Performed legacy application data cleansing, data anomaly resolution and developed cleansing rule sets for ongoing cleansing and data synchronization.\n\u2022 Extensively used Star Schema methodologies in building and designing the logical data model into Dimensional Models.\n\u2022 Involved in project cycle plan for the data warehouse, source data analysis, data extraction process, transformation and loading strategy designing.\n\u2022 Worked on Conceptual, Logical Modeling and Physical Database design for OLTP and OLAP systems.\n\u2022 Designed a Star schema for sales data involving shared dimensions (Conformed) using Erwin Data Modeler.\n\u2022 Designed and build the OLAP cubes using Star schema and Snow Flake Schema using native OLAP Service Manager.\n\u2022 Maintain and enhance data model with changes and furnish with definitions, notes, reference values and check lists.\n\u2022 Extensively used Teradata utilities (BTEQ, Fast load, Multiload, TPUMP) to import/export and load the data from oracle, flat files.\n\u2022 Designed and published visually rich and intuitively interactive excel workbooks and dashboards for executive decision. making.\nEnvironment MS BI (SSIS, SSRS), Tableau 8.x, OLAP, OLTP, Data Analytics, ETL, SQL Server 2008, SSMS, Advanced Data base queries"", u'Data Analyst\nMetlife Insurance - Raleigh, NC\nMay 2011 to February 2012\nResponsibilities:\n\u2022 Involved in gathering user/project requirements from business users and IT managers, translated it into functional and non-functional specifications needed and created documentations for the project.\n\u2022 Assisted in design and data modeling efforts of Data Marts and Enterprise Data Warehouse.\n\u2022 Used T-SQL in SQL Server to develop complex stored procedures, triggers, clustered index & non-clustered index, Views, and User-defined Functions (UDFs).\n\u2022 Designed SSIS packages to extract, transform and load existing data into SQL Server, used lots of components of SSIS, such as Pivot Transformation, Fuzzy Lookup, Derived Columns, Condition Split, Aggregate, Execute SQL Task, Data Flow Task and Execute Package Task.\n\u2022 Created SSIS Packages that involved dealing with different source formats (Text files, XML, Database Tables)\n\u2022 Debugged and troubleshot the ETL packages by using breakpoint, analyzing process, catching error information by SQL command in SSIS.\n\u2022 Create reports with the use of SSRS to generate different types of reports such as tabular, matrix, drill down and charts reports with accordance with user requirement.\n\u2022 Maintained and updated existing reports, analyzed the SQL queries and logic behind them to improve the performance.\n\u2022 Helped deploy the report with scheduling, subscription, history snapshot configured and set up.\n\u2022 Involved in reviewing the Test Plan, Test Execution, Issue Resolution and Report Generation to assure that all aspects of a project follow the Business Requirements.\nEnvironment: MSBI (SSIS, SSRS), Data Marts, Data Analytics, ETL, SQL Server, T-SQL, Database queries']",[],[]
0,https://resumes.indeed.com/resume/5e92c154986d22b6,"[u'Data Analyst\nStanford University\nSeptember 2017 to Present\n\u2022 Working on Big Data Cleaning, Mapping, Web Scrapping and data validation to maintain the integrity of the data\n\u2022 Analyzing the states data to identify the patterns and problem solving.\n\u2022 Working on different projects internally to clean, manage and find the insights from the data.\n\u2022 Coordinated with the engineers and manager to present the insights from the data set.\n\u2022 Eliminating variations in data and coordinating with team for the missing data and additional information required for analysis.\n\u2022 Using Stata, Excel and Python packages for writing the scripts ( Pandas, numpy packages)', u'Student Assistant\nUniversity of California - Santa Cruz, CA\nSeptember 2016 to June 2017\n\u2022 Helped in conducting exams for Social Sciences Division at University of California, Santa Cruz.\n\u2022 Providing appropriate escalation of any issue or incident and providing a written report of the test event at the end of the event according to conditions and requirements established by the Disability Resource Center (DRC) and the instructor for the course.\n\u2022 Verifying the arrangement of testing rooms are to specification and are appropriate for administering exams.', u""Research Analyst\nJMC\nAugust 2014 to May 2015\n\u2022 Provided analysis and feedback on ways to collect, distribute, and compare data on expert forecasts on experimental outcomes\n\u2022 Collected historical data from the empirical studies of advertising elasticity and used pivot tables to create a management dashboard tracking\nconsumers demand.\n\u2022 Executed financial testing on the accuracy of consumer's billings and accounts receivables for over 5000 consumers\n\u2022 Developed financial models to project future revenue of different pricing scenarios to facilitate decision-making process for senior management regarding launching new product and entering new markets\n\u2022 Partnered with Marketing and Operations, developed and maintained reports on KPI metrics for capturing insights on customer shopping\nbehaviors, delivery accuracy and order fulfillment; performed data validation, troubleshot issues.\n\u2022 Predicted future advertising elasticities by building predictive models in R (Linear and exponential models)\n\u2022 Managed research budget and financial data for investigators and communicate with other departments or vendor to solve potential issues.\n\u2022 Conducted daily analysis by market researching and extracted findings to excel forms to reports"", u'Analyst\nThe Blind Relief Association\nAugust 2012 to July 2014\n\u2022 Drove and coordinated budgeting, planning and cost processes\n\u2022 Assisted manager with internal corporate audit of personal files\n\u2022 Maintained and analyzed ledger accounts and develop financial statements and reports\n\u2022 Evaluated relations between Finance and other external business partners\n\u2022 Performed monthly Cycle Count for Equipment and Inventory to internal control\n\u2022 Worked on process improvement activities to develop & automate business metrics & various KPIs for weekly dashboards\n\u2022 Communicated financial and business results including variance analyses to man']","[u'Masters in Applied Economics & Finance', u'Bachelor of Commerce in Commerce']","[u'University of California Santa Cruz, CA\nSeptember 2016 to June 2017', u'Jesus and Mary College, University of Delhi\nJuly 2012 to June 2015']"
0,https://resumes.indeed.com/resume/021354fe2b6f1497,"[u'Sr. Data Modeler / Data Analyst\nUnited Technologies - Louisville, KY\nJuly 2016 to Present\nUnited Technologies Corporation (UTC) is an American multinational conglomerate headquartered in Farmington, Connecticut. It researches, develops, and manufactures products in numerous areas, including aircraft engines, aerospace systems, HVAC, elevators and escalators, fire and security, building systems, and industrial products, among others. UTC is also a large military contractor, getting about 10% of its revenue from the U.S. government.\n\nResponsibilities:\n\u25cf Gathered and translated business requirements into detailed, production-level technical specifications, new features, and enhancements to existing technical business functionality.\n\u25cf Created OLTP, ODS and OLAP architectures\n\u25cf Responsible for the analysis of business requirements and design implementation of the business solution.\n\u25cf Developed Conceptual, Logical and Physical data models for central model consolidation.\n\u25cf Worked with DBAs to create a best fit physical data model from the logical data model.\n\u25cf Conducted data modeling JAD sessions and communicated data-related standards.\n\u25cf Used Erwin for effective model management of sharing, dividing and reusing model information and design for productivity improvement.\n\u25cf Redefined many attributes and relationships in the reverse engineered model and cleansed unwanted tables/ columns as part of data analysis responsibilities.\n\u25cf Developed process methodology for the Reverse Engineering phase of the project.\n\u25cf Involved using ETL tool Informatica to populate the database, data transformation from the old database to the new database using Oracle.\n\u25cf Involved in the creation, maintenance of Data Warehouse and repositories containing Metadata.\n\u25cf Involved in the critical design review of the finalized database model.\n\u25cf Involved in the study of the business logic and understanding the physical system and the terms and condition for database.\n\u25cf Created documentation and test cases, worked with users for new module enhancements and testing.\nEnvironment: Erwin, Informatica, Teradata, Oracle, SQL Server, MS Excel, MS Visio', u""Data Modeler/ Data Analyst\nIntuitive Surgical Inc - Sunnyvale, CA\nJanuary 2015 to June 2016\nIntuitive Surgical, maker of the da Vinci Surgical System, is committed to developing robotic-assisted technologies, tools and services that bring enhanced predictability to surgery. Worked as a Data Modeler on Data warehouse. Application based on Oracle database.\n\nResponsibilities:\n\u25cf Complete study of the in-house requirements for the data warehouse. Analyzed the DW project database requirements from the users in terms of the dimensions they want to measure and the facts for which the dimensions need to be analyzed.\n\u25cf Prepared the questioners to users and created various templates for dimensions and facts.\n\u25cf Conducting user interviews, gathering requirements, analyzing the requirements using Rational Rose, Requisite pro RUP\n\u25cf Designed and developed Use Cases, Activity Diagrams, Sequence Diagrams, OOD (Object oriented Design) using UML and Visio\n\u25cf Created logical data model from the conceptual model and it's conversion into the physical database design using ERwin\n\u25cf Created dimensional model based on star schemas and designed them using ERwin.\n\u25cf Used ERwin for creating tables using Forward Engineering\n\u25cf Assist with user testing of systems, developing and maintaining quality procedures, and ensuring that appropriate documentation is in place\n\u25cf Responsible for identifying and documenting business rules and creating detailed Use Cases\n\u25cf Defined the naming standards for data warehouse\n\u25cf Developed SQL Queries to fetch complex data from different tables in remote databases using joins, database links and Bulk collects.\n\u25cf Maintained warehouse metadata, naming standards and warehouse standards.\n\u25cf Develop the test plan, test conditions and test cases to be used in testing based on business requirements, technical specifications and/or product knowledge.\n\u25cf Collected the information about the existing ODS by reverse engineering the ODS.\n\u25cf Identified/documented data sources and transformation rules required to populate and maintain data warehouse content.\n\u25cf Assisted in designing the overall ETL strategy\n\u25cf Generated DDL statements for the creation of new Sybase objects like table, views, indexes, packages and stored procedures.\n\u25cf Created DataStage Server jobs to load data from sequential files, flat files and MS Access\n\u25cf Used DataStage Manager for importing metadata from repository, new job categories and creating new data elements\n\u25cf Used the DataStage Designer to design and develop jobs for extracting, cleansing, transforming, integrating, and loading data into different Data Marts.\nEnvironment: Erwin, Oracle, SQL server, MS Excel, MS Visio"", u""Data Modeler / Data Analyst\nNavistar International - Lisle, IL\nApril 2012 to December 2014\nNavistar offers one of the world's premier and most trusted truck brands. International Truck offers state-of-the-art performance and solutions for a variety of global markets. International Truck, the flagship vehicle brand, is supported by the industry's largest dealer network and offers a complete lineup of integrated vehicles.\n\nResponsibilities:\n\u25cf Involved with the Business Analysts' team in requirements gathering and based on provided business requirements, defined detailed Technical specification documents.\n\u25cf Developed Data Mapping, Data Governance, Transformation and Cleansing rules for the Master Data\n\u25cf Management Architecture involving OLTP, ODS and OLAP.\n\u25cf Created Logical and Physical Data Models by using Erwin r7.3 based on requirements analysis.\n\u25cf Developed normalized Logical and Physical database models to design OLTP system for banking applications.\n\u25cf Analyzed the Business information requirements and examined the OLAP source systems to identify the measures, dimensions and facts required for the reports.\n\u25cf Performed the data source mapping.\n\u25cf Utilized Power Designer's forward/reverse engineering tools and target database schema conversion process.\n\u25cf Documented logical, physical, relational and dimensional data models.\n\u25cf Designed the data marts in dimensional data modeling using star and snowflake schemas.\n\u25cf Redefined attributes and relationships in the model and cleansed unwanted tables/columns as part of data analysis responsibilities.\n\u25cf Review the data model with functional and technical team.\n\u25cf Worked on the reporting requirements and involved in generating the reports for the Data Model.\n\u25cf Assist developers, ETL, BI team and end users to understand the data model.\nEnvironments: ERWIN, DB2, Teradata, Oracle, Microsoft Visio."", u'Data Modeler/Data Analyst\nLAUSD, LA, CA\nDecember 2010 to March 2012\nLos Angeles Unified School District is the largest public school system in the U.S. state of California and the 2nd largest public school district in the United States.\n\nResponsibilities:\n\u25cf Involved in gathering user requirements along with business analysts.\n\u25cf Expanded Physical Data Model (PDM) for the OLTP application using Power Designer.\n\u25cf Transformed project data requirements into project data models.\n\u25cf Responsible as a member of development team to provide business data requirements analysis services, producing Logical and Physical data models using Power Designer 12.0.\n\u25cf Conducted data modeling JAD sessions and communicated data-related standards.\n\u25cf Facilitated transition of logical data models into the physical database design and recommended technical approaches for good data management practices.\n\u25cf Worked with DBA group to create Best-Fit Physical Data Model from the Logical Data Model using Forward Engineering.\n\u25cf Involved in designing Data marts by using Star Schema and Snowflake schema.\n\u25cf Closely worked with ETL process development team.\n\u25cf Extensive system study, design, development and testing were carried out in the Oracle environment to meet the customer requirements.\n\u25cf Developed Data Migration and Cleansing rules for the Integration Architecture (OLTP, ODS, DW)\n\u25cf Created documentation and test cases, worked with users for new module enhancements and testing.\nEnvironment: Erwin, Teradata, Oracle, SSIS, Informatica Power Center', u'Data Modeler / Data Analyst\nOriental Bank of Commerce\nAugust 2009 to September 2010\nOriental Bank of Commerce (OBC) Group bids loans, advice, and an array of customized resources towards many growing countries and countries within transition. As part of this assignment I was involved in multiple projects enhancing the existing relational database and designing the proposed Data Marts on top of existing enterprise data warehouse as per the reporting needs.\n\nResponsibilities:\n\u25cf The Data Mart (BMDM) system stores marketing data in an Oracle relational database and is interrogated using the end-user reporting tools.\n\u25cf Responsible for data modeling and building a star schema model in Erwin.\n\u25cf Identified the fact tables, dimensions and the key measures.\n\u25cf Installed and configured MS SQL Server\n\u25cf Created and Scheduled jobs on MS SQL Server to transfer data.\n\u25cf Used star schema methodology for the modeling of the data.\n\u25cf Performed Data Profiling to identify data issues upfront, provided SQL prototypes to confirm the business logic provided prior to the development.\n\u25cf In creation of the data mart new functionality had to be added like Trend data Inclusion, Product Profitability Calculation \\ Reporting, Concept Reporting, Size Of Line Reporting, Global Brand Grouping \\ Reporting etc.\n\u25cf Assisted data warehouse project team in extracting business rules.\n\u25cf Created an enterprise data dictionary and maintained standards documentation.\n\u25cf Analysis of data requirements and translation of these into dimensional data models.\n\u25cf Interacted with the end users and found out the performance of the database and also involved in developing test cases.\nEnvironment: Erwin, Teradata, SQL Server', u'Business Analyst / Data Analyst\nTech Vedika\nAugust 2008 to July 2009\nTechvedika provides IT services across the globe and also expertise in web app development services, mobile app development services, Big Data Solutions, Cloud Computing Services, Technology Consulting Services, UE Design, QA & Testing.\n\nResponsibilities:\n\u25cf Gathered business requirements through interviews, surveys, prototyping and observing from account managers and UI (User Interface) of the existing Broker Portal system.\n\u25cf Wrote and automated tools and scripts to increase departmental efficiency and automate repeatable tasks.\n\u25cf Wrote SQL Queries using Joins, Sub Queries and correlated sub Queries to retrieve data from the database.\n\u25cf Used ERwin for creating tables using Forward Engineering\n\u25cf Involved in modifying various existing packages, Procedures, functions, triggers according to the new business needs.\n\u25cf Created and Scheduled jobs on MS SQL Server to transfer data.\n\u25cf Used star schema methodology for the modeling of the data.\n\u25cf Used SQL Loader to upload the information into the database and using UTL_FILE packages write data to files.\n\u25cf Involved in the development backend code, altered tables to add new columns, Constraints, Sequences and Indexes as per business requirements.\n\u25cf Performed analysis and presented results using SQL, SSIS, Excel, and Visual Basic scripts.\n\u25cf Created Database Objects like tables, Views, sequences, Synonyms, Stored Procedures, functions, Packages, Cursors, Ref Cursor and Triggers.\n\u25cf Designed and Populated specific tables, databases for collection, tracking and reporting of data.\n\u25cf Developed and deployed quality T-SQL codes, stored procedures, views, functions, triggers and jobs.\n\u25cf Documented and maintained database system specifications, diagrams, and connectivity charts.\nEnvironment: Erwin, SQL Server, Microsoft Excel, Oracle, TOAD, Windows 7', u'Data Analyst Intern\nMahindra & Mahindra - IN\nAugust 2007 to July 2008\nMahindra & Mahindra Ltd, is a group of companies bound together by a common purpose: To Enable People to Rise. Our operations span more than 20 key industries, from Mobility to Rural Prosperity and IT, from Financial Services to Clean Energy.\n\nResponsibilities:\n\u25cf production orders and purchasing activities related to Inventory\n\n\u25cf Developed queries using PL/SQL for interacting with multiple tables\n\n\u25cf Created ad-hoc reports by analysing business requirements using Base SAS and SAS EG\n\n\u25cf Developed decision trees, CHAID and CART models using SAS EM to predict inventory and Stock.']",[u'BE in Computer Science'],[u'Manipal Institute of Technology']
0,https://resumes.indeed.com/resume/8405a44ed6fd1743,"[u'Data Analyst\nOrakris Solutions - Hyderabad-Deccan, Telangana\nJune 2015 to December 2015\n-> Worked on Microsoft Excel.\n-> Interacting with Customers and Clients.\n-> Excellent communication skills.\n-> Leading the team of 10 people for a Mobile Application Development.']","[u""Master's in Information Assurance"", u""Bachelor's in Computer Science""]","[u'Wilmington University Newark, DE\nMay 2016 to January 2018', u'JNTUH Hyderabad, Telangana\nSeptember 2011 to May 2015']"
0,https://resumes.indeed.com/resume/65b31f51fa5294cf,"[u'Data Analyst\nFIDELITY INVESTMENTS - Boston, MA\nJanuary 2017 to June 2017\n\u2022 Devised processes to automate the data load and refresh process for various digital marketing dashboards\n\u2022 Improved query performance by 20% by re-creating data source mapping and integrating ETL with data-quality check applications\nusing Informatica\n\u2022 Reduced time spent on solving reported issues in databases by 3 times by designing and developing executive dashboards for data\ngovernance team using Qlik which indicated data health of domains and quality metrics of Fidelity technology groups\n\u2022 Developed data models and ran scripts in Oracle SQL developer to store data-quality metadata metrics for data governance team\n\u2022 Conducted parallel load testing on 322+ RDBMS tables (Size: 100+GB) and built 115 Qlik View Documents and data models to design proof of concept for Qlik platform performance to migrate 100+ analysts in marketing group from Oracle to Qlik tool\n\u2022 Collaborated with Data Science team to connect Hadoop and AWS platforms to various reporting tools for decision scientists', u""Statistical Analyst\nNORTHEASTERN UNIVERSITY - Boston, MA\nJanuary 2016 to July 2016\n\u2022 Increased response rate by 35% by creating and implementing survey methodology to collect and store research data\n\u2022 Improved workflow by writing statistical analysis plans, designing virtual team training experiments for students, including data\ncollection methods, sample size calculations and ran statistical tests using SPSS and explored relationships among variables\n\u2022 Applied logistic regression models to existing surveys to anticipate student's responses for different experiments based on their\npreferences of using virtual devices (i.e. Skype, Google Hangouts, Facetime etc.) for the meeting\n\u2022 Performed analysis of large datasets for different experiments and presented insights using Excel and Tableau""]","[u'MS in Engineering Management in Data Science', u'Bachelor of Science in Mechanical Engineering in Mechanical Engineering']","[u'NORTHEASTERN UNIVERSITY Boston, MA\nDecember 2017', u'PANDIT DEENDAYAL PETROLEUM UNIVERSITY Gandhinagar, Gujarat\nMay 2015']"
0,https://resumes.indeed.com/resume/74440da187f9def9,"[u'Data Analyst\nNewGen Strategies & Solutions\nJanuary 2018 to Present\n\u2022 Design and develop an ETL process to incorporate source data into the Data Warehouse for reporting and analytics\n\u2022 Analyze historical data and transform large data sets to consumable format using SQL, RDMS, excel and Power BI\n\u2022 Design, develop, modify, support, debug and evaluate existing production and/or ad-hoc reports from various systems\n\u2022 Create dashboards and visualizations using Power BI to highlight strategic business metrics and actionable insights', u'IT Programmer/Analyst\nCity of Fort Worth\nMay 2017 to December 2017\n\u2022 Implemented BPM solution to move from paper-based forms environment to fully automated electronic environment\n\u2022 Re-engineered current manual processes to streamline business practices; remove redundancies and improve efficiencies\n\u2022 Assisted customers with the BPM application BP Logix Process Director 4 and provided ad-hoc support post deployment', u'Business Analyst\nOcwen Financial Solutions Pvt Ltd\nSeptember 2014 to July 2016\n\u2022 Understood business processes from data perspective and converted business logic to oracle SQL codes for reporting\n\u2022 Designed and developed business area specific schema to facilitate additional attribute coverage\n\u2022 Identified bottle necks and enabled optimization in existing Oracle SQL codes to reduce run-times by 80%\n\u2022 Automated 100+ recurring reports using scripts and macros, eliminated manual dependency\n\u2022 Analyzed large, complex data sets to create executive dashboards; presented solutions during top-level business reviews', u'Data Analyst\nTech Mahindra Ltd\nOctober 2010 to August 2014\n\u2022 Single-handedly managed multiple projects of data analysis, data migration and data cleansing for British Telecom GS\n\u2022 Engineered savings of \xa326M+ by identifying data gaps in service fulfillment and service assurance processes\n\u2022 Assist with data analysis and identifying and resolving data defects']","[u'M.S. in Information Technology and Management', u'B.E. in Computer Engineering']","[u'The University of Texas at Dallas Dallas, TX\nMay 2018', u'University of Mumbai\nMay 2010']"
0,https://resumes.indeed.com/resume/2252c39a9c12c0cd,"[u""Data Analyst\nSYNTEL LTD\nSeptember 2016 to August 2017\n* Functioned with customer to implement data analysis, document deficiencies in data, and generate mapping, and validation reports on client data while using Tableau and SQL.\n* Administered data architecture and modeling activities for data warehouses; analyzed BI data for integration into enterprise conceptual model, formulated new tables and integrated data into existing tables\n* Applied MS-Excel to analyze data and perform daily inventory tracking, RGY status. Gathered requirements to document existing business process and enhancement requests for business system.\n* Based on the daily tracking and RGY's created dashboards in Tableau and Trend analysis using data analysis tool\npack in Excel."", u'Software Engineer\nSYNTEL LTD\nSeptember 2015 to August 2016\n* Played a key role in monitoring system, analyzing trends, and crating, improving, and maintaining support activities of Mainframe systems\n* Eliminated the printing discrepancies by testing of reports on HP Extream Dialogue application to enhance the performance of batch processes.']","[u'MASTER OF SCIENCE in ANALYTICS', u'BACHELOR OF ENGINEERING in ELECTRONICS & TELECOMMUNICATIONS']","[u'Northeastern University Boston, MA\nApril 2019', u'University of Pune, RMD Sinhgad School of Engineering Pune, Maharashtra\nJanuary 2015']"
0,https://resumes.indeed.com/resume/3a388228b5e17295,"[u""BI Analyst\nNVIDIA\nJanuary 2018 to Present\nResponsibilities:\n* Automated the excel sheet formulas in to SQL.\n* Experienced in working with SharePoint Sites, Lists, Libraries and web parts.\n* Designed and developed the Database objects (Tables, Materialized Views, Stored procedures, Indexes) using SQL statements.\n* Successfully transferred old data from various sources like flat files, MS Access, and Excel into MS SQL Server using SSIS Packages.\n* Worked on Process Builder, workflow rules, approval process within Salesforce.\n* Have experience on working tools like data loader, workbench, Force.com IDE etc.\n* Worked on Visualforce Pages and Apex Classes and Apex Triggers.\n* Developed various Custom Objects, Tabs, Components and Visual Force Pages and Controllers using Salesforce.\n* Advanced level data manipulation is done in Excel.\n* Developed the E-R Diagrams for the logical Database Model.\n* Excellent knowledge in Data Analysis, Data Validation, Data Cleansing, Data Verification and identifying data mismatch.\n* Involved in Data loading using SQL Loader.\n* Troubleshooting service problems and correcting errors that cause functional and performance issues for clients.\n* Designed and Developed Complex Active reports and Dashboards with different data visualizations using Tableau desktop.\n* Lead root cause analysis and implementation of corrective actions.\n* Resolved the data related issues.\n* Created pivot tables and ran VLOOKUP's in Excel as a part of data validation.\n* Compare the source data with historical data to get some statistical analysis.\n* Performed the Data Accuracy, Data Analysis, Data Quality checks before and after loading the data."", u'Data Analyst\nCISCO SYSTEMS\nJune 2016 to December 2017\nResponsibilities:\n* Worked on dashboards and reports within Tableau.\n* Interact with business users and understand their requirements.\n* Converts Excel Reports to Tableau Dashboard with High Visualization and Good Flexibility.\n* Well documented the entire process of working with Tableau desktop and in evaluating business requirement.\n* Tested dashboards to ensure data was matching as per the business requirements and if there were any changes in underlying data.\n* Created side by side bars, Scatter Plots, Stacked Bars, Heat Maps, Filled Maps and Symbol Maps according to deliverable specifications.\n* Created report schedules on Tableau server.\n* Publishing reports, workbooks, data sources and dashboards to Tableau server in different Formats.\n* Developed story telling dashboards in Tableau Desktop and published them on to Tableau Server which allowed end users to understand the data on the fly with the usage of quick filters for on demand needed information.\n* Worked on motion chart, Bubble chart, drill down analysis using tableau desktop.\n\nEnvironment: Tableau Desktop 9.0, 9.2.9.3, 10.0, Oracle, MS Office Suite, Tableau server, SQL Server.', u'Business Analyst, Android Developer\nTERRA INFORMATION INDIA PVT. LTD\nMay 2014 to July 2015\nResponsibilities:\n* Conceptualized, designed mobile applications for our customers.\n* Worked on linear, frame and relative layouts.\n* Wrote technical specifications and maintained all reports pertaining to application development process.\n* Test an error logging on different devices of Android.\n* Discussed the requirements with the client and analyzed the requirements.\n* Worked on the software that combines an Android application installed on employee mobile devices with server access to office administrators, at Terra Information India Private Limited.\n\nEnvironment: Eclipse (Indigo), BlueStacks, java version-jdk1.6.0_11, Android Sdk Version-1.5', u'Data Analyst\nTERRA INFORMATION INDIA PVT. LTD\nFebruary 2013 to March 2014\nResponsibilities:\n* Gathering required data from various data sources and process information by interacting with the inventory team\n* Written SQL queries, performed calculations and checked the efficiency of certain processes.\n* Prepared detailed reports and presented trends using tableau dashboards on an ongoing basis.\n* Successfully load data into SQL Server and analyze and explain the result using Server and Access as an important tool.\n* Designed the database, creating Tables, Stored Procedures, Views and Function.\n* Created complex data views manually using multiple measures, also used sort, Filter, group, Create Set functionality.\n* Created audit reports involving charts and graphs using Excel, Tableau dashboards.\n* Created the data story in Tableau by implementing interactive dashboards using drill down filters and parameters.\n\nEnvironment: MS SQL Server, Tableau, Excel']",[],[]
0,https://resumes.indeed.com/resume/4e05f605beb88ae1,[u'Database Engineer & Data analyst\nTelecommunication Company of Esfahan\nAugust 1999 to May 2016\nTCE)'],[u'B.Sc. in Computer Engineering'],[u'Isfahan University of Technology\nJanuary 1998']
0,https://resumes.indeed.com/resume/79307fbf5d7040c3,"[u'Programmer Analyst\nCognizant Technology Solutions - Pune, Maharashtra\nDecember 2014 to June 2016\n\u2022 Performance testing professional with knowledge of various phases of performance testing got edified in requirement analysis, workload model creation, Test Design, Test Script creation and execution, Finding bottlenecks and result analysis.\n\u2022 Utilized software tools such as Load Runner, Neoload, J-Meter, Performance Centre, HP Performance Centre, HP ALM, Team quest.\n\u2022 1.6 yrs. of experience in IT industry in Banking domain and Well versed in writing SQL queries.\n\u2022 Expert using Microsoft Access (design and build), Excel, Macros, MS SQL server, PowerPoint, Visio. Have worked on UML Use Case Diagrams, Sequence Diagrams, Activity Diagrams, and creating these using MS Visio.', u'Data Analyst Intern\nBharat Electronics - Panchkula, Haryana\nJanuary 2014 to June 2014\n\u2022 Used Microsoft Excel to create pivot tables, used Vlookup and other Excel functions.\n\u2022 Reviewed, analyzed and ensured the quality of data loaded into the database system using Excel.\n\u2022 Performed daily data queries and prepared reports on daily, weekly, monthly, and quarterly basis.\n\u2022 Use SQL queries on Microsoft Access/SQL Server in a regular basis to interpret & validate data to detailed reports.']","[u'in Information Systems', u'in Electronics & Communication Engineering']","[u'California State University\nMay 2018', u'Punjab Technical University Jalandhar, Punjab\nJuly 2014']"
0,https://resumes.indeed.com/resume/902d0f7c10ed21ab,"[u'Data Analyst\nBank of America - Houston, TX\nMay 2016 to Present\nProject Description:\nThe project was to develop the Data Warehouse and design relational databases to perform effectively and efficiently, logically and physically, in support of data warehouse applications.\n\nResponsibilities:\n\u2022 Provided tech team support for SDLC Data Warehouse.\n\u2022 Assisted Project Managers in establishing plans, risk assessments and milestone deliverables.\n\u2022 Designed data models using Oracle Designer.\n\u2022 Designed programs for data extraction and loading into Oracle database.\n\u2022 Managed database tables, procedures and indexes.\n\u2022 Tested SQL and SQL Loader programs for database application.\n\u2022 Implemented enhancements to existing software products.\n\u2022 Performed data manipulation using Informatica and Oracle software.\n\u2022 Performed Source system analysis (SSA) to identify the source data that needs to be moved into the target tables.\n\u2022 Defined frameworks for Operational data system (ODS), Brokerage data warehouse (BDW), Central file distribution (CFD) and Data Quality (DQ) and created functional data requirement (FDR) and Master Test Strategy documents.\n\u2022 Defined target load order plan and constraint based loading for loading data correctly into different target tables.\n\u2022 Designed and created Data Quality baseline flow diagram, which includes error handing and test plan flow data.\n\u2022 Created Test Plans, Test Cases, and Test Scripts for all testing events such as System Integration Testing (SIT), User Acceptance Testing (UAT) and Unit Integration Testing.\n\u2022 Coordinated with offshore vendor and established offshore teams and processes for code development.\n\u2022 Coordinated with execution team to define the batch flow for data validation process. Created SQL-Loader scripts to load legacy data into Oracle staging tables and wrote SQL queries to perform Data Validation and Data Integrity testing.\n\nEnvironment: Windows, MS Office(MS Word, MS Excel, MS PowerPoint, MS SharePoint, MS Visio), Wireframes, Informatica, Oracle 10.1, OBIE, IBM Mainframes, RUP, UML, SQL, SWOT analysis, Quality Center', u'Data/System Analyst\nJP Morgan Chase - Houston, TX\nAugust 2014 to April 2016\nProject Description:\nThe project included migration of historical data with retirement of current data warehouse (OSCARR) and develop new banking data warehouse (BDW). This project aims to analyze the data contained in OSCARR, down to the table and field level and identify the source data (HOGAN). Source data will be loaded into BDW which will deed DAL (Data Access Layer). Scope involves planning, designing, constructing and implementing a structured DBW where users can access the data to fulfill analysis and reporting requirements. DAL will provide the ability to obtain data effectively and efficiently as per user\' needs.\n\nResponsibilities:\n\u2022 Interviewed clients to understand their business and functional requirements, gathering their future needs and transforming them to functional specifications to prepare the Business Requirements and Functional Specifications Documents.\n\u2022 Worked closely with Data Architect to review all the conceptual, logical and physical design models with respect to functions, definition, maintenance review and support Data analysis, Data Quality and ETL design that feeds the logical data models.\n\u2022 Worked on reporting requirements gathering for various departments like marketing, management.\n\u2022 Creating Control Charts including Designing, Managing, and Improvement.\n\u2022 Expertise in Pareto analysis and built Pareto chart to prioritize problems and track them simultaneously.\n\u2022 Experience in root cause analysis and metric creation using FMEA, Fishbone or 5 Why Analysis.\n\u2022 Organization of the data, categorization and determine how often cause occurs.\n\u2022 Plot the data and use Pareto analysis- ""Peel the onion"" techniques to show pote\n\u2022 Involved in JAD sessions to solve the different issues occurred during the project development.\n\u2022 Partnered with Project Managers and Technical Designers to access and document the impact of managing current project requirements.\n\u2022 Worked in data management performing data analysis, gap analysis, and data mapping.\n\u2022 Created the data mapping document to specify, the location of the table in database, conversion rules and business rules.\n\u2022 Created and maintained Data Dictionary to describe data flows, processes, data structures tables, fields, procedures, data type and other attributes of database and external entities.\n\u2022 Performed gap analysis between the ""As-Is"" model of the legacy system and the ""To-Be"" model to identify the gaps, logged issues relating to the identified gaps and worked with the SME to get clarifications.\n\u2022 Provided primary liaison between business/operations/vendors and development.\n\u2022 Created data models and data flow diagrams for the to-be process.\n\u2022 Analyzed the source data, coordinated with Data Warehouse team in developing Relational Model. Performed Data Analysis and Data Validation by writing complex SQL queries using TOAD and SAS against the ORACLE database.\n\u2022 Responsible for daily Scrum Meetings, Sprint Planning, and Sprint Retrospective.\n\u2022 Elicit business requirements using Business Rules Analysis, Prototypes, and Workshops.\n\u2022 Mapped current business processes using value-stream mapping and lead teams through workshops to improve processes, reducing cycle time and speeding delivery to the customer.\n\u2022 Facilitated weekly Agile Retrospectives for all teams to determine areas needing improvement then worked with the teams to come up with action items and held them accountable to work on those items before the next retrospective.\n\u2022 Held daily scrum standup meetings for multiple teams and conducted agile retrospectives.\n\u2022 Encrypted file using PGP encryption and FTP\'s using Blue Zone mainframe FTP also using SAS and UNIX.\n\u2022 Conducted UAT to verify whether all the requirements were catered by new data warehouse.\n\u2022 Used Quality Center to get an easy interface to manage and organize activities like requirements coverage, test case management, test execution reporting, defect management and test automation.\n\nEnvironment: MVS, MS Office, Teradata Rational Tools - Requisite Pro, Rose, Quality Center, Clear Case, Windows XP, HTML, XML, XSL, J2EE, JSP, Servlets, Java Script, Net Enterprise, Oracle, SQL Server, TOAD.', u'Data Analyst\nHartford, CT\nFebruary 2013 to June 2014\nProject Description:\nThe project was to build data driven strategies to better understand customer and the use that data to develop more effective more sales and marketing campaigns.\n\nResponsibilities:\n\u2022 Assist Project Manager to create in creating project plan, work break down structure using MS Project and Project charter.\n\u2022 Validated ETL mappings and tuned them for better performance and implemented various Performance and Tuning techniques.\n\u2022 Created data flow diagrams, data mapping from Source to stage and Stage to Target mapping documents indicating the source tables, columns, data types, transformations required and business rules to be applied.\n\u2022 Maintained and improved the GUI interface of the Data Control Department Database in MS Access/SQL for all reports and superusers within the company.\n\u2022 Analyzed the functional specs provided by the data architect and created technical specs documents for all the mappings.\n\u2022 Data mapping work of the data exist on the warehouse.\n\u2022 Created data flow diagrams, data mapping from Source to stage and Stage to Target mapping documents indicating the source tables, columns, data types, transformations required and business rules to be applied.\n\u2022 Prepared the data rules spreadsheet using MS Excel that will be used to update allowed values, findings and profiling results. Translated cell formulas for business users in Excel into VBA code to design, analyze, and deploy programs for their ad-hoc needs. Minimize the waste and improve the containment process.\n\u2022 Participated in the Incident Management and Problem Management processes for root cause analysis, resolution and reporting.\n\u2022 Worked heavily with the XML files and automated and converted them into the specific format to make them easily testable and usable.\n\u2022 Designed and created Data Quality baseline flow diagram, which includes error handing and test plan flow data.\n\u2022 Worked on data settlement in various source system Including Teradata\n\u2022 Worked with Data Flows, Data Architecture and Data Dictionary to keep track of the changes.\n\u2022 Facilitated (JAD) Joint Application Development sessions to resolve issues relating to difference between business requirements and technical design.\n\u2022 Designed the test formats in Excel. Also used VBA to write Macros; automated the test cases and different scenarios.\n\u2022 Worked with Scrum Masters/Product Owners &executives at software startup clients in Lean and Agile Principles and execution\n\u2022 Worked with team members on skill sets necessary for Agile roles, created user stories and assisted scrum masters on optimal team structure and the project roadmap\n\u2022 Identified existing SDLC as outdated and ineffective, spearheaded a corporation wide Agile overhaul\n\u2022 Worked closely with a project team for gathering business requirements and interacted with business users to translate business requirements into technical specifications.\n\u2022 Performed Data Analysis, Data Mapping and Data validation by writing SQL queries and Regular expressions.\n\u2022 Created Test Plans, Test Cases, and Test Scripts for all testing events such as System Integration Testing (SIT), User Acceptance Testing (UAT) and Unit Integration Testing.\n\nEnvironment: RUP, XML, VBA, MS Project, MS Office, MS SQL Server, MS SharePoint.', u'Business/Data Analyst\nBB&T Bank - Winston-Salem, NC\nApril 2012 to January 2013\nProject Description:\nWorked on Home Loan Management System; the business of this division was to process home loan application from potential clients, evaluate their eligibility, approve them, and maintain the loans till they are closed. It included client, debtor, invoice, pricing, and account information. Invoices had to be funded and payments needed to be processed with accounts being managed accordingly. Reports were generated for both internal and external use. Once loan was repaid it then needed to be removed from the system. The five primary stages in the life cycle of a loan were the request, processing, booking, maintenance, and closure stages.\n\nResponsibilities:\n\u2022 Followed Rational Unified Process (RUP) methodology for creating artifacts.\n\u2022 Conducted Interviews, brainstorming and focus groups to identify requirements and then documented them in a format that can be reviewed and understood by both business and developers.\n\u2022 Facilitated (JAD) Joint Application Development sessions to resolve issues relating to difference between business requirements and developers.\n\u2022 Performed Data Analysis and Data validation by writing complex SQL queries using TOAD.\n\u2022 Created Business Requirements and converted them into detailed Use Cases, Report Specifications and Non-Functional Requirements.\n\u2022 Analyzed Requirements and created Use Cases, Use Case Diagrams, Activity Diagrams, sequence diagram using MS-Visio.\n\u2022 Used Rational Requisite Pro for Managing and Configuring the Requirement Analysis effort.\n\u2022 Provided primary liaison between business/operations and development.\n\u2022 Created data models and data flow diagrams for the to-be process.\n\u2022 Researched upstream data sources and ensured all source data sources were attested by its owners.\n\u2022 Created and executed SQL queries using Rapid SQL and scripts to validate data movement and generate expected results for UAT.\n\u2022 Conducted UAT to verify whether all the Requirements were provide to by the application.\n\u2022 Used Rational Test Manager for Test Case Management and Rational Clear Quest for defect tracking and resolution.\n\u2022 Worked with the developers on resolving the reported bugs and various technical issues.\n\u2022 Lead data related projects such as metadata tool & process implementation, data quality tool and process implementation.\n\u2022 Created Business requirement document (BRD), functional requirements specifications (FRS), and technical requirement specification along with Use Case for application development.\n\u2022 Interacted with the end users to identify and gather business requirements and transform them into technical requirements.\n\u2022 Gathered and understood User Requirements through user interviews and Brainstorming sessions. Analyzed the Requirements and created the Business Requirements Document (BRD), Functional Specifications document (FSD) and Use case Specifications.\n\u2022 Identified and prepared Use Cases, BPM, and Activity Diagrams using Rational Rose according to UML methodology.\n\u2022 Strong knowledge of data quality and profiling tools; data management concepts and principles.\n\u2022 Understood and contributed data architectures development and data models reviews.\n\u2022 Data Governance performance metrics scorecard development.\n\u2022 Implemented data related change management process.\n\u2022 Ability to liaison with data management professionals across the organization to achieve goals.\n\u2022 Perform data profiling, data quality rules development and assessment using data quality and metadata tools.\n\nEnvironment: Rational Unified Process (RUP), Rational Suite (Requisite Pro, Clear Quest, Clear Case), MS Visio, MS Project, Oracle, Mercury Test Director MS SQL Server, Rational Rose, MS Access, iRise, metadata tools.']","[u""Master's in information systems""]","[u'Murray State University Murray, KY']"
0,https://resumes.indeed.com/resume/884ef1dd4957ac01,"[u""Data Analyst\nShanghai Hyxell IOT Science and Technology Ltd. - Shanghai\nApril 2017 to November 2017\nInput the students' laundry preference data, the number of students and the schedule of the courses into the database, and then carried out relevant data analysis.\nUsed to the analysis data, find out the students' daily and weekly laundry peaks, and then help the Marketing Department to set the purchase quantity and price."", u'Founder\nHealth & Nutrition Storages - Los Angeles, CA\nFebruary 2014 to December 2015\nCreated a health & nutrition company, which mainly sold different types of seafood and vitamins to China, and established a stable supply channel in the United States and Mexico, and logistics chains between the Unite states and China.\nFor middle and high-end consumers, combined direct marketing and distribution channel to expand actively the Greater China market. Also, researched market demand and determined the marketing orientation to guide and design advertising copy and brochures.']",[u'Bachelor of Science in Mathematics/Applied Science'],"[u'University of California Los Angeles, CA\nMarch 2017']"
0,https://resumes.indeed.com/resume/526721000211a255,"[u'Data Analyst\nPostal Savings Bank of China, H.O. Data Centre - Beijing\nSeptember 2016 to March 2017\nAnalyzed huge amounts of data of the bank\nBuilt data analysis model to provide decision support for sales management\nTook part in the Internet big data research project', u'Data analyst\nHarbin Branch of Guangfa Bank - Harbin, CN\nJune 2015 to August 2015\n\u2022 Familiarized with the daily routine and operations of different departments\n\u2022 Responsible for receptionist works and promoted financing products, credit cards businessand individual loans']","[u'Master of Science in Business Intelligence & Analytics in Knowledge Discovery Databases', u'Bachelor of Science(Honors) in Financial Mathematics']","[u'Stevens Institute of Technology Hoboken, NJ\nMay 2019', u'Beijing Normal University & Hong Kong Baptist University United International College Zhuhai, CN\nJune 2016']"
0,https://resumes.indeed.com/resume/011ed820cc41f35e,"[u'Data Analyst\nLarsen & Toubro Infotech Limited - Chennai, Tamil Nadu\nJuly 2014 to October 2015\nImplemented Statistical Risk Analysis and Data Analysis measures for evaluating, assessing and handling the faulty\nsystems by providing reusable solutions in various phases of the project\n\u2022 Increased productivity by 15% by implementing Risk Mitigation Strategies for over 10 million customers\n\u2022 Created an enterprise data schema for different domains in collaboration with various insurance segments\n\u2022 Developed SQL scripts to load transformed legacy data into datastore categorized by domains using SSIS\n\u2022 Performed various data transformation techniques using SQL Loader, Key Lookups, Pivoting and PL/SQL\n\u2022 Created visualization using interactive dashboards in Tableau to present crucial insights\n\u2022 Performed various ad-hoc reporting tasks which reduced timeline for Business Intelligence decision process\n\u2022 Developed customized reports using Excel and Visual Basic to track daily monitoring tasks\n\u2022 As an Agile team member worked with several front-end and back-end cross-functional teams', u'Data Analyst Intern\nSharadaa Ceramics - Chennai, India\nApril 2014 to July 2014\n\u2022 Built and managed a data pipeline to analyze and predict the likelihood of customers to increase profit margin\n\u2022 Optimized predictive models and performed rich Data visualizations to prepare a comprehensive summary report to various business Consultants']","[u'Masters in Information Technology and Management', u'Bachelor of Engineering in Computer Science and Engineering']","[u'Illinois Institute of Technology\nDecember 2017', u'Anna University Chennai, Tamil Nadu\nMay 2014']"
0,https://resumes.indeed.com/resume/d71cced42ec12ba1,"[u'Data Analyst\nSunTrust Banks - Atlanta, GA\nNovember 2016 to Present\nResponsibilities:\n\u27a2 Onboarding at the initial stages, was involved in requirements gathering through JAD sessions, interviewed business users and stakeholders while learning from legacy documentation.\n\u27a2 Created Functional Requirements FRD, Use Cases and Unified Modeling Diagrams (UML).\n\u27a2 Coding to enable the mapping of various data extracts/data sets (excel, .csv, .txt, Teradata, etc.) to a new standardized SAS dataset.\n\u27a2 Assistance with developing a new SAS data warehouse to enrich and store transactional losses from various sources.\n\u27a2 Design and write functionalities of the website using Ruby on Rails and related technologies such as various Ruby gems, HTML, Java Script, and JQuery.\n\u27a2 Migration of the old databases from MySQL to MongoDB on the new environment\n\u27a2 Active Record using MySQL (mysql2) and Oracle (active record oracle enhanced adapter) databases.\n\u27a2 Database modeling and design. Involved in developing and implementation of the web application using Ruby on Rails.\n\u27a2 Designed the front end application using Rails 3.1, CSS, AJAX and JQuery.\n\u27a2 Used RSpec to create test driven development. Experience with all of the GEMS and libraries for this version of the ruby/rails.\n\u27a2 Working with Ruby gems such as ruby-pg for Oracle- PostGRE-SQL.\n\u27a2 Experience in Coding to enable the reporting of internal loss data to various customers in unique formats.\n\u27a2 Experience in Coding to download line of business specific extracts for updates and validations of losses.\n\u27a2 Testing and ongoing support Used SAS, MySQL, Oracle DB data sources to build predictive models based on customer data, transactional, historical and campaigns data.\n\u27a2 Used Output Delivery system (ODS) facility to direct SAS output to RTF, PDF and HTML files.\n\u27a2 Performed SAS analysis using the customer database to monitor portfolio pricing.\n\u27a2 Built summary reports after identifying the customers, their occupancy period and the revenue generated using PROC, SUMMARY, PROC MEANS, PROC FREQ and GCHART.\n\u27a2 Used MS Visio for Process modeling, Process mapping and Business Process flow diagrams.\n\u27a2 Maintained the business rules and quality parameters for Data Quality services to separate and fix the data errors maintain the integrity of the data.\n\u27a2 Managed change requests with thorough impact and cost analysis while prioritizing and mitigating changes.\n\u27a2 Develop and execute Unit Test cases, scripts and plans in Quality Center. Facilitate testing activities within the development and testing teams.\n\u27a2 Created Numerous SSIS packages for business Interfaces and Applications. Used various Transformations like Derived Columns, Conditional Split, Data type Transformations, File System Tasks, For Loop Containers etc.\n\u27a2 Developed various QlikView Data Models by extracting and using the data from various sources like QVD files, Teradata, Excel, and Flat Files.\n\u27a2 Tuning and optimization of Teradata Queries Created the ETL data mapping documents between source systems and the targetdata warehouse.\n\u27a2 Used Expressions and Package configurations, Variables and also parameter mapping in SSIS Execute tasks. Worked with Excel files, Flat files, csv files with SSIS to Import and Export the data into SQLSERVER 2008R2.\n\u27a2 Design and maintain SQL Views in the Teradata environment, leveraging the Teradata SQL Assistant tool as per requirements.\n\u27a2 Prepared Unit Test Cases, Perl scripts, Load Test, Test Data, Execute test, validate results, Manage defects and report results.\n\u27a2 Worked with different kinds of mortgage data including Advances, Third party sale, Modifications, Penalties, etc.\n\u27a2 Worked on Teradata Environment based on the data from PDM. Conceived, designed, developed and implemented this model from the scratch\n\u27a2 Responsible for analyzing Warehouse (IWH) and Staging data to find expected results and document discrepancies.\n\u27a2 Responsible for monitoring and analyzing ETL jobs pulling data from source system into staging and then warehouse.\n\u27a2 Move product hierarchy design and SQL Views to various environments and supporting implementation and post production activities.\n\u27a2 Create and maintain enhancements and configuration change requests (CCRs) for production Discoverant hierarchies within the Defect, Issue, and Enhancement Tracking (DIET) system.\nEnvironment:SQL, MY SQL, SAS 9.3, SAS/BASE, SAS/MACRO, SAS/STAT, SAS/GRAPH, SAS/SQL, SAS/Enterprise Guide, MS OFFICE, Teradata, python, MS OFFICE, UNIX,: SAS 9.2, R Programming, R Packages, Perl scripting, Python 2.6/2.7, Credit risk modeling approach, CCAR, SR 11-7, SQL Server, Microsoft Office Suite 2010 Microsoft Excel, MS Access, Hadoop, Map Reduce, Bigdata, hive, hue, oozie, Storm, Elastic Search, Redis, Scoop, Java, J2EE, HDFS, XML, PHP, ZooKeeper, PIG, Cassandra, Oracle, NoSQL and Unix/LinuxmPerl, Tableau 8.2', u'Data Analyst\nColumbus, OH\nFebruary 2014 to October 2016\nResponsibilities:\n\u27a2 Redefined many attributes and relationships in the reverse engineered model and cleansed unwanted tables and columns as part of the Source Data Analysis responsibilities.\n\u27a2 Worked extensively in documenting the Source to Target Mapping documents with data transformation logic.\n\u27a2 Worked on Extracting, Transforming and Loading (ETL) process to load data from Excel, Flat file to MS SQL Server using SSIS.\n\u27a2 Used NoSQL database with Cassandra and MongoDB.\n\u27a2 Worked on Code Migration from Pc SAS to Grid (SERVER).\n\u27a2 Used Python, R, SAS, SQL to create machine learning algorithms involving Multivariate Regression, Linear Regression, Logistic Regression, PCA, Random forest models, Matrix factorization models, Bayes collaborative models to target users with email campaigns and native Ads.\n\u27a2 Build and implement mathematical models and with engineering code in Python (Pandas, scikit-learn and others.)\n\u27a2 Involved in Migrating the data model from one database to Teradata database and prepared aTeradata staging model.\n\u27a2 Used matplotlib and the seaborn libraries to create beautiful modern and informative visualizations with python.\n\u27a2 Created SSIS packages to load the data from Text File to staging server and then from staging server to Data warehouse.\n\u27a2 ETL implementation using SQL Server Integration Services (SSIS), Applying some business logic and data cleaning in staging server to maintain child and parent relationship.\n\u27a2 Deliver reports and ad-hoc analysis focused in the area of client behavior and profiling using SAS, SQL and Excel.\n\u27a2 Worked on Control flow tasks such as Execute SQL task, Send Mail Task, File System Task, Dataflow Task and used different data sources and destination with derived column, lookup transformation within Dataflow Task.\n\u27a2 Build Logistic Regression models to identify the control group cohorts using SAS/STAT and other SAS modules.\n\u27a2 Designed SSIS packages to extract data from different sources like SQL server 2008, MS Excel, MS Access, transform and then load into Dimension and Fact tables in Data Warehouse using SSIS.\n\u27a2 Developed SSIS packages using for each loop in Control Flow to process all excel files within folder, File System Task to move file into Archive after processing and Execute SQL task to insert transaction log data into the SQL table.\n\u27a2 Worked on NoSQL databases including HBase, MongoDB, and Cassandra.\n\u27a2 Data Migration from Flat files, CSV, MS-Access, Excel and OLE DB to SQL Database\n\u27a2 Used SQL Server Reporting Services (SSRS) to generate reports.\n\u27a2 Configured report server and report manager scheduling, give permissions to different level of users in SQL Server Reporting Services (SSRS).\n\u27a2 Generated scheduled reports for business analysis or management decision using SQL ServerReporting Services (SSRS).\n\u27a2 Extracted, performed validation and generated SAS data sets from Teradata; applied SQL Pass through Facility.\n\u27a2 Experienced in generating and documenting Metadata while designing OLTP and OLAP systemenvironment.\n\u27a2 Worked on Sql combined in Database environments like SAS , ORACLE , TERADATA , HADOOP\n\u27a2 Involved in generating proper data in excel for Tableau Dashboard.\n\u27a2 Involved in non-transactional data entities of an organization of MDM (Master Data Management) that has the objective of providing processes for collecting, aggregating, matching, consolidating, quality-assurance, persistence and distribution.\n\u27a2 Redefined many attributes and relationships in the reverse engineered model and cleansed unwanted tables and columns as part of the Source Data Analysis responsibilities\n\u27a2 Work with business partners to understand and capture requirements around reporting & analytics, facilitate across multiple stakeholders towards decision/resolution, and translate business requirements into reporting solutions.\n\u27a2 Apply in-depth knowledge of Human Resources to lead the design of moderate to complex reports using Cognos as per specifications from customers, including design and analysis.\n\u27a2 Identify and enhance existing reports and processes.\n\nEnvironment:SQL, MYSQL, SAS, R, R-Studio, Hadoop, LINUX, R, Tableau Desktop, Tableau Server, Unix Shell scripting, Python, SQL Server, Python, Bash, Microsoft Excel', u'Business Analyst/Data Analyst\nFreddie Mac - McLean, VA\nSeptember 2012 to January 2014\nResponsibilities:\n\u27a2 Performed reverse engineering for a wide variety of relational DBMS, including Microsoft Access, Oracle and Teradata, to connect to existing database and create graphical representation (E-R diagram) using Erwin7.3.\n\u27a2 Worked on Teradata Environment based on the data from PDM. Conceived, designed, developed and implemented this model from the scratch.\n\u27a2 Star schema dimension modeling for the data mart using Visio and created dimension and fact tables based on the business requirement.\n\u27a2 Responsible for statistical applications support and programming primarily in SAS, and supporting Marketing team in ACOE.\n\u27a2 used pandas with python to create high-performance data structures\n\u27a2 Worked in importing and cleansing of data from various sources like Teradata, Oracle, flat files, SQL Server 2005 with high volume data\n\u27a2 Performed statistical analysis using SQL, SAS, R, Python, and Excel.\n\u27a2 Generating Regular, Ad-Hoc Reports using SAS Tools and Oracle and ETL sources.\n\u27a2 Building Statistical Models, predictive models and Regression Models using SAS Tools\n\u27a2 Gathered report requirements and determined the best solution to provide the results in either a Reporting Services report, Analytical Cube or an Excel table\n\u27a2 Designed SSIS packages to Extract data from different sources like SQL server 2008, MS Excel, MS Access, transform and then load into Dimension and Fact tables in Data Warehouse using SSIS\nDeveloped SSIS packages using for each loop in Control Flow to process all excel files within folder, File System Task to move file into Archive after processing and Execute SQL task to insert transaction log data into the SQL table\n\u27a2 Writing complex SQL Queries using joins, to extract the data from the Teradata Database and Schedule the jobs in UNIX.\n\u27a2 Develop and support Extraction, Transformation and Load process (ETL) using Informatica power center to populate Teradata tables and flat files.\n\u27a2 Migrating SQL server Objects (Table/Views) to Teradata Objects.\n\u27a2 Create and maintain predictive and multivariate models in SAS BI suite.\n\u27a2 Created and maintained required Data marts and OLAP Cubes using SAS DI Studio and SAS OLAP Cube Studio to fulfill reporting requirements\n\u27a2 Migrating data from SQL server to Teradata using Teradata, Worked on Sql combined in Database environments like SAS , ORACLE , TERADATA , HADOOP\n\u27a2 Implemented metadata standards, data governance and stewardship, master data management, ETL, ODS, data warehouse, data marts, reporting, dashboard and predictive modeling.\n\u27a2 Developed mappings in Informatica Power center 9.6 to load the data from various sources includesSQL server, DB2, Oracle, Flat files into the Data Warehouse, using different transformations like Source Qualifier, Joiner transformation, Update Strategy, Lookup transformation, Rank Transformations, Expressions, Aggregator, Sequence Generator.\n\u27a2 Conducted User Acceptance Testing (UAT) on the application - resolved issues from the participants, prepared and submitted Test Analysis Reports and participated in Product Readiness Review.\n\u27a2 Responsible for debugging, troubleshooting, and tuning PL/SQL stored procedures and functions and Complex SQL.\n\u27a2 Designed, coded, and implemented SAS code for analytic studies to fulfill internal and external client requests, maintained, evaluated, and reported on recurring analytic studies. Performed data hygiene and created data validation program.\n\u27a2 Performed in depth analysis in data & prepared weekly, biweekly, monthly reports by using SQL, SAS, Ms Excel, MS Access, and UNIX.\n\u27a2 Extracted, performed validation and generated SAS data sets from Teradata; applied SQL Pass through Facility.\n\u27a2 Created and manipulated datasets using SAS, Access, and Excel.\n\u27a2 Involved in Building a specific data-mart as part of a Business Objects Universe which replaced the existing system of reporting that was based on exporting data sets from Teradata to Excel spreadsheets\n\u27a2 Migrated three critical reporting systems to Business Objects and Web Intelligence on a Teradata platform.\n\u27a2 Used SAS EG extensively for data analysis and involved in writing macros.\n\u27a2 Used SAS procedures to achieve the required data analysis and for data transformations.\nEnvironment: Linux, MySQL, Spark, R, R-Studio, Tableau, Perl, Environment: R, SPSS, Machine Learning, Tableau, Linux, SQL, Python.\nBlue Cross Blue Shield (BCBS)', u'Business Analyst/Data Analyst\nIBM India Ltd\nMarch 2010 to August 2012\nResponsibilities:\n\u27a2 Reverse engineered from Toad database to Erwin and generated SQL script through forward engineer in Erwin added for this project\n\u27a2 Assisted the ETL team to document the transformation rules for data migration from OLTP to Warehouse environment for reporting purposes\n\u27a2 Involved in extensive Data Analysis on the Teradata and Oracle Systems querying and writing inSQL and TOAD\n\u27a2 Designing the new processes in SAS and VBA for new business units.\n\u27a2 Analysis and building Risk-Metrics using SAS/IML and SAS/ETS and Base SAS.\n\u27a2 Building Monte-Carlo simulation techniques for Risk-Metrics using SAS/IML.\n\u27a2 Development and automation of daily Position and P & L Reporting processes using Base SAS procedures and VBA.\n\u27a2 Integration / synchronization of daily P&L reports with financial forecasts.\n\u27a2 Ensuring the accuracy and timeliness of daily PnL and position reporting using SAS reporting procedures and VBA tools.\n\u27a2 Use Python and its libraries numpy, pandas, matplotlib, scikit-learn etc. for data exploration, data analysis, data\n\u27a2 Used SAS PROC SQL Pass through Facility to work with Teradata database.\n\u27a2 Automation of Data Extraction, Transformation and Loading the results to final datasets using Oracle, SAS and SQL.\n\u27a2 Summarizing the results and data with PROCs FREQ and MEANS.\n\u27a2 Doing Statistical Analysis with statistical procedures and univariate procedures from Base SAS, SAS/STAT and SAS/IML.\n\u27a2 Creating Reports and reporting results with PROC REPORT and ODS.\n\u27a2 Generated comprehensive analytical reports by running SQL queries against current databases to conduct data analysis pertaining to all company products\n\u27a2 Used SQL joins, aggregate functions, analytical functions, group by, order by clauses and interacted with DBA and developers for query optimization and tuning\n\u27a2 Worked on indexes, partitioning and materialized views by interacting with DBA\n\u27a2 Worked with ETL team using SSIS packages, to plan an effective package development process, and design the control flow within the packages\n\u27a2 Interacted with SSRS reporting team to gather reporting requirements, and review summary tables\n\u27a2 Worked with developers to reproduce issues on developer environments.\n\u27a2 Involved in doing Regression Testing across new builds and releases.\n\u27a2 Developed and managed Test Data, Comprehensive Test Cases, and Test Scenario in Excel and uploaded the Test Data into the Version One to prepare for the execution of test Cases.\n\u27a2 Recognized source systems, their connectivity, related tables and fields and ensure data suitably for mapping.\n\u27a2 Created Data Migration and Cleansing rules for the Integration Architecture (OLTP, ODS, DW).\n\u27a2 Worked with ETL team to produce PL/SQL statement and stored procedures in DB2 for extracting as well as writing data.\nEnvironment: SAS, Mainframe, COBOL, JCL, BASE/SAS, SAS/MACRO, SAS/SQL, SAS/CONNECT, SAS/ACCESS, SAS/ODS, Oracle, Teradata, DB2, Python, UNIX and Windows 2003/XP.']",[u'Masters in Computer Science'],"[u'University of Central Missouri Warrensburg, MO']"
0,https://resumes.indeed.com/resume/d37d34255ed32c7a,"[u""Data Analyst\nClient -Pfizer, IL\nMay 2017 to Present\nResponsibilities:\n\u2022 Worked on SQL*Loader to load data from flat files obtained from various facilities every day.\n\u2022 Played a key role in establishing best practices for tableau reporting by conducting meetings with the clients and key stakeholders for gathering reporting requirements and to determine KPI's and developing POC Dashboards.\n\u2022 Leverage strong expertise in analyzing and identifying business requirements, reviewing and communicating with business, and getting sign offs from business stakeholders. Employ Tableau best development practices in developing and formulating reports, dashboards, and SQL queries.\n\u2022 Expertly designed and applied KPIs, Dashboards, and Operational Metrics using Performance Point Server and published them via Tableau to enable KPIs tracking in a real-time.\n\u2022 Expertly created relationships, actions, data blending, filters, parameters, hierarchies, calculated fields, sorting, groupings, live connections, and in-memory in both Tableau and excel.\n\u2022 Integrated Tableau server with SharePoint portal and setup auto refresh.\n\u2022 Optimized reports performance by reducing analysis time from 1 hour to 4 minutes by converting excel spreadsheets into Tableau interactive dashboards.\n\u2022 Produced Excel, Qlik View and Domo reports and managed the overall initiative of developing and documenting technical architecture, system design, performance test results, and other technical aspects of the implementation.\n\u2022 Streamlined multiple performance issues and implemented best process distribution for different projects.\n\u2022 Played a pivotal role in conducting Functional testing, Regression testing, Integration testing, Smoke testing, and UAT.\n\u2022 Reduced analysis time from 2 hours to 4 minutes by converting excel spreadsheets into Qlikview interactive dashboards.\nTools and Environment: Informatica 9.1, Oracle 11g, SQL Developer, PL/SQL, Tableau, TOAD, MS Access, MS Excel"", u'Data Analyst\nThermo Fisher Scientific - Carlsbad, CA\nJune 2015 to April 2017\nWorked with business requirements analysts/subject matter experts to identify and understand\nrequirements. Conducted user interviews and data analysis review meetings.\n\u2022 Defined key facts and dimensions necessary to support the business requirements along with Data Modeler.\n\u2022 Created draft data models for understanding and to help Data Modeler.\n\u2022 Resolved the data related issues such as: assessing data quality, data consolidation, evaluating\nexisting data sources.\n\u2022 Manipulating, cleansing & processing data using Excel, Access and SQL.\n\u2022 Responsible for loading, extracting and validation of client data.\n\u2022 Coordinated with the front-end design team to provide them with the necessary stored procedures and packages and the necessary insight into the data.\n\u2022 Participated in requirements definition, analysis and the design of logical and physical data models\n\u2022 Leading data discovery discussions with Business in JAD sessions and map the business requirements to logical and physical modeling solutions.\n\u2022 Conducted data model reviews with project team members\n\u2022 Captured technical metadata through data modeling tools\n\u2022 Code standard Informatica ETL routines\n\u2022 Developed standard Tableau Reports\n\u2022 Collaborated with ETL teams to create data landing and staging structures as well as source to target mapping documents\n\u2022 Ensure data warehouse database designs efficiently support BI and end user requirements\n\u2022 Collaborated with application and services teams to design databases and interfaces which fully meet business and technical requirements\n\u2022 Maintain expertise and proficiency in the various application areas\n\u2022 Maintain current knowledge of industry trends and standards\n\nTools and Environment: Informatica 9.1, Oracle 11g, PL/SQL, Erwin, TOAD, MS Access, MS Excel', u'Data Analyst\nCisco, CA\nAugust 2013 to May 2015\nResponsibilities:\n\u2022 Analysis of housing trends and review mortgage product characteristics such as underwriting parameters, closing costs, mortgage insurance, qualification rules, rate sheets and price adjustments.\n\u2022 Involved in Agile methodology break tasks into small increments with minimal planning, do not directly involve long-term planning and daily stand-ups, updating tasks.\n\u2022 Processed data received from vendors and loading them into database. The process was carried out on weekly basis and reports were delivered on bi-weekly basis. The extracted data had to be checked for integrity.\n\u2022 Conducted JAD sessions to identify Requirements.\n\u2022 Documented requirements and obtained signoffs.\n\u2022 Coordinated between the Business users and development team in resolving issues.\n\u2022 Documented data cleansing and data profiling.\n\u2022 Wrote SQL scripts to meet the business requirement.\n\u2022 Analyzed views and produced reports.\n\u2022 Tested cleansed data for integrity and uniqueness.\n\u2022 Automated the existing system to achieve faster and accurate data loading.\n\u2022 Generated weekly, bi-weekly reports to be sent to client business team using business objects and documented them too.\n\u2022 Experience creating Business Process Models.\n\u2022 Ability to manage multiple projects simultaneously tracking them towards varying timelines effectively through a combination of business and technical skills.\n\u2022 Good Understanding in clinical practice management, medical and laboratory billing and insurance claim with processing with process flow diagrams.\n\u2022 Worked with compliance team to provide walkthroughs, review requirements and secure approvals to make sure the project is Business Requirements compliant.\n\u2022 Assisted QA team in creating test scenarios that covers day in a life of patient for Inpatient and Ambulatory workflows.\n\nTools and Environment: MS Excel, MS Power Point, Shell Scripting, PL/SQL, Teradata, DB2, Teradata, Business Objects, Windows 2003/2007', u'Software Programmer/Analyst\nAccenture - Bengaluru, Karnataka\nDecember 2012 to July 2013\nResponsibilities:\n\u2022 Developed and tested extraction, transformation, and load ETL Jobs in Informatica processes by utilizing transformations.\n\u2022 Optimized workflow processes by using Workflow manager for session management, database connection management, and scheduling of jobs.\n\u2022 Defined transformation process by working on Power Center client tools such as Source Analyzer, Target Designer, Mapping Designer, and Transformations Developer.\n\u2022 Enabled re usability of business rules through user defined functions in the mapping.\n\u2022 Utilized shortcuts to allow re usability of objects without creating multiple objects in the repository and inheriting changes made to the source automatically.\n\u2022 Skillfully performed Unit Testing of developed mapping.\n\u2022 Developed automation scripts in UNIX for testing purposes, consequentially reducing the duration of test cycles from 2 weeks to 1 week.\n\u2022 Provided daily communications, scrum notes, sprint reviews reports, project retrospectives, and regular snapshots of project velocity and budget burn rate\n\u2022 Advanced and optimized database queries and procedures in SQL improving application proficiency by 13%.\n\u2022 Awarded as STAR Performer on basis of exceptional service delivery.\n\nTools and Environment: Informatica 9.1, Oracle 11g, PL/SQL, TOAD, MS Access, MS Excel', u'Software Analyst\nHimas Solutions - IN\nMay 2010 to November 2012\nResponsibilities:\n\u2022 Processed data received from vendors and loading them into database. The process was carried out on weekly basis and reports were delivered on bi-weekly basis. The extracted data had to be checked for integrity.\n\u2022 Documented requirements and obtained signoffs.\n\u2022 Coordinated between the Business users and development team in resolving issues.\n\u2022 Documented data cleansing and data profiling.\n\u2022 Wrote SQL scripts to meet the business requirement.\n\u2022 Analyzed views and produced reports.\n\u2022 Tested cleansed data for integrity and uniqueness.\n\u2022 Automated the existing system to achieve faster and accurate data loading.\n\u2022 Generated weekly, bi-weekly reports to be sent to client business team using business objects and documented them too.\n\u2022 Learned to create Business Process Models.\n\u2022 Ability to manage multiple projects simultaneously tracking them towards varying timelines effectively through a combination of business and technical skills.\n\u2022 Good Understanding in clinical practice management, medical and laboratory billing and insurance claim with processing with process flow diagrams.\n\u2022 Assisted QA team in creating test scenarios that covers day in a life of patient for Inpatient and Ambulatory workflows.\n\nTools and Environment: SQL, Windows, Oracle 10g']","[u""Bachelor's""]",[u'']
0,https://resumes.indeed.com/resume/be0502bb8c2600b8,"[u""BIG Data Engineer/Analyst\nDSE HEALTHCARE SOLUTIONS - Edison, NJ\nDecember 2016 to Present\n\u2022 Involved in Big data requirement analysis, develop and design solutions for ETL and Business Intelligence platforms.\n\u2022 Installed Kafka on Hadoop cluster and configured producer and consumer in java to establish connection from source to HDFS with popular hash tags.\n\u2022 Load real time data from various data sources into HDFS using Kafka with spark streaming.\n\u2022 Worked with various columnar storages like PARQUET and ORC.\n\u2022 Implemented ETL using python as language of choice in Spark (Py Spark) for faster testing and processing of data.\n\u2022 Worked with Spark2 (Dataset API) and Spark 1.6.3 (Data Frame and RDD APIs)\n\u2022 Involved in converting Hive/SQL queries into Spark transformations using API's like Spark native methods, Data Frames and python.\n\u2022 Analyzed the SQL scripts and designed the solution to implement using python.\n\u2022 Evaluating Spark code to look for various pain points and improving the performance of the solution to ensure resources are utilized more efficiently.\n\u2022 Evaluating the job Performance based upon Virtual Memory, Physical Memory and CPU peak usage with Pepper Data.\n\u2022 Performed transformations, cleaning and filtering on imported data using Spark Data Frame API, Hive, MapReduce, and loaded final data into Hive.\n\u2022 Involved in converting Map Reduce programs into Spark transformations using Spark python API.\n\u2022 Developed Spark scripts by using python and bash Shell commands as per the requirement.\n\u2022 Worked with NoSQL databases like HBase in creating tables to load large sets of semi structured data coming from source systems.\n\u2022 Scheduled the various parts of the job using oozie\n\u2022 Tools utilized included SQL Server (SSIS/SSAS/SSRS).\n\u2022 Successfully served as consultant on team responsible for implementing the largest and most complex client engagement in company history.\n\u2022 Key responsibilities included supporting requirements across stakeholder groups, data warehouse model design, ETL design hardware/infrastructure planning and procurement, development\nEnvironment: Hadoop, Hive, Spark Data Frame API, HBase, MapReduce, HDFS"", u'BIG Data Engineer\nExpress Scripts, NJ\nFebruary 2014 to December 2016\nThe project was to implement a data lake using Bigdata technologies to identify and exploit new sources of value that exist in an organization and to cultivate future opportunities for value creation and protection.\n\n\u2022 Responsible for a setup of 5 node development cluster for a Proof of Concept which was later implemented as a fulltime project by Express Scripts.\n\u2022 Responsible for Installation and configuration of Hive, Sqoop, Zookeeper and Oozie on the Hortonworks Hadoop cluster using Ambari.\n\u2022 Involved in extracting large sets of structured, semi structured and unstructured data from various sources like ftp servers, API calls.\n\u2022 Developed Sqoop scripts to import data from Oracle database and handled incremental loading on the point of sale tables using Sqoop Metastore.\n\u2022 Created Hive external tables and views, on the data imported into the HDFS.\n\u2022 Developed and implemented Hive scripts for transformations such as evaluation, filtering and aggregation.\n\u2022 Worked on partitioning and bucketing of Hive tables and running the scripts in parallel to reduce run-time of the scripts.\n\u2022 Developed User Defined Functions (UDF) in python if required for hive queries.\n\u2022 Worked with data in multiple file formats including Parquet, Sequence files, ORC and Text(delimited)/CSV.\n\u2022 Used Oozie Operational Services for batch processing and scheduling workflows dynamically.\n\u2022 Worked on creating End-End data pipeline orchestration using Oozie.\n\u2022 Developed bash scripts to automate the above process of Extraction, Transformation and Loading.\n\u2022 Very good experience in managing the Hadoop cluster using Ambari.\n\u2022 Created roles and user groups in Ambari for permitted access to Ambari functions\n\u2022 Working knowledge of MapReduce and YARN (Hadoop2) architectures.\n\u2022 Working knowledge on ZooKeeper.\n\u2022 Working knowledge on Tableau.\nEnvironment: Apache Spark, HDFS, Java, Map Reduce, Hive, HBase, Sqoop, SQL, Oozie, Cloudera Manager, Zoo Keeper, Cloudera.']","[u""Master's""]",[u'']
0,https://resumes.indeed.com/resume/98f3a1f23a771140,"[u'Data Analyst\nIntegra Technologies LLC - Wilmington, DE\nAugust 2017 to Present\nEvaluated current data with historic data to capture competitive market data using data analysis, market feedback.\n\u25c6 Extracted, compiled, analyzed and interpret large data sets using excel spreadsheets and separated into logical parameters.\n\u25c6 Used statistical techniques for hypothesis testing to validate large data sets and interpretation of periodic reports.\n\u25c6 Created pivot tables and charts using excel advanced filters from SQL databases and grouped data to achieve analytical goals.\n\u25c6 Improved demand forecasting that reduced backorders to retail partners by 17% and predicted future demands.\n\u25c6 Learnt to prioritize labor needs by analyzing customer production demand based on schedule and graded on efficiency.\n\u25c6 Compile and validate asset database management and manipulation; and maintain compliance with corporate standards.\n\u25c6 Created interactive dashboards using input from different cross-functional teams through Tableau to identify problem areas and initiate action for improvement.', u""Industrial Data Analyst\nWebbusinesslook - Houston, TX\nJanuary 2017 to August 2017\nMaintained daily, weekly and monthly data queries for multiple departments and provided reports to senior management.\n\u25c6 Performed data manipulation, manually entered data, transformed and cleansed data conflicts and created data reports.\n\u25c6 Data Modeling and analysis of spend data on suppliers and identification of categorical factors influencing performance.\n\u25c6 Interacted with users from different business units to gather and analyze requirements for developing new reports in Excel.\n\u25c6 Scheduled and maintained SQL backups reviewed and validated data points for the plant's Facility Monitoring Service.\n\u25c6 Maintained daily, weekly and monthly data queries for multiple departments and provided reports to senior management.""]","[u'Master of Business Administration in Finance & Marketing', u'Bachelor of Science in Chemistry']","[u""St. Peter's Engineering College Chennai, Tamil Nadu\nJanuary 2003 to January 2005"", u'S.F.R.College Sivakasi, Tamil Nadu\nJanuary 2000 to January 2003']"
0,https://resumes.indeed.com/resume/88cf489d0dd2ef2d,"[u'Data Analyst\nHiTouch Business Services - Nashville, TN\nJune 2016 to Present\nDuties include using ETL packages and T-SQL queries to develop and maintain the data warehouse and ensure data integrity. Reporting services are\ndeveloped and maintained to provide dashboards, automated email subscription reports, and on-demand ad hoc reports for end users. Further\nanalysis performed using Excel, R Studio and Python. Work with end users to define business needs, identify ways to enhance current applications, and develop improvements in business processes.']",[u'BS in Computer Information Systems'],"[u'MIDDLE TENNESEE STATE UNIVERSITY Murfreesboro, TN\nJanuary 2016 to May 2019']"
0,https://resumes.indeed.com/resume/a1d3679eb39b9b1b,"[u'Data Analyst\nUniversity of Wyoming\nFebruary 2017 to May 2017\nBillboard Top 100 data was parsed with Python and then analyzed in R to find which musical properties make a song popular. Done at University\nof Wyoming.', u'Data Analyst\nWestern Ecosystem Technologies\nMay 2016 to August 2016\nData analysis job with Western Ecosystem Technologies (WEST) at the Laramie branch in Wyoming. Job duties involved pulling large amounts of messy data from Access, formatting it in R, and pushing it to a new\nSQL server.']",[u'B.S in Statistics and Biology'],[u'Univ. Wyoming']
0,https://resumes.indeed.com/resume/0d48813693cb28b7,"[u'DATA ANALYST\nINFORMATION MANAGEMENT SERVICES\nMay 2015 to Present\nContribute to a dynamic growing\ncompany while leveraging and Create, manage, maintain, and interpret databases using SAS for growing my current the National Cancer Institute to support work with researchers on\nmathematical and data analyst Cancer Studies from inception to final analysis.\nskills.\nPerform data validation and quality control checks to ensure\nquality data, and develop reports that accurately and comprehensively represent data.']",[u'BS in MATHEMATICS'],[u'TOWSON UNIVERSITY\nMay 2015']
0,https://resumes.indeed.com/resume/0637d3e04ce40ee3,"[u'Data Analyst\nFaircom New York Inc - New York, NY\nJune 2016 to Present\n\u2022Conducted machine learning projects to determine the pattern for whether a lapsed donor will donate again. Utilized various data modeling methodologies to select most valuable donors. The response rate increased by 200 percent after implement the model.\n\u2022Lead a biographic modeling project to determine the association between donation and donors\u2019 demographic information.\n\u2022Performed RFM analysis, donor funnel analysis, retention analysis and A/B testing to generate and analyze the direct mail and digital campaign performance reports for clients and deliver actionable business insights. Direct mail donations increased 50% by using data driven fundraising strategy.\n\u2022Developed database system to automatize periodical reporting processes using SQL, Python, and Tableau.', u'Research Assistant\nSchool of Information Studies, Syracuse University\nMarch 2014 to June 2016\n\u2022 Collect and annotated Facebook posts sent by four large US municipal police departments.\n\u2022 Conducted F-test, Chi-square test, MANOVA and multiple statistical methods to analyzed how users engaged with different post types and different topics.\n\u2022 Applied text mining and natural language processing technologies to provide practical insights for police\nadministrators and community members.\n\u2022 Analyzed how users engaged with different post types and different topics.']","[u'M.S. in Applied Statistics in Applied Statistics', u'M.S. in Information in Management', u'B.S. in Actuarial Science in Actuarial Science']","[u'Syracuse University, College of Art & Science, Syracuse\nJanuary 2016 to May 2016', u'Syracuse University, School of Information Studies, Syracuse\nMay 2016', u'Ohio University Athens, OH\nMay 2013']"
0,https://resumes.indeed.com/resume/848a0938bd84becb,"[u'Data Analyst Intern\nSBI Consultants, Inc - New York, NY\nSeptember 2017 to December 2017\n1. Designed predictive models using Regression and Random Forests to predicted costs of real estate projects based on existing projects using scikit-learn in Python\n2. Automated data preprocessing process, aggregated data from SQL databases, visualized datasets using Python and performed PCA dimensionality reduction\n3. Performed ad hoc statistical analysis, compared KPIs of different model designs and measured the validity of different predictive models\n4. Communicated model design and deliverables to decision makers in an explicit and accessible manner', u'Data Science Intern\nKeller Willams - New York, NY\nOctober 2016 to November 2016\n1. Performed time series analysis using ARIMA to predict real estate market trends in New York City across different property types and boroughs\n2. Designed and built a linear regression model to predict condo house value based on housing features and analyzed key parameters that affect pricing\n3. Presented predictive modeling and visualizations to clients to assist in brokerage']","[u'Master of Science in STEM, Civil Engineering and Engineering Mechanics', u'Bachelor of Engineering in Construction Management']","[u'COLUMBIA UNIVERSITY New York, NY\nSeptember 2016 to December 2017', u'TSINGHUA UNIVERSITY BEIJING, CN\nSeptember 2012 to July 2016']"
0,https://resumes.indeed.com/resume/2e9abca8b10ca785,"[u""Data Scientist\nBristol Myers Squibb, NY\nJuly 2017 to Present\nDescription: Bristol-Myers Squibb Company, incorporated on August 11, 1933, is engaged in the discovery, development, licensing, manufacturing, marketing, distribution and sale of biopharmaceutical products. The Company's pharmaceutical products include chemically synthesized drugs, or small molecules, and products produced from biological processes called biologics.\nResponsibilities:\n\n\u2022 As an Architect design conceptual, logical and physical models using Erwin and build DataMart's using hybrid Inmon and Kimball DW methodologies.\n\u2022 Worked closely with business, data governance, SMEs and vendors to define data requirements.\n\u2022 Designed and provisioned the platform architecture to execute Hadoop and machine learning use cases under Cloud infrastructure, AWS, EMR, and S3.\n\u2022 Selection of statistical algorithms - (Two Class Logistic Regression Boosted Decision Tree, Decision Forest Classifiers etc)\n\u2022 Used MLlib, Spark's Machine learning library to build and evaluate different models.\n\u2022 Executed ad-hoc data analysis for customer insights using SQL using Amazon AWS Hadoop Cluster.\n\u2022 Worked on predictive and what-if analysis using R from HDFS and successfully loaded files to HDFS from Teradata, and loaded from HDFS to HIVE.\n\u2022 Designed the schema, configured and deployed AWS Redshift for optimal storage and fast retrieval of data.\n\u2022 Worked in using Teradata14 tools like Fast Load, Multi Load, T Pump, Fast Export, Teradata Parallel Transporter (TPT) and BTEQ.\n\u2022 Involved in creating Data Lake by extracting customer's Big Data from various data sources into Hadoop HDFS. This included data from Excel, Flat Files, Oracle, SQL Server, Mongo DB, Cassandra, HBase, Teradata, Netezzaand also log data from servers\n\u2022 Used Spark Data frames, Spark-SQL, Spark MLLib extensively and developing and designing POC's using Scala, Spark SQL and MLlib libraries.\n\u2022 Created high level ETL design document and assisted ETL developers in the detail design and development of ETL maps using Informatica.\n\u2022 Used R, SQL to create Statistical algorithms involving Multivariate Regression, Linear Regression, Logistic Regression, PCA, Random forest models, Decision trees, Support Vector Machine for estimating the risks of welfare dependency.\n\u2022 Helped in migration and conversion of data from the Sybase database into Oracle database, preparing mapping documents and developing partial SQL scripts as required.\n\u2022 Generated ad-hoc SQL queries using joins, database connections and transformation rules to fetch data from legacy Oracle and SQL Server database systems.\n\u2022 Analyzed data and predicted end customer behaviors and product performance by applying machine learning algorithms using Spark MLlib.\n\u2022 Performed data mining on data using very complex SQL queries and discovered pattern and Used extensive SQL for data profiling/analysis to provide guidance in building the data model\n\nEnvironment: R, Machine Learning, Teradata 14, Hadoop Map Reduce, Pyspark, Spark, R, Spark MLLib, Tableau, Informatica, SQL, Excel, VBA, BO, CSV, Erwin, SAS, AWS Redshift, Scala Nlp, Cassandra, Oracle, MongoDB, Informatica MDM, Cognos, SQL Server 2012, Teradata, DB2, SPSS, T-SQL, PL/SQL, Flat Files, XML, and Tableau"", u'Data Scientist\nFreddie Mac - VA\nMay 2014 to June 2015\nDescription:Freddie Mac is a shareholder-owned corporation whose people are dedicated to improving the quality of life by making the American dream of decent, accessible housing a reality. We accomplish this mission by linking Main Street to Wall Street--purchasing, securitizing and investing in home mortgages, and ultimately providing homeowners and renters with lower housing costs and better access to home financing.\n\nResponsibilities:\n\u2022 Involved in data management including Data Modeling, Metadata, Data Analysis, Data mapping and Data Dictionaries, Erwin9.1 and involved in Data Modeling (Oracle/MySQL/Netezza), Data Characterization, Workflow design and implementation\n\u2022 Used Erwin Data Modeler and Erwin Model Manager to create Conceptual, Logical and Physical data models and maintain the model versions in Model Manager for further enhancements.\n\u2022 Worked on logical and physical modeling of various data marts as well as data warehouse using Taradata14.\n\u2022 Created and maintained Logical Data Model (LDM) for the project. Includes documentation of all entities, attributes, data relationships, primary and foreign key structures, allowed values, codes and glossary terms in accordance with the Corporate Data Dictionary etc.\n\u2022 Implemented Bulk Load Process by converting existing Triggers to Oracle11g packages to improve the Data loading process.\n\u2022 Designed and Developed Oracle11g, PL/SQL Procedures and UNIX Shell Scripts for Data Import/Export and Data Conversions.\n\u2022 Maintained warehousemetadata, naming standards and warehouse standards.\n\u2022 Involved in the validation of the OLAP Unit testing and System Testing of the OLAP Report Functionality and data displayed in the reports.\n\u2022 Worked in using Teradata14.1 tools like Fast Load, Multi Load, T Pump, Fast Export, Teradata Parallel Transporter (TPT) and BTEQ.\n\u2022 Designed the overall ETL solution including analyzing data, preparation of high level, detailed design documents, test plans and deployment strategy.\n\u2022 Strong knowledge of Entity-Relationship concept, Facts and dimensions tables, slowly changing dimensions and Dimensional Modeling(Star Schema and Snow Flake Schema).\n\u2022 Created Software Development Life Cycle (SDLC) document and excelled in the process of Change Management, Release management and Configuration processes.\n\u2022 Prepared Data Visualization reports for the management using R\n\u2022 Documented logical, physical, relational and dimensional data models. Designed the Data Marts in dimensional data modeling using star and snowflake schemas.\n\u2022 Involved in Normalization and De-Normalization of existing tables for faster query retrieval.\n\u2022 Create MDM base objects, Landing and Staging tables to follow the comprehensive data model in MDM.\n\u2022 Used ETL methodology for supporting data extraction, transformations and loading processing, in a complex MDM using Informatica.\n\u2022 Performed Data Validation and Data Cleaning using PROC SORT, PROC FREQ and through various SAS formats.\n\u2022 Designed Data Staging Area and Data Warehouse to integrate the data from various sources including Flat Files to facilitate management to make more fact based decisions.\n\u2022 Created jobs, alerts to run SSIS, SSRS packages periodically. Created the automated processes for the activities such as database backup processes and SSIS, SSRS Packages run sequentially using SQL Server Agent job and windows Scheduler.\n\u2022 Managed data in different locations using different data marts using Azure.\n\u2022 Perform reverse engineering of physical data models from databases and SQL scripts.\n\u2022 Involved in Normalization (3rd normal form), De-normalization (Star Schema for Data Warehousing.\n\u2022 Used SSIS to create ETL packages to validate, extract, transform and load data to data warehouse databases, data mart databases to OLAP databases.\n\u2022 Implemented slowly changing dimensions Type2 and Type3 for accessing history of reference data changes.\n\u2022 Extensively used SAS to query and subset data, summarize and present data, combine tables using joins and merges and created and modified tables.\n\u2022 Combined views and reports into interactive dashboards in Tableau Desktop that were presented to Business Users, Program Managers, and End Users.\n\u2022 Utilized SDLC and Agile methodologies such as SCRUM.\n\u2022 Worked in PL/SQL Programming (Stored procedures, Triggers, Packages) using Oracle (SQL, PL/SQL), SQL Server2008 and UNIX shell scripting to perform job scheduling.\n\nEnvironment: ERwin9.1, Teradata14, Oracle10g, PL/SQL, UNIX, Agile, Azure, TIDAL, MDM, ETL, BTEQ, SQL Server2008, Informatica MDM, Netezza, DB2, SAS, Tableau, UNIX, SSRS, SSIS, T-SQL, MDM, Informatica, SQL', u""Data/Business Analyst\nRabobank - New York, NY\nJune 2013 to April 2014\nDescription:Cooperative Rabobank U.A., New York Branch is a debt issuing vehicle. The company is based in New York, New York. Cooperative Rabobank U.A., New York Branch operates as a subsidiary of Cooperative Rabobank U.A.\nResponsibilities:\n\n\u2022 Coded R functions to interface with CaffeDeepLearning Framework\n\u2022 Working in AmazonWebServices cloud computing environment\n\u2022 Worked with several R packages including knitr, dplyr, SparkR, CausalInfer, spacetime.\n\u2022 Implemented end-to-end systems for DataAnalytics, DataAutomation and integrated with custom visualization tools using R, Mahout, Hadoop and MongoDB.\n\u2022 Gathering all the data that is required from multiple data sources and creating datasets that will be used in analysis.\n\u2022 Performed Exploratory DataAnalysis and DataVisualizations using R, andTableau.\n\u2022 Perform a proper EDA, Univariate and bi-variate analysis to understand the intrinsic effect/combined effects.\n\u2022 Worked with Data governance, Data quality, data lineage, Data architect to design various models and processes.\n\u2022 Independently coded new programs and designed Tables to load and test the program effectively for the given POC's using with Big Data/Hadoop.\n\u2022 Designed data models and data flow diagrams using Erwin and MSVisio.\n\u2022 Performed datacleaning and imputation of missing values using R.\n\u2022 Worked with Hadoop eco system covering HDFS, HBase, YARN and MapReduce\n\u2022 Take up ad-hoc requests based on different departments and locations\n\u2022 Used Hive to store the data and perform datacleaning steps for huge datasets.\n\u2022 Created dash boards and visualization on regular basis using ggplot2 and Tableau\n\u2022 Creating customized business reports and sharing insights to the management.\n\u2022 Worked with BTEQ to submit SQL statements, import and export data, and generate reports in Teradata.\nInteracted with the other departments to understand and identify dataneeds and requirements and work with other members of the ITorganization to deliver data visualization and reportingsolutions to address those needs.\n\nEnvironment: Erwin r9.0, Informatica 9.0, ODS, OLTP, Oracle 10g, Hive, OLAP, DB2, Metadata, MS Excel, Mainframes MS Visio, Rational Rose, Requisite Pro, Hadoop, PL/SQL, etc"", u'Data/Business Analyst\nCoventry Health Care - Minneapolis, MN\nSeptember 2010 to December 2012\nDescription: Coventry Health Care Management utilizes multiple software systems to support the intake and processing of authorization requests, the exchange of data between the payer and vendors contracted to perform services on our behalf, manage Case and Disease programs, provide robust reporting and decision support, and generally automate and facilitate their business processes.\nResponsibilities:\n\u2022 Participated in JAD sessions, gathered information from Business Analysts, end users and other stakeholders to determine the requirements.\n\u2022 Worked in Data warehousing methodologies/Dimensional Data modeling techniques such as Star/Snowflake schema using ERWIN9.1.\n\u2022 Extensively used Aginity Netezza workbench to perform various DDL, DML etc. operations on Netezza database.\n\u2022 Involved in Perform Daily Monitoring of Oracle instances using Oracle Enterprise Manager, ADDM, TOAD, monitor users, table spaces, memory structures, rollback segments, logs, and alerts.\n\u2022 Used ER Studio Data/ Modeler for data modeling (data requirementsanalysis, database design etc.) of custom developed information systems, including databases of transactional systems and data marts.\n\u2022 Involved in Teradata SQL Development, Unit Testing and Performance Tuning and to ensure testing issues are resolved on the basis of using defect reports.\n\u2022 Generated DDL scripts using Forward Engineering technique to create objects and deploy them into the databases.\n\u2022 Involved in database testing, writing complex SQL queries to verify the transactions and business logic like identifying the duplicate rows by using SQL Developer and PL/SQL Developer.\n\u2022 Used Teradata SQL Assistant, Teradata Administrator, PMON and data load/export utilities like BTEQ, FastLoad, Multi Load, Fast Export, Tpump on UNIX/Windows environments and running the batch process for Teradata.\n\u2022 Worked on data profiling and data validation to ensure the accuracy of the data between the warehouse and source systems.\n\u2022 Worked on Data warehouse concepts like Data warehouse Architecture, Star schema, Snowflake schema, and Data Marts, Dimension and Fact tables.\n\u2022 Developed SQL Queries to fetch complex data from different tables in remote databases using joins, database links and Bulk collects.\n\u2022 Wrote TSQL like Indexes, Views, Stored Procedures and Triggers in SSMS to fulfill Requirements.\n\u2022 Involved in Database migrations from legacy systems, SQL server to Oracle and Netezza.\n\u2022 Used SSIS to create ETL packages to validate, extract, transform and load data to pull data from Source servers to staging database and then to Netezza Database and DB2 Databases.\n\u2022 Worked on SQL Server concepts SSIS (SQL Server Integration Services), SSAS (Analysis Services) and SSRS (Reporting Services).\n\nEnvironment: Windows XP , SQL Developer, MS-SQL 2008 R2, MS-Access, MS Excel and SQL-PLU, Java', u'Data Analyst/Data Modeler\nSystrac Solutions - Hyderabad, Telangana\nJanuary 2009 to December 2010\nResponsibilities:\n\n\u2022 Worked with project team representatives to ensure that logical and physical ER/Studio data models were developed in line with corporate standards and guidelines.\n\u2022 Involved in defining the source to target data mappings, business rules, data definitions.\n\u2022 Worked with BTEQ to submit SQL statements, import and export data, and generate reports in Teradata.\n\u2022 Responsible for defining the key identifiers for each mapping/interface.\n\u2022 Responsible for defining the functional requirement documents for each source to target interface.\n\u2022 Document, clarify, and communicate requests for change requests with the requestor and coordinate with the development and testing team.\n\u2022 Work with users to identify the most appropriate source of record and profile the data required for sales and service.\n\u2022 Document the complete process flow to describe program development, logic, testing, and implementation, application integration, coding.\n\u2022 Involved in defining the business/transformation rules applied for sales and service data.\n\u2022 Define the list codes and code conversions between the source systems and the data mart.\n\u2022 Worked with internal architects and, assisting in the development of current and target state data architectures.\n\u2022 Coordinate with the business users in providing appropriate, effective and efficient way to design the new reporting needs based on the user with the existing functionality.\n\u2022 Remain knowledgeable in all areas of business operations in order to identify systems needs and requirements.\n\nEnvironment: Erwin r7.0, SQL Server 2000/2005, Windows XP/NT/2000, Oracle 8i/9i, MS-DTS, UML, UAT, SQL Loader, OOD, OLTP, PL/SQL, MS Visio, Informatica.', u'Data/Business Analyst\nPeople Tech Group - IN\nMarch 2010 to September 2010\nResponsibilities:\n\u2022 Writing and executing customized SQL code for ad hoc reporting duties and used other tools for routine\n\u2022 Developed stored procedures and complex packages extensively using PL/SQL and shell programs\n\u2022 Involved in customized reports using SAS/MACRO facility, PROC REPORT, PROC TABULATE and PROC\n\u2022 Generated ad-hoc SQL queries using joins, database connections and transformation rules to fetch data from legacy SQL Server database systems\n\u2022 Used existing UNIX shell scripts and modified them as needed to process SAS jobs, search strings, execute permissions over directories etc.\n\u2022 Extensively used Star Schema methodologies in building and designing the logical data model into Dimensional Models\n\u2022 Involved in designing Context Flow Diagrams, Structure Chart and ER- diagrams\n\u2022 Worked on database features and objects such as partitioning, change data capture, indexes, views, indexed views to develop optimal physical data mode\n\u2022 Worked with SQL Server Integration Services in extracting data from several source systems and transforming the data and loading it into ODS\n\u2022 Involved in Data Analysis, Data Validation, Data Cleansing, Data Verification and identifying data mismatch.\n\nEnvironment: Windows XP , SQL Developer, MS-SQL 2008 R2, MS-Access, MS Excel and SQL-PLU, Java']","[u""Bachelor's""]",[u'']
0,https://resumes.indeed.com/resume/49cfee6a8ef6a84d,"[u""Data Analyst\nCobena B.A.S., Inc\nJanuary 2017 to January 2018\n\u2022 Scraped websites using Python to acquire data for analysis projects\n\u2022 Prepared data visualizations using R, Tableau, Excel and PowerBI\n\u2022 Collaborated with a team to analyze client's data and determine drivers of sales\n\u2022 Developed code using Python for an in-house project focusing on healthcare data""]",[u'BS in Computer Science'],[u'University of the Philippines\nJanuary 2013 to January 2017']
0,https://resumes.indeed.com/resume/9cb6f07209fb34a2,"[u""Sr. Data Modeler/Data Analyst\nAT&T - Edison, NJ\nJanuary 2016 to December 2016\nResponsibilities:\n\u2022 Gathered business requirements, working closely with business users, project leaders and developers. Analyzed the business requirements and designed Conceptual and Logical Data models.\n\u2022 Created Logical & Physical Data Model on Relational (OLTP) on Star schema for Fact and Dimension tables using Erwin.\n\u2022 Performed GAP analysis to analyze the difference between the system capabilities and business requirements.\n\u2022 Used Agile Method for daily scrum to discuss the project related information.\n\u2022 Prepared ETL technical Mapping Documents along with test cases for each Mapping for future developments to maintain Software Development Life Cycle (SDLC).\n\u2022 Involved in Data flow analysis, Data modeling, Physical database design, forms design and development, data conversion, performance analysis and tuning.\n\u2022 Created sampling specification, performed quality assurance, created multiple datasets to communicate with Medicare Advantage plans and analyzed Medicare plans response.\n\u2022 Created and maintained data model standards, including master data management (MDM) and Involved in extracting the data from various sources like Oracle, SQL, Teradata, and XML.\n\u2022 Designed the data marts using the Ralph Kimball's Dimensional Data Mart modeling methodology using Erwin.\n\u2022 Worked with medical claim data in the Oracle database for Inpatient/Outpatient data validation, trend and comparative analysis.\n\u2022 Proficient in developing Entity-Relationship diagrams, Star/Snow Flake Schema Designs, and expert in modeling Transactional Databases and Data Warehouse.\n\u2022 Implemented Forward engineering to create tables, views and SQL scripts and mapping documents.\n\u2022 Used reverse engineering to connect to existing database and create graphical representation (E-R diagram)."", u'Sr. Data Modeler/Data Analyst\nExperian - Costa Mesa, CA\nOctober 2015 to November 2016\nResponsibilities:\n\u2022 Interacted with the end users to understand the business requirement and identified data sources.\n\u2022 Involved in regular interactions with Business Analysts and participated in data modeling JAD sessions.\n\u2022 Provided subject matter expertise as appropriate to ETL requirements, information analytics, modeling & design, development, and support activities.\n\u2022 Involved in review of the data model and design to monitor the data model changes and system design across Enterprise Data Warehouse.\n\u2022 Developed the logical data models and physical data models that capture current state/future state data elements and data flows using ER/Studio.\n\u2022 Translated business requirements into conceptual, logical data models and integration data models, model databases for integration applications in a highly available and performance configuration using ER/Studio.\n\u2022 Reversed Engineered Oracle and then forward engineered them to Teradata using ER/Studio.\n\u2022 Designed 3rd normal form target data model and mapped to logical model.\n\u2022 Involved in non-transactional data entities of Master Data Management of the organization to provide processes for collecting, aggregating, matching, consolidating, quality-assurance, persistence and distribution.\n\u2022 Responsible for data governance rules and standards to maintain the consistency of the business element names in the different data layers.\n\u2022 Involved in extensive Data validation using SQL queries and back-end testing\n\u2022 Implemented Star Schema methodologies in modeling and designing the logical data model into Dimensional Models.\n\u2022 Extensively used Normalization techniques (up to 3NF).\n\u2022 Responsible for data modeling and building a star schema model in ER/Studio.\n\u2022 Created the data mapping specification in Talend metadata tool to identify the data lineage and data impact of the different system.\n\u2022 Generated DDL statements for the creation of new ER/studio objects like table, views, indexes, packages and stored procedures.\n\u2022 Used SQL for Querying the database in Unix environment\n\u2022 Tested the database to check field size validation, check constraints, stored procedures and cross verifying the field size defined within the application with metadata.\n\u2022 Worked on creation of metadata collection and validation using 3NF; ODS schema maintenance; database-to-database data type.\n\u2022 Designed and developed cubes using SQL Server Analysis Services (SSAS) using Microsoft Visual Studio.\n\u2022 Responsible for different Data mapping activities from Source systems to Teradata.\n\u2022 Developed rule sets for data cleansing and actively participated in data cleansing and anomaly resolution of the legacy application.\n\u2022 Responsible for physical modeling per database and performance requirements.\n\u2022 Worked closely with the ETL SSIS Developers to explain the complex Data Transformation using Logic.\n\u2022 Created and deployed reports using SSRS.\n\u2022 Work inclosed with some Data architects and understands Cassandra data modeling using NoSQL and expert level knowledge designing data models based on access patterns (Query driven data model).\nEnvironment: ER/Studio v17, Oracle 12c, Teradata r15, SQL, DDL, UNIX, SSAS, Microsoft Visual Studio 2016, DB2, SSIS, SSRS, ODS, NoSQL, Cassandra.', u'Sr. Data Modeler/Data Analyst\nCare Source - Dayton, OH\nApril 2013 to September 2015\nResponsibilities:\n\u2022 Analyzed business requirements, system requirements, data mapping specification requirements, and responsible for documenting functional and supplementary requirements.\n\u2022 Analyzed functional and non-functional data elements for data profiling and mapping from source to target data environment. Developed working documents to support findings and assign specific tasks.\n\u2022 Experienced on Software Development Life Cycle (SDLC), testing methodologies, resource management and scheduling of tasks.\n\u2022 Conducted data modeling JAD sessions and communicated data related standards.\n\u2022 Developed Conceptual, Logical and Physical data models for central model consolidation.\n\u2022 Facilitated transition of logical data models into the physical database design and recommended technical approaches for good data management practices.\n\u2022 Worked with DBA group to create Best-Fit Physical Data Model from the Logical Data Model using Forward Engineering.\n\u2022 Used ERWIN for effective model management of sharing, dividing and reusing model information and design for productivity improvement.\n\u2022 Redefined many attributes and relationships in the reverse engineered model and cleansed unwanted tables/columns as part of data analysis responsibilities.\n\u2022 Experienced in Normalization (1NF, 2NF and 3NF) and De-normalization techniques for improved database performance in various environments.\n\u2022 Created Source to Target mappings and Transformations.\n\u2022 Facilitated development, testing and maintenance of quality guidelines and procedures along with necessary documentation.\n\u2022 Generated ad-hoc SQL queries using joins, database connections and transformation rules to fetch data from legacy DB2 and SQL Server database systems.\n\u2022 Used ETL Informatica tool to populate the database, data transformation from the old database to the new database using SQL.\n\u2022 Enforced referential integrity in the OLTP data model for consistent relationship between tables and efficient database design.\n\u2022 Extracted data from the databases (Oracle and SQL Server, DB2, Flat Files) using Informatica to load it into a single data warehouse repository.\n\u2022 Documented testing methodology and devised test scripts and plans to help the QA team in the functional aspect of the package and apply the same before submission to the client.\n\u2022 Assisted in designing implementation plan and monitored the same by actively participating in user training, live data entry and obtained sign-off from the concerned departmental head.\n\u2022 Experienced in designing, error handling and exception handling procedures to identify, record and report errors.\nEnvironment: Erwin 9.0, DB2, SQL, Informatica, OLTP, DB2, Flat files, SQL Server 2008, Agile, Oracle 11g', u'Sr. Data Analyst/Data Modeler\nState Street - Boston, MA\nMay 2012 to March 2013\nResponsibilities:\n\u2022 Participated in requirement gathering sessions, conducted JAD sessions with users, subject matter experts and business analysts.\n\u2022 Developed conceptual model using Erwin based on business requirements.\n\u2022 Developed and normalized logical and physical data base model using OLTP systems for finance applications.\n\u2022 Extensively used normalization techniques (up to 3NF).\n\u2022 Produced functional decomposition diagrams and defined logical data model.\n\u2022 Involved in redesigning of the existing OLTP systems, modification and designing new requirements in the existing systems.\n\u2022 Involved in designing the context flow diagrams, structure chart and ER-diagrams.\n\u2022 Performed forward engineering to create a physical SAS model with DDL, based on the requirements from logical data model.\n\u2022 Used SSIS and T-SQL stored procedures to transfer data from OLTP databases to staging area and finally transfer into data-mart.\n\u2022 Worked closely with ETL SSIS developers to explain the complex data transformation using logic.\n\u2022 Implemented referential integrity using primary key and foreign key relationships.\n\u2022 Involved in development and implementation of SSIS, SSRS and SSAS applications.\n\u2022 Performed extensive data analysis and data validation on Teradata.\n\u2022 Generated ad-hoc SQL queries using joins, database connections and transformation rules to fetch data from legacy oracle and SQL server database systems.\n\u2022 Assisted Reporting developers in building Reports using Crystal Reports.\n\u2022 Used Erwin for reverse engineering to connect to existing database and ODS to create graphical representation in the form of Entity Relationships and elicit more information.\n\u2022 Acted as strong Data Analyst analyzing the data from low level in conversion projects, provided mapping documents between legacy, production and user interface systems.\n\u2022 Used Model Mart of Erwin for effective model management of sharing, dividing and reusing model information and design for productivity improvement.\n\u2022 Interacted with client, management and staff to identify and document business needs and objectives, current operational procedures for creating the logical data model.\n\u2022 Facilitated in developing testing procedures, test cases and User Acceptance Testing (UAT).\n\u2022 Extensively used SSIS import/export wizard for performing the ETL operations.\nEnvironment: JAD, Erwin7.5, OLTP, 3NF, DDL, T-SQL, SSRS, SSAS, Teradata13, Crystal Reports7.0, ODS, UAT', u'Data Analyst/Data Modeler\nITC Infotech - Bengaluru, Karnataka\nAugust 2009 to April 2012\nResponsibilities:\n\u2022 As a Data Modeler/Data Analyst I was responsible for all data related aspects of a project.\n\u2022 Translated business and data requirements into as a Data Modeler/Data Analyst I was responsible for all data related aspects of a project.\n\u2022 Ensured data design follows the prescribed reference architecture and framework.\n\u2022 Ensured design reflects appropriate business rules, and facilitates data integration, data conformity and data integrity; Ensure data structures are designed for flexibility to support future business needs and enhancements.\n\u2022 Developed 3NF modeling and dimensional modeling constructs.\n\u2022 Worked at conceptual/logical/physical data model level using Erwin according to requirements.\n\u2022 Involved in exhaustive documentation for technical phase of the project and training materials for all data management functions.\n\u2022 Used Reverse Engineering approach to redefine entities, relationships and attributes in the data model as per new specifications in Erwin after analyzing the database systems currently in use.\n\u2022 Translated business and data requirements into Logical data models in support of Enterprise Data Models, OLTP, Operational Data Structures and Analytical systems.\n\u2022 Enforced referential integrity in the OLTP data model for consistent relationship between tables and efficient database design.\n\u2022 Developed the required data warehouse model using Star schema for the generalized model.\n\u2022 Used forward engineering approach for designing and creating databases for OLAP model.\n\u2022 Conducted design walk through sessions with Business Intelligence team to ensure that reporting requirements are met for the business.\n\u2022 Collaborated with ETL, BI and DBA teams to analyze and provide solutions to data issues and other challenges while implementing the OLAP model.\n\u2022 Designed and govern data services messaging constructs in XSD format.\n\u2022 Applied conditional formatting in SSRS to highlight key areas in the report data.\n\u2022 Understand the relationships across business information and units of data; collaborate with Business Analysts, Data Stewards and Business Customers to identify data usage patterns and to formulate Business names, definitions and data quality rules for data elements.\n\u2022 Performed source data analysis, data discovery, data profiling and data mapping.\n\u2022 Worked on all data management activities on the project data sources, data migration.\n\u2022 Worked on creating DDL, DML scripts for the data models.\n\u2022 Performance query tuning to improve the performance along with index maintenance.\n\u2022 Understand database performance factors and trends pertaining to very large database design and collaborate with DBAs to implement mitigating physical modeling solutions; provide data structures optimized for information entry and retrieval.\n\u2022 Worked on stored procedures for processing business logic in the database.\n\u2022 Self-directed individual contributor with skills at matrixes and virtual team contribution/integration and with experience in leading and directing other team members. In support of Enterprise Data Models, OLTP, Operational Data Structures and Analytical systems.\n\u2022 Extracted data from databases like Oracle, SQL server and DB2 using Informatica to load it into a single repository for data analysis.\n\u2022 Designed and Developed Oracle PL/SQL Procedures Linux and Unix Shell Scripts for data Import/Export and data Conversions.\nEnvironment: Erwin7.0, 3NF, OLTP, SSRS, DDL, DML, SQL Server2005, Linux.']","[u""Bachelor's""]",[u'']
0,https://resumes.indeed.com/resume/3fdfefce5ed05cc0,"[u'Crime/Data Analyst\nCity of Dallas, Dallas Police Department - Dallas, TX\nMay 2015 to Present\n\u2022 Contributes to the significant reduction in crime rates in Dallas through predictive analysis\n\u2022 Develops databases based on established business rules\n\u2022 Extracts, Transforms, and Loads (ETL) datasets for use by the Police Department\n\u2022 Administers SQL Server Services including Installation, configuration, creating user logins, and assigning permissions\n\u2022 Generates reports, dashboards, and stories using MS Excel, SSRS, Power BI, and Tableau for easy monitoring of crime and arrests data as well as aid in effective decision making\n\u2022 Generates maps and models to predict crime trends thus helping with allocation of personnel to crime hotspots within the city\n\u2022 Optimizes SSIS packages and stored procedures that improve the processing speed of large files\n\u2022 Creates tables, figures, and reports needed by both internal and external clients using SAS/BASE, SAS/STAT, PROC CHART and GPLOT\n\u2022 Builds cubes in SSAS to facilitate data analysis\n\u2022 Creates data dictionary for managerial references.\n\u2022 Utilizes Excel to provide comprehensive analytical and reporting support to identify opportunities to impact operational results, changes to processes, and support strategic initiatives.\n\u2022 Creates hotspot and density maps to help visualize the spatial variations in Crime within the city thus aiding with the allocation of scarce resources.', u""Data Analyst\nUniversity of North Texas, Toulouse Graduate School - Denton, TX\nDecember 2013 to July 2014\n\u2022 Advanced Toulouse Graduate School's knowledge of which students need help with their PhD program based on outcome of research\n\u2022 Integrated and analyzed data from multiple sources using SSIS and SSAS\n\u2022 Built reports using MS Excel, SPSS Base to show milestone status of PhD students at the University.\n\u2022 Presented monthly and quarterly projects reports to management team using MS-PowerPoint. This allowed the Graduate School to identify which colleges and students need the most help regarding staying on track"", u'Research Analyst\nUniversity of North Texas, Geography Department - Denton, TX\nJanuary 2011 to August 2013\n\u2022 Conducted extensive scientific research on HIV and Diabetes in Texas; Slum Health in Ghana, using SPSS, ArcGIS, Excel, and SAS\n\u2022 Visualized research data using Excel for easy presentation\n\u2022 Applied advanced SPSS skills such as ANOVA, Multiple Regression, and T-tests to convey research results\n\u2022 Created shapefiles and metadata for use by other members of research team', u'Data Analyst\nEnvironmental Protection Agency - Accra, GH\nAugust 2009 to November 2010\n\u2022 Built and maintained accurate database of all mining permits in Ghana using MS Access and SQL 2008\n\u2022 Assisted in compliance monitoring and inspection of mining concessions\n\u2022 Developed monthly mining reports using SSRS, MS Access, and MS Excel for internal and external clients\n\u2022 Reviewed Environmental Management Plans and Environmental Impact Assessment reports prior to providing companies with permits\n\u2022 Used ArcMap to create choropleth maps and density maps about mining areas in Ghana']","[u'Master of Business Administration in Business Analytics and Finance', u'Master of Science in Applied Geography', u'', u'in Data Mining', u'Bachelor of Arts in Geography']","[u'University of North Texas\nAugust 2018', u'University of North Texas\nAugust 2013', u'College of Arts and Sciences, University of North Texas\nApril 2013', u'University of North Texas\nApril 2012', u'University of Ghana Accra, GH\nFebruary 2009']"
0,https://resumes.indeed.com/resume/d4845b3c6f5f0352,"[u'Human Expectancy Prediction-Data Mining\nJanuary 2017 to January 2017\n\u2022 Identified five models to predict human expectancy with R Studio using ten factors and compare accuracy to select best model\n\u2022 Performed data cleaning using average value and visualized data in Tableau to rule out irrelevant\nvariables\n\u2022 Implemented shrinkage method (lasso) to increase accuracy of linear regression model by 11%', u'Music Recommendation System-Big Data\nJanuary 2017 to January 2017\nCreated Spark function to calculate song popularity (playing times) to help identify music\nrecommendation for 1000 users\n\u2022 Executed Hadoop and Spark commands with Python to join and filter data and implemented SQLite to read and convert data format\n\u2022 Achieved user-friendly frontend application using C# with Visual Studio and presented to 30+ people', u'Online Product Return System-Agile Team\nJanuary 2017 to January 2017\nDeveloped an online product return and custom feedback system with Django and AWS using agile\nmethodology\n\u2022 Updated task board with Trello and product backlog on each sprint planning meeting\n\u2022 Adjusted project workflow to accommodate requirement change requests from Django to MySQL\ndatabase\n\u2022 Collaborated with four teammates and led final sprint prior delivery, improving velocity by 163%', u'University Comparison Project\nJanuary 2016 to January 2016\n\u2022 Led team of five to normalize original redundant database to third normal form (3NF)\n\u2022 Designed database structure using ER diagram and transformed to SQL server\n\u2022 Implemented stored procedure to customize various comparisons between universities', u'Data Quality Analyst Intern\nBank of China - Beijing, CN\nJanuary 2014 to January 2014\n2 months)\n\u2022 Assisted colleagues in collecting and cleaning consumer data to provide cleaned data to superiors\n\u2022 Monitored the system to find issues and wrote daily report to supervisor']","[u'Master of Science in Information System', u'Bachelor of Management in Information Management and Information System']","[u'University of Maryland College Park, MD\nJanuary 2016 to January 2017', u'Capital University of Economics and Business Beijing, CN\nJanuary 2012 to January 2016']"
0,https://resumes.indeed.com/resume/2c03253bb747a551,"[u'Data Analyst\nAT&T - Dallas, TX\nMarch 2017 to Present\nDescription: AT&T Inc. is an American multinational telecommunications corporation. This project completely structures AT&T Call details record. It holds to generate the reports and charts for data usage per device, market, and region wise etc. in USA. Receiving the data at large volume nearly 2 Tera bytes from different sources through UNIX. Loading the data into different systems by managing into distinct categories. Every day generating the reports, analyzing and managing the data into different formats.\nResponsibilities:\n\u2022 Involved in Data Migration from Oracle Legacy to SQL using SQL Navigator and SQL Management Studio.\n\u2022 Involved in Analysis, profiling and cleansing of legacy data and hence making the data available for further use.\n\u2022 Actively involved in optimization and tuning of SQL queries through utilizing the explain plan to find out the problem area of the query.\n\u2022 Excellent experience in maintaining Batch Logging, Error Logging with Event Handlers and Configuring Connection Managers using SSIS.\n\u2022 Used Erwin for reverse engineering to connect to existing database.\n\u2022 Involved in Development of End - to - End Application from Data Transfer from Source system applications to till report generation.\n\u2022 Involved in Requirement gathering, High level Design, Low level Design, Development, Unit testing & Integration Testing of Data Migration process using ETL Methodology.\n\u2022 Used Excel sheets, flat files, CSV filesand some deployment files to prepare reports through SSRS. Data has been validated and verified manually and automatically.\n\u2022 Maintained stored definitions, transformation rules and targets definitions using Informatica repository Manager\n\u2022 Involved in the Development of ETL processes such as data transformations, data sourcing, mapping, conversion and loading.\n\u2022 Prepared documentation of Data Migration and Data Mapping with its specifications for development from source to target.\nEnvironment: Oracle 10g/11g, PL/SQL Developer, SQL* Loader, SQL Developer, Oracle Enterprise Manager, Windows 2000, ETL Tool, UNIX, Informatica 8.6.1.', u'Senior Data Modeler\nSony Pictures Entertainment - Culver City, CA\nJanuary 2016 to February 2017\nDescription:Sony Pictures Entertainment is a leading creator and distributor of entertainment products, services and technology.As a programmer, I was responsible for working with the DBA team to create programming solutions for data migration, system integration and application enhancements.\nResponsibilities:\n\u2022 Involved in Analysis, profiling and cleansing of source data and understanding the business process.\n\u2022 Programmed in UNIX Shell scripts to provide a user interface to Oracle forms and reports.\n\u2022 Used Oracle packages like DBMS_STATS to collect statistics of tables as an aid to improve performance of application.\n\u2022 Used Erwin for reverse engineering to connect to existing database and ODS to create graphical representation in the form of Entity Relationships and elicit more information.\n\u2022 Worked with Business Analyst clients to clarify business requirements, map them to systems capabilities, and recommend technical solutions.\n\u2022 Worked on Informatica Power Center tools- Designer, Repository Manager, Workflow Manager, and Workflow Monitor.\n\u2022 Worked on SQL*Loader to load data from flat files obtained from various facilities every day.\n\u2022 Developed ETL jobs using ODI tool for data migration operations.\n\u2022 Built Conceptual, Logical & Physical start schema using Erwin.\n\u2022 Used various transformations like Filter, Expression, Sequence Generator, Update Strategy, Joiner, Stored Procedure, and Union to develop robust mappings in the Informatica Designer.\n\u2022 Involved in documentation of Data Mapping & ETL specifications for development from source to target.\n\u2022 Also implemented the change Data Capture (CDC) while integrating the enterprise data sources.\n\u2022 Handled performance requirements for databases in OLTP and OLAP models.\n\u2022 Monitored the Production batches and fixed the issues within SLA.\n\u2022 Debug data quality issues by analyzing the up-stream sources and provide guidance to team\nEnvironment: Oracle 9i/10g/11g, Erwin, SQL*Loader, SQL Navigator, SQL*Plus, ODI, UNIX, WINDOWS2000/XP, SQL Developer, Oracle Forms 10g.', u'Data Architect\nHenry Ford Health System - Detroit, MI\nOctober 2014 to December 2015\nDescription: Henry Ford Health System, one of the largest and most comprehensive integrated U.S. health care systems, is a national leader in clinical care, research and education. Provided data analysis, SQL, and ETL programming for data collection and data extraction of client medical claims, pharmacy claims and eligibility data.\nResponsibilities:\n\u2022 Worked with the Business Analyst and DBA for requirements gathering, business analysis, and testing and project coordination.\n\u2022 Prepared High Level Logical Data Models and BRDs (Business Requirement Documents) supporting documents containing the essential business elements\n\u2022 Created Tables and sequences for the experimental data load capture. Loaded the data into the tables using TOAD and SQL*plus.\n\u2022 Monitored Health Reimbursement Arrangements (HRAs) or Flexible Spending Accounts (FSAs).\n\u2022 Worked on Defect logging and re-testing defects using Team Link.\n\u2022 Used SQL*Loader, Microsoft Excel and Oracle to test the validity of data extracts as well as research related to client data requests for information.\n\u2022 Automation of data migration process using UNIX Shell scripting.\n\u2022 Worked on Database Security like Encryption of Critical Database Login information.\n\u2022 Involved in Dimensional modeling of the Data.\n\u2022 Worked on Creation of scripted automations for DBA operations to help in minimizing manual effort and for standardization of process.\n\u2022 Coded UNIX Shell scripts to call SQL scripts and to manipulate the returned data.\n\u2022 Extensively worked on Explain plans and various HINTS forPerformance tuning.\n\u2022 Analyzed data to check for Data Integrity and Referential Integrity when loaded to source staging tables.\n\u2022 Prepared the Data Analysis Queries for Validating the Data into Source systems\n\nEnvironment: Oracle 10g/11g, SQL*Loader, SQL Navigator, TOAD, SQL*Plus, WINDOWS 2000 SERVER, WINDOWS XP, UNIX, ORACLE SQL DEVELOPER, ETL, PUTTY, ORACLE FORMS 10g, ORACLE REPOTRS 10g.', u""Data Analyst\nTransUnion - Chicago, IL\nMay 2013 to October 2014\nDescription:TransUnion is an American Company that provides credit information and information management services to approximately 45,000 businesses and approximately 500 million consumers worldwide in 33 countries. TransUnion markets credit reports directly to consumers.I was responsible for working with the DBA team to create programming solutions for data migration, system integration and application enhancements.\n\nResponsibilities:\n\u2022 Worked on Agile SDLC methodology while developing DBF to SQL 'Data Reporting transformation' project.\n\u2022 Worked with users and application developers to identify business needs and provide solutions.\n\u2022 Created and implement report modules into database from client system using Oracle reports as per the business requirements.\n\u2022 Analyzing raw data, drawing conclusions & developing recommendations also writing SQL scripts to manipulate data for data loads and extracts.\n\u2022 Designed Oracle views that generate these extracts for upstream applications and created ETLs to read these views and drop them in a file.\n\u2022 Facilitated in developing testing procedures, test cases and User Acceptance Testing (UAT).\n\u2022 Involved in re-design, re-code, and deploy new data extractions using SSIS, and designed and produced documentation of data transformations for all extractions.\n\u2022 Worked with various process improvements, normalization, de-normalization, data extraction, data cleansing, data manipulation.\n\u2022 Used UTL_FILE to load the data into Oracle tables from FLAT, CSV and Text Files.\n\u2022 Involved in extensive Data Validation using SQL queries and back-end testing.\n\u2022 Implemented various customized Oracle reports using different techniques in Oracle SQL/PL-SQL.\nEnvironment: Oracle Database 10g, SQL Loader, WINDOWS XP, UNIX, PL/SQL Developer Oracle Forms 10g, Putty, TOAD, EXPORT, IMPORT."", u""Jr. Data Analyst\nPayPal - Bengaluru, Karnataka\nApril 2012 to April 2013\nINDIA\n\nDescription: PayPal Holdings, Inc. is an American company operating a worldwide online payments system that supports online money transfers and serves as an electronic alternative to traditional paper methods like checks and money orders. PayPal is one of the world's largest Internet payment companies.\nResponsibilities:\n\u2022 Developed PL/SQL Packages, Procedures and Functions accordance with Business Requirements for loading data into database tables.\n\u2022 Created SQL*Loader scripts to load data into temporary staging tables.\n\u2022 Developed SSRS Reports like Drill through Reports, Drilldown Reports, linked reports and parameterized reports\n\u2022 Data discovery, Aggregation and Transformation based on business use cases and requirement.\n\u2022 Exported business data to Excel for further analysis utilizing SSRS.\n\u2022 Configured Server for sending automatic mails to the respective people when a SSIS process failure or success.\n\u2022 Extracted data from client/server databases to transfer and load tables on the report databases using SSIS package SQL 2008 software.\n\u2022 Worked with built-in Oracle standard Packages like DBMS_SQL, DBMS_JOBS and DBMS_OUTPUT.\n\u2022 Involved in Development of End - to - End Application from Data Transfer to Report Generation for User Analysis.\n\u2022 Performed SQL and PL/SQL query tuning and optimization using tools like EXPLAIN PLAN, SQL*TRACE, TKPROF and AUTOTRACE.\n\u2022 Developed UNIX Shell scripts to automate various periodically repetitive database processes.\nEnvironment: Oracle 9i, PL/SQL, UNIX, HTML, SQL *LOADER, SQL*PLUS, SQL."", u""Data Analyst\nCigniti Technologies Ltd - Hyderabad, Telangana\nAugust 2010 to March 2012\nIndia\n\nDescription:Cigniti Technologies Limited Global Leaders in Independent Software Testing (IST) Services, is headquartered at Hyderabad, India. Cigniti's test offerings include Quality Engineering, Advisory & Transformation, Digital Assurance, and Quality Assurance solutions.\n\nResponsibilities:\n\u2022 Played key role in System Development Life Cycle Process consisting of Design and Gap Analysis.\n\u2022 Understanding the specifications for Data Warehouse ETL Processes and interacting with the dataanalysts and the end users for informational requirements.\n\u2022 Worked on Enterprise Data warehouse and upstream products of the organization.\n\u2022 Worked on Data Migration phases like Design, Extract, Transform and load the data to new systems.\n\u2022 Involved in the performance tuning using SQL queries.\n\u2022 Created cubes in SQL Server Analysis Services.\n\u2022 Analyzed database requirements from the users in terms of the dimensions and fact tables using OLAP operations.\n\u2022 Extensively used DTS Package in Migration of the data from the previous timesheet database in SQL Server database to Oracle 10g.\n\u2022 Used Informatica dashboard to check ticketing issues and resolve the bugs raised in the environment.\n\u2022 Developed the shell scripts to process PL/SQL procedures and wrote PL/SQL program units for data extracting, transforming and loading.\n\u2022 Developed SQL*Loader scripts to load data into the staging tables from the flat files.\n\u2022 Deploy the code in Various environments after getting the approval from Change meetings.\n\nEnvironment: Oracle 9i/10g, SQL Server, SQL*LOADER, UNIX, Windows 2000 Professional, PLSQL, SQL, HTML.""]","[u""Bachelor's""]",[u'']
0,https://resumes.indeed.com/resume/4557be6b163e694d,"[u'Business Analyst\nRPM Direct LLC - Lambertville, NJ\nSeptember 2013 to July 2016\n\u2022 Aided in the training and implementation of process improvement projects.\n\u2022 Worked with clients, gathering requirements, editing, and management for monthly direct marketing campaigns, as well as reviewing campaign results.\n\u2022 Worked with the development team to relay and execute direct marketing campaign requirements accurately and on schedule.\n\u2022 Supported the development team as needed to ensure deliverables were output to the client on time.\n\u2022 Aided in the creation and management of the monthly campaign schedule consisting of 30-40 campaigns per month.\n\u2022 Acted as product owner during process improvement projects, including the creation of a system wide dashboard and specific process driven dashboards by creating user stories, product backlogs, and keeping the stakeholders updated.', u'Data Analyst\nDaVita Healthcare Partners INC - Malvern, PA\nAugust 2012 to August 2013\n* Managed, documented, and distributed the task queue for the Clinical Applications Resolution team.\n* Ran daily reports to highlight the priorities including trending problems and aging tasks.\n* Used SQL skills to maintain and resolve issues with the databases through which the clinical applications ran.\n* Worked directly with clients to implement issue resolution.', u""Data Analyst\nTeleos Partners LLC - consulting firm - Perkasie, PA\nMay 2011 to May 2012\n\u2022 Worked to inventory client's existing system, worked to develop a technology plan for presentation to management, and assisted in the implementation.\n\u2022 Aided in the creation of individual client portals to facilitate real time access to the progress of client specific projects.\n\nSKILLS AND COMPETENCIES\nAgile, Scrum, JIRA, Microsoft Suite, CRM, excellent communication and teamwork""]","[u'in Project Leadership & Management', u'Bachelor of Science in Information Science & Technology in Information Science & Technology']","[u'Cornell University\nDecember 2017', u'Pennsylvania State University University Park, PA\nMay 2011']"
0,https://resumes.indeed.com/resume/37d06c802a924722,"[u'Data Conversion Analyst\nABC Financial\nAugust 1998 to March 2018\nThe past 7 years I worked as a Data Conversion Analyst, converting data from previous software to a more user friendly format. As data was converted, I would ensure it was formatted properly and analyze the end result for efficiency while meeting all deadlines. I have the ability to work well with clients and meet their needs. I am accustomed to a fast paced environment, working well independently and or as a team player. Strong communication skills, organization skills, active listening skills, sharp problem solver and seasoned in conflict resolution. Extensive knowledge of Excel and Access. Daily use of Excel, vLookups, queries and researching databases.']",[u'High school or equivalent'],[u'']
0,https://resumes.indeed.com/resume/3c2b1f1fc0e8a1b8,"[u'SR. DATA ANALYST / SCIENTIST\nNTT Data - Boston, MA\nAugust 2016 to October 2017\nPerformed Data Validation and Data Reconciliation between disparate source and target systems for various projects which improved data quality and reduced the process time by 30%.\n\u2022 Calculated metrics, condensed multiple data sources and large amounts of data into concise targeted information summaries and reports for management.\n\u2022 Cleaned and merged data of 300 k users and 1.8 million records using pandas, and a dedupe algorithm in Python to prepare data for machine learning models.\n\u2022 Achieved a 30% reduction in customer attrition by using logistics regression to create an attrition model that helped understand the key causes of attrition.', u'SR. DATA SPECIALIST\nFedEx Services - Memphis, TN\nJanuary 2012 to August 2016\n\u2022 Mined large data sets and created customized compliance reports to improve insights into low booking compliance rates for heavy weight shipments; the suggested implementations resulted in 25% increase in compliance rates across six FedEx international regions.\n\u2022 Developed the methodology for ""Build Ratio"" based on statistical modeling and extensive research with Operations & Network Planning groups. The ratio quantified the actual aircraft capacity consumed by a shipment through shipment characteristics rather than arithmetic space, resulting in 20% increase in efficiency of capacity allocation and optimization process.\n\u2022 Provided marketing teams with a clear path forward through a high-level project; analyzed massive customer data to create a segmentation model based on customers\' level of integration of FedEx products and services, resulting in more effective messaging and tactics, and long-term growth.\n\u2022 Maximized the result of marketing spends, simplified campaigns, and fueled speed to market; achieved 50% increase in campaign response rate and 30% increase in ROI while reducing cycle times by 30%.\n\u2022 Reinvented the marketing program analysis process that slashed evaluation time from 5 days to 1 day.', u'DATA ANALYST\nFedEx Services - Memphis, TN\nJanuary 2009 to January 2012\nFORECASTING & BUSINESS PLANNING\n\u2022 Produced monthly forecast for $2 billion FedEx EMEA International portfolio; analyzed KPI data to identify business trends, mode shifts, and exchange rate impact. Presented findings on business status, outlook and trends to management.\n\u2022 Established new monthly forecast tracking and management reporting system to improve availability and accuracy of corporate performance management dashboard and reports, resulting in 30% time savings.\n\u2022 Wrote requirements for demand planning module required for daily forecast of heavy weight shipments.\n\u2022 Increased revenue by 20% for freight spot quote business by working cross-functionally to reduce quote system downtime and hasten quote response times.\n\u2022 Achieved 0.5% improvement in accuracy of monthly rolling forecast of international shipments with a new process that pinpointed shipping patterns in national accounts.']","[u'MBA', u'Bachelor of Technology in Technology']","[u'University of Texas at Austin, McCombs School of Business Austin, TX', u'IIT (Indian Institute of Technology) Kharagpur, West Bengal']"
0,https://resumes.indeed.com/resume/118f72b092b3e844,"[u'Data Administrator\nRotor Clip, Co inc - Somerset, NJ\nFebruary 2018 to Present\nRotor Clip,Co inc. \u2013 Somerset, NJ\nData Administrator\n\u2022 Perform Project Management responsibilities which include tracking, processing and closing new job orders and change orders utilizing an ERP Business Management System\n\u2022 Maintain SAP Security Queues for tickets involving authorization issues/errors role maintenance user maintenance system tracing user master itransports, monitoring critical production system incident management in SAP one support Launchpad\n\u2022 Create End of the Month Production Report using excel\n\u2022 Create weekly schedule for job order.', u'Data Analyst (Contractor)\nELLUCIAN\nJuly 2017 to January 2018\n\u2022 Working as a remote Data Analyst for one of the leading software provider for top universities across the US. Compiled tracking data from all customer events in a timely manner, record in Salesforce, and make recommendations for future marketing activities used Tableau to created visualizations graphics report.\n\u2022 Optimized standard operating procedures and training materials to promote higher levels of proficiency among staff and adherence to document storage and submission protocols.', u'SharePoint Administrator/Data Analyst (Contractor)\nEDUCATIONAL TESTING SERVICE - Princeton, NJ\nApril 2016 to March 2017\n\u2022 Responsible for data analysis of all global documents to meet site supervisor.\n\u2022 Optimized standard operating procedures and training materials to promote higher levels of proficiency among staff and adherence to document storage and submission protocols.\n\u2022 Administered SharePoint site supervising regulatory compliance of all global documents by Ensured proper delineation of data via centralized libraries and comprehensive lists.\n\u2022 Facilitated and Monitored permissions to control access confidential sensitive corporate data.\n\u2022 Analytics platforms such as Google analytics.\n\u2022 Created visualizations graphics report using Tableau.\n\u2022 Created promoted links, modifying SharePoint site collections with multiple sites, unique navigational elements, custom content types and site columns, site pages, web part pages, workflows, retention policies, governance policies, and granting permissions; design solutions using lists, collections\n\u2022 Building forms with ASP.NET, InfoPath, and SharePoint application pages\n\u2022 Ensuring the page design/layouts are cohesive to internal and external customers.\n\u2022 Collaborate all team members to gather reconcile team info and documents to create Presentation for internal team using SharePoint Online (365)', u""Data Analyst I (Contractor)\nJOHNSON AND JOHNSON - Woodbridge, NJ\nMarch 2015 to March 2016\n\u2022 Supply Chain Information Technology; Quality and Compliance Metrics Support.\n\u2022 Created deck for management. Monitored, updated and reported on QS&S Goals and Objective Design solutions. Gathered information from DRI's (Direct Responsible Individual). Captured final submission documents via use of document management system and facilitated the analysis, editing and approval process prior to final submission using SharePoint 2007. Kept track of headcount reporting by using Excel (Pivot tables & Vlookup).\n\u2022 Data Profiling analysis maintained Documentum Repository ensuring 100% accuracy in online submissions, renewals and registrations. Worked in tandem with Site Quality Operations, Technical Operations, and Supplier Managers to aggregate report data and track regulatory compliance.\n\u2022 Responsible to present Supply Chain Manager SharePoint data to internal teams to present to external customers.\n\u2022 Analyzed Regulatory actions which included data on product ids, Licenses, License Expiration date, regulation action type, regulatory affairs completion dates, submission dates using Excel.\n\u2022 Created Pivot table to show the number of regulatory action per region the county. Then use Excel charting on the data illustrating the numbers per region county over time. Clean data to ensure field were one data type using Excel"", u'Data Analyst (Contractor)\nPSE&G - South Plainfield, NJ\nAugust 2012 to January 2015\n\u2022 Maintained sales data, including retail price lists and marketing databases\n\u2022 Created weekly and monthly metrics reports; tracked profits using Excel\n\u2022 Made sure that all the data was correct for internal audits.', u'Data Analyst (Contractor)\nAT&T - Bedminster, NJ\nJanuary 2007 to July 2012\n\u2022 Processed Online Federal EPA Tier II Reports; responsible for loading, extracting and validation of client data\n\u2022 Processed and tracked environmental permits and registrations handled by Headquarters.\n\u2022 Maintained orderly central site files and uploaded permitting/registration details and supporting documentation to companywide Environmental Management Information System (EMIS\n\u2022 Created SAP expense reports; purchase orders and payment requests\n\u2022 Vendor Add Forms and run reports within SAP\n\u2022 Organized and analyzed multiple data sources\n\u2022 Extracted raw data from Excel and imported to SQL Server\n\u2022 Provided data entry, data auditing, created data reports and monitored all data for accuracy, filing records, faxing and inventory reporting.']","[u'Certification in Supply Chain Management', u'Bachelor of Science in Business Administration in Business Administration', u'Associates of Arts in Arts and Sciences']","[u'Union County College Elizabeth, NJ\nFebruary 2017 to Present', u'Berkeley College Woodbridge, NJ\nJanuary 2013', u'Avtech Institute of Technology South Plainfield, NJ\nJanuary 2011']"
0,https://resumes.indeed.com/resume/e30784b7bb48ebba,"[u'Data Analyst\nDAN CONSULTING COMPANY - Addis Ababa, ET\nJanuary 2011 to May 2016\nDAN is a consulting company on information technology and management since 2007. DAN provides innovative business solutions to Government, private and non-for-profit institutions to control spending, increase revenue and profitability, and improve service and spending effectiveness.\n\nClient - Rock-Bottom Hotels and Resorts\nProject: Enhancing Business Profitability\nDescription: This project was implemented to fully automate information collection and manipulate data to reduce cost and increase profitability.\n\nDuties and Responsibilities\n\u2022 Reviewed client information from various data sources to validate data integrity issues and ensure preservation of the most accurate records for migration to one consolidated system\n\u2022 Conducted data mining and data modeling for market analysis using MYSQL queries, leading to 22% growth in customer acquisition\n\u2022 Used statistical techniques for hypothesis testing to validate data and interpretations\n\u2022 Proposed solutions to improve system efficiencies, leading to a 15% reduction in operating costs\n4629 Southland Ave Alexandria, VA 22312 \u2022 301.844.8650 \u2022 wengel.tekle@gmail.com\nFully Work Authorized \u2022 No Visa Sponsorship Required\n\n\u2022 Optimized data collection procedures and, Assisted in the creation of a novel method of new account information input, resulting in saving of approximately 140 man hours annually\n\u2022 Completed focus group and BI research that helped boost employee productivity by 15%\n\u2022 Created and presented impactful Tableau Dashboards and Excel visualizations by using pivot tables and VLOOKUP to team members and clients to improve system efficiencies and reduce total expenses\n\u2022 Filter and clean data by reviewing computer reports, printouts, and performance indicators to locate and correct code problems\n\u2022 Assisted in the training and mentoring of 5 junior data analysts']","[u""Master's Degree in Development"", u""Bachelor's Degree in Law in Law""]","[u'Swansea University Swansea, MA\nSeptember 2013 to December 2014', u'Addis Ababa University Addis Ababa, ET\nJanuary 2006']"
0,https://resumes.indeed.com/resume/5a95a95aee21ecc9,"[u'Research Assistant\nEconomics Department, Rice University\nApril 2017 to January 2018\nFinished the R version realization for the empirical analysis in the paper ""Panel Estimators and the Identification of Firm-Specific', u'Rotational Data Engineer\nCIDI, O2O Big data center, Shanghai, China - Shanghai, CN\nDecember 2015 to May 2016\n\u2022 Assisted Crawling product data from top 15 e-commerce websites.\n\n\u2022 Created real machine learning models to classify the target clients and Evaluated the model with the designed metrics.\n\n\u2022 Programmed related static web pages for the consultant platform.', u'Data Analyst\nZheshang Securities, Research Centre - Hangzhou, CN\nJuly 2015 to September 2015\nChina\n\u2022 Assisted in ferrous metal products research via investment value analysis.\n\n\u2022 Applied time series analysis and forecast upon stock prices and yields trend via GARCH family.\n\n\u2022 Perfected our database of ferrous metal industry by tracking and prepossessing related important news and events and compiled\n\nweekly and monthly analytical reports.', u""Data Analyst\nNational Bureau of Statistics - Xi'an, CN\nMay 2015 to July 2015\nChina\n\u2022 Led a team to complete metrics design and data collection for over 500 small and micro enterprises.\n\n\u2022 Conducted Feature Engineering and established panel data regression model.\n\n\u2022 Clustered enterprises, compared to historical data and gave recommendation on policy support.""]","[u'Master in Statistics in Computational Economics', u'Bachelor in Statistics in Financial Engineering', u'in Chinese Young Development Program']","[u'Rice University Houston, TX\nAugust 2016 to May 2018', u""Xi'an Jiaotong University Xi'an, CN\nSeptember 2012 to June 2016"", u'Columbia University New York, NY\nJune 2014 to August 2014']"
0,https://resumes.indeed.com/resume/7341766d945ff1ef,"[u'Data Analyst\nMTC BATA - San Francisco, CA\nAugust 2017 to February 2018\nAugust 2017 - February 2018\n\u25cf Conducted data modeling and trending to analyze traffic and toll patterns to make tolling decisions and monitor toll\nsystem statistics\n\u25cf Created an impactful dashboard which consisted of comprehensive data from traffic trends, finances, and systems\nperformance that was submitted daily to MTC executives for their overview to make operations and business decisions\n\u25cf Improved rush hour traffic conditions with analysis of traffic patterns and toll price schedules by recommending\nappropriate toll prices in different zones on the highway during varying peak rush hours\n\u25cf Enhanced the host application by using SQL to crosscheck multiple reports within the host client to ensure that the systems were providing the same accurate data for all reports\n\u25cf Aggregated data and analyzed performance requirements of the TSI (Toll System Integrator) to calculate penalties and to ensure that KPIs are met which resulted in ~20% less penalties being issued and an increase in SPI (Schedule\nPerformance Index) from 0.6 to 0.8', u""Data Analyst Intern\nCoverX Specialty, subsidiary of Crum & Forster Enterprises - Los Angeles, CA\nJuly 2014 to September 2015\nBuilt databases to organize data involving construction clientele that have since been adopted by other divisions within\nCrum & Forster to organize competitors' forms, terms, and risk appetites\n\u25cf Collaborated with a team of underwriters to analyze construction market trends and profit margins in order to pinpoint\nways to create competitive differentiators, which increased profits by ~3%\n\u25cf Analyzed multiple underlying policies such as OCIP (Owner Controlled Insurance Program) and CCIP (Contractor\nControlled Insurance Program) to forecast trends in the market, resulting in 200 new leads and 50 new clients""]",[u'Bachelor of Science in Statistical Science'],"[u'University of California Santa Barbara, CA\nAugust 2017']"
0,https://resumes.indeed.com/resume/235ee9fac4e4ea06,"[u'Data Analyst (GTA)\nOklahoma State University\nAugust 2017 to December 2017\n\u2022 Grading Assignments and exam papers for the class of ""Advanced Database Management"".\n\u2022 Prepare highly organized and detailed oriented ad-hoc reports for all the research with professor.\n\u2022 Performing descriptive and predictive analytics so as to support University\'s research mission.', u'Data Analyst\nAbhudaya Multimedia - Indore, Madhya Pradesh\nJune 2013 to March 2016\n\u2022 Designed, developed and maintained data models while working with cross-functional teams to gather requirements from stakeholders for a major telecommunication company Vodafone India.\n\u2022 Building live and interactive quality control reporting system for production team contributing improvement in quality using tools for data analysis and reporting are SQL server, MS Excel, R studio, Python and Tableau.\n\u2022 Professional in terminologies and implementation spanning the SDLC cycle; Methodologies: Agile, Waterfall, and Incremental.\n\u2022 Hands-on experience on System Testing, Regression Testing, GUI Testing, and Web Testing.']","[u'Master of Science in Management Information Systems', u'Bachelor of Engineering in Information Technology']","[u'Oklahoma State University Stillwater, OK\nDecember 2017', u'Rajiv Gandhi Technical University\nMay 2013']"
0,https://resumes.indeed.com/resume/6a90d1cd4528b8d6,"[u'SQL Data Analyst\nDIGISOFT CONSULT - Cumming, GA\nJanuary 2015 to Present\n\uf076 Responsible for designing, developing and optimizing SQL databases for Digisoft Consult clients.\n\uf076 Experienced in designing tables and picking data types.\n\uf076 Experienced in developing T-SQL (DDL, DML) statements and complicated joins.\n\uf076 Highly proficient in creating indexes for fast retrieval of data, views, triggers, stored procedures and User defined functions.\n\uf076 Experienced in writing and analyzing SQL queries using T-SQL in order to obtain critical data.\n\uf076 Created ETL packages with different data sources (SQL Server, Oracle, Flat files, Excel, DB2, and Teradata) and loaded the data into target tables by performing different kinds of transformations using SSIS.\n\uf076 Practical understanding of the Data modeling (Dimensional & Relational) concepts like Star-Schema Modeling, Snowflake Schema Modeling, Fact and Dimension tables.\n\uf076 Scheduled jobs and alerts using SQL Server Agent and configure database mail for job failure.\n\uf076 Experienced in data integration into a data warehouse from heterogeneous and homogenous data sources including SQL Server database, Oracle database, fixed width / delimited flat files, XML files, and other sources.\n\uf076 Experienced in the migration of data from Heterogeneous Data Sources and legacy system (Mainframe, Access, Excel, Oracle) to SQL Server databases.\n\uf076 Expert knowledge with SSIS, SSAS and SSRS in building and generating business reports for Digisoft consult clients.\n\uf076 Maintained data integrity during extraction, manipulation, processing, analysis and storage.\n\uf076 Developed the required data warehouse model using Star and snowflake schema for the generalized model.\n\uf076 Performed MS SQL DBA duties including: database backups, restoration, performance tuning, and capacity analysis.\n\uf076 Optimized the performance of SQL queries using SQL profiler and SQL server database engine wizard.\n\uf076 Ability to use SQL Server Analysis Services SSAS in creating cubes, data sources, views, named queries, dimensions and deploying of analysis service project in SQL Servers.\n\uf076 Experienced in designing eye-catching reports with data from OLTP database, dimensional data structure and OLAP cubes for business reporting purposes.\n\uf076 Experienced in working on creating SSAS cubes and various cube objects such as KPIs (Key Performance Indicators), calculated members, attributes hierarchies and perspectives.\n\uf076 Experienced in data mapping using Microsoft Visual Studio Data Tools.\n\uf076 Developed various operational Drill-through and Drill-down reports using SSRS.\n\uf076 Developed Reports for business end users using Report Builder with updating Statistics.\n\uf076 Created SQL datasets for Power BI and Ad-hoc Reports.\n\uf076 Created Tabular, Matrix reports, Charts and graphs as per customer requirements using SSRS and Power BI.\n\uf076 Experienced in creating SQL databases on Azure.', u'SQL SERVER DATA ANALYST\nFOOD DEPOT - Marietta, GA\nAugust 2014 to November 2014\nCreated and built business reports with SSRS, Microsoft Access and Excel..\n\u2756 Expert in programming Tables, Views, Stored Procedures, User Defined Functions (UDF), Constraints, Indexes, Temporary tables, CTEs and Triggers to optimize retrieval.\n\u2756 Ran complex queries to retrieve data and ad-hoc reports from multiple tables within SQL server database.\n\u2756 Experience in adding checks, constraints and logging to where not applied.\n\u2756 Ability to write complex SQL queries, script, stored procedures, creating triggers and indexes to improve database performance in SQL servers\n\u2756 Scheduled jobs and alerts using SQL Server Agent and configure database mail for job failure\n\u2756 Managed backup & Recovery procedure on SQL server Databases for Food Depot.\n\u2756 Created ETL packages with different data sources (SQL Server, Oracle, Flat files, Excel, DB2, and Teradata) and loaded the data into target tables by performing different kinds of transformations using SSIS.\n\u2756 Experience in data integration into a data warehouse from heterogeneous and homogenous data sources including SQL Server database, Oracle database, fixed width / delimited flat files, XML files, and other sources\n\nEnvironment: SQL Server 2005/2008, Integration Services (SSIS), Reporting Services (SSRS), Analysis Services (SSAS), DTS, .Net, T-SQL, Windows 2008/2012, XML, MS Excel, MS Access, SSMS, MS visual Studio, BIDS, Power BI.']","[u'Certificate', u'B.Sc. in Mathematics']","[u'Chattahoochee Technical College\nJune 2014', u'Obafemi Awolowo University\nDecember 2012']"
0,https://resumes.indeed.com/resume/20219cd63dd9cb51,"[u'Data Scientist\nTP-AHA - Washington, DC\nOctober 2013 to Present\n\u2022 Designed analysis techniques to extract meaning from big data and machine learning solutions.\n\u2022 Conducted data extraction, data wrangling and analyzed complex and multi-source datasets using advanced querying, statistical analytics and visualization tools.\n\u2022 Designed and implemented data analytics and visualization for client/business exploration and prediction.\n\u2022 Developed Machine Learning solutions by testing the models for accuracy and precision on various classifiers.\n\u2022 Conducted data mining, data modeling, statistical analysis, business intelligence gathering and trend analysis.\n\u2022 Produced and presented data analytic and policy analysis reports to the management that supports on decisions for high-priority, agency initiatives for improvement and realignment.\n\u2022 Created visually interactive analytical dashboards in Tableau, Matplotlib and Seaborn for data reporting and decision-making.\n\u2022 Developed advanced customer segmentation analysis and visualization on Tableau.\n\u2022 Applied logistic regression model to client service use records to predict service use based upon various clients\u2019 features.\n\u2022 Implemented demand-forecasting models, which improved upon forecast accuracy and reduced service provision inefficiencies.', u'Senior Data Specialist\nUNFPA - International\nMay 2010 to September 2013\n\u2022 Researched, extracted and transformed information from raw data into meaningful and actionable data.\n\u2022 Used various internal and external data sources to collect, aggregate and analyze data.\n\u2022 Analyzed and interpreted complex big data using various statistical methods.\n\u2022 Identified best variables to evaluate a range of analysis and scope of information sought to answer questions.\n\u2022 Identified trends and insights for the organization\u2019s financial and programmatic activities.\n\u2022 Produced various visually impactful visualizations and deep analytical reports for decision-makers and stakeholders.\n\u2022 Worked in a cross-functional team to develop a service model and implemented the methodology to explain complex programs.\n\u2022 Trained junior data analysts on research techniques, statistical methods and analytic tools.', u'Data Analyst\nWorld Health Organization - International\nSeptember 2008 to April 2010\n\u2022 Extracted and transform raw health care or public health data into meaningful and actionable information.\n\u2022 Analyzed and interpreted health care data to identify important feature or key metrics.\n\u2022 Analyzed the factors related to prevalence and incidence rates of health problems.\n\u2022 Built statistical models using historical data to depict and predict health trends.\n\u2022 Identified, measured and recommended policy improvement strategies for better program activities.\n\u2022 Identified trends and used the information to help the organization make health program adjustments to reduce disease prevalence.\n\u2022 Interpreted information from a series of database investigations and policy analysis to make predictions and recommendations.\n\u2022 Created and discussed analytic results with various members of management in the organization, and led staff members to realize the significance of the analysis results and policy implications.', u'Analyst\nJune 2004 to August 2008\n\u2022 Analyzed quantitative data gathered to develop an understanding of customer behavior, demographics and socioeconomic situation. Presented data analytics results that helped guide decisions of the agency.\n\u2022 Ensured accurate and consistent statistical analysis by meticulously going through the data and validating results.\n\u2022 Developed agency\u2019s guidelines and best practices based on information learned through an analysis of client data.\n\u2022 Determined additional means of organization improvement with partners by using data collected by project evaluation surveys.\n\u2022 Developed company guidelines and best practices based on information learned through an analysis of consumer behavior data.\n\u2022 Created and presented executive analytic reports to show the trends in the data.']","[u""Doctorate's""]",[u'']
0,https://resumes.indeed.com/resume/f49b0bffd625ced3,"[u'Data Analyst\nSynchrony Financial - Bridgewater, NJ\nJanuary 2017 to Present\nResponsibilities:\n\u2022 Monitor the Database for duplicate records.\n\u2022 Merge the duplicate records and ensure that the information is associated with company records.\n\u2022 Standardize company names, addresses, and ensure that necessary data fields are populated.\n\u2022 Review the database proactively t3 identify inconsistencies in the data, conduct research using internal and external sources to determine information is accurate.\n\u2022 Resolve the data issues by following up with the end user.\n\u2022 Coordinate activities and workflow with other data Stewards in the firm to ensure data changes are done effectively and efficiently\n\u2022 Review the database to identify and recommend adjustments and enhancements, including external systems and types of data that could add value to the system.\n\u2022 Extract the data from database and provide data analysis using SQL to the business user based on the requirements. Create pivots and charts in excel sheet to report data in the format requested\n\u2022 Used VBA for excel to automate the data entry forms to help standardize data\n\u2022 Assist CRM Analyst with Email Marketing Campaigns, including Client Publications, Newsletters, and announcements.\n\u2022 Assist other data Stewards with data Change Management (DCM) Inbox in resolving various tickets created by the User Change Request in Interaction Database.\n\u2022 Developed and Created Logical and Physical Database Architecture using ERWIN.\n\u2022 Designed STAR Schemas for the detailed data Marts and plan Data Marts involving Shared Dimensions.\n\u2022 Coordinated with different users in UAT process.\n\u2022 Conduct Design reviews with the business analysts and content developers to create a proof of concept for the reports.\n\u2022 Ensured the feasibility of the logical and physical design models.\n\u2022 Conducted the required GAP analysis between their AS-IS submission process and TO-BE Encounter Data Submission Process.\nEnvironment: MS Outlook, MS Project, MS Word, MS Excel, MS Visio, MS Access, Power MHS, Citrix, Clarity, MS SharePoint.', u'Data Analyst\nValley National Bank - Fair Lawn, NJ\nSeptember 2015 to November 2016\nResponsibilities:\n\u2022 Created new reports based on requirements. Responsible in Generating Weekly ad-hoc Reports\n\u2022 Planned, coordinated, and monitored project levels of performance and activities to ensure project completion in time.\n\u2022 Automated and scheduled recurring reporting processes using UNIX shell scripting and Teradata utilities such as MLOAD, BTEQ and Fast Load and Experience with Perl\n\u2022 Worked in a Scrum Agile process & Writing Stories with two week iterations delivering product for each iteration\n\u2022 Worked on transferring the Data files to vendor through sftp &Ftp process\n\u2022 Involved in defining and Constructing the customer to customer relationships based on Association to an account & customer\n\u2022 Created action filters, parameters and calculated sets for preparing dashboards and worksheets in Tableau.\n\u2022 Experience in performing Tableau administering by using tableau admin commands.\n\u2022 Worked with architects and, assisting in the development of current and target state enterprise level Data architectures\n\u2022 Worked with project team representatives to ensure that logical and physical Data models were developed in line with corporate standards and guidelines.\n\u2022 Involved in defining the source to target Data mappings, business rules and Data definitions.\n\u2022 Responsible for defining the key identifiers for each mapping/interface.\n\u2022 Performed Data analysis and Data profiling using complex SQL on various sources systems including Oracle and Teradata.\n\u2022 migrated three critical reporting systems to Business Objects and Web Intelligence on a Teradata platform\n\u2022 Created Excel charts and pivot tables for the Adhoc Data pull\nEnvironment: Teradata 13.1, Informatica 6.2.1, Ab Initio, Business Objects, Oracle 9i, PL/SQL, Microsoft Office Suite (Excel, Vlookup, Pivot, Access, Power Point), Visio, VBA, Micro Strategy, Tableau, UNIX Shell Scripting ERWIN.', u'Data Analyst\nNordstorm - Rockaway, NJ\nApril 2014 to July 2015\nResponsibilities:\n\u2022 Involved in Business and Data analysis during requirements gathering.\n\u2022 Assisted in creating fact and dimension table implementation in Star Schema model based on requirements.\n\u2022 Performed segmentation to extract Data and create lists to support direct marketing mailings and marketing mailing campaigns.\n\u2022 Defined Data requirements and elements used in XML transactions.\n\u2022 Reviewed and recommended database modifications\n\u2022 Analyzed and rectified d Data in source systems and Financial Data Warehouse databases.\n\u2022 Generated and reviewed reports to analyze Data using different excel formats\n\u2022 Documented requirements for numerous Adhoc reporting efforts\n\u2022 Troubleshooting, resolving and escalating Data related issues and validating Data to improve Data quality.\n\u2022 Involved in Regression, UAT and Integration testing\n\u2022 Designed developed and implemented 2 professionally finished systems for tracking IT requests, and providing a Data repository about reports. Documented all system functionality\n\u2022 Participated in testing of procedures and Data, utilizing PL/SQL, to ensure integrity and quality of Data in Data warehouse.\n\u2022 Metrics reporting, Data mining and trends in helpdesk environment using Access\n\u2022 Gather Data from Help Desk Ticketing System and write adhoc reports and, charts and graphs for analysis.\n\u2022 Compiled Data analysis, sampling, frequencies and stats using SAS.\n\u2022 Identify and report on various computer problems within the company to upper management\n\u2022 Report on trends that come up as to identify changes or trouble within the systems using Access and Crystal Reports.\n\u2022 Performed User Acceptance Testing (UAT) to ensure that proper functionality is implemented.\n\u2022 Guide, train and support teammates in testing processes, procedures, analysis and quality control of Data, utilizing past experience and training in Oracle, SQL, Unix and relational databases.\n\u2022 Maintained Excel workbooks, such as development of pivot tables, exporting Data from external SQL databases, producing reports and updating spreadsheet information.\n\u2022 Modified user profiles, which included changing users cost center location, changed users authority to grant monetary amounts to certain departments - monetary amounts were part of the overall budget amount granted per department\n\u2022 Extracted Data from DB2, COBOL Files and converted to Analytic SAS Datasets.\n\u2022 Deleted users from cost centers, deleted users authority to grant certain monetary amounts to certain departments, deleted certain cost centers and profit centers from database\n\u2022 Created a report, using SAP reporting feature that showed which users have not performed scanning of journal voucher documents into the system.\n\u2022 Created Excel pivot tables, which showed a table of users that, have not performed scanning of journal voucher documents. Users were able to find documents by double-clicking on his/her name within the pivot table\n\u2022 Load new or modified Data into back-end Oracle database.\n\u2022 Optimizing/Tuning several complex SQL queries for better performance and efficiency.\n\u2022 Created various PL/SQL stored procedures for dropping and recreating indexes on target tables.\n\u2022 Worked on issues with migration from development to testing.\n\u2022 Designed and developed UNIX shell scripts as part of the ETL process, automate the process of loading, pulling the Data\n\u2022 Validated cube and query Data from the reporting system back to the source system.\n\u2022 Tested analytical reports using Analysis Studio\nEnvironment: SAS/BASE, SAS/Access, SAS/Connect, Informatica Power Center (Power Center Designer, workflow manager, workflow monitor), SQL *Loader, Congas, Oracle, SQL Server 2000, Erwin, Windows 2000, TOAD', u'Data Analyst\nImmobile - Burlington, NJ\nJanuary 2012 to March 2014\nResponsibilities:\n\u2022 Analyze the client Data and business terms from a Data quality and integrity perspective.\n\u2022 Perform root cause analysis on smaller self-contained Data analysis tasks that are related to assign Data processes.\n\u2022 Worked to ensure high levels of Data consistency between diverse source systems including flat files, XML and SQL Database.\n\u2022 Develop and run ad hoc Data queries from multiple database types to identify system of records, Data inconsistencies, and Data quality issues.\n\u2022 Involved in translating the business requirements into Data requirements across different systems.\n\u2022 Involved in understanding the customer needs with regards to Data, documenting requirements, developing complex SQL statements to extract the Data and packaging/encrypting Data for delivery to customers.\n\u2022 Participated in the development of Enhancement for the current Commercial and Mortgage Securities Association (CMSA) Application\n\u2022 Wrote SQL Stored Procedures and Views, and coordinate and perform in-depth testing of new and existing systems.\n\u2022 Manipulate and prepare Data, extract Data from database for business Analyst using SAS.\n\u2022 Provided support to Data Architect and Data Modeler in Designing and Implementing Databases for MDM using ERWIN Data Modeler Tool and MS Access.\n\u2022 Worked with Data Modeling team to create Logical/Physical models for Enterprise Data Warehouse.\n\u2022 Reviewed normalized/Renormalization schemas for effective and optimum performance tuning queries and Data validations in OLTP and OLAP environments.\n\u2022 Exploited power of Teradata to solve complex business problems by Data analysis on a large set of Data.', u'Data Analyst\nCovanta Energy - Morristown, NJ\nMay 2010 to December 2012']",[u''],[u'processes the Data using Informatica Power Center Metadata Exchange']
0,https://resumes.indeed.com/resume/407ec5a94ffc68aa,[u'Data Analyst'],"[u""Master's""]",[u'']
0,https://resumes.indeed.com/resume/9ca504c222335c07,"[u'Data Engineer\nSchneider Electric - Andover, MA\nMay 2017 to March 2018\nResponsibilities:\n* Provided data analysis for client engagements across a range of technical industries.\n* Assisted to build analytic tools to manage data and streamline data analyses using R and SQL Server.\n* Develop framework, metrics and reporting to ensure progress can be measured, evaluated, and continually improved.\n* Support the development of performance dashboards on Tableau that encompass key metrics to be reviewed with senior leadership.\n* Built complex SQL queries, generated reports from different sources for various business needs.\n* Provided sales forecasting using predictive models and what-if scenarios.\n* Created statistical models (Machine Learning algorithms) and cluster groups (K-Means) for target marketplaces\n* Worked closely with data scientists to assist on feature engineering, model training frameworks, and model deployments.\n* Used Python to extract information from XML files and wrote Python scripts to clean raw data.\n* Performed data extraction and data wrangling using Pandas and Numpy modules in Python.\n* Maintain database instances and execute SQL queries for data validation.\n* Performed aggregation and missing value imputation using SQL and Python.\n* Experience in using PyCharm editor for writing the python scripts which helped in code analysis, debugging and integrated unit testing.\n* Prepare workflow document reports for future reference.', u""Data Analyst\nEMC Corporation - Bengaluru, Karnataka\nFebruary 2015 to July 2016\nResponsibilities:\n* Analyzed business data on daily basis and resolved data discrepancy to improve data quality\n* Assisted database team in revamping the database from scratch and created ETL workflows on the existed data.\n* Manipulated various tables within Oracle SQL to obtain desired data for application.\n* Predicted customers preference based on order history to maximize sales efficiency using logistic regression.\n* Identified the products with high risk of loss and updated inventory using MS Excel and MS Access.\n* Supported customer service department and increased customers' positive feedback rate from 87% to 93%\n* Worked in conjunction with development team and senior management in preparation of data mapping reports, validation reports, and business reports.\n* Contributed in ensuring all implementations consistent with Software Development Lifecycle (SDLC) standards.\n* Created Dashboards in Tableau using features such as worksheets, functions and actions.\n* Trained and mentored other Data Analysts\n* Involved in troubleshooting and fine-tuning of databases for performance and concurrency.\n* Initiated pilot projects for various data science tasks including advanced NLP and neural nets.\n* Worked in Cross Functional team environment and capability to handle multiple projects simultaneously\nincluding Onsite Offshore co-ordination"", u'Data Analyst\nCognizant - Bengaluru, Karnataka\nNovember 2013 to January 2015\nResponsibilities:\n* Actively monitored database for any issues and troubleshooting of level 1 issues.\n* Programmed complex SQL queries for Views, Functions, Triggers, and Stored Procedures.\n* Involved in designing and developing reports in MS SQL Server environment by developing SSRS reports and managing dashboards on Office 365.\n* Created SSAS Tabular Models for sales reporting.\n* Created incident documentation that includes information about the results of the root cause analysis, approvals.\n* Involved in performance tuning of ETL jobs and SQL queries.\n* Performed and visualized analysis of the export-import databases with Tableau, Pivot tables in Excel.\n* Produced reports and/or data sets for ad hoc requests.\n* Trained new team members to understand marketing concepts and processes and to effectively use Excel and other tools.\n* Executed SQL Queries with all types of Joins to validate data from backend.\n* Performed regression analysis to see the factors causing payment defaults using Excel.\n* Provided 24 X 7 Production Database on Call Support on rotation basis', u'Systems Engineer\nCognizant - Bengaluru, Karnataka\nMay 2012 to October 2013\nStorage/Database\nResponsibilities:\n* Managed storage environment by coordinating with developers, DBAs, and other teams to determine storage requirements and to optimize it.\n* Allocated/de-allocated space from various storage products, fabric troubleshooting, and basic switch configurations, and handled all incidents/requests within defined SLAs under ITIL framework.\n* Modeled EMC Storage solutions after analyzing and assessing customer data and performance criteria, and providing them to Presales field within defined SLAs and quality standards.\n* Involved in coordination with other cross-functional teams to execute critical production Change, Problem and Incident Response procedures towards compliance and assuring seamless Service Delivery.\n* Effectively size newer and better storage solutions for core EMC arrays, Array upgrades, competitive array replacements using Engineering best practices.']","[u'Masters in Computer Science in Computer Science', u'Bachelor of Technology in Information technology']","[u'University of North Carolina Charlotte, NC\nJanuary 2017', u'Vellore Institute of Technology Vellore, Tamil Nadu\nJanuary 2012']"
0,https://resumes.indeed.com/resume/296ebf3d4b9f367f,"[u""Data Scientist\nState Street - Boston, MA\nFebruary 2017 to Present\nDescription: State Street Corporation, through its subsidiaries, provides a range of financial products and services to institutional investors worldwide. The company offers investment servicing products and services, including custody; product- and participant-level accounting; daily pricing and administration; master trust and master custody.\n\nResponsibilities:\n\u2022 Setup storage and data analysis tools in Amazon Web services cloud computing infrastructure.\n\u2022 Used pandas, numpy, Seaborn, scipy, matplotlib, sci-kit-learn, NLTK in Python for developing various machine learning algorithms.\n\u2022 Installed and used CaffeDeep Learning Framework\n\u2022 Worked on different data formats such as JSON, XML and performed machine learning algorithms in Python.\n\u2022 Worked as Data Architects and IT Architects to understand the movement of data and its storage and ER Studio 9.7\n\u2022 Worked with the UNIX team and installed TIDAL job scheduler on QA and Production Netezza environment.\n\u2022 Performed Data preparation on a High dimensional (Big data with large volume and variety) Data sample collected from the live customer data.\n\u2022 Data preparation, writing SQL queries Includes Mapping of unlined data from various formats, Identifying The missing data, Finding the correlations, scaling and removing the Junk data to further process the data for building a predictive model\n\u2022 Closely working with Network engineers and Ericsson analytics Expert Team to find the rule sets to build a predictive model.\n\u2022 Processed data using R programming and developed a Predictive model to predict KPI'S (Key performance indicators) Such as VOLTE accessibility, Connection success, Session Set Up success and Retain Ability.\n\u2022 Handled importing data from various data sources, performed transformations using Hive, Map Reduce, and loaded data into HDFS.\n\u2022 Worked with BTEQ to submit SQL statements, import and export data, and generate reports in Teradata.\n\u2022 Designed and documented Use Cases, Activity Diagrams, Sequence Diagrams, OOD (Object Oriented Design) using UML and Visio.\n\u2022 Summarized data through various tabular and visual modes for preliminary discovery analysis.\n\u2022 Conducted and interpreted multivariate analyses examples, including regressions with various distributions and duration models.\n\u2022 Created customized reports and processes in SAS and R.\n\u2022 Evaluated research for practical application by utilizing preclinical data sets and clinical trial modeling methods.\n\u2022 Pulled data from data lakes comprised of varied sources such as SQL, NoSQL structured and unstructured data.\n\u2022 Interaction with Business Analyst, SMEs, and other Data Architects to understand Business needs and functionality for various project solutions\n\u2022 Researched, evaluated, architected, and deployed new tools, frameworks, and patterns to built sustainable Big Data platforms for the clients\n\u2022 Identifying and executing process improvements, hands-on in various technologies such as Oracle, Informatica, Business Objects.\n\u2022 Designed both 3NF data models for ODS, OLTP systems and dimensional data models using Star and Snowflake Schemas.\n\nEnvironment: R 9.0, Informatica 9.0, ODS, OLTP, Bigdata, Oracle 10g, Hive, OLAP, DB2, Metadata, Python ,MS Excel, Mainframes MS Vision, Rational Rose."", u""Data Scientist\nZions Bancorporation - Salt Lake City, UT\nDecember 2015 to January 2017\nDescription: Zions Bancorporation, a financial holding company, provides a range of banking and related services primarily in Arizona, California, Colorado, Idaho, Nevada, New Mexico, Oregon, Texas, Utah, Washington, and Wyoming. The company offers community banking services, such as small and medium-sized business and corporate banking.\n\nResponsibilities:\n\u2022 Analyzed the business requirements of the project by studying the Business Requirements Specification document.\n\u2022 Extensively worked on Data Modeling tools Erwin Data Modeler to design the datamodels.\n\u2022 Handled importing data from various data sources, performed transformations using Hive and loaded data into HDFS.\n\u2022 Collaborate with data engineers to implement ETL process, write and optimized SQL queries to perform data extraction from Cloud and merging from Oracle 12c.\n\u2022 Collect unstructured data from MongoDB and completed data aggregation.\n\u2022 Worked in Statistical Modeling and Machine Learning techniques (Linear, Logistics, Decision Trees, Random Forest, SVM, K-Nearest Neighbors, Bayesian, XGBoost) in Forecasting/ Predictive Analytics, Segmentation methodologies, Regression-basedmodels, Hypothesis testing, Factor analysis/ PCA, Ensembles.\n\u2022 Designed tables and implemented the naming conventions for Logical and Physical Data Models in Erwin7.0.\n\u2022 Participated in the conversion of ITS (Immigration Tracking System) Visual Basic client-server application into C#, ASP.NET3-tierIntranet application.\n\u2022 Performed Exploratory Data Analysis and Data Visualizations using R, and Tableau.\n\u2022 Perform a proper EDA, Uni-variate and bi-variate analysis to understand the intrinsic effect/combined effects.\n\u2022 Worked with Data Governance, Data quality, data lineage, Data architect to design various models and processes.\n\u2022 Independently coded new programs and designed Tables to load and test the program effectively for the given POC's using with Big Data/Hadoop.\n\u2022 Designed data models and data flow diagrams using Erwin and MS Visio.\n\u2022 Utilized ADO.Net Object Model to implement middle-tier components that interacted with MSSQL Server 2000database.\n\u2022 Participated in AMS (Alert Management System) JAVA and SYBASE project. Designed SYBASE database utilizing ERWIN. Customized error messages utilizing SP_ADDMESSAGE and SP_BINDMSG. Created indexes, made query optimizations. Wrote stored procedures, triggers utilizing T-SQL.\n\u2022 Explained the data model to the other members of the development team. Wrote XML parsing module that populates alerts from the XML file into the database tables utilizing JAVA, JDBC, BEAWEBLOGICIDE, Document Object Model.\n\u2022 As an Architect implemented MDM hub to provide clean, consistent data for anSOA implementation.\n\u2022 Developed, Implemented & Maintained the Conceptual, Logical &Physical Data Models using Erwin for forwarding/Reverse Engineered Databases.\n\u2022 Explored and Extracted data from source XML in HDFS, preparing data for exploratory analysis using data mining.\n\nEnvironment: SQL Server 2008R2/2005 Enterprise, SSRS, SSIS, Crystal Reports, Hadoop, Windows Enterprise Server 2000, DTS, Bigdata, Python, machine learning, SQL Profiler, and Query Analyzer."", u""Data Scientist\nWright Express Corp - South Portland, ME\nMarch 2014 to November 2015\nDescription:Wright Express Corporation provides business payment processing and information management solutions in North America, the Asia Pacific, and Europe. It operates in two segments, Fleet Payment Solutions, and Other Payment Solutions.\nResponsibilities:\n\n\u2022 Coded R functions to interface with Caffe Deep Learning Framework\n\u2022 Working in Amazon Web Services cloud computing environment\n\u2022 Worked with several R packages including knitr, dplyr, Spark R, Causal Infer, space-time.\n\u2022 Implemented end-to-end systems for Data Analytics, Data Automation and integrated with custom visualization tools using R, Mahout, Hadoop, and MongoDB.\n\u2022 Extracting the source data from Oracle tables, MS SQL Server, sequential files and excel sheets.\n\u2022 Perform analysis using predictive modeling, data/text mining, and statistical tools\n\u2022 Developed MapReduce/Spark Python modules for machine learning & predictive analytics in Hadoop on AWS.\n\u2022 Collaborate cross-functionally to arrive at actionable insights\n\u2022 Developing and maintaining Data Dictionary to create metadata reports for technical and business purpose.\n\u2022 Performed Exploratory Data Analysis and Data Visualizations using R, and Tableau.\n\u2022 Perform a proper EDA, Uni-variate and bi-variate analysis to understand the intrinsic effect/combined effects.\n\u2022 Worked with Data Governance, Data quality, data lineage, Data architect to design various models and processes.\n\u2022 Independently coded new programs and designed Tables to load and test the program effectively for the given POC's using with Big Data/Hadoop.\n\u2022 Designed data models and data flow diagrams using Erwin and MS Visio.\n\u2022 As an Architect implemented MDM hub to provide clean, consistent data for anSOA implementation.\n\u2022 Developed, Implemented &Maintained the Conceptual, Logical& Physical Data Models using Erwin for forwarding/Reverse Engineered Databases.\n\u2022 Established Data architecture strategy, best practices, standards, and roadmaps.\n\u2022 Lead the development and presentation of a dataanalytics data-hub prototype with the help of the other members of the emerging solutions team\n\u2022 Performed datacleaning and imputation of missing values using R.\n\u2022 Worked with Hadoop eco system covering HDFS, HBase, YARN, andMapReduce\n\u2022 Take up ad-hoc requests based on different departments and locations\n\u2022 Used Hive to store the data and perform datacleaning steps for huge datasets.\n\u2022 Created dash boards and visualization on regular basis using ggplot2 and Tableau.\n\u2022 Creating customized business reports and sharing insights to the management.\n\u2022 Worked with BTEQ to submit SQL statements, import and export data, and generate reports in Teradata.\n\u2022 Interacted with the other departments to understand and identify data needs and requirements and work with other members of the IT organization to deliver data visualization and reporting solutions to address those needs.\n\nEnvironment: Erwin r, Informatica, ODS, OLTP, Oracle 10g, Hive, OLAP, DB2, Metadata, MS Excel, Mainframes MS Visio, Rational Rose, and Requisite Pro, Hadoop, PL/SQL, etc."", u'Data Scientist\nQuality Systems Inc - Irvine, CA\nMay 2013 to February 2014\nDescription:Quality Systems, Inc., together with its subsidiaries, develops and markets healthcare information systems in the United States. The company operates through four divisions: QSI Dental, NextGen, Inpatient Solutions, and Practice Solutions.\nResponsibilities:\n\n\u2022 Statistical Modeling with ML to bring Insights in Data under guidance of Principal Data Scientist\n\u2022 Data modeling with Pig, Hive, Impala.\n\u2022 Ingestion with Sqoop, Flume.\n\u2022 Define the list codes and code conversions between the source systems and the data mart.\n\u2022 Implementation of Metadata Repository, Maintaining Data Quality, Data Cleanup procedures, Transformations, Data Standards, Data Governance program, Scripts, Stored Procedures, triggers and execution of test plans\n\u2022 Used SVN to commit the Changes into the main EMM application trunk.\n\u2022 Understanding and implementation of text mining concepts, graph processing and semi-structured and unstructured data processing.\n\u2022 Worked with Ajax API calls to communicate with Hadoop through Impala Connection and SQL to render the required data through it. These API calls are similar to Microsoft Cognitive API calls.\n\u2022 Good grip on Cloudera and HDP ecosystem components.\n\u2022 Used Elasticsearch (Big Data) to retrieve data intothe application as required.\n\u2022 Performed Map Reduce Programs those are running on the cluster.\n\u2022 Developed multiple MapReduce jobs in java for data cleaning and preprocessing.\n\u2022 Analyzed the partitioned and bucketed data and compute various metrics for reporting.\n\u2022 Involved in loading data from RDBMS and weblogs into HDFS using Sqoop and Flume.\n\u2022 Worked on loading the data from MySQL to HBase where necessary using Sqoop.\n\u2022 Developed Hive queries for Analysis across different banners.\n\u2022 Extracted data from Twitter using Java and Twitter API. Parsed JSON formatted twitter data and uploaded to the database.\n\u2022 Launching Amazon EC2 Cloud Instances using Amazon Images (Linux/ Ubuntu) and Configuring launched instances with respect to specific applications.\n\u2022 Exported the result set from Hive to MySQL using Sqoop after processing the data.\n\u2022 Analyzed the data by performing Hive queries and running Pig scripts to study customer behavior.\n\u2022 Have hands-on experience working withSequence files, AVRO, HAR file formats and compression.\n\u2022 Used Hive to partition and bucket data.\n\u2022 Experience in writing MapReduce programs with Java API to cleanse Structured and unstructured data.\n\u2022 Wrote Pig Scripts to perform ETL procedures on the data in HDFS.\n\u2022 Created HBase tables to store various data formats of data coming from different portfolios.\n\u2022 Worked on improving the performance of existing Pig and Hive Queries.', u'Data Architect/Data Modeler\nVirtela - Mumbai, Maharashtra\nMarch 2011 to April 2013\nEnvironment: SQL/Server, Oracle, MS-Office, Teradata, Informatica, ER Studio, XML, Business Objects, HDFS, Teradata 14.1, JSON, HADOOP (HDFS),Python, MapReduce, PIG, Spark, R Studio, MAHOUT, JAVA, HIVE, AWS.\nClient: Virtela, Mumbai, India Mar 2011 -Apr 2013\nRole: Data Architect/Data Modeler\n\nDescription: Virtela, an NTT Communications Company, is the smart alternative for enterprise networking and virtualized IT services.\n\nResponsibilities:\n\u2022 Worked with large amounts of structured and unstructured data.\n\u2022 Knowledge of Machine Learning concepts (Generalized Linear models, Regularization, Random Forest, Time Series models, etc.)\n\u2022 Worked in Business Intelligence tools and visualization tools such as Business Objects, ChartIO, etc.\n\u2022 Deployed GUI pages by using JSP, JSTL, HTML, DHTML, XHTML, CSS, JavaScript, and AJAX.\n\u2022 Configured the project on WebSphere 6.1 application servers\n\u2022 Implemented the online application by using Core Java, JDBC, JSP, Servlets and EJB 1.1, Web Services, SOAP, WSDL.\n\u2022 Handled end-to-end project from data discovery to model deployment.\n\u2022 Monitoring the automated loading processes.\n\u2022 Communicated with other Health Care info by using Web Services with the help of SOAP, WSDL JAX-RPC\n\u2022 Used Singleton, factory design pattern, DAO Design Patterns based on the application requirements\n\u2022 Used SAX and DOM parsers to parse the raw XML documents\n\u2022 Used RAD as Development IDE for web applications.\n\u2022 Preparing and executing Unit test cases\n\u2022 Used Log4J logging framework to write Log messages with various levels.\n\u2022 Involved in fixing bugs and minor enhancements to the front-end modules.\n\u2022 Implemented Microsoft Visio and Rational Rose for designing the Use Case Diagrams, Class model, Sequence diagrams, and Activity diagrams for SDLC process of the application\n\u2022 Doing functional and technical reviews\n\u2022 Maintenance in the testing team for System testing/Integration/UAT.\n\u2022 Guaranteeing quality in the deliverables.\n\u2022 Conducted Design reviews and Technical reviews with other project stakeholders.\n\u2022 Used pandas, NumPy, seaborn, SciPy, matplotlib, Scikit-Learn, NLTK in Python for utilizing various machine learning algorithms.\n\u2022 Worked on different data formats such as JSON, XML and performed machine learning algorithms in Python.\n\u2022 Was a part of the complete life cycle of the project from the requirements to the production support\n\u2022 Created test plan documents for all back-end database modules\n\u2022 Implemented the project in Linux environment.\n\nEnvironment: R, Erwin, MDM, QlikView, Machine learning, MLlib, PL/SQL, HDFS, Teradata, Python, JSON, HADOOP (HDFS), MapReduce, PIG, R Studio, MAHOUT, JAVA, HIVE, AWS.', u'Data Analyst/Data Modeler\nPeople Tech Group - Hyderabad, Telangana\nAugust 2009 to February 2011\nDescription: Founded in 2006, People Tech is an emerging leader in the Enterprise Applications and IT Services marketplace. People Tech draws its expertise from strategic partnerships with technology leaders like Microsoft, Oracle and SAP and combines that with the deep understanding of its employees.\n\nResponsibilities:\n\u2022 Developed Internet traffic scoring platform for ad networks, advertisers, and publishers (rule engine, site scoring, keyword scoring, lift measurement, linkage analysis).\n\u2022 Responsible for defining the key identifiers for each mapping/interface.\n\u2022 Clients include eBay, Click Forensics, Cars.com, Turn.com, Microsoft, and Look smart.\n\u2022 Implementation of Metadata Repository, Maintaining Data Quality, Data Cleanup procedures, Transformations, Data Standards, Data Governance program, Scripts, Stored Procedures, triggers and execution of test plans.\n\u2022 Designed the architecture for one of the first analytics 3.0. Online platforms: all-purpose scoring, with on-demand, SaaS, API services. Currently under implementation.\n\u2022 Experience in Creating visually impactful dashboards in Excel and Tableau for data reporting by using pivot tables and VLOOKUP.\n\u2022 Carried out data manipulation, data cleaning, dealt with missing records, purged and consolidated data for analysis and selected the appropriate visualization techniques; developing reports and communicating insights to stakeholders.\n\u2022 Developed Enterprise applications in the Client/Server, Web application using PHP, Word Press, JavaScript, Jquery, AJAX, HTML5, CSS, MySQL & SQL Server database.\n\u2022 Designing and developing dynamic web pages using PHP and WordPress. Effectively used Tableau, Excel, jQuery, HTML, CSS and AJAX interactions.\n\u2022 Creation of multimillion bid keyword lists using extensive web crawling. Identification of metrics to measure the quality of each list (yield or coverage, volume, and keyword average financial value).\n\u2022 Enterprise Metadata Library with any changes or updates.\n\u2022 Document data quality and traceability documents for each source interface.\n\u2022 Establish standards of procedures.\n\u2022 Generate weekly and monthly asset inventory reports.\n\nEnvironment: Erwin r, SQL Server 2000/2005, Windows XP/NT/2000, Oracle, MS-DTS, UML, UAT, SQL Loader, OOD, OLTP, PL/SQL, MS Visio, Informatica.']",[u'Bachelors in Computer Science'],[u'Informatica Power Centre']
0,https://resumes.indeed.com/resume/8736c1a7ea8eb517,"[u""Business Analyst / Data Analyst\nClient -Pfizer, IL\nMay 2017 to Present\nResponsibilities:\n\u2022 Worked on SQL*Loader to load data from flat files obtained from various facilities every day.\n\u2022 Played a key role in establishing best practices for tableau reporting by conducting meetings with the clients and key stakeholders for gathering reporting requirements and to determine KPI's and developing POC Dashboards.\n\u2022 Leverage strong expertise in analyzing and identifying business requirements, reviewing and communicating with business, and getting sign offs from business stakeholders. Employ Tableau best development practices in developing and formulating reports, dashboards, and SQL queries.\n\u2022 Expertly designed and applied KPIs, Dashboards, and Operational Metrics using Performance Point Server and published them via Tableau to enable KPIs tracking in a real-time.\n\u2022 Expertly created relationships, actions, data blending, filters, parameters, hierarchies, calculated fields, sorting, groupings, live connections, and in-memory in both Tableau and excel.\n\u2022 Integrated Tableau server with SharePoint portal and setup auto refresh.\n\u2022 Optimized reports performance by reducing analysis time from 1 hour to 4 minutes by converting excel spreadsheets into Tableau interactive dashboards.\n\u2022 Produced Excel, Qlik View and Domo reports and managed the overall initiative of developing and documenting technical architecture, system design, performance test results, and other technical aspects of the implementation.\n\u2022 Streamlined multiple performance issues and implemented best process distribution for different projects.\n\u2022 Played a pivotal role in conducting Functional testing, Regression testing, Integration testing, Smoke testing, and UAT.\n\u2022 Reduced analysis time from 2 hours to 4 minutes by converting excel spreadsheets into Qlikview interactive dashboards.\nTools and Environment: Informatica 9.1, Oracle 11g, SQL Developer, PL/SQL, Tableau, TOAD, MS Access, MS Excel"", u'Data Analyst\nThermo Fisher Scientific - Carlsbad, CA\nJune 2015 to April 2017\nWorked with business requirements analysts/subject matter experts to identify and understand\nrequirements. Conducted user interviews and data analysis review meetings.\n\u2022 Defined key facts and dimensions necessary to support the business requirements along with Data Modeler.\n\u2022 Created draft data models for understanding and to help Data Modeler.\n\u2022 Resolved the data related issues such as: assessing data quality, data consolidation, evaluating\nexisting data sources.\n\u2022 Manipulating, cleansing & processing data using Excel, Access and SQL.\n\u2022 Responsible for loading, extracting and validation of client data.\n\u2022 Coordinated with the front-end design team to provide them with the necessary stored procedures and packages and the necessary insight into the data.\n\u2022 Participated in requirements definition, analysis and the design of logical and physical data models\n\u2022 Leading data discovery discussions with Business in JAD sessions and map the business requirements to logical and physical modeling solutions.\n\u2022 Conducted data model reviews with project team members\n\u2022 Captured technical metadata through data modeling tools\n\u2022 Code standard Informatica ETL routines\n\u2022 Developed standard Tableau Reports\n\u2022 Collaborated with ETL teams to create data landing and staging structures as well as source to target mapping documents\n\u2022 Ensure data warehouse database designs efficiently support BI and end user requirements\n\u2022 Collaborated with application and services teams to design databases and interfaces which fully meet business and technical requirements\n\u2022 Maintain expertise and proficiency in the various application areas\n\u2022 Maintain current knowledge of industry trends and standards\n\nTools and Environment: Informatica 9.1, Oracle 11g, PL/SQL, Erwin, TOAD, MS Access, MS Excel', u'Data Analyst\nCisco, CA\nAugust 2013 to May 2015\nResponsibilities:\n\u2022 Analysis of housing trends and review mortgage product characteristics such as underwriting parameters, closing costs, mortgage insurance, qualification rules, rate sheets and price adjustments.\n\u2022 Involved in Agile methodology break tasks into small increments with minimal planning, do not directly involve long-term planning and daily stand-ups, updating tasks.\n\u2022 Processed data received from vendors and loading them into database. The process was carried out on weekly basis and reports were delivered on bi-weekly basis. The extracted data had to be checked for integrity.\n\u2022 Conducted JAD sessions to identify Requirements.\n\u2022 Documented requirements and obtained signoffs.\n\u2022 Coordinated between the Business users and development team in resolving issues.\n\u2022 Documented data cleansing and data profiling.\n\u2022 Wrote SQL scripts to meet the business requirement.\n\u2022 Analyzed views and produced reports.\n\u2022 Tested cleansed data for integrity and uniqueness.\n\u2022 Automated the existing system to achieve faster and accurate data loading.\n\u2022 Generated weekly, bi-weekly reports to be sent to client business team using business objects and documented them too.\n\u2022 Experience creating Business Process Models.\n\u2022 Ability to manage multiple projects simultaneously tracking them towards varying timelines effectively through a combination of business and technical skills.\n\u2022 Good Understanding in clinical practice management, medical and laboratory billing and insurance claim with processing with process flow diagrams.\n\u2022 Worked with compliance team to provide walkthroughs, review requirements and secure approvals to make sure the project is Business Requirements compliant.\n\u2022 Assisted QA team in creating test scenarios that covers day in a life of patient for Inpatient and Ambulatory workflows.\n\nTools and Environment: MS Excel, MS Power Point, Shell Scripting, PL/SQL, Teradata, DB2, Teradata, Business Objects, Windows 2003/2007', u'Software Programmer/Analyst\nAccenture - Bengaluru, Karnataka\nDecember 2012 to July 2013\nResponsibilities:\n\u2022 Developed and tested extraction, transformation, and load ETL Jobs in Informatica processes by utilizing transformations.\n\u2022 Optimized workflow processes by using Workflow manager for session management, database connection management, and scheduling of jobs.\n\u2022 Defined transformation process by working on Power Center client tools such as Source Analyzer, Target Designer, Mapping Designer, and Transformations Developer.\n\u2022 Enabled re usability of business rules through user defined functions in the mapping.\n\u2022 Utilized shortcuts to allow re usability of objects without creating multiple objects in the repository and inheriting changes made to the source automatically.\n\u2022 Skillfully performed Unit Testing of developed mapping.\n\u2022 Developed automation scripts in UNIX for testing purposes, consequentially reducing the duration of test cycles from 2 weeks to 1 week.\n\u2022 Provided daily communications, scrum notes, sprint reviews reports, project retrospectives, and regular snapshots of project velocity and budget burn rate\n\u2022 Advanced and optimized database queries and procedures in SQL improving application proficiency by 13%.\n\u2022 Awarded as STAR Performer on basis of exceptional service delivery.\n\nTools and Environment: Informatica 9.1, Oracle 11g, PL/SQL, TOAD, MS Access, MS Excel', u'Software Analyst\nHimas Solutions - IN\nMay 2010 to November 2012\nResponsibilities:\n\u2022 Processed data received from vendors and loading them into database. The process was carried out on weekly basis and reports were delivered on bi-weekly basis. The extracted data had to be checked for integrity.\n\u2022 Documented requirements and obtained signoffs.\n\u2022 Coordinated between the Business users and development team in resolving issues.\n\u2022 Documented data cleansing and data profiling.\n\u2022 Wrote SQL scripts to meet the business requirement.\n\u2022 Analyzed views and produced reports.\n\u2022 Tested cleansed data for integrity and uniqueness.\n\u2022 Automated the existing system to achieve faster and accurate data loading.\n\u2022 Generated weekly, bi-weekly reports to be sent to client business team using business objects and documented them too.\n\u2022 Learned to create Business Process Models.\n\u2022 Ability to manage multiple projects simultaneously tracking them towards varying timelines effectively through a combination of business and technical skills.\n\u2022 Good Understanding in clinical practice management, medical and laboratory billing and insurance claim with processing with process flow diagrams.\n\u2022 Assisted QA team in creating test scenarios that covers day in a life of patient for Inpatient and Ambulatory workflows.\n\nTools and Environment: SQL, Windows, Oracle 10g']","[u""Bachelor's""]",[u'']
0,https://resumes.indeed.com/resume/1c87bf7e5cb0eda3,"[u'Analyst - Data Analytics\nTechniecode Inc\nFebruary 2018 to Present\n\u2022 Perform Data & text mining, web-scraping from existing data sources and web platforms.\n\u2022 Conduct data standardization, pre-processing & joins on mortgage data for analytics using SQL, R & Python.\n\u2022 Perform data integration & blending data to build dashboards using Tableau/ PowerBI using data extracts.\n\u2022 Identify potential leads by leveraging available data for downstream analytics, mortgage solutions.\n\u2022 Develop metrics, reports and dashboards using mortgage data to provide key insights & solutions.', u'Data Analyst\nNorthern Illinois University\nJanuary 2016 to December 2017', u'BI Analyst/Data Analyst\nE-Logistics\nApril 2013 to November 2015\n\u2022 Data extraction, pre-processing & integration using SSIS to facilitate down-stream analysis, reporting & visualization.\n\u2022 Performed Data aggregation using SQL, data blending using extracts from different data sources using Tableau.\n\u2022 Development & support of Sales reports, reporting dashboards, visualizations and consumer analytics presentations.\n\u2022 Built interactive dashboards with drill-down visualizations using Tableau, PowerBI to leverage forecasting, analytics.\n\u2022 Data Stewardship, Ad-hoc reporting (SSRS) to formulate strategic plans; end-user training on dashboards and models.\n\u2022 Developed a Client-Vendor Ticket Management Portal to record, track transactions using ASP.NET, SQL Server & CSS.\n\u2022 Developed stored procs, functions & triggers for performance enhancements, security & to ensure data integrity.\n\u2022 Lead the design & development of cross-functional UIs using the MVC architecture to facilitate all data transactions.', u'Data Analyst & Process Trainer\nGoogle India / Global Logic\nJune 2011 to April 2013\n\u2022 Performed Data mining, standardization and algorithm analysis to ensure quality live data in geo-content products.\n\u2022 Streamlined the ETL/ELT process using SSIS to extract, standardize & improve data quality for geo analytics.\n\u2022 Gather insights from clustered data to optimize and streamline data life cycle to improve algorithms & data quality.\n\u2022 Root cause analysis, evaluation of data to enhance clustering algorithms & ensure qualitative downstream analysis.\n\u2022 Determine KPIs, generate metrics, performance reports & dashboards using MS Excel, SQL Analytics.\n\u2022 Process trainer, mentor - Conducted training sessions for new hires to achieve a quality of 98 % in issue resolution.\n\u2022 Process development - Improved the design, layout of content-review UI to improve the resolution time by 30 %.', u'Technical Intern\nGalaxie Software Solutions\nFebruary 2010 to May 2010\nDesigned & built a mobile phone-controlled robot landmine diffuser using micro-controller & DTMF technology.']","[u""Bachelor's""]",[u'']
0,https://resumes.indeed.com/resume/b830a4631d6ebb32,"[u'Data Analyst\nCONCENTRIX CORPORATION - Pittsburgh, PA\nAugust 2017 to Present\nResponsibilities:\n\u2022 Currently working on the analysis of key operational productivity metrics of the Health Plan Operations Business team using SAS to properly price the service and define the staffing needs.\n\u2022 Proactively automated the monthly report which uses invoice data from various vendors using Tableau, thereby showing the root cause for variance in charge across time.\n\u2022 Developed a dashboard using Tableau that gives a pictorial representation between the actual and forecasted budget per vendor on a per year basis. Analyzed the reason when difference between actual and forecasted is considerable.\n\u2022 Documented data flow for multiple reports by preparing Data Flow Diagrams (DFD) and identifying crucial tables.\n\u2022 Code reviewed scripts written in Toad for Oracle to identify active and inactive tables. This was necessary to optimize data storage by dropping inactive tables.\n\u2022 Optimized the scripts from Toad for Oracle using SAS thereby reducing script runtime by 30%.\nEnvironment: SAS EG 7.1, Tableau Desktop 10.5, Toad for Oracle 12.6', u'Data Scientist Intern\nRAILINC CORPORATION - Cary, NC\nJanuary 2017 to May 2017\nResponsibilities:\n\u2022 Facilitated data migration from Teradata to Green Plum using the ETL tool Informatica to obtain real time data from ""Clear Path System"" that helps railroads plan the train movement through the Chicago Terminal.\n\u2022 Designed and developed Python scripts to automate metrics generation involved as a part of Clear Path System.\n\u2022 Optimized Python script run time by 20 % to facilitate improved efficiency which was previously running in SAS.\n\u2022 Predicted the volume of cars and peak hour traffic entering Chicago everyday thereby helping train movement planning.\n\u2022 Implemented machine learning techniques like clustering and regression on Tableau and created interactive dashboards.\nEnvironment: Python 2.7, Tableau 10.1, SAS EG 7.1, Dbeaver', u""Data Analyst\nTATA CONSULTANCY SERVICES - Chennai, Tamil Nadu\nJune 2014 to December 2015\nResponsibilities:\n\u2022 Responsible for ensuring quality of Sainsbury's Customer data and increase the number of contactable customers.\n\u2022 Analyzed data quality reports generated using SAS and SQL (Teradata)\n\u2022 Provided innovative solutions with processing of customer E-mail address that enabled us to add over 40, 000 customers to Sainsbury's contactable base.\n\u2022 Produced several sales analysis reports at a short notice for senior stakeholders to keep track of the business.\n\u2022 Predicted customer's preference based on order history to maximize sales efficiency using clustering and decision tree.\n\u2022 Built a model to predict the high propensity customers using logistic regression and implemented that for targeted marketing.\n\u2022 Created a table for campaign managers with key customer information from various systems making selections easier, apply business rules consistently and save time.\nEnvironment: SAS EG 7.1, Teradata, Excel""]","[u""Master's""]",[u'']
0,https://resumes.indeed.com/resume/2bbf83a28dcdbd73,"[u'DATA SCIENCE ANALYST (INTERN)\nAll Health Network - Denver, CO\nJuly 2017 to Present\nUsed SQL to extract and transform the data from Freud environment and load the structured data into the Smart Care environment. This\nreduced the service calls from 6000 to 4000.\n\u2022 Conducted queries via Partners EHR/EMR system and output in SQL Server database as part of Readmission Project.\n\u2022 Developed algorithm to convert insurance-orientated ICD-9 codes to clinical practice meaningful disease classification using Python.\n\u2022 Conducted data analysis using logistical model, KNN and random forest method to identify high readmission risk patient and improved the accuracy (C-scores) by 30 percent.\n\u2022 Extracted twitter data using Python and did text mining analysis with BeautifulSoup (Python) and SAS E-Miner to improve the AHN facilities\nwhich increased the occupancy rate by 12%.\n\u2022 Built complex SQL reports to audit $2.5 million of pay and insurance benefits for over 150 individual records.\n\u2022 Designed and developed various analytical reports from multiple data sources by blending data on a single Worksheet in Tableau Desktop\n\u2022 Involved in the planning phase of internal & external table schemas in Hive with appropriate static and dynamic portions for efficiency.', u'DATA ANALYST\nTata Consultancy Services - Hyderabad, Telangana\nMarch 2014 to July 2016\n\u2022 Responsible for analytical data needs, handling of complex data requests, reports, and predictive data modeling\n\u2022 Designed ad-hoc queries with SQL in Cognos ReportNet. Examined reports and presented findings in PowerPoint and Excel.\n\u2022 Used Anomaly & Fraud detection techniques with SAS E-Miner for the American Express client resulting in reduction of 22% of fraudulent\ncases.\n\u2022 Reporting of frauds, missed transactions, forecast, user behavior using Tableau in direct weekly cross-functional team meetings for continuous\nprocess improvement.\n\u2022 Implemented Agile Scrum practices for project implementation which reduced the project touch time by 300 man-hours and cost reduction\nof $30,000/year.\n\u2022 Conducted statistical analysis to leverage the results to drive brand decision making and survey development resulting in 4 new projects\nfrom business partners.', u""SYSTEMS DATA ENGINEER\nFebruary 2011 to February 2014\n\u2022 Evaluated performance of 300+ stores for Nielsen clients based on key metrics and identified opportunities to enable stores to meet and exceed their financial targets through increased sales.\n\u2022 Created data lake by extracting customer's data from various data sources into HDFS. This includes data from Teradata, Mainframes, RDBMS,\nCSV and Excel.\n\u2022 Involved in optimization of SQL scripts and designed efficient queries to query data\n\u2022 Developed the SQL table schema for the effective storage of the customer data.\n\u2022 Involved in preparing design and unit and Integration test documents\n\u2022 Developed an internal web-scraper tool for inspection of ad-hosting on websites using google, URLLib, Beautiful Soup packages in python.""]","[u'MASTER OF SCIENCE IN BUSINESS ANALYTICS in Statistics', u'in Neural Networks and Artificial Intelligence']","[u'University of Colorado Denver, CO\nAugust 2016 to December 2017', u'JNTU(Jawaharlal Nehru Technological University) Hyderabad, Telangana\nJuly 2006 to May 2010']"
0,https://resumes.indeed.com/resume/3507fc19d2e81e7f,"[u'Data Modeler/Data Analyst\nThyssenKrupp-Indianapolis - Indianapolis, IN\nMarch 2016 to Present\nResponsibilities:\n\u2022 Created Logical & Physical Data Modeling on Relational (OLTP), Dimensional Data Modeling (OLAP) on Star schema for Fact & Dimension tables using Erwin.\n\u2022 Gathered business requirements, working closely with business users, project leaders and developers. Analyzed the business requirements and designed Conceptual and Logical Data models.\n\u2022 Prepared ETL technical Mapping Documents along with test cases for each Mapping for future developments to maintain SDLC.\n\u2022 Involved in Data flow analysis, Data modeling, Physical database design, forms design and development, data conversion, performance analysis and tuning.\n\u2022 Create and maintain data model standards, including master data management (MDM).\n\u2022 Involved in extracting the data from various sources like Oracle, SQL, Teradata, and XML.\n\u2022 Implemented Referential Integrity using primary key and foreign key relationships.\n\u2022 Managed database design and implemented a comprehensive Star-Schema with shared dimensions.\n\u2022 Implemented Normalization Techniques and build the tables as per the requirements given by the business users.\n\u2022 Developed and maintained stored procedures. Implemented changes to database design including tables and views.\n\u2022 Worked Normalization and De-normalization concepts and design methodologies like Ralph Kimball and Bill Inmon approaches.\n\u2022 Conduct Design reviews with the business analysts and the developers to create a proof of concept for the reports.\n\u2022 Performed detailed data analysis to analyze the duration of claim processes and created the cubes with Star Schemas using facts and dimensions through SQL Server Analysis Services (SSAS).\n\u2022 Performed Data Analysis and Data Profiling using complex SQL queries on various sources systems including Oracle, Teradata.\n\u2022 Implemented data blending using various data sources and created stories using Tableau for better understanding of the data.\n\u2022 Documented Source to Target mappings as per the business rules.\n\u2022 Implemented Error Handling during ETL load in SSIS packages to identify dimensions and facts that were not properly populated.\n\u2022 Analyzed the system thoroughly and Created System Document of a complex system without any input/document which helped us to get the project from competitors.\n\u2022 Participated in client discussions to gather scope information and perform analysis of scope information to provide inputs for project documents.\n\u2022 Data governance functional and practical implementation and responsible for designing common Data governance frameworks.', u'Data Modeler/Data Analyst\nCVS Health - Monroeville, PA\nJanuary 2015 to February 2016\nResponsibilities:\n\u2022 Resolved business and technical queries from business partners related to decision making and reporting on data available in the Data warehouse.\n\u2022 Responsibility included requirement study, developing use cases for business layer and Dimensional Data Modeling and coordinating the team for development.\n\u2022 Involved in designing logical, physical, and dimensional database designs using reverse and forward engineering of ERWIN Data Modeler.\n\u2022 Used ERWIN Data Modeler for Generating DDL, Stored Procedure and Trigger Code for Source to Target database.\n\u2022 Involved in using Complete-Compare Technology to support iterative development by keeping the models and databases.\n\u2022 Involved in Data Warehouse Support - Star Schema and Dimensional Modeling to help design datamarts and data warehouse.\n\u2022 Identify source systems, their connectivity, related tables, and fields and ensure data suitably for mapping.\n\u2022 Used named queries and named calculations to modify the data source view Worked with STAR and SNOW FLAKE schema designs for the financial data-mart.\n\u2022 Created custom time dimensions and worked with role playing dimensions, degenerated dimensions.\n\u2022 Used Agile software development methodology in defining the problem, gathering requirements, development iterations, business modeling and communicating with the technical team for development of the system.\n\u2022 Worked with the Implementation team to ensure a smooth transition from the design to the implementation phase.', u'Data Analyst\nCVS Health - San Diego, CA\nApril 2015 to November 2015\nResponsibilities:\n\u2022 Created and maintained custom app for client that helps in organizing Healthcare cloud services by creating stronger relationships between patients, doctors and care providers.\n\u2022 Designed, implemented and deployed custom objects, page layouts, custom tabs, components to suit to the needs of the application and worked with roles profiles hierarchies and permission sets and groups extensively\n\u2022 Created workflow rules and defined related tasks, validation rules, auto-response rules\n\u2022 Created and maintained standard reports and dashboards and also maintain user roles, security, profiles, workflow rules, custom objects, custom fields and other system configuration options\n\u2022 Worked with various teams for business requirements to help strategize new product portfolio & business opportunities.\n\u2022 Ensured consistent application of scrum methodologies across the enterprise and produced business use cases, activity diagrams and user stories across cross-functional environment\n\u2022 Performed data Extraction, Pre-processing & cleansing to remove redundant and unwanted data(ETL) to extract insights from unstructured data thereby use the analysis to construct the roadmap for future analysis.', u'Data Analyst\nNCP Finance - Dayton, OH\nJuly 2013 to March 2015\nResponsibilities:\n\u2022 As a member of the Data analytics and reporting team, responsible for using data to deliver influential insights for workplace decisions\n\u2022 Conducted trends and patterns analysis to figure out root cause of Data Issues to improve the Data Quality\n\u2022 Write complex SQL queries and optimizing them to pull the required data from multiple data sources in an effective way\n\u2022 Preprocessing and cleansing data using Excel and SQL queries\n\u2022 Validated any changes of data, Behind Logic/Formula/Calculation of new created fields\n\u2022 Coordinate with partners both inside and outside the company, understand the business need, perform the required data analysis, and provide valuable insights\n\u2022 Worked on expense monitoring projects which involved producing complex financial and risk models to evaluate historical and to predict future performance\n\u2022 Provide ad-hoc reports to key business partners across different groups to help drive the business decisions.\n\u2022 Created Project Plan documents, Software Requirement Documents, Environment Configuration and UML diagrams.\n\u2022 Directly worked with project managers, SME and various stakeholders for project budgets, billing hours and schedules.\n\u2022 Created data flow, process documents and ad-hoc reports to derive requirements for existing system enhancements.\n\u2022 Participated in design discussions and assured functional specifications are delivered in all phases of SDLC in an Agile Environment.\n\u2022 Performed User Acceptance Testing, Device and Performance testing, Functional and Regression testing.', u'Data Analyst\nRoadways Inc - Novi, MI\nNovember 2012 to June 2013\nResponsibilities:\n\u2022 Working closely with project manager/Test Lead and Project team to learn about the project functionalities and provided testing timelines.\n\u2022 Experience in Creating Tables, Views, Triggers, Stored Procedures, User Defined Functions and other T- SQL statements for various applications.\n\u2022 Experience in writing SQL queries involving multiple tables inner and outer joins.\n\u2022 Experience in optimizing the queries by creating various clustered, non-clustered indexes and indexed views.\n\u2022 Experience in Creating Tables, Views, Triggers, Stored Procedures, User Defined Functions and other T- SQL statements for various applications.\n\u2022 Experience in Data Extraction, Transforming and Loading (ETL) using SQL Server Integration Services (SSIS).\n\u2022 Experience in creating various type of reports such as Complex drill down & drill through reports, Matrix reports, Sub reports and Charts using SQL Server Reporting Services (SSRS) based on Relational and OLAP databases.\n\u2022 Experience in designing complex reports like reports using Cascading parameters, Drill-Through Reports, Parameterized Reports and Report Models and adhoc reports using SQL Server Reporting Services (SSRS) based on client requirement.\n\u2022 Absolute knowledge of software development life cycle, database design, RDBMS, data warehousing and data modeling concepts.\n\u2022 Demonstrated leadership abilities and team work skills as well as the ability to accomplish tasks under minimal direction and supervision.\n\u2022 Good understanding of Health care, Financial and Retail domains.\n\u2022 Involved in writing Requirement Tractability Matrix (RTM) detailed Test Plan and Test scenarios.\n\u2022 Involved in testing application on different mobile Platform such as Windows, iOS and Android devices.\n\u2022 Constantly communicated and made recommendations to stakeholders at all levels.\n\u2022 Conducted mobile apps testing across the web.\n\u2022 Conducted updating and data configurations for multiple apps released weekly.\n\u2022 Collaborated with all teams within the agile environment.\n\u2022 Identified issues before they became problems and collaborated to fix them.', u""IT Analyst\nFidelity Investments - Boston, MA\nSeptember 2009 to October 2012\nResponsibilities:\n\u2022 Created and maintained T-SQL stored procedures, functions, created database objects like tables, views, triggers. Updated and maintained database data.\n\u2022 Designed new reports and wrote technical documentation, gathered requirements, analyzed data, developed and built SSRS reports and dashboard, supported more than 20 client companies' general business reports and benefit plan administration processes.\n\u2022 Developed different type of reports including: Sales Report, Client Contact Report, Accounting Statement Report, Employee/benefits census report, Benefits statements report, Enrollment count summary report, Billing report and other insurance/payroll/COBRA file transfer reports by using SSRS and SQL Server 2008/2012.\n\u2022 Scheduled and managed daily/weekly/monthly sales and operational reports based on the business requirements.\n\u2022 Was part of Team that designed, created and maintained ETL packages to extract data from different systems, and load variety of large volume data into data warehouse from different sources like flat file, Excel file etc.\n\u2022 Introduce new ideas/innovations to increase productivity.\n\u2022 Install, configure, test and maintain operating systems, application software and system management tools\n\u2022 Maintain security, backup, and redundancy strategies\n\u2022 Write and maintain custom scripts to increase system efficiency and lower the human intervention time on any tasks\n\u2022 Troubleshoot remote user issues including VPN, smart phones, and hardware failures often over long-distance connections and across time zones\n\u2022 Communicate with vendors, as required, for troubleshooting and scheduling maintenance\n\u2022 Documentation of procedures and policies.\n\u2022 Continually updated personal knowledge of computing hardware operating systems and software\n\u2022 Maintained confidentiality and discretion when working with password or sensitive materials.\n\u2022 Maintained company peripheral network devices regularly, including printers and scanners.\n\u2022 Tested the IT networking systems, monitored the performance of network servers and maintained firm's computer systems.\n\u2022 Made use of helpdesk systems to prioritize work-load and update queries and calls.""]","[u""Master's""]",[u'']
0,https://resumes.indeed.com/resume/28c8716f2973f631,"[u'Supply Chain Project Analyst\nJohns Hopkins Health Systems - Baltimore, MD\nOctober 2017 to Present\nA global health enterprise and one of the leading health care systems in the United States\n\nResponsible for designing analytical data and providing actionable insights to the management and leadership team.\n\u2022 Establish and maintain strong working relationship with senior management, becoming a resource for all analytical support\n\u2022 Analyze, identify, and communicate trends and issues impacting procedures on an ongoing basis and make recommendations\n\u2022 Supports internal departments with service, performance, spend data and reporting', u'Program Director\nChanging Turn Community Outreach Services - Baltimore, MD\nJanuary 2017 to July 2017\n501(c) (3) specializing in providing support and resources to children and adults who are at-risk and have mental health issues\n\nEffectively managed the coordination and administration of all aspects of an ongoing program including planning, organizing, staffing,\nleading, and controlling program activities.\n\u2022 Successfully planned and executed the delivery of the overall program and its activities in accordance with the mission and the goals of the organization\n\u2022 Developed funding proposals for the program to ensure the continuous delivery of services\n\u2022 Communicated with staff, volunteers, and external stakeholders to gain community support for the program and to solicit\ninput to improve the program', u'Project Manager\nNielsen - Oldsmar, FL\nSeptember 2013 to December 2016\nLeading global consumer information and measurement company that provides market research, insights and data about what people watch, buy and listen to\n\nPromoted to manage multiple projects that increased productivity and delivered on-time projects that produced $10 million in revenue\n\u2022 Created detailed project plan with well-defined tasks, milestones, client sign-offs and specifications, which served to communicate project progress to management\n\u2022 Created project plans and scope for multiple assignments\n\u2022 Documented and captured timelines, communication plans and scorecards in order to determine project completion.\n\u2022 Employed proactive management style ensuring operational proficiency and timely completion of assigned projects within or under timeframe and budget', u'Data Analyst\nNielsen - Oldsmar, FL\nSeptember 2012 to August 2013\n\u2022 Accounted for creating adhoc reports, spreadsheets, charts, and graphs describing and interpreting analysis results.\n\u2022 Developed and implemented data collection processes, data analytics and other strategies that optimize creation of relevant\ninformation to support decision-making\n\u2022 Identified, analyzed, and interpreted trends or patterns in complex data sets using statistical techniques\n\u2022 Created Excel reports and Power Point presentations to present results in an accurate, professional-looking, concise and complete manner\n\u2022 Determined conclusions/results based on data-analysis and recommend appropriate courses of action / next steps', u'Data Analyst\nCareCentrix - Tampa, FL\nOctober 2011 to August 2012\nA home health-care agency that integrates patients, clinicians, hospitals, health insurance plans and home care providers\n\n\u2022 Trained users and answered database issue questions\n\u2022 Selected and entered codes to monitor database performance and to create production database\n\u2022 Reviewed and made appropriate changes to database management system procedure manuals\n\u2022 Tested programs and databases, correcting errors and making necessary modifications', u'Data Analyst\nArbitron Inc - Columbia, MD\nSeptember 2007 to July 2011\n\u2022 Processed monthly and weekly summary reports using FTP Pro, SQL and the customer dashboard\n\u2022 Queried data from complex relational databases for various analyses and/or requests\n\u2022 Developed and test experimental designs, sampling techniques, and analytical methods\n\u2022 Analyzed, detected, and resolved data or system anomalies in a time-sensitive environment', u'Data Analyst\nAmerican Red Cross - Baltimore, MD\nApril 2006 to August 2007\nA national 501 (c) (3) organization that provides relief and support to those in crisis by helping to be prepared to respond in emergencies\n\n\u2022 Effectively organized documents, and maintained data files using Access and Excel\n\u2022 Analyzed and interpret statistical data to identify significant differences in relationships among sources of information\n\u2022 Observed collection data behavior and forecast the trends in collection sites\n\u2022 Preformed queries and updating databases for publication of monthly and daily reports', u'Data Analyst\nMxEnergy - Annapolis Junction, MD\nSeptember 2001 to August 2005\nA retail natural gas and electricity supplier in North America\n\n\u2022 Processed customer cancellations, enrollments and rate changes via file imports/exports to various companies using Access,\nExcel, and Word\n\u2022 Predictive analysis with data mining by processing large amounts of data for statistical modeling and graphic analysis, using\ncomputers\n\u2022 Converted files in the system utilizing multiple data applications such as FTP, SQL, Access and Excel\n\u2022 Organized and manipulated data and records while maintaining the integrity of the data in customer service database']","[u'MBA in Management', u'Bachelors in Business Administration']","[u'University of Phoenix\nJuly 2015', u'University of Phoenix\nAugust 2013']"
0,https://resumes.indeed.com/resume/7d69cb530008f346,"[u""Data Analyst Intern\nJiangsu Donghua Futures Co., Ltd - Nanjing, CN\nJune 2016 to September 2016\n\u2022 Conducted data pulling, cleaning and validation on daily market data utilizing Excel and SQL.\n\u2022 Utilized machine learning method Random Forest to predict the pattern of Stock Index futures.\n\u2022 Designed the intertemporal arbitrage strategy by applying Ito's Lemma and simulate the strategy on software.\n\u2022 Used linear regression to calculate the portfolio's beta and hedged it with stock index future to get alpha.""]","[u'MS in Statistics', u'Bachelor of Science in Statistics', u'in statistics']","[u'University of Washington Seattle, WA\nSeptember 2017 to May 2019', u'Nanjing University Nanjing, CN\nSeptember 2013 to July 2017', u'University of California Berkeley, CA\nAugust 2015 to May 2016']"
0,https://resumes.indeed.com/resume/e87fbedda5e7a0d5,"[u""Data Analyst\nReliance Jio - Mumbai, Maharashtra\nJuly 2015 to April 2017\nExtracted, manipulated, cleaned and processed network health data using SQL for R-Jio's SAP MM auto work order generation operations\n\u25cf Performed forecasting using Machine Learning models on network health based on historical data with R & Python to predict number of faults compared to the average environmental temperature with 70% accuracy hence helped to decrease average outages by 60%\n\u25cf Used Advanced MS Excel Pivot table, Vlookup, Hlookup and advanced formulas to manipulate raw data for weekly dashboard preparation\n\u25cf Analyzed network links for high bandwidth utilization and helped to identify and upgrade 20,000 choked links which resulted in improvement of latency by 30%\n\u25cf Leveraged Tableau for making dashboards and reporting network data in weekly dashboards to evaluate IP MPLS (platform domain) operational performance\n\u25cf Developed Python script to identify dead nodes directly from the service manager which helped in saving 3,500 manhours for field technicians\n\u25cf Supervised root cause analysis on catastrophic network outages, and helped to devise solutions for their mitigation\n\u25cf Managed daily scrum meetings with the development team to develop an Android application which network engineers used to identify and input fault data\n\u25cf Assessed fault analysis on major network outages due to various domain issues and helped to resolve as many as 2,000 faults in 40 days"", u'Associate Business Analyst\nReliance Jio - Mumbai, Maharashtra\nJuly 2014 to June 2015\nManaged spare availability across network by distributing spares as per the design basis & fault count\n\u25cf Coordinated with vendor team of Cisco and Juniper for resolution of IP MPLS software and hardware TAC Issues\n\u25cf Configured 100+ access level Cisco routers with HP NNMi for network operation\n\u25cf Documented and implemented best practices for router replacement activity used by field technicians which resulted in 40% less outages during troubleshooting\n\u25cf Supported for trouble ticket creation, updating and solution on HPSM tool\n\u25cf Audited routers and switches located in network core facilities for operational acceptance']",[u'Master of Science in Information Systems in Information Systems'],"[u'University of Cincinnati, Carl H. Lindner College of Business Cincinnati, OH\nApril 2018']"
0,https://resumes.indeed.com/resume/0f9982ed5b81adc6,"[u'Role - Sr. Data Analyst\nClient - MetLife - New York, NY\nJanuary 2016 to Present\nResponsibilities:\n\u2022 Modernizing operations by replacing Legacy Master Data Collection systems with new Oracle Master Data Management System.\n\u2022 Extensively involved in the modeling and development of Reporting Data Warehousing System.\n\u2022 As a Sr. Data Analyst worked closely with the Enterprise Data Warehouse team and Business Intelligence Architecture team to understand repository objects that support the business requirement and process.\n\u2022 Develop dashboards and visualizations using Tableau and SQL Server.\n\u2022 Developed dashboards, reports, templates, and visualizations using Microstrategy and Tableau.\n\u2022 Prototyped data visualizations using Charts, drill-down, parameterized controls using Tableau to highlight the value of analytics in Executive decision support control.\n\u2022 Developed and executed reports using combination of software applications, including Crystal Reports, IBM Rational ClearQuest, MS SQL Server Business Intelligence Development Studio.\n\u2022 Clean and manipulate complex healthcare datasets in order to create the data foundation for further analytics and the development of key insights (MSSQL server, R, Tableau, Excel)\n\u2022 Generated individual customer Reports using sales data of quantity, sales price, standard cost, allocation budget in dollars and standard margin using VBA and Macros in MS Excel.\n\u2022 Data Analysis Primarily Identifying Data Sets, Source Data, Source Meta Data, Data Definitions and Data Formats.\n\u2022 Manipulated data from website using Vlookups, match from MS Excel to obtain accurate data feed for visualization.\n\u2022 Collected and studied the Metadata about the master data for MDM and the transactional data.\n\u2022 Modelling the data relationship to provide assistance for business decision making (R, Tableau).\n\u2022 Developing SSIS packages(DTSX) to get data from various sources like new data feed files, Oracle server, SQL Server and other sources as per the FRD and load them into Staging database involving data analysis, data quality, data conversion, data migration, data validation, error handling, testing and security.\n\u2022 Assure correct information is maintained in EPIC and any applicable downstream systems by notifying all required departments of merges and corrections for correction in other department systems\n\u2022 Performed Logical & Physical Data Modeling and delivering Normalized, De-Normalized & Dimensional schemas.\n\u2022 Used Informatica / SSIS to extract, transform & load data from SQL Server to Oracle databases.\n\u2022 Reporting and dashboarding analytical results to client (R, Tableau, Excel).\n\u2022 Designed and developed the data dictionary and Meta data of the models and maintain them.\n\u2022 Provided analysis of enterprise data warehouse and business intelligence systems, including source data systems, to identify and document complex data mapping, business rule analysis, functional and non-functional requirements Created ad hoc queries, reports and analyses as requested.\n\u2022 Managed all data bases, data mining and reporting. Contributed to design of Six Sigma program infrastructure.\n\u2022 Used forward engineering to create a physical data model with DDL that best suits the requirements from the logical data model using Erwin.\n\u2022 Worked with several marketing managers and project manager to gather financial data and requirements and build reports using Teradata SQL.\n\u2022 Created new macros and implemented them in SAS programs to produce the reports.\n\u2022 Converted ORACLE data tables into SAS data files using SAS SQL Pass through Facility, and uploaded SAS data files into ORACLE tables using SAS Dbload procedure.\n\u2022 Involved in end-to-end ETL process design using various Informatica tools - Source Analyzer, Mapping, Designer, Transformation Designer, Workflow Manager, Workflow Monitor. Designed 60+ mappings and workflows\n\u2022 Worked on Tableau for Data Analysis, Digging the data for source systems for analysis and deeply dive in the data for Predictive findings and for various data Analysis by using dash boards and visualization.\n\u2022 Created SAS dataset from tables in Database using SAS/Access and Retrieved the Sales data from flat files, oracle database and converted to SAS data sets for Analysis using SAS/STAT procedures.\n\u2022 Expert using Business Intelligence tools - Microsoft SSRS, SSIS and SSAS, Visual Studio, Informatica Power\n\u2022 Reviewed requirements together with QA Manager, ETL leads to enhancing the data warehouse for the originations systems and servicing systems.\n\u2022 Performed Data mapping between source systems to Target systems, logical data modeling, created class diagrams and ER diagrams and used SQL queries to filter data\n\u2022 Implemented Unified Modelling Language (UML) methodologies to design Context Diagram, Use Case Diagrams, Data Flow Diagrams, Activity Diagrams, Sequence Diagram and using Erwin.\n\u2022 Design discussions and meetings with the data architects to come out with the appropriate design for the data warehouse.', u'Role - Data Analyst\nClient - New York Life Insurance Company - New York, NY\nApril 2014 to December 2015\nResponsibilities:\n\u2022 Involved in developing and testing the SQL Scripts for report development, Tableau reports, Dashboards and handled the performance issues effectively.\n\u2022 Developed POCs by building reports and dashboards using Tableau to perform.\n\u2022 Involved in project cycle plan for the data warehouse, source data analysis, data extraction process, transformation and ETL loading strategy designing.\n\u2022 Analyzed the Business Logic and implemented the Conceptual model, Physical Data Model using ERWIN.\n\u2022 Created adhoc reports to users in Tableau by connecting various data sources.\n\u2022 Dimensional Data Modeling experience using Data modeling, Erwin Modeling (Erwin 4.5/4.0, Oracle Designer) Star Join Schema/Snowflake Modeling, FACT & Dimensions tables.\n\u2022 Generated tableau dashboards with combination charts for clear understanding.\n\u2022 Supporting upstream data source system upgrade to ensure the data quality in Enterprise Data Warehouse.\n\u2022 Worked on data cleansing using the cleanse functions in Informatica.\n\u2022 Responsible to create Meta data driven data model, development and documentation of Archival process for application database schemas.\n\u2022 Used data mining techniques to analyse customer trends and buying patterns\n\u2022 Builds/enhances data models and database architecture of MDM.\n\u2022 Performed Data analysis, statistical analysis, generated reports, listings and graphs using SAS Tools SAS/Base, SAS/Macros and SAS/Graph, SAS/SQL, SAS/Connect, SAS/Access.\n\u2022 Validated mappings using Informatica debugger to gain troubleshooting information about data and error issues. Performed programming of data listings using SAS. Converted MS-Word documents and Excel tables into SAS data sets.\n\u2022 Used VLOOKUP, countif, date & time functions and pivot tables also created macros using VBA.\n\u2022 Designed and lead the development of production SQL and database changes as well as assistance with the design of efficient database and storage structures\n\u2022 Performed Logical & Physical Data Modelling and delivering Normalized, De-Normalized & Dimensional schemas.\n\u2022 Analyzed source data sets to capture Meta Data, Data Formats and Data Types. Also, responsible for creating, documenting and maintaining the metadata.\n\u2022 Created Use Case Diagrams, Activity Diagrams, Sequence Diagrams and ER Diagrams in MS Visio.\n\u2022 Created functional and non-functional requirement for data movement from all source systems into MDM.', u""Role - Data Analyst\nClient -Northwestern Mutual - Milwaukee, WI\nJuly 2012 to March 2014\nResponsibilities:\n\u2022 Retrieved data from various sources Excel, databases and conducted data integration, analysis including data cleaning and linear regression and performed data profiling and data quality.\n\u2022 Used RUP-iterative process to Conduct Data Analysis to find missing data fields in the application and customize them and extensively used Rational Requisite Pro.\n\u2022 Managed, updated and manipulated report orientation and structures with use of Pivot tables and V-lookups.\n\u2022 Performed Data Analysis and Data validation by writing basic queries against the database.\n\u2022 Performed Detail Data Analysis (DDA), Data Quality Analysis (DQA) and Data Profiling on source objects.\n\u2022 Generated Excel reports for data validation and data reconciliation using V-LOOKUP'S.\n\u2022 Developed Tableau workbooks from multiple data sources using data blending to develop chart, graphs, tables, and provide security using filtering on unwanted data.\n\u2022 Worked on loading data from flat files to Teradata tables using SAS Proc Import and Fast Load Techniques.\n\u2022 As a Data Analyst performed the following -Data collection, data transformation and data loading the data using ETL systems like Informatica Power Center.\n\u2022 Involved in data structuring and data modeling of databases, creating tables, reports and maintaining them in MYSQL.\n\u2022 Identified and created the existing and expected business flows of the master data usage.\n\u2022 Research and development of monetization strategies using data analytics and ad space\n\u2022 Identified owners of the master data and appointed data stewards for the MDM system.\n\u2022 Basic knowledge on ETL tools like Informatica & Data Stage and Reporting tools like Cognos, BO and Tableau.\n\u2022 Developed routine SAS macros to create tables, graphs and listings.\n\u2022 Collected and studied the Meta data about the master data for MDM and the transactional data.\n\u2022 Gathered issues about data-quality and other requirements pertaining to the project with the help of various forums viz. interviews, JAD, brainstorming sessions etc.\n\u2022 Prepared the requirements and use case traceability matrix.\n\u2022 Mapped data for the ETL (Extract, transfer and load) process and wrote detailed specification for the ETL and Reports."", u""Role - Data Analyst\nClient - National Life Group - New York, NY\nAugust 2010 to June 2012\nResponsibilities:\n\u2022 Utilized MS Access and SQL queries to create an interface that merged several tables data to compile ready load template for IT team, participated JAD sessions with SME.\n\u2022 Completed workflow analysis for ambulatory, inpatient and Op-Time for implementation of the hospital's epic software.\n\u2022 Clean and merge data from multiple business units to be used on Tableau reports.\n\u2022 Developing the Data Mart for the base data in Star Schema, Snow-Flake Schema involved in developing the data warehouse for the database.\n\u2022 Extensively used ETL/Data warehouse methodology for supporting data extraction, transformations and loading processing, in a corporate-wide-ETL Solution using ETL tools like Tableau.\n\u2022 Analysed data tables to validate information and load criteria utilizing pivot tables, VLOOKUPs, and other complex MS Excel formulas\n\u2022 Performed Logical & Physical Data Modelling - Relational Modelling & Dimensional Modelling.\n\u2022 Run SQL queries to assist with Greenfield Project\n\u2022 Facilitated meetings to gather, analyse, and baseline detailed business requirements for new and current processes.\n\u2022 Conducted design review of data models, cube designs & ETL architecture.\n\u2022 Worked on Talend to load Data from various sources like to Oracle DB\n\u2022 Created ETL test data and tested all ETL mapping rules to the functionality of the Informatica mapping and Ab Initio graphs.\n\u2022 Involved in implementation of UNIX shell scripts to copy files among different servers and to invoke PL/SQL scripts, DataStage jobs.""]","[u""Bachelor's""]",[u'']
0,https://resumes.indeed.com/resume/5ccf365992a81d39,"[u""Data Analyst\nWells Fargo - Jersey City, NJ\nMarch 2015 to Present\nResponsibilities:\n\u27a2 Meet with customers to determine their needs, gather and document requirements, communicate with customers throughout the project to manage customer expectations, resolve issues, and provide project status.\n\u27a2 Involve in requirement gatherings with Business team for building new Tableau reports and migrating existing BOBJ report to Tableau.\n\u27a2 Create Allowance Limit Reports, Customer Loyalty, Periodical Volume Reports, Brand awareness and other reports for measuring the KPIs.\n\u27a2 Converting existing BO reports to Tableau dashboards.\n\u27a2 Develop Tableau data visualizations using Cross tabs, Heat maps, Box and Whisker charts, Scatter Plots, Geographic Map, Pie Charts, Bar Charts and Density Chart.\n\u27a2 Create complex workbooks and Dashboards by connecting to multiple data sources using data blending and Joins.\n\u27a2 Creation of attributes, filters, advanced chart types, visualizations and complex calculations to manipulate the data and to analyze, obtain insights into large data sets.\n\u27a2 Develop SQLS and extract right data for analysis and build confidence to business that requirements are accurate and are not ambiguous.\n\u27a2 Interpret allowance data residing in various data sources like DB2, Teradata, SQL Server tables and analyze results using statistical techniques and provide ongoing reports.\n\u27a2 Identify, analyze, and interpret trends or patterns in complex Walmart sales', receiving's and allowance data by doing data decomposition.\n\u27a2 Involved in Data Profiling and creation of Data Mapping documents.\n\u27a2 Participate in tech meetings between Technical Architects and Business teams.\n\u27a2 Documenting various artifacts like Requirements, Analysis and Design documents in SharePoint\n\nEnvironment: HP ALM 11.5, Informatica 9.6, Informatica DVO, SQL, PL/SQL, QTP, TOAD, Oracle 11g, Agile/Scrum, MS Office, Business Objects XIR4, XML, HTML, Windows and UNIX, Shell Scripting, Team Foundation server (TFS)."", u""Data Analyst\nEscorts group - IN\nJuly 2014 to December 2015\nResponsibilities:\n\u27a2 Planned, coordinated, and monitored project levels of performance and activities to ensure project completion in time.\n\u27a2 Conducted deep data validation and root cause analysis to ensure high data integrity.\n\u27a2 Attended regular post-shop debrief meetings with shopping services to review results and improve performance.\n\u27a2 Conduct complex adhoc analytics, summaries and recommendations, as required.\n\u27a2 Writing simple Python programming as per the business requirement assisting with data analysis.\n\u27a2 Managed significant volumes of retail transaction activity for the Pricing team, partnering with pricing analysts to ensure timely and accurate retail uploads to stores.\n\u27a2 [['Ensured Pricing audit compliance by organizing and tracking key files/information within systems utilities such as Python, MLOAD, BTEQ and Fast Load.\n\u27a2 Worked in a Scrum Agile process & in Writing Stories with two week iterations delivering product for each iteration.\n\u27a2 Created the dimensional logical model with approximately 10 facts, 30 dimensions with 500 attributes using ER Studio.\n\u27a2 Implemented the Slowly changing dimension scheme (Type III) for most of the dimensions.\n\u27a2 Implemented the standard naming conventions for the fact and dimension entities and attributes of Logical and physical model.\n\u27a2 Reviewed the logical model with Business users, ETL Team, DBA's and testing team to provide Information about the data model and business requirements.\n\u27a2 Extensively worked in Oracle SQL, PL/SQL, SQL*Loader, Query performance tuning, created DDL scripts, created database objects like Tables, Views Indexes, Synonyms and Sequences.\n\u27a2 Worked on transferring the data files to vendor through SFTP & FTP process.\n\u27a2 Involved in defining and constructing the customer to customer relationships based on association to an account & customer.\n\u27a2 Assisted in development of client centric Master Data Management (MDM) solution.\n\nEnvironment: ER Studio 9.7, Tableau 8.2, AWS, Teradata, MDM, GIT, UNIX, Python 3.5.2, SAS, regression, logistic regression, NoSQL, Teradata, OLTP, random forest, OLAP, ODS, NLTK, SVM, JSON, XML."", u'Data Analyst\nCabot Corporation\nFebruary 2013 to June 2014\nResponsibilities:\n\u27a2 As an Architect, design conceptual, logical and physical models using Erwin and build data marts using hybrid Inmon and Kimball Data Warehouse methodologies.\n\u27a2 Interaction with Business Analyst, SMEs and other Data Architects to understand Business needs and functionality for various project solutions.\n\u27a2 Researched, evaluated, architected, and deployed new tools, frameworks, and patterns to build sustainable Big Data platforms for the clients.\n\u27a2 Identifying and executing process improvements, hands-on in various technologies such as Oracle, Informatica, and Business Objects.\n\u27a2 Worked closely with business, data governance, SMEs and vendors to define data requirements.\n\u27a2 Worked with data investigation, discovery and mapping tools to scan every single data record from many sources.\n\u27a2 Designed the prototype of the Data mart and documented possible outcomes from it for end-user.\n\u27a2 Involved in business process modeling using UML.\n\u27a2 Developed and maintained data dictionary to create metadata reports for technical and business purpose.\n\u27a2 Created SQL tables with referential integrity and developed queries using SQL, SQL PLUS and PL/SQL.\n\u27a2 Experience in maintaining database architecture and metadata that support the Enterprise Data warehouse.\n\u27a2 Developed various Tableau Data Reports by extracting and using the data from various source files, DB2, Excel, Flat Files and Big data.\n\u27a2 Designed both 3NF data models for ODS, OLTP systems and dimensional data models using Star and Snowflake Schemas.\n\u27a2 Designing, coding, unit testing of ETL package source marts and subject marts using Informatica ETL processes for Oracle database.\n\nEnvironment: Erwin r9.0, Informatica 9.0, ODS, OLTP, Oracle 10g, OLAP, DB2, Metadata, MS Excel, Mainframes, MS Visio, Tableau, Requisite Pro, PL/SQL.']","[u'', u""Bachelor's degree in Computer Science in Computer Science""]","[u'Saint Peters University', u'Jawaharlal Nehru Technological University']"
0,https://resumes.indeed.com/resume/75b568cc53110c7d,"[u""Business Analyst/Data Analyst\nPNC Financial Services - Pittsburgh, PA\nJanuary 2017 to Present\nProject 1\nThe project involves supporting the Capital Markets and Fixed Income team within Enterprise Risk Management, in building a platform that would integrate the data from different sources, aggregate it and make it available to be consumed by various models. The platform (CCAR central Database) would relay the aggregated data, for creating the CCAR (regulatory reporting standard) results for reporting to the Fed Reserve (XML format), to the Reporting Layer (Tableau based platform)\n\n\u2022 Lead the technology team in gathering and analyzing the business requirements (BRD) based on the guidelines of Dodd Frank Act and CCAR (regulatory reporting standard). Responsible for defining the scope and creation of lineage documents for CCAR Central database for FRY-14A submissions.\n\u2022 Involved with mapping and applying notional calculations (based on Macro Economics and Capital growth rates, Operational Risks, Retail/Mortgage Repurchase and Credit Losses) to the underlying system.\n\u2022 Manage individual work-set project plans and gathering functional requirements by following a mixture of Waterfall and Agile SDLCs.\n\u2022 Create data flow / process flow diagrams using MS Visio for requirements and reporting purposes.\n\u2022 Created the Functional Requirements (FRD) to enable the developers to setup the system per the user's need.\n\u2022 Responsible for data profiling of the source systems required to input data to the CCAR Central Database using various SQL queries for the SQL Databases (Oracle/SSIS). Responsible for ongoing development of key data quality and data profiling processes required for CCAR and other Risk/Regulatory (Compliance) initiatives using the SQL queries\n\u2022 Managed validation/testing criteria to ensure the data is consistent to the overall defined data related requirements.\n\u2022 Validated data between the Oracle databases excel inputs (source) and Tableau reports to ensure computation accuracy.\n\u2022 Responsible for training the users in the User Interface and the features/functionalities of the User Interface (UI).\n\nTools: Microsoft Office (Word, Outlook, Excel, Visio), Oracle PL/SQL, SSMS/SSIS, Tableau, Vena, OLAP, VBA Macro"", u""Business Analyst/Data Analyst\nPittsburgh, PA\nApril 2015 to January 2017\nProject 2\nFocused in area of 14M schedules of CCAR understanding the business requirements in building schedules in Vena based on the requirements drafted by the business users.\n\n\u2022 Created the Functional Requirements (FRD) to enable the developers to setup the system per the user's need.\n\u2022 Performed for data profiling of the source systems using various SQL queries for the SQL Databases\n\u2022 Worked with ongoing development of data quality and data profiling processes required for CCAR and other Risk/Regulatory (Compliance) initiatives using the SQL queries\n\u2022 Managed validation/testing criteria to ensure the data is consistent to the overall defined data related requirements.\n\u2022 Validated data between the various databases, excel inputs (source) and Tableau reports to ensure accuracy.\n\u2022 Responsible for training the users in UAT and explain the features/functionalities of the User Interface (UI).\n\nTools: Microsoft Office (Word, Outlook, Excel, Visio), Oracle PL/SQL, SSMS/SSIS, Tableau, Vena, OLAP, VBA Macro"", u""Business Analyst/Data Analyst\nWells Fargo - Chicago, IL\nAugust 2013 to March 2015\nThe project was based on creating a user interface application for user convenience to process 14M schedules of CCAR related data. The project was to develop and enhance the UI to give the users the ability to view their respective data processes, carryout data transmission and view the progress of their data transmission process status during the submission. Implemented the Waterfall methodology.\n\n\u2022 Responsible for scribing requirements from sessions held for requirements gathering from clients and SMEs. Worked in support with the Asset and Liabilities Management Group, Quantitative Risk Management Group and Credit Loss teams.\n\u2022 Finalized and documented the business requirements and Functional requirements based on the requirements of the clients, documenting the need of the alerts and processes built in the User Interface.\n\u2022 Created Use Cases, Business Workflow diagrams, and assisted the development team in creation of the System Requirements Specification Document.\n\u2022 Responsible for creating the MS Visio diagrams describing the architecture and design of the system. Also created and reviewed the wireframes and screen mock-ups with the users. Assisted the Tableau developers on building the real-time individual process status workflow diagrams in Tableau\n\u2022 Assisted in creation of test deliverables like the test plans and test cases and assisted the Quality Assurance team in testing of the product.\n\u2022 Involved in the User Acceptance Testing phase and communicated the user's suggestions/concerns to the development team.\n\nTools: Microsoft Office (Word, Outlook, Excel, PowerPoint), Oracle PL/SQL, Tableau"", u'Business Analyst/Data Analyst\nMB Financials - Chicago, IL\nDecember 2012 to July 2013\n\u2022 Took part in the commodities trading challenge organized by Chicago Mercantile Exchange (CME).\n\u2022 Managed the portfolio including commodity derivatives for natural gas, gold, oil, e-mini S&P 500.\n\u2022 Finished in the top 35% of all the teams with a return of (7.27%).\n\nValuation of AT&T (Securities Valuation)\n\u2022 Valued (financial forecasting) and analyzed AT&T including future prediction using the Dividend Discount model and Free-Cash Flow models. Built final reports using Pivot tables to include various charts describing the financial status of the company.\n\u2022 Recommended a buy based on the valuation and used porter analysis to compare with industry. The prediction of $34.75 to $39.59.\n\nAlgorithmic Trading:\n\u2022 Used the Tango trading platform provided by the company Real-Time Systems and wrote a code for creating a spread trading based on the gains in the spread. Implemented the algorithm on the securities derivatives structures using moving averages.\n\u2022 Calculated the returns by implementing it on real-time basis and achieved returns over 10%.', u""Business Analyst\nMicrocon - Mumbai, Maharashtra\nApril 2011 to May 2012\nThe project was designing and creating a prototype of the Building Management System (BMS) is a computer-based control system installed in buildings that controls and monitors the building's mechanical and electrical equipment such as ventilation, lighting, power systems, fire systems, and security systems\n\n\u2022 Involved in development of BMS (Building Management Systems), supporting the team with the requirements, updating and modifying the requirements documents\n\u2022 Responsible for managing the testing for the system, including the creation of test documents and supporting the UATs. Involved in testing the data storage using Oracle and Microsoft Excel.\n\nTools: Microsoft Office (Word, Outlook, Excel, Visio), Oracle PL/SQL""]",[u'Masters in Science in Finance'],"[u'Stuart School of Business, Illinois Institute of Technology Chicago, IL\nJuly 2012 to May 2014']"
0,https://resumes.indeed.com/resume/9c683d9bc87a43b9,"[u'Data Analyst\nWalgreens, Northbrook Illinois - Northbrook, IL\nJune 2016 to November 2017\n\u2022 Constructed a new data validation process for the Merchandising team. Analyzing ~300K items in 300+ stores weekly.\n\u2022 Responsible for handling Mastersheet for Walgreens. Analyzing large volume of stores daily. Reduced error rate by 15%\n\u2022 Extracted data from different sources like Oracle and text files using SAS/Access, SAS SQL procedures and created SAS datasets.\n\u2022 Performing data validation, transforming data from RDBMS oracle to SAS datasets.\n\u2022 Used SAS EG, Macros to develop dynamic SAS programs to extract, validate data for Statistical analysis.\n\u2022 Developed SAS Macro programs to create ad-hoc reports.\n\u2022 Extensively used the SET, UPDATE and MERGE statements for creating, updating and merging various SAS data sets\n\u2022 Worked in Tableau environment to create dashboards like Yearly, monthly reports using tableau desktop and publish them to server.\n\u2022 Converted Excel Reports to Tableau Dashboard with High Visualization and Good Flexibility.\n\u2022 Used Trend Lines, Reference Lines and statistical techniques to describe data.\n\u2022 Created various views in Tableau-like Tree-maps, Heat Maps, Scatter plots, Radical chart Geographic maps, Line chart, Pie charts etc.\n\u2022 Created the calculated fields, navigation techniques, Level-of-Detail (LOD) expressions, and parameters to get the dynamic data visualization chats as per the requirement.\n\u2022 Reported on process execution and any uncovered issues to managers for further follow-up.\n\u2022 Managed 7 projects. Found and analyzed errors and communicated among departments.\n\u2022 Responsible for implementation and maintenance of subscriptions of 200k plus Departments of 2000 Walgreens Stores.\n\u2022 Built templates for the team to automate many steps in process also built validation process that involves the complex data structure within an aggressive timeline.\n\u2022 Trained new Managers and Associates for whole project life cycle process of subscription side Data integration, Data Extraction and Validation Process.\n\u2022 Introduced Tableau to my team to produce appealing data visualizations to clients and to help them make effective business decisions\n\u2022 Provided ideas for enhancement for in-house application and part of UAT team.\n\u2022 Gathered business requirements, interacted with the end users, came up with all the possible test case scenarios also helped in building Functional Requirement Document.\n\u2022 Performed testing at SIT (System Integration Testing) level and UAT (User Acceptance testing) level.\n\u2022 Documented the acceptance criteria for each of the test cases. Built the test cases based on test scenarios.\n\u2022 Reported status on test execution including risks/issues and targets. Updated latest information in regular testing status meetings with all involved constituencies to ensure smooth test execution and timely issue resolution.\nEnvironment: Tableau Desktop 10/9/8, Tableau Server, MS-Excel, Golden, SQL Server, Oracle 11g/10g, BDM (in-house application), SAS 9.3 (BASE, MACROS, CONNECT, SQL, GRAPH)', u'Data Analyst\nGS Marines - Mumbai, Maharashtra\nAugust 2013 to May 2014\n\u2022 Used SAS Proc SQL pass through facility to connect to Oracle tables and created SAS datasets using various SQL joins such as left join, right join, inner join and full join.\n\u2022 Performing data validation, transforming data from RDBMS oracle to SAS datasets.\n\u2022 Produce quality customized reports by using PROC TABULATE, PROC RPORT Styles, and ODS RTF and provide descriptive statistics using PROC MEANS, PROC FREQ, and PROC UNIVARIATE.\n\u2022 Developed SAS macros for data cleaning, reporting and to support routing processing.\n\u2022 Performed advanced querying using SAS Enterprise Guide, calculating computed columns, using filter, manipulate and prepare data for Reporting, Graphing, and Summarization, statistical analysis, finally generating SAS datasets.\n\u2022 Involved in Developing, Debugging, and validating the project-specific SAS programs to generate derived SAS datasets, summary tables, and data listings according to study documents.\n\u2022 Created datasets as per the approved specification collaborated with project teams to complete scientific reports and review reports to ensure accuracy and clarity.\n\u2022 Experienced in working with data modelers to translate business rules/requirements into conceptual/logical dimensional models and worked with complex de-normalized and normalized data models\n\u2022 Performed different calculations like Quick table calculations, Date Calculations, Aggregate Calculations, String and Number Calculations.\n\u2022 Good expertise in building dashboards and stories based on the available data points.\n\u2022 Created action filters, user filters, parameters and calculated sets for preparing dashboards and worksheets in Tableau.\n\u2022 Expertise in Agile Scrum Methodology to implement project life cycles of reports design and development\n\u2022 Combined Tableau visualizations into Interactive Dashboards using filter actions, highlight actions etc., and published them to the web.\n\u2022 Gathering business requirements, creating business requirement documents (BRD /FRD).\n\u2022 Work closely with business leaders and users to define and design the data sources requirements and data access Code, test, identify, implement and document technical solutions utilizing JavaScript, PHP & MySQL.\n\u2022 Created Rich dashboards using Tableau Dashboard and prepared user stories to create compelling dashboards to deliver actionable insights\n\u2022 Working with manager to prioritize requirements and preparing reports on weekly and monthly basis.\nEnvironment: SQL Server, Oracle 11g/10g, MS Office Suite, PowerPivot, Power Point, SAS Base, SAS Enterprise Guide, SAS/MACRO, SAS/SQL, SAS/ODS, SQL, PL/SQL, Visio', u""Data Analyst Intern\nCredenTek Software and Consultancy - Pune, Maharashtra\nJuly 2012 to June 2013\n\u2022 Developed an Biometric authentication system for verification of handwritten signatures.\n\u2022 Documented System Proposal involving system request, feasibility analysis and requirements definition.\n\u2022 Performed Analysis Modelling using Use-case diagrams and descriptions, Class Diagrams and Sequence Diagrams.\n\u2022 Performed Design Modelling using Package Diagrams, Database design and Data Access and Manipulation Design.\n\u2022 Responsible for coordination of activities and information.\n\u2022 Developed test cases, established traceability between requirements and test cases.\n\u2022 Identified key performance indicators (KPIs) and created report.\n\u2022 Obtained sample data (signature) from the user and stored it in the form of points in Oracle database.\n\u2022 Performed data pre-processing, after data area cropping identified the relevant data points to be stored.\n\u2022 Extracted data from different sources like Oracle and text files using SAS/Access, SAS SQL procedures and created SAS datasets.\n\u2022 Identified critical features in the data like pressure, velocity, angle and stores in the database.\n\u2022 Used SAS Proc SQL pass through facility to connect to Oracle tables and created SAS datasets using various SQL joins such as left join, right join, inner join and full join.\n\u2022 Performed pattern matching using KPI's\n\u2022 Extensively use SAS procedures like means, frequency and other statistical calculations for Data validation.\n\u2022 Performed Hypothesis testing using SAS to check if the difference in the population mean is significant.\n\u2022 Utilized concepts related to, statistical analysis, multicollinearity, Correlation, ANOVA.\n\u2022 Gathered data from multiple sources and interpreted the data to draw conclusions for managerial actions and strategy\n\u2022 Embed SQL queries in Excel and used Excel functions to calculate parameters like standard deviation,\nangle, velocity which are used to compare new signatures with the stored ones.\n\u2022 Generated customized reports using SAS/MACRO facility, PROC REPORT, PROC TABULATE and PROC SQL\n\u2022 Created and delivered reports and presentations with key findings and recommendations.""]","[u'Master of Science in Information System', u'Bachelor of Engineering in Computer Science']","[u'University of Texas at Arlington Arlington, TX', u'Institute of Computer Technology Pune, Maharashtra']"
0,https://resumes.indeed.com/resume/b36c32055e73862e,"[u'Data Science Analyst\nVigilanz Corporation\nJanuary 2017 to Present\nDeveloped a classification algorithm to predict the onset of severe sepsis(Organ Dysfunction) using data from electronic medical\nrecord and bedside monitoring system which helped in reducing the mortality rate from 35% to 18% within sepsis patients\n\u25cf Designed and built a machine learning model to reduce the hospital 30 day-readmission rate which is currently implemented in 20\ndifferent client sites and helped reduce readmission rate by 35% and increased revenue by 200K dollars during 1st Quarter 2018\n\u25cf Developed an ensemble classification model to predict the adverse drug events and integrated it into current workflow of the client\nwhich reduced the number of adverse kidney Injury by 42%\n\u25cf Currently working on developing an algorithm to detect outbreak in hospital\n\u25cf Built a suite of enterprise level dashboards to showcase compliance management, Alert Monitoring and return of investments on using\ndifferent Vigilanz products in clients landscape for different level of management from Unit Supervisor to C-Suite head, this project\nhelped Vigilanz in retaining their customers', u""Data Analyst Intern\nMedtronic Inc - Boulder, CO\nMay 2016 to December 2016\nCreated dynamic dashboards with drill down capabilities using Tableau allowing different Sales level(VP/AVP/RM/Sellers) to see all\ninformation\n\u25cf Extracted data from different data sources and created calculated fields, quick filters and custom SQL code to achieve requirement\n\u25cf Created view's by joining multiple tables having thousands of records to get the required data in the proper format into dashboard.\nManipulated, merged and create restructured analytical data sets from large-scale database"", u'Data Analyst (USF Academic Project)\nVerizon Telecommunication - Tampa, FL\nFebruary 2016 to May 2016\nLead a proof of concept project that dynamically analyses millions of records of network logs and predicts the network issues\n\u25cf Developed an interactive dashboard, providing information on some key parameters from the log files that can help determine the root cause of the issue much faster than the conventional manual interpretation\n\u25cf Analysed and developed patterns and correlations between unstructured data using regular expressions and text mining', u'Business Technical Analyst\nCognizant Technology Solutions - IN\nDecember 2014 to May 2015\nDeveloped graphical user interfaces that visualize analytical insights in an easily understandable manner, presented the insights to business experts at higher management level.Worked with business stakeholders to model the business requirements, design the\nintegration logic and implement solution.\n\u25cf Developed complex mapping algorithms for parsing and transformation of data between integrated systems', u'Application Developer/Data Analyst\nIBM India Private Limited\nJuly 2012 to December 2014\nAnalysed trends to recommend process and operational improvements in the existing functionality using Excel and Tableau\n\u25cf Applied text mining analysis using CORPUS library to find the relation between cause and error on a huge error log data\n\u25cf Developed business critical interfaces for exchanging the transactional data using Standard and custom JAVA Adapters Designed UI\npages using HTML, Javascript, CSS, and JQUERY\n\u25cf Worked on integration of Clients SAP systems and partner\'s legacy systems & was awarded with ""Excellence Spark -2014""']","[u'Master of Science in Business Analytics/Data Science', u'Bachelor of Technology in Information Technology']","[u'College of Business, University of South Florida, Tampa Tampa, FL\nDecember 2016', u'Amrita University\nMay 2012']"
0,https://resumes.indeed.com/resume/1b42619396d942b4,"[u'Data Scientist\nFidelity National Financial - Orange, CA\nApril 2017 to February 2018\nPerformed tasks that enable learning the semantics of text and image data on scanned legal documents using natural language processing and computer vision techniques.\n\nSolved difficult, non-routine analysis problems, applying advanced analytical methods as needed using artificial intelligence, deep learning, and machine learning.\n\u2022 Barcode detection, document zone identification, and optical character recognition using Tesseract on each identified zone on document-style images.\n\u2022 Document similarity indexing using scanned document images, text recognized from scanned images, intersection-over-union statistics of identified zones on each document, and locality-sensitive hashing.\n\u2022 Dictionary-based bad OCR document rejection.\n\u2022 Document classification using term frequency-inverse document frequency statistics, dictionary count-vectorization, truncated singular value decomposition, and neural networks.\n\u2022 Variable extraction using labeled strided word-gram data preparation, word vector embeddings, and neural networks.\n\u2022 Federal Tax Lien identification using convolutional neural networks.\n\nEvaluated enterprise hardware, managed cloud assets, and administered servers in a multi-user environment.\n\u2022 HP DL380 G9 and Asus ESC8000 G3 server hardware evaluation and setup. Ubuntu Server 16.04 installation and administration through SSH and HP iLO.\n\u2022 VPC network configuration on Google Cloud Platform in a multi-user environment for Jupyter and Tensorboard.\n\u2022 Installation and administration of Ubuntu Server 16.04 virtual instances on Google Cloud Platform.\n\u2022 Secure mount of remote file system shares through FUSE protocol.', u'Junior Data Analyst\nMyriad Consulting - Redmond, WA\nJuly 2016 to April 2017\nWrote stored procedures, views, functions, ad-hoc queries to support analytical dashboards and reports, managed data quality to ensure consistency and accuracy, data manipulation and extraction. Identified, analyzed, and interpreted trends and patterns, and backed up and restored databases as needed.', u'Intern, Pactera\nVanceInfo - Seattle, WA\nMay 2013 to July 2013\nCisco network access server configuration. Configured Cisco networking equipment to set up network devices such as routers, switches, firewalls, terminals, AAA servers, and file servers with their respective MAC and IP addresses.']","[u'BA in Computational Mathematics', u'AS in Mathematics']","[u'University of California Santa Cruz, CA\nJune 2016', u'Orange Coast College Costa Mesa, CA\nAugust 2014']"
0,https://resumes.indeed.com/resume/e008229689b97151,"[u'Program Tester\nPearson Learning Solutions - Austin, TX\nApril 2012 to May 2012\nTested new school learning programs.', u'Data Analyst\nEMSI, Inc. - Waco, TX\nFebruary 2001 to January 2004\nScanned insurance documents into computer program. Input patients information, researched and verified patients information.']",[u'Associate in Business'],[u'Friedrich-List School Darmstadt\nJanuary 1988 to January 1991']
0,https://resumes.indeed.com/resume/569b3ac9e665cab6,"[u'Data Analyst\nCiti Group Inc - New Jersey\nNovember 2017 to Present\nConducted interviews to extract process knowledge from key staff and translate into process maps and\ndetailed training materials that capture step-by-step activities required by each team and role to complete each\nprocess. Gathered requirements and defined critical priorities through interviews and JAD sessions with\nbusiness users, stakeholders and SMEs.\n\u2022 Validating fundamental specifications and monitored production deployment for all the essential tasks\nperformed within the teams.\n\u2022 Lead business initiatives including brainstorming on new functionality for current systems and conducted\nthorough gap analysis to identify new opportunities.\n\u2022 Created a process document featuring requirement analysis and business evaluation with UML modeling\n(Use cases and Activity diagrams) using MS Visio to extract process and work flows for the use and\nunderstanding of both functional and technical teams.\n\u2022 Performing gap analysis in identifying and evaluating the ""As-Is"". Assessed the major flaws in the existing\n""As-is"" system and made clear recommendations of vital business process improvements.\n\u2022 Following the Software Development Life cycle (SDLC) concept as per Rational Unified Process (RUP)\nstandards in conducting the project.\n\u2022 Demonstrated experience in Agile, Scrum methodology activities for Requirements and Analysis phases.\n\u2022 Developing Use Case scenarios, Business Flow, Activity, State and Sequence diagrams to facilitate the\nbusiness process requirements for developers and company stakeholders.\n\u2022 Maintaining an issue-enhancement list, prioritizing outstanding defects and enhancement requests based on\nproject deadlines.\n\u2022 Developing a User Acceptance Test (UAT) plan and test case scenarios to guide a select group of key end-\nusers in testing the user interface and functionality of the application.\n\u2022 Appraised and assisted with key user testing of systems by developing and adhering to strict quality\nprocedures, thus ensuring the accuracy of appropriate business documentation.\n\u2022 Creating Use Case Diagrams, Activity Diagrams, Class and Context Diagrams, Data Flow Diagrams and\nSequence Diagrams in MS Visio.\n\u2022 Conducting Joint Application Development (JAD) sessions with management, subject matter experts,\nvendors, users and other stakeholders for open and pending issues.', u'Business Analyst\nBaxter Healthcare - Bangalore, Karnataka\nDecember 2016 to October 2017\n\u2022 Worked with the Product Owner to elicit, clarify, elaborate, synthesize, simplify and organize requirements in the form of user stories\n\u2022 Acted as a change agent for business processes through developing strong business partnerships and defining technical solutions to business problems\n\u2022 Created Product Backlog, sprint backlog and managed and updated Issues(Epics, Stories and Tasks) onto Jira. Participated in Sprint Planning, daily Scrum Stand up and Sprint Retrospective meetings.\n\u2022 Orchestrating collaboration between product owners, business sponsors, vendors, subject matter experts, designers, QA and development groups by removing roadblocks and fostering communication.\n\u2022 Created Business Requirement Document (BRD), System Requirement Specification (SRS), and Functional Requirement Specifications (FRS) for challenging business requirements\n\u2022 Played a key role in the planning, testing, and implementation of system enhancements and conversions.\n\u2022 Develop reports and data visualizations in Tableau to illustrate patterns and distribution.\n\u2022 Designed the business requirement collection approach based on the project scope and SDLC Methodology.\n\u2022 Conducted and facilitated interviews, focus groups, workshops, and meetings to refine project scope and define project requirements.\n\u2022 Performed requirement analysis including data analysis and gap analysis.\n\u2022 Designed and developed project document templates based on SDLC methodology.\n\u2022 Prepared documentation on business requirements specifications and project plans.\n\u2022 Comprehensively worked on requirement gathering and analysis for enterprise reporting system using RequisitePro.\n\u2022 Driving process maturity by defining, documenting, and implementing process change; identifying performance metrics and measuring the success of change.\n\u2022 Designed and implemented basic SQL queries for QA testing and report / data validation.\n\u2022 Built strategic partnerships with the business unit to develop a solid knowledge base of the business line, including the business plan, products.\n\u2022 Played a key role in the planning, testing, and implementation of system enhancements and conversions in and Scrum and Agile environment.\n\u2022 Functioned as the primary liaison between the business line, operations, and the technical areas throughout the project cycle.\n\u2022 Partnered with the technical areas in the research and resolution of system and process problems.\n\u2022 Analyzed and researched on the operational procedures and methods. Also recommended changes for improvement \u2013 with special emphasis on automation and efficiency.', u'Business Analyst\nCredit Suisse - Bangalore\nSeptember 2013 to November 2016\n\u2022 Performed Requirement Analysis by gathering both functional and non-functional requirements based on interactions with the process owners & stake holders and document analysis, represented them in Requirements Traceability Matrix (RTM).\n\u2022 Held requirements gathering sessions, captures meeting minutes, documents clear and concise requirements, follows up on action items and is accountable for requirements documentation.\n\u2022 Manage scope creep and clearly communicates with business stakeholders on feasibility and effort analysis.\n\u2022 Interacted with department heads to finalize business requirements, functional requirements and technical requirements and also created Business process model.\n\u2022 Facilitated JRP sessions between technical and regulatory teams for meeting requirements as well as solving problems on a daily basis\n\u2022 Involved in creation and reviewing of Functional Specifications and created a Functional Requirement Document.\n\u2022 Developing and maintaining strategies, plans, scripts and cases for User Acceptance Testing purposes.\n\u2022 Reviewed Stored Procedures for reports and wrote test queries against the source system (SQL Server) to match the results with the actual report against the Data mart (Oracle).\n\u2022 Coordinate and Manage Releases including deployment of approved features and maintaining evidence of proper production release protocols to ensure compliance with auditing requirements.\n\u2022 Monitor product bug queues in order to validate, triage and prioritize fixes.\n\u2022 Helped with Data Mapping between the data mart and the Source Systems.\n\u2022 Planned and defined system requirements to Use Case, Use Case Scenario and Use Case Narrative using the UML (Unified Modeling Language) methodologies.\n\u2022 Used Rational Requisite Pro to improve the communication of project goals, enhance collaborative development, reduce project risk and increase the quality of application before deployment.\n\u2022 Data mapping, logical data modeling, created class diagrams and ER diagrams and used SQL queries to filter data within the Oracle database\n\u2022 Created and maintained Narrative Use Cases (Business Use Cases, System Use Cases) and modelled Use Case Diagrams, Activity Diagrams, Data Flow Diagrams using MS Visio.\n\u2022 Followed the adaptive Scrum framework for the whole Project life cycle (PLC)\n\u2022 Fine-tuned search engines and pulled data from different databases and migrated data back and forth using SQL.\n\u2022 Involved in creation and execution of manual test cases in Mercury Test Director and analyzed the Test Results.\n\u2022 Involved with the QA team to conceptualize, determine and develop test approaches and methods for unit testing, integration and functional testing, load and usability testing according to the application complexity and test requirements.', u'Business Analyst\nSingTel - Singapore\nAugust 2012 to August 2013\n\u2022 Involved in all phases of software development life cycle in RUP framework.\n\u2022 Worked in the RUP environment for the elicitation, representation of requirements and in change management.\n\u2022 Performed requirement analysis by gathering both functional and non-functional requirements based on interactions with the process owners & stake holders and document analysis, represented them in requirements traceability matrix (RTM) using Requisite Pro. Performed user interviews and JAD sessions.\n\u2022 Assessed the flaws in the existing as-is system and made clear recommendations of Business process improvements and BPR, incorporated them in the future to-be system design.\n\u2022 Wrote user requirements specification (URS) and Functional requirements specification (FRS) documents as per the business requirements and process flow.\n\u2022 Designed and developed project templates based on SDLC Methodology.\n\u2022 Developed UML Use Cases for the application using Rational Rose and prepared the detailed work flow diagram based on the proposed enhancement for the system.\n\u2022 Performed gap analysis to compare the existing system with the proposed system and documented new requirements and features.\n\u2022 Extensively interacted with both user group and development team in coming up with structured charts, class and sequence diagrams.', u'Business Analyst\nNetTec Pte Ltd - Singapore\nAugust 2011 to July 2012\n\u2022 Gathered business requirements by conducting detailed interviews with business users, stakeholders, and Subject Matter Experts (SME\u2019s).\n\u2022 Prepared Business Requirement Document and then converted Business requirements into Functional Requirements Specification.\n\u2022 Conducted GAP analysis to understand the shortcomings of the paper based process and evaluated the benefits of the new system.\n\u2022 Risk Assessment/Prioritizing drafting, executing and documenting risk areas.\n\u2022 Constructed prototype early toward a design acceptable to the customer and feasible.\n\u2022 Performed User Acceptance Testing (UAT).\n\u2022 Trained fellow employees, provided documentation for procedures, and submitted/tracked problem reports using web spiders', u'Business Analyst/Data Analyst']","[u'Masters of Business Administration in Business Administration', u'Bachelors of Science']","[u'Jawaharlal Nehru Technological University', u'Sri Sathya Sai University']"
0,https://resumes.indeed.com/resume/dceb2163a6657a94,"[u'Data Analyst\nORION FOOD SYSTEMS - Sioux Falls, SD\nPresent']",[u'B.B.A in ACCOUNTING'],[u'UNIVERSITY OF SOUTH DAKOTA\nDecember 2017']
0,https://resumes.indeed.com/resume/3bca5f5f8937a13c,"[u'Quantitative Analyst\nFortune SG Fund Management - Shanghai\nJuly 2017 to September 2017\nNew ESG index issuance for China Market project\n\uf0b7 Investigated ESG factors rating system and compared methodologies among index companies to develop ESG rating criterion for Chinese firms\n\uf0b7 Compared performance of ESG indices by calculating annualized total returns in order to choose the optimal methodology as reference for new index construction\n\uf0b7 Applied the Brinson Attribution Model to decompose index performance and deduce which industry sectors contributed the most to total return\n\uf0b7 Advised department manager on the Chinese ESG index issuance project based on my independent research and market sentiment of ESG problems abroad and in China', u'Data Analyst\nMorningstar - Shenzhen\nSeptember 2015 to November 2015\n\uf0b7 Lowered working hours spent on data research by 2/3 for team by integrating and building an instruction set for data point gathering and searching\n\uf0b7 Communicated error data to fund management companies for correction to guarantee the data quality for clients\n\uf0b7 Reported data process progress of each team member to manager every day to ensure whole team accomplish tasks on schedule']","[u""Master's in Mathematical Finance"", u""Bachelor's in Finance""]","[u'Boston University Boston, MA\nSeptember 2016 to January 2018', u'Shenzhen University Shenzhen\nSeptember 2012 to June 2016']"
0,https://resumes.indeed.com/resume/edfaa306a448aec6,"[u'Data Analyst\nBAE Systems - Fort Worth, TX\nApril 2017 to Present\nPrototyped prognostic regression models to predict aircraft battery degradation,\nestablishing a more efficient maintenance process that extends battery operating life by an average of 2 years.\nImplemented gaussian mixture and ARIMA models to detect anomalies in aircraft\nperformance, leading to improved maintenance recommendations and fault isolation by subsystem SMEs.\nDeveloped reports to evaluate prognostic algorithm performance, generating solutions for data quality and collection issues, and reducing unnecessary maintenance actions by 40% across six months.\nLiaised directly with clients to identify additional domains to apply data analysis,\nsecuring 3 new projects for my team.\n\nProjects Predicting Hacker News Story Popularity\nUtilized Google BigQuery to collect five years of Hacker News story data. Implemented\nxgboost model with additional engineered text and temporal features to predict which stories make it to the front page. Performed topic modelling and extracted important\nfeatures from the implemented model to better understand the dynamics of story\npopularity.\nFacebook Predicting Check Ins Competition\nDeveloped ensemble of gradient boosted decision trees to rank likeliest check in locations from spatial and temporal data. Engineered features to account for seasonality and uncertainty in existing measurements. Placed in the top 10% of competitors.\nSantander Customer Satisfaction Competition\nDeveloped ensemble of multiple classification models to predict customer satisfaction.\nUtilized feature extraction and engineering to gain insight on what factors influence\ncustomer retention. Placed in the top 3% of competitors.']","[u'B.S. in Mathematics', u'in Physics/History']","[u'University of Texas at San Antonio San Antonio, TX', u'University of Texas at Austin Austin, TX']"
0,https://resumes.indeed.com/resume/05d644ef0ca2d4e8,"[u'Data Analyst\nAccounting Advantage - Lake Worth, FL\nFebruary 2017 to Present\n\u2022 Using Excel Spreadsheets to collect, analyze, and report data\n\u2022 Identifying new sources of data and methods to improve data collection, analysis and reporting\n\u2022 Meeting data analysis needs by collecting customer requirements and determine technical issues']","[u'Bachelors of Science in Computer Science in Computer Science', u'Associates of Arts in Computer Science']","[u'Florida Atlantic University Boca Raton, FL\nAugust 2013 to December 2018', u'Broward College Davie, FL\nAugust 2011 to July 2013']"
0,https://resumes.indeed.com/resume/df927c44142d539c,"[u""Data Analyst\nEgress Limited - Lagos, NG\nApril 2007 to July 2015\n\u2022 Design, create and maintain database objects including all physical and logical primary objects such as tablespaces, data files, tables, clusters, indexes, views, sequences, packages, procedures etc.\n\u2022 Create and maintain coding standards for stored procedures, triggers, logical and physical DB schema design.\n\u2022 Responsible for designing, developing, and optimizing company database.\n\u2022 Constantly providing support for test and company production database environments to include problem identification, reporting, tracking, analysis, and resolution.\n\u2022 Constantly monitoring the SQL database performance and space utilization.\n\u2022 Constantly establishing, maintaining, and monitoring database backups, Database security, user-role assignments, and individual logins per company security policies and procedures.\n\u2022 Ensure all database servers are backed up in a way that meets the business's Recovery Point Objectives."", u'DATA ANALYST ASSOCIATE\nCLEMALEX Limited - Lagos, NG\nFebruary 2003 to May 2007']","[u'', u'Bachelor of Science in Engineering Physics in Engineering Physics']","[u'Oracle University\nMarch 2003', u'Obafemi Awolowo University\nJuly 2002']"
0,https://resumes.indeed.com/resume/1029c61464109476,"[u'Investment Intern\nCITIC Securities (Zhejiang) Co., Ltd - Hangzhou, CN\nMay 2015 to August 2015\nZhejiang\n\u2022 Collected financial data and report from Wind and Bloomberg terminal to support company evaluation estimate.\n\u2022 Assisted portfolio manager to compute the discount and premium of 96 graded funds and predicted their net asset\nvalue.\n\u2022 Aided the traders to split or merge the share of graded fund with long-short strategy adopted to lock the price\ndifference of discount and premium.', u'Data Analyst\nPING AN Insurance (Group) Co., Ltd - Hangzhou, CN\nAugust 2014 to January 2015\nZhejiang\n\u2022 Collected and cleaned data (weekly, monthly, quarterly) of all financial services under PING AN group using\nExcel.\n\u2022 Generated business tracking and analysis report (weekly/monthly) and PPT to report performance and support\ndecision.\n\u2022 Participated in the project research and plan-making of real estate and auto financing business.\n\u2022 Hosted the Integrated Finance Conference of PING AN financial group of Zhejiang province with colleagues.\n\u2022 Mediated and address the operational conflicts with manager happened between three sides (Banks, Securities\nand Trust.']","[u'Master of Statistical Science Statistics in Statistical Science Statistics', u'Bachelor of Management Land Resources Management in Real Estate']","[u'Indiana University Bloomington, IN\nAugust 2016 to May 2018', u'Zhejiang Gongshang University Hangzhou, CN\nSeptember 2010 to May 2014']"
0,https://resumes.indeed.com/resume/b2a6f19d02642959,"[u""Sr. Business Analyst/Data Analyst\nState Street - Boston, MA\nMarch 2017 to Present\nAt State Street undertook a Group-wide initiative to streamline its KYC compliance program into a small number of Centers of Excellence that serve multiple business units across different jurisdictions. This restructuring effort is underpinned by Fenergo's KYC Compliance solution, which includes: Client & Counterparty Data Model; Regulatory Rules Engine and Catalogue; Advanced Case Management and Workflow Engine; Integrations with Internal Systems.\n\nResponsibilities:\n\u2022 Performed the required KYC screenings on customers documenting the information obtained on the client's as required by global KYC procedures.\n\u2022 Defined system requirements (BRD - Business Requirements Document) for complex, multi-departmental and multi-functions projects\n\u2022 Assessed existing agile practices and feasibility of conversion from Waterfall to Agile model.\n\u2022 Reviewed with users functional specifications (FRD - Functional Requirements Document), provided feedback to Technology\n\u2022 Ensuring compliance with all AML laws, regulations, guidelines, written procedures\n\u2022 Effectively conducted AML /KYC formality reports on questionable accounts and transactions.\n\u2022 Completed research, analysis and documentation of account activity and used personal judgment to determine appropriate actions that removed or minimized BSA/fraud exposure.\n\u2022 Reviewed and analyzed online banking/ ACH transactions to determine customer fraud activity.\n\u2022 Strong ability to write SQL queries using TSQL.\n\u2022 Provided end user support on every aspect of SharePoint implementation\n\u2022 Independently designed, developed and delivered SharePoint sites.\n\u2022 Participated in Project management framework Planning, Organizing, Leading, and Controlling.\n\u2022 Organized and facilitated Agile and Scrum meetings, which included Sprint Planning, Daily Scrums or Standups, Sprint Check-In, Sprint Review & Retrospective.\n\u2022 Led the coordination of all tasks associated Fix Connectivity, database updates, upload configuration and client communications between cross-functional teams attributing to the seamless onboarding of all new clients.\n\u2022 Analyzed Autosys job flows and created JIL's for data validation process such as pre and post load SQL checks.\n\u2022 Worked with business units & cross functional teams to assists in all aspects of client life cycle from pre-implementation in agile project methodologies.\n\u2022 Identified possible risks and their impact and drive change of scope (requirements) process during the whole project cycle and afterwards as change request (CR) process\n\u2022 Managed communication between key project stakeholders (Operations, Technology, Compliance, Business)\n\u2022 Actively involved in the Business process engineering and participated in generating business continuity process (BCP) documentation.\n\u2022 Integrated information processing work into the real work that produces the information.\n\u2022 Has been acting as a liaison between the client area and technical organization by planning, conducting and completing analysis for moderately complex business problems to be solved with automated systems.\n\u2022 Planed actions and adapt them to the plans of other teams (eg, UAT, Project Team, etc.)\n\u2022 Provided explanations of the problems of data quality when a very thorough analysis is required.\n\u2022 Resolving JIRA issues with technical team and maintaining Confluence JIRA board. Handling daily standup meeting and follow up till issues are resolved.\n\u2022 Expert in Microsoft office tool Excel (Vlookup, Pivot table).\n\u2022 Managed requirements and facilitates all requirements and documentation. Worked with other groups to provide training, resolve questions, assess user's needs, and document user requirements.\n\u2022 Gathered data and prepared necessary reports for clients to support decision-making and requirements analysis.\n\nEnvironment: Agile Scrum, Confluence, JIRA, SharePoint13, Fenergo 7.3, Actimize, Fircosoft, QlikView, Symphony, Oracle Solaris 10, LexisNexis, Rally, Rational Suite, MS Visio, UML."", u'Business Analyst/ Data Analyst\nWells Fargo - Charlotte, NC\nSeptember 2016 to February 2017\nThe goal of the project was to build a User Risk Reporting Module as a part of the Company wide Active Trade Surveillance and AML system implementation. The project covered areas of KYC, CIP and CDD in great depth. The methodology followed for the project was Agile Scrum.\n\nResponsibilities:\n\u2022 Studied business needs, identified project stakeholders and their roles, recognized major deliverables and collected business requirements for the project.\n\u2022 Drafted project charter, which included project goals, business case, opportunity statement, and high level timeline and team assignments for the projects.\n\u2022 Extensive work on financial instruments such as Bonds, Equities, Futures, Options, OTC/ET Derivatives, SWAP, Securities, Trades , Positions, Capital Markets and CFTC regulations to determine pertinence and consequence on functionality of the implemented system.\n\u2022 Established guidelines and parameters for the system based on currency and foreign transaction regulations like, BSA, US Patriot act, OFAC, SDN etc.\n\u2022 Supported Foreign Exchange, Money Market, Foreign Exchange Swaps, Interest Bearing Securities, Discounted Securities and Options for Treasury department.\n\u2022 Assisted the tech team as Lead Business Systems analyst with determining agile workflows, and assisting getting the JIRA tool to match their development cycle.\n\u2022 Supported the various business deliverables for a given release of the regulatory reporting delivery.\n\u2022 Organized the project into phases to impose management control using stage-gate reviews such as high-level user stories, test scenarios, Dimension modeling, Data Mapping, ETL etc.\n\u2022 Interacted with the Third Party Management Office onboarding platform, including tracking the completion of questionnaires, uploading of questionnaire responses, and direct communication with Third Party vendors used by the bank.\n\u2022 Modified SharePoint portal & site collection including: managing libraries, lists, forms, SQL, images and pages.\n\u2022 Created workflows with SharePoint tools (admin & power user), provided user training classes\n\u2022 Understood the AML trade surveillance compliance requirements by interacting with compliance department and SME experts in the laws/act of compliances.\n\u2022 Determined existing customer fraud trends and assisted in the prevention of future trends\n\u2022 Reviewed and updated KYC requirements and applied to appropriate dealer to client project\n\u2022 Accomplished risk management professional with a capacity for research and a positive track record in project management seeks a KYC analyst position\n\u2022 Performed Data Analysis and Data validation by writing SQL queries.\n\u2022 Developed views, functions, procedures, and packages using SQL to transform data between source staging area to target staging area.\n\u2022 Wrote SQL queries to perform Data Validation and Data Integrity testing.\n\u2022 Delivered support for processing of data from sources to target data warehouse by documenting detailed ETL mapping processes.\n\u2022 Managed changes to project scope using a change management procedure in coordination with Change Control Board (CCB).\n\u2022 Provided technical and procedural support for User Acceptance Testing. Tracked testing issues, maintained test matrix, ran daily status meetings, and met the exit criteria for UAT.\n\nEnvironment: Agile Scrum, Actimize, Factiva, Fenergo, Microsoft, Sharepoint13, World-Check, LexisNexis, Rally, Jira, Informatica, Rational Suite, MS Visio, UML.', u""Business Analyst/Data Analyst\nGE Capital - Stamford, CT\nSeptember 2013 to August 2016\nThe Global Product Controller project group worked on design and execution of various strategic and tactical change projects. This project was to develop a Risk Management System, which included daily publishing of risk metrics that covered Migration and Reconciliation of bank accounts data, Counterparty Exposure for Compliance and Regulations (Basel and Dodd-Frank). The SDLC employed was Agile Scrum.\n\nResponsibilities:\nSignature Card Project (Project1)\n\u2022 The aim of the account management project was to analyze the user data from their electronic fund transfer accounts and data update into Vault system and webcash system.\n\u2022 Responsible for providing successful delivery of Business Analytics and Optimization (BAO) technology initiatives.\n\u2022 Understanding business challenges and translating them into process, as well as technical solutions\n\u2022 Performing data research and analysis, as well as, administrating engagement activities in support of a work product.\n\u2022 Acquiring and applying knowledge of consulting methodologies, key technologies, as well as, engagement of support tools.\n\u2022 Developed ETL Jobs using Informatica; performing conversion of complex query into ETL design; and integration.\n\u2022 Team co-ordination for all development related activities followed by migration and implementation as per Releases.\n\u2022 Interacted with Sales, Middle Office, Risk Management, Product Control, Finance, Underwriting, Operation and other stakeholders to gather business requirements and create enterprise level flows, system architecture, trade flows and timelines\n\u2022 Managed and Lead BA for Analytics reports implementation. Provided ability for ad-hoc reporting and dashboard for live reporting capabilities.\n\u2022 Managed over 500 feeds from various systems of records\n\u2022 Assessed overnight releases and worked alongside release managers\n\u2022 Exceeded the project goals and decreased the beta release time by 4 weeks\n\u2022 Tracked post-deployment progress and proposed resolutions to existing issues\n\u2022 Meticulously addressed every detail to ensure a successful end-to-end business process\n\nAccount Reconciliation (Project 2)\n\u2022 Utilized various methods to reconcile account discrepancies.\n\u2022 Quarterly based GE header accounts reconciliation for child accounts in network and non-network banks.\n\u2022 Researched discrepancies and updated accounts with accurate information.\n\u2022 Resolved reconciliation discrepancies in a timely manner. Reviews bank accounts, checks balances against ledger amounts and verifies that the amounts match with the financial statement like asset, liability, expenses and revenues\n\u2022 Reported all issues to supervisor as needed. Performs other accounting tasks under the instructions of Finance Manager\n\u2022 Responsible for creating risk assessment intraday reporting and updating closing balance reporting for North America, Mexico and International Banks.\n\u2022 Regularly extracted precise data from company database to research projects.\n\u2022 Evaluated multiple levels of risk for capital and industrial business.\n\u2022 Worked closely with project engineers to identify risk needs.\n\u2022 Established strong working relationships with several key members of company's project management and finance departments.\n\u2022 Participated in quarterly Cash Sweep Process loan transaction.\n\u2022 Conducted Daily Standup meetings to ensure that proposed changes went through the system as well as the project team produced solutions that meet established standards, procedures, practices, risks and mitigation plans\n\u2022 Led Sprint Planning Sprint Estimation (SPSE) sessions, Sprint Retrospectives and Reviews, User Stories and Acceptance Criteria\n\u2022 Worked extensively with offshore development teams and tracked their progress through various tollgates over the life of the project\n\u2022 Provided coaching on Agile practices to other groups within the company\n\u2022 Reported various counterparty exposures and capital limits for all clients across the firm (Treasury experience.\n\u2022 Facilitated and managed meeting sessions with committee of SMEs from various business areas including Loan Monitoring, Asset Management, and Marketing.\n\nEnvironment: Agile, Rally, Rational Rose, Webcash, Windows 7, SharePoint 13, Microsoft SQL, PL/SQL, HP Quality Center, UML, MS Office Suite (Word, Excel, Power Point, Access, Visio)"", u""Business Analyst\nCITI Bank - New York, NY\nJune 2010 to August 2013\nThe aim of the project was to extensively work on the online banking application, which allowed the customers of the bank to check balances; make transfers; and view recent transactions. Enhancement of an application into the system of transfer of fund to anyone in any bank in India was also ensured.\n\nResponsibilities:\n\u2022 Accumulated system requirements from various departments like accounting, IS (Information Systems) through surveys and interviews.\n\u2022 Incorporated RUP to create Business Requirement Document Specifications using MS Visio and MS Word.\n\u2022 Planned and defined system requirements to Wire Frames with Use Case, Use Case Scenario and Use Case Narrative using the UML (Unified Modeling Language) methodologies.\n\u2022 Created Activity Diagrams, Sequence Diagrams and ER Diagrams in Ms Visio.\n\u2022 Conducted JAD Sessions, SME, vendors, users and other stakeholders for open and pending issues.\n\u2022 Responsible for meetings with users and stakeholders to identify problems, resolve issues and improve the process to ensure a stable and accurate solution.\n\u2022 Created exceptions report which were then submitted to project lead and stakeholders on a regular basis.\n\u2022 Facilitated user acceptance testing and test strategies with Information Systems Group.\n\u2022 Performed manual front-end testing to check all functionalities of different modules.\n\u2022 Suggested measures and recommendations to improve the current application performance with the aid of SCR's (Scope Change Requests).\n\u2022 Developed strategies with Quality Assurance group to implement Test Cases in Mercury Test Director for stress testing and UAT (User Acceptance Testing).\n\u2022 Trained the future users to coordinate their activities.\n\u2022 Assisted the System Analyst to document all custom and system modification after the SCR's were satisfactorily solved.\n\u2022 Identified all the processes in an organization and prioritize them in order of redesign urgency for business process engineering\n\u2022 Created and managed project templates, use case project templates, requirement types and traceability relationships in Requisite Pro and managed the concurrence of the parties involved with respect to the evaluation criterion.\n\nEnvironment: SQL Server 2000, MS Visio, MS Outlook, SharePoint 10, MS Project, SQL, MS Access, MS Excel and MS Word"", u""Business Analyst\nUnion Bank of India - Mumbai, Maharashtra\nSeptember 2005 to July 2007\nUBI Bank offers a wide range of banking products and financial services to corporate and retail customers through a variety of delivery channels and through its specialized subsidiaries in the areas of investment banking, life and non-life insurance, venture capital and asset management. The aim of the project was to extensively work on the online banking application, which allowed the customers of the bank to check balances; make transfers; and view recent transactions. Enhancement of an application into the system of transfer of fund to anyone in any bank in India was also ensured.\n\nResponsibilities:\n\u2022 Accumulated system requirements from various departments like accounting, IS (Information Systems) through surveys and interviews.\n\u2022 Incorporated RUP to create Business Requirement Document Specifications using MS Visio and MS Word.\n\u2022 Planned and defined system requirements to Wire Frames with Use Case, Use Case Scenario and Use Case Narrative using the UML (Unified Modeling Language) methodologies.\n\u2022 Created Activity Diagrams, Sequence Diagrams and ER Diagrams in Ms Visio.\n\u2022 Conducted JAD Sessions, SME, vendors, users and other stakeholders for open and pending issues.\n\u2022 Responsible for meetings with users and stakeholders to identify problems, resolve issues and improve the business process to ensure a stable and accurate solution.\n\u2022 Created exceptions report which were then submitted to project lead and stakeholders on a regular basis.\n\u2022 Facilitated user acceptance testing and test strategies with Information Systems Group.\n\u2022 Performed manual front-end testing to check all functionalities of different modules.\n\u2022 Suggested measures and recommendations to improve the current application performance with the aid of SCR's (Scope Change Requests).\n\u2022 Developed strategies with Quality Assurance group to implement Test Cases in Mercury Test Director for stress testing and UAT (User Acceptance Testing).\n\u2022 Trained the future users to coordinate their activities.\n\u2022 Assisted the System Analyst to document all custom and system modification after the SCR's were satisfactorily solved.\n\u2022 Created and managed project templates, use case project templates, requirement types and traceability relationships in Requisite Pro and managed the concurrence of the parties involved with respect to the evaluation criterion.\n\nEnvironment: SQL Server 2000, MS Visio, MS Outlook, MS Project, SQL, MS Access, MS Excel and MS Word""]","[u'', u'Master of Science in GA USA']","[u'HP Quality Center\nJanuary 2003 to January 2007', u'Mercer University']"
0,https://resumes.indeed.com/resume/83410de673f17adb,"[u'Data Analyst\nCisco Systems - Research Triangle Park, NC\nJuly 2017 to Present\n\u2022 Created seeding data for Alias Project by implementing fuzzy lookups with Iugum Data Software\n\u2022 Worked closely with software developers to ensure all seed data was UTF-8 encoded and contained valid UTF-8 data\n\u2022 Engaged with stakeholders to verify data integrity', u'JELA Data Analyst\nCisco Systems, via Teksystems - Research Triangle Park, NC\nOctober 2016 to April 2017\n\u2022 Data entry/inventory verification for Department of Defense organizations to include US Army, Navy, Marine Corps, Air Force, Defense Information Systems Agency, and more\n\u2022 Managed Federal ICRS tool requests and transactions for 14 military organizations using Microsoft Excel, and Cisco Systems tools CSCC, ConnectDots, and End of Life\n\u2022 Trained new team members on processes and Cisco Tools using documentation I created for training purposes\n\u2022 Acquired data from customers, ICRS database, CSCC, and CMRC. Identify, analyze, and interpret trends or patterns in collected data sets using MS Excel in order to present information to management', u'JELA Data Analyst\nCisco Systems - Research Triangle Park, NC\nAugust 2014 to October 2016\n\u2022 Managed Federal ICRS, database holding equipment information, requests and transactions for 14 military organizations using Microsoft Excel, and Cisco Systems tools CSCC, ConnectDots, and End of Life\n\u2022 Trained new team members on processes and Cisco Tools\n\u2022 Created training documentation in efforts to ease new team members into roles\n\u2022 Created team metrics using tableau queries to pull data from the database, based on team progress in creating customer requests\n\u2022 Metrics presented to management on bi-weekly basis using PowerPoint\n\u2022 Worked with ICRS team to test tools and backend database for issues and customer satisfaction concerns in order to alleviate bugs and make ICRS more user friendly', u'Data Analyst\nBASF Corporation, via Teksystems - Research Triangle Park, NC\nJuly 2013 to August 2014\n\u2022 Processed AgData database requests using Microsoft Excel, adding new entities and cleaning up any former mistakes in the database\n\u2022 Validated database entry information to ensure new entry accuracy to while cross referencing the company database with the national database\n\u2022 Managed the Data Management Team email account, completing requests for employees throughout the company', u'Data Analyst\nTruven Health Analytics - Durham, NC\nMarch 2012 to December 2012\n\u2022 Assisted both the Operational Services and Clinical Services teams with the delivery of consulting services to solve client hospital and healthcare issues\n\u2022 Extracted and analyzed raw client data using Microsoft Access and Excel\n\u25e6 Interpreted hospital data using pivot tables and vlookup function in Excel\n\u25e6 Generated queries to create presentations for hospital management in Access using a populated database\n\u2022 Assisted with project design, development of findings, and recommendations for improvement in order to maximize profit and minimize costs', u'Mathematical Analyst\nHoughton Associates Inc - Hillsborough, NC\nJune 2010 to December 2011\n\u2022 Lead analyst for creation of a document of over 125 pages for the Federal Aviation Administration (FAA) detailing WAAS benefits for flights and showing the results of the data analysis\n\u2022 Created presentations and slides in order to present the analysis completed on flight data to the FAA\n\u2022 Presentations created generated new and more contracts for the company with the FAA\n\u2022 Organized and analyzed large amounts of flight data using SQL and Microsoft Excel']","[u'Master of Science in Mathematics', u'Bachelor of Arts in Mathematics']","[u'Mississippi State University, MS State\nJanuary 2008', u'Mississippi State University, MS State\nJanuary 2006']"
0,https://resumes.indeed.com/resume/b019bbc991fc97f8,"[u'Data Analyst Intern\nMDVIP, Inc. - Boca Raton, FL\nMay 2017 to December 2017\n\u2022 Successful delivery of the product by collaborating with the sales team with a predictive efficiency of 88%.\n\u2022 Created SQL scripts for data extraction, reporting and used Tableau with MS SQL server to build custom dashboards.\n\u2022 Performed data pre-processing with Excel to predict the patients likely to reach for a prospective physician.\n\u2022 Brainstormed with stakeholders to design statistical models using R and preparing the documentation for firm.', u'Business Analyst\nTata Consultancy Services (TCS) - Mumbai, Maharashtra\nDecember 2012 to May 2015\n\u2022 Liaison between stakeholders for procuring business requirements and planning technical deliverables across multiple functional teams.\n\u2022 Collaborated with developers, team leads and system integration architects to drive out details of business and technical requirements and documented them.\n\u2022 Worked in a Scrum team and participated in backlog review and Sprint planning sessions in Agile methodology.\n\u2022 Reengineered 17 SQL scripts to a handle huge amount of customer data, increasing data processing by 24%.\n\u2022 Responsible for writing and confirming test plan reviews, test results and release plans.\n\u2022 Performed builds, packaging and deployed major releases on various environments successfully, maintaining proper version release notes.\n\u2022 Provided production support and have resolved issues without impacting business.\n\u2022 Mentored new Team members by providing functional knowledge transfer sessions on various project modules.']","[u""Master's in Management Information Systems"", u""Bachelor's in Information Technology""]","[u'University of South Florida-Main Campus Tampa, FL\nAugust 2016 to December 2017', u'University of Mumbai Mumbai, India\nJune 2008 to July 2012']"
0,https://resumes.indeed.com/resume/8e19d2181d51dbdd,"[u""Data Analyst\nHSBC Card Service - Des Moines, IA\nOctober 2017 to Present\n\u2022 Performs analysis on 2k+ records using Microsoft Excel to identify opportunities in data through advanced excel functions, Pivot Tables, Charts and presented to the team.\n\u2022 Extracted and manipulated data from SQL server using joins and stored procedure thereby decreasing retrieval time by 25%.\n\u2022 Organized and maintained data for reporting in Data Warehouse.\n\u2022 Analyzed business requirements and classified them as Use Cases and Sequence Diagrams using MS Visio.\n\u2022 Analyzed Data Set with Python, R and Excel.\n\u2022 Hand's on experience on Hadoop and Machine learning techniques.\n\u2022 Worked on Teradata SQL Assistant querying the source/target tables to validate the BTEQ scripts.\n\u2022 Translated business logic from one system (SQL, Excel, etc) into Alteryx in a quick and accurate manner.\n\u2022 Create views in Tableau Desktop that were published to internal team for review and further data analysis and customization using filters and actions."", u'Business/ Data Analyst\nSafety National - St. Louis, MO\nJanuary 2017 to September 2017\n\u2022 My role here as a Data Analyst was to focus and implement the database/data warehouse architecture, along with client-side implementation and analysis of the real-time data to help the business get more savvy on the business analytics like daily flash reports and new business revenue reports and so forth.\n\u2022 Created Adhoc reports to users in Tableau by connecting various data sources.\n\u2022 Worked with Business Analyst and the Business users to understand the user requirements, layout, and look and feel of the application to be developed.\n\u2022 Manipulating, cleansing & processing data using Excel, Access and SQL.\n\u2022 Responsible for loading, extracting and validation of data.\n\u2022 Define the protocol to connect to the source systems and extract data with HIPAA compliance.\n\u2022 Extensive experience in extracting data from various databases, creating SAS datasets, analyzing the data and creating derived datasets and creating reports.\n\u2022 Demonstrated track record of Knowledge of health clinics, employer wellness programs, and claims administration a plus.', u""Business/ Data Analyst\nBBVA Compass Bank - Birmingham, AL\nMay 2016 to November 2016\n\u2022 Analyzing and generating statistical reports & real-time dashboards for the Finance, Management and Localization team using Excel/Google sheets and Tableau.\n\u2022 Gathering and documenting requirements (BRD) from manager/linguists to build interactive automated dashboards using Tableau.\n\u2022 Analyzing and understanding root causes of change in metrics and defining KPI's.\n\u2022 Collaborating with software engineers and project managers to design the tools necessary to further automate and streamline the processes.\n\u2022 Imported Metadata from Teradata tables.\n\u2022 Tracking and Identifying the business elements of vendor relationships, including performance criteria such as quality, delivery and cost for all branches in the country.\n\u2022 Using Alteryx, pulled data from Oracle database to identify such prisoners entering with account benefits.\n\u2022 Good understanding of Data governance."", u""Data Analyst\nGoogle Maps\nJune 2014 to November 2015\n\u2022 Worked on 5 different regions on maps as part of Special Projects. (India, U.S., Canada, U.K. and Australia).\n\u2022 Mentor for 6 months, for New Entrants in the Process and team, answerable for their work.\n\u2022 Introduced Tableau to Client's to produce different views of data visualizations and presenting dashboards on web and desktop platforms to End-user and help them make effective business decisions.\n\u2022 Familiar with Agile Development.\n\u2022 Experience in training Business users through online and in-class sessions.\n\u2022 Generated tableau dashboards for sales with forecast and reference lines.\n\u2022 Worked on to analyze data to provide insight and analysis.\n\u2022 Demonstrated track record of Knowledge of health clinics, employer wellness programs, and claims administration a plus.\n\u2022 Testing Google Maps and raising bugs to engineers, prioritizing them based on severity of impact.""]","[u""Master's in Computer Science in Computer Science""]",[u'University of Illinois']
0,https://resumes.indeed.com/resume/e46343c3a7794934,"[u'Data Analyst\nHyve Solutions - Fremont, CA\nNovember 2017 to Present\n\u2022 Sole and first data analyst for company of 2,000+ employees\n\u2022 Analyze weekly and monthly reporting on staffing needs such as headcount, turnover, cost analysis, prepare presentation slides for weekly meetings with executives\n\u2022 Build reporting on inventory of goods for the products the Warehouses build, communicate with management and supervisors across different campuses and locations', u'People Analytics\nFederal Reserve Bank of San Francisco - Los Angeles, CA\nMay 2017 to September 2017\n\u2022 Cleaned and prepared large HR data sets using SQL and Excel from multiple sources and partnered cross-functionally with HR, Finance, Police teams to determine the optimal staffing level while minimizing overtime for Police Services (140+ employees who protect Bank assets and personnel)\n\u2022 Build interactive and dynamic Tableau dashboards using customizations and visualizations, identifying new trends and unlocking impactful insights into overtime to make workforce more cost effective\n\u2022 Communicate complex analysis and insights to broad audiences through poster and PowerPoint presentations to management and executives\n\u2022 Various ad-hoc analysis and visualizations to support various HR initiatives; translate business problems and requirements into data questions', u'Data Scientist\nCalifornia Department Of Transportation - Sacramento, CA\nJune 2015 to September 2015\n\u2022 Applied expertise in statistical methods such as multiple log-linear regression to understand and analyze how Caltrans past projects can predict future workflows with a combination of Excel and R\n\u2022 Partnered with cross-functional business and technical teams to deliver data insights, identify trends and opportunities, and answer business questions\n\u2022 Acquire, transform, manipulate and analyze large sets of data from multiple sources to understand the impact of project changes and delivery towards goals']","[u'BS in Statistics', u""Bachelor's""]","[u'UC Davis Davis, CA\nSeptember 2013 to March 2017', u'']"
0,https://resumes.indeed.com/resume/2cd0eb7faaf52824,"[u'Data Analyst - AT&T\nDirectv\nFebruary 2016 to Present\nAT&T (Directv) is the second largest provider of mobile telephone services and the largest provider of fixed telephone services in the United States and provides broadband subscription television services through DirecTV.\n\nResponsibilities:\n\u2022 Assisting Data Scientist, Engineering and Business Team in tracking call volumes for Los Angeles and Houston region totaling of more than 7 million customers for Set Top Boxes (STB).\n\u2022 Extracting large data volume (Big Data) from Data Lake for data analysis to draw conclusions for managerial action and strategy for various Set Top Boxes (STB).\n\u2022 Actively engaged in meetings with the business consultants for requirement gathering.\n\u2022 Proficient in using Adobe Analytics (Omniture) for user tracking on various Set Top Boxes (STB).\n\u2022 Working cross-functionally with engineering team to understand how production and 3rd party data is being collected.\n\u2022 Extracting data from various production databases using Teradata to meet Campaign data needs.\n\u2022 Providing weekly and monthly reports to Business Analysts and Operation Analysts.\n\u2022 Extract data from existing data stores, developing and executing Engineering reports for performance and response purposes by using SQL, MS Excel.\n\u2022 Responsible for analyzing business requirements and developing Reports using PowerPoint, Excel to provide data analysis solutions to business clients.\n\u2022 Wrote Bteq (TeraData), SQL scripts for large data pulls and ad hoc reports for analysis.\n\nEnvironment: TeraData, BTeq, TeraData SQL Assistant, Hive, R Cloud, PowerPoint, Excel, Linux, Tableau, Agile, Scrum.', u""Data Analyst\nMarriott Hotels\nOctober 2013 to December 2015\nMarriott International, Inc. is a leading global lodging company with more than 6,000 properties in 122 countries and territories, reporting revenues of more than $17 billion in fiscal year 2016.\nResponsibilities:\n\u2022 Assisted Senior Data Analysts, Data Scientists with analysis of RevPar(Revenue per available room) and hotel occupancy rates for various Marriott owned properties.\n\u2022 Extensive experience in working with enterprise applications including Customer Relationship Management (CRM) tools for improving business strategies for Executives and clients.\n\u2022 Contributed to the implementations of strong revenue management strategies which helped increase RevPar by 10% for East Coast Region for few of the owned properties.\n\u2022 Performed complex T-SQL queries, stored procedures and produced ad-hoc reports from the database.\n\u2022 Made extensive use of Marriott Customer Relationship Management tools and systems: Opera/PMS - Sales & Catering, Sales Force Automation (SFA Web) for ad-hoc analysis and accurate Sales reporting analysis.\n\u2022 Created several tables, views, indexes, functions, triggers, stored procedures, CTEs, temporary tables, result set based upon the functional or business specifications.\n\u2022 Utilized SSIS (SQL Server Integration Services) to manage Extraction, Transformation and Loading (ETL) process to pull large volume of data from various clients' feeds or sources.\n\u2022 Actively participated in data clean-up and validation efforts by reaching out to all business partners to ensure integrity of master data across the systems.\n\u2022 Assisted BI team to set up Tableau for loan Sales metrics and scorecard to analysis.\n\u2022 Worked with SQL Server Analysis Services (SSAS) for data mining and online analytical processing analysis.\n\nEnvironment: SQL Server, T-SQL, MS Excel, Power Point, Tableau, JIRA, SSRS, Linux, Agile, Scrum."", u""Data Analyst\nFirst Republic Bank\nSeptember 2012 to August 2013\nNextGen Digital Platform Online banking platform redefines FRB's online service offering. Best in class solutions are integrated into a customized User Interface to create a compelling digital service which provides access/authentication components for the new Digital banking solution and integrates customer self-service capabilities.\nResponsibilities:\n\u2022 Worked on data profiling using various relational Databases and Data Warehouse systems.\n\u2022 Created validation planning documents and validation scripts using SQL.\n\u2022 Created detailed source to target data mapping specification documents to create mappings.\n\u2022 Used advanced Microsoft Excel functions including VLOOKUP, HLOOKUP to create pivot tables.\n\u2022 Conducted reports for implementation of marketing strategies, leading to growth in customer acquisition\n\u2022 Worked closely with the UAT team for QC/JIRA resolution and bug fixes.\n\u2022 Worked closely with infrastructure team in monitoring critical alerts in production environment.\n\u2022 Effectively interacted with Business Analysts and Data Modelers and defined Mapping documents and Design process for various Sources and Targets.\n\u2022 Gathered user requirements, analyzed and designed software solution based on the requirements.\n\u2022 Involved in creating database objects like tables, views, procedures, triggers, functions using T-SQL to provide definition, structure and to maintain data efficiently.\n\nEnvironment: T-SQL, Sybase, Informatica, SQL Server, MS Excel, MS Power Point, Linux, Agile, Scrum."", u'Business Data Analyst\nWalmart\nJune 2011 to August 2012\nThe Walmart has return integration project undertaken to provide efficiency to day to day business functions offering multi-channel purchase experience and reduce manual workarounds for Walmart returns metrics. Walmart.com must integrate return process system with Walmart stores to get accurate results of overall business\nResponsibilities:\n\u2022 Involved in Inception Phase and prepared vision statement and initial data models that contain Business Requirement Documents and supporting documents that contain the essential business elements and detailed definitions.\n\u2022 Co-authored business requirements document with project teams. Extracted, discussed, and refined business requirements from business users and team members.\n\u2022 Maintained records and statistical information for E-Commerce managers, E-Commerce personnel, other departments and outside vendors.\n\u2022 Hands-on experience working with dynamic dashboards, grids and statically reporting using Micro Strategy.\n\u2022 Validated the test data in DB2 tables on Mainframes and on Teradata using SQL queries.\n\u2022 Worked with SQL Server 2005 Analysis Services (SSAS) for reporting and online analytical processing analysis.\n\u2022 Identified gaps and partner with ETL teams to get the data elements needed.\n\u2022 Assisted faculty and staff with queries, statistical analyses, reports and technical difficulties related to data retrieval.\n\u2022 Worked with other DEV teams and infrastructure team to provided oversight and input for engineering strategy.\n\nEnvironment: SQL Server, UNIX, Linux / Oracle Platform, MicroStrategy, MS Visio, MS Word, Excel, Power Point, Agile.', u'Business Intelligence Analyst\nTrinity Health\nMay 2009 to May 2011\nTrinity Health is one of the largest multi-institutional Catholic health care delivery systems in the nation, employs more than 131,000 colleagues, including 7,500 employed physicians and clinicians.\n\nResponsibilities:\n\u2022 Experienced in working with Health care codes including ICD-9-CM, ICD-10-CM/PCS, CPT-4, and HCPC coding.\n\u2022 Responsible for performing large data compiling and produce regular (daily, weekly) and periodic reports on the activities related to the claims, payments.\n\u2022 Made recommendations and assisted in data capture, data extraction and analysis for cross functional\nteams.\n\u2022 Implemented the best practices in data management to ensure the integrity of the data, the quality of data processes.\n\u2022 Presented analyzed data through visualizations using Excel and Tableau tools to a variety of internal clients of the Medical Center.\n\u2022 Analyzed utilization and membership reports (e.g. for claims data, provider data, utilization data) and provided insights of data consistency between reports and database.\n\u2022 Provided support to users by responding to email inquiries, using existing documentation or doing research/ SQL drill downs on data issues.\nEnvironment: Oracle, T-SQL, Excel, Linux, Outlook, MS Office, Teradata, BTeq, Teradata SQL Assistant, Agile.']",[u'Bachelor of Science in Business Administration in Business Administration'],"[u'San Francisco State University San Francisco, CA']"
0,https://resumes.indeed.com/resume/1f266cd8a0a9cff8,"[u'Data Analyst\nErnst & Young\nJanuary 2013 to February 2015\n\u2022 Interpret data from primary and secondary sources using statistical techniques and provide ongoing reports.\n\u2022 Compile and validate data; reinforce and maintain compliance with corporate standards.\n\u2022 Develop and initiate more efficient data collection procedures.\n\u2022 Identified business requirements and devised implementation strategies to solve the business problems.\n\u2022 Prepared data examples, runs and views for client meetings.\n\u2022 Mined data to uncover insights and identify market trends and inflection points.\n\u2022 Collaborated cross-functionally with business analysts, developers and testers to explain new process\ntransformations.\n\u2022 Developed SQL queries to obtain complex data from tables in remote databases.\n\u2022 Defined naming standards for the data warehouse.\n\u2022 Assessed data and issues and directed concerns to business unit leadership when appropriate.\n\u2022 Made actionable recommendations based on data trends.\n\u2022 1.5 years of experience in data mining.\n\u2022 Manipulating, cleansing & processing data using Excel, Access and SQL, Tableau\n\u2022 Responsible for loading, extracting and validation of client data.\n\u2022 Writing T-SQL scripts to manipulate data for data loads and extracts.\n\nProject#1: EY Accounting\nErnst & Young\n\u2022 Extracted, compiled and tracked data, and analyzed data to generate reports.\n\u2022 Worked with other team members to complete special projects and achieve project deadlines.\n\u2022 Developed optimized data collection and qualifying procedures.\n\u2022 Leveraged analytical tools to develop efficient system operations.\n\u2022 Interpret data from primary and secondary sources using statistical techniques and provide ongoing reports.\n\u2022 Compile and validate data; reinforce and maintain compliance with corporate standards.\n\u2022 Develop and initiate more efficient data collection procedures.\n\nProject #2: Assurance Analytics\nErnst & Young\n\u2022 Performed daily data queries and prepared reports on daily, weekly, monthly, and quarterly basis.\n\u2022 Used advanced Excel functions to generate spreadsheets and pivot tables.\n\u2022 Ensured data accuracy through the creation and implementation of data integrity queries.\n\u2022 Created Stored Procedures.\n\u2022 Designed adhoc queries with SQL.\n\u2022 Examined reports and presented findings in PowerPoint and Excel.\n\u2022 Implemented projects to meet organizational requirements.\n\u2022 Created tables in Access and merged data into Word templates.']","[u'B-tech in Computer Science Engineering', u'in Higher Secondary Education', u'Leaving Certificate']","[u'Cochin University of Science& Technology (CUSAT)\nJanuary 2009 to January 2013', u'Government Board of Higher Secondary School\nJanuary 2007 to January 2009', u'Nirmala Higher Secondary School\nJanuary 2007']"
0,https://resumes.indeed.com/resume/7f965670a83cc73d,"[u""MARKET DATA ANALYST\nKANTAR IMRB - Mumbai, Maharashtra\nFebruary 2014 to June 2016\nProcessed market research data to analyze market trends, quantitative attitude of usage, brand awareness,brand image for ad-hocs and year-long projects\nCoordinated with the client research team on a weekly basis to gather research and project parameters to provide them with the necessary data and analysis reports and assist them with their research and presentation\nDesigned surveys by extracting market research data from various data sources using SQL queries\nPerformed data modeling, trend analysis and data interpretation to generate monthly, quarterly, and yearly\nperformance reports for client market research team\nCreated analytical reports and interactive dashboards in Tableau and in-house reporting tools to effectively\nview Consumer Behaviors and Key Performance Indicators (KPI's) to measure success of marketing strategies\nEnsured integrity, quality and accuracy of the data and performance reports delivered to the clients by performing periodic code and data checks\nIndustries served: Consumer Products| Food and Beverages | Financial Services\nHandled the responsibility to independently coordinate with the research team and data scientists working for\nmajor clients within two months on-board.\nAutomated data extraction, loading and processing for reporting and dashboard improving the efficiency by\n30%""]","[u'M.S. in Business Analytics', u'B.E. in Computer engineering']","[u'The University of Texas at Dallas Dallas, TX\nMay 2018', u'University of Mumbai Mumbai, Maharashtra\nAugust 2013']"
0,https://resumes.indeed.com/resume/7b10454d7b4503a3,"[u'Data Analyst\nVVPatchouli - New York, NY\nMay 2017 to December 2017\nProject Scope: VVPatchouli is a New York based personal luxury fashion brand. The scope of the project was to incorporate data analysis techniques to transform the customer purchase data into actionable insights and improve sales. The major deliverable of the project was to revamp the reporting and analytics capabilities by incorporation business intelligence tools (Tableau, Google Analytics) and capture customer engagement. The data from its customer buying patterns was used for identifying new KPIs (Sentimental Score, NPS, RFM) for improving business and user shopping experience. The project also aimed at crating a future platform based digital business model strategy that will provide a platform for artisans and customers for buying and selling customized handbags\n\nAccountabilities\n\u2022 Data analysis project incorporated various statistical techniques and data mining tools such as R, Python (SciKit, Pandas, NumPy), SQL, and Tableau (Clustering, Regression, ANOVA, Chi-Square) to transform raw data into actionable insights\n\u2022 Developed Recency, Frequency and Monetary model using R script to analyze the customer life time value and tailor marketing initiatives\n\u2022 Trained and tested third party social media campaign data using linear model to evaluate the number of views and brand following per month\n\u2022 Performed Sentimental Analysis using python package (Textblob ()) to understand customer interaction with the brand and introduced new email campaign by analyzing the pain point topics which increased customer engagement by 12%\n\u2022 Implemented Logistics regression with an accuracy of 0.79 (Confusion Matrix) to predict the customer who are more likely to buy the product based on age, area income, daily time spend on the website country, city, and gender\n\u2022 Developed adhoc dashboards to highlight new KPIs (sell through rate, average customer spends, stock turnover rate, customer service email count) using Tableau calculated field function which increased the total customer satisfaction survey results by 17% over month\n\u2022 Analyzed web traffic and tracked site behavior KPIs (Click through rate, Website Traffic, Unique visitors, pageviews per session) to support business decisions and recommended strategies to the web development team for better UI', u'Data Analyst\nCapgemini Consulting\nAugust 2014 to November 2015\nProject Scope: Royal Mail Group (RMG) is a leading postal service provider in United Kingdom. Royal Mail Group (RMG) is a leading postal\nservice provider in United Kingdom. The project aimed at building a reporting infrastructure to standardize reporting and streamline generation across RMG. Part of the project was to understand the business operations and KPIs by conducting off shore knowledge transfer meeting and consolidating reporting solutions using SAP Business Objects\n\nAccountabilities\n\u2022 Designed and developed the (Dimensions, Facts, Aggregations) with integrity constraints into data foundation layer using connection pool,\ndeveloping multiple dimensions and facts/ Measures objects in semantic layer to empower business users with autonomy to analyze sales\n\u2022 Developed reports/dashboards with different analytic views (Pivot table, charts, Column selectors, view selectors) for analyzing monthly sales\n\u2022 Defined various facts and Dimensions in the data mart including Fact Less Facts, Aggregate and Summary facts by evaluating the business\n\u2022 Enhanced existing Business Models & created customized Dashboards (Drilldown, Aggregation) for revenue reports for regions in Europe', u'Data Analyst\nMaster Bridge Consulting\nJanuary 2014 to August 2014\nBusiness Intelligence system which is built to capture New and Recon Part Sales. Parts Pricing is one of the groups under SBI defines Pricing actions every year. Customer requests for adhoc, weekly and monthly BI reports. SBI Prime Margin Reporting is one such reporting which is used to analyze Sales and Expense Trends and used in trend Analysis the project involved in creation of Pricing Analytics and Optimization process using statistical techniques\n\nAccountabilities\n\u2022 Analyzed existing internal data and external data, worked on entry errors, classification errors and defined criteria for missing values\n\u2022 Used cluster analysis for identifying customer segments, decision trees used for profitable and non-profitable customers and market basket\nanalysis for customer purchasing behavior and product association\n\u2022 Quantified the relation between price changes and volume, picked logistic regression which resulted in a better model fit data']","[u'Master of Science in Management of Technology', u'Bachelor of Engineering in Information Technology']","[u'New York University New York, NY\nJanuary 2016 to December 2017', u'Mumbai University Mumbai, Maharashtra\nMay 2014']"
0,https://resumes.indeed.com/resume/2608578580f5ee43,"[u""Data Analyst\nUnited Way - Hackensack, NJ\nDecember 2017 to Present\n\u2022 Created workbooks and Dashboards using calculated metrics for different visualization requirements\n\u2022 Drew upon full range of Tableau platform technologies to design and implement proof of concept solutions and create advanced BI visualizations\n\u2022 Created dashboard views from provider information data to members\n\u2022 Created Rich Graphic visualization/dashboards to enable a fast read on claims and key business drivers and to direct attention to key area\n\u2022 Involved in creating dashboards, reports and then published on to the Tableau server\n\u2022 Created side by side bars, Scatter Plots, Stacked Bars, Heat Maps, Filled Maps and Symbol Maps according to deliverable specifications\n\u2022 Incorporated a regional and national comparative / benchmark metrics to give context to Horizon performance\n\u2022 Effectively used Data blending, filters, actions, Hierarchies feature in tableau\n\u2022 Created extensive custom joins for blending data from several different data sources\n\u2022 Coordinated new data development, which includes the creation of new structure to meet business requirements or streamline existing processes to reduce overhead on the existing warehouse with ETL Development team\n\u2022 Collaborated with business users and members of the BI team, including business analysts, data warehouse developers, and managers, to design and develop as per requirement\n\u2022 Created a proof of concept for the optimal data integration process\n\u2022 Created Tableau scorecards, dashboards using stack bars, bar graphs, scattered plots, geographical maps, Gantt charts using show me functionality during POC\n\u2022 Worked extensively with Advance analysis Actions, Calculations, Parameters, Background images, Maps, Trend Lines, Statistics, and Log Axes. Groups, hierarchies, sets to create detail level summary report and Dashboard using KPI's"", u""Aeries Technology, Data Services - Hyderabad, Telangana\nJanuary 2015 to March 2016\n\u2022 Building, publishing customized interactive reports and dashboards, report scheduling using Tableau server\n\u2022 Created action filters, parameters and calculated sets for preparing dashboards and worksheets in Tableau\n\u2022 Effectively used data blending feature in tableau\n\u2022 Defined best practices for Tableau report development\n\u2022 Administered user, user groups, and scheduled instances for reports in Tableau\n\u2022 Executed and tested required queries and reports before publishing\n\u2022 Mastered the ability to design and deploy rich Graphic visualizations with Drill Down and Drop-down menu option and Parameterized using Tableau\n\u2022 Created side by side bars, Scatter Plots, Stacked Bars, Heat Maps, Filled Maps and Symbol Maps according to deliverable specifications\n\u2022 Consistently attended meetings with the Client subject matter experts to acquire functional business requirements to build SQL queries that would be used in dashboards to satisfy the business's needs\n\u2022 Extensive experience in Tableau Administration Tool, Tableau Interactive Dashboards, Tableau suite\n\u2022 Provided support for Tableau developed objects and understand tool administration\n\u2022 Worked in Tableau environment to create dashboards like weekly, monthly, daily reports using tableau desktop & publish them to server\n\u2022 Created Custom Hierarchies to meet the Business requirement in Tableau\n\u2022 Managed Production, Test and Development environments including installation of patches, security administration and access controls in Tableau\n\u2022 Created Workbooks and dashboards for analyzing statistical billing data using Tableau 8.0\n\u2022 Created various connections, Data foundations and Business layers using Information Design Tool 4.1(IDT) for AP Reporting\n\u2022 Converted WEBI Reports to Tableau Dashboards for Advanced Visualizations\n\u2022 Converted Xcelsius dashboard to Tableau Dashboard with High Visualization and Good Flexibility\n\u2022 Developed formatted, complex reusable formula reports and reports with advanced features such as conditional formatting, built-in/custom functions usage, multiple grouping reports in Tableau\n\u2022 Was Responsible for developing provisioning reports for various groups and upper management\n\u2022 Created Complex Dashboard, Reports using WEBI having multiple metrics and trends and created\n\u2022 Created Hyperlinks for each of the Metric to open a detail report using Open Documents"", u""Data Analyst Intern\nAbacus Concepts - Hyderabad, Telangana\nMay 2013 to November 2013\n\u2022 Implemented R and SAS to obtain the required results and analyze the performance of the students\n\u2022 Integrated R and SQL server 2008, populated the databases with student information and their test scores\n\u2022 Used R to determine student's Mean, Median, Mode and other statistical analysis techniques to check their respective position in class and school. Indicated the areas in the coursework where students can improve and Demonstrated the results of analysis using data visualization to the school committee""]","[u'Master of Professional Studies in Informatics', u'Bachelor of Technology in Information Technology']","[u'Northeastern University Boston, MA\nJuly 2017', u'Jawaharlal Nehru Technological University\nMay 2014']"
0,https://resumes.indeed.com/resume/dfb501764f304450,"[u'Data Analyst\nQvalPro Solutions - Coimbatore, Tamil Nadu\nSeptember 2014 to November 2015\n\u2022 Involved in business and analysis during requirements gathering and translating those requirements into data analytics, business intelligence data sources and reports.\n\u2022 Responsible for the collection, validating and cleaning of raw data. Examined raw data to find and evaluate\nactionable insights that would help the organization.\n\u2022 Summarized and documented required data trends, modifications and insights. Created ad-hoc reports to users in Tableau by connecting various data sources.\n\u2022 Created dashboards using Tableau for reporting and further analysis purposes, dynamic dashboards for identifying trends and forecasting.\nDinesh Palnati, Phone: +1 815-764-2834, Email: dineshpalnati@gmail.com']","[u""Master's in Management Information Systems"", u'Bachelor of Technology in Electronics, and Instrumentation']","[u'Northern Illinois University\nDecember 2017', u'GITAM University\nMay 2014']"
0,https://resumes.indeed.com/resume/a4b2575c127bde1a,"[u'Operations Support\nSeptember 2016 to Present\n09/16 TO PRESENT\n- Prototyped mobile application with React Native, Mac applications with Vue and Electron, and a web application with Vue, Node, Express, Golang, Postgres and Sequelize\n\n- QA testing for a web application\n\n- Designed features for company SFTP login and landing pages, Performed ad-hoc SQL\nqueries on highly relational medical data\n\n- Managed, implemented, and curated an LMS project through Wordpress', u'Software Analyst\nONE DISCOVERY\nFebruary 2015 to April 2016\n- Created and maintained an online help documentation system for a complex web application\n\n- Tested software for errors and performance & provided suggestions for enhancements\n\n- Communicated with developers and management to maintain scope and quality', u'Data Analyst Intern\nMCKESSON\nJune 2013 to August 2013\n- Created an integrated customer database for analytics\n\n- Improved process efficiency for recording data\n\n- Served as liaison between field sales representatives and technology resources\n\n- Assisted department members with project assignments']","[u""Master's in Decision Analytics"", u""Bachelor's in Business Information""]","[u'Virginia Commonwealth University\nJanuary 2016 to January 2017', u'Virginia Tech\nJanuary 2011 to January 2014']"
0,https://resumes.indeed.com/resume/c0aa1961cd07b24f,"[u""IT Business/ Data Analyst\nLegend Solutions Inc - Tampa, FL\nMay 2016 to Present\nTampa, FL May 2016 - Current\nDescription: The world's leading financial institutions, serving individual consumers, small and middle-market businesses and large corporations with a full range of banking, investing, asset management and other financial and risk management products and services. The company provides unmatched convenience in the United States, serving approximately 47 million consumer and small business relationships with approximately 4,700 retail financial centers, approximately 16,000 ATMs, and award-winning online banking with approximately 33 million active users and approximately 20 million mobile users. Client is a global leader in wealth management, corporate and investment banking and trading across a broad range of asset classes, serving corporations, governments, institutions and individuals around the world.\nRole: IT Business/ Data Analyst\nResponsibilities:\n* Created Risk Reports, utilizing Microsoft Excel and PowerPoint, for the Service Delivery Leadership Team to review and take action upon.\n* Created Safety Engagement reports which were presented on Safety Leadership Days to promote the importance of safety in any environment of the company\n* Created User Guides via PowerPoint slides for ServiceNow detailing every step of current Service Delivery processes with images and clear visuals\n* Placed Risk and Audit reports on Team Hub to display how effectively these areas are being managed\n* Assisted in efforts to move the current Risk Register from excel to a more advanced and automated version on SharePoint.\n* Involved in gathering business requirements, designed data maps and data models. Created Internal and External Design Documents and Transaction Definition Documents.\n* As a member of Center Of excellence (COE) team provide solutions for the critical issues of the projects Data @finger tips, DSFM (India), Dealer Franchising and staff augmented matrix and support until they are on track.\n* Involved in Relational Data modeling for creating Logical and Physical Design of Database and ER Diagrams using multiple data modeling tools.\n* Developed high Level Design of ETL Packages for different requirements in MS SQL Server 2008 for integrating data from heterogeneous sources (Excel, CSV, Oracle, flat file, Text Format Data) using SSIS.\n* Extensively created detailed design documents, report templates and enrollment details, Tabular/Matrix Reports, chart Reports for key performance indicators, Summary Reports, Parameterized Reports, dash board reports and sub reports.\n* Worked with stored procedures for data set results for use in Reporting Services to reduce report complexity and to optimize the run time. Exported Reports into various formats from (PDF, Excel) and resolved formatting issues.\n* Involved in deploying reports, schedule reports, and created groups and users. Used role based Security to control access to folders, reports and resources. Worked in creating and modifying data driven subscriptions, fine-tuning of standard subscriptions. Published and scheduled Business Objects Reports to users using Scheduler.\n* Involved in installation of Sql Server 2012 on virtual machines and also Configuration of SQL Server Reporting Services 2012 to run in SharePoint server 2010 to enable the power view mode in share point server and also involved in installing the share point server 2010 on windows servers.\n* Involved in designing, implementing the User metric dashboard POC, Score Card Automation POC and Resource management POC using both power view and tableau Ad-Hoc reporting tools.\n\nEnvironment: Erwin, RUP, DTS, Query Analyzer, Business Intelligence Development Studio (BIDS), MS Excel, MS Access, MS Visio, MS SQL, Windows."", u'Business Data Analyst\nTMHP - Austin, TX\nDecember 2014 to April 2016\nAustin, TX Dec 2014 - Apr 2016\nDescription: The goal of TMHP is to establish as the standard of excellence in management of Medicaid and healthcare services. TMHP partner with HHSC and state of Texas to deliver cost effective, customer focused Medicaid and healthcare services through a commitment to continuous improvement, pragmatic technical solution and operational excellence.\nRole: Business Data Analyst\nResponsibilities:\n* Maintain Weekly reports, Change Management documentation, project plans, and project minutes of meeting.\n* Analyze Survey Results and conduct trend analysis.\n* Develop parameters of and maintain standard dashboard for KPI measurement on a weekly reporting cycle.\n* Create Current and Future state diagrams and process flows, BRD, FSD for business and supporting technology teams.\n* Act as SME (Subject Matter Expert) and liaison, supplying business knowledge and functional recommendations and evaluation input to technology development teams.\n* Responsible for customer on boarding/contract Management process analysis through CSCC.\n* Possess knowledge & familiarity with Cisco RMA process & service compliance.\n* Use expert knowledge to understand customer business requirements, translate them into technical requirements and map these to differentiated value proposition.\n* Help drive business process design efforts through requirements analysis and information gathering.\n* Identify metrics, develop & generate reporting processes to track key business indicators and metrics for Business Process Design efforts.\n* Identify and provide analysis around resource or process gaps, business indicators and metrics.\n* Take ownership of key deliverables to help facilitate the transformation of Service Delivery best practices.\n* Assist with cost/effort estimates in collaboration with IT and Business Partners.\n* Quantifying the value of business case and Service Delivery opportunities acquired a solid understanding of business/financial modeling and business case development.\n* Maintain & improve Customer Support Engineer Workflow in My WorkZone which is maintained in SFDC.\n* Perform Business Acceptance Testing (BAT) to verify requirements and business solution and supported User Acceptance Testing (UAT).\n* Support Change Management team to develop impacts on Customer & Partner Experience.\n* WebEx Social Site & Cisco Doc Maintenance to manage project deliverables in a timely manner.\n\nEnvironment: UML, MS Office (MS Word, MS Excel, MS PowerPoint, MS Visio), BRE (Business Rules Engine), Business Objects, Pentaho, CSOne, Salesforce, Windows.', u""Business Analyst\nTata Tele Service Limited\nAugust 2011 to November 2014\nINDIA Aug 2011 - Nov 2014\nDescription: Tata Teleservices Limited spearheads the Tata Group's presence in the telecom sector. The Tata Group includes over 90 companies, over 395,000 employees worldwide and more than 3.5 million shareholders.\nRole: Business Analyst\nResponsibilities:\n* Acted, as an effective liaison between the technical team and the business team to ensure smooth flow of communication existed for the project\n* Framed a detailed agenda for JAD and facilitated JAD sessions between the various teams involved in the development of the application\n* Proficient in Product development from beginning through development, implementation, testing and release.\n* Delivered all key documentation for the above including Business Case, BRD, FRD, Use cases, Business Rules, & Process flows\n* Provided testing support to the testing team. Actively participated in overnight app releases, running test cases, revivifying defects and bugs and properly communicating to the required\n* Reported on key metrics and indicators to ensure effective and accurate communication to Senior Management on the project progress and milestones.\n* Requirements walk-through and single point of contact for Development and QA\n* Facilitated QA and end user testing of the application from end to end\n* Using SDLC and Microsoft Project planned score, cost, requirements and risk analysis of four-person web application development team in Visual Studio 2003, ASP .Net, VB .Net and SQL Server 2000.\n* Provided all aspects of support for both the 2007 and 2010 SharePoint Environments - mange Service Desk request, document issues.\n* Identified candidate business processes which leverage SharePoint tools to enhance day-to-day office functions using workflows, Info Path forms and Corasworks.\n* Participated in review, revisions, UAT, documentation and user notification of the migration from Lotus Notes to SharePoint.\n* Created, assigned and maintained tickets in JIRA for bugs and defects.\n* Worked directly with Product manager, third Party, Dev team and off shore testing teams.\n* Facilitated timely meetings between key stakeholders including Dev and QA UAT to ensure timely implementation and testing.\n* Support end user requests for new saved searches, reports, KPI's, and dashboards. Monitor end-user usage of systems and track performance.\n* Managed the product feature backlog for the end to end implementation - worked with HR business partners and SMEs to contribute to and effectively implement key asks into the release.\n\nEnvironment: MS Visio, Rational Requisite Pro, Agile, UML, EBS R12, Netsuite CRM+, MS Office -Excel, Word, PowerPoint.""]",[],[]
0,https://resumes.indeed.com/resume/19422f2585ad53a3,"[u'Data Analyst\nSun Life Financial - Gurugram, Haryana\nDecember 2015 to June 2017\n\xb7 Optimized SQL queries and views for reporting applications by fine-tuning the indexes; improved processing time by 12 %\n\xb7 Computed trend analysis on Active vs Terminated Contracts. Built production similar data utilizing R and predicted policies with high probability of termination employing SAS predictive analytics and implemented visualizations on Tableau, resulted in reducing terminated policy by 14%\n\xb7 Engineered process to provide graphical representation using Tableau for Agent satisfaction index by capturing results from 13 different reports; resulted in increasing agent satisfaction index by 2 points\n\xb7 Collaborated with client to understand data needs, built visualization models using Tableau representing Client Demographic; facilitated the organization to understand client requirement\n\xb7 Automated KPI measurement for 9 teams by developing 9 new Jira dashboards; saved 80 manual hours per month', u'Software Engineer\nNTT DATA Services - hyderbad, Telangana\nNovember 2013 to June 2015\n\xb7 Re-engineered process for processing Tax-Free Savings Account policies; proposed a new Java \u2013 COBOL hybrid architecture; reduced project upgrade cost by 100,000 Dollars; received Gem Award from the Project Director for outstanding contribution\n\xb7 Played instrumental role in migrating legacy mainframe applications designed to handle Business Owner application to JAVA\n\xb7 Estimated cost and evaluated market risk of moving to DevOps compatible development environment, analyzed various software tools, calculated project feasibility; resulted in implementation of agile methodology in the development process\n\xb7 Devised optimized process for unit testing mainframe applications, cut down unit testing time by 16% per project\n\xb7 Executed root cause analysis for critical mainframe applications; ensured 100 % batch availability\n\xb7 Created custom reports to track batch running stability using SAS and SQL; increased batch stability by 19%\n\xb7 Conducted POC for ETL (Extract, Transform and Load) of DB2 tables to test the feasibility of moving to Oracle database\n\xb7 Led a team of 3 members to implement cross team audit process, lowered project documentation error by 23%']","[u'Masters In Management Information System in Management Information System', u'in Bachelor of Technology in Electronics and Communication Engineering']","[u'Mays Business School, Texas A&M University\nJune 2017 to May 2019', u'University of Allahabad (Central University of India) Allahabad, Uttar Pradesh\nAugust 2009 to May 2013']"
0,https://resumes.indeed.com/resume/29ad1120abb9bddd,"[u'Data Analyst\nThe Publisher Desk - New York, NY\nApril 2017 to November 2017\n\u2022 Implemented display ad campaigns using googles DFP (DoubleClick for Publishers)\n\u2022 Monitored ad campaigns to ensure optimal delivery and performance\n\u2022 Stored and generated ad performance and company revenue reports via spreadsheets in MS Excel', u'Pharmacy Technician\nCVS Pharmacy - Brooklyn, NY\nApril 2016 to May 2017\n\u2022 Processed prescriptions and dispensed appropriate medications\n\u2022 Submitted insurance claims and resolved rejected claims\n\u2022 Performed clerical tasks, such as answering phones and maintaining prescription records', u'Data Research Analyst\nCmple.com Inc - Brooklyn, NY\nJuly 2014 to January 2016\n\u2022 Entered appropriate statistical information into the database\n\u2022 Analyzed the online marketplace and adjusted product prices accordingly\n\u2022 Prepared shipments and generated barcode labels\n\u2022 Monitored inventory to ensure adequate supply levels']","[u'Bachelor of Technology in Computer Systems', u'Associates of Arts']","[u'CUNY New York City College of Technology\nMarch 2019', u'CUNY Kingsborough Community College\nJune 2016']"
0,https://resumes.indeed.com/resume/190b600b0efe3d9f,"[u""Optician\nVision 4 Less - Moline, IL\nMarch 2012 to Present\n\u2022 Assist clients in selecting frames ensuring proper fit in relation to clients' bridge and eye size, temple length, vertex and pupillary distance, optical centers of eyes and determined prescription.\n\u2022 Instruct clients how to wear and care for their glasses.\n\u2022 Assemble eyeglasses by cutting and edging lenses, and properly fitting them into frames.\n\u2022 Perform administrative duties such as order and purchase frames and lenses, tracking inventory and sales, submitting insurance claims, and performing simple bookkeeping."", u""Waitress\nApplebee's - Oklahoma City, OK\nMay 2008 to July 2011\n\u2022 Maintained knowledge of current menu items, garnishes, ingredients, and preparation methods.\n\u2022 Delivered exceptional service by greeting and serving customers in a timely, friendly manner.\n\u2022 Regularly checked on guests to ensure satisfaction with each food course and beverages.\n\u2022 Managed closing duties, including restocking items and reconciliation of the cash drawer."", u""Data Analyst\nAlpha Plus Learning - Oklahoma City, OK\nSeptember 2007 to April 2008\n\u2022 Developed educational materials and programs for low-scoring school districts throughout the state of Oklahoma.\n\u2022 Created simulated standardized benchmark tests for Reading and Math, grades 3 - 8, English I, English II, English III, Algebra I, Algebra II, and Geometry aligned with Oklahoma's PASS skill requirements.\n\u2022 Participated in all stages of manuscript preparation, workbook production, and creating learning aids such as posters and flash cards, to prepare students for standardized testing.\n\u2022 Proofread, reviewed, and revised all print and electronic content for correct grammar and adherence to house style.\n\u2022 Maintained company website content, including test score results for numerous school districts throughout the state. Worked closely with Principals to assist with website navigation and discuss test results.\n\u2022 Compiled statistical information for special reports such as test data reports (Principal Report, Class Report, Standards At-Risk, Learner Test Report and Teacher Efficiency Report).\n\u2022 Packaged and shipped (or delivered) finished products to schools to meet deadlines.""]",[u''],"[u'Sherrard High School Sherrard, IL\nJanuary 2007']"
0,https://resumes.indeed.com/resume/8e3ec9e23e5ad26b,"[u""Library Data Analyst\nUniversity of Bridgeport - Bridgeport, CT\nAugust 2017 to Present\n\u2022 Performs regular tasks and special projects involving data collection, extraction, analysis, and reporting; database management; and administration of large-scale assessments\n\u2022 Coordinates collection, publication and distribution of library statistics and surveys\n\u2022 Enhancing the Library's data intelligence capabilities through analysis, visualization and automation of reporting processes\n\u2022 Extract, validate and manipulate data from a range of enterprise systems and Library data sources will provide strategic insights into UB's publishing trends, research impact, and Library collections\n\u2022 Develops databases and on-line statistical collection"", u'Data Analytics Intern\nUniversity of Bridgeport - Bridgeport, CT\nAugust 2016 to August 2017\n\u2022 Successfully interpreted data to draw conclusions for managerial action and strategy\n\u2022 Used statistical techniques for hypothesis testing to validate data and interpretations\n\u2022 Used advanced Microsoft Excel to create pivot tables, used VLOOKUP, and other Excel functions\n\u2022 Extracted data from marketplaces as per the need and transformed unstructured data into structured model and loaded it to the database, so it can be used for the analysis and business initiatives\n\u2022 Used Tableau analysis tool to visually display the Data Maps, create reports and dashboards.\n\u2022 Handled large databases of 1TB and extracted SQL data into Excel for validation and visualization purpose\n\u2022 Designed SQL database for Entity Relationship Diagram(ERD) and normalized data for better analysis']","[u'M.S. Technology Management in Business Intelligence', u'B.E. in Electronics and Telecommunication']","[u'University of Bridgeport Bridgeport, CT\nMay 2018', u'University of Mumbai Mumbai, Maharashtra\nJune 2014']"
0,https://resumes.indeed.com/resume/c42268af6860d9c7,"[u'Data Analyst (Contractor)\nCSG Solutions - Fayetteville, NC\nDecember 2016 to June 2017\nfor classified materials\n\u2022 Data Analysis and Exploitation for raw data to be organized and arranged into a workable product.\n\u2022 Query downloads and analytical research for raw data models to be analyzed and exploited within reasonable deadlines\n\u2022 Excellent working knowledge and leadership given to accomplish tasks independently of supervision', u'Data Analyst (Contractor)\nUnited States Air Force Reserve - Hampton, VA\nJuly 2014 to January 2017\n\u2022 Exploits and analyzes multisensory imagery and geospatial data and products in conjunction with all source intelligence information.\n\u2022 Coordinates with mission team to plan missions, maintain collection list, identify collection sequence and provide specific target requirements.\n\u2022 Assist in the identification of key features and determining optimal sensor selection and exploitation of parameters and the assessment of weapons impact and effects']","[u'Bachelor of Science in Information Technology Management', u'Associate of Arts in University Transfer', u'']","[u'American Military University Charles Town, WV\nJanuary 2019', u'Wake Technical Community College Raleigh, NC\nAugust 2015 to December 2016', u'Wake Technical Community College']"
0,https://resumes.indeed.com/resume/605c95bd19e5f9d9,"[u'Data Analyst\nCeratizit India Pvt. Ltd\nMay 2015 to July 2016\n\u2022 Analyzed customer orders using SQL queries and Excel to forecast sales and inventory, reducing shipping costs\n\u2022 Performed in-depth analysis on tool parameters data, optimizing the same to reduce tool failure by 40%\n\u2022 Created visualizations and interactive dashboards in Tableau to develop strategies, achieving 18% hike in sales', u'Business Analyst\nHindustan Prefab Ltd\nApril 2013 to April 2015\n\u2022 Interacted with project stakeholders to prepare project proposals as per the gathered functional requirements\n\u2022 Automated data gathering process by performing system analysis of a mobile application for project monitoring\n\u2022 Created context, use case, BPMN diagrams and logical data model to provide detailed view of the improved system\n\u2022 Performed PERT/CPM analysis in MS Project for project planning and allocating resources to meet milestones']","[u'M.S., Management in Information Systems', u'B.Tech. in Manufacturing Technology']","[u'The University of Texas at Dallas Richardson, TX\nDecember 2017', u'Uttar Pradesh Technical University Lucknow, Uttar Pradesh\nJune 2012']"
0,https://resumes.indeed.com/resume/326b8fcf9861483f,"[u""Data Analyst\nPP&D - Woodbr\nApril 2015 to March 2018\n\u2022 Worked along CPA's in a team-based environment to create forensic accounting reports for use in litigation, arbitration and mediation\n\u2022 Orchestrated money tracking initiatives to detect fraud and ensure funds were properly allocated or to calculate damages if they were not\n\u2022 Utilized specialized knowledge of foreign exchange rates to investigate charges of undeclared income on an client with money overseas\n\u2022 Managed forecasting salaries for usage in collective wage bargaining including police and firefighter organizations\n\u2022 Calculated average cost of living expenses for usage in marital disputes and divorce cases""]",[u'BA in Mathematics'],"[u'Rut New Brunswick, NJ\nJanuary 2012 to May 2014']"
0,https://resumes.indeed.com/resume/866ca462c5e09bfc,"[u""Assistant Manager\nMetro PCS - New York, NY\nMarch 2017 to December 2017\n\u2022 Contributed to merchandising ideas at team sale meetings\n\u2022 Analyzed marketing information and translated it into strategic plans\n\u2022 Trained staff to deliver outstanding customer service\n\u2022 Completed weekly schedules according to payroll policies\n\u2022 Possessed a competitive spirit and desire to meet and exceed sales goals\n\u2022 Understood customer's needs and help them discover how products meet those needs\n\u2022 Stayed up to date on latest data, technology and devices"", u""Data Analyst\nBobby's Department - New York, NY\nJune 2016 to June 2017\n\u2022 Provided analytical support to the accounting team\n\u2022 Prepare data for inputting into databases\n\u2022 Input data in an accurate and efficient manner\n\u2022 Created purchase orders for companies\n\u2022 Update records and information in the database\n\u2022 Ability to analyze potential cost reduction opportunities""]","[u'in Liberal Arts', u'in Business Management']","[u'CUNY Kingsborough Community College\nSeptember 2017 to December 2017', u'CUNY Brooklyn College\nJanuary 2015 to December 2015']"
0,https://resumes.indeed.com/resume/cade746840d501b0,"[u'Data Analyst\nSPSS China - Shanghai, CN\nJune 2006 to August 2008\n\u2022 Extracted and manipulated data to create wide tables using SQL.\n\u2022 Performed data analysis and data mining projects using SPSS or SPSS Clementine for clients, such as High-Value Customer Churn prediction for telecom clients.\n\u2022 Trained clients about methodology of data mining CRISP-DM and the use of SPSS Clementine.', u'Data Analyst\nShanghai Magnsoft Consulting - Shanghai, CN\nJune 2004 to June 2006\n\u2022 Extracted and aggregated data to create wide tables using SQL for further analysis and modeling.\n\u2022 Analyzed the effectiveness of promotions for a supermarket company.\n\u2022 Performed club member persona for a famous magazine company.\n\u2022 Constructed predictive models, such as Bills of Entry Fraud Detection Models.']","[u'MS in Applied Statistics', u'MS in Mathematical Statistics and Probability', u'B.S. in Mathematic']","[u'California State University Hayward, CA\nSeptember 2015 to September 2017', u'Huazhong University of Science and Technology Wuhan, CN\nSeptember 2001 to June 2004', u'Central China Normal University San Francisco, CA\nSeptember 1997 to June 2001']"
0,https://resumes.indeed.com/resume/5029feccda05b1c4,"[u'Tableau/Data Analyst\nBed Bath & Beyond - Elizabeth, NJ\nJanuary 2017 to Present\nDescription:\nThe Department Supervisor in Bed Bath & Beyond stores act in a leadership role to promote outstanding service and ensure smooth daily operations. Our Department Supervisors are responsible for assisting Store and Assistant Managers in executing daily priorities, in leading the store staff in the absence of the Store and Assistant Managers, and in maintaining specific areas or departments within the store.\n\nResponsibilities:\n\u2022 Interaction with the business users/ top management to gather requirements and turn them into BRD\'s and FRD\'s.\n\u2022 Documented all phases of project life cycle (requirement gathering, planning, development, testing, Execution).\n\u2022 Worked closely with senior management, cross departmental teams and attended weekly status meeting.\n\u2022 Analyzed data from various source systems and performed ETL to load data to the backend databases.\n\u2022 Created logical/ physical/Dimensional models of the different databases used and also developed Source to Target mapping document using Erwin.\n\u2022 Create custom Excel-based data entry forms that (a) validate data with extracts from the Clarity database (via ODBC) and (b) automate input into Clarity via the XML.\n\u2022 Developed custom portlets using Clarity Management tool.\n\u2022 Created Project Management dashboard in Tableau using Clarity database.\n\u2022 Automated reporting solutions and documentation that resulted in 100 % reduction in manual process.\n\u2022 Re-engineered existing data feeds thus saving ~ 5 to 8 hours of development time.\n\u2022 Performed data cleansing using SQL queries to remove data redundancies.\n\u2022 Created extensive joins for data blending from several different data sources.\n\u2022 Extensive use of calculated fields, parameters, calculations, groups, sets and hierarchies in Tableau.\n\u2022 Created sheet selector to accommodate multiple chart types like Side by Side bars, Stack bars, circle graphs etc. in a single dashboard by using parameters.\n\u2022 Created complex dashboards on YTD, MTD, Trailing Month equity, funds, broker visits from Bloomberg vendor data to provide powerful insight of the brokers and maximum investments done in the funds.\n\u2022 Extensively used calculated fields for different logics for Trend, Extended price calculations for the different cost types.\n\u2022 Created Market Compliance Reports for tracking, reviewing and approving the marketing communications within Legal and Compliance department.\n\u2022 Converted the legacy SSRS Reports into interactive Tableau dashboards.\n\u2022 Created Tableau reports on ""Tableau Server Group/User Permissions"" for auditing purposes.\n\u2022 Developed several reports to capture Tableau\'s meta data information using PostgreSQL.\n\u2022 Extensive use of SQL server 2012 for querying the raw data and then running complex queries using Joins, subqueries and aggregate functions.\n\u2022 As an Admin Created sites, added new users, published data refresh extract, scheduling subscriptions, configured row and object level securities based on Users and groups.\n\u2022 Configured data extractions by schedule, every business day at 7 am to enhance performance.\n\u2022 Developed Standard corporate Templates, Naming standards, executed RE of databases using Erwin, maintained Erwin Model Mart.\n\u2022 Worked on importing Tableau DS into Erwin and exporting ER diagrams out of Erwin to SharePoint site.\n\u2022 Led investigations on server performance using Tableau\'s Log Files.\n\u2022 Created SSIS package for daily email subscriptions to alert Tableau subscription failure using the ODBC driver and PostGreSQL database.\n\u2022 Used TWB Auditor to audit the workbooks / data sources, tables, columns used in a particular database directly from the server.\n\u2022 Used TabJolt for Tableau server performance and load testing.\n\u2022 Performed routine maintenance of Tableau server by optimizing and decommissioning internal resource usage.\n\u2022 Developed dashboards showing the render times of different views / workbooks within all sites and also calculated the execution times/ delay times\n\u2022 Provided overall support in troubleshooting Tableau desktop/Server problems within IT, Investment Services, Corporate Applications.\n\u2022 Actively participated in quarterly Tableau user group meetings within the organization.\n\u2022 Participated in creating Erwin SharePoint Project site and helped in the enhancement of the site.\n\u2022 Actively participated in the Erwin training sessions as a part of the learning process for the project.\n\u2022 Provided guidance in gathering the training requirements and collaborated with other team members on setting up the Erwin training process.\nENVIRONMENT:\nTableau Desktop9.x, Tableau Server 9.x,SQL Server 2012/2014, MS Visual Studio Professional 2013, MS Access, TWB Auditor, SSIS, Tab Migrate, Erwin DM workgroup edition 9.64, CA Erwin Model Mart, SharePoint 2013, CA Clarity PPM.', u""Data Analyst/Modeler\nWhirlpool - Saint Joseph, MI\nSeptember 2015 to January 2017\nDescription:\nAt Whirlpool, prudent risk-taking, innovation and empowerment are fostered. Furthermore, each employee is additive and brings to the organization unique skills and attributes which impact the business we share. Whirlpool Corporation is consistently ranked as one of the top 25 most reputable U.S. We collectively embrace and deliver focused on our enduring values: Respect, Integrity, Diversity & Inclusion, Teamwork and, Spirit of Winning; we help our employees, consumers, shareholders and communities accomplish their goals so they can make the most of moments that matter. Companies and as one of Fortune Magazine's most admired companies. Our vision is to offer the Best Branded Consumer Products in Every Home around the World through our mission: Create Demand and Earn Trust Every day.\n\nResponsibilities:\n\u2022 Effective interaction with the business users to gather the business requirements and prepare the BRD's.\n\u2022 Worked closely with the SME's and the Project Manager and attended daily meetings involving the business users, VP, offshore team and also with the implementation partners.\n\u2022 Worked with the DBA's to walk them through the conceptual data model to meet the business requirements using ERWIN tool.\n\u2022 Analyzed Source data and created source to target mapping documents to define the work flow from source system PDS to target system SAP cloud based application.\n\u2022 Built several dashboards using Supply Chain Data to support week based reporting efforts for C-Level and Senior Audiences.\n\u2022 Created dashboards with HRIS dataset for following subject areas: Organization Management, Personnel Administration (including Payroll, Benefits, Compensation and Time Management components).\n\u2022 Created Tableau Dashboards using stack bars, bar graphs, charts, scattered plots.\n\u2022 Used parameters, calculated fields to analyze the payroll data for the employees depending on conditions.\n\u2022 Involved in creating dashboards and reports in Tableau and Maintaining server activities, user activity, and customized views on Server Analysis.\n\u2022 Created line graphs to differentiate the earnings and deductions for an employee over the period of time.\n\u2022 Extensive use of blended axis, log axis and dual lines.\n\u2022 Provide database coding to support various business applications using T-SQL.\n\u2022 Imported standard reports, ad hoc reports from Success Factors with a new all the employee and contributor information.\n\u2022 Provide different test case scenarios using advanced excel features like VLOOKUP, Match, Exact function.\n\u2022 Create, customize & share interactive web dashboards in minutes with simple drag & drop method and access dashboards from any browser or tablet.\n\u2022 Extensive use of SQL server 2012 and used complex queries to query the SQL database, used date functions, joins and sub queries to extract the necessary information.\n\u2022 Hands on experience with the SAP cloud based application system. Created login credentials for a particular person and also load personal information in that application.\n\u2022 Documented procedures on data replication from Success Factors to SAP application.\nENVIRONMENT:\nTableau 9.x, SQL Server, SSIS, SAP, Success Factors, Microsoft Office Suite (Word, Excel, PowerPoint)."", u'Tableau Developer\nNorthSouth GIS LLC - Los Angeles, CA\nAugust 2014 to July 2015\nDescription:\nThe Office of the Minnesota Secretary of State seeks an Executive Assistant and Scheduler to provide confidential, high-level administrative and scheduling support to the Secretary of State so that all needs are met and the administrative work of the office is carried out in an accurate and professional manner. The incumbent will support senior leadership in the operations of the office.\n\nResponsibilities:\n\u2022 Involved in gathering Business Requirements, Interaction with the production users, Project Manager and SMEs.\n\u2022 Worked with DBAs to design and implement Conceptual data model, Physical Data Model, SSIS mappings and workflows to meet business requirements using ERWIN.\n\u2022 Responsible for creating ETL mapping documents to define data flow from source system to target database.\n\u2022 Worked with Source system Subject Matter Expert (SME) to ensure that the extracts are properly mapped.\n\u2022 Worked on the backend SSAS cube development which was the source data for the tableau.\n\u2022 Identified Facts, Dimensions, Levels / Hierarchies of Dimensional Modeling.\n\u2022 Designing and deploying Tableau Reports to the server with required security constraints\n\u2022 Drew upon full range of Tableau platform technologies to design and implement proof of concept solutions and create advanced BI visualizations.\n\u2022 Created extensive joins for data blending from several different data sources.\n\u2022 Extensive use of calculated fields, parameters, calculations, groups, sets and hierarchies in Tableau.\n\u2022 Advanced working knowledge on Individual Axes, Blended Axes, and Dual Axes.\n\u2022 Extensively used calculated fields for different logics for Trend, Extended price calculations for the different cost types.\n\u2022 Found efficient ways to make tables and graphs which were visually easy to understand and at the same time maintaining the accuracy of the core information content.\n\u2022 Used Sql Queries at the custom Sql level to pull the data in tableau desktop and validated the results in Tableau by running Sql queries in Sql developer.\n\u2022 Attended daily standup meetings to update scrum master with what is achieved, what is being done and is there any impediment.\n\u2022 Conducted user acceptance testing and mentored business users how to slice and dice data using filters and parameters. Proper use of context and traditional filters for performance improvement.\n\u2022 Performance improvement using JavaScript API for creating filters, navigation between sheets.\n\u2022 Created and maintained Log Files for easy debugging of issues in production server\n\u2022 Published the developed dashboards into production and ensured the data was up to date by scheduling extract refreshes, performed performance tuning of the dashboard and created tableau functional design document.\n\u2022 Participated in the retrospective meetings and next sprint planning meetings.\nENVIRONMENT:\nTableau (Desktop/Server), Teradata, SSIS, Oracle, Microsoft SQL Server, SSAS, Salesforce, ERWIN.', u""Data Analyst/Tableau Developer\nHDFC - Bengaluru, Karnataka\nJune 2013 to July 2014\nResponsibilities:\n\u2022 Effectively interacted with Business Analysts and Data Modelers and defined Mapping documents and Design process for various Sources and Targets.\n\u2022 Gathered user requirements, analyzed and designed software solution based on the requirements.\n\u2022 Develop, Organize, manage and maintain graphs, tables, slides and document templates for the efficient creation of reports.\n\u2022 Interact professionally with diverse group of professionals in the organization including managers and executives.\n\u2022 Developed Standard Operating Procedures (SOP) for Monitoring.\n\u2022 Extensively used data blending, embed functionalities in Tableau.\n\u2022 Deployed Tableau Server in clustered environment by mapping server nodes to primary machine.\n\u2022 Building, publishing customized interactive reports and dashboards, report scheduling using Tableau server. Creating New Schedule's and checking the task's daily on the server.\n\u2022 Created Tableau scorecards, dashboards using stack bars, bar graphs, scattered plots, geographical maps, Gantt charts using show me functionality.\n\u2022 Worked extensively with Advance analysis Actions, Calculations, Parameters, Background images, Maps.\n\u2022 Trend Lines, Statistics, and Log Axes. Groups, hierarchies, Sets to create detail level summary report and Dashboard using KPI's.\n\u2022 Gathered user requirements, analyzed and designed software solution based on the requirements.\n\u2022 Interacted with Business Analysts and Data Modelers and defined Mapping documents and Design process for various Sources and Targets.\n\u2022 Defined best practices for Tableau report development and effectively used data blending feature in tableau.\n\u2022 Create, customize & share interactive web dashboards in minutes with simple drag & drop method and access dashboards from any browser or tablet.\n\u2022 Connect to Oracle directly, created the dimensions, hierarchies, and levels on Tableau desktop.\n\u2022 Creating users, groups, projects, Data connections, settings as a tableau administrator.\n\u2022 Involved in creating dashboards and reports in Tableau and Maintaining server activities, user activity, and customized views on Server Analysis\n\u2022 Involved in creating database objects like tables, views, procedures, triggers, functions using T-SQL to provide definition, structure and to maintain data efficiently.\n\u2022 Implemented data security by creating user filters.\n\u2022 Provided 24/7 production support for Tableau users.\nENVIRONMENT:\nTableau (Desktop/ Server), Oracle, SQL Developer, Teradata, SQL Assistant, HPQC, Microsoft Office."", u""Data Analyst\nPfizer Inc - Bengaluru, Karnataka\nMay 2012 to April 2013\nResponsibilities:\n\u2022 Responsible for meetings with users and stakeholders to identify problems, resolve issues and improve the process to ensure a stable and accurate solution\n\u2022 Created Use Case Diagrams, Activity Diagrams, Sequence Diagrams and ER Diagrams in MS Visio\n\u2022 Prepared High Level Logical Data Models and BRD's (Business Requirement Documents) supporting documents containing the essential business elements, detailed definitions, and descriptions of the relationships between the actors to analyze and document business data requirements\n\u2022 Prepared Logical Data Models that contains set of diagrams and supporting documents containing the essential business elements, detailed definitions, and descriptions of the relationships between the data elements to analyze and document business data requirements\n\u2022 Created Interface test cases, report test cases, expected results validation, defect tracking using clear quest, version control using Clear Case\n\u2022 Used SQL queries for organizing and abstracting data from MS access databases, created reports, forms on MS Access\n\u2022 Captured Data flow diagrams and formatting of the data flowing through interfaces\n\u2022 Accumulated system requirements from various departments like accounting, retail, IS (Information Systems) through surveys and interviews\n\u2022 Conducted JAD sessions with management, SME, vendors, users and other stakeholders for open and pending issues.\n\u2022 Provide database coding to support business applications using T-SQL.\n\u2022 Incorporated Rational Unified Process (RUP) to create Business Requirement Document Specifications using MS Visio and MS Word\n\u2022 Planned and defined system requirements to Wire Frames with Use Case, Use Case Scenario and Use Case Narrative using the UML (Unified Modeling Language) methodologies\n\u2022 Involved in Logical & Physical Data Modeling. Database Schema design and modification of Triggers, Scripts, and Stored Procedures in Database Servers\n\u2022 Configured the Data mapping between Oracle and SQL Server 2005\n\u2022 Prepared Logical Data Models that contains set of diagrams and supporting documents containing the essential business elements, detailed definitions, and descriptions of the relationships between the data elements to analyze and document business data requirements\n\u2022 Created Use Case Diagrams, Activity Diagrams, Sequence Diagrams and ER Diagrams in MS Visio\n\u2022 Facilitated user acceptance testing and test strategies with Information Systems Group.\n\u2022 Perform quality assurance and testing of SQL server environment.\n\u2022 Worked with Source system Subject Matter Expert (SME) to ensure that the extracts are properly mapped. Used SQL for data mapping and querying.\n\u2022 Performed manual front-end testing to check all functionalities of different modules\n\u2022 Suggested measures and recommendations to improve the current application performance with the aid of SCR's (Small Change Requests)\n\u2022 Developed strategies with Quality Assurance group to implement Test Cases in Mercury Test Director for UAT (User Acceptance Testing)\nENVIRONMENT:\nInformatica, Oracle, TOAD, SQL, Share Point, TFS, Agile-Scrum, MicroStrategy, AutoSys, UNIX, SQL Server, ERwin, MS VISIO, Microsoft Office (Word, Access, Excel, Outlook, Project).""]","[u""Master's""]",[u'']
0,https://resumes.indeed.com/resume/354d5f487001bf0b,"[u'Data Analyst\nTata Consultancy Services\nAugust 2013 to July 2016\n\u2022 Analyzed customer and market data for telecom client and identified target segments and product growth drivers using SQL and Python, visualized key insights using Tableau.\n\u2022 Analyzed multiple attributes of customer demographic data and built predictive models using regression to identify most significant factors driving revenue.\n\u2022 Automated IVRS announcements for customers by analyzing payment data from multiple servers and increased resolution rate.\n\u2022 Implemented parallel processing in PL/SQL using dbms_scheduler in dunning process and increased revenue by $85K per month.\n\u2022 Executed E-mail campaigns and SMS notifications to consumers using UNIX shell script and PLSQL in multi-system environment.\n\u2022 Discovered delinquent customers in online/offline payment systems by analyzing various consumer attributes.\n\u2022 Collaborated with the marketing and product teams to streamline operations and present insights on customer and market data']","[u'Master of Science in (MS) - Business Analytics', u'Bachelor of Engineering in Computer Science and Engineering']","[u'The University of Texas at Dallas Dallas, TX\nMay 2018', u'Rajiv Gandhi Technical University\nMay 2012']"
0,https://resumes.indeed.com/resume/3c358cee0fc8d5b8,"[u'Data Scientist Intern\nData Society - Washington, DC\nJune 2017 to October 2017\n\u2022 Outlining the course flow for Data Science courses and guiding a team of two interns\n\u2022 Creation of course content and exercises for advanced classification, clustering and Principal Component Analysis\n\u2022 Building a shiny dashboard using plotly, highcharter, leaflet and deploying on shiny server', u'Data Analyst Intern\nIndian Institute of Tropical Meteorology\nMay 2014 to March 2016\n\u2022 Analyzing 9 years of Clouds, Rainfall and Sea Surface Temperature data to study time series correlations, spatial correlations and finding dominant patterns using PCA\n\u2022 Comparing 45 years of rainfall and Sea Surface Temperature data based on bias and seasonal cycles\n\u2022 Presenting and discussing results with a group of scientists']","[u'Master of Science in Computational Science', u'Master of Technology in Modeling and Simulation', u'Bachelor of Engineering in Computer Engineering']","[u'George Mason University Fairfax, VA\nMay 2018', u'University of Pune Pune, Maharashtra\nJune 2015', u'University of Pune Pune, Maharashtra\nJune 2013']"
0,https://resumes.indeed.com/resume/6860d1b23f9f8d7f,"[u'Territory Coordinator\nLexisNexis - Oklahoma City, OK\nAugust 2003 to December 2010\nPrepared, reviewed and made decisions on such factors as to volume, frequency, currency, cost per record and other related factors. Analyzed and determined production projections for assigned territory and act as project lead for projects and initiatives. Managed activities ensuring effectiveness, accuracy, efficiency and alignment with customer requirements such as quality, timeliness, etc. Performed analysis (such as production projections/results, volume, frequency, historical information, revenue, identified and developed action plans. Developed and negotiated contract terms such as piece rates/etc. Developed and maintained knowledge and understanding of assigned states, territories, volume analysis, courts, and other related information. Compiled and maintained state, county and court specific information and managed work flow in areas such as business verification, public records, customer inquiries, and other related areas. Responded to customer inquiries with completeness in a timely manner.', u'Data Analyst Assistant\nLexisNexis - Oklahoma City, OK\nAugust 2000 to August 2003\nCreated, printed and mailed reports that Data Analyst needed. Worked on Spreadsheet reports for Department Manager. Did miscellaneous jobs that\nDepartment Manager needed to be completed.', u'Data Auditor\nLexisNexis - Oklahoma City, OK\nAugust 1999 to August 2000\nVerified data received from contractors was accurate. Maintained audit schedule to ensure all contractors were audited every six months. Kept spreadsheet to ensure that contractors returned all documents needed to complete audits.']",[u'Diploma'],"[u'Edmond Memorial High School Edmond, OK\nJanuary 1987 to January 1991']"
0,https://resumes.indeed.com/resume/29e4b56b25bcbb3f,"[u'Data Analyst\nLeasily, Co\nMay 2016 to January 2017\nPerformed exploratory analysis using Python for rental marketplace industry\n\nfor initial research of the startup.\n\n\u2022 Created financial reports, charts, graphs to visualize the expense of the project as a startup.\n\n\u2022 Graphical analysis of data for user adaption.\n\n\u2022 Analyzed key metrics for customer acquisition costa and projected impact on the market.']","[u'Master of Science in Computer Science', u'Bachelors of Engineering in Computer Engineering']","[u'Long Island University Brooklyn\nDecember 2017', u'Gujarat Technological University\nJune 2015']"
0,https://resumes.indeed.com/resume/5e982b4ae49d2872,"[u""Teaching Assistant\nIllinois Institute of Technology - Chicago, IL\nAugust 2017 to December 2017\n\u2022 Provided assistance for two MATH courses: 'Probability & Statistics' and 'Applied Statistics'. Date of birth\n\u2022 Responsible for assigning grades and solving students' queries. 1991-12-01"", u""Data Science Intern\nKnack.it - Chicago, IL\nMay 2017 to August 2017\n\u2022 Responsible for all data science tasks starting from data cleaning, data munging, data\nvisualization to applying machine learning models and model evaluation. Courses\n\u2022 Comprehended firm's products(gaming apps) and its requirement by providing data insights to the business problems. Applied Statistics\n\u2022 Deduced irregularity in game-play by quantifying the statistical unusualness of data from a\ngiven game session. Data Mining\n\u2022 Applied unsupervised machine learning techniques like PCA, Isolation Forest, DBSCAN, K- Machine Learning\nMeans for anomaly detection.\nData Preparation & Analysis"", u'Data Analyst\nCognizant - Pune, Maharashtra\nMarch 2014 to June 2016\n\u2022 Built and maintained star schema-based data warehouse.\nAdvance design & analysis of experiments\n\u2022 Developed SSIS packages and implemented SCD type I & type II Logic in ETL mappings and stored procedures. Project Management\n\u2022 Designed customized dashboards and SSRS reports for business users. Public Engagement Systems\n\u2022 PL/SQL query optimization which reduced query run time.\n\u2022 Responsible for data gathering, data preparation, data extraction & data cleaning process.\n\u2022 Conducted statistical & graphical analysis on customers data to determine existing trends & Skills\npatterns.\nPL/SQL\n\u2022 Implemented classification & regression, performed model selection & model evaluation using\nmetrics like accuracy, rmse etc.\n\u2022 Communicated sophisticated machine learning concepts to customers and the business R (Statistical Analysis)\ncommunity.']","[u""Master's in Data Science""]","[u'Illinois Institute of Technology Chicago, IL\nAugust 2016 to December 2017']"
0,https://resumes.indeed.com/resume/6db1dcf09bd99857,"[u""Data Analyst\nDassault Syst\xe8mes - Pune, Maharashtra\nJanuary 2015 to July 2017\nProfiled and built data visualizations and dashboards for tracking end-users' level of engagement with the company based\non industry KPIs using 3DX Platform and Tableau. Provided management a one-stop dashboard to track over 50 metrics\n\u2022 Built logistic regression model for Dassault Aviation(DA) to predict spot-weld failures under different testing environments\nusing past simulation results. The project yielded the Transportation and Mobility team two additional DA projects\n\u2022 Performed requirement gathering and gap analysis for different processes and documented them along with the as-is and to-be state, thereby exploring areas for process improvement/re-engineering for upcoming releases\n\u2022 Performed engineering analytics for Renault's self-driving car project using DS EXALEAD CloudView application. Analyzed\nreal-time and simulated car performance data to compare and improve product behavior. Reported findings for further\ndata assimilation and design changes by Renault"", u""Data Analyst\nHonda Cars India Ltd - Gurgaon, Haryana\nJuly 2013 to January 2015\nAnalyzed Honda's in-house production defect data of Press and Weld shop using R and presented results to Honda R&D,\nThailand. Findings were incorporated in the die design of Honda Mobilio, reducing production defects by 17%\n\u2022 Developed reports and dashboards using Tableau providing clear visualizations of industry-specific performance KPIs and presented key findings to executive management teams\n\u2022 Achieved operational efficiency of 94% in the Press automation line, an increase of 10% within 2 months. Predictive\nmaintenance plans were made by examining historical breakdown data of production lines from India and Japan\n\u2022 Visualized, text mined and examined reviews and feedbacks of former and current employees. Reported drivers\nresponsible for attrition of employees from company, as part of a special talent retention project of the HR department""]","[u'Master of Science in Business Analytics and Project Management', u'Bachelor of Technology in Industrial Engineering']","[u'University of Connecticut School of Business Hartford, CT\nAugust 2017 to December 2018', u'National Institute of Technology Bhopal, Madhya Pradesh\nJuly 2009 to April 2013']"
0,https://resumes.indeed.com/resume/3945204dbd35615e,"[u'Data Analyst\nBaylor University - Waco, TX\nJanuary 2015 to Present\n1. Implement data analysis to support the Anti-Cancer Drug Discovery Project at Baylor University.\n(i) Successfully migrated the database from Texas A&M University to Baylor University and upgraded SQL server 2008 to SQL server 2012;\n(ii) Managed and maintained databases with the data sets having exceeded more than 1 million records;\n(iii) Consolidated the data and performed data mining by BI tools to provide strategies for drug design;\n(iv) Generated formal and adhoc reports using SSRS and Power BI tools based on the project\u2019s requirements.\n(v) Presented the research at the Innovations in Cancer Prevention and Research Conference in 2015 and 2017.\n(vi) Three very potent anti-cancer drugs candidates were discovered and two patents were published.\n2. Business and Laboratory databases management for the Chemistry Department.\n(i) Worked together with DBA to setup the department\u2019s first chemical inventory databases and the purchase orders databases;\n(ii) Routinely loaded data by SSIS packages and stored procedures;\n(iii) Assisted business units to project spend across diverse categories and generated quarterly and annually financial reports; Prepared budget reports for Research Grant Proposal.\n(iv) Helped the department reduce ca. 20% costs in procurements in 2016 and 2017 fiscal years by practicing spend analysis.\n3. Gave tutorial seminars to students and colleagues on applying advanced Excel skills into scientific research.\n4. Utilizing in silico (computer-assisted medicine design) High Throughput Screen (HTS) campaign to generate hits for drug discovery.\n(i) Extracted the data of ~650,000 compounds from the Scripps HTS Data Library in the SDF (Structure-data file, holing information of chemical structures and properties) file format;\n(ii) Cleaned, modified and transformed the data following certain chemical criteria;\n(iii) Loaded ~1.2 million records of data to SQL server database; the data were evaluated by the computer modeling team at MD Anderson Cancer Center;\n(iv) Aggregated and grouped the top 1,000 hits of the evaluation data to identify a variety of drug candidates.', u'Data Analyst\nTexas A&M University - College Station, TX\nJanuary 2011 to January 2015\n1. Implemented data analysis to support the Anti-Cancer Drug Research Project at Texas A&M.\n(i) Designed, created and updated the SQL server database to effectively manage a large volume of data generated from the research with more than half million records;\n(ii) Optimized the original database to realize 3rd Normal Form; performed data analysis using BI tools to build a sophisticated understanding of the drug profiles;\n2. Business and Laboratory databases management for the Chemistry Department.\n(i) Maintained the chemical inventory and purchase orders databases;\n(ii) Optimized the process of data loading and aggregation by SSIS packages and stored procedures;\n(iii) Generated cascading, drill through, drill down business reports using SSRS, Crystal Reports and Excel.']","[u'in Chemistry', u'Ph.D. in Chemistry']","[u'University of Arkansas\nJanuary 2009 to January 2010', u'Martin-Luther University\nJanuary 2006']"
0,https://resumes.indeed.com/resume/c4adb507009363a8,"[u""Data Support Analyst\nIBM - Bengaluru, Karnataka\nAugust 2014 to June 2016\n\u2022 Providing Level 1-2 support and code fixes for PC Bank web applications built on Java/J2EE framework in production environment.\n\u2022 Providing both production support and data analysis support within the technology platform management organization.\n\u2022 Developed automated solutions of support tasks & created real time batch processing using Bash Scripting & automated logger to generate reports.\n\u2022 Developed and took ownership of dashboards of KPI's built using Tableau.\n\u2022 Performing SQL performance tuning, query optimization, indexing.\n\u2022 Analyzing malfunctions occurring in the existing database systems and providing recovery solutions for the data corrupted in the process.\n\u2022 Created SQL modules to integrate the existing data on to the database.\n\u2022 Application Deployment using IBM Websphere Application Server.\n\u2022 Writing SQL Queries for fault diagnostics, resolution or Change controls.""]","[u""Master's in Computer Science"", u""Bachelor's in Computer Science and Engineering""]","[u'University of North Carolina at Charlotte Charlotte, NC\nAugust 2016 to December 2017', u'University Visvesvaraya College of Engineering\nSeptember 2010 to June 2014']"
0,https://resumes.indeed.com/resume/ef84075c316ce0a1,"[u'Data Analyst\nSystemax - Global Industrial (NYSE SYX) - Port Washington, NY\nMarch 2018 to Present', u""Data Analyst Intern\nJetro Restaurant Depot - College Point, NY\nJune 2017 to September 2017\n\u2022 Extracted data using ETL, ODI and performed data Analytics using OBIEE.\n\u2022 Established key performance indicators to cleanup data and addressed root cause analysis to improve.\n\u2022 Created reports analyzing large-scale database utilizing Microsoft Excel Analytics within legacy system.\n\u2022 Manipulated classification rule sets to perform data mapping to improve system performance.\n\u2022 Created reports from several discovered patterns using Microsoft excel to analyze pertinent data by pivoting.\n\u2022 Implemented Data modelling with Star Schema and Snowflakes Schema in various departments.\n\u2022 Handled process for 2 department's workload and remodeled data structure to perform quality analysis."", u'Data Analyst\nAbhyudaya Multimedia - Indore, Madhya Pradesh\nJuly 2013 to September 2015\n\u2022 Developed data structures designing in database and migrated data using ETL tools.\n\u2022 Implemented Data modelling via Star Schema and Snowflakes Schema, defining facts and dimension.\n\u2022 Researched and addressed issues in legacy data models.\n\u2022 Assisted during the implementation of Business Intelligence in financial projects.\n\u2022 Utilized weblogic platform to control and manage services at server side.\n\u2022 Worked on the Physical layer and logical layer to create mapping.\n\u2022 Handled data operations of server side application.']","[u""Master's in Information Systems"", u""Bachelor's in Mechanical Engineering""]","[u'Pace University-New York New York, NY\nJanuary 2016 to December 2017', u'Rajiv Gandhi Technical University Indore, Madhya Pradesh\nAugust 2009 to June 2013']"
0,https://resumes.indeed.com/resume/cf7c8b791455a0a9,"[u'Service Adviser, data analyst\nDoc Auto - Santa Cruz, CA\nSeptember 2009 to February 2018\nService adviser for independent auto repair facility. Also business analyst for all areas of the company for last few years']","[u""Bachelor's in Environmental Economics"", u'MBA', u'']","[u'University of California Santa Cruz Santa Cruz, CA\nSeptember 2009 to June 2011', u'Lucas Business School- San Jose State University', u'UCSC']"
0,https://resumes.indeed.com/resume/02d17c021c74ee86,"[u'Marketing Data Analyst\nEmpowerM Mobility Sol - Hyderabad, Telangana\nDecember 2014 to May 2016\n\u2022 Designed Analytical database using SQL for analyzing, reporting and forecasting. Preprocessed the data using R for Churn Analysis and behavior analysis. Successfully aided in reducing the Customer Acquisition Cost by 23%.\n\u2022 Conducted A/B testing using R to improve Advertisement Effectiveness Tracking, Consumer Satisfaction, Product/Package Test, and usage Analysis of the web/mobile application and generate reports using Tableau.', u'Analyst\nHindustan Times Pvt. Ltd - Delhi, Delhi\nApril 2014 to December 2014\nPerformed Product Lifecycle analysis; established KPIs; designed and developed Tableau dashboard for sales data. Improved the sales by achieving 106% growth over previous year. Introduced various newspaper supplements to increase reader viewership and Circulation\nEnvironment: SalesForce Marketing Cloud, Tableau, and MS Office Suite']","[u'Master of Science in Business Analytics in Commerce', u'Diploma in Management', u'Bachelor of Engineering in Metallurgy and Materials Engineering in Metallurgy and Materials Engineering']","[u'Texas A&M University\nMay 2018', u'New Delhi Institute of Management Delhi, Delhi\nJune 2014', u'Mahatma Gandhi Institute of Technology Hyderabad, Telangana\nJune 2012']"
0,https://resumes.indeed.com/resume/a04e9c84f7488412,"[u'DATA ANALYST\nCapgemini - New York, NY\nMarch 2016 to Present\n\u2022 Develop neural networks software packages for spreadsheets (OpenOffice, MS Excel, etc.) using C++\n\u2022 Develop software tools using R in order to solve various business and client related needs.\n\u2022 Analyze business databases and create reports using Tableau and Excel.', u'DATA ANALYST\nRodan + Fields, LLC - San Francisco, CA\nOctober 2014 to February 2016\n\u2022 Performed data-driven decisions to increase sales and effectiveness of marketing efforts\n\u2022 Provided sales forecasting using predictive models and what-if scenarios\n\u2022 Created statistical models (Machine Learning algorithms, Regressions) and cluster groups (Kohonen maps, K-Means) for target marketplaces\n\u2022 Performed and visualized analysis of the export-import databases with Tableau, Pivot tables in Excel, etc.\n\u2022 Produced reports and/or data sets for ad hoc requests\n\u2022 Organized and aggregated data to obtain descriptive statistics for analyses', u'MARKETING DATA ANALYST\nFELIX - Moscow, RU\nJuly 2006 to September 2013\n\u2022 Led team to analyze marketing data, create reports and provide customer service\n\u2022 Trained new team members to understand marketing concepts and processes and to more effectively use MS Excel and other tools\n\u2022 Determined significant factors affecting successful sales\n\u2022 Investigated multiple data sources and variables for project use\n\u2022 Performed data-driven decisions to optimize warehouse assortment.\n\u2022 Analyzed sales data and trends using time-series forecasting algorithms\n\u2022 Performed quantitative and qualitative marketing surveys, consumer insights and analyzed results']","[u'PhD in Economics', u'Diploma in Chemistry', u'']","[u'Voronezh State Agricultural University', u'Voronezh State University', u'STANFORD']"
0,https://resumes.indeed.com/resume/7b52f6bfea73441e,"[u'Data Analyst\nSaras Analytics - Hyderabad, Telangana\nOctober 2014 to November 2015\nPrimary responsibilites included stakeholder communication, data collection, cleaning, feature selection, modeling and visualization to generate insights, predictions, and recommendations from the data.', u'Business Analyst\nGrail Research - Noida, Uttar Pradesh\nAugust 2013 to September 2014\nMain responsibilities included primary, secondary research, survey analysis, and data visualization']","[u""Master's in Data Science"", u""Bachelor's in Pharmaceuticals""]","[u'Indiana University-Bloomington Bloomington, IN\nJanuary 2016 to December 2017', u'Birla Institute of Technology and Sciences - Pilani Pilani, Rajasthan\nJune 2008 to August 2012']"
0,https://resumes.indeed.com/resume/5e411489540559fd,"[u'DATA ANALYST\nAOL Inc - Dulles, VA\nJanuary 2015 to Present\nRegional Lead analyst; reviewed and analyzed invoice revisions, solved for discrepancies, processed & approved credit memos in compliance with SOX regulations. \u2022Processed and approved contracts in SAP to ensure contract accuracy. \u2022Worked alongside customers, account managers, and deal management to resolve disputes. \u2022Balance sheet reconciliations; managed accrual process and revenue booking for all advertising entities. \u2022Extracted internal and external reporting data, and/or systems, using third party user interfaces and SQL based queries. \u2022Supported and handled all billing, reporting, and disputed cases in Salesforce, as well as Zendesk for my assigned pod/region.', u'Accountant\nOnsite Health, Inc\nJanuary 2012 to January 2015\n\u2022Compiled and analyzed financial information to prepare entries to accounts, such as general ledger and journal entries. \u2022Processed AP and AR while resolving discrepancies with collections. \u2022Performed bank reconciliations and maintained all bank loan accounts, liaised for all new accounts. \u2022Submitted audiology-hearing bills, with the CPT, ICD-9 codes to Veteran Administration. \u2022Monitored WAWF for submitted invoices and liaised with DFAS and Veterans Affairs for all receivables. \u2022Assisted in processing semi-monthly payroll, submitted for payments ETF. \u2022Prepared auditing documentation as support for external audit. \u2022Assisted with financial closing and preparation of financial statements']","[u'MBA in Accounting', u'B.S in GEOGRAPHY']","[u'University of Northern Virginia Annandale, VA\nMarch 2007 to December 2012', u'University of Tirana\nJanuary 1998 to January 2002']"
0,https://resumes.indeed.com/resume/2b785e576b8c7aef,"[u'Facility Attendant\nUniversity of Alabama at Birmingham Campus Recreation - Birmingham, AL\nApril 2017 to February 2018\n\u2022 Service, clean, or supply restrooms.\n\u2022 Gather and empty trash.\n\u2022 Clean building floors by sweeping, mopping, scrubbing, or vacuuming.\n\u2022 Follow procedures for the use of chemical cleaners and power equipment to prevent damage to floors and fixtures.\n\u2022 Notify managers concerning the need for major repairs or additions to building operating systems.\n\u2022 Clean windows, glass partitions, or mirrors, using soapy water or other cleaners, sponges, or squeegees.\n\u2022 Set up, arrange, or remove decorations, tables, chairs, ladders, or scaffolding to prepare facilities for events, such as banquets or meetings.\n\u2022 Clean and polish furniture and fixtures.\n\u2022 Dust furniture, walls, machines, or equipment.\n\u2022 Move heavy furniture, equipment, or supplies, either manually or by using hand trucks.', u""Move Master's Moving Company - Pelham, AL\nMay 2017 to May 2017\nAccountable for handling client property safely and efficiently\n\u2022 Coordinated pick-ups and deliveries with clients\n\u2022 Prioritized routes based upon job size and complexity\n\u2022 Breakdown and setup furniture items based on customer needs\n\u2022 Securely transport contracted items avoiding any damage"", u'Data Analyst\nCenter for Neurodegeneration and Experimental Therapeutics - Birmingham, AL\nOctober 2016 to February 2017\nCompare data with source documents, or re-enter data in verification format to detect errors.\n\u2022 Maintain logs of activities and completed work.\n\u2022 Locate and correct data entry errors, or report them to supervisors.\n\u2022 Store completed documents in appropriate locations.\n\u2022 Compile, sort and verify the accuracy of data before it is entered.']","[u""Bachelor's"", u'']","[u'University of Alabama at Birmingham Birmingham, AL\nDecember 2020', u'New Century Technology High School Huntsville, AL']"
0,https://resumes.indeed.com/resume/3f97cc958f2b5c6b,"[u'Data Analyst\nSPRINT FINANCIAL SERVICES - Irving, TX\nFebruary 2001 to October 2017\nThe daily activities of this role have consistently evolved during my tenure and currently support a data and analytics driven environment where research, analysis, reporting, and process improvement support\ndecision-making. This has served as the ideal training ground to leverage my unique skillset coupled with advanced knowledge of MS Excel and various other tools to create value-added reporting. This has culminated in becoming the subject matter expert on various teams and projects that focus on business optimization and improved performance.']",[u'High school or equivalent'],[u'American Home School Illinois']
0,https://resumes.indeed.com/resume/568a036e78f0b365,"[u'Data Analyst\nFINDREAM LLC - Houston, TX\nNovember 2017 to Present\n\u2022 Conducted exploratory data analysis and built predictive modeling (e.g. Random Forest) to predict business outcomes customers\u2019 request.\n\u2022 Visualized the correlation of different KPIs (Tableau) and reported to customer\u2019s senior leadership team.\n\u2022 Performed segmentation analysis based on consumer preferences to deliver verifiable and actionable client recommendation on customer service strategy.\n\u2022 Utilized Microsoft Excel and SQL to compile and validate data reinforce and maintain compliance with corporate standards.', u'Research Analyst Intern\nMD Anderson Cancer Center - Houston, TX\nJune 2016 to September 2017\n\u2022 Conducted data collection, cleaning, and analysis utilizing various statistical methods (regression models, ANOVA, hypothesis testing, etc.); quantified and visualized data to generate dashboards and reports by R/Shiny or Markdown.\n\u2022 Leveraged MDA 2016 Li-Fraumeni syndrom clinical source to test our algorithm LFSPRO results, and improved AUC to 0.91.\n\u2022 Co-wrote an R package in testing, correcting and wrapping an algorithm about a Bayesian variable selection method (https://cran.r-project.org/web/packages/BayesS5/index.html)\n\u2022 Utilized HTML and CSS to create a website for research Lab (http://odin.mdacc.tmc.edu/~wwang7/index.html)']","[u'Nano Degree in Machine Learning', u'M.S. in Statistics', u'B.S. in Mathematics']","[u'Udacity\nSeptember 2017 to January 2018', u'Rice University Houston, TX\nAugust 2015 to December 2016', u'University of Minnesota Minneapolis, MN\nSeptember 2010 to May 2014']"
0,https://resumes.indeed.com/resume/987f05644e1afc3a,"[u'Data Analyst\nHongkong Li & Fung Group\nJuly 2017 to August 2017\nl Conduct an ARIMA time series model to predict the terminated staff number in the next few months\nl Made some basic statistics and data visualization to display the distribution of terminated staff in Li & Fung Group\nthese years\nl Written a report to demonstrate the result of prediction', u'Business Assistant\nShenwan Hongyuan Securities CO., LTD. Shanghai FTZ Branch\nMay 2017 to June 2017\n\u2022 Had a basic knowledge of how security company works and participated in the weekly meeting\n\u2022 Collected and analyzed data of company profits\n\u2022 Revised business contract of Shenwan Hongyuan Securities CO., LTD. and other financial institution']","[u'Master of Science in Statistics', u'Bachelor of Science in Geophysics']","[u'University of Virginia Charlottesville, VA\nAugust 2016', u'Tongji University Shanghai, CN\nSeptember 2012 to June 2016']"
0,https://resumes.indeed.com/resume/0de3762e7d803077,"[u'Business Data Analyst\nCisco Systems - San Jose, CA\nDecember 2017 to Present\nResponsibilities\n\u2022 Gathered Business Requirements, Interacted with the Users, stakeholders and SMEs to get a better understanding of the Business Processes.\n\u2022 Effectively facilitates and participates in Joint Application Development sessions with project stakeholders to gather detailed business rules and functional requirements\n\u2022 Analyzing data and presenting actionable information to help sales leadership to make business decisions.\n\u2022 Develop and run ad hoc Data queries from multiple database types to identify system of records, Data inconsistencies, and data quality issues.\n\u2022 Extract the data from different data source and provide data analysis to the business user based on the requirements.\n\u2022 Used Data Loader for insert, update, and bulk import or export of data from Salesforce.com Objects. Used it to read, extract, and load data from comma separated values (CSV) files\n\u2022 Standardize company names, addresses, and ensure that necessary data fields are populated by making them mandatory fields in SFDC\n\u2022 Review the database proactively to identify inconsistencies in the data, conduct research using internal and external sources to determine information is accurate\n\u2022 Monitor the Database for duplicate records and merge the duplicate records and ensure that the information is associated with company records\n\u2022 Used advanced Excel functions, VLOOKUP, and pivot tables for data analysis\n\u2022 Involved in executing various SQL database queries from Python using Python SQL connector and SQL dB package\n\u2022 Designed and constructed real time dashboards for sales and marketing teams using Tableau\n\u2022 Created relationships, actions, data blending, filters, parameters, hierarchies, calculated fields, sorting, groupings, live connections, and in-memory in both Tableau and Excel.\n\u2022 Created Data Connections, Published on Tableau server for usage with Operational or Monitoring Dashboards.\n\u2022 Executed and tested required queries and reports before publishing and performed quality analysis with data validation by comparing raw data in SQL/Excel to Tableau Dashboards\n\u2022 Testing the reports and dashboards by validating data from database\nEnvironment: Oracle 11g, Python 3.x, SQL Server management Studio, SFDC, Tableau desktop and server 9.x/10.3, Microsoft Excel', u'Data Analyst\nSterling, OH\nJanuary 2017 to October 2017\nResponsibilities\n\u2022 Analyzed Business user and technical requirements for proposed projects system solutions.\n\u2022 Interacted with various business team members to gather the requirements and documented the requirements.\n\u2022 Performed Gap Analysis to check the compatibility of the existing system infrastructure with the new business\nRequirements.\n\u2022 Analyzed data by creating SQL queries and identified fact and dimension tables.\n\u2022 Created new database objects like Procedures, Functions, Packages, Triggers, Indexes and Views using SQL in\nSQL Server\n\u2022 Performed Data Analysis and Data validation by writing complex SQL queries using SQL against the SQL server database\n\u2022 Monitor, tune and analyze database performance and allocate server resources to achieve optimum database performance\n\u2022 Preparing Dashboards using calculations, parameters, calculated fields, groups, sets and hierarchies in Tableau\n\u2022 Extensive experience in various reporting objects like Facts, Attributes, Hierarchies, Transformations, Action filters, Calculated fields like YTD, MTD, Sets, Groups, Parameters etc., in Tableau\n\u2022 Extensively used advance chart visualizations in Tableau like Dual Axis, Bar chart, Pareto Chart, Heat Maps, Tree maps etc., to assist business users in solving complex problems\n\u2022 Created views in Tableau desktop that was published to internal team for review and further data analysis and customization using filters and actions.\n\u2022 Designed and Optimized Data Connections, Data Extracts, Schedules for Background Tasks and Incremental Refresh for the weekly and monthly dashboard reports on Tableau Server.\n\u2022 Scheduled data refresh on Tableau server for weekly and monthly increments based on business\n\u2022 Performed Data profiling using SQL queries on various sources.\n\u2022 Data conversions and data loads from various databases and file structures and also compared the target data with the source data\nEnvironment: Oracle 11g, SQL, Developer, UNIX, Tableau Desktop and server 8.x/9.x', u""Business Data Analyst\nThe Grounds of Alexandria - Sydney NSW\nDecember 2013 to October 2016\nResponsibilities\n\u2022 Coordinating with Business Analysts in requirement analysis meetings to gather requirements up to the mark\n\u2022 Created complex SQL queries, stored procedures, indexes, functions and views to support efficient data storage and manipulation\n\u2022 Designed Packages, Procedures, Functions and Triggers. Handle database performance and DB tuning.\n\u2022 Created and executed SQL Server Integration service packages to populate data from the various\n\u2022 Developed several Tables, Stored Procedures, functions and views to load data from the SQL server\n\u2022 Data sources, created packages for different data loading operations for many applications\n\u2022 Contributed from design to implementation and maintenance phase of the application in AGILE environment/methodology\n\u2022 Involved in integrating legacy applications into Salesforce.com SOAP & REST callouts\n\u2022 Developed SQL Queries to fetch complex data from different tables in remote databases using joins, database links and formatted the results into reports and kept logs\n\u2022 Developed work flows and data flows to extract, transform and load data in to the Business Intelligence Center using Data Integrator's designer\n\u2022 Performed efficient tuning of SQL source queries for data load/ Usage of stored procedures for performance\n\u2022 Administered user, user groups, and scheduled instances for reports in Tableau\n\u2022 Created Tableau scorecards, dashboards using stack bars, bar graphs, scattered plots, geographical maps, Gantt charts using show me functionality.\n\u2022 Trend Lines, Statistics, and Log Axes. Groups, hierarchies, Sets to create detail level summary report and Dashboard using KPIs.\n\u2022 Used Data Loader for insert, update and bulk import or export of data from Salesforce.com Objects and also used to extract and load data from comma separated values (CSV) files\n\u2022 Integration of data from different integration teams to Salesforce using Data Loader\nEnvironment: Oracle 11g, SQL Developer, Tableau desktop/Server 7/8.2, SFDC"", u'Data Analyst\nFlasto IT Solutions - IN\nNovember 2011 to September 2013\nResponsibilities\n\u2022 Involved in Design, Development and testing of the system.\n\u2022 Worked closely with Maintained the quality of data analyst, researched output and reporting, and ensured that all deliverables met specified requirements.\n\u2022 Developed SQL Server Stored Procedures, Tuned SQL Queries.\n\u2022 Created Views to facilitate easy user interface implementation and Triggers on them to facilitate consistent data entry into the database.\n\u2022 Performed data analysis and data profiling using SQL queries on various sources.\n\u2022 Created source to target mapping documents of data mart for all sources\n\u2022 Worked on client requirement and wrote Complex SQL Queries to generate Crystal Reports.\n\u2022 Tuned and Optimized SQL Queries using Execution Plan and Profiler.\n\u2022 Rebuilding Indexes and Tables as part of Performance Tuning Exercise.']",[u'Bachelors of Technology in Technology'],"[u'Jawaharlal Nehru Technological University Hyderabad, Telangana\nJanuary 2011']"
0,https://resumes.indeed.com/resume/ce574181034680d1,"[u""Data Analyst\nMancon Inc - Corpus Christi, TX\nJuly 2015 to February 2017\n\u2022 Worked to gather and elicit requirements across many disciples to facilitate creation of KPI (Key Performance Indicator) and CP (Cost Performance) reports for depot leadership.\n\u2022 Conducted 5+ levels of testing including functional, regression, user acceptance integration and performance to validate customer requirements.\n\u2022 Communicate client's business requirements by constructing clear data and process models.\n\u2022 Developed and implemented several reports using Agile development and project management principles.\n\u2022 Provide guidance to individuals (as well as in meetings/trainings to larger audiences)\n\u2022 Maintain quality customer service to ensure continuing positive relationships with customers and stakeholders.\n\u2022 Used project management information system to prioritize requirements, record defects and manage team deliverables."", u'Data Analyst\nMancon Inc - Corpus Christi, TX\nJuly 2009 to July 2015\n\u2022 Principal Analyst on several major division projects requiring integration of financial and program data from multiple sources to create a cohesive dashboard for decision making.\n\u2022 Participation in multiple strategic planning efforts and development of metrics for executive level reporting.\n\u2022 Use of CRISP-DM for data mining and Agile for project management.\n\u2022 Practice ETL (Extract, Tailor, Load) and EDA (Exploratory Data Analysis) and profiling for project analytics.\n\u2022 Conduct data cleansing, data selection, required field identification, review methodologies for secondary source data.\n\u2022 Discovery of problematic areas using data analysis (overrun, misdistribution of hours, trend analysis). Created numerous ad hoc reports, technical assistance for methodologies and data gathering, and quantitatively tested subjective theories for cost consumption issues.\n\u2022 Prepare and present reports on the analytical findings of the tools developed in support of accurate program costs. Coordinate with other departments to tailor reports and analysis as required.']",[u'Bachelor in Management'],[u'Texas A&M Corpus Christi\nDecember 2009']
0,https://resumes.indeed.com/resume/12c86653901aef64,"[u'Data Analyst\nChina Mobile - Beijing, CN\nMay 2016 to May 2017\nDocumented structure of company business data which serves as a guide in the execution of tasks.\nEvaluated business data to identify their different characteristics and located their similarities to find ways they can be\nintegrated for better results.\nDesigned comprehensive data reports and other business tools to assist business managers and executive in made good\nbusiness decisions.\nCarried out statistical analysis of all business data from the company.\nHandled client related issues and worked with both business managers and supervisors in facilitating deliverables.\nArranged and evaluated client payment records and assess payment issues in a timely manner, and resolved issues that\nmight arise.\nArranged anomalies of consumption and identified errors in data and took measures to resolve them.\nInteracted with company staff and customers to have good knowledge of the business as this helps the data analysis.\n\nPROJECTS\nMachine Learning - Email Spam Filter with R Programming\nText Mining - Using R (tm) package and xpdf to analysis words relationship, positive and negative words and show words cloud of several text files, include Ted Talk, - Bible, Horror storybooks and Random tests file which generated by R.\nInformation System - Using SQL to manipulate database for a hospital; ERD, create the table, define the constraint, insert data\nentry. Once a customer cancels an appointment, the next customer on the waiting list will become the first person on the appointment list and gets the notification that he or she is being filled for this vacancy.', u'Data Entry Intern\nAmerican SMS Real Estate - Houston, TX\nJanuary 2016 to May 2016\nPrepared source data for computer entry by compiling and sorting information; established entry priorities.\nProcessed customer and account source documents by reviewing data for deficiencies; resolved discrepancies by using\nstandard procedures or returning incomplete documents to the team leader for resolution.\nEntered customer and account data by inputting alphabetic and numeric information on keyboard or optical scanner according to screen format.\nVerified entered customer and account data by reviewing, correcting, deleting, or reentering data; combined data from both systems when account information is incomplete; purged files to eliminate duplication of data.\nTested customer and account system changes and upgrades by inputting new data; reviewing output.\nContributed to team effort by accomplishing related results as needed.']","[u'Master of Computer Information System Conc in Data Analytics', u'Bachelor of Business Administration in Finance']","[u'Boston University Boston, MA\nJanuary 2018', u'Houston Baptist University Houston, TX\nJanuary 2015']"
0,https://resumes.indeed.com/resume/713412c12321911b,"[u'Data Consultant\nMorgan Stanley\nFebruary 2016 to Present\n\u2022 Support 8 different VP on their Day to Day operation data need (reclass, Data investigate, reallocate expense, data Reporting ect)\n\u2022 Produce Divisional Financial ""Dashboards"" for management Review\n\u2022 Prepare monthly & weekly ""flash"" reports for department\n\u2022 Work with controllers group to standardize contents and timing of recurring data needs\n\u2022 Data clean and data preparation using advanced Excel(pivotTable, advance formula) and Alteryx\n\u2022 Monitor and QC data cleaning and preparation automation process\n\u2022 Develop and maintain Sharepoint sites to house key reports to be shared with management\n\u2022 Supports ops officer with adhoc research and analysis, including regional analysis & Support\n\u2022 Produce other supplemental expense performance reports as needed\n\u2022 Develop new Module for special Project\n\u2022 Document & Maintain related end-to-end processes and procedures', u""Data Analyst\nPhoenixMart LLC\nMay 2015 to December 2015\nCommercial Real estate)\n\u2022 Alteryx Big Data Blending & GIS Mapping Software user\n\u2022 Responsible for managing large fundamental database such as Alteryx and MS Excel\n\u2022 Data Mining, Data Research, Data Clean up, Data Dedupe\n\u2022 Automate Data Mining Process\n\u2022 Geographic and spatial analysis, including creating maps using spatial dataset.\n\u2022 Prepare project pro forma\n\u2022 Prepare Marketing Material\n\u2022 Advance Excel skill (Pivot Table, Advance Formula)\n\u2022 Prepare Data and Power point Presentation for use by executive in wide range of conferences, speaking engagements and media appearances.\n\u2022 Analyzing and Researching current Market\n\u2022 Support company's Adhoc\n\u2022 Develop new Module for special project\n\u2022 Maintain Company sales database and Alteryx server\n\u2022 Update Alteryx server and licensing update\n\u2022 Experience tableau"", u""Research Analyst\nMarcus&Millichap Real Estate Investment NYSE(MMI) - Phoenix, AZ\nDecember 2013 to May 2015\n\u2022 Alteryx Big Data Blending Software user.\n\u2022 Interpret data from a variety of sources to assess market's economic environment and to evaluate commercial real estate opportunities and trends\n\u2022 Prepare data and presentations for use by executives in a wide range of conferences, speaking engagements and media appearances. \u2022 Author reports on market trends and behavior, offering investors insights into market performance.\n\u2022 Develop analysis and reports to enhance client understanding of market trends and investment opportunities\n\u2022 Monitor and trouble shoot automation program to ensure Data pull's accuracy\n\u2022 Support company's High net worth investment broker with Adhoc project\n\u2022 Maintain, Update and Trouble Shoot current Data Process\nModule\n\u2022 Develop new Module for special project""]",[u'Bachelor of Science in Economics'],"[u'ARIZONA STATE UNIVERSITY Tempe, AZ']"
0,https://resumes.indeed.com/resume/b0d6314943eef62d,"[u'Data Analyst\nIDoo Global\nDecember 2017 to Present\nResponsibilities:\n\u2022 Experience in writing SQL queries and optimizing the queries in SQL Server.\n\u2022 Generated claims report weekly, monthly and yearly.\n\u2022 Worked on reports to give parallel comparison of different-2 fiscal year of claims.\n\u2022 Fine-tuned SQL queries using hints for maximum efficiency and performance.\n\u2022 Worked closely with Administrator for proper backup and recovery plans.\n\u2022 Monitored performance and changed performance requirements through application of Database tuning and performance optimization techniques.']","[u'Masters in Business Intelligence', u'Bachelor of Technology in Mechanical Engineering']","[u'Ferris state University\nJanuary 2015 to January 2016', u'Lovely Professional University\nJanuary 2010 to January 2014']"
0,https://resumes.indeed.com/resume/8d760f97690bc281,"[u'Data Analyst\nAffline Analytics - Bengaluru, Karnataka\nJune 2015 to November 2015\n\u2022 Work with large amount of unstructured data which can be preprocessed and structured by appropriate tools.\n\u2022 Strong ability to analysis of statistics and generate reports in given time.\n\u2022 Built ability to develop and maintain good relationship and communication with team members at all level.\n\u2022 Responsible for generating reports in terms of data visualization using R or Excel tools.', u'Data Engineer\nAztec Software and Technology Services PVT.LTD - Bengaluru, Karnataka\nAugust 2014 to April 2015\n\u2022 Create and maintain optimal data pipeline architecture.\n\u2022 Assemble large, complex dataset that meets functional / nonfunctional business requirements.\n\u2022 Worked with Data wrangling and Data preprocessing.\n\u2022 Responsible for Integration of large data with different sources and formats.']","[u'Master of Science in Computer Science', u'Bachelor of Engineering in Electronics and Communication Eng']","[u'New Jersey Institute of Technology Newark, NJ\nDecember 2017', u'Shree Devi Institute of Technology\nJuly 2014']"
0,https://resumes.indeed.com/resume/bd074126fd811236,"[u'Research Analyst\nSmart Water Networks (SWAN) Forum - Tel Aviv, IL\nSeptember 2017 to January 2018\n\u2022 Conducted research on global applications of data-oriented water technologies.\n\u2022 Executed office management responsibilities such as invoicing, social media outreach, and website management.\n\u2022 Maintained email and phone communications with representatives from international water\norganizations, utilities, and technology providers.\n\u2022 Assisted in writing partnership, conference, and workshop proposals.', u'Environmental Data Analyst\nCSL Services, Inc - Pennsauken, NJ\nDecember 2015 to July 2017\nMonitored and maintained dozens of water flow metering sites and rain gauges, to ensure\nconsistent and accurate data collection.\n\u2022 Recognized equipment malfunctions, created service lists, and supported field crews in troubleshooting and replacing flow metering equipment.\n\u2022 Performed quality assurance and quality control reviews, to finalized and submit reports.']","[u'Bachelor of Environmental Studies in Anthropology', u'Associates in Liberal Arts']","[u'Temple University Philadelphia, PA\nMay 2015', u'Northampton Area Community College Bethlehem, PA\nMay 2013']"
0,https://resumes.indeed.com/resume/d57dd9bcf536a209,"[u'Data Scientist\nLEXIS NEXIS - New Providence, NJ\nJuly 2016 to Present\nProject Description: A market research project where we developed a predictive model for understanding the Market value, Consumer acceptance and Sale understanding the effect of market sale for a Home Automation Security Product in the area of South Boston. Survey methodologies for consumer demographics and historical data was supplied.\n\n\u2022 Factorial analysis was used to group the data into different factors followed by Supervised machine learning techniques.\n\u2022 Dimensionality reduction algorithm was applied to filter the required variables.\n\u2022 Created training and test data set and with help of Logistic regression to find out the dependent variables or significant factors\n\u2022 Responsible to validate the model by calculating the AIC and BIC for the developed model.\n\u2022 Data cleaning, validation and string manipulation with Substr, grep, strsplit functions.\n\u2022 Extracted data from one or more source files and Databases.\n\u2022 Led the development for extraction, transformation and preparation of datasets Python libraries (NumPy and Pandas) to perform exploratory data analysis.\n\u2022 Performed PCA analysis by using prcomp and pcacomp functions\n\u2022 Predict the dependent variable with predict function, performed Logistic regression with glm function.\n\u2022 Directed analytical efforts to enhance statistical models.\n\u2022 Vector Machine with Deployment, Classification Decision Trees: CHAID with deployment,\n\u2022 Stochastic Gradient Boosting with Deployment etc. and then used comparative tools such as lift charts, gain charts and cross-tabulation matrix to select the models that performs the best and to find the accuracy rate of each model.\n\u2022 Derived new variable for the transformation of change analysis to explains patterns in the observations of quality.\n\u2022 Using stats package was able to calculate the AIC and BIC values of the predicted variables.\n\u2022 Reported the values and validation parameters with ggplot, histograms and caret packages.\n\u2022 Statistics: Base and advanced. Logistic Regression, Bayesian inference\n\u2022 Machine Learning: Supervised learning algorithm, Dimensionality reduction, Principal Component Analysis.\n\u2022 Project management: Planning, preparation, stakeholder analysis, data collection, reporting, etc.\nResult: Successful in finding out the predicted values and find a significant correlation between consumer acceptance and sales, successfully created the model with Low AIC values.\nEnvironment: R, Sql, Plotly, Rstats, ggplot, Rshiny, Python', u'Research Analyst\nAIG, Berkley Heights, NJ\nSeptember 2014 to May 2015\nData Analytics\nResponsibilities/Contribution:\n\u2022 Developed Prediction model for prediction of Yeast cell expression data when exposed to reagents using R and Python.\n\u2022 Comparison of Clustering and Classification algorithm with Iris data set from UCI Machine learning repository using R.\n\u2022 Collected raw data from multiple sources using SQL, spreadsheets & secondary sources\n\u2022 Transformed raw data to processed data.\n\u2022 Performed hypothesis testing, causal analyses on developed models.\n\u2022 Developed a robust predictive model for yeast cell expression.\n\u2022 Used tableau for reporting and visualization of statistical inference of predicted models.\n\nEnvironment:\nR, Oracle 11, Rstats, Rplotly, Python, Tableau.', u'Data Analyst/Analytics\nAIG, Berkley Heights, NJ\nAugust 2011 to February 2014\nProject Description: Project focused on enhancing the sales, forecast the market value of products and assist in decision making.\nResponsibilities/Contribution:\n\u2022 Manipulated, cleaned and processed data using SAS, Excel.\n\u2022 Perform data cleaning with data steps using DLM, DSD, COMMA9, TRUNCOVER, MISSOVER etc. finding missing values with Nulif functions.\n\u2022 Merging data sets and components with Union and Union all function.\n\u2022 Used different SAS procedures such as PROC REPORT, UNIVARIATE, TABULATE, FREQ, MEANS, TRANSPOSE, SUMMARY and Data _NULL_\n\u2022 Analyzed raw data and developed recommendations.\n\u2022 Executed SQL scripts to manipulate data for data loads and extracts.\n\u2022 SQL integration with SAS, performing SQL joins, Proc SQL Means and Summary.\n\u2022 SQL data management with Proc functions.\n\u2022 Monitored the data generated from automated manufacturing processes using statistical process control techniques.\n\u2022 Involved in loading, extracting and validation of manufacturing process data.\n\u2022 Led meetings and presented results to cross-functional teams.\n\u2022 Delivered data-driven actionable insights to business management & manufacturing teams.\n\u2022 Performed data manipulation and prepared the training and testing sets for modeling.\n\u2022 Created the visual summaries to understand the shape and distribution of the data using R package ggplot2 and plotly.\n\u2022 Explored variables and selected the best predictors by doing a feature selection such as chi-square iterative method and the p-value feature selection.\n\u2022 Performed data cleaning and data preparation with R packages Dplyr & Tidyr. Imported and divided the input process control data into the training and testing data sets.\n\u2022 Utilized Stratified Random Sampling method to extract data to clearly differentiate the characteristics or patterns.\n\u2022 Performed Exploratory Data Analysis (Histograms, Normal Distribution Plots, Box Plots etc.), ANOVA, Chi-Square Test and Root Cause Analyses (RCA) to identify the best predictors that clearly discriminate\n\nEnvironment: R, Sql, Plotly, Rstats, SAS, Proc, Sql, Python.']",[u''],"[u'Northeastern University Boston, MA\nJuly 2015 to May 2016']"
0,https://resumes.indeed.com/resume/fbe2cff24091c27a,"[u'Data Analyst\nJanuary 2016 to December 2016\n\u2022 January 2016 - December 2016\nAt Frost Brown Todd, I was responsible assisting the lawyers and attorneys there with things such as making copies, sending faxes, running errands, filing, handling mail, setting up rooms and meetings, data entry, and anything else you could possibly think of. This was a contract position for a year which ended in December 2016.\n\u2022 Amazon\n\u2022 Data Analyst\n\u2022 June 2014 - December 2015\nAt Amazon, I was responsible for keeping track of inventory and also filing and issuing returns from customers. This required constant data entry and also plenty of communication throughout our team on a daily basis. Here I was reliable as well, But decided to resign in December of 2015 due to new opportunities that I wanted to take advantage of.']",[u'in general'],"[u'Warren Central High School Indianapolis, IN\nJanuary 2014']"
0,https://resumes.indeed.com/resume/c565cece7aeba446,"[u'Data Analyst\nThe Office of Strategic Analysis and Data Management\nSeptember 2017 to Present\nDeveloped and presented meaningful insights by designing and creating dashboards of student data\nusing Tableau Desktop.\n\u2022 Automated the task of performing incremental updates to the existing database in MS Access using VBA\nMacros and SQL queries which reduced 20 hours of monthly work to less than half an hour.', u'Innovation Analyst\nErnst and Young - IN\nMarch 2013 to December 2015\n\u2022 In-house private cloud implementation using OpenStack and providing a platform to help clients in implementing ideas and testing them in physical world.\n\u2022 Architected and implemented a VoIP telephony network infrastructure within the organization which helps in optimizing operations and efficiency of network solution.\n\u2022 Utilised Cloud and Big Data solutions for implementing various Business Strategies.\n\u2022 Worked with clients in delivering exceptional results and quantifiable improvements in metrics such as productivity, cost, quality and time-to-market.\n\u2022 Gathered information, analysed data, performed top class benchmarks and recommended deep insights into business technologies to develop performance improvements for client services and products.\n\u2022 ITIL (IT Infrastructure Library) v3 certified Professional.']","[u'Master of Science in Business Analytics', u'Bachelor of Engineering in Computer Science Engineering']","[u'University of Massachusetts Lowell, MA\nDecember 2018', u'University of Kerala\nMay 2013']"
0,https://resumes.indeed.com/resume/312e2d62591c2622,"[u""Principal Consultant\nSogeti - Bridgewater, NJ\nMarch 2016 to Present\nProject 3:\nBecton Dickinson, NJ\nBusiness Data Analyst\n\nBuild the Approved Supplier List (ASL) application for Global Approved BDX Supplier List (legacy BD (BD1) and legacy care-fusion (BD2))\n\u2022 Responsible for preparing the following documents\n\u25e6 SRS to develop the application and get the signoff\n\u25e6 URS to develop the system life cycle for business and regulatory reasons\n\u25e6 Work Flow process diagrams\n\u25e6 Migration of legacy data of two system to one system\n* Analyze the source data and document the discrepancies of data type, Data Size and the data duplication.\n* Data mapping of various supplier source data from ERP system to the destination (ASL application)\n* Load the ERP source data to data warehouse using ETL tool, SSIS on weekly basis\n\nProject 2:\nProcter and Gamble, OH\nBusiness Analyst\n\nMigration of Packaging spec origination of CSS to ENOVIA PLM\n\u2022 Implementing the PLM packaging capability parts and specs origination process in ENOVIA's Digital Spec Management (DSM) application.\n\u2022 Build the data assets (like common performance specifications and material compositions) in libraries and mapped to specifications for maximum reuse.\n\u2022 Creation and release of an FPP (Finished Product Part) that include materials, Customer Units and Consumer Units.\n\u2022 Data validation and data mapping\n\nProject 1:\nExpress Scripts Inc., NJ\nBusiness Analyst\n\nProvide integrated PBM (Pharmacy Benefit Management) services including network-pharmacy claims processing and home delivery pharmacy services\n\u2022 Analyze the existing system and the future system\n\u2022 Interact with the various team members to gather the information.\n* Prepare HLR (High Level Requirement) document and DLR (Detail Level Requirement) document"", u'Business Data Analyst\nBank of America, NJ\nSeptember 2015 to February 2016\nPurpose of ORCIT (Operational Risk & Compliance Integrated Technology) is to provide the components\n\u2022 Compliance Risk Assessment\n\u2022 Monitoring and testing activity\n\u2022 Metrics and key risk indicators\n\u2022 Identification and reporting of compliance risk issues and exams\n\u2022 Regulatory inventories and regulatory change management process\n\nResponsibilities\n\u2022 Analyze the existing systems and the futures system\n\u2022 Interact with the various system team members to gather the information.\n\u2022 Prepare source and target data mapping\n\u2022 Involve in preparing the future architectural diagram\n\u2022 Prepare the Work Flow process diagrams using VISIO\n\u2022 SQL queries for data validations', u""Business Data Analyst\nKnipper, NJ\nMarch 2014 to December 2014\nPharmaceutical Sampling services system to refine the customer sampling strategies to achieve client's highest impact with lowest cost while insuring PDMA compliance.\n\n\u2022 Prepare SRS to validate the client source data to Target system.\n\u2022 Practitioner Validation to support PDMA compliance.\n\u2022 Responsible for data analysis and mapping of external source data to the internal target system.\n\u2022 Responsible for analyzing the Internal and external source data with the reference data for creating the staging data\n\u2022 Use of TOAD to run the SQL queries for data validation\n\u2022 Interact with the systems for testing and report generation"", u'Business Data Analyst\nIndependent BlueCross - Philadelphia, PA\nMay 2013 to February 2014\nThe purpose of the Affordable Health Care Act is to assure that all Americans have access to affordable Health insurance.\n\u2022 Prepare the BRD for HIPAA transactions, 834 and 837 for on exchange and off exchange enrollments file from IBC exchange gateway to EDI platform that routed to AEDW.\n\u2022 Prepare the Business System Requirement Document with use cases for Health Care Reform Billing System.\n\u2022 Used data analysis techniques to validate business rules and identify low quality missing data in the existing enterprise data warehouse (EDW) to populate the internal DB tables.\n\u2022 Prepare the Interface Control Document (ICD). The purpose of this document is to\n\u25e6 Gather the data elements (Field Types, Field Length etc.) of the existing systems\n\u25e6 Analyze the data elements of each source and destination of all the systems\n\u25e6 Analyze the data redundancy to crate the data dictionary.', u'Business Data Analyst\nLincoln Financial Group - Philadelphia, PA\nSeptember 2012 to March 2013\nUnified Account Opening system (""UAO"") is to provide a common solution for advisors to initiate, track, and gain approval for new accounts across multiple channels.\nProducts include Life, Term, Annuities and Disability\n\u2022 Project initiate and prepare BRD, SSD, Use Case details, Use Case Diagrams, Activity Diagrams, Process Diagrams of Unified Account Opening system.\n\u2022 Data mapping of different source data, reference data for migration to a new system.', u""Business Data Analyst\nWealth Management Systems, NY\nMay 2010 to August 2012\nWealth Management Systems Inc. (WMSI) is the leading provider of rollover products and services in the financial services industry major clients: LPL Financial, Merrill Wealth Management, Pershing, Franklin Templeton, and Fidelity\n\nThe Rollover Desk Portal is designed to be used by client call center to support participant services and facilitate IRA rollover transactions. The rollover desk portal will provide call center representatives the ability to review complete participant data, refer rollover request to specific plan and Wealth Management Advisors.\n\u2022 Gather information to prepare the rollover desk portal, advisor portal and advisor call center application for a new IRA and existing IRA. The primary information required for IRA rollover\n\u2022 Prepare the Business Requirement Documents (BRD's), Functional Requirement Specifications (FRS) after the collection of Functional Requirements from System Users that provided appropriate scope of work for technical team to develop prototype, Mockups, Use Cases, Flow diagrams for overall system\n\u2022 Prepare the Test Plan, Test Cases and Test Scenarios to be used in testing based on Business Requirements, technical specifications and/or product knowledge for the User"", u""Business Data Analyst\nJP Morgan - Chase, NY\nSeptember 2008 to December 2009\n\u2022 Project initiation, documentation of Data Acquisition and Control components of Credit Infrastructure Transformation program initiated by Global Credit Risk Management.\n\u2022 Worked on DAC system for Cash Transactions of various LOB\n\u2022 Define the source to target data mappings, business rules and data definitions\n\u2022 Cash securities source data mapping with reference data elements like counter party, collateral, instruments\n\u2022 Transform and load the data: DW to staging to ODS\n\u2022 Generate the reconciliation reports, financial reports, Basel II reports and regulatory reports\n\u2022 Worked with project team representatives to ensure that logical and physical data models developed in line with corporate standards and guidelines.\n\u2022 Responsible for defining the functional requirement documents for each source to target interface\n\u2022 Documented, clarify, and communicate requests for change requests with the requestor and coordinate with the development and testing team.\n\u2022 Reverse engineered all the Source Database's using Erwin.\n\u2022 Extensively used SQL queries to generate the reports.\n\u2022 Documented data quality and traceability documents for each source interface.\n\nApplication Development\nJohnson & Johnson, NJ - Data model and data migration of taxonomy data from different databases to a single database\n\u2022 Erwin Data Modeling and Sybase\n\nCredit Suisse First Boston, NY - Global Price Testing, Dividend Received Deductions and Y2K\n\u2022 Erwin Data Modeling, PowerBuilder and Sybase\n\nNew York Life Insurance, NY- Fixed Income Research Management, Fixed Income Compliance reports, Foreign Holding report\n\u2022 Erwin Data Modeling, PowerBuilder and Sybase\n\nPrudential Health Care, NJ - Performance Management Workstation to design dimension data model\n\u2022 PowerBuilder and Sybase\n\nJP Morgan Chase, NY and DE - Parallel Reporting Information system, Transaction Management System, Money Transfer Module via SWIFT messages, ADR via SWIFT messages, TRIPARTY and collateral services\n\u2022 PowerBuilder and Sybase\n\nZenith Computers, India\n\u2022 MF-COBOL, MS-COBOL, DB2 and Mainframe""]",[u'Bachelor of Science'],[u'Andhra University']
0,https://resumes.indeed.com/resume/59af3c73942b07b8,"[u'Data Analyst\nBNP Paribas - Jersey City, NJ\nJune 2016 to Present\nEngaged with various business units across the company to provide analytics from a quantitative perspective; worked in areas\nincluding asset securitization, trade and treasury financing, cash management and client management.\n\u2022 Explored various data sources to identify relevant data for business problem using SQL; conducted data ETL and performed analysis\nusing R, Alteryx, and SQL; conducted statistical analysis, built various models and visualized analytical results using R and Tableau.', u'Summer Analyst\nPing An Securities\nMay 2015 to August 2015\nConducted earnings quality analysis and due diligence investigation for a construction company; presented in site visits and obtained\npertinent information by attending various industry conferences.\n\u2022 Conducted quantitative and qualitative analysis incorporating multiple analysis, DCF valuation and industry analysis; assisted in drafting a research report.']","[u""Master's Degree in Statistics"", u""Bachelor's Degree in Mathematics and Applied Mathematics""]","[u'Columbia University, Graduate School of Arts and Sciences New York, NY\nFebruary 2016', u'Zhejiang University Hangzhou, CN\nJuly 2014 to June 2015']"
0,https://resumes.indeed.com/resume/7b77fbca14dbe913,"[u'Data Analyst\nUtah State University\nSeptember 2017 to December 2017\n\u2022 Worked with clickstream data to analyze and predict online customer purchase behavior using Python\n\n\u2022 Performed comparative analysis to identify users sessions that appears to be bot traffic/non-customer activity using PostgreSQL', u""Data Analyst Intern\nIntermountain Healthcare\nMay 2017 to August 2017\nDesigned and implemented data extraction and analytics project as sole architect which resulted in savings amount for each customer post-business decision\n\nFetched medical data from multiple data warehouse systems and created meaningful data clusters across the database server\n\nConnected multiple database clusters to Tableau to gain insights into patients' medical history, geographic location of hospital, and cost of treatment\n\nCreated weekly automated dashboards on Tableau for reporting insights and outcomes for senior leadership\n\nWorked with Alteryx Designer tool on patients' demographical data"", u""Data Analyst\nLenovo Group Ltd\nJanuary 2017 to April 2017\nCreated BI tables with Lenovo's clickstream data using HiveQL in Hadoop Cluster to find successful patterns between converted and non-converted users\n\nVisualized success KPI's and patterns on Tableau to find correlation between page load time, most common path, and revenue\n\nBuilt statistical model using Random Forest algorithm in R to show profit and loss due to various page load time of converted and non-converted customers"", u'Data Analyst\nTATA Consultancy Services\nAugust 2014 to August 2015\n\u2022 Worked with National Insurance Provider to predict customer spending across multiple insurance services using R\n\n\u2022 Used Logistic Regression for predictability of multiple insurance data models based on claims, geographical data, and customer parameters']","[u'Master of Science in Information Science', u""Bachelor's in Computer Science and Engineering""]","[u'Utah State University Logan, UT\nAugust 2016 to December 2017', u'Rajiv Gandhi Technological University Indore, Madhya Pradesh\nAugust 2010 to May 2014']"
0,https://resumes.indeed.com/resume/a237e756bb423fd8,"[u'Business Analyst\nEClinincalWorks Pvt Ltd\nJune 2015 to June 2017\nResponsibilities: Application Support, Data Modelling, Test Plans, Report Generation, Documentation, Presentation.\n\u2022 Used SQL Server for data modelling and designed UML diagrams to provide as a guideline for technical staff members\n\u2022 Designed logical & physical conceptual data models, Star & Snowflakes schema of given data\n\u2022 Created detailed BRD, FRS, Current state analysis, User stories, and Test Case documentations\n\u2022 Generated business reports & charts to present key insights of an organization\n\u2022 Analysed raised workflow and technical issues to help Project manager and sales team in decision making\n\u2022 Responsible to monitor client interaction performance by analysing reopened, CSAT and escalated product based issues\n\u2022 Interacted & attended business meetings with stakeholders to discuss about project progress and challenges if any faced\nTools Used: MS Excel, Tableau, SQL Server, Visio, MS PowerPoint, Office 365, JIRA, MS Word, MS Access', u""Data Analyst\nEClinincalWorks Pvt Ltd\nJuly 2014 to May 2015\nResponsibilities: Data Visualization, Data Modelling, Data Preprocessing, ETL, Data reporting, Dashboards, Statistical Analysis\n\u2022 Generated reports to analyse different KPI's of patient engagement modules (Pivot table, functions, formulas)\n\u2022 Developed interactive dashboards using Tableau about valuable KPI's of the EHR modules\n\u2022 Collected data from multiple internal & external sources and managed ETL process using MS Excel & SQL Server\n\u2022 Handled missing data using XLMiner and standardized each variables as per requirement\n\u2022 Performed statistical analysis on datasets to identify data pattern and relationships among variables\n\u2022 Created PowerPoint presentation to represent insights of large data sets across all teams for business decisions\nTools Used: MS Excel, SQL Server, Tableau, MS Visio, MS PowerPoint""]","[u'M.S. in Information Systems', u'B.E in Computer Engineering']","[u'California State University\nAugust 2017 to May 2019', u'University of Mumbai Mumbai, Maharashtra\nAugust 2011 to May 2014']"
0,https://resumes.indeed.com/resume/64b07aa54578ee06,"[u'Senior Financial Data Analyst\nWells Fargo\nJanuary 2015 to August 2016\n\u2022 Performed data mining, data cleaning and data analysis of financial data using MS Excel, R and MySQL.\n\u2022 Developed algorithms to predict the mortgage customer ability that cover the expenses or default.\n\u2022 Assisted in the development of the budget, planning and financial requirements.\n\u2022 Prepared business reports and dashboards using Tableau for advisory recommendations to investors.', u'Financial Data Analyst\nWells Fargo\nNovember 2013 to January 2015\n\u2022 Involved in analyzing period end OPEX and CAPEX reports and provided commentary on the variances using trend analysis.\n\u2022 Involved in extraction, transformation, and loading of financial data directly from different source systems (flat files/Excel/HTML/JSON) using Python.\n\u2022 Prepared weekly productivity reports using MS Excel.\n\u2022 Analyze and document customer requirements, translate them into project definitions and work\nbreakdown structures using MS Project']","[u'Masters in Management Information System in Management Information System', u'Master of Business Administration in Finance in Finance', u'Bachelor of Commerce in Commerce']","[u'University of Mary Hardin-Baylor Belton, TX\nAugust 2016 to December 2017', u'Jawaharlal Nehru Technology University Hyderabad, Telangana\nApril 2014 to May 2016', u'Badruka College of Commerce Hyderabad, Telangana\nApril 2010 to April 2013']"
0,https://resumes.indeed.com/resume/1887b9c12948ce34,"[u'Business Data Analyst\nUPMC Health Plan - Columbus, OH\nJanuary 2017 to Present\nThe project offered a full range of group health insurance, Medicare, Special needs, CHIP, Medical Assistance, behavioral health, employee assistance and workers compensation products and services to over 2.9 million members. This project was to design and create database object, transfer the data from flat files, MS excel and MS Access to SQL Server 2005 by creating various SSIS packages, produce regular/adhoc reports and perform analysis.\nResponsibilities\n\u2022 Interviewed Business Users to gather Requirements and analyzed the feasibility of their needs by coordinating with the project manager and technical lead.\n\u2022 Established a business analysis methodology around the RUP (Rational Unified Process). Developed use cases, project plans and manage scope.\n\u2022 Supported project metrics analysis, team communication, resource planning, risk analysis, report generation and documentation control using workbench application.\n\u2022 Perform Gap Analysis of the processes to identity and validate requirements\n\u2022 Identified/documented data sources and transformation rules required populating and maintaining data Warehouse content.\n\u2022 Collected requirements from Business Users and take part in preparation of technical specifications\n\u2022 Created new database objects like tables, procedures, Functions, Indexes and Views\n\u2022 Designed Constraints, rules and set Primary, Foreign, Unique and default key and hierarchical database.\n\u2022 Developed stored procedures in SQL Server to standardize DML transactions such as insert, update and delete from the database.\n\u2022 Created SSIS package to load data from Flat files, Excel and Access to SQL server using connection manager\n\u2022 Transform the data using various SSIS tools such as derived Columns, conditional splits, data conversions, Aggregate and Pivot transformation\n\u2022 Created data transformation task such as BULK INSERT to import data from client\n\u2022 Identified the dimension, fact tables and designed the data warehouse using star schema and designed new schema for the data mart.\n\u2022 Worked on generating various dashboards in Tableau Server using different data sources such as Teradata, Oracle, Microsoft SQL Server and Microsoft Analysis Services.\n\u2022 Maintained stored definitions, transformation rules and targets definitions using Informatica repository Manager.\n\u2022 Used various transformations like Filter, Expression, Sequence Generator, Update Strategy, Joiner, Stored Procedure, and Union to develop robust mappings in the Informatica Designer.\n\u2022 Worked on Informatica Power Center tools- Designer, Repository Manager, Workflow Manager, and Workflow Monitor.\n\u2022 Implemented SCD1, SCD2 type maps to capture new changes and to maintain the historic data\n\u2022 Created SSRS report in BI studio, prepared prompt generated/ parameterized report using SSRS 2008\n\u2022 Created reports from OLAP, sub reports, bar charts and matrix reports using SSRS\n\u2022 Take in-charge for unit testing of each reports and sub reports', u""Business Data Analyst\nKaiser Permanente - Silver Spring, MD\nNovember 2014 to December 2016\nThe project was creating totally new web services to replace the existing application. The customer can compare individual health insurance plans by providing zip code, date of birth and gender. I was responsible of Data Warehouse architecture as well as the development, testing, deployment, management and troubleshooting of ETL solutions for large- scale heterogeneous data environment.\n\nResponsibilities:\n\u2022 Conducted data research and analysis to resolve healthcare Medicare/Medicaid eligibility, enrollment and billing issues.\n\u2022 Ensure data integrity, data security and process optimization by identifying trends and provide reports as necessary.\n\u2022 Created databases, tables, stored procedures, DDL/DML Triggers, views. User defined data types, functions, cursors and indexes using T-SQL\n\u2022 Develop all the required stored procedures, user defined functions and triggers using T-SQL and SQL\n\u2022 Worked on Dynamic SQL and queries\n\u2022 Conducts data mapping, designs, develops and implements ETL process\n\u2022 Extracted, Transformed and loaded data for business users to manipulate into reports\n\u2022 Supported management team with reports generation and analysis using ETL tools, Excel and SQL\n\u2022 Performed ETL repository administration including promotion of software packages to proper environment, troubleshooting and technical support\n\u2022 Analyzed invoice data against carrier contracts to alert of any discrepancies\n\u2022 Applied master data Management Standards in data conversion, mapping and migration\n\u2022 Produced report using SQL Server Reporting Services (SSRS) and creating various types of reports such as drill down, parameterized, cascading, conditional, table, matrix, chart and sub reports formatting them using SSRS\n\u2022 Developed reporting and various dashboards across all areas of the client's business to enable quick identify claims issues\n\u2022 Worked on preparation of Test plans, Test data and execution of test cases to check application functionality that meets user requirements\n\u2022 Documented and reported issues that affect claims processing or accuracy to claims leadership team."", u'Business Data Analyst\nCisco - Lubbock, TX\nJanuary 2011 to October 2014\nDescription:\nI worked in a team of designing dashboards that ran calculations to help determine year over year percentage growth for 30 national partners for over 200 end users.\n\nResponsibilities:\n\u2022 Maintained accurate data in channel reporting tools, systems and processes and use Business Objects, Excel and Access to accomplish data clean assignments\n\u2022 Used SSIS workflow Manager to create workflows, database connections. Sessions and batches to run mappings\n\u2022 Created custom reports and pivot tables for Sales Reps that were tailored by segment, area and technology\n\u2022 Worked with Finance leads to investigate discrepancies between databases.\n\u2022 Assisted management with special projects such as collecting and consolidating survey data for validation\n\u2022 Facilitated development, testing and maintenance of quality guidelines and procedures along with necessary documentation.\n\u2022 Gathered and translated business requirements into detailed, production-level technical specifications, new features, and enhancements to existing technical business functionality.\n\u2022 Extracted data from the databases (SQL Server) using SSIS ETL tools.\n\u2022 Designed and Developed SSIS packages for performing ETL process using SQL Server 2008 Integration Services\n\u2022 Expert in extracting, transforming and loading data using SSIS Import/Export Wizard.\n\u2022 Created SSIS Packages to integrate data coming from Text files and Excel files\n\u2022 Created Stored Procedure and Views, Indexes, SQL joins and Sub queries\n\u2022 Worked with integration services for transferring and reviewing data from different sources like (Flat file, Excel, CSV)\n\u2022 Generated Daily, Weekly, Monthly reports for the analysis of managers and end users by using SQL Server Reporting Services.\n\u2022 Created data mapping documents from source to target systems.\n\u2022 Created logical and physical design of the centralized Relational Database.\n\u2022 Identified and processed the facts and dimension tables.\n\u2022 Normalization to 3NF/denormalization techniques for optimum performance in relational and dimensional\ndatabase environments.']",[u'Bachelors in Business Administration in Business Administration'],"[u'2. Independent University of Bangladesh Dhaka, BD']"
0,https://resumes.indeed.com/resume/3cf27ecd2df1dd4c,"[u'Data Analyst\nGoogle - Mountain View, CA\nSeptember 2015 to Present\n\u2022 Accurately executed primary daily tasks that include but are not limited to, document planning and accurate and precise data reporting, data mapping and data mining\n\u2022 Made data driven recommendations by data visualization and creating reports in Tableau\n\u2022 Designed and updated database with MySQL and created reports by validating data and integrating with Tableau Server\n\u2022 Developed content and generated reports using Excel, Tableau and Google Analytics\n\u2022 Extensively involved in designing star schema and snow flake schema with data modeler and architect.\n\u2022 Tested the ETL Informatica mappings and other ETL Processes (Data Warehouse Testing)\n\u2022 Converted quantitative data to qualitative data and enhanced the product quality for validation\n\u2022 Performed database testing as needed to ensure that customers receive prompt and efficient service\n\u2022 Performed A/B testing on GSX website to make it user friendly and to increase customer flow\n\u2022 Extracted, compiled and formatted data in EXCEL (V-lookup, Pivot Tables)\n\u2022 Wrote SQL queries to extract data from database and modified it according to business needs\n\u2022 Converted data into graphical representation using data visualization tool and google sheets\n\u2022 Performed black box testing, performance testing, stress testing and unit testing on day to day basis to collect more data points\n\u2022 Adhere to and assist in the implementation of Standard Operating Process in Agile environment\n\u2022 Participated in daily, weekly, and quarterly KPI collection, review, and publishing\n\u2022 Managed SLA performance through metrics to meet project deadlines\n\u2022 Provided training and product demonstrations to business users, executives, and customers\n\u2022 Configured various workflows for filing bug in defect tracking tool Jira and validated product quality\n\u2022 Actively participated in team meetings to discuss current process and resolve issues with local and off shore teams\n\u2022 Documented various data quality mapping documents\n\u2022 Excellent interpersonal and organizational skills to interface with internal customers, management and outside vendors\n\u2022 Technical abilities and troubleshooting skills with track record of removing project roadblocks', u'Data Analyst\nCitizens Bank - Cranston, RI\nJune 2015 to September 2015\n\u2022 Experience in understanding & supporting both the Business & Technical teams in all phases of Software Development Life Cycle (SDLC) by using different methodologies like Waterfall and Agile-Scrum\n\u2022 Analyzed functional and non-functional data elements for data profiling and mapping from source to target data environment\n\u2022 Analyzed large data sets to provide strategic directions to the bank and build and document system specifications\n\u2022 Interpreted complex simulation data using statistical methods and visualization using BI tool Tableau and BI technics\n\u2022 Verified and maintained Data Quality, Integrity, data completeness, ETL rules, business logic\n\u2022 Conducted database testing using SQL queries on MS SQL and My-SQL server\n\u2022 Created a test plan and a test suite to validate the data extraction, data transformation and data load and used SQL and Microsoft Excel\n\u2022 Performed data analysis and data profiling using complex SQL on various sources systems for system validation\n\u2022 Assisted the database modelers in preparing the logical and physical data models and ascertained that requirements have been met\n\u2022 Created SQL scripts to find data quality issues and data validation issues for banking database\n\u2022 Created data trace map and data quality mapping documents (RTM) to make sure all business requirements match with test cases\n\u2022 Performed Smoke testing, Sanity testing, Unit testing, Regression testing and integration testing for application\n\u2022 Coordinated with business users, database administrator and testing team for production testing data for validation\n\u2022 Managed and collaborated with data operations team and developers to meet business user needs and provide required data to meet deadlines\n\u2022 Reported found issues in bug tracking tool JIRA and Quality Center (QC-ALM) for test and system management', u'Data Analyst\nBrick Red Solution - Irvine, CA\nAugust 2014 to April 2015\n\u2022 Worked with Business Analysts and DBA for requirements gathering, business analysis, testing and project coordination\n\u2022 Collected business requirements to set rules for proper data transfer from Data Source to Data target in Data Mapping\n\u2022 Worked on importing and cleansing of data from various sources\n\u2022 Wrote and executed unit, system, integration and UAT scripts in a data warehouse projects\n\u2022 Developed mappings using various transformations and SQL Packages to extract, transformation and loading of data\n\u2022 Developed SQL scripts to validate the data loaded into data warehouse and data mart tables using ETL\n\u2022 Created Match Rule Sets for preventing duplicate data entries into MDM tables\n\u2022 Wrote SQL for data profiling and developed data quality reports\n\u2022 Tested the ETL process for both before data validation and after data validation process\n\u2022 Testing and Debugging of all ETL objects to evaluate the performance\n\u2022 Troubleshoot test scripts, SQL queries, ETL jobs, data warehouse/data mart/data store models\n\u2022 Extensively used MS Access to pull the data from various databases and integrate the data\n\u2022 Generated excel reports for data validation and data reconciliation using V-Lookups\n\u2022 Worked on building load rules and transformation logic for ETL of the data into data warehouse\n\u2022 Worked on Agile Development methodologies\n\u2022 Involved in writing several complex SQL queries and Involved in backend testing and data quality issues\n\u2022 Experience on performing Detail Data Analysis (DDA), Data Quality Analysis(DQA)and Data Profiling on source objects', u'Data Analyst\nElectronics Systems - Gujarat, IN\nDecember 2012 to November 2013\n- Gujarat, India\n\u2022 Involved in documenting and developing test data using Tableau\n\u2022 Expertise in bug reporting, bug tracking using various bug tracking tools\n\u2022 Developed and executed data sets and created data reports using SQL queries\n\u2022 Created, executed, and updated Functional, Regression, Integration, and Unit test cases manually\n\u2022 Developed manual test cases for positive, negative tests and document the results\n\u2022 Involved in planning & performance testing, verification & validation, and quality assurance of the application\n\u2022 Implemented data quality and production changes to manual reports\n\u2022 Ensured quality levels are maintained in all phases of testing\n\u2022 Regularly interacted with the development team to communicate issues and simulate the defects\n\u2022 Responsible to provide up-to-date training and knowledge transfer to the new team member\n\u2022 Wrote SQL queries to extract data from MySQL to validate back-end data\n\u2022 Coordinated with developers and record defects in QC to track until they are resolved\n\u2022 Followed scrum methodology to meet company requirements\n\u2022 Prepared test data for functionality testing\n\u2022 Used Quality Center for requirement analysis, scheduling and generating test cases\n\u2022 Defects were tracked, reviewed, analyzed and compared using Quality Center\n\u2022 Generated reports and documented and communicated test data on daily bases\n\u2022 Coordinated with developers and system engineers to fix application defects\n\u2022 Attended test status meetings and test review meetings and defect triage meetings', u'Network Testing Intern\nReliance Communications - Gujarat, IN\nDecember 2011 to November 2012\n- Gujarat, India\n\u2022 Studied fundamentals about different sections of Network Infrastructure and their Performances\n\u2022 Reviewed and performed functionalities of each departments like Switch, Data and Router\n\u2022 Performed on-site testing to get practical knowledge about connections of each department\n\u2022 Involved in generating RTM-Requirements Traceability Matrix to trace the requirements to test cases\n\u2022 Developed detailed test plans and test cases and responsible for executing them\n\u2022 Attended walkthroughs and review meetings with business analysts & development team']","[u""Master's""]",[u'']
0,https://resumes.indeed.com/resume/a95211575edea0b8,"[u""Data Analyst\nPracto Technologies - Pune, Maharashtra\nMay 2015 to June 2017\n\u2022 Consultant to Tier I healthcare establishments across India to optimise their IT and resource management\n\u2022 Managed sales and marketing of Analytical, BI and ERP solutions in the enterprise market\n\u2022 Analysed Sales, Marketing and Operational reports for higher management using R, Tableau, QlikView and DOMO\n\u2022 Evaluated business strategies and implemented various decision models across different domains\n\u2022 Implemented 'Territory associative marketing' model which increased sales by 15% across products\n\u2022 Mediated issue resolution, project development & project delivery with tools such as Jira and Slack\n\u2022 Devised metrics for clients from historical analysis to increase efficiency in operations and marketing"", u""Business Analyst\nPracto Technologies - Mumbai, Maharashtra\nJuly 2014 to May 2015\n\u2022 Liaison between clients, development teams and managers to identify requirements and deliver an exacting solution\n\u2022 Composed business and analytical reporting for clients using SQL, Tableau, SAS and Excel\n\u2022 Established and audited different metrics governing project/product development, marketing and sales\n\u2022 Amplified the revenue in the first quarter of a project by 80% by identifying a new client base\n\u2022 Mapped business scope and use cases for multiple projects with experience in both agile and lean methodologies\n\u2022 Interacted with priority clients for crucial projects in person to ensure project retention and satisfaction\n\u2022 Charted growth plans, risk mitigation and bottle neck reduction for clients in various consulting projects\nAWARDS AND LEADERSHIP\nRepresented Practo Technologies at the IDC roundtable conference on 'Data infrastructure & analytics' 2016""]","[u'Master of Science in Information Systems', u'Graduate Certificate in Data Analytics']","[u'University of Cincinnati, Carl H. Lindner College of Business Cincinnati Cincinnati, OH', u'Mumbai University Mumbai, Maharashtra']"
0,https://resumes.indeed.com/resume/98da4a03ccdf8e73,"[u'Data Analyst\nESTsoft Inc - Irvine, CA\nOctober 2016 to Present\n\u2022 Manage planning and development of the design for metrics and reports, slashing output time by half\n\u2022 Perform market analysis to efficiently achieve objectives, increasing yearly sales gross revenue by 28%\n\u2022 Present findings and data to the team to improve strategies and operations, leading to efficacy increase of 14%\n\u2022 Engineer data models to predict financial outcome of marketing events, decreasing wasted resources by 60%']","[u'Master of Science in Aerospace Engineering', u'Bachelor of Science in Aerospace Engineering']","[u'Delft University of Technology Delft, NL\nJune 2014', u'Instituto Superior T\xe9cnico Lisbon, PT\nJune 2011']"
0,https://resumes.indeed.com/resume/1c6a1c261917620f,"[u""Data Capture Analyst\nDigital Risk LLC\nAugust 2013 to Present\nWorked at both Maitland and Lake Mary\nOffices. Responsible for bookmarking Mortgage\nLoan files, and scripting borrower information.\nOther duties consisted of checking coworkers'\nfiles as a Quality Control Specialist.""]","[u""Bachelor's Degree (BSBA) in Finance"", u'Associate in Arts', u'']","[u'University of Central Florida Oviedo, FL\nSeptember 2017', u'Seminole State College Lake Mary, FL\nJune 2015', u'Oviedo High School Oviedo, FL\nMay 2013']"
0,https://resumes.indeed.com/resume/e9227373b59f5007,"[u'Administrative Assistant\nRasko Supply\nApril 2015 to May 2015\nWorked with Time Sensitive Information to insure all deadlines were met', u'Data Analyst\nAmerican Mutual Group\nMarch 2015 to April 2015']",[u'Associate in Information Technology'],"[u'Heald College Honolulu, HI\nOctober 2014']"
0,https://resumes.indeed.com/resume/b37c0dba6a164cb5,"[u""Data Analyst\nTE Connectivity - Philadelphia, PA\nSeptember 2017 to Present\n\u2022 Designed revenue forecasting of sales data using ARIMA models in R which helped to set the right sales target to the team\n\u2022 Used Fixed LOD in Tableau & designed dashboard to visualize revenue generated by TE's product lines to identify top performers\n\u2022 Led efforts to support Sales team's ad hoc analytical business needs through deep-dive analysis and hypothesis validation"", u""Data Analyst\nHilton Worldwide - Dallas, TX\nJune 2017 to August 2017\n\u2022 Performed exploratory analysis & clustering in R to help HR executives analyze employee satisfaction & build retention strategies\n\u2022 Carried out statistical analysis to analyze variability in agents' performance using bubble plot to resolve performance issues\n\u2022 Developed ETL workflows in Alteryx and created dashboards in Microstrategy to provide insights to Chief Customer Officer"", u""Data Analyst\nApplied Data Science, Syracuse University\nJanuary 2017 to April 2017\n\u2022 Extracted, cleaned and transformed key attributes from 10 million records of customer survey using R for visualization\n\u2022 Depicted analysis results visually using gglplot2, identified and plotted co-relations among data points to showcase trends\n\u2022 Designed Supervised learning model using SVM to solve classification problem and predict customer's category"", u""Business Technology Analyst\niConsult - Syracuse, NY\nNovember 2016 to April 2017\n\u2022 Consolidated operational data from multiple data sources using SQL to a centralized location feeding data to multiple dashboards\n\u2022 Involved in building, pruning & maintaining the team backlog based on client's feedback & timeline to improve existing processes\n\u2022 Planned and facilitated the User Acceptance Testing process for all new product features, enhancements and defect remediation"", u'Product Analyst\nIntelliber Inc - Jamshedpur, Jharkhand\nJune 2015 to August 2015\n\u2022 Researched market trends and devised exclusive product solutions that contributed to a raise of $80k in the fund by investors\n\u2022 Brainstormed ideas and identified areas of improvement to produce a refined analytical capability for the new product\n\u2022 Used Informatica to carry out ETL process on source data & loaded it into target tables to build the required data marts', u'Analyst\nBihar e-governance Services & Technologies Limited - Patna, Bihar\nAugust 2014 to April 2015\n\u2022 Fine-tuned database stored procedures & queries leading to 40% decrease in the load and processing times of web based reports\n\u2022 Developed SSIS solutions to reduce work time by more than 50% in company projects and automate daily tasks\n\u2022 Analyzed 5 years of academic result data using Tableau to understand trend for each course to build better educational strategies\nACADEMIC PROJECT']","[u'MS in Information Management', u'BS in Electronics and Communication', u'Certificate of Advanced in Data Science']","[u'Syracuse University Syracuse, NY\nMay 2018', u'R.V College of Engineering Bengaluru, Karnataka\nJune 2014', u'Syracuse University Syracuse, NY']"
0,https://resumes.indeed.com/resume/ddad883264f44e07,"[u'Volunteer\nFort Collins Christian Church - Fort Collins, CO\nJune 2016 to Present\nHelping lead and grow the campus ministry.', u'Umpire\nNeurophysiology Class, Dr. Lee, Georgia Institute of Technology\nMarch 2016 to June 2016\nSpring 2016. \u2022 Hands-on learning scientific testing processes in a lab setting using\nproposed cancer treatments on cancer cell lines.\nNeurophysiology Class, Dr. Lee, Georgia Institute of Technology. Spring 2016.\n\u2022 In depth and broad understanding of the neural system with a focus on the physiology.\nAdditional experience includes Maintenance Worker, Zak George Landscaping. Umpire,\nNorthern Colorado Sports Officials.', u'Data Analyst\nSmart Lab, Emory University - Atlanta, GA\nJune 2014 to August 2014\nLed group in analyzing p-waves to determine when decision making\noccurred.\nQuantitative Engineering Physiology Lab I, Dr. Pai, Georgia Institute of Technology.']",[u'Bachelor of Science in Biomedical Engineering'],[u'Georgia Institute of Technology\nMay 2016']
0,https://resumes.indeed.com/resume/0ed0fe9eed392637,"[u'Data Analyst\nCapgemini India - Pune, Maharashtra\nJune 2014 to July 2017\nWorked as a Data Analyst for Payments and Investment Banking domain projects.\nI was awarded Rising Star Award for the year 2016 by Capgemini.']","[u'MASTER OF SCIENCE in Business Analytics & Information Systems', u'BACHELOR OF TECHNOLOGY in ACADEMIC PROJECTS', u'in Research']","[u'Muma College of Business\nSeptember 2017 to March 2018', u'University of South Florida Tampa, FL\nJanuary 2010 to January 2014', u'Institute of Technical Education & in the same neighborhood Bhubaneshwar, Orissa']"
0,https://resumes.indeed.com/resume/2375336139741100,"[u'Operational Excellence Data Analyst\nTarget - Minneapolis, MN\nFebruary 2016 to May 2016\n\u2022 Linked Teradata on SAS platform to obtain datasets for analysis\n\u2022 Implemented SAS codes to compare various promotional Vehicles (Circular, TPC, mail)\n\u2022 Supported operational excellence team, understanding optimization problem regarding labour-hour vs. profit', u'Data Analyst\nHoneywell Inc. / VSU - Chester, VA\nAugust 2015 to January 2016\n\u2022 Effectively used Macros to import Excel files with various Worksheets\n\u2022 Data visualization is performed for better understanding of dataset\n\u2022 Predicted Single Fibre strength from its bundle', u'Data Analyst\nData plus Value - Delhi, Delhi\nJanuary 2012 to July 2013\n\u2022 Collected data from relational databases ORACLE, SQL SERVER\n\u2022 Performed SQL queries to perform join to create Tables for managing data']","[u'PhD in Mathematics', u'M.S. in Mathematics in Mathematics', u'B.Tech. in Electronics and Communication Engineering in Electronics and Communication Engineering']","[u'North Dakota State University Fargo, ND\nAugust 2016 to Present', u'Virginia State University Petersburg, VA\nAugust 2013 to May 2015', u'Maharaja Agrasen Institute of Technology Delhi, Delhi\nAugust 2007 to May 2011']"
0,https://resumes.indeed.com/resume/3a4a0b8ec0e7aad4,"[u'Data Analyst Intern\nWebby InfoTech - Surat, Gujarat\nJune 2016 to May 2017\n- Surat, India\n\u2022 Collaborated with senior-level developers to develop projects for e-commerce website throughout various phases of project lifecycle.\n\u2022 Provided both internal performance reporting and web analytics.\n\u2022 Performed data analysis using R statistical software and results were interpreted according to relevant\n\u2022 Building, publishing customized interactive reports and dashboards using Tableau server.\n\u2022 Designed and built statistical analysis models that helped increase online sales.']","[u'Master of Science in Computer Science', u'Bachelor of Engineering in Computer Science & Engineering']","[u'Illinois Institute of Technology Chicago, IL\nMay 2019', u'Gujarat Technological University\nAugust 2013 to June 2017']"
0,https://resumes.indeed.com/resume/cbd0ae4672088379,[u'Data Analyst\nMayo Clinic\nMay 2012 to Present'],"[u""Bachelor's""]",[u'Minnesota State University-Moorhead']
0,https://resumes.indeed.com/resume/896cafa2030e354d,"[u""Data Analyst\nHCL TECHNOLOGIES\nOctober 2014 to January 2017\nClient: Trusted Media Brands, Inc.\nTrusted Media Brands, Inc. (TMBI), formerly known as the Reader's Digest Association, Inc. (RDA), is\nan American multi-platform media and publishing company. It is a home to several media brands across multiple digital platforms, social media, magazines, books, music, and events.\n\nProject Overview:\n\u2022 Understanding and Analyzing Marketing Sales & Research data to produce regular and adhoc reports.\n\u2022 Provide insights into business related queries.\n\u2022 Build codes for new requests.\n\u2022 Perform report validations.\n\u2022 Schedule and automate reporting process.\n\u2022 Interact with client to understand business requirements and translate to reporting needs.\n\nResponsibilities:\n\u2022 Create SAS codes for new reports.\n\u2022 Schedule and monitor monthly and quarterly run of reports, perform report validations.\n\u2022 Automate reporting requirements using Advanced Excel, VBA coding.\n\u2022 Create Excel dashboards, charts, pivots, reporting templates for data analysis.\n\u2022 Create Excel formulations for Quality check of reports.\n\u2022 Develop Oracle database procedures, functions and packages.\n\u2022 Analyze data, draw conclusions and develop recommendations.\n\u2022 Create documentations for all reports.\n\u2022 Direct Client interaction to understand business metrics.\n\u2022 Conduct knowledge sharing sessions with team for skills enhancement.""]",[u'Bachelor of Technology in Computer Science & Engineering'],[u'UTTAR PRADESH TECHNICAL UNIVERSITY\nJanuary 2014']
0,https://resumes.indeed.com/resume/4ee2ee862c727097,"[u'Data Scientist\nGoogle - Sunnyvale, CA\nSeptember 2017 to Present\nAnalyse de donnees pour Google Platform.\nExtraction des donnees a partir de Google Cloud, creation de requetes et tableaux parametrables.\nMachine learning pour analyse des textes libres.', u""Data Scientist\nPaypal - San Jose, CA\nMarch 2017 to September 2017\nModelisation de donnees financieres. Creation d'un modele de prediction (time series) des couts et revenus. Extraction des donnees (Haddop, Hive) et controle des donnees."", u""Data Scientist\nGooglex - Mountain View, CA\nMarch 2015 to January 2017\nDans le cadre du projet d'intelligence artificielle pour la Self Driving Car: indicateurs de qualite des donnees, affinement des nets, modelisation de donnees pour determiner les facteurs influencant la qualite des images acquises par les vehicules.\nAnalyse approfondie des incidents.\nAnalyse des donnees geographiques."", u'Data Analyst\nLegal Medicine Laboratory - Grenoble (38)\nFebruary 2009 to August 2014\nCollaborated with study teams, lawyers, and physicians to develop solutions and evaluate costs.\n\u25cf In charge of methodologies, data entry and cleaning, statistical analysis plans, bibliography.\n\u25cf Designed databases and coded (SQL, SAS, STATA, R, SQL), testing.\n\u25cf Completed Statistical data analysis (tests, regressions, time series and correlation), interpretation, randomization.\n\u25cf Created reports, summaries, presentation to study teams and customers (Microsoft Office).\n\u25cf Took care of quality standards compliance (GCP, CDISC), related legislation (government regulated),', u'Clinical Data Analyst\nBiostatistics Unit - University Hospital - Grenoble (38)\nJanuary 2001 to January 2008\nManaged projects, completed statistical studies including audits, case controls, surveys, cohorts and clusters.\n\u25cf Designed and managed data collections associated (SAS, STATA, SQL).\n\u25cf Deployed incident reporting software and conducted user training (JAVA).', u'Quantitative Analyst (Internship)\nAvalanche Laboratory - University of Grenoble (France) - Grenoble (38)\nFebruary 2004 to September 2004\nCleaned and reorganized data collections (SQL, JAVA, C++), provided documentation.\n\u25cf Modeled geographical data to create new top quality maps about avalanche risk.\n\u25cf Presented conclusions and solutions to accurate risk management.', u""Business Analyst\nDecathlon, Toulon (France)\nJanuary 1992 to January 1997\nManaged projects, completed marketing studies: audits, clients' workflows, surveys, campaign impact.\n\u25cf Developed innovative solutions and guidance for customers.""]","[u'M.S. in Statistics and Information Systems', u'in Statistics and Data Management']","[u'Grenoble University\nJanuary 2004', u'Grenoble University\nJanuary 1992']"
0,https://resumes.indeed.com/resume/7b2c149c0b444c78,"[u""Data Scientist\nCognizant Tech Solutions US Corp - Fort Worth, TX\nSeptember 2017 to Present\nPricing analytics:\nBuilt an analytics tool at its core which help to making better pricing and contracting decisions and that enabled\ncommercial pricing strategy teams to properly assess the profitability of proposed deals.\nResponsibilities:\n\u2022 Performed data cleansing and data mapping using Python programming language for generating price simulations to enable pricing decisions for profit analysis of contracts.\n\u2022 Created customer segments using unsupervised clustering techniques based on sales, contracts and NBO. Compared the customer attrition and player market share with the cluster segments created.\n\u2022 Integrated disparate data from sources like SalesForce, SAP BI extracts, Siebel and third-party data like AnalySource,\nIMS NSP, Predictive Acquisition Cost\xae (PAC) and internal sources.\n\u2022 Leveraged big data technology tools HIVE, PySpark and developed functions to interact with Hadoop data lake on\nAWS for data transformation(ETL).\nwww.linkedin.com/in/davidsnayakanti https://github.com/nayaksu7 nsundeepdavid@gmail.com 313-505-5479\n\u2022 Developed comprehensive data visualizations in QlikSense to illustrate complex ideas to various stakeholder levels on KPI's like product supply & capacity, cost for moving out of recommended tiers, price increase trend analysis by competitor etc.\n\u2022 Helped client to accelerate responses o competitive situations and provide insights into revenue leaks for stronger\nnegotiating position."", u""Graduate Assistant\nBGSU - Bowling Green, OH\nJanuary 2017 to May 2017\nNegative Outcomes Risk Prediction Model:\nAnalyzed Medicare resource utilization groups (RUG's) and Managed Care insurance claims data from healthcare\nprovider and predicted negative outcomes risk using regression analysis. Built a logistic regression model that\nhelped providers and members keeping patient population healthy, assist healthcare providers, improve outcomes and reduce costs.\nResponsibilities:\n\u2022 Developed predictive model to identify negative outcome margins.\n\u2022 Handled class imbalance using re-sampling techniques.\n\u2022 Utilized Logistic regression in R to identify the factors affecting margin and predict residents with negative margins.\n\u2022 Build Gradient Boost Model utilizing H20.ai in R to analyze variable importance and evaluate model performance.\n\u2022 Performed clustering analysis on historical patient level data to classify them into payment (total expense per stay)\ngroups and identified parameters impacting expenditures and provided recommendations to drive reimbursements.\nTools/Techniques: R /H2O-R/Tableau/H2O-FLOW/HIVE/HDFS"", u'Data Scientist\nNovartis\nFebruary 2014 to August 2015\nText analytics:\n\u2022 Implemented a text mining based algorithmic approach in R language to find nearest-neighbour NCIs (Non-\nConformance Incidents) for each NCI based on distance metric computed between vectors of keywords.\n\u2022 Identified recurring NCIs through similarity between the product/process related and issue related information\navailable in NCI records.\n\u2022 Distance match between products and issue key words was computed using cosine similarity.\n\u2022 Generated percentile confidence scores for product distance, issue distance and overall distance between complaints and CAPAs to help user define the cut-off confidence.\n\u2022 Developed CAPA/Complaints effectiveness metrics dashboard in Tableau to provide visual insights to business\nusers.\nTools/Techniques: R /SQL server/ Tableau /Microsoft Excel\n\nMarketing analytics:\nSegmented physicians and provided meaningful insights to marketing & salesforce team for physician targeting\nleveraging APLD patient Level data from Symphony, IMS Xponent, IMS Sales and Distribution data(DDD) and various internal datasets.\nResponsibilities:\n\u2022 Created a robust physician segmentation framework based on physician prescribing potential and adoption rate.\n\u2022 Performed AS-IS analysis on physician level data and assess initial current target list provided by sales force team.\n\u2022 Leveraged k-means algorithm to profile physicians and provided recommendations to salesforce team to set brand\nstrategy, direct promotional activity, allocate resources and to ultimately increase brand profitability through efficient and effective marketing.\n\u2022 Provided distinct segments with key physician characteristics which helped marketing team to prioritize market\nsegments and devise promotional messages.\nwww.linkedin.com/in/davidsnayakanti https://github.com/nayaksu7 nsundeepdavid@gmail.com 313-505-5479', u'Data Analytics Specialist\nNovartis\nJanuary 2013 to January 2014\nPatient Satisfaction analytics:\nAnalyzed CAHPS survey data which included clinical and demographic data and came up with factors leading to dis-satisfaction by patients.\nResponsibilities:\n\u2022 To Identify key variable for which hypothesis testing was done using Chi-Squared, Kruskal Wallis and Wilcoxon tests.\n\u2022 Classification of patients based on Clinical and demographic traits (like age, race, financial characteristics) to enable\ncustomized servicing.\n\u2022 Identification and prioritization of the pain areas that are leading to lower satisfaction like Discharge codes, discharge\nhour, age, length of stay, deprivation Index etc.\n\u2022 Quantify the impact under interactions for the key variables. Leveraged Decision tree and Logistic to build the models and quantify the impact.\n\u2022 Inferences were drawn from the models and variables were prioritized.\nTools/Techniques: R /SQL/ Microsoft Excel/Tableau', u'Data Analyst\nNovartis\nJune 2012 to January 2013\nResponsibilities:\n\u2022 Performed data pre-processing and cleaning to prepare data sets for further statistical analysis; including outlier\ndetection and treatment, missing value treatment, variable transformation and various other data manipulation\ntechniques using SAS programming language.\n\u2022 Developed codes utilizing SAS Base/SAS SQL and prepared datasets of adverse events generated from clinical trials.\n& Post-Market Surveillance for further analysis by HEOR (Health Economics & Outcomes Research) team.\n\u2022 Modified existing SAS/SQL programs and created new programs using SAS macro variables to improve ease and speed of modification as well as consistency of results.\n\u2022 Data Extraction, Data analysis and Data compilation using SQL and Microsoft Excel.', u""Data Analyst\nCR BIO Sciences\nOctober 2011 to May 2012\nResponsibilities:\n\u2022 Performed data pre-processing and cleaning to prepare data sets for further statistical analysis; including outlier\ndetection and treatment, missing value treatment, variable transformation and various other data manipulation\ntechniques using R programming language.\n\u2022 Data pulling, Data analysis and Data compilation using SQL and Microsoft Excel.\n\u2022 Ensured reconciliation between Clinical Database (OCRDC-Oracle Clinical Remote Data Capture) and Safety\ndatabase.\nOTHER ANALYTICS PROJECTS\n\u2022 Parkinson's Disease Prediction model Toolkit: Python, SAS Base & SAS E-Miner & Advanced Excel\nFocus: Developed a prediction model that help healthcare practitioners for early detection of Parkinson's disease\nbased on speech signals. The model reduces the effort and cost of the patient visiting the clinic multiple times.\n\u2022 Transportation Safety Board of Nebraska Toolkit: R, SAS Base & SAS E-Miner, Tableau, Advanced Excel\nFocus: Implemented CRISP-DM to build a predictive model. Identified the factors which determined whether a\ncrash will occur using techniques such as Logistic Regression, correlation Analysis, Random Forest and extracted\nkey factors using association rule mining.\n\u2022 Big Data- Analysis of Climatic and Temperature Data from NCDC Toolkit: Cloudera Impala, Hive (Hadoop\nEnvironment), Spark, Pyhton & Tableau.\nFocus: Finding descriptive statistics such as the min/max/mean/median temperatures of 40 years of data and clustered weather stations with similar trends, build a time series model to predict future temperatures.""]","[u'Masters of Science in Analytics', u'Certificate in Big Data Analytics & Optimization', u'Master of Pharmacy in Pharmacy']","[u'Bowling Green State University\nMay 2017', u'Carnegie Mellon University\nAugust 2015', u'Jawaharlal Nehru Technological University\nMay 2012']"
0,https://resumes.indeed.com/resume/7316e222136bbc3f,"[u'Data Analyst\nNissan\nJanuary 2016 to January 2017\nMonitored and analyzed financial, statistical operational data trends for dealers in the central Thailand region\n\u2022 Built and maintained databases for forecasting future financial\nperformance based on CSI and SSI index\n\u2022 Developed spreadsheet models for diverse projects and analysis that\nsimplifies month to month decisions for Nissan dealers\n\u2022 Presented oral and written reports on general market trends, individual\ndealers and entire region']",[u'B.S. in Statistics'],[u'University of California - Santa Barbara\nJanuary 2011 to January 2015']
0,https://resumes.indeed.com/resume/99242640b078e711,"[u'Data Analyst\nAlpha Bookings - Delhi, Delhi\nJune 2017 to July 2017\n\u2022 Used custom SQL queries with Tableau to make dashboard for visualizing behavior of users on hotel booking website\n\u2022 Achieved 15% increase in sales by identifying up-selling and cross-selling opportunities using recommender system', u'Data Analyst\nJacobs Engineering Group - Gurgaon, Haryana\nJune 2012 to June 2016\n\u2022 Interacted at all levels of business to present proposals as per client requirements and update project status\n\u2022 Developed algorithms for application of finite-element method and performed quantitative analysis\n\u2022 Created databases for different activities on SQL server and leveraged it using Python to do statistical analysis\n\u2022 Decreased project cost by $200,000 by quantifying the lag in activities and minimizing it using discrete-event simulation\n\u2022 Automated the visualizations and in-depth analysis using Python resulting in increased efficiency by 15%', u'Co-founder\nJacobs Engineering Group - Delhi, Delhi\nOctober 2014 to September 2015\nAn innovative print media advertising platform using shopping bags that served local fashion brands in India\n\u2022 Decreased the transport cost by 10% modeling linear programming in Python using Scipy, numpy and pandas\n\u2022 Designed experiment for A/B testing and implemented model results to enhance the effectiveness of advertisement\nPROJECTS, COMPETITIONS AND ACADEMIC STUDIES']","[u'Masters of Science in Management Information System', u'Bachelors of Engineering in Civil Engineering']","[u'The University of Texas at Dallas, School of Management Dallas, TX\nMay 2018', u'Delhi College of Engineering Delhi, Delhi\nJune 2012']"
0,https://resumes.indeed.com/resume/55b97bfc2ecb274f,"[u'Data Scientist\nSouth Dakota State University - Brookings, SD\nMarch 2017 to January 2018\n\u2022 Analyzed and processed complex data sets using advanced querying, visualization and analytics tools.\n\u2022 Create complex SQL stored procedures, Triggers, Functions, Views, Indexes in Microsoft SQLServer\n\u2022 Scheduled jobs to automate different database related activities such as backups, maintaining index, and monitoring disk space and backup verification\n\u2022 Developed subject segmentation algorithm using R.\n\u2022 Involved in the process of load, transform, and analyze data from various sources into HDFS (Hadoop\nDistributed File System) using Hive, Pig and Sqoop.\n\u2022 Used Python 3.0 (numpy, scipy, pandas, scikit-learn, seaborn, NLTK) and Spark 1.6 / 2.0 (PySpark, ML) to develop variety of models and algorithms for analytic purposes.\n\u2022 Developed an algorithm that can identify ""bad"" assessments that are expected to Fail under central review.\n\u2022 Used Statistical methods to analyze the performance of each clinical site across 27 countries on 30 studies.\nPredicting number of days to reach the target number of sites for a clinicalstudy.\n\u2022 Processed huge datasets (over billion data points, over 1 TB of datasets) for data association pairing and provided insights into meaningful data association and trends.\n\u2022 Developed pipelines for test data.\n\u2022 Enhanced statistical models(linear mixed models) for predicting the best products for commercialization\nusing Machine Learning Linear regression models, KNN and K-means clustering algorithms.\n\u2022 Builds machine learning models on independent AWS EC2 server to enhance data quality.\n\u2022 Handle Unstructured Data to derive some information from which helps in development of the company.\n\u2022 Finding the sentiment about the organization using Text Mining and NLP techniques.', u""Data Analyst\nATOM SOLUTIONS\nJanuary 2015 to June 2016\nProject:\n\u2022 Creating a web application to narrow job search for job seekers.\n\u2022 Scheduled jobs to automate different database related activities such as backups, maintaining index, and monitoring disk space and backup verification\n\u2022 Using text mining techniques to infer information from unstructured data.\n\u2022 Used unsupervised learning algorithms on job description to classify them into different clusters.\n\u2022 Applied Clustering Algorithms such as K-Means to categorize customers into certain groups.\n\u2022 Data cleansing, transformation and creating new variables using R\n\u2022 Coded JSP pages, Java Servlets in struts framework, prepared test cases for reviewing and understanding\nflow of application.\nProduct Sales:\n\u2022 Performed exploratory data analysis like calculation of descriptive statistics, detection of outliers,\nassumptions testing, factor analysis, etc., in R\n\u2022 Analyze and Prepare data, identify the patterns on dataset by applying historical models\n\u2022 Performed data transformation method for rescaling and normalizing variables\n\u2022 Exploratory data analysis and Feature engineering to best fit the regression model.\n\u2022 Applied predictive models to car maintenance records to predict car sales based upon it's usage and fuel\nefficiency.\n\u2022 Implemented forecasting models to predict car sales for to coming seasons so that we can meet the demand.""]","[u'Master of Science in Data Science', u'Bachelor of Technology in Computer Science and Engineering']","[u'SOUTH DAKOTA STATE UNIVERSITY\nAugust 2016 to December 2018', u'GITAM UNIVERSITY\nJune 2012 to April 2016']"
0,https://resumes.indeed.com/resume/9d7e3a92a2a46cd0,"[u'Sr. Data Analyst\nAT&T - Dallas, TX\nDecember 2016 to Present\nMajor Contribution:\n\u2022 Setup databases for Employee, Facility, Vendor and Payroll data.\n\u2022 Extensively used Stylist, Personate, and Accumail to cleanse data and map zip codes to 9-digit codes for zip to district matching.\n\u2022 Created SSIS package to export and import data from SQL Server to Access, text and Excel file. Used Event Handlers for Exception Handling in SSIS packages\n\u2022 Developed and deployed SSIS packages for ETL from OLTP and various sources to staging and staging to Data warehouse using Lookup, Fuzzy Lookup, Derived Columns, Condition Split, Slowly Changing Dimension and more. Performed ETL mappings using MS SQL Server Integration Services\n\u2022 Modified the existing SQL queries according to the requests and executed them to check the performance.\n\u2022 Designed and implemented Data Marts, Facts, Dimensions and OLAP cubes having Star/ Snowflake schemas using Dimensional Data Modeling principles in SSAS\n\u2022 Created data flow model using MS Visio\n\u2022 Used SQL Server Profiler to trace the slow running queries and the server activity\n\u2022 Scheduled Jobs and Alerting using SQL Server Agent\n\u2022 Created stored procedures, views, triggers, user defined functions to incorporate the flow of business\n\u2022 Involved in project status report updates/ creation of issue items for reviews/ updates of technical and transformation files on timely basis\n\u2022 Designed, Developed and Deployed reports in MS SQL Server environment using SSRS\n\u2022 Generated Sub-Reports, Drill down reports, Drill through reports and Parameterized reports using SSRS\n\u2022 Created SSRS reports to retrieve data using Stored Procedures that accept parameters\n\u2022 Performed daily tasks including backup and restore by using MS SQL Server tools like SQL Server Management Studio, SQL Server Profiler, SQL Server Agent, and Database Engine Tuning Advisor\n\u2022 Documented all the new procedures, troubleshooting issues and solutions, Enhancement and changes in Daily and monthly running processes, and maintain the database for future reference\n\u2022 Performed database backup, performance optimization, and database maintenance.\n\u2022 Provided technical support for database related problems and troubleshooting back-end issues.\n\u2022 Participated in the design, execution, and ongoing maintenance of disaster recovery and high availability planning.\n\u2022 Processed client requests using data mining tools in Visual FoxPro, SQL Server, and MS Access. Design and create custom reports using Active Report Writer.\n\u2022 Implemented T-SQL data access methods and processes (stored procedures, views, triggers).', u'Sr. Data Analyst\nBerlex Pharmaceuticals - Montville, NJ\nJuly 2015 to December 2016\nMontville, NJ.\nDuration: July 2015 to Dec 2016\nEnvironment: ETL, Requisite Pro, Rational Rose, Clear Quest, Oracle, SAS, MS Visio, MS Office, Test Direct.\nRole: Sr. Data Analyst\n\nMajor Contribution:\n\u2022 Acted as liaison among business units, primarily between the stake holders, Project Manager, IT Specialist (Developers), Administrators and Testers.\n\u2022 Understood the project scope, objectives, and timelines with MS Project and helped detail to various teams and prepared the ""As is"" and ""Will be"" document to help identify the benefits and project requirements.\n\u2022 Converted User Requirements into Technical Requirements and vice versa.\n\u2022 Worked in accordance to the methodology laid down by the RUP for various phases of the business and created business vision, business architecture documents and mock reports.\n\u2022 Facilitated JAD to identify business rules and requirements and then documented them in a format that can be reviewed and understood by both business teams and technical teams.\n\u2022 Organized requirements into groupings of essential business processes, business rules, and information needs, and ensured that critical requirements are not missed.\n\u2022 Implemented business modeling, data modeling using MS Visio, UML and Rational Rose.\n\u2022 Documented system specifications in Rational RequisitePro to track and analyze the requirements matrix as linked to code.\n\u2022 Used data warehousing tools like ETL to collect the data from different departments and make it available to the management department for quick response. Also used Online Transaction Process (OLTP) and Online Analytical Process (OLAP) for day to day basis operation and analysis.\n\u2022 Identified data elements from the source systems, performed data analysis to come up with data cleansing and integration rules for the ETL process.\n\u2022 Performed ETL (Informatica) tool evaluations. Involved in normalized and multi-dimensional modeling and building of metadata repository\n\u2022 Used Client/Server, Multi-Tier and Web-Based Architectures by applying Top-Down, Bottom-Up / both approaches in data modeling.\n\u2022 Used SAS/ACCESS to extract data and analyzed clinical research data using SAS/STAT and used SAS/GRAPH to display results as graphs.\n\u2022 Created the ad hoc reports using SAS reporting tool.\n\u2022 Performed risk analysis and claims analysis with technical team to identify the key business risks areas for the project and prioritized the application development and testing.\n\u2022 Coordinated project scheduling with the software development manager.', u'Data Analyst\nPenguin Random House - New York, NY\nSeptember 2014 to June 2015\nMajor Contribution:\n\u2022 Worked on e-book price validation\n\u2022 Accessed data from IBM Netezza database from SQL\n\u2022 Data cleaning, Data Visualization, Detection of missing data and skewness, correlation analysis, principal component analysis and model development were performed on the data\n\u2022 Developed a predictive model using scikit-learn library to classify e-book price changes as revenue positive or not.\n\u2022 Counterfactual data was estimated using Bayesian structural time-series models were used to calculate target variables\n\u2022 Developed and initiated more efficient data collection procedures\n\u2022 Utilized Microsoft Excel and access to manipulate, cleanse, and process large data sets\n\u2022 Entrusted with the timely developments and maintenance of data collection protocol.\n\u2022 Generated and performed comprehensive reviews of qualitative and quantitative statistical reports for internal and external personnel\n\u2022 Managed end-to-end process for updating and verifying special orders data\n\u2022 Analyzed inventory usage reports to avoid backordering\n\u2022 Ensured data accuracy through the creation and implementation of data integrity queries\n\u2022 Provided explanation of complex classifiers such as Random Forest', u'Data Analyst\nLKP Securities Ltd - Hyderabad, Telangana\nJanuary 2013 to June 2014\nMajor Contribution:\n\u2022 Involved in understanding and analyzing the Share Market Industry and Company Requirements\n\u2022 Interacted with the stakeholders to identify business needs and gather requirements for data analysis\n\u2022 Involved in developing the Mail, Telephonic, Personal and Electronic Questionnaire\n\u2022 Comparative Analysis for Different organization (SWOT Analysis, Market Share, Profitability and their existing Products)\n\u2022 Collected Data as per the Industry which is relevant to share market\n\u2022 Analyzed the current company and their existing products\n\u2022 Acquired data from primary or secondary data sources and maintained databases/data systems.\n\u2022 Interpreted data, analyzed results using statistical techniques and provide ongoing reports.\n\u2022 Pie-charts, Bar graph, Factor analysis, ANOVA, Correlation, statistical data analytics techniques used to Analyze the data Using SPSS, Excel and R\n\u2022 Identified, analyzed, and interpreted trends or patterns in complex data sets.\n\u2022 Identified the patterns and frequency of the data in the dataset and prepared the reports.\n\u2022 Managed the data quality audits/rules and generated the reports\n\u2022 Converted business rules into technical rules for data quality analysis.\n\u2022 Created work plans, programming in R, created presentations for the client and designed algorithms,\n\u2022 Designed & implemented demand generation incentive compensation plans.', u'Jr. Data Analyst\nNettlinx Pvt Ltd - Hyderabad, Telangana\nMarch 2012 to December 2012\nMajor Contribution:\n\u2022 Predictive Modelling - Ford Challenge\n\u2022 Built model to predict binary response on dataset with 28variables\n\u2022 Data cleaning, data visualization, detection of missing data and skewness, correlation analysis using heat maps, principal component analysis and model development were performed on the data\n\u2022 Ensemble learning method applied for better predictive performance\n\u2022 Modelling using Logistic Regression for the binary response variable\n\u2022 Random forest modelling was explored for better predictability\n\u2022 Achieved model accuracy of 75% and sensitivity of 54% with Random forest']",[u'Bachelors in Computer Science'],"[u'Osmania University Hyderabad, Telangana']"
0,https://resumes.indeed.com/resume/0e72527492b2dacb,"[u'Data Scientist\nFresh Direct - Long Island, NY\nAugust 2017 to Present\nDescription: Fresh Direct is an online grocer that delivers to residences and offices in the New York City metropolitan area. It also offers next-day delivery to much of New York City and parts of Nassau and Westchester Counties, New York; Fairfield County, Connecticut; Hoboken, Newark, and Jersey City; Philadelphia, Pennsylvania; and Washington, DC. Fresh Direct custom-prepares groceries and meals for its customers, a manufacturing practice called Just In Time that reduces waste and improves quality and freshness. The service is popular for its distribution of organic food and locally grown items, as well as items that consumers see in supermarkets daily. It also delivers numerous kosher foods and is recognized by the Marine Stewardship Council as a certified sustainable seafood vendor.\n\nResponsibilities:\n\u2022 Improving Fraud Detection using Digital Links at Amazon, Seattle.\n\u2022 Scaled up to Machine Learning Pipelines: 4600 processors, 35000 GB memory achieving 5-minute execution.\n\u2022 Configured the project on Web-Sphere 6.1 application servers.\n\u2022 Handled 2+ TB data with graphs up to 130 GB (50M nodes, 100M edges) using single-node in-disk scaling.\n\u2022 Developed a Machine Learning test-bed with 24 different model learning and feature learning algorithms.\n\u2022 By thorough systematic search, demonstrated performance surpassing the state-of-the-art (deep learning).\n\u2022 Up to 10 times more accurate predictions over existing state-of-the-art algorithms.\n\u2022 Developed in-disk, huge (100GB+), highly complex Machine Learning models.\n\u2022 Used SAX and DOM parsers to parse the raw XML documents.\n\u2022 Used RAD as Development IDE for web applications.\n\u2022 Demonstrated performances comparable to other state-of-the-art deep learning models.\n\u2022 Applied Machine Learning algorithms to diagnose blood loss from vital signs (ECG, HF, GSR, etc.)\n\u2022 Devised and implemented a Vehicle Speed Detector using low-power LEDs and field-tested for robustness.\n\u2022 National Highways Authority (Govt. of India) is evaluating the design for installations across the country.\n\u2022 IIT Madras has installed the speed detectors across the institute for permanent speed limit enforcement.\n\u2022 Developed & tested feature tracking algorithms for Intelligent Transportation Systems Computer Vision.\n\u2022 Analyzed SIFT feature descriptors and their resilience to changes in illumination.\n\u2022 Devised a novel machine learning algorithm for classification of ECG abnormalities.\n\u2022 Designed a new Machine Learning pipeline to replace existing prod: AUC performance Increase from 83% to 90%.\n\u2022 Deployed GUI pages by using JSP, JSTL, HTML, DHTML, XHTML, CSS, JavaScript, and AJAX.\n\u2022 Long Short-Term Memory Recurrent Neural Networks (LSTM RNNs) learnt using Deep Learning techniques applied to Problem X.\n\u2022 LSTM RNNs applied to Problem Y.\n\nEnvironment: R 9.0, Informatica 9.0, ODS, OLTP, Oracle 10g, Hive, OLAP, DB2, Metadata, MS Excel, Mainframes MS Vision, Rational Rose.', u""Data Scientist\nBlack Rock Inc - Wilmington, DE\nMay 2016 to July 2017\nDescription: Black Rock, Inc. is an American global investment management corporation based in New York City. Founded in 1988, initially as a risk management and fixed income institutional asset manager, Black Rock is the world's largest asset manager with $6.3 trillion in assets under management as of December 2017. Black Rock operates globally with 70 offices in 30 countries and clients in 100 countries. Due to its power and the sheer size and scope of its financial assets and activities, Black Rock has been called the world's largest shadow bank.\n\nResponsibilities:\n\u2022 Manipulating, cleansing & processing data using Excel, Access and SQL.\n\u2022 Responsible for loading, extracting and validation of client data.\n\u2022 Liaising with end-users and 3rd party suppliers.\n\u2022 Analyzing raw data, drawing conclusions & developing recommendations\n\u2022 Writing T-SQL scripts to manipulate data for data loads and extracts.\n\u2022 Developing data analytical databases from complex financial source data.\n\u2022 Performing daily system checks.\n\u2022 Data entry, data auditing, creating data reports & monitoring all data for accuracy.\n\u2022 Designing, developing and implementing new functionality.\n\u2022 Monitoring the automated loading processes.\n\u2022 Advising on the suitability of methodologies and suggesting improvements.\n\u2022 Carrying out specified data processing and statistical techniques.\n\u2022 Supplying qualitative and quantitative data to colleagues & clients.\n\u2022 Using Informatica & SAS to extract transform & load source data from transaction\nsystems.\n\u2022 Creating data pipelines using big data technologies like Hadoop, spark etc.\n\u2022 Creating statistical models using distributed and standalone models to build various diagnostics, predictive and prescriptive solution.\n\u2022 Utilize a broad variety of statistical packages like SAS, R , MLIB, Graphs, Hadoop, Spark , Map-Reduce, Pig and others\n\u2022 Refine and train models based on domain knowledge and customer business objectives\n\u2022 Deliver or collaborate on delivering effective visualizations to support the client business objectives\n\u2022 Communicate to your peers and managers promptly as and when required.\n\u2022 Produce solid and effective strategies based on accurate and meaningful data reports and analysis and/or keen observations.\n\u2022 Establish and maintain communication with clients and/or team members; understand needs, resolve issues, and meet expectations\n\u2022 Developed web applications using .net technologies; work on bug fixes/issues that arise in the production environment and resolve them at the earliest.\n\nEnvironment: SQL Server 2008R2/2005 Enterprise, SSRS, SSIS, Crystal Reports, Hadoop, Windows Enterprise Server 2000, DTS, SQL Profiler, and Query Analyzer."", u""Data Scientist\nIntrepid Potash Inc - Denver, CO\nJanuary 2015 to April 2016\nDescription: Intrepid Potash, Inc. produces and markets muriate of potash and langbeinite under the Trio brand name primarily in the United States.\nResponsibilities:\n\n\u2022 Data mining using state-of-the-art methods.\n\u2022 Extending company's data with third party sources of information when needed.\n\u2022 Enhancing data collection procedures to include information that is relevant for building analytic systems.\n\u2022 Processing, cleansing, and verifying the integrity of data used for analysis.\n\u2022 Doing ad-hoc analysis and presenting results in a clear manner.\n\u2022 Creating automated anomaly detection systems and constant tracking of its performance.\n\u2022 Strong command of data architecture and data modelling techniques.\n\u2022 Hands on experience with commercial data mining tools such as Splunk, R, Map reduced, Yarn, Pig, Hive, Floop, Oozie, Scala, HBase, Master HDFS, Sqoop, Spark, Scala (Machine learning tool) or similar software required depending on seniority level in job field.\n\u2022 Developed scalable machine learning solutions within a distributed computation framework (e.g. Hadoop, Spark, Storm etc.).\n\u2022 Utilizing NLP applications such as topic models and sentiment analysis to identify trends and patterns within massive data sets.\n\u2022 Knowledge in ML & Statistical libraries (e.g. Scikit-learn, Pandas).\n\u2022 Having knowledge to build predict models to forecast risks for product launches and operations and help predict workflow and capacity requirements for TRMS operations\n\u2022 Having experience with visualization technologies such as Tableau\n\u2022 Draw inferences and conclusions, and create dashboards and visualizations of processed data, identify trends, anomalies\n\u2022 Generation of TLFs and summary reports, etc. ensuring on-time quality delivery.\n\u2022 Participated in client meetings, teleconferences and video conferences to keep track of project requirements, commitments made and the delivery thereof.\n\u2022 Solved analytical problems, and effectively communicate methodologies and results\n\u2022 Worked closely with internal stakeholders such as business teams, product managers, engineering teams, and partner teams.\n\u2022 Created automated metrics using complex databases.\n\u2022 Foster culture of continuous engineering improvement through mentoring, feedback, and metrics.\n\nEnvironment: Erwin r9.0, Informatica 9.0, ODS, OLTP, Oracle 10g, Hive, OLAP, DB2, Metadata, MS Excel, Mainframes MS Visio, Rational Rose, Requisite Pro, Hadoop, PL/SQL, etc."", u'Data Scientist\nTyler Technologies Inc - Dallas, TX\nMay 2013 to December 2014\nDescription: Tyler Technologies, Inc. provides integrated information management solutions and services for the public sector with a focus on local governments in the United States and internationally.\n\nResponsibilities:\n\u2022 Statistical Modeling with ML to bring Insights in Data under guidance of Principal Data Scientist\n\u2022 Data modeling with Pig, Hive, Impala.\n\u2022 Ingestion with Sqoop, Flume.\n\u2022 Used SVN to commit the changes into the main EMM application trunk.\n\u2022 Understanding and implementation of text mining concepts, graph processing and semi structured and unstructured data processing.\n\u2022 Worked with Ajax API calls to communicate with Hadoop through Impala Connection and SQL to render the required data through it .These API calls are similar to Microsoft Cognitive API calls.\n\u2022 Good grip on Cloudera and HDP ecosystem components.\n\u2022 Used Elastic Search (Big Data) to retrieve data into application as required.\n\u2022 Performed Map Reduce Programs those are running on the cluster.\n\u2022 Developed multiple Map-Reduce jobs in java for data cleaning and preprocessing.\n\u2022 Analyzed the partitioned and bucketed data and compute various metrics for reporting.\n\u2022 Involved in loading data from RDBMS and web logs into HDFS using Sqoop and Flume.\n\u2022 Worked on loading the data from MySQL to HBase where necessary using Sqoop.\n\u2022 Developed Hive queries for Analysis across different banners.\n\u2022 Extracted data from Twitter using Java and Twitter API. Parsed JSON formatted twitter data and uploaded to database.\n\u2022 Launching Amazon EC2 Cloud Instances using Amazon Images (Linux/ Ubuntu) and Configuring launched instances with respect to specific applications.\n\u2022 Exported the result set from Hive to MySQL using Sqoop after processing the data.\n\u2022 Analyzed the data by performing Hive queries and running Pig scripts to study customer behavior.\n\u2022 Have hands on experience working on Sequence files, AVRO, HAR file formats and compression.\n\u2022 Used Hive to partition and bucket data.\n\u2022 Experience in writing Map-Reduce programs with Java API to cleanse Structured and unstructured data.\n\u2022 Wrote Pig Scripts to perform ETL procedures on the data in HDFS.\n\u2022 Created HBase tables to store various data formats of data coming from different portfolios.\n\u2022 Worked on improving performance of existing Pig and Hive Queries.\n\nEnvironment: SQL/Server, Oracle 9i, MS-Office, Teradata, Informatica, ER Studio, XML, Business Objects, HDFS, Teradata 14.1, JSON, HADOOP (HDFS), Map-Reduce, PIG, Spark, R Studio, MAHOUT, JAVA, HIVE, AWS..', u'Data Architect/Data Modeler\nCorporation Bank - Mangalore, Karnataka\nDecember 2011 to April 2013\nDescription: Corporation Bank is a public-sector banking company headquartered in Mangalore, India. The bank has a pan-Indian presence. Presently, the bank has a network of 2,440 fully automated CBS branches, 3,040 ATMs, and 4,724 branchless banking units across the country.\n\nResponsibilities:\n\u2022 Worked with large amounts of structured and unstructured data.\n\u2022 Knowledge in Machine Learning concepts (Generalized Linear models, Regularization, Random Forest, Time Series models, etc.)\n\u2022 Worked in Business Intelligence tools and visualization tools such as Business Objects, Tableau, ChartIO, etc.\n\u2022 Deployed GUI pages by using JSP, JSTL, HTML, DHTML, XHTML, CSS, JavaScript, and AJAX.\n\u2022 Configured the project on Web-Sphere 6.1 application servers\n\u2022 Implemented the online application by using Core Java, Jdbc, JSP, Servlets and EJB 1.1, Web Services, SOAP, WSDL.\n\u2022 Handled end-to-end project from data discovery to model deployment.\n\u2022 Monitoring the automated loading processes.\n\u2022 Communicated with other Health Care info by using Web Services with the help of SOAP, WSDL JAX-RPC\n\u2022 Used Singleton, factory design pattern, DAO Design Patterns based on the application requirements\n\u2022 Used SAX and DOM parsers to parse the raw XML documents\n\u2022 Used RAD as Development IDE for web applications.\n\u2022 Preparing and executing Unit test cases\n\u2022 Used Log4J logging framework to write Log messages with various levels.\n\u2022 Involved in fixing bugs and minor enhancements for the front-end modules.\n\u2022 Implemented Microsoft Visio and Rational Rose for designing the Use Case Diagrams, Class model, Sequence diagrams, and Activity diagrams for SDLC process of the application\n\u2022 Doing functional and technical reviews\n\u2022 Maintenance in the testing team for System testing/Integration/UAT.\n\u2022 Guaranteeing quality in the deliverables.\n\u2022 Conducted Design reviews and Technical reviews with other project stakeholders.\n\u2022 Was a part of the complete life cycle of the project from the requirements to the production support\n\u2022 Created test plan documents for all back-end database modules\n\u2022 Implemented the project in Linux environment.\n\nEnvironment: R 3.0, Erwin 9.5, Tableau 8.0, MDM, QlikView, MLLib, PL/SQL, HDFS, Teradata 14.1, JSON, HADOOP (HDFS), Map-Reduce, PIG, Spark, R Studio, MAHOUT, JAVA, HIVE, AWS.', u'Data Analyst/Data Modeler\nGMR Group - Bengaluru, Karnataka\nMay 2009 to November 2011\nDescription: GMR Group is an infrastructural company headquartered in Bengaluru. The company was founded in 1978 by Grandhi Mallikarjuna Rao.\n\nResponsibilities:\n\u2022 Developed Internet traffic scoring platform for ad networks, advertisers and publishers (rule engine, site scoring, keyword scoring, lift measurement, linkage analysis).\n\u2022 Responsible for defining the key identifiers for each mapping/interface.\n\u2022 Clients include eBay, Click Forensics, Cars.com, Turn.com, Microsoft, and Looksmart.\n\u2022 Designed the architecture for one of the first analytics 3.0. Online platforms: all-purpose scoring, with on-demand, SaaS, API services. Currently under implementation.\n\u2022 Web crawling and text mining techniques to score referral domains, generate keyword taxonomies, and assess commercial value of bid keywords.\n\u2022 Developed new hybrid statistical and data mining technique known as hidden decision trees and hidden forests.\n\u2022 Reverse engineering of keyword pricing algorithms in the context of pay-per-click arbitrage.\n\u2022 Implementation of Metadata Repository, Maintaining Data Quality, Data Cleanup procedures, Transformations, Data Standards, Data Governance program, Scripts, Stored Procedures, triggers and execution of test plans\n\u2022 Performed data quality in Talend Open Studio.\n\u2022 Coordinated meetings with vendors to define requirements and system interaction agreement documentation between client and vendor system.\n\u2022 Automated bidding for advertiser campaigns based either on keyword or category (run-of-site) bidding.\n\u2022 Creation of multimillion bid keyword lists using extensive web crawling. Identification of metrics to measure the quality of each list (yield or coverage, volume, and keyword average financial value).\n\u2022 Enterprise Metadata Library with any changes or updates.\n\u2022 Document data quality and traceability documents for each source interface.\n\u2022 Establish standards of procedures.\n\u2022 Generate weekly and monthly asset inventory reports.\n\nEnvironment: Erwin r7.0, SQL Server 2000/2005, Windows XP/NT/2000, Oracle 8i/9i, MS-DTS, UML, UAT, SQL Loader, OOD, OLTP, PL/SQL, MS Visio, Informatica.']",[u'Bachelor of Computer Science & Technology in TOOLS AND TECHNOLOGIES'],[u'Quality Center\nOctober 2011']
0,https://resumes.indeed.com/resume/9db8fd073a5fec35,"[u'Data Analyst Intern\nETHNA ATTRIBUTES SOFT TECHNOLOGIES - Chennai, Tamil Nadu\nJanuary 2017 to July 2017\n\u2022 Presented seminars on Data Warehouse, Big Data and Machine Learning.\n\u2022 Responsible for visualization of ""Jazz Cinemas"" project data on Tableau software.\n\u2022 Prepared the final report for ""Airlines- Trends in seasonal delay"" project.\n\u2022 Mastered Hadoop and Hive configurations for UbuntuOS.']","[u'Master of Science in Information System', u'Bachelor of Engineering in Electronics and Communication']","[u'NEW JERSEY INSTITUTE OF TECHNOLOGY Newark, NJ\nMay 2019', u'ANNA UNIVERSITY Chennai, Tamil Nadu\nAugust 2013 to May 2017']"
0,https://resumes.indeed.com/resume/86ed748804aad1a6,"[u'Data Analyst\nGAP International Inc - Springfield, PA\nJanuary 2013 to January 2016\nResponsibilities:\n\u2022 Acquire data from primary or secondary data sources and maintain databases/data systems.\n\u2022 Established new client data preparing them for entry into the new platform.\n\u2022 Loaded data by converting a CSV file into the corresponding database tables.\n\u2022 Work with management team to create a prioritized list of needs for each business segment.\n\u2022 Ran diagnostic survey tool to measure and predict team performance.\n\u2022 Extracted, compiled and analyzed data using Excel and Adobe to build reports and provide recommendations to clients to improve team performance.\n\u2022 Generated ongoing reports of each active account as they are being consulted.\n\u2022 Involved in client-facing activities where reports were presented to upper-management and to each team.\n\u2022 Communicated generated reports to GAP management showcasing the progress of each account daily.\n\u2022 Identify and address data quality problems by eliminating duplicates and standardizing data sets.\n\u2022 Locate and define new process improvement opportunities.\n\u2022 Used advanced Excel functions to generate spreadsheets and pivot tables.\n\u2022 Performed daily data queries and prepared reports on daily, weekly, monthly, and quarterly basis.\n\u2022 Advise client on system usage.\n\u2022 Execute customized self-service client dashboards.', u'Data Analyst\nAl Nabil Company for Food Products\nJanuary 2008 to January 2013\nResponsibilities:\n\u2022 Maintain strong relationships with all clients to ensure invoices are clear for payment.\n\u2022 Manage the timely and effective collection of all debts and payments.\n\u2022 Negotiate repayment plans when necessary.\n\u2022 Resolve all issues raised internally and externally around outstanding invoices.\n\u2022 Manage relations with the sales department.\n\u2022 Manage customer credit files.\n\u2022 Maintained credit and billing of over 100 accounts of major contracting companies in the Middle East.\n\u2022 Generated reports assisting the sales department to forecast demand for product distribution.\n\u2022 Prepared reports modeling the performance of different pipelines and presenting them to management.\n\u2022 Coordinated with multiple departments to establish business needs and better understanding of data.\n\u2022 Analyzing and evaluating accounting documentation.\n\u2022 Analyze data to identify problematic areas and suggest improvements.']",[u'Bachelor of Science in Business Administration'],[u'Baghdad University - College of Administration and Economics']
0,https://resumes.indeed.com/resume/b6339922aa04bee6,"[u'Data Modeler/Data Analyst\nCVS Health - Richardson, TX\nAugust 2016 to Present\nDescription: CVS is a healthcare provider which combines advanced data analytics with highly targeted interventions to achieve meaningful impact in clinical outcomes and financial performance across the healthcare landscape.', u""Anthem Inc - Norfolk, VA\nFebruary 2014 to June 2016\nData Modeler\nDescription: Anthem Inc. is a leading global provider of insurance, annuities, and employee benefit programs, serving 90 million customers. The project involved designing OLTP platform and is responsible for building, enhancing, and maintaining databases in support of applications across the enterprise.\nResponsibilities:\n\u2022 Worked with Business Analysts team in requirements gathering and in preparing functional specifications and translating them to technical specifications.\n\u2022 Extensively involved in developing logical and physical data models that fit in exactly for the current state and the future state data elements and data flows using Erwin.\n\u2022 Documenting business names and descriptions for tables and columns, relationships among datasets and data types, business rules and domains used ER/Studio.\n\u2022 Using Kettle Scripting/ Pentaho processed data from one table and replaced some values, filtered some values and syncing two data sources.\n\u2022 Utilized existing Informatica, Teradata, and SQL Server to deliver work and fix production issues on time in fast paced environment.\n\u2022 Defined the key columns for the Dimension and Fact tables of both the Warehouse and Data Mart.\n\u2022 Normalized the tables/relationships to arrive at effective Relational Schemas without any redundancies.\n\u2022 Interacted with the End users frequently and transferred the knowledge to them.\n\u2022 Conducted and participated JAD sessions with the Project managers, Business Analysis Team, Finance and Development teams to gather, analyze and document the Business and reporting requirements.\n\u2022 Worked on the Snow-flaking the Dimensions to remove redundancy.\n\u2022 Conduct Design discussions and meetings to come out with the appropriate Data Mart at the lowest level of grain for each of the Dimensions involved.\n\u2022 Interacting with the Business Users for gathering data design requirements and taking feedback on improvements.\n\u2022 Performed Normalization of the existing OLTP systems (3rd NF) to speed up the DML statements execution time.\n\u2022 Designed a STAR schema for the detailed data marts and Plan data marts involving shared dimensions (Conformed).\n\u2022 Worked on Informatica Data Quality to resolve customers address related issues.\n\u2022 Created and maintained Logical Data Model (LDM) for the project. Includes documentation of all entities, attributes, data relationships, primary and foreign key structures, allowed values, codes, business rules, glossary terms, etc.\n\u2022 Worked with data owners/stewards to ensure awareness of data quality standards and monitoring requirements.\n\u2022 Performed Data Profiling to assess the risk involved in integrating data for new applications, including the challenges of joins and to track data quality.\n\u2022 Validated and updated the appropriate LDM's to process mappings, screen designs, use cases, business object model, and system object model as they evolve and change.\n\u2022 Conduct Design reviews with the business analysts and content developers to create a proof of concept for the reports.\n\u2022 Worked with data mapping from source to target and data profiling to maintain the consistency of the data.\n\u2022 Ensured the feasibility of the logical and physical design models.\n\u2022 Collaborated with the Reporting Team to design Monthly Summary Level Cubes to support the further aggregated level of detailed reports.\n\u2022 Prepared documentation for all entities, attributes, data relationships, primary and foreign key structures, allowed values, codes, business rules, and glossary evolve and change during the project.\nEnvironment: Erwin 9.2, Informatica Power Center 9.X, Oracle 10g, Microsoft Excel, MS Word, SIEM Technology, Power Point, TOAD, and SQL developer for Oracle."", u'Data Modeler/ Data Analyst\nUSAA Bank - San Antonio, TX\nOctober 2012 to December 2013\nDescription: USAA Bank has grown to become leading provider of a wide range of services including investment management, retirement planning and benefits outsourcing services. The project was for the enhancement of fixed income portfolio management systems, specifically to provide their most valuable advisory services to clients on combined portfolio reporting (PDW) & total risk analysis. The project also involved extensive data analysis, ETL, building data marts and Data Quality Management (DQM) covering Data Profiling and Data Cleansing.\nResponsibilities:\n\u2022 Maintained the stage and production conceptual, logical, and physical data models along with related documentation for a large data warehouse project.\n\u2022 Assisted in migration of data models from Oracle Designer to Erwin and updating the data models to correspond to the existing database structures.\n\u2022 Investigated data sources to identify new data elements needed for data integration.\n\u2022 Performed data analysis using SQL queries on source systems to identify data discrepancies and determine data quality.\n\u2022 Served as a resource for analytical services utilizing SQL Server and TOAD/Oracle and Created SQL queries using TOAD and SQL Navigator and also created various databases object stored procedure, tables, views.\n\u2022 Developed the required data warehouse model using Star schema for the generalized model.\n\u2022 Involved in Relational and Dimensional Data modeling for creating Logical and Physical Design of Database and ER Diagrams with all related entities\n\u2022 and relationship with each entity based on the rules provided by the business manager.\n\u2022 Updated or Created new data models for each release and generated database scripts from Erwin.\n\u2022 Used Erwin to create report templates. Maintained and changed the report templates as needed to generate varying data dictionary formats as contract deliverables.\n\u2022 Designed the Data Marts in dimensional data modeling using star and snowflake schemas.\n\u2022 Coordinated Data Profiling/Data Mapping with Business Subject Matter Experts, Data Stewards, Data Architects and ETL Developers.\n\u2022 Worked with the DBAs on maintaining existing, and creating new, database Tools & Technologies and communicate to leadership within our company to convey the importance and business value of data descriptions for entities, attributes, domain values and relationships using data modeling tools to create Entity Relationship Diagrams.\n\u2022 Created Data stage jobs (ETL Process) for populating the data into the Data warehouse constantly from different source systems.\n\u2022 Analyzed and designed the business rules for data cleansing that are required by the staging and OLAP & OLTP database.\n\u2022 Updated or worked with test team members to help them understand data changes and write test cases.\n\u2022 Identified and documented data sources and transformation rules required to populate and maintain data warehouse content.\n\u2022 Involved in developing Data Mapping, Data Governance, Transformation and Cleansing rules for the Master Data Management Architecture involving OLTP and OLAP.\n\u2022 Was responsible for indexing the tables in the data warehouse and performed data modeling within information areas across the enterprise including data cleansing and data quality using data modeling methods and processes.\n\u2022 Involved in logical and Physical Database design & development, Normalization and Data modeling using Erwin and SQL Server Enterprise manager.\n\u2022 Implemented the Data Cleansing using various transformations.\n\u2022 Used Data Stage Director for running and monitoring performance statistics.\n\u2022 Reverse Engineered the existing ODS into Erwin.\n\u2022 Created reports to retrieve data using Stored Procedures.\n\u2022 Designed and implemented basic SQL queries for testing and report/data validation.\n\u2022 Ensured the compliance of the extracts to the Data Quality Center initiatives.\n\u2022 Gathered and documented the Audit trail and traceability of extracted information for data quality.\nEnvironment: MS Word, MS Excel, MS Access, Erwin, SQL Server, MS Visio, Oracle, UNIX, Linux, Teradata, DB2.', u""Data Analyst\nFifth Third Bank - Cincinnati, OH\nOctober 2010 to September 2012\nDescription: The purpose of the project was to build an Enterprise Data Warehouse (EDW) for Customer Relationship Management (CRM), Operations Management (OM), Risk Management (RM) and Finance and Performance Management (FPM) which included regulatory compliance, data management including standards, customer profiling, integration and Data Quality. EDW Project is involved in design, develop and implement the Enterprise Data Warehouse (EDW) and Datamarts ensuring the EDW to conform to the business requirements and quality check the data before making it available for use.\nResponsibilities:\n\u2022 Analyze the OLTP Source Systems and Operational Data Store and research the tables/entities required for the project.\n\u2022 Designing the measures, dimensions and facts matrix document for the ease while designing.\n\u2022 Created data flowcharts and attribute mapping documents, analyzed the source meaning to retain and provide proper business names following the very stringent FTB's data standards.\n\u2022 Developed several scripts to gather all the required data from different databases to build the LAR file monthly.\n\u2022 Implemented data quality checks on the monthly LAR file to make sure the data is within the federal regulations.\n\u2022 Developed numerous reports to capture the transactional data for the business analysis.\n\u2022 Developed complex SQL queries to bring data together from various systems.\n\u2022 Organized and conducted cross-functional meetings to ensure linearity of the phase approach.\n\u2022 Collaborated with a team of Business Analysts to ascertain capture of all requirements.\n\u2022 Created multiple reports on the daily transactional data which involves millions of records.\n\u2022 Used Joins like Inner Joins, Outer joins while creating tables from multiple tables.\n\u2022 Created Multiset, temporary, derived and volatile tables in Teradata database.\n\u2022 Implemented Indexes, Collecting Statistics, and Constraints while creating tables.\n\u2022 Utilized ODBC for connectivity to Teradata via MS Excel to retrieve automatically from Teradata Database.\n\u2022 Developed various ad hoc reports based on the requirements.\n\u2022 Designed & developed various Ad hoc reports for different teams in Business (Teradata and Oracle SQL, MS ACCESS, MS EXCEL).\n\u2022 Developed SQL Queries to fetch complex data from different tables in remote databases using joins, database links and formatted the results into reports and kept logs.\n\u2022 Involved in writing complex SQL queries using correlated sub queries, joins, and recursive queries.\n\u2022 Delivered the artifacts within the time lines and excelled in the quality of deliverables.\n\u2022 Validated the data during UAT testing.\n\u2022 Performing source to target Mapping\n\u2022 Involved in Metadata management, where all the table specifications were listed and implemented the same in Ab Initio metadata hub as per data governance.\n\u2022 Developed Korn Shell scripts to parallel extract and process data from different sources simultaneously to streamline performance and improve execution time in a parallel process for better time, resource management and efficiency.\n\u2022 Used Teradata utilities such as TPT (Teradata Parallel Transporter), FLOAD (Fastload) and MLOAD (Multiload) for handling various tasks.\n\u2022 Developed Logical data model using Erwin and created physical data models using forward engineering.\nEnvironment: Erwin 8.0, Teradata 13, TOAD, Oracle 10g/11g, MS SQL Server 2008, Teradata SQL Assistant, XML Files, Flat files."", u'Business Analyst/ Data Analyst\nSutherland - Hyderabad, Telangana\nApril 2009 to August 2010\nDescription: Sutherland is leading Health Care Solutions in India. I joined this organization as a data modeler/analyst consultant to support the on-going relational and dimensional (data warehouse) enhancements, as a part of which I worked on multiple projects which involves enhancements of the application supporting relational databases and creation of many new fact tables and dimension tables on the reporting side, also involves creation of data marts as per the reporting needs.\nResponsibilities:\n\u2022 Gathered and translated business requirements into detailed, production-level technical specifications, new features, and enhancements to existing technical business functionality.\n\u2022 Reverse Engineered existing Relational and Dimensional data base systems as there were no existing data models for them.\n\u2022 Applied data warehousing methodologies in enhancing the existing data model.\n\u2022 Enhanced the warehouse model using Erwin tool.\n\u2022 Implemented Star Schema, snowflake methodologies in enhancing data warehouse.\n\u2022 Identified new facts, dimensions and reused existing conformed dimensions as per business requirements.\n\u2022 Enhanced existing facts and dimensions based on the business requirements.\n\u2022 Scheduled working sessions with ETL team to understand the need of work tables where needed.\n\u2022 Performed gap analysis where applicable to make sure there are no data anomalies.\n\u2022 Followed standards and best practices in indexing and partitioning where performance can be a concern.\n\u2022 Provided detailed data mapping documents for both OLTP and OLAP.\n\u2022 Supported ETL team in understanding the process and creating/enhancing the ETL maps.\n\u2022 Supported QA by providing assistance in writing the sql prototypes as needed.\nEnvironment: Erwin r7.0, SQL/MS SQL Server, MS OLAP Manage, MS Analysis Services, Windows NT, MS Visio, XML, Informatica, Crystal Reports 9.']",[u'Bachelors in Computer Science in Computer Science'],"[u'Gandhi Institute of Technology and Management Visakhapatnam, Andhra Pradesh']"
0,https://resumes.indeed.com/resume/5a634680fa4a7b03,"[u""Data Science Analyst\nAffine Analytics - Seattle, WA\nApril 2017 to December 2017\n\u2022 Provided analytical architecture analysis, design, development, and enhancements\n\u2022 Developed analytical frameworks with the help of offshore team for the project\n\u2022 Prepared the data dictionary for the list of variables from the client's data\n\u2022 Finalized the list of variables for modeling based on business and technical aspects\n\u2022 Defined and identified appropriate analytical models for the projects\n\u2022 Developed business insights based on the output of models relevant to client's business needs\n\u2022 Prepared the benchmarking report (technical report) to analyze and compare the results of different models/variables\n\u2022 Presented final results to the clients and discussed further opportunities within/outside the project"", u""Analytics Consultant\nScry Analytics - San Jose, CA\nJanuary 2016 to March 2017\n\u2022 Partnered with strategic clients to help them map their business goals into optimal marketing programs through incorporation of digital analytics, attribution modelling & optimization\n\u2022 Lead client engagements/ assignments and ensured appropriate resource allocation & utilization during the course of the projects\n\u2022 Ensured that all deliverables met client expectations, and adhered to all regulatory requirements\n\u2022 Understood client's business questions and developed solution architecture/complex Business modules\n\u2022 Worked with the delivery team to build cutting edge solutions by developing & testing modular, reusable, efficient and scalable code\n\u2022 Enhanced the existing scripts to achieve better performance and throughput\n\u2022 Guided and trained colleagues across the organization"", u""Jr. Data Scientist\nTech Mahindra Ltd - Hyderabad, Telangana\nMarch 2013 to December 2015\n\u2022 Mahindra Holidays Ltd:\n\u25e6 Worked with the CTO and team to address member referral conversion challenge and to develop a recommendation system to advertise holiday packages\n\u25e6 Performed Exploratory Analysis on 3 years historic data to determine the key influencers and capture trends\n\u25e6 Constructed a simulation model based on Linear Regression to predict the outcome based on different referral benefits, utilized the model to prescribe the best suitable Referral and Sign-up benefits\n\u25e6 Achieved a 20% increase in Referral conversions within half-years' time\n\u25e6 Engineered a Recommendation System used to advertise new Holiday Packages to existing members\n\u25e6 Used Items based Collaborative Filtering in the model to determine similar holiday packages\n\u25e6 Benefits to the extent of 10% more Bookings was achieved through this\n\u2022 Mahindra Agriculture Ltd:\n\u25e6 Worked with the Agriculture R&D team to develop a Prescriptive Analytics product for the 'Farm Help' device\n\u25e6 'Farm Help' is an AI device continuously capturing data pertaining to Soil Nutrients, Water and Weather conditions to provide inputs to farmers on Farming actions be undertaken\n\u25e6 Constructed a Prescriptive Data model based on Ideal conditions/ nutrient levels for the Time Series Data provided by the device to provide specific inputs to farmers\n\u25e6 Embedded the model in the AI device, deployed on-site to provide real-time inputs to farmers and succeeded in achieving better yield and reducing cost of farming\n\u2022 Tech Mahindra Analytics Platform (TAP):\n\u25e6 TAP is the in-house Analytics product of Tech Mahindra akin to IBM Watson\n\u25e6 Developed Generalized modules in R for Exploratory Data Analysis and Forecasting\n\u2022 Tech Mahindra Sales Operations:\n\u25e6 Worked with the Tech Mahindra Business Development team to address high Day Sales Outstanding challenge\n\u25e6 Executed very detailed Exploratory Analysis using historic data to identify Root Causes, Trends and Best Practices in different Geographies\n\u25e6 Advised Changes to Payment Clauses in Contractual agreements and increased Sales Collections Oversight, this contributed to 30% decrease in Collection time"", u'Data Analyst\nTech Mahindra Ltd - Hyderabad, Telangana\nJanuary 2011 to February 2013\n\u2022 Worked as the Chief Data Analyst in the Corporate Operations Unit, closely supporting the Chief Operating Officer\n\u2022 Developed various Performance based Operations KPIs and the Computational Methodology\n\u2022 Collaborated with Business stakeholders in acting on complex, multi-source data to Explore, Generate and Test Business Assumptions\n\u2022 Partnered with other Analysts to Develop Data Infrastructure (data pipelines, reports etc.) and other tools to make Analytics easier and more Effective\n\u2022 Developed Ad-hoc analyses to aid team in understanding Customer Behavior, developed POCs, and drove Decision-Making\n\u2022 Collaborated with Data Architects on changes associated with Data Systems and System Interfaces\n\u2022 Mined Large amounts of Structured Data to Determine how our customers interact with our Products and Service Offerings\n\u2022 Lead teams in Complex Analytical Initiatives - Combining Data from multiple Sources to Extract Meaning, Reconcile assumptions, and Identify Logical paths for Action\n\u2022 Significantly contributed in achieving 7X increase in Net Profit from -3% to 21%, through very efficient Optimization Measures backed by solid Data Analysis\n\u2022 Took part in two major Mergers and Acquisitions worth $1.6B as a key member of the Operations track, worked on Systems and Processes Integration, Headcount Optimization and PMOs Training']","[u""Bachelor's in Technology""]","[u'Jawaharlal Technological University Anantapur, IN\nMay 2010']"
0,https://resumes.indeed.com/resume/c7571719729b5200,"[u'Data Analyst Contractor\nPetSmart - Phoenix, AZ\nFebruary 2018 to Present\nUnited States\n\u2022 Conducted analysis on PetSmart customers focused on low retention and provided information for business decision\n\u2022 Wrote queries in Netezza SQL to extract data and used Excel to find possible reasons for low customer retention\n\u2022 Created data visualization charts in MicroStrategy to present analysis results. Made business suggestions', u'Data Analyst Intern\nDeloitte Consulting - Shenzhen, CN\nFebruary 2016 to May 2016\nDeloitte Consulting, Shenzhen, China\n\u2022 Conducted extensive analysis on electricity usage of enterprises and individuals\n\u2022 Wrote queries to clean, aggregate and prune data. Transformed raw data to structural well-organized dataset\n\u2022 Provided concise data reports. Created data visualization for management with Tableau and Microsoft Excel\nSkills: HiveSQL, Microsoft Excel, Tableau, Hadoop', u'Data Analyst\nAnhui Xinhua Media - Hefei, CN\nJune 2014 to October 2014\nHefei, China\n\u2022 Created customer insights from Xinhua Bookstore buying/selling data\n\u2022 Built a machine learning model to find correlation between customer profile information and book reading preference\n\u2022 Wrote MySQL queries to aggregate and clean daily order data. Responded data ad-hoc requests\n\u2022 Built Tableau dashboards, created indicators, plotted charts for data visualization and presentation\nSkills: MySQL, Tableau, Python, Sklearn, Classification Modeling']","[u'Master of Science in Business Analytics', u'Master of Administration Management in Administration Management', u'Bachelor of Business Administration in Business Administration']","[u'Arizona State University Tempe, AZ\nJuly 2017 to May 2018', u'Shenzhen University Shenzhen\nSeptember 2014 to June 2017', u'Shenzhen University Shenzhen\nSeptember 2010 to June 2014']"
0,https://resumes.indeed.com/resume/f441fc1fcd4eca7f,"[u'Data Scientist\nAircraft Fasteners International LLC - Dallas, TX\nJune 2017 to December 2017\n\u27a2 Achieved revenue increment using sales-inventory data by targeting top 30% core customers (segmentation)\n\u27a2 Developed a machine learning model with 82% accuracy to check the conversion rate from quotes to sales\n\u27a2 Identified next best selling products by using customer/item based collaborative filtering algorithms', u'Senior Data Analyst\nZS Associates Pvt. Ltd - Pune, Maharashtra\nJanuary 2016 to July 2016\n\u27a2 Predicted budget allocation model for sales managers with 87% accuracy (Time Series forecasting)\n\u27a2 Targeted physicians optimally for each sales rep by developing a Call Planning recommendation solution\n\u27a2 Managed a team of 5 members and mentored 2 colleagues on machine learning models and A/B testing', u'Data Analyst\nZS Associates Pvt. Ltd - Pune, Maharashtra\nAugust 2014 to December 2015\n\u27a2 Executed end-to-end ETL process and designed sales dashboards for pharmaceutical clients\n\u27a2 Reduced data cleansing time by 25% by conducting exploratory data profiling and SQL query optimization\n\u27a2 Communicated business insights with data visualization tools by building KPI metric dashboards (Tableau)']",[u'M.S in Business Analytics'],"[u'The University of Texas at Dallas Richardson, TX\nApril 2018']"
0,https://resumes.indeed.com/resume/f76fc37112a378a0,"[u""Supervisor\nXL Box Incorporated - Valparaiso, IN\nJanuary 1998 to Present\nDuties Include:\n\u2022 Supervise eight to fifteen employees\n\u2022 Eliminated waste and increased employees performance\n\u2022 Coordinate employee placement for efficient production\n\u2022 Oversee shipping and receiving\n\u2022 Inspect quality of product\n\u2022 Maintain customer's inventory\n\u2022 Control company's inventory."", u'Intern/Data Analyst\nXL Box Incorporated - Gary, IN\nJanuary 2005 to January 2006\nDuties Included: Morning Report\n\u25cf Collect and analyze coke reports data\n\u25cf Consolidate the Delay Reports for the Area Manager\n\u25cf Identified and explain the reason for production loss\n\u25cf Maintain inventory of product produced and used\n\u25cf Document environmental readings\n\u25cf Document daily power usage\n\u25cf Saved and post Morning Report to the Company Website\n\u25cf Exceeded expectations and was asked to stay past summer obligation']","[u'Master of Science in Accounting', u'Bachelor in Industrial Engineering Technology']","[u'Indiana Wesleyan University Marion, IN\nAugust 2016', u'Purdue University Calumet Hammond, IN\nMay 2006']"
0,https://resumes.indeed.com/resume/d0369e57aa4d11d9,"[u'Data Analyst\nStearns County Jail Team, SCSU\nSeptember 2017 to Present\n\u2022 Enhanced analytical skills through working with large amounts of data\n\u2022 Interpreted real world data results by using statistical techniques\n\u2022 Implemented data analysis strategies to make predictions']",[u'Bachelor of Science in Statistics'],"[u'St. Cloud State University Saint Cloud, MN\nMay 2019']"
0,https://resumes.indeed.com/resume/382353d962529008,"[u'Data Analyst\nClient -Thermo Fisher Scientific - Carlsbad, CA\nAugust 2017 to Present\nResponsibilities:\n\u2022 Worked with business requirements analysts/subject matter experts to identify and understand\nrequirements. Conducted user interviews and data analysis review meetings.\n\u2022 Defined key facts and dimensions necessary to support the business requirements along with Data Modeler.\n\u2022 Created draft data models for understanding and to help Data Modeler.\n\u2022 Resolved the data related issues such as: assessing data quality, data consolidation, evaluating\nexisting data sources.\n\u2022 Manipulating, cleansing & processing data using Excel, Access and SQL.\n\u2022 Responsible for loading, extracting and validation of client data.\n\u2022 Coordinated with the front-end design team to provide them with the necessary stored procedures and packages and the necessary insight into the data.\n\u2022 Participated in requirements definition, analysis and the design of logical and physical data models\n\u2022 Leading data discovery discussions with Business in JAD sessions and map the business requirements to logical and physical modeling solutions.\n\u2022 Used facilitation in business and organizational settings to ensure the designing and running of successful meetings and workshops\n\u2022 Conducted data model reviews with project team members\n\u2022 Captured technical metadata through data modeling tools\n\u2022 Created Tableau scorecards, dashboards using stack bars, bar graphs, scattered plots, geographical maps, Gantt charts using show me functionality\n\u2022 Created organized, customized analysis and visualized projects and dashboards to present to Senior Level Executives\n\u2022 Collaborated with ETL teams to create data landing and staging structures as well as source to target mapping documents\n\u2022 Ensure data warehouse database designs efficiently support BI and end user requirements\n\u2022 Collaborated with application and services teams to design databases and interfaces which fully meet business and technical requirements\n\u2022 Maintain expertise and proficiency in the various application areas\n\u2022 Maintain current knowledge of industry trends and standards\n\nTools and Environment: -, Tableau Desktop 9.0,10.0,10.2, Informatica 9.1, Oracle 11g, SQL Developer/SQL, Cognos, TOAD, MS Access, MS Excel', u'Data Analyst\nClient -Wells Fargo\nJune 2016 to June 2017\n\u2022 Worked on SQL*Loader to load data from flat files obtained from various facilities every day.\n\u2022 Conducted data modeling review sessions for different user groups, participated in requirement sessions to identify requirement feasibility.\n\u2022 Created the conceptual model for the data warehouse using Erwin data modeling tool.', u""Data Analyst\nClient - State Farm - Bloomington, IL\nJanuary 2015 to May 2016\nResponsibilities:\n\u2022 Analysis of housing trends and review mortgage product characteristics such as underwriting parameters, closing costs, mortgage insurance, qualification rules, rate sheets and price adjustments.\n\u2022 Minimize losses within the delinquent collections segments by providing specific collection treatments.\n\u2022 Involved in data Integration tool Pentaho for designing ETL jobs in the process of building Data warehouses and Data Marts\n\u2022 Analyzed data and reporting requirements and designed marketing dashboards in Tableau Desktop that tracks letter performance.\n\u2022 Ability to authoring on Tableau server by changing mark types and adding filters.\n\u2022 Administered user, user groups, and scheduled instances for reports in Tableau.\n\u2022 Monitored and maintained incremental refreshes for data sources on Tableau server.\n\u2022 Reviewed basic SQL queries and altered for better performance in Tableau Desktop\n\u2022 Created pivot tables and ran VLOOKUP's in Excel as a part of data validation.\n\u2022 Management reporting design and automation in Access/Excel with dashboards, pivot tables, charts (standard/pivot) and linked data refreshes.\n\u2022 Performed the Unit testing and prepared the Test Cases.\n\u2022 Conducted PPC and display campaigns for end users.\n\u2022 Formulated and executed analytics strategies for properties, Analyzed and reported latest industry trends for business implications\n\u2022 Produce reports to support analysis of loans status including Performing, Delinquent, and Foreclosures.\n\u2022 Working on the single-family project and data is extracted from the Teradata.\n\u2022 Generated reports for short sales and perform the analysis of uniform appraisal data also the status of severity.\n\u2022 Development of macros to provide transformation variables from a single input variable.\n\u2022 Compare the source data with historical data to get some statistical analysis.\n\u2022 Perform statistical analyses and QC statistical output.\n\u2022 Skills Performing transformations like Merge, Sort and Update to get the data in required format.\n\nTools and Environment: Tableau Desktop/Server 7.0/8.1, MS Excel, MS Power Point, PL/SQL, Teradata, Crystal reports, DB2, Teradata, Windows 2003/2007"", u'Data Analyst\nEricsson Broadcast and Media - Piscataway, NJ\nMay 2014 to January 2015\nResponsibilities:\n\u2022 Mentored sophisticated organizations on large scale data and analytics using advanced statistical and machine learning models.\n\u2022 Architected and implemented analytics and visualization components for device data analysis platform to predict hardware\n\u2022 Optimized factors for sales conversions and designed an algorithm for deal recommendations for a large daily deals website.\n\u2022 Performed Data filtering, Dissemination activities, trouble shooting of database activities, diagnosed the bugs and logged them in version control tool.\n\u2022 Created source to target mappings for multiple source from SQL server to oracle. This was used by ETL developers.\n\u2022 Worked extensively on Erwin and ER Studio in several projects in both OLAP and OLTP applications.\n\u2022 Performed the batch processing of data, designed the SQL scripts, control files, batch file for data loading.\n\u2022 Performed the Data Accuracy, Data Analysis, Data Quality checks before and after loading the data.\n\u2022 Coordinated with the Business Analyst Team for requirement gathering and Allocation Process Methodology, designed the filters for processing the Data.\n\u2022 Designed and developed the Database objects (Tables, Materialized Views, Stored procedures, Indexes), SQL statements for executing the Allocation Methodology and creating the OP table, CSV, Text files for business.\n\u2022 Performed the physical database design, normalized the tables, worked with Deformalized tables to load the data into fact tables of Data warehouse.\n\u2022 Designed star schema with dimensional modeling, created fact tables and dimensional tables.\n\u2022 Involved in data analysis, data discrepancy reduction in the source and target schemas.\n\u2022 Designed and developed the star schema Data model, Fact Tables to load the Data into Data Warehouse.\n\u2022 Implemented one-many, many-many Entity relationships in the data modeling of Data warehouse.\n\u2022 Developed the E-R Diagrams for the logical Database Model, created the physical Data\n\u2022 Model with Erwin data modeler.\nTools and Environment: Oracle 11g, Data Warehouse, OLAP, SQL Navigator, SQL Developer, Erwin 4.0, XML, OLTP, MS-Excel 2000, MS-office 2000, Microsoft XP Professional.', u'Data/Tera Data Analyst\nSBI Life Insurance Company - Bengaluru, Karnataka\nMarch 2013 to April 2014\nResponsibilities:\n\u2022 Planning, design and migration of Client data centers between buildings after accessing all requirements and using current hardware and new hardware.\n\u2022 Remediation and patching of Linux Servers.\n\u2022 Analyzed the customer behaviors on progressive website.\n\u2022 Analyzed different versions of the site and the traffic for each month, quarter and year.\n\u2022 Created documentation and updated them to reflect the most current process.\n\u2022 Involved in developing many Dash Boards for home, rental and auto insurance.\n\u2022 Worked on different Segmentations such as Email Segmentation process.\n\u2022 Coordinated with several managers in linking information and data within the financial departments.\n\u2022 Involved in user trainings during system upgrades and enchantments.\n\u2022 Compiled reports such as Marketing reports, Brand reports and branch reports\n\u2022 Creating SQL files indexes and calculated the table spaces for each table, tuning the\n\u2022 SQL queries (based on the plan tables). Grant and revoke table access to users.\n\u2022 Created Control files to load data into databases by help of ETL tool (Extract Transform Load Process).\n\nTools and Environment: Teradata V2R6, BTEQ, FLOAD, SQL, Windows XP, SAS', u'Data Analyst\nPhilips Health Care - Bengaluru, Karnataka\nAugust 2010 to January 2013\nResponsibilities:\n\u2022 Processed data received from vendors and loading them into database. The process was carried out on weekly basis and reports were delivered on bi-weekly basis. The extracted data had to be checked for integrity.\n\u2022 Documented requirements and obtained signoffs.\n\u2022 Coordinated between the Business users and development team in resolving issues.\n\u2022 Documented data cleansing and data profiling.\n\u2022 Wrote SQL scripts to meet the business requirement.\n\u2022 Analyzed views and produced reports.\n\u2022 Tested cleansed data for integrity and uniqueness.\n\u2022 Automated the existing system to achieve faster and accurate data loading.\n\u2022 Generated weekly, bi-weekly reports to be sent to client business team using business objects and documented them too.\n\u2022 Learned to create Business Process Models.\n\u2022 Ability to manage multiple projects simultaneously tracking them towards varying timelines effectively through a combination of business and technical skills.\n\u2022 Good Understanding in clinical practice management, medical and laboratory billing and insurance claim with processing with process flow diagrams.\n\u2022 Assisted QA team in creating test scenarios that covers day in a life of patient for Inpatient and Ambulatory workflows.']",[u'in Physical & Logical'],[u'Branch Center Management and Product Management']
0,https://resumes.indeed.com/resume/fdd8f8ecb0ab33ae,"[u'Data Analyst\nCapital One - Chicago, IL\nAugust 2017 to Present\n\u2022 Built a tool which is internal to the capital one called LEGOLAND. LEGOLAND utilizes containerization via Docker and AWS ECS to allow users to customize their environments.\n\u2022 Performed statistical analysis and build classification models to predict customer churn in the partnership portfolios.\n\u2022 Performed predictive analysis and customer segmentation to identify the customers who generate the maximum revenue for marketing campaigns (80/20 rule) which improved the customer response significantly.\n\u2022 Cloud Migration: Lead early adaptor in the cloud migration environment and build standards of practice for the entire marketing segmentation team to have a smooth transition from legacy Mainframe (z/os) and SAS to Amazon Web Services.\n\u2022 Build AWS lambda architecture to monitor S3 buckets and triggers and updates the daily marketing one file.\n\u2022 Designed and implemented Cloud solutions with AWS Virtual private cloud (VPC), Elastic Compute Cloud (EC2), Elastic Load Balancer (ELB), S3, Auto scaling, RDS, Cloud watch and other AWS services.\n\u2022 Scripting python code for transition from legacy Mainframe (z/os) and SAS to Amazon Web Services and equivalent SAS code.\n\u2022 Build python code for data analysis for setting different campaigns like promotions, new user welcome letters, and birthday campaigns.\n\u2022 Worked with business professionals in constructing the right architecture for cloud migration.\n\u2022 Proficient in ""Know-Your-Customer"" Phase 4 and phase 5 works and executed end to end KYC campaigns and Data Anomalies.', u'Data Analyst Intern\nSurvey Research Organization, University of Illinois at Springfield - Springfield, IL\nSeptember 2016 to May 2017\n\u2022 Designed and implemented a Command line tool using python and BOTO3 to perform AWS EC2 tasks using simple commands and flags.\n\u2022 Created an AWS Lambda architecture to monitor AWS S3 Buckets and triggers a thumbnail create event whenever a user uploads a new picture.\n\u2022 Designing and developing various machine learning frameworks using R.\n\u2022 Summarize and visualize conclusions from large-scale data sets to support the decision making of internal business stakeholders using SAS, tableau.\n\u2022 Administered a research study related to the 2016 elections in the state of Illinois and other clients like Realtors Association.', u""Data Analyst\nAmazon - IN\nAugust 2015 to December 2015\n\u2022 Conducted data extraction and exploratory data analysis on huge customers query data sets using AWS Redshift and Python BOTO3.\n\u2022 Partnered with Data Science team to perform statistical analysis with tool like SAS to evaluate the right survey questions for measuring customer's experience.\n\u2022 Worked with business partners and key stakeholders to identify business needs and delivered high-exposure reoccurring and ad-hoc reports to answer business questions.\n\u2022 Used data-driven approach to improve the response rate of the customer experience.\n\u2022 Created Tableau based dashboards to track progress against different approaches applied to improve customer experience."", u'Data Analyst\nEMorphosys Solutions Pvt. Ltd\nMarch 2014 to May 2015\n\u2022 Extensive use of SQL Server Management Studio to write and prepare queries for further data analysis.\n\u2022 Designed databases, stored procedures, reports using SQL Server 2008, 2014 and Excel.\n\u2022 Imported, exported and manipulated large data sets in multi-million-row databases under tight deadlines.']","[u'', u'Master of Science in Management Information Systems']","[u'University of Illinois Springfield Springfield, IL\nMay 2017', u'Osmania University Hyderabad, Telangana\nMay 2015 to May 2015']"
0,https://resumes.indeed.com/resume/2a15574db90975cf,"[u'Sr. Data Modelling / Data Migration Specialist\nBank of America - Sacramento, CA\nJune 2016 to Present\nResponsibilities:\n\u2022 Worked on creating custom components, templates and workflows based of classic and touch UI\n\u2022 Successfully executed 9 Data Migration releases from Source Systems to Target System.\n\u2022 Defining the process of Extraction from SQL server and loading it to Oracle Database.\n\u2022 Created queries to extract data from SQL Server (Source Database) to Flat Files or Excel files.\n\u2022 Maintained enterprise data models using Erwin, 9.5.01 and contributed to enterprise level data vision.\n\u2022 Balanced the need for loan-volume growth with the equally important necessity of optimum underwriting quality to minimize risk while maximizing profitability.\n\u2022 Provided attention to detail in underwriting mortgages. Evaluated the financial strength of borrowers to determine risk and repayment capacity.\n\u2022 Demonstrated excellent follow-through with borrowers, loan processors, loan originators and management throughout all phases of the underwriting process.\n\u2022 Reviewed and analyzed assets, tax returns, tax transcripts and/or other income documents.\n\u2022 Assisted with training of new hires by doing peer level side by sides.\n\u2022 Developed ETL Jobs using Power Center which will load data from Flat Files or Excel Files to Staging Area in Oracle, stage environment to production environment (Test Table).\n\u2022 Created procedures to perform source to target record level and field to field Reconciliation.\n\u2022 Responsible in running Reconciliation programs for each Go Live Migration release and report the mismatches encountered.\n\u2022 Leveraged knowledge of loan products to help restructure declined loans for subsequent approvals. Maintained compliance with lending, underwriting and government requirements.\n\u2022 Defined the process of User Acceptance Testing for Data Migration and responsible in creating Testing Guides, Instruction Packets and Data Samples.\n\u2022 Responsible in presenting the UAT process and guidelines to user so that everyone are familiar to what to do before UAT.\n\u2022 Enabled data lineage from the Oracle EDW to the Teradata EDW, by building a metadata catalog in Oracle.\n\u2022 Successfully hosted Data Migration - UAT approximately 50 users virtually and assisted users in performing testing.\n\u2022 Created Burndown chats on In Progress Submissions/Deals/Tasks for each migration and reported to higher management for Risk Analysis.\n\u2022 Responsible in creating weekly reports for Executive Team for each Migration releases which contains Risks Business (Identified based on the Data), Action Items and their Status.\n\u2022 Responsible in performing BA testing for Migrated Data in UAT Environment\n\u2022 Performing Smoke Testing immediately after successful load of Data to Production from Source systems.\n\u2022 Responsible in analyzing and fixing the issues reported by reporting team during testing.\n\u2022 Created Stored Procedures, User Defined functions, Views and implemented the Error Handling in the Stored Procedures and SQL objects and also Modified already existing stored procedures (Bug fixing), triggers, views, indexes depending on the requirement.\n\u2022 Implemented Custom Error Handling (Record Level) by using the Script component to generate custom error messages and redirected the error rows to a staging error table.\n\u2022 Performed Data Validation by a three step process as mentioned below which is a part of Rapid Data Validation.\n\u2022 Data Validation through manual Unit Test Cases to compare the Source to Target data based on the scenarios provided by the Business Users.\n\u2022 Data Validation of the migrated Data by browsing through Application to ensure that the migrated data is in compliance with the Target Architecture and is reflected correctly on Application.\n\u2022 Preformed Data Validation by reports extract. Created numerous reports which measure various attributes that are most critical for business from Source and target tables and validated them to 100%.\n\u2022 Designed and executed the test cases for Data Migration in Quality Center which is a part of automation process.\n\u2022 Implemented workflow to collect Business and Technical Metadata throughout IT releases and implemented to repository\n\u2022 Formatted HTML, RTF and PDF reports using SAS output delivery system ODS\n\u2022 Fixed various bugs under Data Migration Category which are raised by Technical SME and Business Users through Rapid Data Validation.\n\u2022 Involved in creating formulas, rename objects and organize data elements in a meaningful way using Framework Manager and in building Universe.\n\u2022 Created interactive reports for sorting, different Parameterized Reports which consist of report Criteria in various reports to make minimize the report execution time and to limit the no of records required\n\u2022 Supported bi-weekly and monthly Informatica data batch loads for the data mart\n\u2022 Developed Complex SSRS Reports involving Sub Reports (Matrix/Tabular Reports, Charts and Graphs).\n\u2022 Scheduling jobs and Alerting using SQL Server Agent.\n\u2022 Improved performance of the system by suggesting composite indexes.\n\u2022 Monitored performance monitor and SQL Profiler to optimize queries and enhance the performance of database servers.\n\u2022 Involved in logical and physical design and partitioning the database tables; writing SQL queries, advanced PLSQL artifacts; coding of stored procedures, triggers, packages, functions and complex queries at an advanced level; SQL loading using SQL Loader, external tables.\n\u2022 Developed and Modified PL/SQL packages, functions, Procedures as per customer requirement to make new enhancements or to resolve problems.\n\u2022 Extensively involved in the SSAS storage and partitions, and aggregations, calculation of queries with MDX, Data Mining Models, developing reports using MDX and SQL.\n\u2022 Created and worked with drill down reports and identified the report parameters.\n\u2022 Helped the users to test the reports created and optimized the performance of the end reports getting the data from SQL server Databases and SQL server analysis services.\n\u2022 Developed Cubes using SQL Analysis Services (SSAS) and Experience in Developing and Extending OLAP Cubes, Dimensions and data source views.', u""Sr.Data Migration Analyst/Data Modelling\nWorld Finance - Grovetown, GA\nJune 2014 to June 2016\nResponsibilities:\n\u2022 Gathered requirements from the business Technical experts and created the Functional Specifications, Technical Specs, Design documents for Data Migration.\n\u2022 Reverse engineering the Database Architecture through ER Studio.\n\u2022 Created Source Data dictionary for entire Tax initiatives (Modules) and Payments, Overpayment, Appeals, Billing and Collection, USDOL Compliance & IRS Compliance modules of Benefits and getting it approved from state.\n\u2022 Defining the process of Source Data Extraction of Source Data from legacy system to DB2 on Linux Server. Source data is available in various formats in which most of them are VSAM files and few of them are of various formats viz. MS Access, SQL, Excel, Flat Files etc.\n\u2022 Responsible for Explaining to the State DM team about the Data Migration process and the activities that are involved in Data Migration phase.\n\u2022 Defined the process of Data Profiling of Source data and quantifying the source data quality.\n\u2022 Performed Data Profiling for Overpayment, Appeals & Billing and Collection initiatives to quantify the Source Data Quality.\n\u2022 Defined the process of Data Cleansing using the outputs of Profiling.\n\u2022 Developed ETL Jobs using SAP Business Objects Data Services which will extract data from Source VSAM, SQL, Flat Files and Excel files and load it in Intermediate Staging DB2 Database on Linux Server.\n\u2022 Created Extraction Manual which documents the Source Data Extraction for PC Application which has various sources viz. SQL, MS Access, Excel, and Flat Files.\n\u2022 Defined the end to end Source Data Extraction from Legacy System to DB2 Linux Staging Database\n\u2022 Created Data Mapping Specification document for Payments & Weekly Certifications, Benefits Review and Disputes & Overpayment module tables in UI System which involves discussion of the business logic from Internal Users & Technical SME's and mapping them to target fields. Few of the mappings are one to one and a lot of fields have complex derivation logic due to vast difference in designs of the two systems.\n\u2022 Developed ETL Jobs using SAP Business Objects Data Services which will extract data from Intermediate Staging DB2 Database on Linux Server and load it in UI Target DB2 Database on Linux Server following the Data Mapping Specifications document.\n\u2022 Managed client metadata/data by researching, data gathering of forms and records and maintaining database projects\n\u2022 Enabled data lineage from the Oracle EDW to the Teradata EDW, by building a metadata catalog in Oracle.\n\u2022 Defined the process of Bridging & Back Bridging as the Benefits part of UI application will go Live 6 months before leaving behind the Tax system in Legacy. Hence Synchronization of data between both is required so that both systems can perform daily regular activities on their respective functional areas.\n\u2022 Defined the process of automation of Unit Test Cases to test the developed ETL jobs which load source data to UI System.\n\u2022 Facilitated data integrity testing of reports and data mart\n\u2022 Created RTF, PDF, and HTML Reports using SAS ODS.\n\u2022 Created programming code using advanced concepts of Records, Collections and Dynamic SQL.\n\u2022 Involved in performance tuning on SQL using AUTO TRACE, EXPLAIN PLAN, TKPROF utilities.\n\u2022 Worked with multiple business / reporting team and developed dynamic SQL, complex SQL queries using joins, sub-queries and inline views to address reporting requirement.\n\u2022 Used In-house built java automation framework to automate regression scripts, smote test scripts and functional test scripts\n\u2022 Performing data management projects and fulfilling ad-hoc requests according to user specifications by utilizing data management software programs and tools like Perl, Toad, MS Access, Excel and SQL\n\u2022 Created materialized views using different clauses like FAST, COMPLETE, and FORCE, ON COMMIT.\n\u2022 Defined the process of Error Handling mechanism by creating Validation rules according to Target Database Design and rejected any Erroneous Data which does not comply with Target Database. All the error records will be rejected into their Rejection tables which are pre-defined.\n\u2022 Defined and documented the Data Migration Test Plan which includes Verification and Validation of Migrated data from Legacy to UI.\n\u2022 Data Migration Validation: State inputs were taken for writing the scenarios on which migrated data needs to tested and based on that scenario, scripts were written which will automatically validates the Migrated Data from Intermediate Source to UI Target.\n\u2022 Data Migration Verification: This includes the verification of Data from Legacy Screens to UI Screens. More than 100 Verification scenarios were written and its corresponding screen shots from Legacy and UI Systems are captured for selected cases and submitted for user testing."", u'Data Migration Analyst\nSantander Bank - Boston, MA\nJanuary 2012 to May 2014\nResponsibilities\n\u2022 Involved in gathering requirements from the business users and created the Functional Specifications, Technical Specs, Design documents and Approach documents.\n\u2022 Worked on Source Data Analysis, Design and creation of Data Mappings for Extraction of bulk data from sources like Flat files, Excel files etc.\n\u2022 Developed SSIS packages using various Control Flow and Data Flow items to Transform and Load the Data using SQL Server Integration services (SSIS).\n\u2022 Extensively used Execute SQL Task, Data flow Task, Look up, Script logic component, Union All etc. in SSIS packages.\n\u2022 Created Stored Procedures, User Defined functions, Views and implemented the Error Handling in the Stored Procedures and SQL objects and also Modified already existing stored procedures, triggers, views, indexes depending on the requirement.\n\u2022 Implemented Custom Error Handling (Record Level) by using the Script component to generate custom error messages and redirected the error rows to a staging error table.\n\u2022 Processed client requests for: checks, journal entries, management fees, wired funds, REIT check requests and paperwork etc. Reviewed work to ensure accuracy.\n\u2022 Performed telephone customer service functions: advises field representatives of account status, provides solutions and alternatives to problems and issues, and instructs field representatives on appropriate forms, form acquisition and completion.\n\u2022 Performed ancillary projects and duties as assigned.\n\u2022 Supported Application team for messaging and queuing applications in the SQL Server Database Engine using SQL Server Service Broker.\n\u2022 Providing Adhoc reports using MYSQL with optimized queries.\n\u2022 Involved in documentation of MYSQL Code and analyzing the code for bugs and fixing them according to the business logic.\n\u2022 Conducted data integrity testing of resulted data mart table loads\n\u2022 Reviewed and analyzed borrower documentation to determine income and repayment ability, assess borrower hardship, analyze housing expenses, and determine proper loan resolution.\n\u2022 Reviewed the recommended foreclosure prevention alternative offered to the homeowner to ensure all investor/insurer guidelines were followed.\n\u2022 Analyzed delinquency and performed escrow analysis analyze modification scenarios and proposals and recommend appropriate loan workout.\n\u2022 Preformed Data Validation by reports extract. Created numerous reports which measure various attributes that are most critical for business from Source and target tables and validated them to 100%.\n\u2022 Designed and executed the test cases for Data Migration in Quality Center which is a part of automation process.\n\u2022 Fixed various bugs under Data Migration Category which are raised by Technical SME and Business Users through Rapid Data Validation.\n\u2022 Used the SSIS Configuration files to make the packages connections, variables etc. configurable and maintained the same on Test and Production environments.\n\u2022 Created interactive reports for sorting, different Parameterized Reports which consist of report Criteria in various reports to make minimize the report execution time and to limit the no of records required\n\u2022 Report parameters included single valued parameters, multi-value parameters that also consisted of different parameter types like hidden, internal, default (queried and non-queried parameters).\n\u2022 Monitored performance monitor and SQL Profiler to optimize queries and enhance the performance of database servers.', u""Data Migration Analyst\nSunTrust Bank - Atlanta, GA\nNovember 2009 to December 2012\nResponsibilities:\n\u2022 Performed review and analysis of functional requirements and test plan design documentation.\n\u2022 Gathered requirements from the business Technical experts and created the Functional Specifications, Technical Specs, Design documents and Approach documents for DM.\n\u2022 Designing the Database Architecture as per Business Functionality through ER Studio.\n\u2022 Updated/Corrected Low Level Design (LLD'S) for Tier-3 Modules which involved Payment Processing, Power to Sale and Tax Compliance and Operation.\n\u2022 Created Mapping documents for Extension of Roll, Tax bill Creation, Payment Processing, Power to Sell and Tax Compliance and Operation modules which involves discussion of the business logic from Internal Users and applying them on Legacy database to target system.\n\u2022 Implemented Program code for data migration from the Mapping Specs for Tax Bill Creation and Penalty Application modules.\n\u2022 Constructed a program to apply Penalties for Secured and Unsecured Tax Bills as per Business Functionality which includes the complex logic due to difference in Source and Target Architecture.\n\u2022 Worked on Source Data Analysis and creation of Data Mappings for Extraction of bulk data from various sources.\n\u2022 Extensively done Data Profiling and Data Mining for Source Data.\n\u2022 Extensively used ad hoc Transact-SQL statements and scripts using SQLCMD utility.\n\u2022 Developed SSIS packages using various Control Flow and Data Flow items to Transform and Load the Data using SQL Server Integration services (SSIS).\n\u2022 Extensively used Execute SQL Task, Data flow Task, Look up, Script logic component, Union All etc. in SSIS packages.\n\u2022 Created Stored Procedures, User Defined functions, Views and implemented the Error Handling in the Stored Procedures and SQL objects and also Modified already existing stored procedures (Bug fixing), triggers, views, indexes depending on the requirement.\n\u2022 Implemented Custom Error Handling (Record Level) by using the Script component to generate custom error messages and redirected the error rows to a staging error table.\n\u2022 Performed Data Validation by a three step process as mentioned below which is a part of Rapid Data Validation.\n\u2022 Data Validation through manual Unit Test Cases to compare the Source to Target data based on the scenarios provided by the Business Users.\n\u2022 Data Validation of the migrated Data by browsing through Application to ensure that the migrated data is in compliance with the Target Architecture and is reflected correctly on Application.""]",[],[]
0,https://resumes.indeed.com/resume/df896affc102081d,"[u'Data Science\nMarch 2017 to January 2018', u'Financial Crisis\nJanuary 2006 to January 2018\nwith Python, made time series lines of its price and volume and got conclusion of the regression relationship between price, volume and the economic environment, such as 2008 Financial Crisis.', u'Data Analyst Intern\nUT Southwestern Medical Center - Dallas, TX\nSeptember 2017 to December 2017\n\u2022 Processed row biological research data\n\u2022 Categorized data into application level\n\u2022 Transformed spreadsheets to diagrams using R, Python', u'ACADEMIC PROJECT\nSeptember 2017 to December 2017\nBuilt a database of the nationwide real estate market and realized data integration using SQL', u""Data Analyst\nSuzhou Branch\nFebruary 2012 to December 2015\n\u2022 Built 'VIP Customer Group' which had over 500 members on social network through data mining using R to improve the interaction between these VIP customers and the company\n\u2022 Provided VIP customers with meaningful and visualized reports for investment products\n\u2022 Designed and analyzed high-dimensionality data to predict customer needs\n\u2022 Assisted sales team with qualified data information to achieve 32.5% sales increase"", u'Data Analyst\nShanghai Pudong Development Bank - Shanghai, CN\nAugust 2009 to February 2012\n\u2022 Categorized data into application level\n\u2022 Reported visualized outcomes to marketing and finance departments']",[u'M.S. in Business Analytics'],"[u'The University of Texas at Dallas Dallas, TX\nAugust 2017 to May 2019']"
0,https://resumes.indeed.com/resume/678f87b977c0118a,"[u'Data Analyst\nJay Engineering Company - Mumbai, Maharashtra\nMay 2016 to May 2017\n\u2022 Working on the continued development of an application built using Python, C#, and .NET, that controls the greenhouse equipment and sensors used in the automated crop growing industry.\n\u2022 Leading the software development effort, testing strategies and design of the test plans to ensure that the software is developed and maintained efficiently throughout the software development lifecycle.\n\u2022 Working in an Agile environment involving scrum meetings and bi-weekly coding sprints resulting in a minor and a major software release every month resulting in high customer satisfaction.', u'Data Analyst Intern\nJay Engineering Company - Mumbai, Maharashtra\nMay 2015 to November 2015']","[u'Master of Science in Computer Science in Computer Science', u'Bachelor of Engineering in Information Technology in Mining, Artificial Intelligence']","[u'California State University\nMay 2019', u'University of Mumbai\nMay 2017']"
0,https://resumes.indeed.com/resume/53c26a1de2abbcf1,"[u'Data Analyst Intern\nKAISER PERMANENTE - Pleasanton, CA\nJanuary 2018 to Present\n\u2022 Lead a 5-member team to gather requirements to design and develop an efficient data model for the clinical alarm system\n\u2022 Explore database for data extraction(SSIS) and perform SQL transformations for data reporting\n\u2022 Optimized ETL query used in data extraction and reduced SSIS job execution time by 60%\n\u2022 Analyze and explore patient data to generate insightful visualizations (Tableau) and facilitate managerial decision making', u'IT Analyst\nTCS - Chennai, Tamil Nadu\nSeptember 2011 to May 2015\nDesigned and developed over 75 Integration components using IBM Websphere Framework (Programming Language: ESQL,\nJava) for order management process of an E-Commerce system\n\u2022 Configured DBMS (Oracle, DB2, SQL) for middleware integration layer and built SQL queries to fetch details on routing\nconfigurations, to store dynamic data and audit information\n\u2022 Automated exception handling mechanism for middleware integration to resolve 30% of live production incidents\n\u2022 Conducted in-depth analysis of business optimization projects and improved efficiency by 25%']","[u'Master of Science in Information Systems', u'B.Tech in Electronics and Communication Engineering']","[u'Santa Clara University, Leavey School of Business\nJanuary 2017 to Present', u'SRM University Chennai, Tamil Nadu\nAugust 2007 to May 2011']"
0,https://resumes.indeed.com/resume/5642c4ec545de5bd,[u'Data Analyst'],"[u'Bachelor of Engineering in Information Technology', u'Master of Science in Information Systems']","[u'Mumbai University\nJuly 2011 to August 2015', u'University of Texas At Arlington Arlington, TX']"
0,https://resumes.indeed.com/resume/c6d1b27fb7847b9d,"[u""Data Analyst\nVaad Hoeir of St. Louis - St. Louis, MO\nJune 2014 to August 2014\nEXPERIENCE\n\u2022 Built an Alexa skill to provide Amazon Echo users times of afternoon religious services based on current time, date, and location. Currently in planning for development of an Alexa skill for a video sharing company that would allow users to ask for information to be retrieved from the company's database.\n\u2022 Built a simulated subway system using Java programming language including JOptionPane class and persistence for writing files.\n\u2022 Collaborated with a team on a simulated bank program using Java programming language and SQL. Contributed to the application's algorithm, designed UML, created Apache Derby database.\nData Analyst, Vaad Hoeir of St. Louis - St. Louis, MO Summer 2014\n\u2022 Developed an Excel program to compare food ingredients used in production to ingredients in Vaad Hoeir's data records to ensure ingredients were consistent and were still able to be sold as kosher for Vaad Hoeir.\n\u2022 Stored data on manufacturer, food brand, ingredients, verification, and dates to allow continual monitoring of ingredient data.""]","[u'Bachelor of Talmudic Law in Talmudic Law', u'Certification']","[u'Ner Israel Rabbinical College Baltimore, MD\nJanuary 2014', u'Division of Touro College - Institute for Professional Studies Brooklyn, NY']"
0,https://resumes.indeed.com/resume/13bc5670f7b9beae,"[u'Senior Data Analyst\nDecember 2017 to Present\n* Developing reports and analysis from single or multiple systems.\n* Identify and interpret trends or patterns from analyzing complex dashboards.\n* Providing statistical analysis to AB tests result.\n* Delivering machine-learning solutions to advanced business problems, including preprocessing data, feature engineering, building and optimizing models.\n* Perform strategic and operational data analysis for all internal teams.', u'Data Analyst\nFebruary 2016 to November 2017\n* Managing relational database including creation, updates, and deletion.\n* Create and manage ETL data pipelines in Alteryx. (Sourcing data from Redshift, SQL server etc.)\n* Integrate data from single or multiple sources to make data actionable.\n* Design and deliver internal & external KPI dashboards in Tableau to enable managers and stakeholders to access and monitor the business trends\n* Ad-hoc analysis and communicating results and recommendations to stakeholders']","[u""Bachelor's in Applied Mathematics""]","[u'University of California, Berkeley Berkeley, CA\nAugust 2013 to May 2015']"
0,https://resumes.indeed.com/resume/7905cfbf6142f1ff,"[u'Data Analyst\nNew York University\nApril 2017 to Present\n\u2022 Consolidate datasets, reviewing clinical files, retrieve and abstract relevant information. Organize\ndatasets for internal and external (collaborators) use all performed in a timely manner\n\u2022 Complete the consolidation of past data into a main data depository, create a pipeline of routine data\ncollection and storage\nData Analyst\nNew York University \u2022 Prepare in a timely manner requested information for members of the research group and outside\nSchool of Medicine collaborators\nApril 2017 - Present \u2022 Collaborate with IT team and research team to satisfy requirements of software in development\n\u2022 Collect specified requirements, determine technical issues, and design reports to meet data analysis\nneeds\n\u2022 Identify new sources of data and methods to improve data collection, analysis, and reporting\n\u2022 Individually train and assign work to medical students/staff members on analyzing data\n\u2022 Collaborating on projects involving multiple teams and managing ongoing relationships\n\u2022 Self-start and self-direct work in an unstructured environment']",[u'Bachelors of Business Administration in Computer Information Systems'],[u'Baruch Zicklin School of Business\nMay 2018']
0,https://resumes.indeed.com/resume/805d80635471aeed,"[u'Enrollment Analyst\nEnactus at UT Dallas - Richardson, TX\nJanuary 2018 to Present\n\u2022 Collected, organized and analyzed student data from disparate sources using Pivot tables and Data analysis in Excel\n\u2022 Conducted online survey using Google forms and designed visualizations in Tableau to explain student interests\n\u2022 Developed strategies along with other members using survey data which increased the enrollments by 60%\n\u2022 Improved the Enrollment process by saving cost and time invested in the data management and analysis', u'Data Analyst\nCrete India - Indore, Madhya Pradesh\nMay 2017 to August 2017\n\u2022 Performed data cleaning on state roadways financial data and ran data quality reports to ensure the data integrity\n\u2022 Designed visualizations in Tableau to explain spend analysis that helped reduce costs and increase efficiency\n\u2022 Increased the cost savings by 9% and improved the quality of state highways by 12% as reported in October 2017']","[u'Master of Science in (M.S.), Information Technology and Management', u'Bachelor of Engineering in (B.E.), Information Technology']","[u'The University of Texas at Dallas Richardson, TX\nMay 2018', u'Rajiv Gandhi Technical University Bhopal, Madhya Pradesh\nMay 2015']"
0,https://resumes.indeed.com/resume/c3939f977d5a7357,"[u""Data Analyst\nHuaJinCheng Weaving Facty Ltd\nJune 2017 to August 2017\n\u2022 Analyzed clothing industry market based on the unit prices of different kinds of cloth\n\u2022 Purposed corresponding strategies to increase company's profit\n\u2022 Communicated well with sales team and management team""]","[u'Bachelor of Arts in Statistics', u'Bachelor of Science in Chemistry']","[u'University of California Berkeley, CA\nJanuary 2015 to January 2017', u'Santa Monica College Santa Monica, CA\nJanuary 2012 to January 2015']"
0,https://resumes.indeed.com/resume/c3bd858597d604fd,"[u'Data Analyst\nSick Inc.\nJuly 2017 to February 2018\n* Assist with the development of reports and projects based on clinical knowledge and data analysis.\n* Perform data extraction, manipulation, cleaning, analysis and QA of large datasets\n* Familiarity with clients products to classify data sets into working sheets on Excel, working with team members to assure all payment, location, and specific instructions for clients are correct.\n* Research missing data or incorrect by using company records and/or researching company information.\n* Classify all data into categories, create a Master Sheet which is uploaded to CRM for Customer Service employees to view. Use the data to create a unique system as a growing solution to identify, analyze and understand data efficiently.', u'Payroll Specialist\nSeptember 2016 to July 2017\n* Entering hours for all hourly employees when timesheets are received.\n* Ensuring that all personnel entries are correct in system and making any and all payroll corrections when errors are discovered.\n* Preparing weekly reports of production metrics, and performance and productivity to company executives.']",[u'Bachelor in Communication Studies'],[u'Minnesota State University Mankato\nJanuary 2017 to Present']
0,https://resumes.indeed.com/resume/5a58115bc99d91d1,"[u'Analyst\nPlug and Play Ventures - Sunnyvale, CA\nJuly 2017 to Present\n\u2022 Use MySQL, Excel and Tableau to clean, store, analyze and visualize raw data from startup databases to obtain insights for future investments\n\u2022 Perform data-enabled market research and due diligence for potential investments\n\u2022 Communicate extensively with startups, venture capitalists and corporations', u'Data Analyst\nDuke University Course Project\nSeptember 2017 to January 2018\n\u2022 Identified the most critical business metric (increase the number of Dognition tests completed by users) for analysis and constructed a structured analysis plan\n\u2022 Used Tableau and SQL (Teradata) to identify key factors that would strongly affect the business metric and made recommendations based on the analysis\n\u2022 Produced compelling data visualization using Tableau', u'Data Analyst\nUC Berkeley\nJanuary 2017 to May 2017\n\u2022 Utilized Python to learn raw data of car prices and analyzed factors affecting car prices\n\u2022 Built a multiple linear regression model to make car price prediction based on various car features\n\u2022 Compared models with different levels of complexities and determined the optimal level to avoid overfitting\n\u2022 Visualized data using Matplotlib (Python) and Microsoft Excel', u'Research/Data Analyst Intern\nUrodynamic Center\nJanuary 2015 to March 2015\n\u2022 Used Python and MySQL to extract and clean raw data from multiple databases\n\u2022 Performed Meta-analysis using random-effects modelling to systematically evaluate the efficacy and safety of laparoscopic pyeloplasty and open pyeloplasty\nPUBLICATION\n\u2022 Meta-analysis of laparoscopic versus open pyeloplasty for infant hydronephrosis (2015). Journal of Modern Urology, 20(10).']","[u""Master's in Engineering"", u'Bachelor of Engineering in Biomedical Engineering']","[u'University of California Berkeley, CA\nAugust 2016 to May 2017', u'National University of Singapore Singapore\nAugust 2012 to July 2016']"
0,https://resumes.indeed.com/resume/c285d6dfa0e8ea0a,"[u'Sr Business Data Analyst, I\nInternational Monetary Fund - Washington, DC\nFebruary 2016 to Present\nwas involved in to building an Operational Data Store for analyzing data. The ODS was scheduled to get data from different databases of operational system, which then is loaded into staging area. This is a huge Data warehousing Implementation involving ETL, web services, Oracle 8i, MS SQL Server and Business Objects.\nResponsibilities:\n\u2022 Worked according to the software development life cycle.\n\u2022 Gathered requirements from remotely based business users and defined and elaborated the requirements by holding meetings with the users (who are also Fifth-third employees).\n\u2022 Used Query Analyzer, Execution Plan to optimize SQL Queries.\n\u2022 Wrote the test cases and technical requirements and got them electronically signed off.\n\u2022 Created new reports based on requirements.\n\u2022 Supported SOA, data warehousing, data mining, and Enterprise Service model standards in designs, and developed standardization of processes like configuration management.\n\u2022 Created data transformations from internal and third party data sources into data suitable for handheld devices, including XLST, XQuery, XPath.\n\u2022 Utilized simple methods like PowerPoint presentations while conducting walkthroughs with the stakeholders.\n\u2022 Implemented SDLC methodologies including RUP, RAD, Waterfall and Agile.\n\u2022 Developed PL/SQL Business Functions and calling into the Cognos Tools Designed and Developed Power Prompts for Web Users for Flexible Querying.\n\u2022 Conducted GAP analysis so as to analyze the variance between the system capabilities and business requirements.\n\u2022 Designed high level ETL architecture for overall data transfer from the OLTP to OLAP with the help of SSIS.\n\u2022 Involved in defining the source to target data mappings, business rules, business and data definitions.\n\u2022 Created Logical/physical Data Model in ERwin and have worked on loading the tables in the Data Warehouse. Documented various Data Quality mapping document.\n\u2022 Extensively designed Data mapping and filtering, consolidation, cleansing, Integration, ETL, and customization of data mart.\n\u2022 Conducted User Acceptance testing (UAT) and worked with users and vendor who build the system.\nEnvironment: MS Office 2016, MS Visio, Data Mining, Cognos, Agile, Waterfall, SOA, XML, XLST, XQuery, XPath, SharePoint, PowerPoint, MS Project, SSIS, SSRS,UML, SQL Server, Erwin, Business Objects, MS Outlook', u'Data Analyst, I\nINENT Software Services - Hyderabad, Telangana\nJuly 2012 to February 2014\nHyderabad, INDIA\nClient 2: Capital One, Hyderabad July 2012 - February 2014\nICP (Integrated Customer Platform)\nAs a Data Analyst, I was involved in documenting the business processes by identifying data required in determining order data from multiple sources and the method of obtaining and storing data. Also, involved in gathering and documenting the requirements, for building ICP (Integrated Customer Platform) data mart required for various reporting purposes.\nResponsibilities:\n\u2022 Work with users to identify the most appropriate source of record and profile the data required for sales and service.\n\u2022 Document the complete process flow to describe program development, logic, testing, and implementation, application integration, coding.\n\u2022 Involved in defining the business/transformation rules applied for ICP data.\n\u2022 Define the list codes and code conversions between the source systems and the data mart.\n\u2022 Worked with internal architects and, assisting in the development of current and target state data architectures.\n\u2022 Worked with project team representatives to ensure that logical and physical ER/Studio data models were developed in line with corporate standards and guidelines.\n\u2022 Involved in defining the source to target data mappings, business rules, business and data definitions.\n\u2022 Responsible for defining the key identifiers for each mapping/interface.\n\u2022 Responsible for defining the functional requirement documents for each source to target interface.\n\u2022 Involved in configuration management in the process of creating and maintaining an up-to-date record of all the components of the development efforts in coding and designing schemas.\n\u2022 Developed the financing reporting requirements by analyzing the existing business objects reports.\n\u2022 Utilized Informatica toolset (Informatica Data Explorer, and Informatica Data Quality) to analyze legacy data for data profiling.\n\u2022 Responsible in maintaining the Enterprise Metadata Library with any changes or updates.\n\u2022 Establish standards of procedures.\n\u2022 Generate weekly and monthly asset inventory reports.\n\u2022 Evaluated data profiling, cleansing, integration and extraction tools(e.g. Informatica).\n\u2022 Coordinate with the business users in providing appropriate, effective and efficient way to design the new reporting needs based on the user with the existing functionality.', u""Data Analyst\nDell Inc - Bengaluru, Karnataka\nOctober 2009 to June 2012\nBangalore, India October 2009 - June 2012\nThe Project is to build an Impact Data Mart (IDM) from the existing Analytic Data Warehouse (ADW) to Global B2B Buyer Hosted Catalogue (BHC) data with supporting Buyer and Catalogue information.\nRole: Data Analyst\nResponsibilities:\n\u2022 Met with Customers to determine User requirements and Business Goals.\n\u2022 Blended technical and business knowledge with communication skills to bridge the gap between internal business and technical objectives and serve as an IT liaison with the business user constituents.\n\u2022 Conducted JAD sessions to gather requirements, performed Use Case and work flow analysis, outlined business rules, and developed domain object models.\n\u2022 Performed Data Analysis and Data validation by writing complex SQL queries using TOAD against the ORACLE database.\n\u2022 Created a conceptual design based on the interaction with the functional and technical team.\n\u2022 Analyzed the existing data model and incorporated new additions for the advancement data into the data model identifying the cardinality of the new tables to the existing tables and ensure proper referential integrity of the system.\n\u2022 Analyzed, created and formatted the Reports as per Business Requirements.\n\u2022 Analyzed and created various adhoc Reports for users based on User Requirements.\n\u2022 Created both list and drill down reports identifying the data hierarchy.\n\u2022 Performed report validation writing SQL queries using TOAD on an Oracle database.\n\u2022 Created process documents, reporting specs and templates, training material and slideshow presentations for the application development teams and managements.\n\u2022 Analysis of the data identifying source of data and data mappings of BHC.\n\u2022 Worked extensively in documenting the Source to Target Mapping documents with data transformation logic.\n\u2022 Interact with the SME's to analyze the data extracts from Legacy Systems (Mainframes and COBOL Files) and determine the element source, format and its integrity within the system.\n\u2022 Transformation of requirements into data structures which can be used to efficiently store, manipulate and retrieve information.\n\u2022 Collaborate with data modelers, ETL developers in the creating the Data Functional Design documents.\n\u2022 Ensure that models conform to established best practices (including normalization rules) and accommodate change in a cost-effective and timely manner.\n\u2022 Created and maintained specifications and process documentation to produce the required data deliverables (data profiling, source to Client maps, flows).\n\u2022 Worked in collaboration with various areas of the organization, identified additional stakeholder Requirements, and Documented requirements in a Software Requirements Specification document.\n\u2022 Created use cases, activity diagrams and state diagrams using Rational Rose.\nEnvironment: Oracle 10g, Windows, Microstrategy, MS Visio, MS Project, Informatica""]",[u''],[u'Quality Center']
0,https://resumes.indeed.com/resume/3862d1cc7cdc3738,"[u'Data Analyst\nEuprime - IN\nJanuary 2016 to December 2016', u""Data Analyst Intern\nFebruary 2015 to December 2015\nUsed linear regression, logistic regression, clustering, decision tree modelling and multivariate modelling to provide\nvaluable analytical insights and predict market price for real estate properties.\n\u2022 Performed qualitative and quantitative analysis on surveys for few non-profit organizations based in USA.\n\u2022 Provided Insights and recommendations by building Tableau Dashboards and Excel Reports\n\u2022 Carried out sensitivity analysis to characterize the impact of certain input metrics like marketing costs, discounts on revenue. Revenue predictions for several pricing and marketing strategies were provided for a retail firm based in India.\nDrop shipping E-Commerce Website June '17 - Dec '17\n\u2022 Created a drop shipping E-Commerce website for hands-on experience of various Internet Marketing practices and made it\nprofitable. Created custom audiences for retargeting and created Look Alike Audiences which gave terrific ROI.\n\u2022 Can understand, interpret and make recommendations based on data from google analytics.\n\u2022 Leveraged MailChimp to develop abandoned cart recovery email and executed email marketing""]","[u""Master's in Data Science and Business Analytics"", u'Bachelor of Science in Electronics and Communication Engineering']","[u'University of North Carolina at Charlotte Charlotte, NC\nJanuary 2017 to May 2018', u'Manipal Institute of Technology\nAugust 2012 to May 2016']"
0,https://resumes.indeed.com/resume/c9156386bd300764,"[u'Reporting Data Analyst\nFreedom Mortgage (Contract Position)\nOctober 2017 to February 2018\n\u2022 Experience using VBA to Design and develop a Quality Control Access database\n\u2022 Developed Ad-hoc Reports using SQL Server and SSRS', u'Data Analyst\nUniversal Supply Lumber & Building Materials (Contract Position)\nMarch 2017 to August 2017\n\u2022 Developed SQL Server queries for SSRS reports\n\u2022 Developed SSRS Tablix and Matrix reports using interactive and cascading parameters', u'Data Analyst\nPHH Mortgage\nNovember 2015 to December 2016\n\u2022 Designed & developed a KRI and KPI metrics Access database\n\u2022 Developed monthly metrics charts and data visualizations in Tableau to find trends and outliers\n\u2022 Developed interactive Dashboards in Excel to show trends and up to date performance analysis', u'Data and Report Analyst\nModway\nSeptember 2013 to September 2015\n\u2022 Developed Dashboards to highlight KPIs using Excel PowerPivot, Pivot Tables, Charts, Slicers, Timelines and Trendlines\n\u2022 Developed Reports to track Inventory Management, Sales Revenue, Customer Satisfaction and Employee Performance\n\u2022 Developed SQL Queries using: JOINS, Aggregate functions, GROUP BY, HAVING and ORDER BY clauses\n\u2022 Applied Excel functions & conditional statements to convert raw data into useful business information', u'Data and Report Analyst\nABB Group\nNovember 2012 to August 2013\n\u2022 Analyzed Data using Excel including: Data Lookups, Conditional Formatting, Applying Filters, Text and String Manipulation\n\u2022 Generated Dashboards using PowerPivot in Excel to review employee performance and customer satisfaction', u'Data and Report Analyst\nFederal Reserve Bank of Philadelphia\nJanuary 2007 to April 2009\n\u2022 Developed SQL Queries to Aggregate Data, Update Data and Filter Data according to specific business requirements\n\u2022 Experience using Microsoft SQL Server Reporting Services (SSRS) to design and develop economic and financial reports', u'Data Analyst\nFederal Reserve Bank of Philadelphia\nDecember 1997 to December 2006\n\u2022 Developed SQL Queries to Retrieve Data and Aggregate Data from Multiple Tables\n\u2022 Analyzed, Cleansed, Merged, Parsed, and Updated data to develop Custom and Ad-hoc Reports']",[u'Bachelor of Science in Business Administration in Business Information Systems'],"[u'DeVry University Fort Washington, PA\nJanuary 2009 to January 2013']"
0,https://resumes.indeed.com/resume/d5724413787193a8,"[u'Healthcare Data Analyst II\nSuperior Healthcare - Austin, TX\nJanuary 2011 to January 2012\n* Responsible for inbound healthcare and hospital data verification and validation using established SQL procedures and processing.\n* Wrote complex SQL queries to evaluate and update inbound data. Created SQL script templates for extracting data from Oracle databases. Verified raw data and compiled Ad-Hoc and standard reports per client requirements.', u'RIMS/Database Analyst\nJacksonville, FL\nJanuary 2001 to January 2009\n* Monitored, maintained and ensured that all metering systems are functioning properly and data is accurately being provided to departments dependent on metering data.\n* Provided detailed interval data reports for internal and external customer support by extracting data using SQL scripts from Oracle database platforms.', u'Data/Report Analyst\nAT&T American TransTech - Jacksonville, FL\nJanuary 1987 to January 1997\n* Developed custom reporting and analysis for major client accounts.\n* Enhanced business decision making capabilities by providing strategic data analysis to executive.']",[u''],[u'CENTER & CUSTOMER SERVICE REPRESENTATIVE']
0,https://resumes.indeed.com/resume/036f47d0c3661578,"[u'Statistician/Modeler\nSwigel Capital Management - Bellevue, WA\nJune 2017 to September 2017\nBuilt multilinear regression models in R, by using data-collection, attributes-expansion and data-camping steps. Then\nfinalized the result with 92.6% accuracy in test sets.\n\xa7 Finished cross validation by building moving, rolling and expanding test models in R.\n\xa7 Participated in ARIMA time series models in R to predict Vanguard Utilities ETF index direction in utility sector by significantly improving up to 91% accuracy in lag dataset.', u'Data Analyst\nShandong Aotai Electric Co., Ltd - Jinan, CN\nJune 2016 to September 2016\nBuilt reports for estimating price of new clean energy by large data resources to show the trend of clean energy market. Then\npresent result to management team to save 10% of electricity cost (200,000 RMB annually).\n\xa7 Used Excel functions to generate spreadsheets and pivot tables based on the new clean energy policy for the project.', u'Junior Data Analyst\nIndustrial and Commercial Bank of China - Jinan, CN\nJune 2015 to September 2015\nReceived Excellent Intern Rewards for 1,000 Best ICBC Bank Branches Rating Competition in Jinan.\n\xa7 Fluently used customer solution system to open new accounts and reduce process by 24 minutes per customer visit.']","[u'Master of Science in Engineering Sciences and Applied Mathematics', u'Bachelor of Science in Computational Mathematics and Economics']","[u'Northwestern University Evanston, IL\nSeptember 2016 to December 2017', u'University of Washington Seattle, WA\nSeptember 2012 to March 2016']"
0,https://resumes.indeed.com/resume/7b293cfe97277805,"[u""Data Scientist/Data Analyst\nShoptaki - New York, NY\nMarch 2017 to Present\n\u2022 Created smart chain framework to help new entrepreneurs to grow at global scale by using disruptive technology of data science, block-chain, artificial intelligence, multilateral netting and cryptography.\n\u2022 Handled the current and historical foreign exchange data to find patterns and helped company gain insights from it using machine learning and advanced excel tools.\n\u2022 Developed a machine learning framework to analyze customer data, payment method using block chain technology.\n\u2022 Created AI bots for Multilateral netting code for real time conversion and forecasting through Yahoo finance API.\n\u2022 Improved content curation by incorporating user recommendation system using SQL and Google Analytics\n\u2022 Used Distributed TensorFlow data parallelism to set up parallel processes on GPU's resulting reduced processing time.\n\u2022 Coordinate with the business users and stake holders providing appropriate, effective and efficient way to adapt new technologies and use cases with the existing functionality."", u'Data Analyst\nAbhyudaya Multimedia - Indore, Madhya Pradesh\nMarch 2014 to August 2015\n\u2022 Extracted, analyzed and synthesized data from data mart using SQL, prepared reports and provided actionable insights using Tableau.\n\u2022 Implemented ETL strategies for processing data while working with key users to derive reports used by high level management.\n\u2022 Dealt with the database designing and development issues and proposed solutions to improve system efficiency and reduce total expenses.\n\u2022 Used R and advanced excel tools to interpret data to draw conclusion for managerial action and strategy.\n\u2022 Used agile method by developing user and small stories to complete projects efficiently in particular amount of time.\n\u2022 Work with users to identify the most appropriate source of record and profile the data required for sales and service.\n\u2022 Document the complete process flow to describe program development, logic, testing, and implementation, application integration, coding.', u'Data Analyst\nPractolearn Solution Pvt. Ltd - Indore, Madhya Pradesh\nMarch 2013 to February 2014\n\u2022 Executed data repositories strategies and maintained the existing database systems by keeping them available, secure and stable.\n\u2022 Shape and disseminate analyses and syntheses to help decision makers to drive business activity, translate recommendations for internal plans and business plans\n\u2022 Support the domain ""business owners"" in the definition of their needs and translate them into recommendations for development and improvement of model applications.\n\u2022 Provided SQL management, installation, monitoring, tuning, optimization, backup and recovery.\n\u2022 Executed data migration procedures for moving data between database platforms without service disruption.']","[u'Master of Science', u'Bachelor of Engineering in Electronics and Communication']","[u'Pace University, Seidenberg School of Computer Science and Information Systems New York, NY\nMay 2017', u'RGPV University, Rishiraj Institute of Technology Indore, Madhya Pradesh\nMay 2013']"
0,https://resumes.indeed.com/resume/3a0286293efe2616,"[u""Sr. Data Analyst\nPrime Therapeutics - Minneapolis, MN\nMarch 2012 to Present\nManaged Care Organizations (MCOs) & Pharmacy Benefit Managers (PBMs)\nWorked as a Sr.Data analyst, involved in creating MCOs & PBMs Data mart to maintain all\nCoverage information of health insurers, employers&seniors on Medicare and individuals. Also captures all processed claims & members' medicine delivery data and clinical services data for people with complex medical conditions. Different teams also use MCOs & PBMs data across the organization for Adhoc, daily and weekly reporting purposes.\n\nResponsibilities:\n\u2022 Work with users to identify the most appropriate source of record and profile the data required for sales and service.\n\u2022 Document the complete process flow to describe program development, logic, testing, and implementation, application integration, coding.\n\u2022 Involved in defining the business/transformation rules applied for sales and service data.\n\u2022 Define the list codes and code conversions between the source systems and the data mart.\n\u2022 Worked with internal architects and, assisting in the development of current and target state data architectures\n\u2022 Worked with project team representatives to ensure that logical and physical ER/Studio data models were developed in line with corporate standards and guidelines\n\u2022 Involved in defining the source to target data mappings, business rules, business and data definitions\n\u2022 Worked with BTEQ to submit SQL statements, import and export data, and generate reports in Terradata.\n\u2022 Responsible for defining the key identifiers for each mapping/interface\n\u2022 Responsible for defining the functional requirement documents for each source to target interface.\n\u2022 Document, clarify, and communicate requests for change requests with the requestor and coordinate with the development and testing team.\n\u2022 Coordinated meetings with vendors to define requirements and system interaction agreement documentation between client and vendor system.\n\u2022 Enterprise Metadata Library with any changes or updates\n\u2022 Document data quality and traceability documents for each source interface\n\u2022 Establish standards of procedures.\n\u2022 Generate weekly and monthly asset inventory reports.\n\u2022 Evaluated data profiling, cleansing, integration and extraction tools(e.g. Informatica)\n\u2022 Coordinate with the business users in providing appropriate, effective and efficient way to design the new reporting needs based on the user with the existing functionality\n\u2022 Remain knowledgeable in all areas of business operations in order to identify systems needs and requirements.\n\u2022 Responsible for defining the key identifiers for each mapping/interface\n\u2022 Implementation of Metadata Repository, Maintaining Data Quality, Data Cleanup procedures, Transformations , Data Standards, Data Governance program, Scripts, Stored Procedures, triggers and execution of test plans\n\u2022 Performed data quality in Talend Open Studio\nEnvironment: SQL/Server, Oracle 9i, MS-Office, Teradata, Informatica, ER Studio, XML, Business Objects"", u'Sr.Data Analyst\nAT&T, GA\nAugust 2010 to March 2012\nTelecom Expense Management (TEM)\nWorked as a Sr.Data analyst, to create a Telecom Expense Management (TEM) data mart required for Business Intelligence and reporting purposes. The Existing Telecom Expense Management (TEM) system which allows the agents to enter, modify new sales/service data is used as one of major source systems.\n\nTelecom Expense Management (TEM) Data Quality Integrity\nWorked as Sr. Data Quality analyst to perform Data Quality Integrity for the claim Telecom Expense Management (ATT-EM) DWH which contains billing information, contract rate analysis , Vendor relationship management information and ensuring expenditures match logical inventory and are in compliance with Carrier contract rates and commitments.\n\nResponsibilities:\n\u2022 Work with users to identify the most appropriate source of record and profile the data required for sales and service.\n\u2022 Document the complete process flow to describe program development, logic, testing, and implementation, application integration, coding.\n\u2022 Involved in defining the business/transformation rules applied for sales and service data.\n\u2022 Define the list codes and code conversions between the source systems and the data mart.\n\u2022 Worked with internal architects and, assisting in the development of current and target state data architectures\n\u2022 Worked with project team representatives to ensure that logical and physical ER/Studio data models were developed in line with corporate standards and guidelines\n\u2022 Involved in defining the source to target data mappings, business rules, business and data definitions\n\u2022 Responsible for defining the key identifiers for each mapping/interface\n\u2022 Responsible for defining the functional requirement documents for each source to target interface.\n\u2022 Document, clarify, and communicate requests for change requests with the requestor and coordinate with the development and testing team.\n\u2022 Coordinated meetings with vendors to define requirements and system interaction agreement documentation between client and vendor system.\n\u2022 the Enterprise Metadata Library with any changes or updates\n\u2022 Document data quality and traceability documents for each source interface\n\u2022 Establish standards of procedures.\n\u2022 Generate weekly and monthly asset inventory reports.\n\u2022 Evaluated data profiling, cleansing, integration and extraction tools(e.g. Informatica)\n\u2022 Coordinate with the business users in providing appropriate, effective and efficient way to design the new reporting needs based on the user with the existing functionality\n\u2022 Remain knowledgeable in all areas of business operations in order to identify systems needs and requirements.\n\u2022 Responsible for defining the key identifiers for each mapping/interface\n\u2022 Implementation of Metadata Repository, Maintaining Data Quality, Data Cleanup procedures, Transformations , Data Standards, Data Governance program, Scripts, Stored Procedures, triggers and execution of test plans\n\u2022 Performed data quality in Talend Open Studio\n\nEnvironment: SQL/Server, Oracle 9i, MS-Office, Teradata, Informatica, ER Studio, XML, Business Objects', u""Sr.Data Analyst\nCapital one Bank - Chicago, IL\nJanuary 2009 to July 2010\nAs a Sr.Data Analyst, I was Involved in the Data Integration project, for the Small Business Group. My role was to assess the impact on downstream data management environments of processing different banking products via different systems and processes. I was also, involved in requirements gathering, data mapping, and documenting metadata.\n\nRetail Credit Model Redevelopment (RCMR) & Retail Data Quality& Integrity (Catch-up)\nAs a Sr.Data Quality Analyst involved in identifying data consumed by RCMR that is captured or created by LOB or operational personnel (e.g. Origination data) and verify that it is being captured accurately\n\u2022 Data identified at consumption point\n\u2022 Consumption mappings traced and documented\n\u2022 Sampled generated\n\u2022 Sample data traced and inspected\n\u2022 Document outcomes\n\nResponsibilities:\n\u2022 Work with the Project Management in the creation of project estimates.\n\u2022 Analysis of the data identifying source of data and data mappings of HCFG.\n\u2022 Worked extensively in documenting the Source to Target Mapping documents with data transformation logic.\n\u2022 Interact with the SME's to analyze the data extracts from Legacy Systems (Mainframes and COBOL Files) and determine the element source, format and its integrity within the system\n\u2022 Transformation of requirements into data structures which can be used to efficiently store, manipulate and retrieve information\n\u2022 Collaborate with data modelers, ETL developers in the creating the Data Functional Design documents.\n\u2022 Ensure that models conform to established best practices (including normalization rules) and accommodate change in a cost-effective and timely manner.\n\u2022 Enforce standards to ensure that the data elements and attributes are properly named,\n\u2022 Work with the business and the ETL developers in the analysis and resolution of data related problem tickets.\n\u2022 Support development teams creating applications against supported databases.\n\u2022 Provide 24 x 7 problem management support to the development team.\n\u2022 Document various Data Quality mapping document, audit and security compliance adherence.\n\u2022 Perform small enhancements (SOR element additions, data cleansing/data quality).\n\u2022 Create various Data Mapping Repository documents as part of Metadata services (EMR).\n\u2022 Provide inputs to development team in performing extraction, transformation and load for data marts and data warehouses.\n\u2022 Developed large number of standard and customized reports using SSRS for data feeds and for Reports on server.\n\u2022 Involved in Creating and Testing the SSIS packages developed for security.\n\u2022 Actively involved in developing Complex SSRS Reports involving Sub Reports, Matrix/Tabular Reports, Charts and Graphs\n\u2022 Ensure that models conform to established best practices (including normalization rules) and accommodate change in a cost-effective and timely manner.\n\u2022 Enforce standards to ensure that the data elements and attributes are properly named,\n\u2022 Work with the business and the ETL developers in the analysis and resolution of data related problem tickets.\n\u2022 Support development teams creating applications against supported databases.\n\u2022 Provide 24 x 7 problem management support to the development team.\n\u2022 Document various Data Quality mapping document, audit and security compliance adherence.\n\u2022 Perform small enhancements (SOR element additions, data cleansing/data quality).\n\u2022 Create various Data Mapping Repository documents as part of Metadata services (EMR).\n\u2022 Provide inputs to development team in performing extraction, transformation and load for data marts and data warehouses.\n\u2022 Provide support in developing and maintaining ETL processes that extract data from multiple SOR's residing on various technology platforms then transport the data to various delivery points such as data marts or data warehouses.\nEnvironment: SQL Server 2005/2008 Enterprise Edition, SSIS 2005, MS Excel, MS Access, Oracle 10g, UNIX, Windows XP, SQL, PL/SQL, Power Designer, Informatica"", u'Data Analyst\nLarson & Turbo Ltd - Mumbai, Maharashtra\nSeptember 2006 to June 2007\nProject: HDFC Housing Finance\n\nResponsibilities\n\u2022 Plan, design, and implement application database code objects, such as stored procedures and views.\n\u2022 Build and maintain SQL scripts, indexes, and complex queries for data analysis and extraction.\n\u2022 Created ad-hoc reports for the upper level management using Stored Procedures and MS SQL Server 2005 Reporting Services (SSRS) following the business requirements.\n\u2022 Created reports by dragging data from cube and wrote mdx scripts.\n\u2022 Created reports by extracting data from cube.\n\u2022 Generated reports using SQL Server Reporting Services 2005/2008 from OLTP and OLAP data sources.\n\u2022 Provide database coding to support business applications using Sybase T-SQL.\n\u2022 Perform quality assurance and testing of SQL server environment.\n\u2022 Develop new processes to facilitate import and normalization, including data file for counterparties.\n\u2022 Work with business stakeholders, application developers, and production teams and across functional units to identify business needs and discuss solution options.\n\u2022 Ensure best practices are applied and integrity of data is maintained through security, documentation, and change management.\nEnvironment: SQL Server 2005 Enterprise Edition, T-SQL, Enterprise manager, VBS.']",[],[]
0,https://resumes.indeed.com/resume/6c25984df44187a4,"[u'Health data analyst\nAssuring medical record charts are complete with needed dictations, signatures, matching accounts and names. Scanning medical records, verifying scanned documents for accuracy. Answering phones, responding to medical record requests. Collecting discharges and processing.']",[u'Some college'],[u'']
0,https://resumes.indeed.com/resume/d1b53b2ec831375e,"[u""Data Acquisition Analyst Senior\nCorelogic Real Estate Tax Service - Saint Petersburg, FL\nJanuary 1996 to May 2017\nRetrieved tax roll data files from the County and City tax collectors in FL, TN, MS, AL, OH, MI, IL, and GA. Analyzed the tax roll files and setup automated mass real estate tax payments for the mortgage companies CoreLogic services. CoreLogic reports the real estate tax amounts to the mortgage companies. The mortgage companies then send the tax amounts to CoreLogic. All of the tax amounts are then consolidated into a mass wire or check. A wire or check is sent to the tax collector with a payment file detailing all of the parcels and amounts being paid.\n\u2022 Created a multi user Access data base to track the work flow in the office by consolidating multiple spread sheets into one database, saving processing time.\n\u2022 Created a program to identify missing lower level city liens with property taxes due. The program cut down on claims due to penalty and interest charged by the tax collectors for non-payment of taxes.\n\u2022 Worked with the software vendor AutoAgent to create an automated process to eliminate the need for individual mortgage codes for each tax id in the AutoAgent payment files sent to the Tax Authorities across the US. This automated an extremely manual process and saved many hours of work.\n\u2022 Created ad hoc reconciliation reports for the tax collectors and the mortgage companies using Access and Excel, when a miss-reporting of the tax amounts to the mortgage companies had occurred. The reports avoided Corelogic penalties and interest charged by the Tax Authorities when the property taxes weren't paid on time.\n\u2022 Claims were reduced by identifying bad tax roll files sent to Corelogic by the Tax Authorities.\n\u2022 Based on extensive file analysis, data processing problems were identified, and programming instructions were written to resolve the problems.\n\u2022 Learned the Automate software language to create programs to get tax amounts from tax collectors websites. This automated a manual process and saved time, when the tax roll files weren't available.""]",[u'B.S. in Mathematics'],"[u'University of Northern Colorado Greeley, CO']"
0,https://resumes.indeed.com/resume/873a9a97a2da480b,"[u'Risk Analyst\nBank of America - Chicago, IL\nMarch 2017 to Present\n\u2022 Created, designed and developed an Economic Indicator and Portfolio Analyser tableau dashboard that can perform analytics at the zipcode /metro level. This tool is used to determine the portfolio at Risk and the results are presented to Senior Management.\u2022\n\u2022 Utilized a machine learning clustering model to group various metros using industry employment data. \u2022\n\u2022 Gathered wholesale and consumer portfolio data to build a loss forecast tableau dashboard that provides a platform to assess risk.\n\u2022 Automated the Data Quality and Data Integrity checks using SAS for the reports generated.\u2022', u""Utility Asset Inventory Analyst\nNC State University\nMay 2015 to February 2017\nTeam Lead:\n\u2022 Led a team of 6 and constructed a web interface to overlay the utilities on Google maps using Key Markup language (KML).\n\u2022 Developed/maintained a database for various utilities in the campus using advanced Excel skills (Vlookup's and Pivot Tables).\n\u2022 Mapped 112,000 Linear feet of utilities and calculated their corresponding deferred maintenance costs (~ 44 Million USD)."", u'Data Analyst / Data Scientist\nNC State University\nSeptember 2015 to December 2015\n\u2022 Performed analysis, imputation, data cleaning and developed predictive models using R for forecasting wood pellet sales.\n\u2022 Confidence interval of the predictions obtained and used to optimize the production rate to best meet the desired sales.\u2022\n\u2022 Identified and reduced the errors that can occur in the manufacturing and testing of the wood pellets.\u2022', u'Data Analyst (Intern)\nBSNL - Chennai, Tamil Nadu\nApril 2013 to July 2014\n\u2022 Identified potential areas for expanding the coverage, and meeting user requirements of BSNL network in the Chennai area, India.\n\u2022 Regression and Anova models were adopted in the statistical analysis of the acquired data.']","[u'Masters in Operations Research in Operations Research', u'Bachelors in Electronics and Communication Engineering in Electronics and Communication Engineering']","[u'North Carolina State University\nMay 2016', u'SRM University\nMay 2013']"
0,https://resumes.indeed.com/resume/72415ecc252cc3a9,"[u'Data Analyst / Scientist\nVerizon - Township of Warren, NJ\nJanuary 2017 to January 2018\nResponsibilities:\n\u2022 Writing test cases in Python for statistical analysis and created predictive analysis reports in different domains.\n\u2022 Created a Machine Learning model to predict the business that will be most successful in region wise.\n\u2022 Performing data analysis using statistical and machine learning methodologies to advance unstructured data sets.\n\u2022 Working on consensus algorithms using Python.\n\u2022 Created interactive dashboards in tableau to assist end-user decision making.\n\u2022 Building, publishing customized interactive reports and dashboards, report scheduling using Tableau server.\n\u2022 Extensively worked Tableau Server and created report schedules, backups, and restoring backups into the repository.\n\u2022 Involved in the design and development of user interfaces and customization of Reports using Tableau and designed cubes for data visualization, forecasting reports using historical data.\n\u2022 Generated various presentable reports and documentation using report designer.\n\u2022 Analyze data model, write business algorithms using stored procedures and queries, and perform proof of concepts for new data architecture.\n\u2022 Created databases, tables, views & schemas. Performed users access authorization, defined locations, populated tables with data and performed unload/copy/replicate data from one database into another after detail analysis and design.\n\u2022 Collected, reviewed and analyzed quarterly and annual financial asset data.\n\u2022 Preparing Dashboards using calculations, parameters in Tableau Desktop and Server.\n\u2022 Evaluated data profiling, quality, cleansing, integration and extraction tool.\n\u2022 Created reports to show quality and quantitative trends to define key metrics.\n\u2022 Verified new deployments and contributed to evolution of various tools to improve data quality.\n\u2022 Worked with cloud based technology like Redshift, S3, AWS, EC2 Machine, etc. and extracting the data from the Oracle financials and the Redshift database.\nEnvironment:\nTableau, Python, Statistics, Artificial Intelligence, multi-threading, MATLAB R2017a, Amazon Web Services, No SQL, PL/SQL.', u""Data Analyst\nAltria\nMay 2014 to February 2016\nResponsibilities:\n\u2022 Provided assistance for defining of SAP roles and new business processes.\n\u2022 Reviewed and maintained data structures in SAP modules.\n\u2022 Extracted data from SAP BI using OLTP and OLAP connections to do modelling and designing in Universe level.\n\u2022 Utilized SAP ETL toolset (Data Explorer, and Data Quality) to analyze legacy data for data profiling.\n\u2022 Generated Xcelsius dashboards and reports using BEx, WEBI, DESKI, and Crystal and performed analysis for decision making.\n\u2022 Performed modeled data extraction process from universe to Xcelsius to generate dashboards using QAAWS.\n\u2022 Building, publishing customized interactive reports and dashboards, report scheduling using Tableau server.\n\u2022 Generated Tableau ad-hoc reports using excel sheet, flat files, CSV files to.\n\u2022 Generated tableau dashboards with combination charts for clear understanding.\n\u2022 Monitored end - to - end workflow process to enter, create, or change the data.\n\u2022 Conducted or performed data modeling exercises in support of subject areas and/or specific client needs for data, reports, or analyses, with a concern towards reuse of existing data elements, alignment with existing data assets and target enterprise data architecture.\n\u2022 Tested various ETL transformation rules based on log files, data movement and with help of SQL.\n\u2022 Involved with Business Unit for Gap Analysis to check the compatibility of the existing system infrastructure with the new business requirements. Executed SQL Queries to check the data table updates after test executions.\n\u2022 Tested various ETL transformation rules based on log files, data movement and with help of SQL.\n\u2022 Developed conceptual, logical, and physical Enterprise Data Model based on industry standards.\n\u2022 Involved in preparing logical data models and conducted controlled brain-storming sessions with project focus groups.\n\u2022 Involved in designing Context Flow Diagrams, Structure Chart and ER- diagrams and worked on database features and objects such as partitioning, change data capture, indexes, views, indexed views to develop optimal physical data mode.\n\u2022 Designed the Logical Model into Dimensional Model using Star Schema and Snowflake Schema.\n\u2022 Executed enterprise data governance strategies in line with the organization's strategic plan and objectives.\n\u2022 Assessed data repositories for compliance with data governance policies and standards. Works with all areas of the organization to ensure data quality and integrity.\n\u2022 Involved in the data profiling activities for the column assessment and natural key study.\n\u2022 Developed complex SQL queries, and perform execution validation for remediation and Analysis.\n\u2022 Used Normalization methods up to 3NF and De-normalization techniques for effective performance in OLTP systems.\nEnvironment:\nSAP NetWeaver, SAP Business Intelligence, Business Objects, Crystal Reports, Excelsius Dashboards, Rational Requisite Pro, MSOffice (Word, Excel and PowerPoint), MS Project, MS Access, CSV files, UML, Erwin, MS Visio, Oracle 11g, RDBMS, SQL Server 2008, SQL."", u'Data Analyst\nFibcom India Limited\nSeptember 2009 to April 2014\nResponsibilities:\n\u2022 Gathered business requirements through interviews, surveys, prototyping and observing from account managers and UI (User Interface) of the existing Broker Portal system.\n\u2022 Translated the business requirements into workable functional and non-functional requirements at detailed production level using Workflow Diagrams, Sequence Diagrams, Activity Diagrams and Use Case Modelling.\n\u2022 Identified the objects and relationships between the Healthcare objects to develop a logical model in Erwin tool.\n\u2022 Implemented Slowly Changing Dimensions Type2 and Type3 for accessing history of reference Healthcare data changes.\n\u2022 Identified entities and attributes and developed conceptual, logical and physical models using Erwin.\n\u2022 Used reverse engineering to create Graphical Representation (E-R diagram) and to connect to existing database.\n\u2022 Writing Complex SQL queries in Teradata to pull the records with data issues reported by the users using complex joins, partitions, ordered analytical functions, aggregate functions etc.\n\u2022 Defined and implemented Data Governance Management tools.\n\u2022 Created and implemented the Data Governance framework that meets data objectives for the organization.\n\u2022 Established and executed the Data Quality Governance Framework, which includes end-to-end process.\n\u2022 Quality framework for assessing decisions that ensure the suitability of data for its intended purpose.\n\u2022 Developed Healthcare data mapping documents between Legacy, Production, and User Interface Systems.\n\u2022 Creating or modifying the T-SQL queries as per the business requirements.\n\u2022 Designed and Developed Oracle database Tables, Views, Indexes with proper privileges and Maintained and updated the database by deleting and removing old data.\nEnvironment:\nOracle 11g, SQL, Erwin, ODBC, RDBMS, SQL Server, Rational Rose, HTML, Agile methodology.']","[u'MS in Information Systems and Technology', u""Bachelor's degree in Reproducible Research""]","[u'Wilmington University\nMay 2017', u'Nagarjuna University\nMay 2008']"
0,https://resumes.indeed.com/resume/3061c32cf0e5341d,"[u'Data Analyst\nCumberland County Partnership for Children - Fayetteville, NC\nOctober 2017 to Present\nResponsibilities: Conducting quality improvement and quality assurance activities including data collection and data management, monitoring, evaluation, technical assistance, and training to support effective implementation of programs and strategies. Manage end-to-end delivery of data management services for single/multiservice projects, ensuring quality deliverables on time and within budget. Assume general responsibility for the collecting, editing, processing, and distribution of data required in the PFC data information systems.']",[],[]
0,https://resumes.indeed.com/resume/a861f2797c9b0ad9,"[u""Data Analyst\nFujita - Erlanger, KY\nJanuary 2018 to Present\n\u2022Conduct industrial research and business analysis for potential business opportunities and assist senior management in business development strategies.\n\u2022Assisted in the creation of market research reports and projects covering Fasteners industry and assist senior management to draft presentation of execution plans\n\u2022Performing competitive analysis on sales and client\u2019s data to monitor sales trend, capture client\u2019s behaviors, have an insight into clients' preference by using SAS, SPSS and Tableau.\n\u2022Compiled and consolidated quantitative and qualitative reports using Microsoft Excel.\n\u2022Utilized pivot tables to manipulate and analyze financial data of inventory maintenance work orders\n\u2022Prepared daily progress reports and maintained spreadsheets with varied data to ensure accuracy\n\u2022Conducted secondary research on investment opportunities for vendors and organized data in Excel databases.\n\u2022Built Tableau dashboards for the team for broader visuals, quick access and better forecasting of research results. Maintain and update the dashboard with new data."", u""Data Analyst and SAS programmer\nEastern Kentucky University\nJanuary 2016 to May 2017\nAs a data analyst in Institutional research department my primary responsibility is to coordinate, prioritize, preform data analysis, and create data reports for Finance, Financial-Aid, Human Resources, Faculty and Student Admissions/Enrolment. My duties include retrieving and submitting data for internal reporting and external agencies for decision making.\nKey responsibilities:\n\n\u2022 Accompanied senior statisticians in various consulting sessions and provided support in statistical analysis including choice of experiment design, management of data, statistical procedures for analyzing experiments, interpretation of the results, statistical modeling and statistical computing.\n\u2022 Designed Tableau dashboards/reports and automated process flows eliminating manual generation and thus decreasing latency and increasing productivity\n\u2022 Experience in querying BANNER ERP and Oracle databases, developing complex queries, procedures and scheduling jobs using IDE's like SQL management studio, and Microsoft Access\n\u2022 Experience in filling surveys/data/reports for Federal, State and external agencies like IPEDS, US News, and rankings.\n\u2022 Performed predictive modeling and analytics for matriculation, enrolment, and retention and student success using SPSS and SAS.\n\u2022 Validated data and created reports using different procedures like PROC FREQ, PROC MEANS, PROC TRANSPOSE, PROC DATASETS, PROC SORT, PROC SQL, and PROC REPORT etc.\n\u2022 Extensively used statistical procedures like PROC FREQ and PROC MEANS to compute elementary statistical measures which include descriptive statistics based on moments, quin-tiles, confidence intervals, frequency counts, correlations and distribution tests\n\u2022 Utilized various SAS procedures to sort datasets by the specific order, generate statistical summaries/reports, and draw tables/graphs/figures/charts.\n\u2022 Created ad hoc reports/dashboards, student reports and visualization extensions using Tableau.\nEnvironment: SPSS, SAS/STAT, SAS/Graph, Base SAS 9.3, SAS Macros, SAS\xae Visual Analytics, DB2, My SQL.,MS Project, MS Excel, MS Access."", u""Data Analyst\nAmazon Development Center - Hyderabad\nMay 2014 to December 2015\nDescription: Prepared strategic business requirements and used accepted concepts, standards, SDLC methodologies, and toolsets. Worked with Data migration from legacy to new systems and worked with resolving all the data related issues.\n\nKey Responsibilities:\n\n\u2022 Provided analytical business insights to stakeholders across multiple domains in the Amazon brand by characterizing product catalog data issues and resolved ad hoc process related queries with a high level of quality.\n\u2022 Performed data mapping of over 200,000 merchants in a new backend software system and developed processes and procedures for data mapping, compliance and statement validation for reduced merchant statement validation time by 3 hours.\n\u2022 Analyzed data to highlight process gaps and identified trends for implementing process improvements/eliminating manual touch-points.\n\u2022 Built customized dashboard reports in Tableau to track the team\u2019s performance metrics and deliverables.\n\u2022 Performed Root Cause Analysis and organized a catalog quality clean-up project where 37,000 obsolete listings from sneaker brand were removed from the Amazon detail page leading to a 22% reduction in queries related to item data quality.\n\u2022 Led training sessions for following best practices of data analysis and effectively managed a team of 8 customer service representatives providing subject matter expertise for the Shoes/Handbags department on Amazon catalog.\n\u2022 Involved in publishing of various kinds of live, interactive data visualizations, dashboards, reports and workbooks from Tableau Desktop to Tableau servers.\n\u2022 Done performance tuning to optimize the run time.\n\u2022 Predicted customers' preference based on order history to maximize sales efficiency using logistic regression and data modelling.\n\u2022 Extensively involved in identifying and solving the problems by coordinating with senior managers in emergency and normal issues to meet the goals.\n\u2022 Supported customer service department and increased customers' positive feedback rate from 78% to 91%.\n\nEnvironment: Spreadsheets, My SQL, Tableau, MS Project, and MS Office, MS Excel.""]","[u'Masters in Engineering and Technology Management', u'Bachelors in Bioinformatics']","[u'Eastern Kentucky University Richmond, KY\nJanuary 2016 to August 2017', u'SASTRA University\nJanuary 2010 to January 2014']"
0,https://resumes.indeed.com/resume/5625ed1592057653,"[u""Data Analyst\nL&R Distributors - Brooklyn, NY\nJanuary 2014 to January 2017\n88 35th St, Brooklyn, NY 11232\nCustomer Logistics\n\u2022 Maintain Customer File\n\u2022 Enter customer new store data into system\n\u2022 Enter reset customer reset schedule\n\u2022 Order merchandise/fixtures in a timely manner to arrive in store prior to reset\n\u2022 Enter customer promotional distributions\n\u2022 Effectively communicate needs and schedules to appropriate sales associate.\n\u2022 Coordinate Reset Billing with reset division\n\u2022 Track reset merchandise and fixtures PO's\n\nDuties include:\n\u2022 Data entry\n\u2022 Maintain customer files\n\u2022 Inventory tracking\n\u2022 Ordering merchandise\n\u2022 Maintain pricing\n\u2022 Filing information\n\nGiordano's Pizzeria\n45 W Main St, Bay Shore NY 11706\nCashier/Waitress\n\n\u2022 Delivered excellent customer service to restaurant goers\n\u2022 Worked in fast paced high volume environment\n\u2022 responsible for taking customer's orders, serving food and beverages and performing other duties as assigned\n\u2022 Prepare and serve specialty dishes at tables\n\u2022 Inform patrons of each day specials\n\u2022 Stock service areas with supplies; coffee, food, tableware, and linens\n\u2022 Accurately and efficiently complete all sales transactions and maintain proper cash and media accountabilities at registers\n\nMarshalls.\n2650 Sunrise Hwy\nEast Islip NY 11751\u202c\nCashier\n\u2022 Ring up purchases, operate cash registers, and process various forms of payment.\n\u2022 Answering customer inquiries, cleaning work areas, and performing sales duties as necessary.\n\u2022 Identify prices of goods using memory or scanner\n\u2022 Respond to customers' complaints and resolve their issues\n\u2022 Assist in stocking shelves, rotating merchandise and marking prices""]",[u'Associate in Biology'],[u'Suffolk County Community College\nJanuary 2014 to January 2018']
0,https://resumes.indeed.com/resume/663383f212e61c25,"[u'Data Analyst\nAdvantage Solutions - Fort Dodge, IA\nJune 2017 to Present\nScanned products and entered data for different store locations.']",[u''],"[u'Fort Dodge Senior High Fort Dodge, IA\nJanuary 2012 to January 2016']"
0,https://resumes.indeed.com/resume/40c9fd1786d8e0a4,"[u'Analytics Advisor\nCVS - Woonsocket, RI\nJanuary 2014 to Present\n\u2022 Retail Analytics Support for the Front Store, responsible for the JDA reporting, analytics and integration projects\n\u2022 Planning and operations support to add Promotional/Seasonal planning capabilities within JDA to improve efficiency\n\u2022 Evaluate new tools/systems and find the best fit for heavy data processing CVS systems within Inventory and Analytics\n\u2022 Optimize inventory by solving for the buying multiple, calendar, safety stock, presentation and planning parameters\n\u2022 Design, creation, maintenance of native/custom JDA process to efficiently plan slow moving items saving $100+M\n\u2022 Design and publish dashboards using tableau for planners and directors for supply chain execution, planning, reporting\n\n\u2022 Reporting and Analytics support for Inventory management specific to JDA Forecasting and replenishment System\n\u2022 Implemented Statistical Safety Stock, bringing down inventory by $ 35M USD which increased the instock by 100 bps\n\u2022 Simulate expected impact on Instock, Inventory investment, shipments, receipts, pick efficiency for various initiatives\n\u2022 Support seasonal sales by managing lifts and through seasonal profiles without any impact to Logistics and Store Ops\n\u2022 Generate Exception reports for JDA Super users to prevent inaccurate orders and shipments reaching vendors or Store', u'Data Analyst\nAccenture\nSeptember 2008 to November 2012\n\u2022 Provided data management support during the merging of Bank of America, Merrill Lynch and Nations Bank systems\n\u2022 Implemented process improvements for applications which saved 25% delivery time and saved 15% human resources\n\u2022 Created automated scripts and reports utilizing VBA and Excel macros continuously saving 3000+ hours per release\n\u2022 Performed advanced data mining operations to reduce the redundancy of data and improve data quality of the testbed']","[u'MS in Supply Chain Management', u'MS Computer Science in Computer Science']","[u'University of Texas at Dallas Dallas, TX\nDecember 2013', u'Anna University']"
0,https://resumes.indeed.com/resume/54a60062c3c4e0fd,"[u'Big Data Analyst\nEricsson - Bellevue, WA\nSeptember 2016 to Present\n\u25cf Analyzed 500,000+ rows of SMSI data using Tableau to provide solid evidences for Comcast project proposal\n\u25cf Sampled 200k from 80m subs and merged output via impala and PostgreSQL to analyze reasons for device returns\n\u25cf Parsed h5 file to csv via Python and created Sparkflow diagram to automate data ingestion reducing 40% running time\n\u25cf Deployed multiple Node JS charts codes to make data visualization for dashboard and presented to customer\n\u25cf Involved in Rogers Analytics POC project to work on Elastic Search, Spark and Kibana on AWS and set up Elastic\nSearch instances for both master and data node to form cluster\n\u25cf Performed Scala code to load source data from Amazon S3 and saved to Elastic Search after running SQL queries\n\u25cf Optimized time length for writing 14 GB data into Elastic Search from 10 minutes to 6 minutes by testing the number\nof cores and executors that running on Spark Shell\n\u25cf Performed bucket aggregation (sum, average and percentile) in Elastic Search based on data filtered by keywords', u'Network Analyst\nAirbus DS Communications - Beijing, CN\nMay 2014 to August 2014\n\u2022 Led Taipei Police radio network project by using Planet software for RF network planning and optimization\n\u2022 Inserted, searched and updated base stations database using SQL based on the frequency coverage prediction maps\n\u2022 Made a frequency plan supported by prediction results to recommend on optimized number of carriers\n\u2022 Contributed to 30% increase in Received Signal Strength Indicator in remote areas in Taipei']","[u'Master of Science in Computer Science', u'Master of Science in Electrical Engineering']","[u'The University of Texas at Dallas Dallas, TX\nMay 2016', u'The University of Texas at San Antonio San Antonio, TX\nDecember 2013']"
0,https://resumes.indeed.com/resume/49b4836672e3c276,"[u'Senior Data Analyst\nSevenHills e-Health Pvt. Ltd - Mumbai, Maharashtra\nJuly 2015 to July 2016\nProduction time and Customer Satisfaction Model(Leading Diet Control Food chain from Dubai)\n\u2022 Reduced customer complaints by 17.3% by analysis of feedbacks using Natural Language Tool Kit and clustering in Python\n\u2022 Developed a customer segmentation model using K-means clustering on orders data to develop focussed marketing campaigns by identifying the key market segments that led to improved revenue by 4.2% in four months\n\u2022 Led a two member onsite team in Dubai during the requirements gathering and implementation stage of the project\nData Modelling & Visualization(Health Care client based out of Tanzania)\n\u2022 Developed a visualization model in Tableau to analyse user volume, re-evaluate employee scheduling that led to reduction of patient wait time by 22.47% in five months\n\u2022 Developed an Incentive Model for Doctors by linking revenue generated and departmental costs using Shiny Package in R\n\u2022 Led a team of three analysts who provided support for the web based Information Management System implemented for a\n300 beds hospital in Tanzania that helped them move to a paperless system', u'Business Analyst\nI3 Consulting Pvt Ltd - Noida, Uttar Pradesh\nDecember 2013 to July 2015\nCampaign management Model using Segmentation of Voter Base(Political Party from India)\n\u2022 Analysed the behaviour patterns of individuals from the survey data of over 8 million voters using Clustering & Segmentation\ntechnique in SAS that helped the client to secure 73.44% of votes in General elections in India in 2014\nMarket Share and Sales Improvement using Analytics (Banking client based out of Riyadh)\n\u2022 Designed and developed Governance Mechanism tool for a banking client in Middle East to monitor progress of strategic\nprojects across departments that resulted in achieving target market share of 4.7%']","[u'Masters in Information Systems Management', u'Bachelor of Technology in Electronics and Communication Engineering']","[u'Carnegie Mellon University Pittsburgh, PA\nAugust 2016 to May 2018', u'Indian Institute of Information Technology Allahabad, Uttar Pradesh\nJuly 2010 to June 2014']"
0,https://resumes.indeed.com/resume/bd0c567add1febef,"[u""Data Analyst\nDon's Johns Inc - Gainesville, VA\nNovember 2016 to January 2017\n\u2022 Worked on Acquisition Data Extraction, Validation, Transport and Load from foreign database to the company database\n\u2022 Designed database, created tables in SQL Server for company to merge new data from newly acquired company\n\u2022 Designed and Execute SQL views in SQL Server Reporting Studio for financial needs\n\u2022 Prepared and analyzed financial documents including Annual Budget, PNL, Financial Summary, Sales Forecast, Sales plan, Historical Sales Analysis etc. as per business need\n\u2022 Prepared multiple incentive and reward documents including weekly pay stub analyzing various factors using Advance Excel\n\u2022 Gathered business requirements form SMEs and User and translate those requirements into functional requirement for routing software including Territory Planner, Road Net Anywhere and Serveman\n\u2022 Analyzed fleet financial and carbon emission data using vehicle mile and mileage, fuel consumption and fuel price for financial planning\n\u2022 Developed SQL data base for daily routing, analyzed routing efficiency, presented results to the executives in MS power point\n\u2022 Researched and prepared new business opportunity documents within government subsidies including DBE\n\u2022 Analyzed credits to customer, third party business, post-acquisition sales, identified gaps and recommended solution as per statistical analysis\n\u2022 Recommended changes for system design, methods, procedures, policies and workflows affecting routing\n\u2022 Engaged in training for co-workers and new hires for data systems, software and other topics as assigned"", u""Data Analyst\nOptum - Pittsburgh, PA\nFebruary 2016 to October 2016\n\u2022 Gathered Business Requirements from the Subject Matter Experts (SMEs) and documented the requirements in the BRD. Prepared Functional Specification Documents (FSDs), screen mockups and UML diagrams including Use Case Diagram, Sequence Diagram, and Class Diagrams for MC 400 consolidation and modernization activity\n\u2022 Extensively wrote SQL Queries (Sub queries, correlated sub queries and Join conditions) for Data Accuracy, Data Analysis and Data Extraction needs\n\u2022 Worked on Data mapping, logical data modeling used SQL queries to filter data within the Oracle database tables\n\u2022 Developed and maintained reports using relational databases as data sources for internal and external client reporting\n\u2022 Prepared high level and detailed system requirements documents for the application and prepared system requirement documentation.\n\u2022 Gathered relevant research in support of all Business Segments team efforts to provide in-depth analysis\n\u2022 Analyzed company performance versus competitors and industry benchmarks\n\u2022 Defined and documented business needs and objectives, current operational procedures/problems, input/output requirements as to ensure a company's competitiveness and performance"", u'Analyst\nSMX at Amazon - Sterling, VA\nSeptember 2015 to January 2016\n\u2022 Analyzed various financial documents using Advance excel skills such as Pivot table, VLOOKUP\n\u2022 Disaster planning, severe weather guidelines and evacuation procedures as per company\u2019s guideline and safety and weather data analysis\n\u2022 Conducted performance evaluations using employee hours, work skill area and work velocity and document them in company data base for work efficiency analysis\n\u2022 Prepared work schedules and processed leave request for the staff\n\u2022 Reviewed Amazon\u2019s safety plan and conducted safety audit according to the OSHA guideline\n\u2022 Prepared incident reports and analyzed as per business needs', u'Junior Analyst\nOffice of Human Rights and Disability, City of Worcester - Worcester, MA\nOctober 2014 to August 2015\n\u2022 Gathered data of ""location of accessible housing facility"" in Worcester from various sources, geocoded those locations and prepared a map using Arc GIS\n\u2022 Assisted City Annual Budget planning in gathering information and needs of city residences through stakeholder meetings related to human rights, disability and climate change\n\u2022 Designed and Coordinated housing development survey, coded data and uploaded in city database\n\u2022 Assisted in developing, organizing, planning and promoting a range of local level events including housing conference\n\u2022 Reviewed housing and disability documents in relation to climate change to prepare disability reports\n\u2022 Provided administrative support to the Human Rights and Disability Rights Commission of City of Worcester as needed']",[u'Master of Science'],"[u'Clark University Worcester, MA\nAugust 2012 to May 2014']"
0,https://resumes.indeed.com/resume/d8e81f6f05724c7c,"[u'Trainee Data Analyst\nPresentation Convent School - Delhi, Delhi\nJanuary 2014 to April 2014\nNew Delhi, India\n\n\u2022 Worked on LYBSYS (library management software) to manage library inventory - involved tasks like data entry of available books and new orders.\n\u2022 Analyzed system generated reports\n\u2022 Used excel to analyze and produce meaningful statistics', u'Data Analyst Associate\nLotus Pharmaceuticals - Delhi, Delhi\nJune 2012 to January 2014\nNew Delhi, India\n\n\u2022 Data Analysis for Inventory management software that tracks inventory, provides list of suppliers, track dispatch, and delivery of orders\n\u2022 Designed and created data reports to help business executives in their decision making\n\u2022 Data mining and analysis of data to categorize and document data']","[u'Master of Professional Studies in Informatics', u'Master of Arts in History']","[u'Northeastern University Boston, MA\nJanuary 2019', u'Delhi University Delhi, Delhi\nJuly 2014']"
0,https://resumes.indeed.com/resume/84633292f4a9883e,"[u'Data Analyst\nPepsico - Purchase, NY\nOctober 2017 to Present\nRoles and Responsibilities:\n\n\u2022 Analyzed business requirements into detailed, production-level technical specifications, detailing new features and enhancements to existing business functionality.\n\u2022 Executed data modeling using ERStudio data modeling tool.\n\u2022 Created the data models for OLTP and analytical systems.\n\u2022 Engaged in data profiling to integrate data from different sources.\n\u2022 Performed the analysis to improve the performance to meet the performance metrics.\n\u2022 Assisted DBAs on support needs and provided guidance on architectural issues.\n\u2022 Worked with Development teams with loading and extracting data (ETL).\n\u2022 Facilitated QA for developing test plans and test cases for unit, system and Enterprise testing.\n\u2022 Collaborated on the data mapping document from source-to-target and the data quality assessments for the source data.\n\u2022 Modeled new data models based on user requirements and updating existing data model.\n\u2022 Discussed with business clients on providing tactical solutions and designing logical data models accordingly. Worked with clients to generate use case scenarios based on user requirements.\n\u2022 Verified if the data model helps in retrieving the required data by creating data access paths in the data model.\nEnvironment: ER Studio, Teradata, MS-Excel, STAR Team Migration Request generation tool.', u""Data Analyst\nWells Fargo - Charlotte, NC\nNovember 2016 to July 2017\nLocation: Charlotte, NC Domain: Banking\nProject: Data Migration, Verification & Validation\nPosition: Data Analyst\n\nTechnical Environment: Informatica Power Center, HP-ALM, Sharepoint, MS-Visio, MS-Excel, Teradata SQL Assistant, Qlikview, SAP-BO, Oracle 11g, Microsoft SQL Server.\n\nRoles and Responsibilities:\n\u2022 Performed Database testing and the Report level testing as per the requirement with excellent knowledge in understanding the data workflow by referring through FSD's(Functional Specification Document).\n\u2022 Excellent understanding of mapping between Source and Target by referring the mapping document.\n\u2022 Performed end to end mapping testing for database as well as reports. The type of mapping involved is one to one and its lift and shift process, that means need to check whether the data gathered in target table is mapped properly to the Source table and the same target table is populating the same records into the report tool(SAP-BO, Qlikview) properly.\n\u2022 Performed Smoke test to do the primary checks like record counts, Column matching for database and Dashboard testing.\n\u2022 Performed testing at SIT(System Integration Testing) level and UAT(User Acceptance testing) level.\n\u2022 Gathered requirements from the development team and database developers to analyze the tables and entity relationships for understanding the database.\n\u2022 Designed the integration document/xls. Derive input and output of each of the integration points.\n\u2022 Documented the acceptance criteria for each of the test cases. Built the test cases based on test scenarios.\n\u2022 Created test plan and strategy for the given LOB(Line of Business).\n\u2022 Verified of import/export and obfuscation data.\n\u2022 Verified of known issues, development of work arounds and wrappers as required.\n\u2022 Identified data scenarios and business cases. Created test case development.\n\u2022 Scripted and automated test cases. Identified source data pattern and reports.\n\u2022 Developed scripts for comparison with target. Planed and run the SIT(System Integration Testing) for the given LOB.\n\u2022 Developed test cases, established traceability between requirements and test cases.\n\u2022 Performed data analysis to determine the completeness and accuracy of the data or checked if new data needs to be pulled up/ requested.\n\u2022 Executed test cases & log results, performed data validation testing as appropriate, tracked defects and participated in defect resolution.\n\u2022 Provided inputs to the test lead for documentation and reporting purposes. Identified, documented and updated testing dependencies and participants.\n\u2022 Identified primary point of contact to raise the risks/issues around testing dependencies.\n\u2022 Reported status on test execution including risks/issues and targets. Updated latest information in regular testing status meetings with all involved constituencies to ensure smooth test execution and timely issue resolution."", u'Data Analyst\nEquifax Workforce Solutions - Alpharetta, GA\nJanuary 2015 to October 2016\nDomain: Healthcare/Medicare\nLocation: Alpharetta, GA Jan 2015 - Oct 2016\nProject: Affordable Care Act Management(ObamaCare)\nPosition: Data Analyst\n\n\u2022 Expertise in configuring the instance in eThority tool as per the Configuration Acceptance document approved by Business Analyst and Activation Manager. Experience in Configuring the CORE/IRS Instance that involves data files with single feed and multiple feeds, single control group or multiple control group, at the same time preparing Instance Verification document for all the CORE/IRS instances that has different version of Specification files created by Business Analyst as per the Client requirement for every individual instances in order to check the eligibility of the employee for the 1095C form.\n\u2022 Experience in modifying the instance and purging (i.e. Clearing the data from the database) the data from the data warehouse as required by the client to get the clean validation pass for the files.\n\u2022 Ability to analyze the data by performing Extraction, Transformation & Load function mentioned by clients to process and get the accurate feedback that includes cause for warnings & failures. Ability to explain the cause for the loss of records when the data moves from import table to Transformation table.\n\u2022 Ability to analyze the problems occurring with the clients data by verifying the contents of the file from the data warehouse in order to provide the accurate feedback by providing the exact cause of the problem along with its fix. Ability to analyze data in the database SQL Server by running SQL queries & troubleshoot the issues related to the clients data. Worked and created a PPACA Document as per the client requirement.\n\u2022 Good experience in Data mining with regards to collecting, searching through, and analyzing a large amount of data in a database, as to discover patterns or relationships. Good experience in transforming EMR(Electronic Medical Record) to Database.\n\u2022 Experience in performing the SME (Subject Matter Expert) review done by Business Analyst & Activation Manager by thoroughly verifying list of questionnaire & updating the same and inform Business Analyst and Activation Manager about the necessary actions to be taken to resolve the issue.\n\u2022 Experience in performing Regression testing i.e. finding the loopholes or bug in the tool by building up the test data as per the test cases which is then sent to development team to develop a software patch for that, in order to fix the issue. Experience working with large amounts of data: facts, figures, and number crunching. Ability to see through the data and analyze it to find conclusions. Experience in working as Quality Analyst by reviewing the task done by QA ETL Tester. Experience in mentoring the new joinees about the project and training them.\n\u2022 Ability to present findings, or translate the data into an understandable document. Expertise to write and speak clearly, easily communicating complex ideas. Ability to look at the numbers, trends, and data and come to new conclusions based on the findings.\n\u2022 Ability to understand current business processes and implement efficient business process.\n\u2022 Expertise in defining scope of projects based on gathered Business Requirements including documentation of constraints, assumptions, business impacts & project risks. Strong background in support documentation. Analysis and review of Business Requirement Documents.\n\u2022 Conducting requirement gathering sessions, feasibility studies and organizing the business requirements in a structured way.\n\u2022 Gathering business and Technical requirements that would best suit the needs of the technical architectural development process.\nEnvironment: Ethority(BI Tool), SQL Server, MS-Office, Tableau, Teradata , SQL, WinRunner', u""Data Analyst\nHometown National Bank - Longview, WA\nFebruary 2014 to November 2014\n\u2022 Performed data profiling in the source systems that are required for New Customer Engagement (NCE) Data mart.\n\u2022 Document the complete process flow to describe program development, logic, testing, and implementation, application integration, coding.\n\u2022 Manipulating, cleansing & processing data using Excel, Access and SQL.\n\u2022 Responsible for loading, extracting and validation of client data.\n\u2022 Good experience in Data mining with regards to convert raw data into useful information. By using software to look for patterns in large batches of data.\n\u2022 Liaising with end-users and 3rd party suppliers. Analyzing raw data, drawing conclusions & developing recommendations Writing SQL scripts to manipulate data for data loads and extracts.\n\u2022 Developing data analytical databases from complex financial source data. Performing daily system checks. Data entry, data auditing, creating data reports & monitoring all data for accuracy. Designing, developing and implementing new functionality.\n\u2022 Monitoring the automated loading processes. Advising on the suitability of methodologies and suggesting improvements. Involved in defining the source to target data mappings, business rules, business and data definitions. Responsible for defining the key identifiers for each mapping/interface.\n\u2022 Responsible for defining the functional requirement documents for each source to target interface.\n\u2022 Document, clarify, and communicate requests for change requests with the requestor and coordinate with the development and testing team. Reverse engineered all the Source Database's using Embarcadero.\n\u2022 Coordinate with the business users in providing appropriate, effective and efficient way to design the new reporting needs based on the user with the existing functionality.\n\u2022 Document data quality and traceability documents for each source interface.\n\u2022 Designed and implemented data integration modules for Extract/Transform/Load (ETL) functions.\n\u2022 Involved in Data warehouse and Data mart design. Experience with various ETL, data warehousing tools and concepts.\n\u2022 Documented the complete process flow to describe program development, logic, testing, and implementation, application integration, coding.\n\u2022 Worked with internal architects and, assisting in the development of current and target state data architectures.\n\u2022 Worked with project team representatives to ensure that logical and physical ER/Studio data models were developed in line with corporate standards and guidelines.\n\u2022 Also worked on some impact of low quality and/or missing data on the performance of data warehouse client. Identified design flaws in the data warehouse.\n\u2022 Designed application components in an Agile environment utilizing a test driven development approach. Created and maintained project tasks and schedules.\n\u2022 Provided task estimates, identified potential problems and recommended alternative solutions.\n\u2022 Worked in close cooperation with project managers and other functional team members to form a team effort in development. Collaborated with other members of the product development team.\n\u2022 Coordinated configuration of back-end components in support of application development.\nEnvironment: SQL/Server, Oracle10 &11g, MS-Office, Netezza, Teradata , MDM, Informatica Data Quality, ER Studio, TOAD, Business Objects, Microstrategy, SAP, Greenplum Database, Qlikview, PowerPivot, Selenium, SoapUI, CruiseControl.Net, HP Quality Center 10, Maven, PL/SQL, OBIEE, Cognos."", u'Data Analyst\nLeap Wireless - SanDiego, CA\nNovember 2013 to February 2014\n\u2022 Work with users to identify the most appropriate source of record required to define the asset data for financing\n\u2022 Perform data profiling in the source systems that are required for financing\n\u2022 Document the complete process flow to describe program development, logic, testing, and implementation, application integration, coding.\n\u2022 Involved in defining the trumping rules applied by Master Data Repository\n\u2022 Define the list codes and code conversions between the source systems and MDR.\n\u2022 Worked with internal architects and, assisting in the development of current and target state enterprise data architectures.\n\u2022 Worked with project team representatives to ensure that logical and physical ER/Studio data models were developed in line with corporate standards and guidelines\n\u2022 Involved in defining the source to target data mappings, business rules, business and data definitions\n\u2022 Responsible for defining the key identifiers for each mapping/interface\n\u2022 Responsible for defining the functional requirement documents for each source to target interface.\n\u2022 Document, clarify, and communicate requests for change requests with the requestor and coordinate with the development and testing team.\n\u2022 Created the DDL scripts using ER Studio and source to target mappings (S2T- for ETL) to bring the data from JDE to the warehouse.\n\u2022 Created the dimensional logical model with approximately 10 facts, 30 dimensions with 500 attributes using ER Studio.\n\u2022 Involved in configuration management in the process of creating and maintaining an up-to-date record of all the components of the development efforts in coding and designing schemas\n\u2022 Developed the financing reporting requirements by analyzing the existing business objects reports\n\u2022 Interact with computer systems end-users and project business sponsors to determine, document, and obtain signoff on business requirements.\n\u2022 Responsible in maintaining the Enterprise Metadata Library with any changes or updates\n\u2022 Document data quality and traceability documents for each source interface\n\u2022 Establish standards of procedures. Generate weekly and monthly asset inventory reports.\n\u2022 Coordinate with the business users in providing appropriate, effective and efficient way to design the new reporting needs based on the user with the existing functionality.\n\u2022 Remain knowledgeable in all areas of business operations in order to identify systems needs and requirements.\n\u2022 Provided direction and shares knowledge with and mentored team members in areas of expertise.\n\u2022 Identified and continuously acted to improve individual and team knowledge of new technologies, business processes and project management skills.\n\u2022 Stayed current on trends, latest industry developments and shared knowledge among colleagues.\n\u2022 Excellent skills in user research and analysis of the existing systems, with knowledge of traceability matrix.\n\u2022 Strong Experience in conducting User Acceptance Testing (UAT), Unit Testing and documenting Test Cases and Test Scripts.\n\u2022 Involved in Test Planning, Test Preparation, Test Execution, Issue resolution and Report Generation to assure that all aspects of a project are in compliance with the business requirements.\n\u2022 Excellent verbal and written communication skills with the ability to interact professionally with stakeholders at all levels of the organization, including the ability to exercise good judgment in frequency and nature of communications to senior management, stakeholders and team members.\n\u2022 Possess a disciplined, professional and quality centered approach with strong analytical and problem solving skills.\n\nEnvironment: SQL/Server, Oracle 9i, MS-Excel, Teradata, MDM, SAP, QlikView, Microstrategy, Informatica, PowerPivot, ER Studio, XML, Selenium, SoapUI, CruiseControl.Net, HP Quality Center 10, Maven Business Objects, OBIEE, Cognos', u""Data Analyst\nKoolKart - IN\nJune 2012 to May 2013\n\u2022 Creation of Teradata FastLoad, BTEQ, Fast export scripts and UNIX shell scripts to perform the fulfillments Creation of Tables, Views to fulfill the business needs and support the Production systems with day to day activities and support the performance issues.\n\u2022 Developed SQL, BTEQ (Teradata) queries for Extracting data from production database and built data structures, reports.\n\u2022 Used the MS Excel, MS Access for data pulls and ad hoc reports for analysis.\n\u2022 Performance tuning and Optimization of large database for fast accessing data and reports Ms Excel\n\u2022 Informatica, Oracle, Teradata being used for data transformation\n\u2022 Reports generated for various departments like Tele Marketing, Mailing, and New Accounts by using SQL, BTEQ, Ms Access.\n\u2022 Developed Data mining processes for large data files.\n\u2022 Ad-hoc reports developed using Oracle, SQL, and UNIX.\n\u2022 Performed in depth analysis in data & prepared weekly, biweekly, monthly reports by using SQL, SAS, Ms Excel, Ms Access, and UNIX.\n\u2022 Extensively used excel and VBA to generate reports\n\u2022 Created and manipulated datasets using SAS, Access, and Excel.\n\u2022 Wrote JCL programs to communicate with the MVS (TSO, ISPF) operating system.\n\u2022 Designed and implemented SQL queries for QA Testing and Report / Data Validation\n\u2022 Assisted in preparing BRDs, FRDs, and Test Cases.\n\u2022 Design Extract Transform load (ETL) procedures for data import and Manipulation.\n\u2022 Worked with data modelers and assist them in verifying and validating the relationships between tables using Teradata SQL assistant.\n\u2022 Execution, Maintenance & Debugging of the scripts with Teradata Bteq.\n\u2022 Execution, Maintenance & Debugging of SAS scripts to update the Teradata Tables\n\u2022 Creat Teradata tables, Views, Macros and analyzed various Teradata tables for UPI's and Monitor the same. Execution of Mainframes JCL scripts.\n\u2022 Utilized ODBC for connectivity to Teradata & MS Excel.\n\u2022 Good experience in identification and requirements gathering as per the business requirements via meetings, interviews, interface analysis, research, etc.\n\u2022 Experience in documentation of business requirements and system functional specifications in the form of Use Cases.\n\u2022 Utilized a combination of business knowledge, technical skills and strategic analysis to provide solutions and creative insights to critical business problems.\n\u2022 Maintained channels of organizational communication and acted as the point of contact between teams.\nEnvironment: Teradat, Informatica, Ab Initio, Business Objects, Oracle 9i, PL/SQL, Microsoft Office Suite (Excel, Vlookup, Pivot, Access, Power Point), Visio, PowerPivot, QlikView, SAP, Cognos, Micro Strategy, Tableau , ERWIN, OBIEE."", u""Data Analyst\nIBM - IN\nSeptember 2009 to April 2012\n\u2022 Stored reformatted data from different Mainframe applications using Informatica (ETL).\n\u2022 Translated business-reporting requirements into data warehouse architectural design.\n\u2022 Designed and maintained logical and physical enterprise data warehouse schemas\n\u2022 Designed, developed and deployed new data marts along with modifying existing\n\u2022 Marts to support additional business requirements.\n\u2022 Extracted business customer product/revenue data from Express Track systems and consumer product/revenue data. Created naming and system standards for lookup, transformation and target tables. Loaded consolidated data using SQL*Loader in parallel and direct mode.\n\u2022 Used Business Objects Designer, Reporter and Supervisor.\n\u2022 Scheduled and monitored transformation processes using Informatica Server Manager.\n\u2022 Designed, developed and implemented universes using Business Objects and Web intelligence.\n\u2022 Maintained stored definitions, transformation rules and targets definitions using Informatica repository manager. Generated reports for end client using Query tools.\n\u2022 Developed data mining to validate various data pulls for accuracy and completeness.\n\u2022 Involved in meeting with Project owners and project managers to define the scope of the project.\n\u2022 Followed the Agile methodology for elicitation and representation of requirements based on interaction with the Project owner, SME's and the development team, participated in daily scrum.\n\u2022 Facilitated requirement sessions, including creation of agendas and recording of action items from meetings and writing meeting minutes.\n\u2022 Documented the requirements in the form of detailed use cases and sent out the same for inspection to the team.\n\u2022 Responsible for creating Use case diagrams and process flow diagrams by using tools like MS-Visio.\n\u2022 Involved in preparing test cases and also involved in reviewing test plans and test scripts developed by QA team to make sure that all requirements are covered in scripts and tested properly.\nEnvironment:\nInformatica ETL , ER-Win, Shell Scripts, SQL*Loader, QlikView, SAP, Business Objects 5.0, PowerPivot, Web Intelligence 2.0, BigData, Oracle 7.x/8.0, Selenium, SoapUI, CruiseControl.Net, HP Quality Center 10, Maven Windows NT, OBIEE"", u'Data Analyst\nInfoSource Tech Systems Pvt Ltd - IN\nSeptember 2006 to September 2009\n\u2022 Involved in the design of the overall database using Entity Relationship diagrams.\n\u2022 Wrote triggers, menus and stored procedures in PL/SQL.\n\u2022 Involved in developing interactive forms and customization of screens using Forms 4.5.\n\u2022 Involved in building, debugging and running forms.\n\u2022 Involved in Data loading and Extracting functions using SQL*Loader.\n\u2022 Designed and developed all the tables, views for the system in Oracle.\n\u2022 Designing and developing forms validation procedures for query and update of data.\n\u2022 Extensive learning and development activities.\n\u2022 Performed back-end testing for the Application.\n\u2022 Performed manual testing for integration and acceptance.\n\u2022 Performed negative and positive testing for the application.\n\u2022 Modified existing Test Plans and Test Scripts for regression testing.\n\u2022 Conducted data integrity testing by extensive use of SQL.\nEnvironment: Oracle 8.0, SQL*plus, SQL*Loader, T-SQL, PL/SQL, Forms 4.5, Reports 2.5, Business Objects, WinRunner']","[u'Masters in Computer Engineering', u'Bachelors in Electronics Engineering']","[u'Mumbai University Mumbai, Maharashtra\nFebruary 2006', u'Mumbai University Mumbai, Maharashtra\nJune 2001']"
0,https://resumes.indeed.com/resume/12be56aa6c25215c,"[u""Credit Specialist\nTechnology and Life Sciences Division\nJune 2017 to Present\nMaintained accurate financial records and provided financial spreads to aid the underwriting department in their RECAP process\n\u2022 Assisted the lenders with the underwriting and documentation of loan requests.\n\u2022 Prepared various reports such as Delinquent Statement Report, Past Due Loans,\nCovenant Exception Report and/or Input risk rating.\n\u2022 Managed the compliance for one of the bank's growing portfolios.\n\u2022 Supported lending department by creating borrowing base certificates and new borrower\nkits for new and existing deals."", u""Data Analyst\nGummicube Inc\nJune 2016 to October 2016\nManaged and prepared results on tracking/reporting/metrics for keyword movements for app store optimization purposes.\n\u2022 Performed data entry and advanced data analyses to produce reports to support business\noperations in creating new rounds of marketing campaigns.\n\u2022 Generated daily reports in support of various account managers to relay back to B2B\nclients in showing which keywords and genres are trending for their app.\n\u2022 Provided reporting assistance to strategic management team to create new keyword\nproposals, icon changes, and app store descriptions to increase clients' app visibility.""]",[u'Bachelors of Science in Economics'],"[u'San Jose State University San Jose, CA\nJanuary 2011 to January 2015']"
0,https://resumes.indeed.com/resume/a25e5a3f16a47643,"[u'Data Analyst\nPrisma-P Laboratory - Gainesville, FL\nJune 2017 to Present\nDivision of Nephrology, College of Medicine, University of Florida, Gainesville, Florida\nRisk scores automation development using 4-year medical data from Shands Hospital with Python,\nincreased 15% accuracy by eliminating human factor\nData management, cleaning, outlier detection and visualization with Python and R', u'Operation Consultant\nBaijiahulian Technology Co., Ltd\nFebruary 2016 to May 2016\nProvided independent teachers with online products scheme including product transformation, pricing and resources allocation etc.\nCooperated with head office with website advertising resources utilization for local branch office']","[u'Master of Engineering in Industrial Engineering', u'Bachelor of Natural Science in Materials Physics']","[u'University of Florida Gainesville, FL\nMay 2018', u'Sichuan University Chengdu, CN\nJune 2016']"
0,https://resumes.indeed.com/resume/ba94b28641832458,"[u'DATA ANALYST\nKPIT TECHNOLOGIES (INDIA)\nMay 2014 to December 2016\nWorked with data analyst team to generate reports and dashboards on TABLEAU and SAP LUMIRA using business\ndata.\n\u2022 Extensive use of SAS to perform different statistical methodologies to derive inference from the dataset\n\u2022 Provided multiple demonstration of Tableau functionality and efficient data visualization to the senior management\nas a part of business requirement.\n\u2022 Analyzing and reporting data in order to identify issues, trends to drive improvements of results and find solution.\n\u2022 Importing the raw data from various databases like Excel, Access and CSV files into SAS system\n\u2022 Gather master data requirements and strategies to support supply chain and manufacturing transition into SAP.\n\u2022 Used Microsoft Excel tools like pivot tables, charts and graphs to perform quantitative analysis\n\u2022 Performed data cleansing to maintain data consistency and minimizing error\n\u2022 Performed data analysis and data profiling using complex SQL on various system including Oracle and SAP.\n\nACADEMIC EXPERIENCE\n\u2022 SOCIAL MEDIA ANALYTICS R, Twitter API, Tableau\nDeriving Social media strategies of the fast food companies impacting their customer sentiments and profitability.\nExtracted data from different social media platforms such as Facebook, twitter, and Instagram using different\npackages in R-studio.\nExtensive use of Tableau, IBM Watson and Gephi for data visualization.\nCreated Visualization such as word cloud, dendrogram and histogram\nPerformed lexicon based positive and negative sentiment analysis on Facebook and twitter data.\nPerformed data cleansing to ensure data consistency and minimal errors using R-studio.\n\n\u2022 DATA ANALYTICS USING STATISTICAL METHODOLOGIES (SAS) SAS ENTERPRISE MINER\nConducted data analysis on dataset ""US Mass Shooting"" of past 10 years, gathered from Kaggle, Mother Jones and USA Today. Merged different datasets into one and performed data cleansing using MS Excel and R. Performed\nvarious statistical methods such as regression, clustering and moderation using SAS. Drawn conclusion based on significance/insignificance of statistical models.']","[u'M.S. in Information Systems in Information Systems', u'M.S. in Computer Application in Computer Application']","[u'Northern Illinois University', u'University of Pune Pune, Maharashtra']"
0,https://resumes.indeed.com/resume/70f126c3afb08d84,"[u'Data Scientist\nIWhere Co., Ltd - Pleasanton, CA\nDecember 2016 to Present', u'Data Scientist - Consultant\nIWhere(Beijing) Co., Ltd - Beijing, CN\nJune 2015 to December 2016\n\u2022 Identified key metrics to measure the success of products and features, design A/B test or hypothesis test plan, processed data sets with advanced querying, analyzed data with analytics packages in Python and R, and interpreted results to the cross-functional team.\n\u2022 Indicated and diagnosed problems with KPIs variations, located effective variables with statistical models, and recommended improvement solutions.\n\u2022 Built regression, clustering, and classification models to evaluate and optimize business actions. Boosted 35% on user acquisition, extended life-circle by 20%.\n\u2022 Cooperated with product and engineering team on feeds recommender with Bayes and Markov techniques, lifted user engagement by 15%.\nData Scientist - Consultant 2015.6-2016.12\nIWhere(Beijing) Co., Ltd. Beijing, China', u'Data Analyst\nAHome LLC - Arcadia, CA\nFebruary 2014 to June 2015']","[u'Master of Science in Statistics', u'Bachelor of Science in Statistics']","[u'San Jose State University San Jose, CA\nAugust 2011 to December 2014', u'Capital University of Economics and Business Beijing, CN\nAugust 2007 to July 2011']"
0,https://resumes.indeed.com/resume/0c41d694bd0ca9f3,"[u'Senior Data Analyst\nCenterPoint Group - Mount Laurel, NJ\nOctober 2014 to Present\n\u2022 Generate and implement cost-saving models on ERP platforms for Fortune 500 Companies by providing qualitative and quantitative analysis of multinational supplier agreements and optimizing purchasing programs.\n\u2022 Analysis of over $500M in client spend successfully reducing expenditures by over $100M.\n\u2022 Perform procurement Contract Audits and generate SQL reports using Microsoft Reporting Services (SSRS) to reflect client specific KPIs, Price Trends, Benchmarks, Billing, and Overcharges.\n\u2022 Conduct statistical research on Tableau and SAS to build efficiencies in big data analysis.\n\u2022 Perform cross-platform database migration from Microsoft SQL Server to Microsoft Access, create and modify tables, views, forms and reports based on new business models.\n\u2022 Create Ad Hoc reports and Key Performance Dashboard as well as import current data into Salesforce.\n\u2022 Create User and Developer Manuals for all SQL reports on Microsoft Dynamics CRM system, including end-user guidelines, data flow charts, E-R diagrams and calculating methods.\n\u2022 Develop Excel Training Program and Custom Protocols on Microsoft Dynamics for junior analysts.\n\u2022 Analyze and forecast large data sets with intermediate Excel functions including VBA, pivot table, solver, advance lookup functions, conditional format and built-in statistical tools.\n\u2022 Conduct A/P & A/R reconciliation with clients, suppliers, service providers and third-party auditors.']",[u'B.S. in Finance'],"[u'Rutgers University, Rutgers Business School New Brunswick, NJ\nMay 2014']"
0,https://resumes.indeed.com/resume/cbb67b6356600313,"[u""Data Scientist/ Machine Learning\nFIS - Jacksonville, FL\nJanuary 2017 to Present\nDescription: FIS provides financial software, world-class services and global business solutions. Let us help you compete and win in today's chaotic marketplace. Fidelity National Information Services Inc., better known by the abbreviation FIS, is an international provider of financial services technology and outsourcing services. FIS is the world's largest global provider dedicated to financial technology solutions. FIS empowers the financial world with software, services, consulting and outsourcing solutions focused on retail and institutional banking, payments, asset and wealth management, risk and compliance, trade enablement, transaction processing and record-keeping.\n\nResponsibilities:\n\u2022 Extracted data from HDFS and prepared data for exploratory analysis using data munging\n\u2022 Built models using Statistical techniques like Bayesian HMM and Machine Learning classification models like XGBoost, SVM, and Random Forest.\n\u2022 Participated in all phases of data mining, data cleaning, data collection, developing models, validation, visualization and performed Gap analysis.\n\u2022 A highly immersive Data Science program involving Data Manipulation&Visualization, Web Scraping, Machine Learning, Python programming, SQL, GIT, MongoDB, Hadoop.\n\u2022 Setup storage and data analysis tools in AWS cloud computing infrastructure.\n\u2022 Installed and used Caffe Deep Learning Framework\n\u2022 Worked on different data formats such as JSON, XML and performed machine learning algorithms in Python.\n\u2022 Worked as Data Architects and IT Architects to understand the movement of data and its storage and ER Studio 9.7\n\u2022 Used pandas, numpy, seaborn, matplotlib, scikit-learn, scipy, NLTK in Python for developing various machine learning algorithms.\n\u2022 Data Manipulation and Aggregation from different source using Nexus, Business Objects, Toad, Power BI and Smart View.\n\u2022 Implemented Agile Methodology for building an internal application.\n\u2022 Focus on integration overlap and Informatica newer commitment to MDM with the acquisition of Identity Systems.\n\u2022 Coded proprietary packages to analyze and visualize SPCfile data to identify bad spectra and samples to reduce unnecessary procedures and costs.\n\u2022 Programmed a utility in Python that used multiple packages (numpy, scipy, pandas)\n\u2022 Implemented Classification using supervised algorithms like Logistic Regression, Decision trees, Naive Bayes, KNN.\n\u2022 As Architect delivered various complex OLAPdatabases/cubes, scorecards, dashboards and reports.\n\u2022 Updated Python scripts to match training data with our database stored in AWS Cloud Search, so that we would be able to assign each document a response label for further classification.\n\u2022 Used Teradata utilities such as Fast Export, MLOAD for handling various tasks data migration/ETL from OLTP Source Systems to OLAP Target Systems\n\u2022 Data transformation from various resources, data organization, features extraction from raw and stored.\n\u2022 Validated the machine learning classifiers using ROC Curves and Lift Charts.\n\nEnvironment:Unix, Python 3.5.2, MLLib, SAS, regression, logistic regression, Hadoop 2.7.4, NoSQL, Teradata, OLTP, random forest, OLAP, HDFS, ODS, NLTK, SVM, JSON, XML and MapReduce."", u""Data Scientist\nCBRE - Dallas, TX\nOctober 2015 to November 2016\nDescription: CBRE Group, Inc. is the largest commercial real estate services and investment firm in the world. It is based in Los Angeles, California and operates more than 450 offices worldwide and has clients in more than 100 countries.\n\nResponsibilities:\n\u2022 Utilized Spark, Scala, Hadoop, HQL, VQL, oozie, pySpark, Data Lake, TensorFlow, HBase, Cassandra, Redshift, MongoDB, Kafka, Kinesis, Spark Streaming, Edward, CUDA, MLLib, AWS, Python, a broad variety of machine learning methods including classifications, regressions, dimensionally reduction etc.\n\u2022 Utilized the engine to increase user lifetime by 45% and triple user conversations for target categories.\n\u2022 Application of various machine learning algorithms and statistical modeling like decision trees, text analytics, natural language processing (NLP), supervised and unsupervised, regression models, social network analysis, neural networks, deep learning, SVM, clustering to identify Volume using scikit-learn package in python, Matlab.\n\u2022 Worked onanalyzing data from Google Analytics, AdWords, Facebook etc.\n\u2022 Evaluated models using Cross Validation, Log loss function, ROC curves and used AUC for feature selection and elastic technologies like ElasticSearch, Kibana.\n\u2022 Performed Data Profiling to learn about behavior with various features such as traffic pattern, location, Date and Time etc.\n\u2022 Categorized comments into positive and negative clusters from different social networking sites using Sentiment Analysis and Text Analytics\n\u2022 Performed Multinomial Logistic Regression, Decision Tree, Random forest, SVM to classify package is going to deliver on time for the new route.\n\u2022 Performed data analysis by using Hive to retrieve the data from Hadoop cluster, Sql to retrieve datafrom Oracle database and used ETL for data transformation.\n\u2022 Performed Data Cleaning, features scaling, features engineering using pandas and numpy packages in python.\n\u2022 Exploring DAG's, their dependencies and logs using AirFlow pipelines for automation\n\u2022 Performed data cleaning and feature selection using MLlib package in PySpark and working with deep learning frameworks such as Caffe, Neon.\n\u2022 Developed Spark/Scala,R Python for regular expression (regex) project in the Hadoop/Hive environment with Linux/Windows for big data resources.\n\u2022 Used clustering technique K-Means to identify outliers and to classify unlabeled data.\n\u2022 Tracking operations using sensors until certain criteria is met using AirFlowtechnology.\n\u2022 Responsible for different Data mapping activities from Source systems to Teradata using utilities like TPump, FEXP,BTEQ, MLOAD, FLOADetc\n\u2022 Analyze traffic patterns by calculating autocorrelation with different time lags.\n\u2022 Ensured that the model has low False Positive Rate and Text classification and sentiment analysis for unstructured and semi-structured data.\n\u2022 Addressed over fitting by implementing of the algorithm regularization methods like L1 and L2.\n\u2022 Used Principal Component Analysis in feature engineering to analyze high dimensional data.\n\u2022 Used MLlib, Spark's Machine learning library to build and evaluate different models.\n\u2022 Implemented rule based expertise system from the results of exploratory analysis and information gathered from the people from different departments.\n\u2022 Created and designed reports that will use gathered metrics to infer and draw logical conclusions of past and future behavior.\n\u2022 Developed MapReduce pipeline for feature extraction using Hive and Pig.\n\u2022 Created Data Quality Scripts using SQL and Hive to validate successful data load and quality of the data. Created various types of data visualizations using Python and Tableau.\n\u2022 Communicated the results with operations team for taking best decisions.\n\u2022 Collected data needs and requirements by Interacting with the other departments.\n\nEnvironment:Python 2.x, CDH5, HDFS, Hadoop 2.3, Hive, Impala, AWS, Linux, Spark, Tableau Desktop, SQL Server 2014, Microsoft Excel, Matlab, Spark SQL, Pyspark."", u'Data Analyst\nWalgreens - Deerfield, IL\nDecember 2013 to September 2015\nDescription:The Walgreens Companyis an American company that operatesas the second-largest pharmacy store chain in the United States. It specializes in filling prescriptions, health and wellness products, health information, and photo services\n\nResponsibilities:\n\u2022 Worked with BI team in gathering the report requirements and also Sqoop to export data into HDFS and Hive\n\u2022 Involved in the below phases of Analytics using R, Python and Jupyter notebook.\n\u2022 a. Data collection and treatment: Analysed existing internal data and external data, worked on entry errors,classification errors and defined criteria for missing values\nb. Data Mining: Used cluster analysis for identifying customer segments, Decision trees used for profitable and non-profitable customers, Market Basket Analysis used for customer purchasing behaviour and part/product association.\n\u2022 Developed multiple Map Reduce jobs in Java for data cleaning and preprocessing.\n\u2022 Assisted with data capacity planning and node forecasting.\n\u2022 Installed, Configured and managed Flume Infrastructure\n\u2022 Administrator for Pig, Hive and HBase installing updates patches and upgrades.\n\u2022 Worked closely with the claims processing team to obtain patterns in filing of fraudulent claims.\n\u2022 Worked on performing major upgrade of cluster from CDH3u6 to CDH4.4.0\n\u2022 Developed Map Reduce programs to extract and transform the data sets and results were exported back to RDBMS using Sqoop.\n\u2022 Patterns were observed in fraudulent claims using text mining in R and Hive.\n\u2022 Exported the data required information to RDBMS using Sqoop to make the data available for the claims processing team to assist in processing a claim based on the data.\n\u2022 Developed Map Reduce programs to parse the raw data, populate staging tables and store the refined data in partitioned tables in the EDW.\n\u2022 Created tables in Hive and loaded the structured (resulted from Map Reduce jobs) data\n\u2022 Using HiveQL developed many queries and extracted the required information.\n\u2022 Created Hive queries that helped market analysts spot emerging trends by comparing fresh data with EDW reference tables and historical metrics.\n\u2022 Was responsible for importing the data (mostly log files) from various sources into HDFS using Flume\n\u2022 Enabled speedy reviews and first mover advantages by using Oozie to automate data loading into the Hadoop Distributed File System and PIG to pre-process the data.\n\u2022 Provided design recommendations and thought leadership to sponsors/stakeholders that improved review processes and resolved technical problems.\n\u2022 Managed and reviewed Hadoop log files.\n\u2022 Tested raw data and executed performance scripts.\n\nEnvironment:HDFS, PIG, HIVE, Map Reduce, Linux, HBase, Flume, Sqoop, R, VMware, Eclipse, Cloudera, Python.', u""Data Scientist\nJohnson and Johnson - Raritan, NJ\nSeptember 2012 to November 2013\nDescription:Transamerica is the US-based brand of Aegon, a Dutch financial services firm. Aegon is one of the world's leading providers of life insurance, pensions and asset management and is helping approximately 30 million customers globally to achieve a lifetime of financial security.\n\nResponsibilities:\n\u2022 Manipulating, cleansing & processing data using Excel, Access and SQL.\n\u2022 Responsible for loading, extracting and validation of client data.\n\u2022 Modelled clean data into the Kafka servers for use over the spark engine.\n\u2022 Zookeeper along with Kafka was used to stream data and end-to-end client communication.\n\u2022 Performed transformations over the warehoused data using Scala& Python and modelled the data back into the servers for iterative transformations into KAFKA.\n\u2022 Modelled data using Machine learning libraries(Sci-kit learn) apart from SVN and KNN based classificationto create a training dataset for use in a predictive model.\n\u2022 Liaising with end-users and 3rd party suppliers.\n\u2022 Analyzing raw data, drawing conclusions & developing recommendations\n\u2022 Writing T-SQL scripts to manipulate data for data loads and extracts.\n\u2022 Developing data analytical databases from complex financial source data.\n\u2022 Performing daily system checks.\n\u2022 Data entry, data auditing, creating data reports & monitoring all data for accuracy.\n\u2022 Designing, developing and implementing new functionality.\n\u2022 Monitoring the automated loading processes.\n\u2022 Advising on the suitability of methodologies and suggesting improvements.\n\u2022 Carrying out specified data processing and statistical techniques.\n\u2022 Supplying qualitative and quantitative data to colleagues & clients.\n\u2022 Using Informatica& SAS to extract, transform & load source data from transaction\nsystems.\n\u2022 Performed sequential analytics using SAS Enterprise miner using jobs fed by the SAS Grid Manager.\n\u2022 Loaded packages and stored procedures using Base SAS and integrated functional and business requirements using the EBI suite.\n\u2022 Creating data pipelines using big data technologies like Hadoop, spark etc.\n\u2022 Creating statistical models using distributed and standalone models to build various diagnostics, predictive and prescriptive solution.\n\u2022 Utilize a broad variety of statistical packages like SAS, R , MLIB, Graphs,Hadoop, Spark , MapReduce, Pig\n\u2022 Performed a check using quality parameters fed using the SAS QC engine.\n\u2022 Created a UI dashboard for end users and performed prototype testing using Tableau.\n\u2022 Refine and train models based on domain knowledge and customer business objectives\n\u2022 Deliver or collaborate on delivering effective visualizations to support the client business objectives\n\u2022 Communicate to your peers and managers promptly as and when required.\n\u2022 Produce solid and effective strategies based on accurate and meaningful data reports and analysis and/or keen observations.\n\u2022 Establish and maintain communication with clients and/or team members; understand needs, resolve issues, and meet expectations\n\u2022 Developed web applications using .net technologies; work on bug fixes/issues that arise in the production environment and resolve them at the earliest\n\nEnvironment:Cloudera, HDFS, Pig, Hive, Map Reduce, python, Sqoop, Storm, Kafka, LINUX, Hbase, Impala, Java, SQL, Cassandra, MongoDB, SVN."", u'Data Architect/Data Modeler\nInfotech Enterprises IT Services Pvt. Ltd - Hyderabad, Telangana\nDecember 2010 to August 2012\nDescription: Infotech Enterprises IT Services Pvt. Ltd. (Infotech IT), part of the $ 260 million Infotech Enterprises group, leverages its business process knowledge, technological competence, strategic alliances and strong global presence to offer innovative IT solutions to the Retail Industry.\n\nResponsibilities:\n\u2022 Worked with large amounts of structured and unstructured data.\n\u2022 Knowledge in Machine Learning concepts (Generalized Linear models, Regularization, Random Forest, Time Series models, etc.)\n\u2022 Worked in Business Intelligence tools and visualization tools such as Business Objects, Tableau, ChartIO, etc.\n\u2022 Deployed GUI pages by using JSP, JSTL, HTML, DHTML, XHTML, CSS, JavaScript, AJAX.\n\u2022 Configured the project on WebSphere 6.1 application servers\n\u2022 Implemented the online application by using Core Java, Jdbc, JSP, Servlets and EJB 1.1, Web Services, SOAP, WSDL.\n\u2022 Handled end-to-end project from data discovery to model deployment.\n\u2022 Monitoring the automated loading processes.\n\u2022 Communicated with other Health Care info by using Web Services with the help of SOAP, WSDLJAX-RPC\n\u2022 Used Singleton, factory design pattern, DAO Design Patterns based on the application requirements\n\u2022 Used SAX and DOM parsers to parse the raw XML documents\n\u2022 Used RAD as Development IDE for web applications.\n\u2022 Preparing and executing Unit test cases\n\u2022 Used Log4J logging framework to write Log messages with various levels.\n\u2022 Involved in fixing bugs and minor enhancements for the front-end modules.\n\u2022 Implemented Microsoft Visio and Rational Rose for designing the Use Case Diagrams, Class model, Sequence diagrams, and Activity diagrams for SDLC process of the application\n\u2022 Doing functional and technical reviews\n\u2022 Maintenance in the testing team for System testing/Integration/UAT.\n\u2022 Guaranteeing quality in the deliverables.\n\u2022 Conducted Design reviews and Technical reviews with other project stakeholders.\n\u2022 Was a part of the complete life cycle of the project from the requirements to the production support\n\u2022 Created test plan documents for all back-end database modules\n\u2022 Implemented the project in Linux environment.\n\nEnvironment: R 3.0, Erwin 9.5, Tableau 8.0, MDM, QlikView, MLLib, PL/SQL, HDFS, Teradata 14.1, JSON, HADOOP (HDFS), MapReduce, PIG, Spark, R Studio, MAHOUT, JAVA, HIVE, AWS.', u""Data Analyst/ Data Modeler\nGlobalLogic Technologies - Hyderabad, Telangana\nApril 2009 to November 2010\nDescription:GlobalLogic provides experience design, Digital Product Engineering Services, and Agile Software Development to the world's top brands by leveraging UX UI Design, next-gen technologies, and cloud software, with end-to-end solution by the best Software Development Company.\n\nResponsibilities:\n\u2022 Developed Internet traffic scoring platform for ad networks, advertisers and publishers (rule engine, site scoring, keyword scoring, lift measurement, linkage analysis).\n\u2022 Responsible for defining the key identifiers for each mapping/interface.\n\u2022 Clients include eBay, Click Forensics, Cars.com, Turn.com, Microsoft, and Looksmart.\n\u2022 Designed the architecture for one of the first analytics 3.0. online platforms: all-purpose scoring, with on-demand, SaaS, API services. Currently under implementation.\n\u2022 Web crawling and text mining techniques to score referral domains, generate keyword taxonomies, and assess commercial value of bid keywords.\n\u2022 Developed new hybrid statistical and data mining technique known as hidden decision trees and hidden forests.\n\u2022 Reverse engineering of keyword pricing algorithms in the context of pay-per-click arbitrage.\n\u2022 Implementation of Metadata Repository, Maintaining Data Quality, Data Cleanup procedures, Transformations, Data Standards, Data Governance program, Scripts, Stored Procedures, triggers and execution of test plans\n\u2022 Performed data quality in Talend Open Studio.\n\u2022 Coordinated meetings with vendors to define requirements and system interaction agreement documentation between client and vendor system.\n\u2022 Automated bidding for advertiser campaigns based either on keyword or category (run-of-site) bidding.\n\u2022 Creation of multimillion bid keyword lists using extensive web crawling. Identification of metrics to measure the quality of each list (yield or coverage, volume, and keyword average financial value).\n\u2022 Enterprise Metadata Library with any changes or updates.\n\u2022 Document data quality and traceability documents for each source interface.\n\u2022 Establish standards of procedures.\n\u2022 Generate weekly and monthly asset inventory reports.\n\nEnvironment: Erwin r7.0, SQL Server 2000/2005, Windows XP/NT/2000, Oracle 8i/9i, MS-DTS, UML, UAT, SQL Loader, OOD, OLTP, PL/SQL, MS Visio, Informatica.""]","[u""Bachelor's""]",[u'']
0,https://resumes.indeed.com/resume/141fe2dadc325880,"[u'Data Analyst Intern\nScottish Re - Charlotte, NC\nMarch 2018 to Present\n\u2022 Develop SSIS packages to populate data from various data sources.\n\u2022 Created SQL Jobs to schedule SSIS Packages.\n\u2022 Developed and deployed T-SQL codes, stored procedures & views.\n\u2022 Support actuarial model development, maintenance, management and deployment.\n\u2022 Assist with monthly and quarterly valuation reporting using SSRS.\n\u2022 Support experience study development and analysis.\n\u2022 Assist with modelling, valuation, and other ad hoc projects.\n\u2022 Increase automation, checks and documentation of processes.', u'Analyst Intern\nSawyer Studios - New York, NY\nAugust 2017 to December 2017\n\u2022 Built relational schemas and databases to store campaign performance data.\n\u2022 Designed and maintained an ETL process (SSIS) to gather campaign performance data.\n\u2022 Analyzing, synthesizing, and interpreting a range of data types and sources.\n\u2022 Prepare ad hoc, daily, weekly and monthly reports using SSRS to track KPIs and monitor performance.\n\u2022 Translates business requirements into optimized reports using T-SQL.', u""SQL Analyst\nCandour Machinery\nJuly 2014 to July 2015\nJan 2017- June 2017\n\u2022 Utilized key components of SQL Server (SSIS, SSAS, and SSRS).\n\u2022 Build, automate, and maintain data movements and flows from various data sources/types. (SSIS & SQL Agent Jobs)\n\u2022 Creation of ETL's to facilitate data movement (SSIS)\n\u2022 Query data bases to extract information needed to develop reports using SQL Server Reporting Services (SSRS)."", u'Data Integration Intern\nCandour Machinery PVT Ltd\nMarch 2014 to June 2014\n\u2022 Installed, Configured SQL Server Database Servers\n\u2022 Work as part of a project team to coordinate database development\n\u2022 Responsible for processing database integration from multiple file and database formats (Flat files, Excel, SQL, etc.).\n\u2022 SQL development and troubleshooting, including developing stored procedures and ETL processes, such as SSIS\n\u2022 Extracted, compiled and tracked data, and analyzed data to generate reports.\n\u2022 Develop reports using SQL Server Reporting Services (SSRS).\n\u2022 Used advanced Excel functions to generate spreadsheets and pivot tables.']","[u'Master of Science', u'Bachelor of Technology in Technology']","[u'New Jersey Institute of Technology\nJanuary 2015 to January 2016', u'Indus Institute of Technology and Engineering\nJanuary 2010 to January 2014']"
0,https://resumes.indeed.com/resume/249e1d6aae10b725,"[u""Data Analyst\nBeshton Software - San Jose, CA\nSeptember 2017 to Present\nCollected, cleaned and reconsolidated data across Teradata and Hadoop. Utilized clients' CRM\nsystems (Salesforce & SAP Business Object).\n\u25cf Developed machine learning models such as multi-linear regression models, r andom forest to do\nmarket segmentation researches on big data using Python, SQL and Teradata. Predicted future costs\nusing models of PCA, K-means, SVM in Spark and Python.\n\u25cf Supplied troubleshooting, analysis, and solutions for Beshton Software Oracle database."", u'Data Analyst\nAdvanced Clinical - San Francisco, CA\nJune 2017 to August 2017\nPerformed statistical modeling such as logistic regression and time series analysis in Hive/SAS to support drug development projects.\n\u25cf Worked with DBA to migrate scripts for stability using ETL and Spark framework.\n\u25cf Optimized SQL scripts and developed PL/SQL procedures to automate data loads in database\nplatform such as Oracle and Teradata; Dealt with 1 Million+ data. Estimated to save 1hr/day of manual work and reduced ~50% turn-around time of major changes.', u'Data Analyst\nMasswell Development Group Inc - New York, NY\nOctober 2014 to April 2016\nUsed Hadoop ecosystem(Hive, Spark) to perform big data analysis and used Tableau for visualization.\n\u25cf Designed and implemented distributed data processing pipelines using Spark, Hive, Python.\n\u25cf Successfully delivered BI & Analytics metrics, KPIs and reporting packages using Tableau.', u'Data Analyst Intern\nHSBC - Guangzhou, CN\nApril 2011 to May 2011\nGuangzhou, China\n\n\u25cf Conducted statistical analysis on customer and transactional data to determine risk to the company.\n\u25cf Forecasted operating costs and predicted market trends using SQL and Python on large dataset (over\n300k rows). Generated reports and ad-hoc analysis.']","[u'MS in Statistics', u'BA in Economics']","[u'Rice University Houston, TX\nAugust 2012 to May 2014', u'Guangdong University of Foreign Studies\nSeptember 2008 to June 2012']"
0,https://resumes.indeed.com/resume/7889922df3a5669d,"[u'Senior Business/Data Analyst\nWipro Technology Ltd\nJune 2012 to September 2016\nResponsibilities:\n* Building comprehensive Customer Profile that encompasses sales behavior,\ndemographics, Income Level and other key details\n* Analyze Point of Sale(POS) data to identify Dead stock and Top Selling Items\n* Measure Customer Loyalty Programs by looking at metrics such as number of purchases,\npurchase frequency, customer lifetime value etc\n* Research & Understand the data from different databases/campaigns/jobs by using\ncomplex SQL queries in Teradata server and going through the data model diagrams\n* Create and schedule ad hoc reports with which business models are made\n* Deliver key insights by uncovering root causes for trends in sales, gains, losses by transaction type, channel, campaign, product and over time\n* Create visually impactful dashboards in Excel and Tableau for data reporting by using\npivot tables and VLOOKUP\n* Analyzed 5,000+ contacts in a spreadsheet, successfully drawing conclusions about consumer data\n* Use of various Email metrics like Clickthrough rate (CTR), Conversion rate, Bounce rate\nto analyze offers sent through Emails\n* In-depth performance analysis of offers using the present and historical data\n* Add value in terms of Report automations (Excel VBA) and streamlining the project\nexecutions\n\nEnvironment/Technology: SAS, R, Teradata, SQL, MS Excel, VBA\nProject: TJ-MAX\nRole: Senior Business/Data Analyst\n\nResponsibilities:\n\u2022 Planning targeting strategy across various lines of business starting from Apparel to Home Electronics to Home Service\n\u2022 Drive efficiencies within the current reporting structure including enhanced\nautomation(VBA)\n\u2022 Create visually impactful dashboards in Excel and Tableau for data reporting by using\npivot tables and VLOOKUP\n\u2022 Extracted, interpreted and analyzed data to identify key metrics and transform raw data\ninto meaningful, actionable information\n\u2022 Extensive use of SQL Queries in Teradata server to extract data from client database\n\u2022 Work with management to prioritize business and information needs\n\u2022 Imports, connects, and manages metadata within the metadata repository including\ndata dictionary terms and definitions, business rules and constraints, and technical\nmetadata\n\u2022 Performed daily data queries and prepared reports on daily, weekly, monthly, and quarterly basis\n\u2022 Frequent use of Power Point to create impactful presentations\n\u2022 Documenting the processes for future needs\n\nEnvironment/Technology: R, Teradata, SQL, MS Excel, VBA\nProject: Best Buy\nRole: Business/Data Analyst\n\nResponsibilities:\n\u2022 Identify, analyze, and interpret trends or patterns in complex data sets\n\u2022 Performance analysis of various campaigns to generate business strategies and rules\n\u2022 Use of various metrics like coupon redemption rate, CTR, sales to measure campaign\neffectiveness\n\u2022 Troubleshoot and fix data issues on campaigns as and when needed\n\u2022 Mentor the team in technical, analytics, and job performance to improve their skills on larger projects\n\u2022 Analyze various offer channels to identify and increase the efficiency of offers\n\u2022 Continually explore opportunities to improve processes in the ongoing development of analytical tools and models, ad-hoc reports, dashboards and analysis\n\u2022 Prepare user guides and provide training on new / enhanced reporting solutions\n\u2022 Analyze data to determine if there are outliers or data accuracy issues and work with related departments to make the corrections\n\u2022 Liaise with onsite team and clients for resolving technical dependencies, issues, and risks\n\u2022 Identify gaps in current reporting process and drive the implementation of new controls\nand strategic solutions\n\u2022 Work with DBAs continuously to fine-tune in-efficient queries\n\nEnvironment/Technology: R, Teradata, SQL, MS Excel, Tableau\nProject: Web Analytics for International Airlines\nRole: Business/Data Analyst\n\nResponsibilities:\n* Extract the social media data of airlines from various platforms like Twitter, Facebook,\nTumblr etc. and analyze their social footprints\n* Analyze the broad geographical footprints of airlines across the globe to have a clear\npicture of opportunities and challenges\n* Gender analysis based on the positive/negative sentiments of gender towards the airlines\n* Effectively established digital presence for an International airline\n* Design a graph on event analysis that compares the historic data and the data at the time of event\n* Took overall responsibility for the delivery of work by the web team to standards,\nagreed timescales and to an acceptable quality\n* Tracked and reported key performance metrics, traffic behaviors and campaign\nperformance\n\nEnvironment/Technology: Alterian, SPSS']",[],[]
0,https://resumes.indeed.com/resume/b20515c37226d577,"[u'Data Analyst\nVCU Health - Richmond, VA\nFebruary 2017 to Present\nResponsibilities:\n\u2022 Monitor the Database for duplicate records.\n\u2022 Merge the duplicate records and ensure that the information is associated with company records.\n\u2022 Standardize company names, addresses, and ensure that necessary data fields are populated.\n\u2022 Review the database proactively t3 identify inconsistencies in the data, conduct research using internal and external sources to determine information is accurate.\n\u2022 Resolve the data issues by following up with the end user.\n\u2022 Coordinate activities and workflow with other data Stewards in the firm to ensure data changes are done effectively and efficiently\n\u2022 Review the database to identify and recommend adjustments and enhancements, including external systems and types of data that could add value to the system.\n\u2022 Extract the data from database and provide data analysis using SQL to the business user based on the requirements. Create pivots and charts in excel sheet to report data in the format requested\n\u2022 Used VBA for excel to automate the data entry forms to help standardize data\n\u2022 Assist CRM Analyst with Email Marketing Campaigns, including Client Publications, Newsletters, and announcements.\n\u2022 Assist other data Stewards with data Change Management (DCM) Inbox in resolving various tickets created by the User Change Request in Interaction Database.\n\u2022 Developed and Created Logical and Physical Database Architecture using ERWIN.\n\u2022 Designed STAR Schemas for the detailed data Marts and plan Data Marts involving Shared Dimensions.\n\u2022 Coordinated with different users in UAT process.\n\u2022 Conduct Design reviews with the business analysts and content developers to create a proof of concept for the reports.\n\u2022 Ensured the feasibility of the logical and physical design models.\n\u2022 Conducted the required GAP analysis between their AS-IS submission process and TO-BE Encounter Data Submission Process.\nEnvironment: MS Outlook, MS Project, MS Word, MS Excel, MS Visio, MS Access, Power MHS, Citrix, Clarity, MS SharePoint.', u'Data Analyst\nMerck & Co - Elkton, VA\nSeptember 2015 to November 2016\nResponsibilities:\n\u2022 Created new reports based on requirements. Responsible in Generating Weekly ad-hoc Reports\n\u2022 Planned, coordinated, and monitored project levels of performance and activities to ensure project completion in time.\n\u2022 Automated and scheduled recurring reporting processes using UNIX shell scripting and Teradata utilities such as MLOAD, BTEQ and Fast Load and Experience with Perl\n\u2022 Worked in a Scrum Agile process & Writing Stories with two week iterations delivering product for each iteration\n\u2022 Worked on transferring the Data files to vendor through sftp &Ftp process\n\u2022 Involved in defining and Constructing the customer to customer relationships based on Association to an account & customer\n\u2022 Created action filters, parameters and calculated sets for preparing dashboards and worksheets in Tableau.\n\u2022 Experience in performing Tableau administering by using tableau admin commands.\n\u2022 Worked with architects and, assisting in the development of current and target state enterprise level Data architectures\n\u2022 Worked with project team representatives to ensure that logical and physical Data models were developed in line with corporate standards and guidelines.\n\u2022 Involved in defining the source to target Data mappings, business rules and Data definitions.\n\u2022 Responsible for defining the key identifiers for each mapping/interface.\n\u2022 Performed Data analysis and Data profiling using complex SQL on various sources systems including Oracle and Teradata.\n\u2022 migrated three critical reporting systems to Business Objects and Web Intelligence on a Teradata platform\n\u2022 Created Excel charts and pivot tables for the Adhoc Data pull\nEnvironment: Teradata 13.1, Informatica 6.2.1, Ab Initio, Business Objects, Oracle 9i, PL/SQL, Microsoft Office Suite (Excel, Vlookup, Pivot, Access, Power Point), Visio, VBA, Micro Strategy, Tableau, UNIX Shell Scripting ERWIN.', u'Data Analyst\nBiglots - Leesburg, VA\nApril 2014 to July 2015\nResponsibilities:\n\u2022 Involved in Business and Data analysis during requirements gathering.\n\u2022 Assisted in creating fact and dimension table implementation in Star Schema model based on requirements.\n\u2022 Performed segmentation to extract Data and create lists to support direct marketing mailings and marketing mailing campaigns.\n\u2022 Defined Data requirements and elements used in XML transactions.\n\u2022 Reviewed and recommended database modifications\n\u2022 Analyzed and rectified d Data in source systems and Financial Data Warehouse databases.\n\u2022 Generated and reviewed reports to analyze Data using different excel formats\n\u2022 Documented requirements for numerous Adhoc reporting efforts\n\u2022 Troubleshooting, resolving and escalating Data related issues and validating Data to improve Data quality.\n\u2022 Involved in Regression, UAT and Integration testing\n\u2022 Designed developed and implemented 2 professionally finished systems for tracking IT requests, and providing a Data repository about reports. Documented all system functionality\n\u2022 Participated in testing of procedures and Data, utilizing PL/SQL, to ensure integrity and quality of Data in Data warehouse.\n\u2022 Metrics reporting, Data mining and trends in helpdesk environment using Access\n\u2022 Gather Data from Help Desk Ticketing System and write adhoc reports and, charts and graphs for analysis.\n\u2022 Compiled Data analysis, sampling, frequencies and stats using SAS.\n\u2022 Identify and report on various computer problems within the company to upper management\n\u2022 Report on trends that come up as to identify changes or trouble within the systems using Access and Crystal Reports.\n\u2022 Performed User Acceptance Testing (UAT) to ensure that proper functionality is implemented.\n\u2022 Guide, train and support teammates in testing processes, procedures, analysis and quality control of Data, utilizing past experience and training in Oracle, SQL, Unix and relational databases.\n\u2022 Maintained Excel workbooks, such as development of pivot tables, exporting Data from external SQL databases, producing reports and updating spreadsheet information.\n\u2022 Modified user profiles, which included changing users cost center location, changed users authority to grant monetary amounts to certain departments - monetary amounts were part of the overall budget amount granted per department\n\u2022 Extracted Data from DB2, COBOL Files and converted to Analytic SAS Datasets.\n\u2022 Deleted users from cost centers, deleted users authority to grant certain monetary amounts to certain departments, deleted certain cost centers and profit centers from database\n\u2022 Created a report, using SAP reporting feature that showed which users have not performed scanning of journal voucher documents into the system.\n\u2022 Created Excel pivot tables, which showed a table of users that, have not performed scanning of journal voucher documents. Users were able to find documents by double-clicking on his/her name within the pivot table\n\u2022 Load new or modified Data into back-end Oracle database.\n\u2022 Optimizing/Tuning several complex SQL queries for better performance and efficiency.\n\u2022 Created various PL/SQL stored procedures for dropping and recreating indexes on target tables.\n\u2022 Worked on issues with migration from development to testing.\n\u2022 Designed and developed UNIX shell scripts as part of the ETL process, automate the process of loading, pulling the Data\n\u2022 Validated cube and query Data from the reporting system back to the source system.\n\u2022 Tested analytical reports using Analysis Studio\nEnvironment: SAS/BASE, SAS/Access, SAS/Connect, Informatica Power Center (Power Center Designer, workflow manager, workflow monitor), SQL *Loader, Congas, Oracle, SQL Server 2000, Erwin, Windows 2000, TOAD', u'Data Analyst\nFremont Bank\nJanuary 2012 to March 2014\nCalifornia, CA\n\nResponsibilities:\n\u2022 Analyze the client Data and business terms from a Data quality and integrity perspective.\n\u2022 Perform root cause analysis on smaller self-contained Data analysis tasks that are related to assign Data processes.\n\u2022 Worked to ensure high levels of Data consistency between diverse source systems including flat files, XML and SQL Database.\n\u2022 Develop and run ad hoc Data queries from multiple database types to identify system of records, Data inconsistencies, and Data quality issues.\n\u2022 Involved in translating the business requirements into Data requirements across different systems.\n\u2022 Involved in understanding the customer needs with regards to Data, documenting requirements, developing complex SQL statements to extract the Data and packaging/encrypting Data for delivery to customers.\n\u2022 Participated in the development of Enhancement for the current Commercial and Mortgage Securities Association (CMSA) Application\n\u2022 Wrote SQL Stored Procedures and Views, and coordinate and perform in-depth testing of new and existing systems.\n\u2022 Manipulate and prepare Data, extract Data from database for business Analyst using SAS.\n\u2022 Provided support to Data Architect and Data Modeler in Designing and Implementing Databases for MDM using ERWIN Data Modeler Tool and MS Access.\n\u2022 Worked with Data Modeling team to create Logical/Physical models for Enterprise Data Warehouse.\n\u2022 Reviewed normalized/Renormalization schemas for effective and optimum performance tuning queries and Data validations in OLTP and OLAP environments.\n\u2022 Exploited power of Teradata to solve complex business problems by Data analysis on a large set of Data.', u'Data Analyst\nCovanta Energy - Morristown, NJ\nMay 2008 to December 2012']",[u''],[u'processes the Data using Informatica Power Center Metadata Exchange']
0,https://resumes.indeed.com/resume/aa31f0f0e67c6440,"[u'Data Analyst\nSynced Technology - San Jose, CA\nJuly 2016 to Present\n\u2022 Generated extensive industry analysis reports with independent thinking for thousands of subscribers\n\u2022 Extracted, cleaned and visualized data to support the viewpoints in the reports (Python, SQL, Excel)\n\u2022 Internal expert who educated colleagues in machine learning algorithms and AI concepts and trends', u'Data Analyst\nSennosen LLC - San Jose, CA\nOctober 2016 to February 2017\n\u2022 Organized and integrated the stock data by Python coding and generated valuable data matrix\n\u2022 Calculated data regression in Python, visualized and presented the results']",[u'Master of Science in Geophysics in Udacity'],"[u'University of Tulsa Tulsa, OK\nAugust 2014 to July 2016']"
0,https://resumes.indeed.com/resume/b31f8394a53ba2ab,"[u'Data Analyst Intern\nTriNet Group - Austin, TX\nMay 2017 to August 2017\n\u2022 Analyzed Payroll and Compensation data to depict pay rate, labor accruals and additional pay insights\n\u2022 Developed Census reports and presented key insights on attrition rate, performance trends to business partners\n\u2022 Created interactive dashboards with built in analytics, action filters, formatting and deployed to Tableau Server\n\u2022 Improved dashboards performance from 60 to 5 mins by developing custom SQL queries and refresh extracts\n\u2022 Generated quarterly, yearly summary analysis reports identifying KPIs which improved financial performance by 6%\n\u2022 Tested and diagnosed Reports by running advanced SQL queries to improve reporting accuracy and data consistency\n\u2022 Conducted POC and developed benchmarking dashboard by collecting, mining and analyzing market research data', u'Data Analyst\nCA Technologies - Hyderabad, Telangana\nJune 2013 to July 2016\nPerformed extraction, aggregation and cleaning of data collected from internal, external sources by developing SQL\nqueries to maintain data in primary relational database system by ensuring data consistency and quality\n\u2022 Provided analytical support for business, data management product teams by developing ad-hoc reports\n\u2022 Analyzed customers data and developed models for customer segmentation for future marketing of business\n\u2022 Created different Tables, Views, Triggers, Stored Procedures and other objects to perform database operations\n\u2022 Automated collection of storage, performance metrics of products and developed Pivot Charts for reporting\n\u2022 Presented products performance Reports to internal clients and customers which increased the sales by 10%\n\u2022 Presented on Data Management Tools to clients and stakeholders at CAWorld15 International Business Conference']","[u'M.S. in Management Information Systems', u'in Information Technology']","[u'The University of Texas at Dallas Dallas, TX\nMay 2018', u'Jawaharlal Nehru Technological University\nMay 2013']"
0,https://resumes.indeed.com/resume/ecd4a20c54557e8a,"[u""Data Modeler/ Data Analyst\nWells Fargo - Charlotte, NC\nOctober 2016 to Present\nWells Fargo Wholesale group has a strategic vision to provide customers with a payment experience that is consistent across the different payment channels. The desire is to build a new application system called global payments origination to have consistent global features as it relates to payment types, currencies, foreign exchange features and settlement features.\nResponsibilities:\n\u2022 Worked with the Business Owners, Business Analysts, Subject Matter Experts and external teams as a representative from the Technology team for backend database.\n\u2022 Responsible to create Meta data driven data model, development and documentation of Archival process for application database schemas.\n\u2022 Created logical data model from the conceptual model and it is conversion into the physical database design and generating data dictionary using Erwin.\n\u2022 Development and maintenance of application and reporting services views, functions, procedures, triggers and dynamic queries for better performance and to meet the SLA's.\n\u2022 Conducted data analysis on application views, functions and triggers, and tuned for performance, fixed missing/wrongly mapped data.\n\u2022 Rewrote the dynamic SQL queries in services to meet the SLA's (1-3 sec) and to avoid long waiting responses (1 min to 20+ min) from DB servers.\n\u2022 Customized Oracle fusion Middleware SOA Suite schemas by partitioning the tables/indexes and developed maintenance jobs to purge /drop partitions, shrink space and rebuild indexes.\n\u2022 Generated ad-hoc reports and POC analysis by joining multiple tables in teradata database.\n\u2022 Reviewed databases with DBA to best fit Indexes, SGA, undo space and shared pool memory sizes for better DB performance.\n\u2022 Split test the new code with different approaches for better performance (A/B Testing).\n\u2022 Writing object types, bulk collect and FORALL insert to load the data using the nested tables.\n\u2022 Resolved Priority Production Support Issues and defect fixes in PL/SQL code, which were mainly because of migrated data.\n\u2022 Created various procedures and functions to avoid multiple request calls from services.\n\u2022 Prepared and maintained different model ddl's for Production releases, keep track of DB changes in lower environments and prepared scripts for production issues on support calls.\n\u2022 Writing PL/SQL packages, procedures, functions and other database objects like sequences, objects, global temporary tables, ref cursors etc, to support custom report development\n\u2022 Extensively used the advanced features of PL/SQL like Records, Tables, Object types and dynamic SQL.\n\u2022 Enhancing the existing data model by adding new tables based on the business requirements, which will enhance in web services.\n\u2022 Designed and added new tables to the existing models GWA, GPO (payment & template) and migration as per business requirements."", u""Data Modeler/ Data Analyst\nPNC Bank, PA\nApril 2016 to September 2016\nPNC Payment optimizer is an automated B2B payment decision tool, which helps payers to determine an optimal payment type for each supplier at a given point in time.\n\nResponsibilities:\n\u2022 Involved in gathering business requirements and documented business requirements.\n\u2022 Actively participated in JAD sessions and meetings with the subject matter experts, stakeholders and other management team in the finalization of User Requirement Documentation.\n\u2022 Reverse engineering existing Teradata data warehouse and staging to conduct source data analysis to identify the relationship between different data source objects.\n\u2022 Conducted data profiling for better understanding of data sources.\n\u2022 Interpret data, analyze results using statistical techniques and provided ongoing reports using Tableau.\n\u2022 Acquire data from primary or secondary data sources and maintain data systems.\n\u2022 Identify, analyze, and interpret trends or patterns in complex data sets\n\u2022 Filter and cleanse data, and review reports, and performance indicators to locate and correct code problems\n\u2022 Worked closely with management to prioritize business and information needs\n\u2022 Locate and defined new process improvement opportunities\n\u2022 Created performance oriented Teradata views for data analysis.\n\u2022 Developed several reports, KPI's and parameterized dashboard for decision-making using Tableau."", u""Data Modeler/Data Analyst\nWells Fargo - San Francisco, CA\nOctober 2014 to March 2016\nWells Fargo Wholesale group has a strategic vision to provide customers with a payment experience that is consistent across the different payment channels. The desire is to build a new application system called global payments origination to have consistent global features as it relates to payment types, currencies, foreign exchange features and settlement features.\nResponsibilities:\n\u2022 Worked with the Business Owners, Business Analysts, Subject Matter Experts and external teams as a representative from the Technology team for backend database.\n\u2022 Responsible to create Meta data driven data model, development and documentation of Archival process for application database schemas.\n\u2022 Created logical data model from the conceptual model and it is conversion into the physical database design and generating data dictionary using Erwin.\n\u2022 Development and maintenance of application and reporting services views, functions, procedures, triggers and dynamic queries for better performance and to meet the SLA's.\n\u2022 Conducted data analysis on application views, functions and triggers, and tuned for performance, fixed missing/wrongly mapped data.\n\u2022 Rewrote the dynamic SQL queries in services to meet the SLA's (1-3 sec) and to avoid long waiting responses (1 min to 20+ min) from DB servers.\n\u2022 Customized Oracle fusion Middleware SOA Suite schemas by partitioning the tables/indexes and developed maintenance jobs to purge /drop partitions, shrink space and rebuild indexes.\n\u2022 Generated ad-hoc reports and POC analysis by joining multiple tables in teradata database.\n\u2022 Reviewed databases with DBA to best fit Indexes, SGA, undo space and shared pool memory sizes for better DB performance.\n\u2022 Split test the new code with different approaches for better performance (A/B Testing).\n\u2022 Writing object types, bulk collect and FORALL insert to load the data using the nested tables.\n\u2022 Resolved Priority Production Support Issues and defect fixes in PL/SQL code, which were mainly because of migrated data.\n\u2022 Created various procedures and functions to avoid multiple request calls from services.\n\u2022 Prepared and maintained different model ddl's for Production releases, keep track of DB changes in lower environments and prepared scripts for production issues on support calls.\n\u2022 Writing PL/SQL packages, procedures, functions and other database objects like sequences, objects, global temporary tables, ref cursors etc, to support custom report development\n\u2022 Extensively used the advanced features of PL/SQL like Records, Tables, Object types and dynamic SQL.\n\u2022 Enhancing the existing data model by adding new tables based on the business requirements, which will enhance in web services.\n\u2022 Designed and added new tables to the existing models GWA, GPO (payment & template) and migration as per business requirements."", u'Data Modeler/ Data Analyst\nWestern Union - Denver, CO\nAugust 2012 to September 2014\nWestern Union portfolio includes business solutions, consumer-to-consumer money transfer, bill-payment services and stored-value options such as prepaid cards. Online, by phone, from more than 100,000 ATMs and through more than 500,000 Agent locations worldwide, Western Union aims to move money anytime, anywhere and any way our customers choose, helping consumers and businesses grow. This system provides accurate and timely information on their portfolios on a daily and monthly basis, regarding credit gains and losses for potential candidates so that they may take appropriate actions proactively.\n\nResponsibilities:\n\u2022 Gathered the business requirements and create High-Level Requirements Documentation by conducting a series of meetings with business users.\n\u2022 Created logical data model from the conceptual model and it is conversion into the physical database design using Erwin.\n\u2022 Involved with data analysis using Teradata SQL assistant from data source.\n\u2022 Developed the data warehouse dimensional models & schema for the proposed central model for the project.\n\u2022 Developed many subject models from scrap.\n\u2022 Worked on the Snow-flaking the Dimensions to remove redundancy whenever required.\n\u2022 Ensured production data being replicated into the ODS and the Data Warehouse without any data anomalies from the processing databases.\n\u2022 Designed Fact & Dimension Tables, Conceptual, logical and Physical Data Models using Erwin tool.\n\u2022 Worked with Architecture team to get the metadata approved for the new data elements that are added for this project.\n\u2022 Involved using ETL tool informatica to populate the database, data transformation from the old database to the new database.\n\u2022 Maintained the integrity of data and data quality.\n\u2022 Conducted data profiling for better understanding of joins in ER models\n\u2022 Involved in high-level ETL design review sessions and assisted ETL developers in the detail design and development of ETL maps using Informatica.\n\u2022 Worked with project release meeting manager for UAT tables going to production.\n\u2022 Handled metadata publish tasks to our internal data dictionary from internal DB MS Access\n\u2022 Did Performance Tuning of the database that included creating indexes, optimizing SQL Statements & monitoring the server.\n\u2022 Assisted Reporting developers in building Reports using Crystal Reports.', u""Business Intelligence Consultant\nAMP, Melbourne, AUS\nOctober 2008 to June 2012\nAMP is Australia's largest retail and corporate superannuation provider, it also provides financial planning and advice, banking, life insurance, managed funds, property, listed assets and infrastructure.\nThe aim of the project is to design and build an OLAP data warehouse by Extract and Transform data from an OLTP sources and reload it to an OLAP destination. Primary responsibilities were focused on designing and performing ETL tasks using SSIS and generating reports and dashboard by using SSRS which would support and help the management team to engage in a variety of analysis in order to evaluate investments, market data, fixed income and derivates for the purpose of reporting, and decision support to improve client services.\n\nResponsibilities:\n\u2022 Involved in requirement gathering from Business Representatives.\n\u2022 Involved in data modeling (logical and physical model of databases), normalization and building referential integrity constraints\n\u2022 Involved in creating database objects like tables, views, procedures and triggers using T-SQL to provide definition, structure and to maintain data efficiently.\n\u2022 Designed complex SSIS packages using appropriate control and data flow elements with SSIS standards like event and error handling, logging and working with package configuration files.\n\u2022 Worked on import & export of data from one server to other servers using tools like, bulk insert copy, script task, data flow tasks and maintenance plan tasks in SSIS.\n\u2022 Performed the tasks of data conversions, lookups, adding derived columns, sending E-mails is done using various tasks in SSIS.\n\u2022 Involved in the creation and maintenance of Analysis Services objects such as cube, dimensions and analyzing the Performance of a Cube by partitioning it and creating perspectives.\n\u2022 Created KPI's, calculations, aggregations and actions in cubes.\n\u2022 Generated dashboard and different types of reports using Table, Chart and Gauge in SSRS.\n\u2022 Retrieved data by using MDX for cubes as a data source and setup the report service configuration manager and data driven subscriptions.\n\u2022 Performed quality and unit testing for SSIS packages.\n\u2022 Deployed reports to Web service and Share point Library.\n\u2022 Scheduled jobs, Alerts and backups using SQL Server Agent."", u'Software Developer\nSipsoft Logic Pvt Ltd - Hyderabad, Telangana\nSeptember 2007 to August 2008\nSIP SOFT LOGIC is a global diversified outsourced IT services and have successfully implemented strategic offshore programs in the areas of custom software development, software product development, game development and outsourced digital marketing production services to India.\nResponsibilities:\n\u2022 Development of physical data models and created DDL scripts to create database schema and database objects.\n\u2022 Created user requirement documents based on functional specification.\n\u2022 Created new tables, written stored procedures, triggers, views, functions.\n\u2022 Created SSIS Packages by using transformations like Derived Column, Sort, Lookup, Conditional Split, Merge Join, Union and Execute SQL Task to load into database.\n\u2022 Created SQL scripts for tuning.\n\u2022 Data Extracted from Flat files, Excel and Transformed as per the logic and loaded into Data warehouse.\n\u2022 Created packages to schedule the jobs for batch processing.\n\u2022 Involved in performance tuning to optimize SQL queries.\n\u2022 Created and Maintained Indexes for various fast and efficient reporting processes\n\u2022 Understanding the DLD specifications document and working with design to develop the mappings using SQL Server Integration Services (SSIS).']",[],[]
0,https://resumes.indeed.com/resume/9d351460ac53b4ba,"[u'Business Intelligence Analyst\nWestblue Consulting - Accra - Ghana\nJanuary 2016 to Present\nRequirements gathering for the development and deployment of the Cognos Analytics reporting solution based on user needs.\nImport and Export Data analysis to establish pattern and trends.\nMetadata modelling of data with the Cognos Analytics Framework Manager.\nPublishing of modelled package onto the Cognos Analytics interface.\nDesign and development of management reports on Cognos Analytics.\nSingle Window monthly project status report preparation.\nManagement of the Cognos Analytics software and Server.\nData validation and projection reporting for optimal performance.', u'Data/Policy Analyst (Intern)\nSheffield Futures Charity - Sheffield - United Kingdom\nMarch 2015 to November 2015\nRequirements gathering from key users for the new system.\nInterviewing key system users.\nValidating requirements with the implementation team.\nAssist in liaising with the new service provider as regards migration and stability.\nAnalyzing contract data and remuneration model.\nDelivery of a system with clear reporting structure and responsibilities.\nDelivery of a unique performance tracking and assessment criteria system.\nProcess delivery that caters for multi-contract remuneration model and compensation.\nSuccessful migration of employee data and records to a new provider.', u'Data Analyst\nSoft Alliance and Resources Limited - Lagos - Nigeria\nNovember 2012 to August 2014\nData gathering and conversion to usable format.\nData migration to the server.\nData validation with a proprietary system to sieve out duplication entries\nData modelling for onward migration to the application interface.\nRequirements validation with key stakeholders.\nSuccessful data migration and process creation which transformed the automated processes.\nTraining of key end users of the system for knowledge exchange and to take ownership.\nDelivery of an automated payroll and human resources system that surpassed expectations as regards efficiency and time for process execution.\nEnhanced process delivery systems to centrally run employee pay which eliminates ghost workers and saves cost.\nImplementation of a central payroll system that guarantee central payment of state employees.\nManagement of the payroll reporting system using the Oracle Bi Publisher software.\nConcise documentation of the process change.']","[u'Masters in Management Information Systems', u'Certificate in Project Managment', u'Degree in Pure and Applied Physics']","[u'The University of Sheffield Sheffield-UK\nSeptember 2014 to September 2015', u'Astute Trainers and Consultants Nigeria\nJuly 2013 to August 2013', u'Ladoke Akintola University of Technology Oyo State - Nigeria\nSeptember 2007 to September 2012']"
0,https://resumes.indeed.com/resume/34c86467dbb1e31c,"[u'Data Analyst\nLizhirun Trading Co., Ltd - Zhengzhou, CN\nJanuary 2017 to January 2018\n\u2022 Cleaned and Standardized company\u2019s data definition in R. Enhanced company\u2019s data collection, check data quality, and validation processes by exploring data from different departments helped business teams understand data collection processes.\n\u2022 Building and develop predictive and segmentation models to help determine the overall strategic outlook for the company.\n\u2022 Building data visualization reports through various sources to assist with delivering consultative insights within various aspects of business including best client product mix, location strategy, market trends, and sales forecasting.', u'Data Analyst Intern\nArticles of Incorporation of Kentwell Inc - San Jose, CA\nMarch 2015 to August 2015\n\u2022 Processed raw data and evaluated variables to prepare for running in models. Implemented statistical models and marketing mix models such as SLR, ANOVA, and Random forest in R and Python, evaluated the coefficients and parameters to optimize models.\n\u2022 Creating slides and spreadsheets to show results from models, summarizing information from graphs and data, offering suggestions to marketing activities, and communicated with consultants and clients to revise slides. Engaged in various analytics project on Marketing and Business strategy optimization.']","[u'Master of Art in Statistics', u'Bachelor of Science in Statistics']","[u'Columbia University, Graduate School of Arts and Sciences New York, NY\nDecember 2016', u'University of California Los Angeles, CA\nMarch 2015']"
0,https://resumes.indeed.com/resume/42d8f17ff5219090,"[u'Data Analyst\nUrbieta Oil Company - Medley, FL\nJune 2012 to Present\nPlease see resume', u'Customer Support\nSummit Aerospace\nJanuary 2003 to January 2010\nProvided outstanding service to customers: responded to inquiries, sent and processed quotes to customers\n\u2022 Followed-up on service order completion within a contractual time\n\u2022 Autonomously prepared and completed shipments to customers and vendors\n\u2022 Coordinated outside maintenance and repair service\n\u2022 Maintained service manual library up-to-date as per FAA requirements.\n\u2022 Compiled a monthly report of data and generated status for sales and marketing staff']",[u'BA in Health Services Admin in Microsoft Word'],"[u'Florida International University Miami, FL\nDecember 2017']"
0,https://resumes.indeed.com/resume/482452a44978bcea,"[u'Sr. Data Analyst\nPhysicians Endoscopy - Jamison, PA\nFebruary 2017 to Present\nResponsibilities:\n\u2022 Conducted Hypothesis Testing by using Null Hypothesis, Alternative Hypothesis, Significant Hypothesis, P-value, to analyze the behavior of patients on sample data.\n\u2022 Calculated random chance probability by taking different samples of data and analyzing them.\n\u2022 Calculated P-value to check if the sample data is behaving differently from the main data.\n\u2022 Retrieved list of comments, explored them and posted replies.\n\u2022 Used JSON methods to get response and get method of requests library.\n\u2022 Worked with audit department to help them asses the financial statement.\n\u2022 Generated income trend charts on revenue data using tableau.\n\u2022 Cleaned raw data on patient details using Python.\n\u2022 Conducted Cohort Analysis using MS SQL.\n\u2022 Used Csvkit library for data cleaning and exploring.\n\u2022 Calculated full suit of summary statistics using csvstat.\n\u2022 Used Rollup functions, quartile calculations, ranking functions for statistical analysis using MS SQL.\n\u2022 Extensive data cleansing and analysis, using pivot tables, formulas (v-lookup and others), data validation, conditional formatting using Excel.\n\u2022 Created detailed documentation.\nEnvironment: Python, PyCharm, R, Rstudio, Microsoft Office Suite, including Excel 2016, Tableau desktop, Windows XP/ 10, Microsoft SQL Server 2012, ASP.NET, VB.NET, SSRS', u'Data Analyst\nAppShark - Iselin, NJ\nJanuary 2016 to January 2017\nResponsibilities:\n\u2022 Worked with APIs to query and retrieve data dynamically.\n\u2022 Retrieved list of comments, explored them and posted replies.\n\u2022 Used JSON methods to get response and get method of requests library.\n\u2022 Made API requests, authenticate with an API server, and parse responses.\n\u2022 Calculated Marginal Frequency of applications that are downloaded\n\u2022 Converted the observations to proportional tables and analyzed data using Pearson Chi Square Test.\n\u2022 Used T-test to compare means for distinct groups to set up the hypothesis test for low sample size.\n\u2022 Performed Z-test based on Z-Statistic to test the effect of fitness apps on people.\n\u2022 Performed statistical analysis using R-programming.\n\u2022 Calculated p-values to check if the data sample has different behavior to main data.\n\u2022 Developed Worksheets and Dashboards using Tableau\n\u2022 Using Excel pivot tables to manipulate substantial amounts of data to perform data analysis\nEnvironment: Python, PyCharm, R, Rstudio, Microsoft Office Suite, including Excel 2013, Tableau desktop, Microsoft SQL Server 2012, Windows XP/7, Jupyter Notebook', u""Data Analyst\nInfosys Limited - Hyderabad, Telangana\nFebruary 2014 to December 2015\nResponsibilities:\n\u2022 Worked with Jupyter console to quickly explore datasets, run files and do basic analysis.\n\u2022 Used Csvkit library to clean, munge and explore csv files.\n\u2022 Calculate summary statistics on subsets of a table in SQL\n\u2022 Queried SQLite from Python using sqlite3 module, explored outliers, recorded the observations.\n\u2022 Compile and validate data; reinforce and maintain compliance with corporate standards.\n\u2022 Write Scripts to extract required data in SQL, constructing complex queries\n\u2022 Working with managing leadership to prioritize business and information requirements.\n\u2022 Performed Customer acquisition analysis on different clients\n\u2022 Building, publishing customized interactive reports and dashboards, report scheduling using Tableau server.\n\u2022 Created action filters, parameters and calculated sets for preparing dashboards and worksheets in Tableau.\n\u2022 Developed Tableau workbooks from multiple data sources using Data Blending.\n\u2022 Collected data from a survey and performed forecast analysis for retention of customers.\n\u2022 Performed daily data queries and prepared reports on daily, weekly, monthly, and quarterly basis.\n\u2022 Performed extensive GAP analysis in the project as there were numerous 'As-Is' and 'To-Be' conditions.\nEnvironment: Bash shell scripting, SQL Server Management Studio, Tableau Desktop, Python, PyCharm, SQLite (sqlite3 Python module), Microsoft Office Suite, including Excel 2013, Windows 7 and Linux (Ubuntu 14.04) Operating systems"", u'BI Developer\nDCS Labs - Hyderabad, Telangana\nJuly 2013 to February 2014\nResponsibilities:\n\u2022 Extracted, compiled and tracked data, and analyzed data to generate reports.\n\u2022 Worked on end-to-end data analysis project by acquiring the raw data, handling files with different formats and columns\n\u2022 Analyzed, joined (by using concat function in python to combine dataframes), filtered, created padded columns to the dataframes\n\u2022 Filtered the data to remove the unnecessary columns which will make it easier find correlations within it.\n\u2022 Created calculated columns by combining related columns for quantitative analysis.\n\u2022 Used regular expressions (re module in Python) and text processing to extract coordinates from a string\n\u2022 Parsed the latitude and longitude coordinates which enabled us to map and uncover any geographic patterns in the data.\n\u2022 Involved in data cleaning, handled missing values with various python methods and merged data sets in python\n\u2022 Calculated Correlations by finding r-values to know how closely two columns are related.\n\u2022 Worked with other team members to complete project and achieve project deadlines.\n\u2022 Created action filters, parameters and calculated sets for preparing dashboards and worksheets in Tableau.\nEnvironment: Python, IDE: PyCharm, Tableau Desktop, Windows XP, Microsoft Office Suite, including Excel 2013', u'Programmer Analyst\nInfotech Solutions - Hyderabad, Telangana\nFebruary 2012 to June 2013\nResponsibilities:\n\u2022 Involved in gathering analyzing and documenting business requirements functional requirements and data specifications.\n\u2022 Worked with business analyst to understand business requirements and created functional specifications documents along with mapping documents.\n\u2022 Understanding the warehouse Start Schema Model and wrote SQL queries joining multiple dimension and fact tables.\n\u2022 Extensive data cleansing and analysis, using pivot tables, formulas (v-lookup and others), data validation, conditional formatting\n\u2022 Created custom territories via groups in Tableau.\n\u2022 Created Database Objects - Tables, Indexes, Views, Stored Procedures and User defined functions according to the requirements of the project.\n\u2022 Analyzing the data to assess the financial statement of organization.\nEnvironment: Microsoft SQL Server 2008 Enterprise Edition, Tableau Server, Microsoft Office Suite including Excel, Python 2.6']","[u""Bachelor's in Computer Science in Computer Science""]","[u'Jawaharlal Technological University Hyderabad, Telangana']"
0,https://resumes.indeed.com/resume/e6990293beb1dff4,"[u'Data Analyst\nJayaram Technologies - Hyderabad, Telangana\nJune 2015 to December 2015\nWorked as an ETL developer and as a System Engineer for Business Intelligence Team.\n\u2022 Extracted data from Flat files, MS SQL Server, Oracle to build an Operational Data Source and applied business logic to load data into Global Data Warehouse.\n\u2022 Worked on database SQL tuning and performance optimizations.\n\u2022 Generating of DB scripts from Data modeling tool and Creation of physical tables in DB.\n\u2022 Creating UNIX shell scripts for database connectivity and executing queries in parallel job execution.\n\u2022 Implementation of Data Warehousing (Relational, Dimensional, & OLAP) modeling, schemas (Star & Snowflake), logical data marts and table structures (fact & dimension).\n\u2022 Involved in analysis of the user requirement, identifying the sources and designing models.', u""Business Analyst\nState Bank of India (SBI)\nMay 2014 to March 2015\n* Reviewed, drafted, and outlined business requirements that impact strategic initiatives and/or business unit objectives that sanctioned loans\n* Assisted in the development and documentation of business requirements and business solutions to solve problems and issues related to business operations\n* Examined various data sources and applied consistent approaches to data interpretation and performance measurement results.\n* Coordinated with business partners and users, assists to ensure that critical strategic decisions are vetted and well thought out prior to implementation\n\nPraja Vaidhya Saala Khammam, Telangana\nIntern in Summer as Clinical Analyst.\n* Developed and delivered Education and training for new Employee Orientation to support competency of new and existing users in Health System's Clinical information systems\n* Facilitated the use of information technology by physicians, nurses and other medical practitioners across the care continuum\n* Improved efficiency and care transitions by sharing information across the continuum of care\n* Made knowledge and data actionable which led to improved patient outcomes""]","[u'in Database Design for IMDB', u'Bachelor of Pharmacy in Chemical', u'Masters of Science in Computer Information']","[u'New England College Henniker, NH\nDecember 2016', u'Jawaharlal Nehru Technological University Hyderabad, Telangana\nMay 2015', u'New England College Henniker, NH']"
0,https://resumes.indeed.com/resume/0321201bb5f55eee,"[u""Data Analyst Intern\nANHUI XINGQITIAN NETWORK TECH - Hefei, CN\nSeptember 2016 to December 2016\n\u2022 Used Excel and R to extract and analyze data of financial clients' consumer behavior and payment history and graded credit scores using FICO scoring model.\n\u2022 Developed data analysis report, enabling department to determine risk control scheme and supporting partner banks in efforts to minimize lending risk."", u'Data Analyst Intern\nSHANGHAI WONDERTECH SOFTWARE - Shanghai, CN\nJune 2016 to August 2016\n\u2022 Participated in the Digital Marketing Analysis Program for China Mobile Communications Corporation, used SQL and Python for data collection and data visualization of the consumers\u2019 data from mobile apps and analyzed customers\u2019 behaviors to understand their preferences for certain in-app functions.\n\u2022 Presented data analysis report, assisting the partner company to develop better marketing and advertising schemes.']","[u'Master of Science in Business Analytics', u'Bachelor of Science in Applied Statistics']","[u'GEORGE WASHINGTON UNIVERSITY, School of Business Washington, DC\nSeptember 2017 to December 2018', u'ANHUI UNIVERSITY, School of Mathematical Sciences Hefei, CN\nSeptember 2013 to June 2017']"
0,https://resumes.indeed.com/resume/4a7fd1aad5e2e5eb,"[u'Data Analyst\nMARTA\nMarch 2016 to Present\n\uf0a7 Develops database processes and procedures in coordination with safety data source departments and the Department of Technology.\n\uf0a7 Establishes requirements for and conducts analyses of centralized databases of safety data. Performs analysis of data contained in various data systems to identify potential factors that may lead to accidents or incidents.\n\uf0a7 Establishes statistical criteria in conjunction with industry standards to evaluate and complete in-depth risk assessment of events. Coordinates the use of data with internal and external stakeholders to support and facilitate analysis.\n\uf0a7 Develops metrics and applies analytical tools to analyze data to identify trends and assess known risks.\n\uf0a7 Provides comprehensive data quality verification through the reviews, classification, and coding of bus, rail, mobility, non-revenue, and police incident reports for compliance with the Federal Transit Administration National Transit Database requirements.\n\uf0a7 Coordinates and facilitates monthly on-line reporting to the Federal Transit Administration National Transit Database to ensure the Authority\u2019s compliance with FTA requirements.\n\uf0a7 Performs other related duties as assigned.', u'Inventory Analyst\nMcCormick NA - Duluth, GA\nOctober 2014 to November 2015\nResponsibilities\n\uf0a7 Identified and recommended solutions for any existing inventory issues\n\uf0a7 Maintained all warehouse stock, by verifying, completing and submitting weekly stock reports, daily orders, monthly sales reports, shipping logs, purchase/sales orders, and other various reports\n\uf0a7 Tracked, maintained, prepared, and reported all metrics and trending behavior to management\n\uf0a7 Answered all inquiries, tracking, and questions to external/internal clientele\n\uf0a7 When necessary, escalated and expedited existing orders\n\uf0a7 Provided communication and solutions with international subsidiaries including Korea, Italy and Canada\n\uf0a7 Ran queries; submitted various forms of reporting to management, team, and internal clientele. Ad hoc reporting', u'Data Analyst\nPhoenix Clerical Unlimited - Atlanta, GA\nNovember 2013 to October 2014\nProvided reporting and metrics solutions to all levels of management using Excel/In-house platforms\n* Data gathering and information analysis from within the company, competitive companies, industry organizations, outside resources, etc. upon which intelligent strategic Business Development decisions were made\n* Identify any trending behavior. Research data integrity issues\n* Developed and delivered any additional progress reports, proposals, requirements documentation, and presentations using MS Excel/MS PowerPoint\n* Maintained and updated existing records into databases using MS Access/MS Excel\n* Billing and ad hoc reporting using MS Excel/MS SharePoint', u'Data Analyst\nCoca Cola Refreshments\nJuly 2013 to October 2013\nContractor)\n\n* Collected large amounts of affiliated raw data from various sources and databases including MS Sharepoint and Excel\n\n* Using MS Excel, presented findings using pivot tables and v-lookups; updated existing and generated new metrics\n\n* Audit accessible and create new statistics and present findings to team and upper management\n\n* Proofread and identified any trending/patterns and submitted findings to supporting staff\n\n* Ran queries; submitted various forms of reporting to management, team, and internal clientele. Ad hoc reporting\n\n* Tracked, logged, and updated any shipment of equipment pertinent to the project\n\n* Performed weekly/daily ETL transactions utilizing databases and MS Office Suite', u'Data Analyst\nLexxis Transportation\nJune 2012 to July 2013\nContractor)\n* Extracted data from various databases, interpreted data and based on findings, developed recommendations\n* Monitored and reported all forms of trending analysis to additional departments, team leads, upper management\n* Tracked, edited, and monitored daily/weekly reports using Business Objects, pivoting out results in MS Excel\n* Maintained, updated, and ran queries, proofread all data insuring accuracy\n* Prepared and restructured existing presentations pertinent to the business to upper management\n* Conducted research to identify any problematic areas, identified and presented corrective action\n* Provided necessary feedback to QA, developers, and help desk when needed. Ad hoc reporting and billing', u'Data Analyst\nAT&T\nOctober 2011 to May 2012\nContractor)\n* This position was responsible for data mining, writing SQL, running queries, and performing complex and confidential reporting to ensure operations flow smoothly\n* Followed marketing trending and provided significant feedback to upper management\n* Ensured all daily reports were successfully run and delivered to internal/external clientele as well as ad hoc reporting using Business Objects and MS SharePoint\n* Participated in the upgrade of Business Objects platform from, including testing and reconstructing all reports during the migration\n* Ran queries, prepared data, scrubbed and validated data by performing various duties\n* Proofread, edited, sorted, and distributed thousands of rows of data\n* Created, extracted, and maintained queries and reports using the Business Objects web interface\n* Accessed data stored in Oracle database data warehouse to fulfill new reporting requests (and maintain/modify existing ones) through Business Objects web interface\n* Provided stakeholders with decision-quality information to investigate issues, identify root causes of problems, and explore new opportunities without relying on IT to create additional reports\n* Constructed reports that were geographically sensitive to process trends and performance using Business Objects and SQL server\n* Monitored database systems to ensure data integrity', u'Business Analyst/Data Analyst\nPhoenix Clerical Unlimited - Atlanta, GA\nNovember 2010 to October 2011\nProvided reports and metrics to all levels of management using Oracle/Crystal Reports/Excel\n* For the Department of Human Services, sorted through thousands of rows of data, checking for any existing/new discrepancies\n* Determined efficient utilization of resources by analyzing data procured through data mining\n* Provided data gathering and information analysis from within the company, competitive companies, industry organizations, outside resources, etc. upon which intelligent strategic Business Development decisions can be made\n* Identified and researched data integrity issues\n* Worked with business partners to identify project business value and associated metrics\n* Provided relevant feedback and information to QA\n* Developed and delivered any additional progress reports, proposals, requirements documentation, and presentations', u'Data Analyst\nAT&T\nMay 2010 to November 2010\nPerformed marketing as well as statistical research and analysis based on figures and projections pertinent to the existing project using MS SharePoint\n* Created custom reports for upper management as well as updated all metrics using Crystal Reports/Excel\n* Received inbound calls in order to reconcile and consolidate live inventoried data obtained from technicians via CATS\n* Placed outbound calls as well as email correspondence with all levels of management in order to obtain essential data\n* Made necessary changes and updates in existing databases (CATS & SAD database)\n* Assisted in the preparation and distribution of training materials pertinent to Zodiac acquisition using MS Project\n* Entered and verified accessible detailing data using SAD database\n* Reconciliation of various support center statistics and reporting using MS Excel\n* Provided miscellaneous clerical, data entry, and customer service duties as assigned in order to assist the support center', u'Data Analyst/Business Intelligence Support\nAccess Insurance\nFebruary 2008 to November 2009\nRebuilt all reports after successful migration of Business Objects release\n* Reviewed/proofread existing reassurance contracts, verified no existing discrepancies in coinsurance claims\n* Examined policies by ceding companies within the organization\n* Performed all support functions associated with the collection, quality checking, and trending of data that is used for measuring the effectiveness of the IT department\n* Responsible for creating daily/weekly/monthly reports and metrics that measure success/failure ratio of production reports as well as monitoring database backups using Business Objects/Crystal Reports, MS Sharepoint and Web trending software\n* Provided relevant feedback and information to QA\n* Monitored IT infrastructure metrics, helpdesk call metrics, scheduled database processing, prepared and scheduled production reports for the CIO/Underwriting/Finance/Marketing\n* Administered, organized, modified as well as troubleshoot reports using Crystal Reports XI and Infoview\n* Maintained accurate daily and monthly metrics for the Data Services department, including production uptimes, server/disk utilization, inventory, and help desk calls\n* Used software applications, such as Access, Excel and PowerPoint to assemble, manipulate, and format data for the creation of databases and reports\n* Processed and extracted data for a third party system and resubmit it updating agent data using SQL IVANS\n* Verified data integrity by executing stored procedures in SQL server to authenticate statistics']",[u'Bachelor of Science in Business Information Systems'],"[u'DeVry University Atlanta, GA\nJanuary 2005']"
0,https://resumes.indeed.com/resume/3da11a9996ba59a2,"[u'Data Analyst\nBusseto Foods - Fresno, CA\nMay 2014 to Present\n- Maintained and created Various Spreadsheets to track Daily, Weekly and Monthly Data trends\n- Coordinated with production planner to estimate production totals for incoming orders\n- Organized and filed paperwork.\n- Helped track down information for food recalls as needed\n- Helped send invoices via ADX Client Portal\n- Maintained Inventory and provided Cycle counts.', u'Administrative Assistant\nElectronic Recyclers - Fresno, CA\nJanuary 2008 to January 2014\n- Generate BOL, and Other shipping documents\n- Review time cards and correct when needed using ADP and Paylocity\n- Scanned and Electronically Filed paperwork\n- Maintained and created various spreadsheets to track data.\n- Helped unload and load trucks when needed\n- Assist in Online sales for refurbished electronics.']",[u'High School Diploma'],"[u'Sanger High School Sanger, CA']"
0,https://resumes.indeed.com/resume/e98d68331808ac5e,"[u'Representative\nAlexandria, VA\nJanuary 2011 to Present\n\u2022 Participated in meetings and presentations on environmental and recreation issues\n\u2022 Took part in the Engleside community initiatives\n\n\u2022 Voted on resolutions and their implementation\n\n\u2022 Participated in the writing of proposals', u'TekPro Services - Arlington, VA\nJanuary 2016 to January 2017', u'Exceed Corporation - Arlington, VA\nJanuary 2009 to January 2016', u'Data Analyst\nNW Systems Inc - Arlington, VA\nJanuary 2005 to January 2009', u'Data Analyst\nCHM High Tech Computer Management - Arlington, VA\nJanuary 2001 to January 2005\nData Analyst Metrica Inc Arlington, VA', u""ESL Tutor\nESL studentsK\nJanuary 1975 to January 1976\n\u2022 Taught 1 - 2 hr group classes to adult ESL learners\n\n\u2022 Taught 1 to 1 1/5 hr classes to grade 4 ESL studentsK\n\n\u2022 Prepared detailed lesson plans\n\n\u2022 Adapted lessons to students' learning styles\n\n\u2022 Evaluated academic performance\n\n\u2022 Prepared students for exams and tests""]","[u'Certificate in Events Planning Course', u'in computer programming', u""Master's in Literature and Linguistics"", u'']","[u'Trendimi Academy\nJanuary 2017', u'U.S. Department of Agriculture Graduate School Washington, DC\nJanuary 1980', u'University of Wroclaw Wroclaw, PL\nJanuary 1978', u'University of Dundee Dundee\nJanuary 1975']"
0,https://resumes.indeed.com/resume/e8d47d05ce49864a,"[u'Data Analyst\nSyracuse University Graduate Enrollment Services - Syracuse, NY\nAugust 2017 to Present\n\u2022 Clean, transform and visualize student-enrollment data using R, to analyze trends and patterns in student drop-out rate\n\u2022 Develop and validate K-means clustering model in R, to identify target student segment for attracting top talent globally\n\u2022 Proposed technology-driven solutions to improve system efficiencies by 12% and reduce total expenses by 14%', u'Business Intelligence Analyst\nSoftpro India - Lucknow, Uttar Pradesh\nJuly 2016 to June 2017\n\u2022 Successfully interpreted data to draw conclusion for managerial action and strategy\n\u2022 Designed executive dashboards using Tableau to facilitate business users visualize and monitor KPIs across 5 business units\n\u2022 Performed statistical analysis on global sales data and presented reports to the C-Suite for taking informed business decisions']","[u'Master of Science in Information Management in Business Analytics', u'Bachelor of Science in Computer Science and Engineering in Database Management System']","[u'Syracuse University Syracuse, NY\nMay 2019', u'AK Technical University\nJune 2016']"
0,https://resumes.indeed.com/resume/5e005634f4eccc3c,"[u'Sr. Data Modeler/DATA ANALYST\nTHE VANGUARD GROUP INC - Malvern, PA\nOctober 2015 to Present\nResponsibilities:\n\u27a2 Created conceptual, logical and physical models based on requirements gathered through interviews with the business users.\n\u27a2 Updated existing models to integrate new functionality into an existing application. Conducted one-on-one sessions with business users to gather warehouse requirements.\n\u27a2 Analyzed database requirements in detail with the project stakeholders by conducting joint Requirement Development sessions.\n\u27a2 Developed normalized Logical and Physical database models to design OLTP system\nCreated dimensional model for the reporting system by identifying required dimensions and facts using Erwin.\n\u27a2 Created DDL scripts for implementing Data Modeling changes. Created ERWIN reports in HTML, RTF format depending upon the requirement, Published Data model in model mart, created naming convention files, co-coordinated with DBAs to apply the data model changes\n\u27a2 Used forward engineering to create a Physical Data Model with DDL that best suits the requirements from the Logical Data Model.\n\u27a2 Maintaining and implementing Data Models for Enterprise Data Warehouse using ERWIN.\n\u27a2 Create and maintain Metadata, including table, column definitions.\n\u27a2 Worked with Database Administrators, Business Analysts and Content Developers to conduct design reviews and validate the developed models.\n\u27a2 Responsible for defining the naming standards for Data warehouse.\n\u27a2 Possess strong Documentation skills and knowledge sharing among Team, conducted Data modeling review sessions for different user groups, participated in sessions to identify requirement feasibility.\n\u27a2 Extensive experience in PL SQL programming Stored Procedures, Functions, Packages and Triggers\n\u27a2 Used Model Mart of Erwin for effective model management of sharing, dividing and reusing model information and design for productivity improvement.\n\u27a2 Massaged the existing model to create new logical and physical models that formed the basis for the new application.\n\u27a2 Eliminated errors in ERwin models through the implementation of Model Mart (a companion tool to ERwin that controls the versioning of models).\n\u27a2 Used Erwin for reverse engineering to connect to existing database and ODS to create graphical representation in the form of Entity Relationships and elicit more information\nDeveloped Data Migration and Cleansing rules for the Integration Architecture (OLTP, ODS, DW).\n\u27a2 Identified the most appropriate data sources based on an understanding of corporate data thus providing a higher level of consistency in reports being used by various levels of management.\n\u27a2 Verified that the correct authoritative sources were being used and that the extract, transform and load (ETL) routines would not compromise the integrity of the source data.\n\nEnvironment: Erwin r8, Windows XP NT 2000, SQL Server 2008, Teradata, Oracle11g, DB2, Informix, MS Excel, Mainframes MS Visio, Rational Rose, Requisite Pro.', u""DATA MODELER / DATA ANALYST\nTOYOTA - Torrance, CA\nNovember 2012 to August 2015\nResponsibilities:\n\u27a2 Worked with Business users for requirements gathering, business analysis and project coordination.\n\u27a2 Developed a Conceptual Model and Logical Model using Erwin based on requirements analysis.\n\u27a2 Created various Physical Data Models based on discussions with DBAs and ETL developers.\n\u27a2 Worked on data mapping process from source system to target system.\n\u27a2 Created dimensional model for the reporting system by identifying required facts and dimensions using Erwin.\n\u27a2 Extensively used Star and Snowflake Schema methodologies.\n\u27a2 Developed and maintained data Dictionary to create Metadata Reports for technical and business purpose.\n\u27a2 Worked on Performance Tuning of the database which includes indexes, optimizing SQL Statements.\n\u27a2 Used Model Mart of Erwin for effective model management of sharing, dividing and reusing model information and design for productivity improvement.\n\u27a2 Implemented Forward engineering to create tables, views and SQL scripts and mapping documents.\n\u27a2 Prepared data dictionaries and Source-Target Mapping documents to ease the ETL process and user's understanding of the data warehouse objects\n\u27a2 Translated business concepts into XML vocabularies by designing XML Schemas with UML.\n\u27a2 Used Normalization (1NF, 2NF & 3NF) and Denormalization techniques for effective performance in OLTP and OLAP systems.\n\u27a2 Good understanding and experience with the entire data Migration process from analyzing the existing data, cleansing, validating, translating tables, converting and subsequent upload into new platform.\n\u27a2 Created documentation and test cases, worked with users for new module enhancements and testing.\n\u27a2 Worked with business analyst to design weekly reports using combination of Crystal Reports.\n\u27a2 Understood existing data model and documented suspected design affecting the performance of the system.\n\u27a2 Extracted data from databases like Oracle, SQL server and DB2 using Informatica to load it into a single repository for data analysis.\n\u27a2 Involved in development and implementation of SSIS, SSRS and SSAS application solutions for various business units across the organization.\n\u27a2 Experienced in migration and cleansing rules for the integrated architecture (OLTP, ODS, DW)\n\u27a2 Wrote DDL and DML statements for creating, altering tables and converting characters into numeric values.\n\nEnvironment: Oracle SQL Developer, Oracle Data Modeler, Teradata, SSIS, Business Objects, Teradata, Oracle12c, SQL server 2012, SQL Assistant 13.11, data stage 8.1, DB2, Informatica Power Center 9.5."", u""DATA MODELER/DATA ANALYST\nL.A. CARE - Los Angeles, CA\nSeptember 2010 to June 2012\nResponsibilities:\n\u27a2 Implemented dimension model (logical and physical data modeling) in the existing architecture using ER Studio.\n\u27a2 Created ER diagrams using Open Sphere Model and implemented concepts like Star-Schema Modeling, Snowflake Schema Modeling, Fact and Dimension tables.\n\u27a2 Designed and Developed Oracle PL/SQL Procedures LINUXand UNIX Shell Scripts for data Import/Export and data Conversions.\n\u27a2 Designed the ER diagrams, logical model (relationship, cardinality, attributes, and, candidate keys).\n\u27a2 Written Procedures and Functions using Dynamic SQL and written complex SQL queries using joins, sub queries and correlated sub queries.\n\u27a2 Generated DDL statements for the creation of new ER/studio objects like table, views, indexes, packages and stored procedures.\n\u27a2 Experience in designing a canonical model\n\u27a2 Generate DDL scripts for database modification, Macros, Views and set tables.\n\u27a2 Developed logical and physical data models and mapping logical entities to the enterpriseData Model.\n\u27a2 Designed and developed star schema model for target database using ER/Studio.\n\u27a2 Extensively worked in SQL, PL/SQL, SQL Plus, SQL Loader, Query performance tuning, DDL scripts, database objects like Tables, Views Indexes, Synonyms and Sequences.\n\u27a2 Handled multiple projects simultaneously and manage the projects to accomplish desired results.\n\u27a2 Applied appropriate level of abstraction in designs and confirmed that data designs support the integration of data and information flow across systems and platforms.\n\u27a2 Analyzed and made recommendations on the impact of designs as they related to common conceptual models.\n\u27a2 Updated and enforced data architecture policies, standards and procedures.\n\u27a2 Provided data SME knowledge to both IT and business users to promote a common understanding and the appropriate usage of the bank's data assets.\n\u27a2 Worked in generating and documenting Metadata while designing OLTP and OLAP systems environment.\n\u27a2 Ensured data objects adhere to defined standards and best practices.\n\u27a2 data Promoted data models into production metadata environments.\n\u27a2 Worked with both relational and dimensional data modeling, data standards and data model practices.\n\u27a2 Performed maintenance of current data diagrams and metadata to enable further enhancements.\n\u27a2 Provided Level of Efforts, activity status, identified dependencies impacting deliverables, identified risks and mitigation plans, identified issues and impacts.\n\u27a2 Designed database solution for applications, including all required database design components and artifacts.\n\nEnvironment: Data modeler, SSAS, T-SQL, SQL Server, LINUX, MDM, PL/SQL, ER/Studio, ETL, Normalization and De-normalization, Metadata, Star-Schema Modeling, UNIX, Snowflake Schema Modeling etc."", u'DATA ANALYST\nONE TECHNOLOGIES - Hyderabad, Telangana\nJune 2009 to August 2010\nResponsibilities:\n\u27a2 Involved in gathering user requirements along with the Business Analyst.\n\u27a2 Participated in creating the logical model of an online processing system for a large financial institution using Erwin.\n\u27a2 Worked with DBAs to generate physical model.\n\u27a2 Worked on Bill inmon methodologies for modeling.\n\u27a2 Created tables, views, procedures and SQL scripts and mapping documents.\n\u27a2 Worked on slowly changing dimensions (SCD) and hierarchical dimensions\n\u27a2 Worked on conversion process of data, which is stored in flat files into oracle tables.\n\u27a2 Designed and Developed SQL procedures, functions and packages to create Summary tables.\n\u27a2 Generating ad-hoc reports using crystal reports 9.\n\u27a2 Developed database backup and restore policy.\n\u27a2 Expertise in Creating Report Models for building Ad-hoc Reports Using SSRS.\n\u27a2 Expertise in Generating Reports using SSRS and Excel Spreadsheet.\n\u27a2 Expertise in Creating various Parameterized, Cascaded, Linked, Drill-through and Drill-down Reports.\n\u27a2 Hands on Experience in creating ETL Packages using SQL Server 2005 Integration Services (SSIS).\n\u27a2 Good Understanding in Database and Data Warehousing Concepts.\n\nEnvironment: Oracle8i, SQLServer, SSIS, SSRS, ERWIN, HTML, Crystalreports9, MSOFFICE, Windows XP.']","[u""Bachelor's""]",[u'']
0,https://resumes.indeed.com/resume/cf2916833172840b,"[u""Data Analyst\nCentene - Saint Louis County, MO\nDecember 2017 to Present\nWhat i do:\n1) Assisting in building an SQL data Warehouse which helps to validate the data provided to business team is according to the demands.\n2) validate with SQL queries to ensure that the data from ETL process loads to QSI-XL's landing zone.\n3) IRS, CMS, HHSC, HEDIS data reporting to Business team:\n4) suggest ways to the management on hoe to improve the processes which involve SQL queries.""]","[u""Master's in Information Technology"", u""Master's""]","[u'Marshall University\nJanuary 2015 to May 2017', u'']"
0,https://resumes.indeed.com/resume/d09ae0be5feccd36,"[u'Data Analyst\nCommscope - Richardson, TX\nJuly 2017 to Present\n\u2022 Comparing reporting values with original files by writing complicated SQL join queries that reduces 60% manual processing time, and cleaning over 300+ invalid sales records daily\n\u2022 Integrating monthly sales reports from 240+ distributors global wise, applying advanced Excel VBA to create SKU mapping lookups and providing resolution reports\n\u2022 Collaborating with analysts and engineers to analyze business requirements, creating customized reports in SAP, ensuring the accuracy of data integration and timely data reporting on both corporate and third party sides', u'Data Analyst\nAXA Advisors - Dallas, TX\nMarch 2017 to July 2017\n\u2022 Managed the data processing for over 1,000 clients, supported database ETL, conducted customer analysis, researched and delivered tangible solutions to enhance the customer relationship management\n\u2022 Processed applications and service requests for clients on financial products, supported marketing events, presentations and proposals for financial service and products', u'Data Analyst\nAmerican Marketing Association - Dallas, TX\nJanuary 2016 to April 2017\n\u2022 Formulated membership acquisition plans, filtered and analyzed 2 years\u2019 membership data using SAS, detected 5 key KPIs, retained over 100 members in 1 year, assured the KPIs are well deployed continuously\n\n\u2022 Surveyed 50 members\u2019 demands for professional related events, and increased appeal by offering 2 events related to the marketing analytics field\n\n\u2022 Reported to the directors with the Tableau geographic heat maps including 8 regions and over 100 DFW areas, delivered the insights about the ideal regions to hold events, and nominated as the staff of the month']","[u'M.S. in Business Analytics', u'B.S. in Accounting']","[u'The University of Texas at Dallas Dallas, TX\nMay 2017', u'Guangdong University of foreign studies\nMay 2015']"
0,https://resumes.indeed.com/resume/5b3c4d9a9c2f2ab6,"[u""Data Analyst\ncomScore - Reston, VA\nNovember 2015 to Present\n\u2022 Investigate and manage resolution of issues related to data collection technologies, data processing, and internal and third party (Qlik) data visualization tools.\n\u2022 Review large data sets (1BN+ records) for comScore executive team and enterprise clients (large media companies and financial organizations) to identify valuable insights, data quality issues etc.\n\u2022 Managed expansion of product support model to include all comScore products in 2017. New products added to support model valued at 100MM USD annually.\n\u2022 Selected by VP/Director to participate in comScore's management training program.\n\u2022 Lead and train team of 2-3 associate data analysts."", u'Technical Analyst\nGeisinger Health System - Danville, PA\nMay 2014 to November 2015\n\u2022 Managed enterprise applications used by ~500 healthcare providers.\n\u2022 Built and configured production databases for software development team.\n\u2022 Developed several production web applications (C#) used by radiology technicians.\n\u2022 Developed dashboard (C#) to track productivity of 80+ radiologists for dept. chair.\n\u2022 Managed migration of ~800K radiology studies (images, reports, and metadata) during an imaging center acquisition in 2015.']","[u'Bachelor of Science in Information Technology', u'in Program Design and Data Structures']","[u'Juniata College Huntingdon, PA', u'George Mason University Fairfax, VA']"
0,https://resumes.indeed.com/resume/597385e7ecd99c1d,"[u""Data Analyst\nKonnections - Mumbai, Maharashtra\nAugust 2015 to July 2016\nActed as Data Analyst Lead for a technology startup specializing in design and development of commercial websites\nand mobile apps.\n\u2022 Used data analysis for storytelling and turning data into actionable insights.\n\u2022 Produce daily, weekly and monthly reports to track KPI's and build customized dashboards in Google Analytics\n\u2022 Created, gathered and analyzed campaign performance data to find trends and predict future sales.\n\u2022 Performed market analysis to efficiently achieve objectives, increasing sales of service by 20%.\n\u2022 Used SQL for maintaining ETL processes to gather product performance data.\n\u2022 Developed ad-hoc reports and presented visualizations as Tableau Dashboards using time series analysis, bar graphs,\nscattered plots etc.\n\u2022 Documented key findings and recommendations and presented it to the team to help improve strategies and operations."", u'Data Analyst\nSun Life Financial\nAugust 2013 to July 2015\nCreated Test Cases based on the Business requirements that is Source to Target Detailed mapping document\nTransformation rules document.\n\u2022 Involved in Data mapping specifications to create and execute detailed system test plans. The data mapping specifies\nwhat data will be extracted from an internal data warehouse, transformed and sent to an external entity.\n\u2022 Involved in extensive DATA validation using SQL queries and back-end testing\n\u2022 Used SQL for Querying the database in UNIX environment\n\u2022 Designed separate test cases for ETL process (Inbound & Outbound) and reporting\n\u2022 Involved with Design and Development team to implement the requirements.\n\u2022 Developed and Performed execution of Test Scripts manually to verify the expected results\n\u2022 Design and development of ETL processes using Informatica ETL tool for dimension and fact file creation\n\u2022 Involved in Manual and Automated testing using QTP and Quality Center.\n\u2022 Performed Black Box - Functional, Regression and Data Driven. White box - Unit and Integration Testing (positive\nand negative scenarios)\n\u2022 Defects tracking, review, analyzes and compares results using Quality Center.\n\u2022 Participated in the MR/CR review meetings to resolve the issues.\n\u2022 Trained and mentored new team recruits while leading their initial project orientation sessions.', u""Data Quality Analyst /Test Engineer\nNational Australian Bank\nFebruary 2011 to March 2013\n\u2022 Performed Data warehouse and Automation quality assurance for Waterfall and Agile projects.\n\u2022 Validated ETL data flows using shell scripts from upstream OLTP systems to OLAP systems.\n\u2022 Automated Regression test suite using QTP/UFT, which led to 40% reduction in Data Verification efforts\n\u2022 Effectively utilized SQL and Shell scripting skills to successfully deliver Database Testing project with excellent\ncoverage. Received coveted GEM award.\n\u2022 Reduced 35% of manual efforts in Test Preparation phase by scripting VBA macro application using HP ALM's API.\nWas featured in Infosys's Hall of Fame.\n\u2022 Prepared interactive dashboards using Tableau to validate upstream data extracts.\n\u2022 Experienced in handling Functional, Non-Functional & Regression Testing.\n\u2022 Trained and mentored new team recruits while leading their initial project orientation sessions.\n\u2022 Assisted senior management with project estimations, effort billing and cost tracking activities\n\u2022 Performed Requirement Analysis, Project Plan Designing, Execution and E2E Defect Tracking.""]","[u'Master of Science in Business Intelligence & Analytics in Business Intelligence & Analytics', u'Bachelor of Engineering in Engineering']","[u'Stevens Institute of Technology Hoboken, NJ\nAugust 2016 to December 2017', u'MDU University\nJune 2006 to May 2010']"
0,https://resumes.indeed.com/resume/c1346c69b4cd9d09,"[u'Sr. Business Analyst\nMcGraw Hill - New York, NY\nFebruary 2017 to Present\nResponsibilities:\n* Followed structured methodology to initiate the project with BRD and Use Case documents as deliverables at the end of Concept Definition/ Requirements Gathering phase.\n* Facilitated and drove all Concept Definition sessions, along with deciding Agenda for each session and emailing that beforehand to all the participants.\n* Gathered detailed business requirements from the Stakeholders, as well as Product Owner throughout the Concept Definition phase, and provided solutions for the problems that were encountered during the process, mainly technical, with the help of Technical Lead and User Interface (UI) Designer.\n* Interviewed and Interacted with the diverse group of stakeholders from various departments, such as Business, Legal etc.\n* Prepared Process Flows, as well as started creating BRD and writing all the requirements into the BRD as and when they were locked.\n* Prepared a separate document for Functional Requirements, Business Practices, Business Policies, and Business Rules, which were gathered during the Concept Definition sessions, many of which were eventually incorporated in Function Specifications (FS) document.\n* Prepared Minutes of Meeting after every Concept Definition meeting, as well as provided inputs in creating Use Case document.\n* Arranged and Facilitated Concept Definition review meetings with Technical Lead, Product Manager and an SSO (Strategic Solution Owner) of the project.\n* Prepared a list of Action Items and assigned owners to each Action Item, along with a deadline in which each owner had to come-up with a reply. In addition, followed-up with the respective owner through email about particular open questions.\n* Reviewed all the documents, including BRD, in the Concept Definition in front of all the stakeholders, along with Line of Business, to verify all the requirements. In addition, provided Process Flow walkthroughs to the whole team.\n* Coordinated with Project Manager during BRD sign-off process, as well as answered all the questions that stakeholders had about specific requirements in BRD.\n* Prepared Functional Specification document and defined all the requirements in detail, also got that document reviewed by the Technical Lead.', u'Business/Data Analyst\nJPMorgan Chase - Jersey City, NJ\nAugust 2016 to January 2017\nResponsibilities:\n\u2022 Extracted the Business Requirements from the end users keeping in mind their need for the application and prepared Business Requirement Documents (BRD) using Rational Requisite Pro.\n\u2022 Coordinated daily status/stand up call minutes of the meeting, test case scenarios, user training and developing documentation for raining guides.\n\u2022 Competent in Creating Unified Modeling Language (UML) diagrams such as use case diagrams, Activity Diagrams, Class Diagrams and Sequence Diagrams.\n\u2022 Designed and reviewed of various documents including the Software Requirement Specifications (SRS), Business requirements document (BRD), Use Case Specifications, Functional Specifications (FSD), Systems Design Specification (SDS), Requirement Traceability Matrix (RTM) and testing documents.\n\u2022 Prepared graphical depictions of Use Cases like Use Case Diagrams, State Diagrams, Activity Diagrams, Sequence Diagrams, Component Based Diagrams, and Collateral Diagrams and creation of technical design (UI screen) using Microsoft Visio.\n\u2022 Worked on Document for Version Controlling, to maintain up to date changes in the Documents.\n\u2022 Assisted to develop the Test Plan, Test Cases and Test Scenarios to be used in testing based on Business Requirements, technical specifications and/or product knowledge.\n\u2022 Developed and consolidated user and Training Manuals as per Project Specifications and timelines.', u'Business Analyst\nState Farm Insurance - Bloomington, IL\nMay 2015 to June 2016\nState Farm originated enterprise level project called Integrated Customer Platform (ICP) with a goal to differentiate itself from the competition by acknowledging the changing preferences and expectations of customers through the development of ICP; meeting multiple needs; and delivering consistently remarkable customer experiences in all customer interactions, for all products and services they offer through any access point such Mobile, Desktop, Tablet etc. As a part of ICP, company has migrated existing website to Responsive Web Design using SDL Tridion Content Management system in order to provide products and services online more effectively.\n\nResponsibilities:\n\u2022 Worked as a BA and coordinator on Content management team where we developed Statefarm.com as a responsive website for customers to have a significant improvement in experience with mobile and tablets (in par with the desktop experience).\n\u2022 Gathered the requirements from business teams on how the UI & customer experience should be built on mobiles and tablets. Coordinated with offshore, marketing, SEO teams to validate the developed web pages are in accordance with the expected customer experience.\n\u2022 Worked with web analytics teams to develop and design the reports needed to monitor customer experience on State Farm website on mobiles and tablets.\n\u2022 Conducted UAT, usability tests and quality assurance.\n\u2022 Conducted JAD and elicitation sessions for user interviews to gather requirements. Worked with business partners to document detailed business requirements.\n\u2022 Defined and created a wide range of documentation: business requirements (BRD), functional requirements (FRD), system requirements (SRS), UML use cases, test cases, Visio flows, training manuals, and user guides.\n\u2022 Developed business case for migration constrained by time, cost and functionality.\n\u2022 Strategized and performed web content migration to new platform to satisfy the requirements for responsive web design.\n\u2022 Conducted As-Is content analysis to confirm scope and determine level of complexity for migration.\n\u2022 Identified and resolved issues such as mobile risk/compliance, content complexity, non-conforming files and other conversion issues when migrating existing site content to Responsive Web. Defined the target system and web site to determine the transformation requirements.\n\u2022 Formulated test plans, test cases and scenarios for core subsystems. Performed Unit Testing, System Integration Testing, Software Quality Testing, A/B & Regression testing, etc.\n\u2022 Participated in Project Planning and coordination. Developed Road maps and detailed work plans. Prepared a Master Site Matrix with a list of all pages and assets in scope.', u""Business Analyst/Data Analyst\nKarvy IT services - Hyderabad, Telangana\nFebruary 2013 to June 2014\nResponsibilities:\n\u2022 Conducted business requirements gathering sessions through questionnaires and business scenarios with end users, SME's and developers.\n\u2022 Object Oriented Design and Analysis using UML.\n\u2022 Worked with the client and project manager in defining the functional scope of the project.\n\u2022 Responsible for interacting with clients to gather and analyze business requirements of the project and transmit the same to development team.\n\u2022 Prepared Software Requirement Specifications (SRS) and Requirements Traceability Matrix.\n\u2022 Prepared User Interface Guides using industry standard methodologies.\n\u2022 Wrote detailed test cases.\n\u2022 Actively involved in developing, executing and managing User Acceptance Testing (UAT).\n\u2022 Responsible for Product Demos to clients."", u""Business Analyst\nICICI BANK - Hyderabad, Telangana\nMay 2011 to November 2012\nThe project was to develop personal banking application enabling the customers to apply for personal loans, auto loans, and home mortgage over the web.\n\nResponsibilities:\n\u2022 Conducted JAD sessions and analyzed and documented the business requirements from various teams and stakeholders.\n\u2022 Created Use Case Diagrams to define the functional requirements of the application.\n\u2022 Created detailed workflow diagrams of the proposed system.\n\u2022 Analyzed the security implementation of the application with respect to the customer's perspective.\n\u2022 Involved in executing test cases, evaluating test results and preparing test summary reports.\n\u2022 Involved in user acceptance testing and tested various object states using extensive verification points.""]","[u'Master of Science in Computer Science in Computer Science', u'Bachelor of Engineering in Computer Science in Computer Science']","[u'New York University Tandon School of Engineering Brooklyn, NY', u'Osmania University Hyderabad, Telangana']"
0,https://resumes.indeed.com/resume/b1cd73421a554859,"[u'Data Analyst\nDartmouth College - Hanover, NH\nJanuary 2011 to Present\nResponsible for creating, managing, researching, analyzing, and maintaining constituent data to serve as a foundation for fundraising and alumni relations initiatives across Dartmouth College and the professional schools. Provides critical operations support to the Alumni Records and Gift Recording offices using a proactive/collaborate approach, technical expertise, automated tools and web-based applications and provides outstanding customer service to the Dartmouth community. The Data Analyst analyzes and processes transactional requests within the department via a variety of methods and manages special projects.', u'Biographical Data Analyst\nDartmouth College - Hanover, NH\nJanuary 2005 to May 2009\nResponsible for managing, analyzing and disseminating biographical data on Data Warehouse Advance Web Database for assigned prospects, donors, and other people as needed.']","[u'', u'in Paralegal']","[u'South Royalton High South Royalton, VT\nJanuary 1978 to January 1982', u'Woodbury College Montpelier, VT']"
0,https://resumes.indeed.com/resume/0fdf3eadcdc0d34a,"[u""Data Analyst\nInteractive Data Corporation - Santa Monica, CA\nMay 2011 to December 2012\nContributed to the design and development of company's flagship BondEdge NextGen software. Was closely involved with monitoring and verifying the accuracy of financial data output to provide quality risk analytics solutions for clients.\n\u25cf Safeguarded against potential data challenges between Interactive Data and its clients; solely implemented a third party's data into NextGen software after discovering data inaccuracies and thereafter reconciled.\n\u25cf Coordinated with VP of Product Design regularly to discuss/plan GUI's and facilitate software compatibility and usability for clients."", u""Quality Control Analyst\nMay 2007 to May 2011\nConferenced weekly with all departments' VPs to strategize the product development and innovative marketing of upcoming versions of NextGen.\n\u25cf Provided support for Quality Control Manager when absent by running status reports on outstanding tasks for senior management and providing Accounting department with a file containing an itemized list of the departments' billable hours.\n\u25cf Solely responsible for maintenance of administrative procedures including file creation and organization of the specifications of all future projects pending approval for development.\n\nSKILLS\nSoftware: Highly proficient in Microsoft Office applications, Matlab, Mathematica, Windows XP and 7, and internet research with experience in SQL and Access databases.\nLanguage:\nConversational Korean (Basic)\nConversational Tagalog/Pilipino (Basic)\n\nACTIVITIES""]","[u'in Systems Biology', u'M.S. in Applied Mathematics', u'B.A. in Mathematics', u'in Education']","[u'University of California at Irvine Irvine, CA\nJanuary 2014', u'California State University Los Angeles, CA\nJanuary 2012 to January 2014', u'University of California at Santa Barbara Santa Barbara, CA\nSeptember 2004 to December 2006', u'Yonsei University Seoul, KR\nJune 2005 to December 2005']"
0,https://resumes.indeed.com/resume/a95256ce35548ab7,"[u""Senior Data Analyst\nWells Fargo - San Francisco, CA\nJuly 2016 to Present\nProject#: CALM (Corporate Asset Liabilities Management)\nResponsibilities:\n\u2022 Partnered with product owners and the software development team in the definition, development, testing, implementation and support of functional requirements\n\u2022 Teamed up with stakeholders and led a team of data analysts to translate business requirements into technical designs and solutions, and coordinated offshore - onshore teams\n\u2022 Design and implementation of Data Architecture, Database Schema and Data flow\n\u2022 Designing Schema, loading and analyzing data using SQL queries.\n\u2022 Involved in creating tables, loading data from multiple data sources and writing complex SQL queries\n\u2022 Wrote legacy load and incremental load scripts for loading data into hive tables and added partitions\n\u2022 Involved in writing custom collectors to insert or update data into SQL tables\n\u2022 Developed complex custom SQL scripts to be used as Tableau data sources\n\u2022 Created and validated sample data sets using data manipulation techniques in SQL\n\u2022 Visualizing the datasets using Tableau and generate the reports\n\u2022 Writing Python scripts to parse XML files and load the data in database\n\u2022 Responsible for defining, scheduling, maintaining and troubleshooting automated Tidal jobs and writing shell scripts to send email notifications in case of job failures\n\u2022 Created and maintained Technical documentation for executing Hive queries and deploying client's functionalities\n\nEnvironnent: MySQL, Tableau, Hive, Shell Scripting, Python, Oracle, SQL Server, LINUX, UNIX, MariaDB."", u'Senior Data Analyst\nCEB - Oakland, CA\nJune 2015 to July 2016\nProject#: CEB Dashboard project\n\nResponsibilities:\n\u2022 Worked closely with various levels of individuals to coordinate and prioritize multiple projects\n\u2022 Estimate scope, schedule and track projects throughout SDLC\n\u2022 Assess existing and available data warehousing technologies and methods to ensure our Data warehouse/BI architecture meets the needs of the business unit and allows for business growth\n\u2022 Handling structured and unstructured data and applying ETL processes. Writing UDFs and Queries, scripts for data processing\n\u2022 Experience in parsing and transforming data using sed commands\n\u2022 Design/Develop the BO Universes, Ad-hoc/Canned Reports, and Complex Reports using predefined conditions as well as Business Objects Full Client & Thin Client reports\n\u2022 Used Python to extract weekly information from XML files\n\u2022 Configured Hive Metastore, which stores the metadata for Hive tables and partitions in a relational database\n\u2022 Prepare Developer (Unit) Test cases and execute Testing\n\u2022 Exported the analyzed data to the relational databases for visualization and to generate reports for the BI team\n\u2022 Production Rollout Support, which includes monitoring the solution post go-live, and resolving any issues that are discovered by the client and client services teams.\n\nEnvironment: MySQL, Hive, Tableau, Python, shell scripting, Bash, Oracle, Splunk.', u'Senior Data Analyst\nDEPARTMENT OF HUMAN SERVICES - Atlanta, GA\nJanuary 2014 to June 2015\nProjects#: PREP, ODIS, CPS and EPAC\nResponsibilities:\n\u2022 Participate in entire lifecycle for Business Intelligence solution delivery.\n\u2022 Work with Business, technology client representatives and Architects to identify functional and technical requirements, and translate them into Architectural designs\n\u2022 Work with data warehouse Architect to perform Source system analysis, identification of key data issues, data profiling, business processes and analytical/reporting requirements.\n\u2022 Work as Tableau admin activities granting access, managing extracts and Installations.\n\u2022 Created solution driven dashboards by developing different chart types including Crosstab, Heat/Geo/Tree/Symbol Maps, Pie/Bar Charts, Circle Views, Line/Area Charts, Scatter Plots, Bullet Graphs, and Histograms, in Tableau Desktop\n\u2022 Effectively used data blending feature in Tableau to connect different databases like Oracle, MS SQL Server.\n\u2022 Extensively worked with SSIS tool suite, designed and created mappings using various SSIS transformations like OLEDB Command, Conditional Split, Lookup, Aggregator, Multicast and Derived Column\n\u2022 Involved in designing, developing and deploying reports in MS SQL Server environment using\n\u2022 Involve actively in critical data mappings, data analysis, data modeling, and data warehouse design.\n\u2022 Designed ETL process for acquiring data from various sources including flat files, Oracle database, SQL Server and applied business logic to load them in the data warehouse.\n\u2022 Used different types of SSIS Transformations for data conversion, sorting and cleaning the data from different sources into company format.\n\u2022 Responsible for defining, scheduling, maintaining and troubleshooting automated jobs\n\u2022 Designed SSIS packages for comprehensive error handling and package logging.\n\u2022 Applied various performance tuning techniques at Database, Scripts and UI level.\n\u2022 Involved in testing, wrote test cases for all requirements mappings.\n\u2022 Helped business and end users with any operational issues by identifying and implementing resolutions.', u'Data Analyst\nTHE HARTFORD - Hartford, CT\nJanuary 2010 to December 2013\nProject #: CCG (Complex Claims Group)\nResponsibilities:\n\u2022 Worked with Business and technology client representatives to identify functional and technical requirements.\n\u2022 Involved in designing, developing and deploying packages in MS SQL Server environment using SSDT.\n\u2022 Involved in Designing, Analyzing, building and testing of OLAP cubes in SSAS and adding calculations using MDX\n\u2022 Used Tableau to develop different reports like ad -hoc, drilldown and drill through reports for the different users.\n\u2022 Involved in administration tasks such as publishing workbooks, setting permissions, managing ownerships, providing access to the users and adding them to the specific group and scheduled instances for reports in Tableau Server.\n\u2022 Generated Dashboards with Quick filters, Parameters and sets to handle views more efficiently.\n\u2022 Applied various performance tuning techniques at Database, Scripts and UI level.\n\u2022 Created and reviewed Business Requirement document, Functional and Technical Specifications.\n\u2022 Involved actively in critical Source to Target mappings, data analysis, data modeling, and data warehouse design.\n\u2022 Experience in Dimensional Data Modeling Star and Snowflake Schemas.\n\u2022 Designed ETL process for acquiring data from various sources including flat files, Oracle database and applied business logic to load them in the data warehouse.\n\u2022 Used different type of SSIS Transformations for data conversion, sorting and cleaning the data from different sources into company format.\n\u2022 Designed SSIS packages for comprehensive error handling and package logging.\n\u2022 Involved in Design walkthrough sessions with Business users and QA team.', u'Database Developer\nEnvironnent\nJune 2008 to January 2010\nResponsibilities:\n\u2022 Involved in Requirement gathering and architecture reviews.\n\u2022 Worked on developing the ETL load process.\n\u2022 Performed data analysis and profiling.\n\u2022 Developed bteq scripts for loading the data.\n\u2022 Worked on tuning the SQL to optimize the process and to enhance the performance of scripts\n\u2022 Participated in database objects creation, maintenance and capacity planning.\n\u2022 Designed, developed, optimized and maintained database objects such as tables, views, indexes, soft RI, common procedure macro etc\n\u2022 Performed SQL Explain plan analysis, SQL rewriting & testing.\n\nEnvironnent: TD 13.0, 13.10, Fastload, Multiload, Bteq, TD viewpoint portal, SQL Assistant']",[],[]
0,https://resumes.indeed.com/resume/4ad1a10d8e99ab49,"[u'Senior Data Analyst\nManagement Science Associates\nJune 2016 to Present\n\u2022 Query, extract and validate data from client databases in order to fulfill client and internal requests. Understands the data and processes used to produce intended results. Using Apache Drill.\n\u2022 Perform quality assurance functions to validate data.\n\u2022 Create documentation and adhere to standard operating procedures\n\u2022 Communicate with team members on project assignments\n\u2022 Learn the Hadoop environment\n\u2022 Work with Developer so they understood the needs of the group.\n\u2022 Build client databases using client defined business rules.', u'Experienced Data Analyst\nManagement Science Associates - Pittsburgh, PA\nJuly 2000 to Present', u""Production Coordinator\nManagement Science Associates\nSeptember 2013 to September 2016\n\u2022 Query, extract and validate data from client databases in order to fulfill client and internal requests. Understands the data and processes used to produce intended results.\n\u2022 Interact with client teams to understand and define processes needed for client databases.\n\u2022 Using root cause analysis define checks and correct plan to ensure potential data issues won't occur again.\n\u2022 Perform quality assurance functions to validate data, software, hardware, or procedure changes\n\u2022 Provide guidance to team members\n\u2022 Provide project management to team members projects.\n\u2022 Maintain detailed documentation and adhere to standard operating procedures\n\u2022 Communicate with team members on project assignments\n\u2022 Understand strategic goals for respective area and align personal goals\n\u2022 Supervise workloads of team members.\n\u2022 Train team members."", u'Data Analyst\nManagement Science Associates\nJuly 2007 to September 2013\n\u2022 Query, extract and validate data from client databases in order to fulfill client and internal requests. Understands the data and processes used to produce intended results.\n\u2022 Identify problems based on project documentation and processes. Research problems and present findings to team lead.\n\u2022 Perform quality assurance functions to validate data, software, hardware, or procedure changes\n\u2022 Review processes, looking for ways to improve performance, improve turnaround, and reduce risk. Present suggestions to team leaders and work with them to implement approved changes.\n\u2022 Maintain detailed documentation and adhere to standard operating procedures\n\u2022 Communicate with team members on project assignments', u'Data Specialist\nManagement Science Associates\nJuly 2000 to July 2007\n\u2022 Learn and understand client data (able to determine if data appears to be erroneous)\n\u2022 Creates and maintains detailed documentation and adheres to standard operating procedures\n\u2022 Produce and verify reports\n\u2022 Set up, submit and check special requests\n\u2022 Answers and evaluates incoming telephone calls, voice mails, and emails from internal inquiries\n\u2022 Researches product information for Master Brand creation']","[u'Bachelor of Science in Information Technology', u'in Computer Science and Information Technology', u'']","[u'Point Park College Pittsburgh, PA\nJanuary 1998 to January 2000', u'North Central Technical College Mansfield, OH\nJanuary 1996 to January 1997', u'Pilgrim High School Warwick, RI\nJanuary 1993']"
0,https://resumes.indeed.com/resume/453f9948759e043e,"[u""Data Scientist\nFiat Chrysler Automobiles - Auburn Hills, MI\nAugust 2017 to Present\nDescription:\nFiat Chrysler Automobiles N.V. (FCA), incorporated on April 1, 2014, is an international automotive group. The Company is engaged in designing, engineering, manufacturing, distributing and selling vehicles, components and production systems. The Company's segments include regional mass-market vehicle segments, which include NAFTA , LATAM, APAC and EMEA, Maserati, its luxury brand segment, and a global components segment.\n\nResponsibilities:\n\u2022 Transformation of data using SSIS.\n\u2022 Build analytical solutions and models by manipulating large data sets.\n\u2022 A highly immersive Data Science program involving Data Manipulation&Visualization, Web Scraping, Machine Learning, Python programming, SQL, GIT, MongoDB, Hadoop.\n\u2022 Applied machine learning and statistical techniques to large datasets to find actionable insights.\n\u2022 Involved in complete Software Development Life Cycle (SDLC) process by analyzing business requirements and understanding the functional work flow of information from source systems to destination systems.\n\u2022 Played critical role in collecting data from different data sources and data system like SAP, JDE, Lubes, Hadoop, etc.\n\u2022 Created ETL packages using SSIS to extract data from relational database and then transform and load into the data mart.\n\u2022 Transforming and merging all the weekly client data into yearly file using ETL SSIS\n\u2022 Used Visual Team Foundation server for version control, source control and reporting.\n\u2022 KT with the client to understand their various Data Management systems and understanding the data.\n\u2022 Worked on different data formats such as JSON, XML and performed machine learning algorithms in Python.\n\u2022 Creating meta-data and data dictionary for the future data use/ data refresh of the same client.\n\u2022 Structuring the Data Marts to store and organize the customer's data.\n\u2022 Used pandas, numpy, seaborn, matplotlib, scikit-learn, scipy, NLTK in Python for developing various machine learning algorithms.\n\u2022 Mapping flow of trade cycle data from source to target and documenting the same.\n\u2022 Performing QA on the data extracted, transformed and exported to excel.\n\nEnvironment: Hadoop, Oracle 11g, MS Office, SSMS, SSIS, Power BI, Microsoft reporting tools, Big Data."", u""Data Scientist\nBecton Dickinson\nMay 2016 to July 2017\nDescription:\nBecton, Dickinson & Co. is a global medical technology company. The company is engaged in the development, manufacture and sale of medical devices, instrument systems and reagents used by healthcare institutions, life science researchers, clinical laboratories, the pharmaceutical industry and the general public.\nResponsibilities:\n\n\u2022 A highly immersive Data Science program involving Data Manipulation & Visualization, Web Scraping, Machine Learning, Python programming, SQL, GIT, Unix Commands, NoSQL, MongoDB, Hadoop.\n\u2022 Setup storage and data analysis tools in Amazon Web Services cloud computing infrastructure.\n\u2022 Used pandas, numpy, seaborn, scipy, matplotlib, scikit-learn, NLTK in Python for developing various machine learning algorithms.\n\u2022 Installed and used CaffeDeepLearning Framework\n\u2022 Worked on different data formats such as JSON, XML and performed machine learning algorithms in Python.\n\u2022 Utilized Spark, Scala, Hadoop, HQL, VQL, oozie, pySpark, Data Lake, TensorFlow, HBase, Cassandra, Redshift, MongoDB, Kafka, Kinesis, Spark Streaming, Edward, CUDA, MLLib, AWS, Python, a broad variety of machine learning methods including classifications, regressions, dimensionally reduction etc.\n\u2022 Worked as Data Architects and IT Architects to understand the movement of data and its storage and ER Studio 9.7\n\u2022 Participated in all phases of data mining; data collection, data cleaning, developing models, validation, visualization and performed Gap analysis.\n\u2022 Data Manipulation and Aggregation from different source using Nexus, Toad, Business Objects, Power BI and Smart View.\n\u2022 Implemented Agile Methodology for building an internal application.\n\u2022 Focus on integration overlap and Informatica newer commitment to MDM with the acquisition of Identity Systems.\n\u2022 Used MLlib, Spark's Machine learning library to build and evaluate different models.\n\u2022 Good knowledge of Hadoop Architecture and various components such as HDFS, JobTracker, Task Tracker, NameNode, DataNode, Secondary NameNode, and MapReduce concepts.\n\u2022 As Architect delivered various complex OLAP databases/cubes, scorecards, dashboards and reports.\n\u2022 Programmed a utility in Python that used multiple packages (scipy, numpy, pandas)\n\u2022 Implemented Classification using supervised algorithms like Logistic Regression, Decision trees, KNN, Naive Bayes.\n\u2022 Used Teradata15 utilities such as Fast Export, MLOAD for handling various tasks data migration/ETL from OLTP Source Systems to OLAP Target Systems\n\u2022 Experience in Hadoop ecosystem components like Hadoop MapReduce, HDFS, HBase, Oozie, Hive, Sqoop, Pig, Flume including their installation and configuration.\n\u2022 Updated Python scripts to match training data with our database stored in AWS Cloud Search, so that we would be able to assign each document a response label for further classification.\n\nEnvironment: regression, logistic regression, Hadoop, Teradata, OLTP, Unix, Python, MLLib, SAS, random forest, OLAP, HDFS, NLTK, SVM, JSON and XML."", u'Data Scientist\nHarvard Pilgrim Health Care - Boston, MA\nJanuary 2015 to April 2016\nDescription: Harvard Pilgrim Health Plan is a not-for-profit health services company based in the New England region of the United States.\n\nResponsibilities:\n\u2022 Review and determine risk profiles of data based on metadata and underlying data elements\n\u2022 Worked on Developing Fraud Detection Platform using various machine learning algorithms in Python\n\u2022 Worked on Linear discriminant analysis, Greedy Forward Selection, Greedy Backward Selection and Feature reduction algorithms like Principal Component Analysis (PCA) and Factor Analysis\n\u2022 Manipulating and Cleaning data using missing value treatment in Pandas and performed standardization\n\u2022 Implemented Classification using Supervised algorithms like Logistic Regression, Decision trees, KNN, NaiveBayes\n\u2022 Worked on customer segmentation using an unsupervised learning technique - clustering\n\u2022 Performed Exploratory Data Analysis and Data Visualizations using Python\n\u2022 Strong skills in data visualization like matplotlib and seaborn library\n\u2022 Create different charts such as Heat maps, Bar charts, Line charts etc.,\n\u2022 Experienced in working with SVM-Kernel method like RBF, polynomial, linear\n\u2022 Implemented Ensemble models like Boosting and Bagging\n\u2022 Worked with cross validation technique and grid search to improve project model results\n\nEnvironment:Python, SQL, Microsoft Excel', u'Data Analyst\nSprint corporation Inc - Overland Park, KS\nMay 2013 to December 2014\nDescription: Sprint Inc., incorporated June 19, 1899, is a holding company. The Company and its subsidiaries provide communications and digital entertainment services in the United States and the world. The Company operates through four segments: Business Solutions, Entertainment Group, Consumer Mobility and International\n\nResponsibilities:\n\u2022 Generated cost-benefit analysis to quantify the model implementation comparing with the former situation\n\u2022 Worked on model selection based on confusion matrices, minimized the Type II error\n\u2022 Worked on data cleaning and reshaping, generated segmented subsets using Numpy and Pandas in Python\n\u2022 Wrote and optimized complex SQL queries involving multiple joins and advanced analytical functions to perform data extraction and merging from large volumes of historical data stored in Oracle 11g, validating the ETL processed data in target database\n\u2022 Applied various machine learning algorithms and statistical modeling like decision tree, logistic regression, Gradient Boosting Machine to build predictive model using scikit-learn package in Python\n\u2022 Developed Python scripts to automate data sampling process. Ensured the data integrity by checking for completeness, duplication, accuracy, and consistency\n\u2022 Identified the variables that significantly affect the target\n\u2022 Continuously collected business requirements during the whole project life cycle.\n\u2022 Conducted model optimization and comparison using stepwise function based on AIC value\n\u2022 Generated data analysis reports using Matplotlib, Tableau, successfully delivered and presented the results for C-level decision makers\n\nEnvironment:SAP ERP, SCM APO GATP Software, User acceptance testing (UAT), Rational clear case, Clear quest, Use cases, UML, MS Office, Requisite Pro.', u""Data Modeler\nInfogain India Pvt Ltd - Noida, Uttar Pradesh\nDecember 2011 to April 2013\nDescription: Infogain Implements Oracle Retail Store Solutions for Healthcare Leader using Agile SCRUM ERP a Necessary Tool for Indian SMEs: CRO, Infogain The Infogain team will be in Las Vegas for the National Workers' Compensation and Disability Conference\xae & Expo, which brings together thousands of professionals.\n\nResponsibilities:\n\u2022 Worked with large amounts of structured and unstructured data.\n\u2022 Knowledge in Machine Learning concepts (Generalized Linear models, Regularization, Random Forest, Time Series models, etc.)\n\u2022 Worked in Business Intelligence tools and visualization tools such as Business Objects, Tableau, ChartIO, etc.\n\u2022 Deployed GUI pages by using JSP, JSTL, HTML, DHTML, XHTML, CSS, JavaScript, AJAX.\n\u2022 Configured the project on WebSphere 6.1 application servers\n\u2022 Implemented the online application by using Core Java, Jdbc, JSP, Servlets and EJB 1.1, Web Services, SOAP, WSDL.\n\u2022 Handled end-to-end project from data discovery to model deployment.\n\u2022 Monitoring the automated loading processes.\n\u2022 Communicated with other Health Care info by using Web Services with the help of SOAP, WSDLJAX-RPC\n\u2022 Used Singleton, factory design pattern, DAO Design Patterns based on the application requirements\n\u2022 Used SAX and DOM parsers to parse the raw XML documents\n\u2022 Used RAD as Development IDE for web applications.\n\u2022 Preparing and executing Unit test cases\n\u2022 Used Log4J logging framework to write Log messages with various levels.\n\u2022 Involved in fixing bugs and minor enhancements for the front-end modules.\n\u2022 Implemented Microsoft Visio and Rational Rose for designing the Use Case Diagrams, Class model, Sequence diagrams, and Activity diagrams for SDLC process of the application\n\u2022 Doing functional and technical reviews\n\u2022 Maintenance in the testing team for System testing/Integration/UAT.\n\u2022 Guaranteeing quality in the deliverables.\n\u2022 Conducted Design reviews and Technical reviews with other project stakeholders.\n\u2022 Was a part of the complete life cycle of the project from the requirements to the production support\n\u2022 Created test plan documents for all back-end database modules\n\u2022 Implemented the project in Linux environment.\n\nEnvironment: R 3.0, Erwin 9.5, Tableau 8.0, MDM, QlikView, MLLib, PL/SQL, HDFS, Teradata 14.1, JSON, HADOOP (HDFS), MapReduce, PIG, Spark, R Studio, MAHOUT, JAVA, HIVE, AWS"", u'Data Analyst\nPidilite Industries - IN\nMay 2009 to November 2011\nDescription: Pidilite Industries Limited is an Indian-based adhesives manufacturing company. It also sells art material, construction chemicals and other industrial chemicals. Pidilite markets the Fevicol range of adhesives.\n\nResponsibilities:\n\u2022 Performed data profiling in the source systems that are required for New Customer Engagement (NCE) Data mart.\n\u2022 Document the complete process flow to describe program development, logic, testing, and implementation, application integration, coding.\n\u2022 Manipulating, cleansing & processing data using Excel, Access and SQL.\n\u2022 Responsible for loading, extracting and validation of client data.\n\u2022 Liaising with end-users and 3rd party suppliers. Analyzing raw data, drawing conclusions & developing recommendations writing SQLscripts to manipulate data for data loads and extracts.\n\u2022 Developing data analytical databases from complex financial source data. Performing daily system checks. Data entry, data auditing, creating data reports & monitoring all data for accuracy. Designing, developing and implementing new functionality.\n\u2022 Monitoring the automated loading processes. Advising on the suitability of methodologies and suggesting improvements.\n\u2022 Involved in defining the source to target data mappings, business rules, and business and data definitions.\n\u2022 Responsible for defining the key identifiers for each mapping/interface.\n\u2022 Coordinate with the business users in providing appropriate, effective and efficient way to design the new reporting needs based on the user with the existing functionality.\n\u2022 Document data quality and traceability documents for each source interface.\n\u2022 Designed and implemented data integration modules for Extract/Transform/Load (ETL) functions.\n\u2022 Documented the complete process flow to describe program development, logic, testing, and implementation, application integration, coding.\n\u2022 Worked with internal architects and, assisting in the development of current and target state data architectures.\n\u2022 Worked with project team representatives to ensure that logical and physical ER/Studio data models were developed in line with corporate standards and guidelines.\n\nEnvironment:SQL/Server, Oracle10 &11g, MS-Office, Netezza, Teradata, Enterprise Architect, Informatica Data Quality, ER Studio, TOAD, Business Objects, Green plum Database, PL/SQL.']","[u""Bachelor's""]",[u'']
0,https://resumes.indeed.com/resume/bc9f57205dac62b9,[u'Data Analyst\nImagefortress\nMarch 2014 to Present'],[u'Some college'],[u'']
0,https://resumes.indeed.com/resume/7aac98f11c8faf56,"[u'The University of Texas at Dallas - Dallas, TX\nAugust 2017 to December 2017\nDallas\nAnalytics Practicum\nAHA uses Tableau to understand its business data. The role included development and support of reporting and analytical solutions. Provided the leadership and decision makers with the information to track performance, achieve goals, provide operational reporting and deliver analytical solutions.\nResponsibilities:\n\u2022 Responsible for designing and developing various weekly and monthly reports showing detailed information that could be used to send information to a diverse group of users, clients, and managers.\n\u2022 Published various kinds of interactive data visualizations, dashboards, reports and workbooks from Tableau Desktop to Tableau servers.\n\u2022 Connected Tableau to various databases to establish live connections. Created schedules for incremental refreshes for data sources on Tableau server.\n\u2022 Deftly designed dashboard templates as per the requirements & dashboards content which included bar charts, waterfall charts, score cards, Gantt charts, Bubble charts, Word maps, Geospatial visualizations, network charts etc.\n\u2022 Performed generating Daily/Weekly Sales, Finance trending & forecasting reports using Tableau by identifying the dimensions, measures, measure values and level of details for Business Analytics & Top Executives to give an overview of current, historical and future trends.\nEnvironment: Tableau Server, Tableau 9.x, R, SQL Server, Oracle 10, Microsoft Office Suite (Word, Excel, Power Point)', u'Independent Case study - Health Insurance Client - Humana\nMay 2017 to June 2017\nAsk:\n\u2022 Customer would like to explore, the impact of increasing awareness and number of visitors on the total sales.\n\u2022 Which media vehicle drives the Units, awareness and Visitors and by how much\n\u2022 How much does ""out of window"" spend drive units\nApproach\n\u2022 Identify customer risk profiles, model shopping behavior and potential customer segments\n\u2022 To analyze the impact awareness on units and visitors, we use (2SLS) Simultaneous equations\n\u2022 Tobit Analysis, to account for ""Window"", since sales occurs only during ""Window""\n\u2022 Marcom Analysis, for financial impact to see where company should invest the money.\nSoftware used- SAS, Excel, Power Point', u'Independent Case study - The Hertz Corporation\nJanuary 2017 to April 2017\nAsk\n\u2022 Client would like segment the customers according to their purchase preference\n\u2022 Predict the number of days vehicle rented\nApproach\n\u2022 Identify the influential observation, check for collinearity and check for missing values\n\u2022 Optimally segment the customers according to their preference using latent class analysis\n\u2022 Estimate the number of days a vehicle rented using Poisson analysis\n\u2022 Result-If we target potential group of customers the probability to increase the number of days will increase by ~7%.\nEnvironment: Base SAS, Tableau Desktop, Tableau Server, MySQL Server, Microsoft Office', u'Data Analyst\nIndependent Case study - JO-ANN Fabrics\nAugust 2016 to December 2016\nof its store and determine marketing strategy to improve store revenue.\nApproach\n\u2022 Check for influential observation, set the Standard deviation range with 2 SD under normal distribution to remove the outliers\n\u2022 Check for collinearity, by running the regression model, if collinearity is found, then tackle them with either factor analysis or removing the variables\n\u2022 Using marketing communication and product items data, we propose a strategy to increase the revenue\n\u2022 Performed segmentation analysis using K-means clustering to target potential segment, analyzed the impact of price and communication channels on each segment using price elasticity and Marcomm valuation\n\u2022 Recommendation of bundle products using Market Basket Analysis\nEnvironment: Base SAS, Microsoft Excel, Power BI, Power Point', u'Data Analyst\nBank of America - Mumbai, Maharashtra\nOctober 2012 to July 2016\nResponsibilities:\n\u2022 Designed and prepared interactive and intuitive year end dashboards and reports using Tableau to show the test case performance by the engine this year\n\u2022 Developed ""Automated Engine"" using R-programing to identify discrepancies in the production and development codes using quality, cost, human resources and Technical debt as metrics (KPI), reduce the production cost by 30%\n\u2022 Analyzed data before and after changes in the operating server environment, to critically determine weaknesses, including errors in processes; and make recommendations to improve performance.\n\u2022 Used SQL queries for data-analysis and data extraction from database landscape and comprehend the data to perform sensitivity analysis\n\u2022 Performed Data profiling to examine the pattern of data coming from existing source systems for collecting the statistics and information about the data\n\u2022 Promoted to team leader, supervised and cross-trained 11 members and made them ready for projects in pipeline\nEnvironment: R Studio, MySQL DB, MS Excel, Tableau', u'Data Analyst Intern\nALTIMA SYSTEMS PRIVATE LIMITED - Delhi, Delhi\nJune 2011 to December 2011\n\u2022 Maintained inventory transactions in oracle and performed analysis to minimize payments defaults and reduced losses by 6%\n\u2022 Performed regression Analysis to see the factors causing payment defaults using Microsoft Excel\n\u2022 Created interactive dashboard and reports to analyze daily market sales and compared them with target sales\n\u2022 Assisted database team in revamping the database from scratch and created ETL workflows on the existed data\nEnvironment: MS Excel, Tableau. Oracle']","[u'Master of Science in Business Analytics in Business Analytics', u'Bachelor of Engineering in Electronics in Electronics']","[u'The University of Texas at Dallas Dallas, TX', u'Maharishi Dayanand university Faridabad, Haryana']"
0,https://resumes.indeed.com/resume/a238e5d0d046874b,"[u'Sr. Data Analyst\nMeridian Health - Gainesville, FL\nOctober 2017 to Present\nResponsibilities:\n\n\u2022 Good Working knowledge of Oracle Database Version 11g and SQL server with regards to SAP administration such as database backup, archive log backup, table space management, update statistics.\n\u2022 Integrated SAP BW with BOBJ 4.1 reporting system include IDT, WebI reports and Design Studio.\n\u2022 Involved in Data Analysis, Data Migration, Data Cleansing, Transformation, Integration, Data Import, and Data Export using multiple ETL tools such as Abilities and Informatica Power Center and testing and writing SQL and PL/SQL statements - Stored Procedures, Functions, Triggers and packages.\n\u2022 Written PL/SQL Stored Procedures and Functions for Stored Procedure Transformation in Informatica.\n\u2022 Analysing raw data, concluding, developing recommendations and manipulatedatafordataloads and extracts.\n\u2022 Data output - Made data chart presentations and coded variables from original data, conducted statistical analysis as and when required and provided summaries of analysis.\n\u2022 Carried out data manipulation, data cleaning, dealt with missing records, purged and consolidated data for analysis and selected the appropriate visualization techniques; developing reports and communicating insights to stakeholders.\n\u2022 Implemented PL/SQL scripts in accordance with the necessary Business rules and procedures.\n\u2022 Generated SQL and PL/SQL scripts to create and drop database objects including: Tables, Views, and Primary keys, Indexes, Constraints, Packages, Sequences and Synonyms.\n\u2022 Involved in the new projects and helped the team to make the decisions for Business Objects Universes and Reports.\n\u2022 Leading the team for SAP Data Migration. Managing and allocating work among the team members.\n\u2022 Developed reports using business objects tools (Crystal Reports) and exported reports into various formats like Excel, PDF etc.\n\u2022 Used various Crystal Reports features like conditional formatting, designing formulas and parameter passing.\n\u2022 Utilized ODBC for connectivity to Teradata & MS Excel for automating reports and graphical representation of data to the Business and Operational Analysts.\n\u2022 Worked on loading data from flat files to Teradata tables using SAS Proc Import and Fast Load Techniques.\n\u2022 Performed in depth analysis in data & prepared weekly, biweekly, monthly reports by using SAS, Ms Excel, Ms Access, SQL, and UNIX.\n\u2022 Designed, created and implemented WebI reports, Web services (BIWS) and Design Studio Dashboards using best practices.\n\u2022 Used Business Objects XIR3 extensively and Involved in Full SDLC (software development life cycle) rules are followed for designing new components.\n\u2022 Trained the users to write the Business Objects reports and helped them with some advanced functions.\n\u2022 Experience in Data Extraction from and Load into heterogeneous Data Sources such as Oracle, MS SQL Server, Flat files, XML and loaded into SAP ECC, SAP HANA, Data Warehouses.\n\u2022 Created SSIS Packages for import and export of data between MS SQL Server 2014 database and others like MS Excel and Flat Files.\n\u2022 Delivered and compelled the demonstrations on Lumira Predictive analysis and HANA Complex Models.\n\u2022 Involved in Requirement Gathering, analysis of the requirements from the business owners and users.\n\u2022 Gathering requirements and creating User specification, Functional specification and technical specification\n\u2022 Developed SSIS transformations & Event Handlers for Error handling and Debugging for the Packages.\n\u2022 Wrote complicated SQL queries to load tables in the data warehouse DB2 database, decreased data loading time by an estimated 60%.\n\u2022 Comprehensive understanding of SAP Business Objects configuration and performance tuning.\n\u2022 Created source to target mapping documents from staging area to Data Warehouse/Data Mart.\n\u2022 Created dashboards, reports, and analyses that provides a platform for decision making on a variety of business issues.\n\u2022 Utilized Tableau to create effective data visualizations and dashboards to deliver key business insights.\n\u2022 Presented and defended complex analysis internally and externally to both technical and non-technical audiences.\n\u2022 Collaborated with the team to develop a whole process of launching a membership system including points awarding and redemption, gift fulfilment and account maintenance.', u'Sr. Data Analyst\nCambia Health Solutions - Portland, OR\nAugust 2015 to September 2016\nResponsibilities:\n\n\u2022 Wrote several Teradata SQL Queries using Teradata SQL Assistant for Ad Hoc Data Pull request.\n\u2022 Developed Python programs for manipulating the data reading from various Teradata and convert them as one CSV Files.\n\u2022 Worked on creating filters, parameters and calculated sets for preparing dashboards and worksheets in Tableau.\n\u2022 Created data models in Splunk using pivot tables by analyzing vast amount of data and extracting key information to suit various business requirements.\n\u2022 Implemented data refreshes on Tableau Server for biweekly and monthly increments based on business change to ensure that the views and dashboards were displaying the changed data accurately.\n\u2022 Maintenance of large data sets, combining data from various sources by Excel, SAS Grid, Enterprise, Access and SQL queries.\n\u2022 Design and development of ETL processes using Informatica ETL tool for dimension and fact file creation.\n\u2022 Develop and automate solutions for a new billing and membership Enterprisedata Warehouse including ETL routines, tables, maps, materialized views, and stored procedures incorporating Informatica and Oracle PL/SQL toolsets.\n\u2022 Extracted data from the database using SAS/Access, SAS SQL procedures and create SAS datasets.\n\u2022 Data Profiling to help identify patterns in the source data using SQL and Informatica and thereby help improve quality of data and help business to understand the converteddata better to come up with accurate business rules.\n\u2022 Involved with data profiling for multiple sources and answered complex business questions by providingdata to business users.\n\u2022 Performed data analysis and data profiling using complex SQL on various sources systems including Oracle and Teradata.\n\u2022 Migrated three critical reporting systems to Business Objects and Web Intelligence on a Teradata platform.\n\u2022 Utilized Informatica toolset (Informatica Data Explorer, and Informatica Data Quality) to analyze legacy data for data profiling.\n\u2022 Reverse Engineered the Data Models and identified the Data Elements in the source systems and adding new Data Elements to the existing data models.\n\u2022 Generated comprehensive analytical reports by running SQL queries against current databases to conduct data analysis.\n\u2022 Coordinated with DBAs and generated SQL codes from data models. Generate reports for better communication between business teams.', u""Data Analyst\nAthenahealth - Watertown, MA\nJanuary 2013 to July 2015\nResponsibilities:\n\n\u2022 Used Data warehousing for Data Profiling to examine the data available in an existing database and created Data Mart.\n\u2022 Responsible for creating & managing the Business Objects repository and coordinating/performing universe development.\n\u2022 Extensively used SQL for Data Analysis and to understand and documenting the data behaviour.\n\u2022 Developed an interactive web page (HTML/ JavaScript) for users to search restaurants, update preference and view recommended restaurants.\n\u2022 Developed a web service using (Java servlet, REST API) to fetch restaurant data from Yelp API.\n\u2022 Created project metrics and analytics dashboards each month to measure team statistics and performance.\n\u2022 Analysed complex server utilization data; developed simulation models and managed data repositories.\n\u2022 Implemented data collection systems and other strategies that optimize statistical efficiency and data quality.\n\u2022 Reduced issue-tracking efforts by implementing ETL to resolve policy processing issues and used SSIS to integrate databases and manage metadata\n\u2022 Designed and developed a filtering and sorting algorithm to match similar restaurants based on categories.\n\u2022 Improved the precision by ordering restaurants based on stars, distance and matched categories.\n\u2022 Design, develop, validate and integrate reporting and information solutions with Design Studio.\n\u2022 Using SAP BI (BI Universe design, WebI reports, Dashboard) to develop performance repots and converting Crystal reports.\n\u2022 Performed various levels of testing (i.e. regression, functional, integration and performance) to ensure user acceptance criteria were met.\n\u2022 Collect, query, standardize, and cleanse structured and unstructured data used for business initiatives.\n\u2022 Utilized Excel V-Lookup and Data Validation to extract data across tables and create query functionality.\n\u2022 Reversed engineered existing data bases to understand the data flow and business flows of existing systems and to integrate the new requirements to future enhanced and integrated system.\n\u2022 Conducted presentations training sessions and created training documents for the Business users on the use of Business Objects reporting tool and overview of the Universe.\n\u2022 Performed end to end ETL testing of Custom Tables which were developed for Cognos Custom reports.\n\u2022 Coordinated the testing process for various periodic reports like (CTPR, NDA, IND, and PSUR).\n\u2022 Developed complex SQL's to validate the data in periodic reports as a part of Aggregate Testing.\n\u2022 Involved in Regression testing of Argus Safety(Periodic) and Insight reports (Cognos Custom and Out of the box) to verify if the Hot fix defects have been fixed in the system (Hot fix1 and Hot fix2 Testing).\n\u2022 Test various BizTalk interfaces developed for the automation of product, license and study configurations.\n\u2022 Designed the procedures for getting the data from all systems to Data Warehousing system.\n\u2022 Performed analysis, coding, testing, implementation and troubleshooting on production reports and tables, as well as producing ad hoc reports using SAS.\n\u2022 Generated various reports on a daily, weekly and monthly basis using Business Objects Report module.\n\u2022 Develops reports, charts, tables, graphs, and intermediate statistical analysis using tools such as SAS, SQL, Tableau, and MS Excel.\n\u2022 Involved in Data Analysis, Data Migration, Data Cleansing, Transformation, Integration, Data Import, and Data Export using multiple ETL tools such as Abilities and Informatica Power Center and testing and writing SQL and PL/SQL statements - Stored Procedures, Functions, Triggers and packages.\n\u2022 Worked with Business Objects Professional services to build a online training material using the SAP Knowledge Accelerator tool for reporting.\n\u2022 Strong ability to synthesize available data from multiple sources, identify opportunities to maintain, clean & process the data to provide quality reports & analysis.\n\u2022 Work with large data sets, analyse the data to understand the key business drivers & the implications, engage in discussions on what the data means and presenting it with dashboards reports, graphs, and summaries."", u""Data Analyst\nUnitedHealth Group - Hopkins, MN\nAugust 2010 to December 2013\nResponsibilities:\n\n\u2022 Used ETL methodology for data extraction, transformations and loading in a complex Enterprise DataWarehouse (EDW).\n\u2022 Responsible for all aspects of SAP Basis Administration, system installation, upgrades, database management.\n\u2022 Experience in using SSIS tools like Import and Export Wizard, Package Installation, and SSIS Designer.\n\u2022 Extensively used the SET, UPDATE and MERGE statements for creating, updating and merging SAS data sets.\n\u2022 Extracted data from disparate sources and performed operations on incorrect, redundant, incomplete and transformed out of formatted data, worked on loading the large datasets into enterprise data warehouse.\n\u2022 Converted the insurance data retrieved from vendors into SAS readable format and executed the datasets to produce necessary insurance financial reports using SAS/STAT procedures such as PROC FREQ, PROC REPORT\n\u2022 Performed complex statistical analysis using PROC MEANS, PROC FREQ, PROC SUMMARY, PROC SORT, and PROC SQL\n\u2022 Performed analysis, coding, testing, implementation and troubleshooting on production reports and tables, as well as producing ad hoc reports using SAS.\n\u2022 Converted and loaded data from flat files to temporary tables in Oracle database using SQL*Loader.\n\u2022 Extensively used PL/SQL in writing database packages, stored procedures, functions and triggers in Oracle 10g.\n\u2022 Created interactive dashboards and visualizations of customer reports, competitor analysis and improved revenue statistics data using Tableau.\n\u2022 Built predictive models using Machine Learning algorithms to answer business questions and validated model to produce more accurate results.\n\u2022 Determined the cost of operations by establishing standard costs; collecting operational data and by using research analytics and data modeling techniques.\n\u2022 Maintained technical knowledge by attending educational workshops; reviewing publications and contributing to team efforts.\n\u2022 Created the data story in Tableau by implementing interactive dashboards using drill down filters and parameters.\n\u2022 Performed data manipulation that includes inserting, updating, selecting and deleting data from large complex data sets using SQL Server.\n\u2022 Used Excel's VLOOKUP's to determine the customer data and created pivots to easily access and validate data.\n\u2022 Guided cost analysis process by establishing and enforcing policies and procedures; providing trends and forecasts; explaining processes and techniques; recommending actions\n\u2022 Used Ref cursors and Collections with bulk bind and bulk collect for accessing complex data resulted from joining of large number of tables to extract data from data warehouse.\n\u2022 Extracted data from various production databases using Teradata to meet Campaign data needs.\n\u2022 Data loaded into Teradata from flat files using data stage dynamic sequence job with schema file concept.\n\u2022 Studying Business Requirements Design (BRD) for the enhancements / Physical Data Model changes raised by the business.\n\u2022 Performance tuning, SQL query enhancements, code enhancements to achieve performance targets using Explain Plans.\n\u2022 Graphical representation of the reports using MS Power Point Presentation and MS Excel Charts.\n\u2022 Optimized high volume tables (Including collection tables) in Teradata using various join index techniques, secondary indexes, join strategies and hash distribution methods.\n\u2022 Educated the team on how to utilize the Teradata tools like Database query logs (DBQL), Visual explain etc.\n\u2022 Extensively used MLoad, FLoad, TPump and Fast Export Teradata tools both in ETL and UNIX scripts for high volume flat files.\n\u2022 Fine Tuned (performance tuning) SQL queries and PL/SQL blocks for the maximum efficiency and fast response using Oracle Hints, Explain plans.\n\u2022 Used Teradata as a Source and a Target for few mappings. Worked with Teradata loaders within Workflow manager to configure FastLoad and MultiLoad sessions.\n\u2022 Designed ETL Mapping Specs for ETL team for Source to Target Mappings and corresponding business rules involved.\n\u2022 Used SSIS tasks such as conditional split, derived column, which did data scrubbing, including data validation checks during staging, before loading the data into the data warehouse.""]","[u""Bachelor's""]",[u'']
0,https://resumes.indeed.com/resume/9e9cc15b3b39d07f,"[u'Data Analyst\nDavid H Paul Inc - Farmington, NM\nSeptember 2014 to December 2015\nOrganized and analyzed business cards by year of relevance for communication\ninformation 10 hours a week\n\u25cf Sorted analyzed data into a database']",[u''],"[u'Animas High School Durango, CO\nMay 2019']"
0,https://resumes.indeed.com/resume/2d7d0376b19e66f5,"[u""Data Analyst\nCarlos Lopez and Associates LLC - Lincoln, ME\nSeptember 2015 to Present\nAnalyzed veteran data mined from spreadsheets and databases to find ways of process improvement.\nCreated data input systems for data mining and report structures.\nDeveloped custom SQL queries, excel tracking sheets, pivot tables and dashboards.\nConceptualized and implemented Client Activity Management Systems (CAMS) to track Veteran activities.\nMonitor accuracy of reports submitted to State and Federal agencies.\nImplemented company policies, technical procedures and standards for preserving data integrity and security, reports and access.\nAssembled Technology Intergrading Group to collaborate on current processes.\nFacilitated program/process modifications in accordance with changing user needs, increases in operating efficiency by\n60%, and adaptation of new procedures independently.\nDesigned SharePoint master page and page layouts, serving as company's main SharePoint support for all technical\ncomplications.\nLead introduction and induction of Microsoft Office technology."", u'Data Analyst\nCorpus Christi Army Depot - Corpus Christi, TX\nJanuary 2012 to September 2015\nStreamlined acquisition package routing process to keep Executive Staff informed of request status.\nAnalyzed incoming acquisition packages for clarity, consistency and compliance.\nDeveloped custom SQL queries, excel tracking sheets, pivot tables and dashboards for quality assurance specialists and veteran tracking.\nMonitored acquisition open action items and new discussions that could cause potential delays in contract award and\nproduct receipt.\nActed as liaison between government and point of contact staff.\nPresent and translate findings to Contract Officer Representatives and Supervisor.\nPerformed data entry, data auditing/cleansing, and monitoring data for accuracy.\nCoordinate with Army Contracting Command on policies and procedures.\nDevelop and implement acquisition templates, forms and process flow maps for employee use.\nReview and assist with implementing Standard Operating Procedures.\nSummarize and report on data mined from Acquisition Status Tracker maintaining data integrity.\nEstablish network, system and data availability and integrity through preventative maintenance and upgrades.\nImplemented company policies, technical procedures and standards for preserving integrity and security of data, reports and access.\nSpearheaded technical feasibility solutions for new functional designs and performance improvement.\nEvaluated methodologies for object-oriented software development and efficient database design.', u'Special Projects Coordinator\nTexas Department of Transportation - Corpus Christi, TX\nDecember 2006 to January 2012\nTexas\nLiaison between local and state government on contract progression to ensure federal guideline compliance.\nReceive, oversee and respond to public and private open records request.\nEnsure project progress is disseminated to sustain fluidity and transparency.\nServe as contract monitor and assisted with auditing and special investigation.\nCompliance with policies and regulations and reviews of recipient internal control system.\nKnowledge of methods and equipment used in electronic data processing.\nStreamlined acquisition package routing process to keep Executive Staff informed of request status.\nAnalyzed incoming acquisition packages for clarity, consistency and compliance.\nDeveloped custom SQL queries, excel tracking sheets, pivot tables and dashboards for quality assurance specialists and veteran tracking.\nMonitored acquisition open action items and new discussions that could cause potential delays in contract award and product receipt.']","[u'Master of Science in Education Technology', u'Bachelor of Science', u'Associate of Applied Science in Word Processing']","[u'Texas A&M University Corpus Christi, TX', u'Texas A&M University Corpus Christi, TX', u'Coastal Bend College Beeville, TX']"
0,https://resumes.indeed.com/resume/aa7270a9e01ba02b,"[u""Sr. Data Modeler/Data Analyst\nHBC - St. Louis, MO\nApril 2016 to Present\nEnvironment: MS Visio 2010, Erwin r9.6, Oracle 11g, Informatica 9.6, Tableau, PL/SQL, ER Studio, TOAD , Unix, Teradata, Microsoft SQL Server 2014, Oracle SQL developer.\n\u2022 Participated in JAD session with Business Users and Sponsors to understand and document the business requirements in alignment to the financial goals of the company.\n\u2022 Experienced in gathering/analyzing the business/user requirements, analyzing the source/target dependencies, root cause & Production Troubleshooting.\n\u2022 Participated in brain storming sessions with application developers and DBAs to discuss about various de-normalization, Partitioning and Indexing Strategies for Physical Model.\n\u2022 Created the Conceptual Model for the Data Warehouse using Erwin Data Modeling tool.\n\u2022 Reviewed the Conceptual EDW (Enterprise Data Warehouse) Data Model with Business Users, App Development and Information Architects to make sure all the requirements are fully covered.\n\u2022 Identified the Facts & Dimensions Tables and established the Grain of Fact for Dimensional Models.\n\u2022 Worked on Requirements Traceability Matrixto trace the business requirements back to Logical Model.\n\u2022 Experienced in designing the data marts using the Ralph Kimball and Inmon's dimensional data mart modeling techniques.\n\u2022 Experience in Informatica data quality and analytics tools.\n\u2022 Responsible for Dimensional Data Modeling, Semantic Data Modeling and Modeling Diagrams using ERWIN.\n\u2022 Managed full Software Development Life Cycle (SDLC) processes involving requirements management, workflow analysis, source data analysis, data mapping, metadata management, data quality, testing strategy and maintenance of the model.\n\u2022 Reviewed the Physical Data Model with Application Developers, ETL Team, DBAs and Testing Team to provide information about the Data Model and business requirements.\n\u2022 Performed ETL SQL optimization designed OLTP system environment and maintained documentation of Metadata.\n\u2022 Maintained database architecture and metadata that support the Enterprise Data Warehouse (EDW)\n\u2022 Designed and Developed Oracle PL/SQL Procedures and UNIX Shell Scripts for Data Import/Export and Data Conversions.\n\u2022 Performed Data analysis and Data profiling using complex SQL on various sources systems including Oracle and Teradata\n\u2022 Performed Data modeling for an existing Databases using Toad Data Modeler and ER Studio\n\u2022 Designed and developed star schema, snowflake schema and created fact tables and dimension tables for the warehouse and data marts using Erwin.\n\u2022 Worked on creating few Tableau dashboard reports.\n\u2022 Created Multi-Way Aggregate Fact Tablesas a specific summarization across dimensions of product, region and date.\n\u2022 Worked on creation of metadata collection and validation using 3NF.\n\u2022 Created Snowflake Schemas by normalizing the dimension tables as appropriate, and creating a Sub Dimensionnamed Demographic as a subset to the Customer Dimension.\n\u2022 Created refined data models pertaining to reassessed business requirements conforming to applicable data standards and successfully documenting the existing system model and changes proposed/applied to the model.\n\u2022 Developed Scripts that automated DDL and DML statements used in creations of Databases, Tables, Constraints, and updates.\n\u2022 Worked in generating and documenting Metadata while designing OLTP and OLAP systems environment.\n\u2022 Incorporated business requirements in quality conceptual, logical data models using Embarcadero ER Studio and created physical data models using forward engineering techniques to generate DDL scripts in agile environment.\n\u2022 Extensively used Metadata & Data Dictionary Management, Data Profiling, Data Mapping.\n\u2022 Applied Data Governance rules (primary qualifier, class words and valid abbreviation in Table name and Column names).\n\u2022 Worked on the Snow-flaking the Dimensions to remove redundancy.\n\u2022 Involved in capturing Data Lineage, Table and Column Data Definitions, Valid Values and others necessary information in the data models."", u'Data Modeler/Data Analyst\nXAXIS - New York, NY\nFebruary 2014 to March 2016\nEnvironment: Oracle 11g, Erwin r9.1, ER Studio, TOAD, DB2, Tableau, PL/SQL, Teradata, MS Office, MS Access, CSV files, UML.\n\u2022 Troubleshooting, resolving and escalating Data related issues and validating Data to improve Data quality.\n\u2022 Revisited the business rules and redefined many attributes and relationships in the model and cleansed unwanted tables and columns as part of Data Analysis tasks.\n\u2022 As a Data Modeler/Analyst generated Data Models using Erwin and developed relational database system and performing root cause analysis.\n\u2022 Used Reverse Engineering and Forward Engineering techniques on databases and created tables and models in data mart, data warehouse and staging.\n\u2022 Involved in design and conversion of business requirements to Conceptual Data Model, logical Data Model and Physical Data Model for subject areas.\n\u2022 Worked with Data Modeling team to create Logical/Physical models for Enterprise Data Warehouse.\n\u2022 Responsible for different data mapping activities from Source systems to Teradata.\n\u2022 Created a logical design and physical design in Erwin.\n\u2022 Maintained security and data integrity of the database.\n\u2022 Worked on creation of metadata collection and validation using 3NF.\n\u2022 Performed Semantic Modeling and Metadata Management.\n\u2022 Experience in PL/SQL programming Stored Procedures, Functions, Packages and Triggers.\n\u2022 Handled performance requirements for databases in OLTP and OLAP models\n\u2022 Data modeling and design of for data warehouse and data marts in star schema methodology with Dimensions and Fact tables.\n\u2022 Involved with all the phases of Software Development Life Cycle (SDLC) methodologies throughout the project life cycle.\n\u2022 Performed Data modeling for an existing Databases using Toad Data Modeler and ER Studio\n\u2022 Excellent experience and knowledge on data warehouse concepts and dimensional data modeling using Ralph Kimball methodology.\n\u2022 Conducting data profiling and data analysis on source and target systems.\n\u2022 Performed Exploratory DataVisualizations using Tableau.\n\u2022 Identified the business function activities and processes, data attributes, and table metadata, and documented detailed design specifications.\n\u2022 Run batch jobs for loading database tables from Flat Files using SQL*Loader.\n\u2022 Extensively used Erwin for developing data model using star schema methodologies.\n\u2022 Created Entity relationship diagrams (ERD), Function relationship diagrams (FRD), data flow diagrams (DFD) and enforced all referential integrity constraints using Erwin.', u'Data Modeler/Data Analyst\nSynchrony Financial - Stamford, CT\nNovember 2011 to January 2014\nEnvironment: Microsoft SQL Server, Oracle 11g, PL/SQL, ERWIN r9, Informatica Power Center 9.5, Tableau, Teradata, DB2, Visio, SQL Assistant, ER Studio, Linux.\n\u2022 Gathered Business requirements by organizing and managing meetings with business stake holders, Application architects, Technical architects and IT analysts on a scheduled basis.\n\u2022 Analyzed the business requirements by dividing them into subject areas and understood the dataflow within the organization.\n\u2022 Created a Data Mapping document after each assignment and wrote the transformation rules for each field as applicable.\n\u2022 Developing the Conceptual Data Models, Logical data models and transformed them to creating schema using ERWIN.\n\u2022 Created and maintained Metadata, including table, column definitions.\n\u2022 Performed Data Profiling and Data Quality.\n\u2022 Wrote SQL queries and optimizing the queries in Oracle, SQL Server and Teradata.\n\u2022 Extensively used SQL for Data Analysis.\n\u2022 Created PL/SQL packages and Database Triggers and developed user procedures and prepared user manuals for the new programs.\n\u2022 Created and reviewed the conceptual model for the EDW (Enterprise Data Warehouse) with business users.\n\u2022 Created action filters, parameters and calculated sets for preparing dashboards and worksheets in Tableau.\n\u2022 Designed Informatica ETL Mappings documents, created ETL staging area framework, created data mapping documents, data flow diagram, ETL test scripts.\n\u2022 Worked on Software Development Life Cycle (SDLC) with good working knowledge of testing, Agile methodology, disciplines, tasks, resources and scheduling.\n\u2022 Normalized the database based on the new model developed to put them into the 3NF of the data warehouse.\n\u2022 Performed dimensional Modeling on OLAP system using Ralph Kimball methodologies.\n\u2022 Performed data modeling to differentiate between OLTP and Data Warehouse data models.\n\u2022 Created DDL scripts for implementing Data Modeling changes. Created ERWIN reports in HTML, RTF format depending upon the requirement.\n\u2022 Published Data model in model mart, created naming convention files, co-coordinated with DBAs to apply the data model changes.\n\u2022 Used Model Mart of ERWIN r9 for effective model management of sharing, dividing and reusing model information and design for productivity improvement.\n\u2022 Used ERWIN r9 for reverse engineering to connect to existing database and ODS to create graphical representation in the form of Entity Relationships and elicit more information.\n\u2022 Helped the BI, ETL Developers, Project Manager and end users in understanding the Data Model, data flow and the expected output for each model created.\n\u2022 Involved in OLAP model based on Dimension and FACTS for efficient loads of data based on Star Schema structure on levels of reports using multi-dimensional models such as Star Schemas and SnowFlake Schema.\n\u2022 Gained Comprehensive knowledge and experience in process improvement, normalization/de-normalization, data extraction, data cleansing, data manipulation.\n\u2022 Created, documented and maintained logical and physical database models in compliance with enterprise standards and maintained corporate metadata definitions for enterprise data stores within a metadata repository.\n\u2022 Extensively used ERWIN r9 as the main tool for data modeling along with MS VISIO.\n\u2022 Applied data naming standards, created the data dictionary and documented data model translation decisions and also maintained DW metadata.\n\u2022 Extensively used SQL for performance tuning.\n\u2022 Used Star Schema and Snow flake schema methodologies in building and designing the logical data model in the dimensional Models.\n\u2022 Performed gap analysis between the present data warehouse to the future data warehouse being developed and identified data gaps and data quality issues and suggested potential solutions.\n\u2022 Assisted developers, ETL, BI team and end users to understand the data model.\n\u2022 Analysing raw data, concluding, developing recommendations and manipulate data for data loads and extracts.', u'Data Analyst\nNorton Healthcare - Louisville, KY\nJanuary 2010 to October 2011\nEnvironment: ERWIN r4.0, Windows NT, MS Excel, MS Visio, DB2, Oracle 10g, XML files, Agile SCRUM, PL/SQL, SCRUM/XP.\n\u2022 Conducted JAD sessions, wrote meeting minutes and also documented the requirements.\n\u2022 Collected requirements from business users and analyzed based on the requirements.\n\u2022 Designed and built Data marts by using Star Schema.\n\u2022 Involved in designing Context Flow Diagrams, Structure Chart and ER- diagrams.\n\u2022 Extensive system study, design, development and testing were carried out in the Oracle environment to meet the customer requirements.\n\u2022 Developed the logical data models and physical data models that capture current state/future state data elements and data flows using ERwin.\n\u2022 Worked as part of a team of Data Management professionals supporting a Portfolio of development projects both regional and global in scope.\n\u2022 Performed various data analysis at the source level and determined the key attributes for designing of Fact and Dimension tables using star schema for an effective Data Warehouse and Data Mart.\n\u2022 Worked on PL/SQL collections, index by table, arrays, bulk collect, etc.\n\u2022 Working Knowledge of Software Development Life Cycle (SDLC)\n\u2022 Worked on slowly changing dimensions (SCD) and hierarchical dimensions\n\u2022 Normalized the tables up to 3NF (Third Normal Form)\n\u2022 Involved in defining the source to target Data mappings, business rules and Data definitions.\n\u2022 Applied organizational best practices to enable application project teams to produce data structures that fully meet application needs for accurate, timely, and consistent data that fully meets its intended purposes.\n\u2022 Conducted peer reviews of completed data models and plans to ensure quality and integrity from Data Cleansing through usage and archiving.\n\u2022 Reversed engineered existing data bases to understand the data flow and business flows of existing systems and to integrate the new requirements to future enhanced and integrated system.', u'OBIEE Developer\nAurobindo Pharma - Hyderabad, Telangana\nAugust 2008 to November 2009\nEnvironment: Siebel Analytics 7.8, Informatica 7.8, DAC, UNIX Platform, ITSM , Putty.\n\u2022 Interacting with business users and gathering the business requirement for Application Change Requests.\n\u2022 Developed custom reports/Ad-hoc reports using Siebel Analytics and assigned them to application specific dashboards\n\u2022 Configured Dashboard and page prompts for various departments supporting the global organization structure and configured drill down reports to provide insight into Sales.\n\u2022 Configured iBots to deliver Analytics content based on schedule and send alerts to related managers and secured team members.\n\u2022 Created dimension hierarchies aggregates and logical sources in the BMM layer\n\u2022 Developed different kinds of Reports (pivots, charts).\n\u2022 Implemented Security Groups and Roles using LDAP Authentication.\n\u2022 Created materialized views and aggregates to improve performance.\n\u2022 Used Siebel Analytics Answers to develop Ad-hoc and standard Reports / Dashboards with various Analytics Views (Drill-Down / Dynamic, Pivot Table, Chart, Column Selector, Tabular with global and local Filters)\n\u2022 Created Initialization Blocks, Repository Variables and IBots\n\u2022 Troubleshooting for Data discrepancy issues reported by Users. This involves a back tracking of data from Report to ETL and then to the source Siebel applications.\n\u2022 Handling Change requests reported by user groups.\n\u2022 Contributed in Deployment activities.\n\u2022 Communicating and interacting with the Users and external teams to resolve tickets(ITSM) as part of production support.\n\u2022 Contributed to activities in DAC (Data Warehouse Application Console).']","[u""Bachelor's""]",[u'']
0,https://resumes.indeed.com/resume/f2a869ee3f3bee60,"[u""Data Analyst Intern\nNASA Space Grant Consortium - Tucson, AZ\nJanuary 2014 to January 2015\nCoordinated with 6 selected students over a year period to successfully complete a NASA\nfunded experiment at PCC.\n\u2022 Advised team members as the group's Data Analyst specialist, and accelerated the completion by organizing/quantifying information.\n\u2022 Publicly presented project findings during two large NASA funded events in Arizona.\n\u2022 Successfully collected various atmospheric particles (reaching upwards of ~90,000 feet),\nwhich returned to earth with secure data.""]","[u'Bachelor in Science in Computer Science in Coursework include Computer Organization', u'Associates in Science in Mathematics']","[u'University of Arizona\nAugust 2014 to May 2019', u'Pima Community College\nAugust 2012 to May 2014']"
0,https://resumes.indeed.com/resume/de88e5f0f941c9f9,"[u'Data Analyst\nXXXX - Alpharetta, GA\nSeptember 2015 to Present\nProject: Affordable Care Act Management Protocol (ACAMP)\n\n\u2022 Expertise in configuring the instance in eThority tool as per the Configuration Acceptance document approved by Business Analyst and Activation Manager. Experience in Configuring the CORE/IRS Instance that involves data files with single feed and multiple feeds, single control group or multiple control group, at the same time preparing Instance Verification document for all the CORE/IRS instances that has different version of Specification files created by Business Analyst as per the Client requirement for every individual instances in order to check the eligibility of the employee for the 1095C form.\n\u2022 Experience in modifying the instance and purging (i.e. Clearing the data from the database) the data required by the specific client to get the clean validation pass for the files submitted.\n\u2022 Ability to analyze the data by performing Extraction, Transformation & Load function mentioned by clients to process and get the accurate feedback that includes cause for warnings & failures. Ability to explain the cause for the loss of records when the data moves from import table to Transformation table.\n\u2022 Ability to analyze the problems occurring with the clients data by verifying the contents of the file in order to make it work and provide the accurate feedback by providing the exact cause of the problem along with its fix. Ability to analyze data in the database SQL Server by running SQL queries & troubleshoot the issues related to the clients data.\n\u2022 Experience in performing the SME (Subject Matter Expert) review done by Business Analyst & Activation Manager by thoroughly verifying list of questionnaire & updating the same and inform Business Analyst and Activation Manager about the necessary actions to be taken to resolve the issue.\n\u2022 Experience in performing Regression testing i.e. finding the loopholes or bug in the tool by building up the test data as per the test cases which is then sent to development team to develop a software patch for that, in order to fix the issue. Experience working with large amounts of data: facts, figures, and number crunching. Ability to see through the data and analyze it to find conclusions. Experience in working as Quality Analyst by reviewing the task done by Data analyst. Experience in mentoring the new joinees about the project and training them.\n\u2022 Ability to present findings, or translate the data into an understandable document. Expertise to write and speak clearly, easily communicating complex ideas. Ability to look at the numbers, trends, and data and come to new conclusions based on the findings.\n\u2022 Ability to understand current business processes and implement efficient business process.\n\u2022 Expertise in defining scope of projects based on gathered Business Requirements including documentation of constraints, assumptions, business impacts & project risks. Strong background in support documentation. Analysis and review of Business Requirement Documents.\n\u2022 Conducting requirement gathering sessions, feasibility studies and organizing the business requirements in a structured way.\n\u2022 Gathering business and Technical requirements that would best suit the needs of the technical architectural development process.\nEnvironment: Ethority(BI Tool), SQL Server, MS-Office, Tableau, Teradata ,SQL', u""Data Analyst/Business Analyst\nXXXXXXXX - Longview, WA\nFebruary 2014 to August 2015\nProject: New Customer Engagement (NCE)\n\n\u2022 Performed data profiling in the source systems that are required for New Customer Engagement (NCE) Data mart.\n\u2022 Document the complete process flow to describe program development, logic, testing, and implementation, application integration, coding.\n\u2022 Manipulating, cleansing & processing data using Excel, Access and SQL.\n\u2022 Responsible for loading, extracting and validation of client data.\n\u2022 Liaising with end-users and 3rd party suppliers. Analyzing raw data, drawing conclusions & developing recommendations Writing SQL scripts to manipulate data for data loads and extracts.\n\u2022 Developing data analytical databases from complex financial source data. Performing daily system checks. Data entry, data auditing, creating data reports & monitoring all data for accuracy. Designing, developing and implementing new functionality.\n\u2022 Monitoring the automated loading processes. Advising on the suitability of methodologies and suggesting improvements. Involved in defining the source to target data mappings, business rules, business and data definitions. Responsible for defining the key identifiers for each mapping/interface.\n\u2022 Responsible for defining the functional requirement documents for each source to target interface.\n\u2022 Document, clarify, and communicate requests for change requests with the requestor and coordinate with the development and testing team. Reverse engineered all the Source Database's using Embarcadero.\n\u2022 Coordinate with the business users in providing appropriate, effective and efficient way to design the new reporting needs based on the user with the existing functionality.\n\u2022 Document data quality and traceability documents for each source interface.\n\u2022 Designed and implemented data integration modules for Extract/Transform/Load (ETL) functions.\n\u2022 Involved in Data warehouse and Data mart design. Experience with various ETL, data warehousing tools and concepts.\n\u2022 Documented the complete process flow to describe program development, logic, testing, and implementation, application integration, coding.\n\u2022 Worked with internal architects and, assisting in the development of current and target state data architectures.\n\u2022 Worked with project team representatives to ensure that logical and physical ER/Studio data models were developed in line with corporate standards and guidelines.\n\u2022 Also worked on some impact of low quality and/or missing data on the performance of data warehouse client. Identified design flaws in the data warehouse.\n\u2022 Designed application components in an Agile environment utilizing a test driven development approach.Created and maintained project tasks and schedules.\n\u2022 Provided task estimates, identified potential problems and recommended alternative solutions.\n\u2022 Worked in close cooperation with project managers and other functional team members to form a team effort in development. Collaborated with other members of the product development team.\n\u2022 Coordinated configuration of back-end components in support of application development.\nEnvironment: SQL/Server, Oracle10 &11g, MS-Office, Netezza, Teradata ,Enterprise Architect, Informatica Data Quality, ER Studio,TOAD, Business Objects, Greenplum Database,PL/SQL."", u'Data Analyst/Business Analyst\nXXXXXXX\nDecember 2013 to February 2014\n\u2022 Work with users to identify the most appropriate source of record required to define the asset data for financing\n\u2022 Perform data profiling in the source systems that are required for financing\n\u2022 Document the complete process flow to describe program development, logic, testing, and implementation, application integration, coding.\n\u2022 Involved in defining the trumping rules applied by Master Data Repository\n\u2022 Define the list codes and code conversions between the source systems and MDR.\n\u2022 Worked with internal architects and, assisting in the development of current and target state enterprise data architectures\n\u2022 Worked with project team representatives to ensure that logical and physical ER/Studio data models were developed in line with corporate standards and guidelines\n\u2022 Involved in defining the source to target data mappings, business rules, business and data definitions\n\u2022 Responsible for defining the key identifiers for each mapping/interface\n\u2022 Responsible for defining the functional requirement documents for each source to target interface.\n\u2022 Document, clarify, and communicate requests for change requests with the requestor and coordinate with the development and testing team.\n\u2022 Created the DDL scripts using ER Studio and source to target mappings (S2T- for ETL) to bring the data from JDE to the warehouse.\n\u2022 Created the dimensional logical model with approximately 10 facts, 30 dimensions with 500 attributes using ER Studio.\n\u2022 Involved in configuration management in the process of creating and maintaining an up-to-date record of all the components of the development efforts in coding and designing schemas\n\u2022 Developed the financing reporting requirements by analyzing the existing business objects reports\n\u2022 Interact with computer systems end-users and project business sponsors to determine, document, and obtain signoff on business requirements.\n\u2022 Responsible in maintaining the Enterprise Metadata Library with any changes or updates\n\u2022 Document data quality and traceability documents for each source interface\n\u2022 Establish standards of procedures. Generate weekly and monthly asset inventory reports.\n\u2022 Coordinate with the business users in providing appropriate, effective and efficient way to design the new reporting needs based on the user with the existing functionality\n\u2022 Remain knowledgeable in all areas of business operations in order to identify systems needs and requirements.\n\u2022 Provided direction and shares knowledge with and mentored team members in areas of expertise.\n\u2022 Identified and continuously acted to improve individual and team knowledge of new technologies, business processes and project management skills.\n\u2022 Stayed current on trends, latest industry developments and shared knowledge among colleagues.\n\u2022 Excellent skills in user research and analysis of the existing systems, with knowledge of traceability matrix.\n\u2022 Strong Experience in conducting User Acceptance Testing (UAT), Unit Testing and documenting Test Cases and Test Scripts.\n\u2022 Involved in Test Planning, Test Preparation, Test Execution, Issue resolution and Report Generation to assure that all aspects of a project are in compliance with the business requirements.\n\u2022 Excellent verbal and written communication skills with the ability to interact professionally with stakeholders at all levels of the organization, including the ability to exercise good judgment in frequency and nature of communications to senior management, stakeholders and team members.\n\u2022 Possess a disciplined, professional and quality centered approach with strong analytical and problem solving skills.\n\nEnvironment: SQL/Server, Oracle 9i, MS-Excel, Teradata, Informatica, ER Studio, XML, Business Objects.', u""Data Analyst\nKaiser Permanente - Pasadena, CA\nJune 2012 to December 2013\nWMS (Warehouse Management System) &CRP (Central Refill Pharmacy)\n\n\u2022 Creation of Teradata BTEQ, fastLoad, Fast export scripts and UNIX shell scripts to perform the fulfillments Creation of Tables, Views to fulfill the business needs and support the Production systems with day to day activities and support the performance issues.\n\u2022 Developed SQL, BTEQ (Teradata) queries for Extracting data from production database and built data structures, reports.\n\u2022 Used the MS Excel, MS Access for data pulls and ad hoc reports for analysis.\n\u2022 Performance tuning and Optimization of large database for fast accessing data and reports Ms Excel\n\u2022 Unix, Oracle, Teradata being used for data transformation\n\u2022 Reports generated for various departments like Tele Marketing, Mailing, New Accounts by using SQL, BTEQ, Ms Access, Unix\n\u2022 Ad-hoc reports developed using Oracle, SQL, and UNIX.\n\u2022 Performed in depth analysis in data & prepared weekly, biweekly, monthly reports by using SQL, SAS, Ms Excel, Ms Access, and UNIX.\n\u2022 Extensively used excel and VBA to generate reports\n\u2022 Created and manipulated datasets using SAS, Access, and Excel.\n\u2022 Wrote JCL programs to communicate with the MVS (TSO, ISPF) operating system.\n\u2022 Designed and implemented SQL queries for QA Testing and Report / Data Validation\n\u2022 Assisted in preparing BRDs, FRDs, and Test Cases.\n\u2022 Design Extract Transform load (ETL) procedures for data import and Manipulation.\n\u2022 Worked with data modelers and assist them in verifying and validating the relationships between tables using Teradata SQL assistant.\n\u2022 Execution, Maintenance & Debugging of the scripts with Teradata Bteq.\n\u2022 Execution, Maintenance & Debugging of SAS scripts to update the Teradata Tables\n\u2022 Creat Teradata tables, Views, Macros and analyzed various Teradata tables for UPI's and Monitor the same. Execution of Mainframes JCL scripts.\n\u2022 Utilized ODBC for connectivity to Teradata & MS Excel.\n\u2022 Good experience in identification and requirements gathering as per the business requirements via meetings, interviews, interface analysis, research, etc.\n\u2022 Experience in documentation of business requirements and system functional specifications in the form of Use Cases.\n\u2022 Utilized a combination of business knowledge, technical skills and strategic analysis to provide solutions and creative insights to critical business problems.\n\u2022 Maintained channels of organizational communication and acted as the point of contact between teams.\nEnvironment: Teradata V2R5, Informatica 6.2.1, Ab Initio, Business Objects, Oracle 9i, PL/SQL, Microsoft Office Suite (Excel, Vlookup, Pivot, Access, Power Point), Visio, VBA, Micro Strategy, Tableau ,UNIX Shell Scripting ERWIN."", u""Data Analyst\nXXXXXX, NJ\nJanuary 2011 to June 2012\nResponsibilities:\n\u2022 Stored reformatted data from different Mainframe applications using Informatica (ETL).\n\u2022 Translated business-reporting requirements into data warehouse architectural design.\n\u2022 Designed and maintained logical and physical enterprise data warehouse schemas\n\u2022 Designed, developed and deployed new data marts along with modifying existing\n\u2022 Marts to support additional business requirements.\n\u2022 Extracted business customer product/revenue data from Express Track systems and consumer product/revenue data. Created naming and system standards for lookup, transformation and target tables. Loaded consolidated data using SQL*Loader in parallel and direct mode.\n\u2022 Used Business Objects Designer, Reporter and Supervisor.\n\u2022 Scheduled and monitored transformation processes using Informatica Server Manager.\n\u2022 Designed, developed and implemented universes using Business Objects and Web intelligence.\n\u2022 Maintained stored definitions, transformation rules and targets definitions using Informatica repository manager. Generated reports for end client using Query tools.\n\u2022 Involved in meeting with Project owners and project managers to define the scope of the project.\n\u2022 Followed the Agile methodology for elicitation and representation of requirements based on interaction with the Project owner, SME's and the development team, participated in daily scrum.\n\u2022 Facilitated requirement sessions, including creation of agendas and recording of action items from meetings and writing meeting minutes.\n\u2022 Documented the requirements in the form of detailed use cases and sent out the same for inspection to the team.\n\u2022 Responsible for creating Use case diagrams and process flow diagrams by using tools like MS-Visio.\n\u2022 Involved in preparing test cases and also involved in reviewing test plans and test scripts developed by QA team to make sure that all requirements are covered in scripts and tested properly.\nEnvironment:\nInformatica ETL V1.7, ER-Win, Shell Scripts, SQL*Loader, Business Objects 5.0, Web Intelligence 2.0, BigData, Oracle 7.x/8.0, Windows NT"", u'Data Analyst\nXXXXX - IN\nJuly 2008 to September 2010\nResponsibilities:\n\u2022 Involved in the design of the overall database using Entity Relationship diagrams.\n\u2022 Wrote triggers, menus and stored procedures in PL/SQL.\n\u2022 Involved in developing interactive forms and customization of screens using Forms 4.5.\n\u2022 Involved in building, debugging and running forms.\n\u2022 Involved in Data loading and Extracting functions using SQL*Loader.\n\u2022 Designed and developed all the tables, views for the system in Oracle.\n\u2022 Designing and developing forms validation procedures for query and update of data.\n\nEnvironment: Oracle 8.0, SQL*plus, SQL*Loader, PL/SQL, Forms 4.5, Reports 2.5', u'Data Analyst\nAlpharetta, GA\n\u2022 Expertise in configuring the instance in eThority tool as per the Configuration Acceptance document approved by Business Analyst and Activation Manager. Experience in Configuring the CORE/IRS Instance that involves data files with single feed and multiple feeds, single control group or multiple control group, at the same time preparing Instance Verification document for all the CORE/IRS instances that has different version of Specification files created by Business Analyst as per the Client requirement for every individual instances in order to check the eligibility of the employee for the 1095C form.\n\u2022 Experience in modifying the instance and purging (i.e. Clearing the data from the database) the data required by the specific client to get the clean validation pass for the files submitted.\n\u2022 Ability to analyze the data by performing Extraction, Transformation & Load function mentioned by clients to process and get the accurate feedback that includes cause for warnings & failures. Ability to explain the cause for the loss of records when the data moves from import table to Transformation table.\n\u2022 Ability to analyze the problems occurring with the clients data by verifying the contents of the file in order to make it work and provide the accurate feedback by providing the exact cause of the problem along with its fix. Ability to analyze data in the database SQL Server by running SQL queries & troubleshoot the issues related to the clients data.\n\u2022 Experience in performing the SME (Subject Matter Expert) review done by Business Analyst & Activation Manager by thoroughly verifying list of questionnaire & updating the same and inform Business Analyst and Activation Manager about the necessary actions to be taken to resolve the issue.\n\u2022 Experience in performing Regression testing i.e. finding the loopholes or bug in the tool by building up the test data as per the test cases which is then sent to development team to develop a software patch for that, in order to fix the issue. Experience working with large amounts of data: facts, figures, and number crunching. Ability to see through the data and analyze it to find conclusions. Experience in working as Quality Analyst by reviewing the task done by Data analyst. Experience in mentoring the new joinees about the project and training them.\n\u2022 Ability to present findings, or translate the data into an understandable document. Expertise to write and speak clearly, easily communicating complex ideas. Ability to look at the numbers, trends, and data and come to new conclusions based on the findings.\n\u2022 Ability to understand current business processes and implement efficient business process.\n\u2022 Expertise in defining scope of projects based on gathered Business Requirements including documentation of constraints, assumptions, business impacts & project risks. Strong background in support documentation. Analysis and review of Business Requirement Documents.\n\u2022 Conducting requirement gathering sessions, feasibility studies and organizing the business requirements in a structured way.\n\u2022 Gathering business and Technical requirements that would best suit the needs of the technical architectural development process.', u'Data Analyst/Business Analyst\nLongview, WA\n\u2022 Performed data profiling in the source systems that are required for New Customer Engagement (NCE) Data mart.\n\u2022 Document the complete process flow to describe program development, logic, testing, and implementation, application integration, coding.\n\u2022 Manipulating, cleansing & processing data using Excel, Access and SQL.\n\u2022 Responsible for loading, extracting and validation of client data.\n\u2022 Liaising with end-users and 3rd party suppliers. Analyzing raw data, drawing conclusions & developing recommendations Writing SQL scripts to manipulate data for data loads and extracts.\n\u2022 Developing data analytical databases from complex financial source data. Performing daily system checks. Data entry, data auditing, creating data reports & monitoring all data for accuracy. Designing, developing and implementing new functionality.\n\u2022 Monitoring the automated loading processes. Advising on the suitability of methodologies and suggesting improvements. Involved in defining the source to target data mappings, business rules, business and data definitions. Responsible for defining the key identifiers for each mapping/interface.\n\u2022 Responsible for defining the functional requirement documents for each source to target interface.\n\u2022 Document, clarify, and communicate requests for change requests with the requestor and coordinate with the development and testing team. Reverse engineered all the Source Database\u2019s using Embarcadero.\n\u2022 Coordinate with the business users in providing appropriate, effective and efficient way to design the new reporting needs based on the user with the existing functionality.\n\u2022 Document data quality and traceability documents for each source interface.\n\u2022 Designed and implemented data integration modules for Extract/Transform/Load (ETL) functions.\n\u2022 Involved in Data warehouse and Data mart design. Experience with various ETL, data warehousing tools and concepts.\n\u2022 Documented the complete process flow to describe program development, logic, testing, and implementation, application integration, coding.\n\u2022 Worked with internal architects and, assisting in the development of current and target state data architectures.\n\u2022 Worked with project team representatives to ensure that logical and physical ER/Studio data models were developed in line with corporate standards and guidelines.\n\u2022 Also worked on some impact of low quality and/or missing data on the performance of data warehouse client. Identified design flaws in the data warehouse.\n\u2022 Designed application components in an Agile environment utilizing a test driven development approach.Created and maintained project tasks and schedules.\n\u2022 Provided task estimates, identified potential problems and recommended alternative solutions.\n\u2022 Worked in close cooperation with project managers and other functional team members to form a team effort in development. Collaborated with other members of the product development team.\n\u2022 Coordinated configuration of back-end components in support of application development.']","[u'Masters in Computer Engineering in Computer Engineering', u'Bachelors in Electronics Engineering in Electronics Engineering', u""Bachelor's in Computer Science""]","[u'Mumbai University Mumbai, Maharashtra\nFebruary 2006', u'Mumbai University Mumbai, Maharashtra\nJune 2001', u'']"
0,https://resumes.indeed.com/resume/7cad27bb50c0f304,"[u'Data Analyst/BI Analyst\nSiriusxm, NJ\nAugust 2017 to Present', u""Data Analyst/BI Analyst\nEXECA, DE\nMay 2016 to June 2017\nEXECA is a Professional Services Automation software is an On Demand, integrated front office and back office solution with an adaptable business architecture and far-reaching functionality, meeting demanding needs of today's organizations. EXECA improves all aspects of your business: workflow, tracking and controlling. Easy-to-implement, web-based solution increases organizational efficiency, maximizes resource utilization, facilitates team collaboration, fosters individual responsibility and productivity.\nResponsibilities:\n\u2022 Actively involved as part of a team for gathering and analyzing the needs of End User Requirement and System Specifications.\n\u2022 Prepare dashboards using calculated fields, parameters, calculations, groups, sets and hierarchies in Tableau.\n\u2022 Extensively used advance chart visualizations in Tableau like Dual Axis, Box Plots, Bullet Graphs,\n\u2022 Tree maps, Bubble Charts, Water Fall charts, funnel charts etc., to assist business users in\nSolving complex problems.\n\u2022 Utilized Tableau visual analytics and best practices like Dashboard structure, view orientation, Sizing and Layout, Data Emphasis, Highlighting, Color, Fonts, Tooltips, and performance improvement of complex dashboards/reports\n\u2022 Designed, Developed and modify Interactive Dashboards and Creating guided navigation links within Interactive Dashboards.\n\u2022 Created and Presented Agenda's, BRD's, Functional Specifications, Project task lists and plans\n\u2022 Regularly conducted Knowledge transfer sessions with Analysts, Developers, QA's\n\u2022 Responsible for running Daily, Weekly, Monthly reports from both SQL Server and Oracle databases\n\u2022 Primary contact for one of the Vendor regarding compliance and data related check points for one of the process\n\u2022 Downloaded data sets from other databases into MS-Excel spreadsheet. This data was then imported into MS-Access database tables, created queries and Reports including graphical representation of data.\n\u2022 Utilized Advanced Excel skills (pivot tables, graphs, vlookup, if statements) in working with Excel.\n\u2022 Utilized VLOOKUP and Pivot table to perform detailed analysis of Human Resources data.\n\u2022 Generate context filters and use performance actions while handling huge volume of data.\n\u2022 Involved in Tableau Server Installation and Configuration, creating users, groups, and projects and assigning access levels.\n\u2022 Monitored user activity on a weekly basis for benchmarking adaptability and usability of reports.\n\u2022 Extensively used Tab admin and Tab cmd commands in creating backups and restoring backups of Tableau repository.\nEnvironment: Tableau (Desktop/Server), SQL Server, Java API, Microsoft office Suite (Word, Excel, PowerPoint)"", u""Data Analyst\nHealth Spring - Nashville, TN\nAugust 2015 to March 2016\nHealth Spring is now a CIGNA owned subsidiary Company which is one of the Country's Largest and fastest growing coordinated care plans whose primary focus is Medicare Advantage Plans. My role is to initiate and handle processes and projects and provide data analytics from a Business analyst perspective.\nResponsibilities:\n\u2022 Conducted JAD Sessions with the Business, IT and SME's and documented the requirements for various projects\n\u2022 Created and Presented Agenda's, BRD's, Functional Specifications, Project task lists and plans\n\u2022 Regularly conducted Knowledge transfer sessions with Analysts, Developers, QA's and provided updates to the Directors\n\u2022 Responsible for running Daily, Weekly, Monthly reports from both SQL Server and Oracle databases\n\u2022 Primary contact for one of the Vendor regarding compliance and data related check points for one of the process\n\u2022 Developed various automated and customized Dashboards along with improved template formats for various data metrics as part reporting for executive directors\n\u2022 Strong knowledge and implementation of HIPAA guidelines and framework of rules for various initiatives including working on frequent audit requests for government records\n\u2022 Performed statistical analysis using SQL, Access & Excel tools for various analytical and reporting tasks on various kinds of data\n\u2022 Regularly conducted working sessions with Analysts, Developers, QA's and provided updates to the Directors\n\u2022 Worked on Medicare contract plans, prescription drug plans, drug claims and benefits and overall management of provider and pharmacy services\n\u2022 Worked with the Business and Marketing team and generated various reports, charts, Pie diagrams, graphs using sales, timeliness, complaints and other data at county level for each state\n\u2022 Strong Domain knowledge of member benefits , pharmacy benefits, claims processing, Medicaid/Medicare programs, physician contract, Medical billing codes, ICD codes,( 9 & 10), MMIS, Part A to Part D and compliance guidelines\n\u2022 Scrum Master for Two internal projects both involving automating various manual processes in AGILE mode in order to improve timely delivery and quality\n\u2022 Responsible for maintaining the Data warehouse by utilizing proper ETL procedures, Staging tables and Stored procedures where entire focus is to have proper formatted data as required\n\u2022 Responsible for creating sales reporting metrics across all markets and providing with improvement solutions which benefitted the individual market sales revenue\n\u2022 Analyzed different kinds of data from many systems and reported various comparisons, trends, statistics, errors and suggestions\n\u2022 Tracking requirements, Continuously monitoring the deliverables, handling day to day issues by escalating them to proper channel are some of the key activities performed from a analyst perspective\n\u2022 Created and handled various ad-hoc queries, SQL scripts, Stored procedures, and worked with QA on some of the testing procedures\n\u2022 Responsible for conducting technical breakout sessions with different groups in order to capture process improvement requirements for some of the systems as per priority list\n\u2022 Maintained Change control process, conducted thorough analysis on various parameters, documented and presented the same to the Managers and Directors\nEnvironment: SQL Server Management Studio, Oracle SQL Developer, Data Stage, MS Project, MS Access, MS Visio, Share Point, Elvis, Market Prominence, MS Office"", u""Data Analyst / Business Analyst\nJP Morgan Chase - Newark, DE\nJanuary 2015 to July 2015\nPPMNRT-Production Process Management near Real time Mortgage Express (MX) is an online transactional processing platform that originates loans and stores information related to current loan pipeline and resources assigned to them. The PPMNRT project is to drive empirical, fact based staffing and performance monitoring of MX Operations staff, so that customer communication will be improved in the loan completion process.\nResponsibilities:\n\u2022 Worked on Phase3 of the overall PPM program which is to ensure data from SOR systems have answers to business monitoring questions and also to make sure data is available in the interim database in hourly cycles for reporting purposes\n\u2022 Responsible for creating several ETL mapping documents (Source to Stage, Stage to Integration) for different SOR's making sure the column load rules, data types, TDQ/BDQ rules are all in place for all the required source data fields\n\u2022 Created several corresponding ISD (Integration Specific Document) which includes interface list (In-bound/Out-bound), detailed file sizes, production support details (SLA's, servers etc.,)\n\u2022 Created functional specific document for the Phase3 work including but not limited to requirements, architectural references, sequence diagrams, data mappings, quality management, use cases and data reconciliation details\n\u2022 Responsible for handling defects and escalations making sure they are addressed within time by updating corresponding documents (DDL, s, Mappings, Model changes etc.,)\n\u2022 Created wellness check scripts (SQL) to make sure the both stage and production environments are getting the data as intended and addressing all the production related issues\nEnvironment: MS Access 2007, MS-Excel 2007, MS Office, SQL Server, Oracle. Windows Family, Script, Javascript, HTML, XML, LINUX"", u'Data Analyst /SQL Developer\nVibertech Solutions Pvt. Ltd - Hyderabad, Telangana\nJanuary 2013 to December 2013\nVibertech Solutions is a global IT services provider which focuses mainly on enterprise integration, customer management, web engineering and collaboration solutions. I worked on different databases to gather the required information.\nResponsibilities\n\u2022 Performed creation, manipulation and support for SQL Server databases.\n\u2022 Involved in creation of tables, indexes, sequences, constraints, functions, triggers, views and joins.\n\u2022 Helped in integration of the front end with the SQL server backend.\n\u2022 Created triggers to enforce data and referential integrity.\n\u2022 Created stored procedures for maintaining SQL server, and also wrote stored procedures for application developers.\n\u2022 Planned the complete backing up of database and restored the database from disaster recovery.\n\u2022 Designed logical and physical data models using ERWIN.\n\u2022 Involved in migration of data (Import & Export of data) from one server to other servers using tools like data transformation services (DTS).\n\u2022 Extensively used SQL profiler for troubleshooting, monitoring, and optimization of SQL Server and non-production database code as well as T-SQL code from developers and QA.\n\u2022 Managing the security of the servers, creating the new logins and users, changing the roles of the users.\nEnvironment: Windows 2000 Server, SQL Server 2000, Query Analyzer, Enterprise Manager, Import and Export, SQL Profiler.', u'Data Analyst\nHealth Care Client\nJuly 2010 to November 2012\niPhysicianHub enables healthcare providers to establish electronic medical office, offer online appointments, collect payments, manage outpatient/inpatient care, expand patient reach, and receive matched patient/provider leads. For patients, iPhysicianHub offers Ideal Health Plan -a health portal and e-locker for storing health records, emergency access card, health & fitness planner, appointment booking, and discounted healthcare services through participating providers.\nResponsibilities:\n\u2022 Developed, analyzed and managed reporting requirements for new clients and reporting enhancements for existing clients.\n\u2022 Modify Reports content and export Reports in multiple formats (Excel, .CSV, tab separated,\nPDF, .RPT etc) based on user input.\n\u2022 All the products of the client- Iphysician Hub were tabulated, synchronized and arranged using a customized Product Information Management System(PIMS).\n\u2022 All the PIMS tables and data was stored in Oracle, MS Access and MS Excel\n\u2022 Worked on performance tuning of all the reports before moving them into production.\n\u2022 Attended the regular client call and discussed the weekly status with the client.\n\u2022 Provided data analysis from both within and outside of department.\n\u2022 Developed reports, interactive dashboards and internal procedures\n\u2022 Coordinated with technical teams for research and evaluation of healthcare and operational problems.\n\u2022 Participated in critical review and revision of existing systems for updates and enhancements.\n\u2022 Tracked and reported upon development activities, required resources, defects reworked and their status, performance baselines\n\u2022 Reviews existing logical and physical data environment to understand ""as is"" environment\n\u2022 Develop and maintain CIM data models (conceptual, logical, and physical master data models)\n\u2022 Design and build data integration/synchronization between current Business Systems, CIM, and Enterprise Data Warehouse\n\u2022 Produces CIM code modules meeting technical specifications and business requirements\n\u2022 Worked with the client directly to understand the requirements clearly and to update the daily status of work\n\u2022 Have sent the Status Report (Daily, Weekly etc.) to the Client\n\u2022 Published the Reports onto server and scheduled the reports to run monthly, quarterly, weekly, daily.\n\u2022 Prepared strategic trend reports to measure KPI, Increased leadership focus on key Performance Indicator\nEnvironment: Oracle 11g Enterprise version, PRO*C, SQL, PQL, SQL Reporting, SSRS, PIMS Data, MS Office, Windows Family, UNIX', u""Database Developer\nBanking Client\nAugust 2008 to June 2010\nDescription: Information Technology Professional with experience in Technical support and Trouble Shooting. Worked at Financial Software & Systems (P) LTD, as Team Member (Implementation) and stationed at ANDHRA BANK. Most of my work has been Onsite Implementation with constant interaction with the customer.\nDCMS is a debit card issuance and management software that manages the lifecycle of ATM and Debit cards. It offers a comprehensive solution for issuance and management, transaction processing, fees and charges, waiver and loyalties. Processing applications for cards; initiating generation of PINs, PIN Mailers; generating embossa and card holder authorization files; maintaining customer and card records; managing replacement of damaged, lost and stolen cards and managing the card renewal cycle. Setting customer and account debit limits and charging fees.\nResponsibilities:\n\u2022 Implementation of the system according to the customer requirement.\n\u2022 Adding new ATMs in the network as and when new ATM's are installed\n\u2022 Used to generated reports for the online payment bills such as TTD and Shirdi temple payments.\n\u2022 Used to generate the report monthly, quarterly, weekly, daily for the ATM's which are not working.\n\u2022 Ensured integrity of data in reporting systems and resolved discrepancies.\n\u2022 Formulated short/long-term plans and key objectives. Worked out measurable plans of action to ensure success.\n\u2022 Provided comprehensive reports for client companies interested in determining presence across the marketplace.\n\u2022 Designed new queries and utilized existing query models to draw relevant customer information for development of financial reports utilized in forecasting, trending, and results analysis.\nEnvironment: Oracle , PL/SQl, PRO*C, Oracle Forms & Reports, MS Access Reporting, MS Excel Reporting, Windows, Legacy systems,""]","[u'Bachelor of Engineering in Engineering', u'Masters in Information Technology & Information Assurance', u'in Management']","[u'JNT University\nJanuary 2008', u'Wilmington University', u'The Data Warehousing Institute']"
0,https://resumes.indeed.com/resume/7c24fbdacbd9dcb9,"[u'Statistical Data Analyst\nUniversity of South Florida\nAugust 2016 to Present\nFlorida Mental Health Institute\nPolicy Services Research Data Center\nProject Name: Medicaid Drug Therapy Management Program for Behavioral Health\n\u2022 Evaluated outcomes experienced by children and adult with severe Mental Illness(SMI) including the incidence of restrictive behavioral health treatment, involuntary examinations and criminal justice encounters that occur before and after plan enrollment\n\u2022 Worked as technical consultant for Agency in developing reports on patient demographics, disease, utilization of health services and calculated medication possession ratios from various claims data\n\u2022 Developed reports on prescriber utilization, distribution of provider type, type of anti-psychotic prescriptions, frequency of high dose prescriptions among mental health patients.\n\u2022 Involved in report writing for the analysis we have done for the agency\n\u2022 Developed SAS code to analyze data from multiple sources like baker act data, healthcare claims data\n\u2022 Identified and tracked high risk Medicaid recipients by developing SAS code to analyze patient, prescriber, plan & provider level data\n\u2022 Processed monthly data MMA encounters data in SAS which again includes Institutional, Professional, Dental claims files\n\u2022 Extracted data from flat files using PROC IMPORT and merged the datasets as per requirement using PROC SQL\n\u2022 Optimized PROC SQL queries for reports which take long time for execution.\n\u2022 Developed SAS/Macros for weekly, monthly and quarterly reports\n\u2022 Good Experience in various SAS procedures and tools like SAS/BASE, SAS/MACROS, SAS/ODS, SAS/ACCESS, SAS/ CONNECT, SAS E Miner, SAS Enterprise Guide and SAS/SQL in Windows.\n\u2022 Proficient in use of various procedures like proc SQL, proc report, proc freq, proc means, proc contents, proc dataset, proc append, proc import, proc export, proc print, proc sort.\n\u2022 Created RTF and PDF formatted reports using SAS/ ODS for presentation to Agency for Healthcare Administration- A government board which administers Medicare and Medicaid in the state of Florida.\n\u2022 Developed Tableau data visualization using pie charts, bar charts\n\u2022 Worked on various federal grants and state of Florida contracts\n\u2022 Developed SAS code to create various Tables and Graphs\n\u2022 Converted SAS data sets in to excel and CSV files using PROC EXPORT\n\u2022 Familiar with ICD 10 codes', u'Associate\nCognizant Technology Solutions\nApril 2016 to August 2016\n\u2022 Worked in Analytics team for a healthcare client\n\u2022 Worked on data analysis of healthcare claims\n\u2022 Developed dashboards using tableau as visualization software\n\u2022 Dealt with different relational databases to pull the data and used SAS for manipulation and Senior Analyst at Merck&Co Feb 2014-Dec 2014\n\u2022 Analyzed market trends using primary and secondary sales data\n\u2022 Involved in Data extraction from the application system and rectifying the data, Data blending as per the business requirements\n\u2022 Transformed and manipulated data for building models\n\u2022 Applied statistical data analytics and data mining as per the needs.\n\u2022 Presented statistical methodology and convinced the higher management change the prices of critical care drugs', u""Analyst\nGlaxoSmithKline Pharmaceuticals Limited\nOctober 2011 to February 2014\n\u2022 Built models using SAS on behavioral profiles of prescribers which lead to increase in the sales of drug called Augmentin (Augmentin is India's number one pharmaceutical brand)\n\u2022 Identified business requirements and analytics needs\n\u2022 Predominantly worked on analysis of primary and secondary sales data\n\u2022 Created and analyzed reports, data models to extract business insights\n\u2022 Presented insights and advocated changes with executives to optimize sales and marketing effectiveness\n\u2022 Partnered with internal reporting team for data mining""]","[u'Master of Science in Management Information Systems in Management Information Systems', u'Bachelor of Science in Pharmacy']","[u'University of South Florida Tampa, FL\nDecember 2015', u'Rajiv Gandhi University of Health Sciences Bengaluru, Karnataka\nAugust 2011']"
0,https://resumes.indeed.com/resume/d964230ac442327c,"[u'Marketing Data and Operations Coordinator\nCompany - San Mateo, CA\nMarch 2018 to Present', u'Senior Data Quality Analyst\nNerdWallet - San Francisco, CA\nJuly 2017 to March 2018', u'Data Quality Analyst\nNerdWallet - San Francisco, CA\nJuly 2016 to July 2017\nI worked on the Data Quality team under QA and Engineering. In this, I:\n\n\u25cf Coordinate with marketing strategists to craft headlines and body text\n\u25cf Approve or reject marketing headlines as well as Facebook ads, Pinterest ads, and Twitter posts\n\u25cf Edit marketing materials for clarity and effectiveness\n\u25cf Train marketing team about the compliance rules surrounding credit cards\n\u25cf Effectively communicate compliance across multiple departments\n\u25cf Liaise cross-functionally between different teams and verticals\n\u25cf Develop and streamline escalation processes for both internal and external affliates to increase\neffciency', u'Data Assurance Analyst\nLinkedIn - Sunnyvale, CA\nJanuary 2015 to January 2016\nI worked on the Data Quality team under Global Sales Systems. In this, I:\n\u25cf Improved operations by identifying trends, training new hires, and providing auditing feedback\n\u25cf Engaged in process and content improvement projects within the Sales Systems team\n\u25cf Supported sales representatives and managers in the 3 lines of business (LTS, LMS, LSS)\n\u25cf Effectively communicated with cross-functional teams through Salesforce Chatter, Google Hangouts, email and direct phone calls to ensure ef cient and timely delivery of commitments\n\u25cf Closed out ~150 verification and merge cases per week with 98% accuracy, closing 4.2 more cases communicator than daily average and leading 6 member team in number closed cases 31% of the time\n\u25cf Pulled account reports through SalesForce for the Sales Operations and the Data Assurance team\n\u25cf Used Salesforce, Hoovers, Data.com, and Guidestar.com for account verification cases\n\u25cf Monitored, analyzed and upheld the weekly changes to the ROE provided by Sales Operations']",[u'B.S. in Human Development in Relevant Projects'],[u'University of California']
0,https://resumes.indeed.com/resume/899dd66d248ad5c0,"[u""Senior Data Modeler/Analyst\nPolycom - Austin, TX\nJanuary 2015 to Present\nProject was combined effort across various teams to model requirements and data obtained from client through business analyst teams.\nResponsibilities:\n\u2022 Interacted with Business Analysts to gather the user requirements\n\u2022 Translate business requirements into conceptual, logical data models and physical data models.\n\u2022 Analyzed existing Conceptual and Physical data models and altered them using Erwin to support enhancements.\n\u2022 Worked with DBA's to create a best-fit physical data model from the logical data model.\n\u2022 Created Design Fact & Dimensions Tables using ERWIN tool.\n\u2022 Created a Normalized logical model for the business to understand the relationships and at the deeper level of granularity.\n\u2022 Exhaustively collected business and technical Metadata and maintained naming standards.\n\u2022 Forward Engineering the Physical Model to generate the DDL scripts to implement on Oracle 11g database.\n\u2022 Reverse engineering of the existing reports to identify the key data elements and data entities required to design the data warehouse.\n\u2022 Used Model Mart of ERWIN for effective model management of sharing, dividing and reusing model information and design for productivity improvement.\n\u2022 Developed Star and Snowflake schemas based dimensional model to develop the data warehouse\n\u2022 Worked on slowly changing dimension tables and hierarchies in dimensions.\n\u2022 Good working knowledge in normalizing the tables up to 1NF, 2NF & 3NF.\n\u2022 Conducted performance tuning of the database that included creating indexes, optimizing SQL statements.\n\u2022 Regularly conducted and participated in weekly status meetings.\nEnvironment: Erwin 7.1/4.5 (Data Modeling, ER Diagrams), Informatica 8.1, Business Objects XI, Oracle 11i, Sybase, Rational Rose, Sun Solaris 9, Windows NT, Business Objects Data Integrator"", u'Senior Data Modeler/Analyst\nSouthwestern Energy - Houston, TX\nAugust 2013 to December 2014\nProject was focused on using and re-evalutating existing ETL strategy.\nResponsibilities:\n\u2022 Participated in all phases including Analysis, Design, Coding, Testing and Documentation. Gathered Requirements and performed Business Analysis.\n\u2022 Extensively used SQL for Data Analysis.\n\u2022 Done Reverse engineering on existing data model to understand the data flow and business flows.\n\u2022 Designed ER diagrams, logical model (relationship, cardinality, attributes, and, candidate keys) and physical database (capacity planning, object creation and aggregation strategies) as per business requirements.\n\u2022 Created Schema objects like Indexes, Views, and Sequences\n\u2022 Conducted team meetings and proposed ETL strategy.\n\u2022 Worked on Gap Analysis.\n\u2022 Data analysis of existing data base to understand the data flow and business rule applied to Different data bases by SQL\n\u2022 Facilitated meetings with the business and technical team to gather necessary analytical data requirements.\n\u2022 Reverse Engineer the existing Stored Procedures and write Mapping Documents for them.\n\u2022 Designed and Developed Oracle PL/SQL and Shell Scripts, Data Import/Export, Data Conversions and Data Cleansing\n\u2022 Creating Mappings, Mapplets, Sessions and Workflows using Informatica to replace the existing Stored Procedures\n\u2022 Pre populate the static tables in the Data warehouse using PL/SQL procedures and SQL Loader.\n\u2022 Written maps for daily data loads.\n\u2022 Designed the procedures for getting the data from all systems to Data Warehousing system. The data was standardized to store various Business Units in tables.\n\u2022 Created Informatica mappings with PL/SQL procedures to load data.\n\u2022 Extensively used ETL to load data from flat files (excel/access) to Oracle database.\n\u2022 Also worked on Business Object ETL tool Data Integrator to create Mapping from Sybase existing data Sybase (ASE) to newly created data base Oracle 11i\n\u2022 Worked on defining deadlines and helped team to meet the deadlines.\n\u2022 Extensively worked on documentation of Data Model, Mapping Transformation and Scheduling jobs.\n\u2022 Worked extensively with Business Objects XI Report Developers in solving critical issues of defining hierarchy, loops and Contexts.\n\u2022 Designed Mapping Documents and Mapping Templates for Data Integrator ETL developer.\n\u2022 Deployed naming standard to the Data Model and followed company standard for Project Documentation.\nEnvironment: Pl-SQL, Oracle 10g, PLSQL Developer, SAP BO, MS Excel. MS VISIO', u'Analyst\nData Modeler - Buffalo, NY\nJanuary 2013 to July 2013\nProject helped to facilitate data validation and verification in the BW, also monitoring and maintaining systems during uptime and downtime.\nResponsibilities:\n\u2022 Created test case scenarios, executed test cases and maintained defects in internal bug tracking systems.\n\u2022 Developed and executed various manual testing scenarios and exceptionally documented the process to perform functional testing of the application.\n\u2022 Managed multiple OLAP and ETL projects for various testing needs.\n\u2022 Debugging the SQL-Statements and stored procedures for various business scenarios.\n\u2022 Developed advanced SQL queries to extract, manipulate, and/or calculate information to fulfill data and reporting requirements including identifying the tables and columns from which data is extracted.\n\u2022 Executed the UNIX shell scripts that invoked SQL loader to load data into tables.', u'Data Analyst\nNCH Healthcare - Naples, FL\nOctober 2011 to December 2012\nProject involved educating the Database team and getting them involved in the testing and mapping processes. Also involved analysis to ensure data integrity, completeness and quality.\nResponsibilities:\n\u2022 Worked to analyze the on-boarding systems and determine the requirement for target system.\n\u2022 Involved in understanding the under writing business practice for individual source system. Reason to map data we need to understand the paper policy.\n\u2022 Involved in creating the mapping document using the business and data in same board.\n\u2022 Involved in pre-load and post-load data analysis, data mapping, data integrity and completeness, data cleansing and quality and gap analysis and data profiling.\n\u2022 Involved in Data Extraction by applying required business logic.\n\u2022 Worked with the enterprise DBAs to ensure the database environments are operational 24/7.\n\u2022 Involved in providing assistance to the Database team as needed.\n\u2022 Involved in database design reviews to ensure the need of the Business is fully converted with design.\n\u2022 Involved in creating a test-bed for the repetitive analysis work. This test-bed was done in Pl-sql and excel. Created Schema objects like Indexes, Views, and Sequences\n\u2022 Conducted team meetings and proposed ETL strategy.\n\u2022 Worked on Gap Analysis.\n\u2022 The entire analysis work was done in Pl-SQL\n\u2022 Worked closely with clinical and non-clinical teams in healthcare setting.\n\u2022 Involved providing assistance to clinical staff as required.\nEnvironment: MS Excel, Oracle 9i, SQL, PL/SQL, MS VISIO, SQL*Loader and UNIX.', u'Data Analyst\nAllscripts Inc\nJuly 2010 to September 2011\nWorked in healthcare IT setting for manipulation and maintainance of databases.\nResponsibilities:\n\u2022 Created tables, views, procedures and SQL scripts and mapping documents.\n\u2022 Create logical and physical models using best practices to ensure high quality and reduced redundancy\n\u2022 Optimize and update logical and physical data models to support new and existing data models\n\u2022 Created FTP connections, database connections for the sources and targets.\n\u2022 Recommend opportunities for reuse of data models in new environments\n\u2022 Perform reverse engineering of physical data models from databases and SQL scripts.\n\u2022 Examine new application design and recommend corrections if required.\n\u2022 Write SQL code and debug them, unit testing was performed\n\u2022 Provided maintenance and support to customized reports developed in crystal reports.\n\u2022 Gathered statistics on large reports and redesigned indexes.\n\u2022 Maintained security and data integrity of the database.\nEnvironment: SQL Server 2000, MS-Excel 2003, Visio 2010, MS-Access 2003, Oracle 9i, SQL, PL/SQL', u'Data Analyst\nCybage - IN\nOctober 2008 to June 2010\nProject involved minute documentation of UML, and project plans for analysis, testing and implementation.\nResponsibilities:\n\u2022 Worked on data modeling and produced data mapping and data definition documentation\n\u2022 Responsible for numerous decisions in respect to business policies, procedures, methods for implementation.\n\u2022 Identified and resolved (or maintain accountability for ensuring the resolution of) issues identified during all phases of projects.\n\u2022 Formulated project plans including staffing, analysis, development, testing and implementation.\n\u2022 Used UML to produce Use-Cases, Use case diagrams and Activity diagrams to explain process flows and functional specifications.\n\u2022 Used the tool Systems Architect and MS-Visio to create use case diagrams and process flows.\n\u2022 Involved in identification of business rules, business and system process flows, user administration, requirements and assumptions.\n\u2022 Produced and managed documentation on release activities to support problem resolution.\nEnvironment: Oracle 9i/10g, SQL Server 2000/ 2005, Quality center 9.2, Windows XP, MS Excel, MS Visio 2003, MS Power Point.']",[],[]
0,https://resumes.indeed.com/resume/7594e11357379f91,"[u'Data Analyst\nThe LIVESTRONG Foundation - Austin, TX\nJanuary 2017 to May 2017\nLIVESTRONG is a foundation known for providing free cancer support services to 2.5+ million people\n\u2022 Saved $5k annual cloud storage cost by analyzing, segmenting, cleaning, and optimizing Salesforce CRM\ndatabase of 4.5 million data entries with Microsoft SQL server\n\u2022 Developed a proprietary Salesforce workflow tool in Apex to automate the internal task assignment process\nutilizing the test-driven development methodology that reduced the projected cycle time by 20%\n\u2022 Conducted system and user acceptance testing to ensure system integrity and usability']","[u'M.S. in Management Information Systems in Management Information Systems', u'B.B.A. in Management Information Systems in Management Information Systems']","[u'Texas A&M University, Mays Business School College Station, TX\nMay 2019', u'The University of Texas at Austin, McCombs School of Business Austin, TX\nDecember 2016']"
0,https://resumes.indeed.com/resume/4e15c473327efb2a,"[u'Relief Manager\nPublic Storage, Westland\nOctober 2006 to October 2013\nLivonia & Canton.\n* Telephone and in-person customer service to include leasing and problem solving.\n* Organize customer files and information and follow daily office procedures as set forth by the company.\n* Maintain the office, property and all vacant units to insure they will meet or exceed customer and company expectations.', u'Data Analyst\nHousing Consultants - Clarkston, MI\nJune 2004 to October 2006\n48346\n* Traveled to city/township Municipal offices thought nine counties in S.E. Michigan gathering information on growth and development of single family home sits.\n* Performed field audits of each location to determine the status of the development and rate of growth in the entire area over a given time line', u'Owner\nComm-Line Cable Television - Hamburg, MI\nOctober 1991 to January 2002\nI ran a company that supplied sub-contract labor for various cable television contractors. In this capacity I recruited crews, drew up contracts, kept inventory of customer supplied equipment, invoiced contractors, did payroll and preformed duties necessary to maintain a small business', u'Data Manager\nMichigan Academy of Dentistry for Handicapped - Ann Arbor, MI\nJune 1988 to October 1991\n* Was responsible for maintaining all data received from dental hygienists in sixteen satellite offices\n* Analyzed and compiled data for monthly, quarterly and yearly reports as required by the State of Michigan, Department of Mental health\n* Responsible for updates and maintenance of programming software and hardware']","[u'in Special Education-Speech Therapy', u'in Psychology and Fine Arts', u'College Prep']","[u'Eastern Michigan University Ypsilanti, MI', u'Monroe County Community College Monroe, MI', u'De Vilbiss High School Toledo, OH']"
0,https://resumes.indeed.com/resume/c7e566531fbe5607,"[u'Data Scientist\nFORD MOTOR COMPANY\nOctober 2017 to March 2018\n\u2022 Designed and developed new dashboards from scratch using Qlikview and Tableau for BDD and MIAMI AV Taas projects.\n\n\u2022 Developed applications using different components of QlikView Enterprise like List boxes, Multiboxes, slider, current selections box, buttons, charts, text objects, Calendar, Cyclic and Drill down groups, bookmarks, etc.\n\u2022 Created Tableau scorecards, dashboards using stack bars, bar graphs, scattered plots, geographical maps, Gantt charts.\n\u2022 Usage of Pivot Tables, created side by side bars, Scatter Plots, Stacked Bars, Heat Maps, Filled Maps and Symbol Maps according to deliverable specifications.\n\u2022 Created Rich Graphic visualization/dashboards to enable a fast read on taxi data and key business drivers and to direct attention to key area.\n\u2022 Extracted the data from various data sources such as traditional databases, Excel documents and CSV files, transforming the data into standard format and load the data so that it is available for analyzing and creating reports.\n\u2022 Used Alteryx for Joining, Preparing and Blending data for Chariot Shuttle services project.\n\u2022 Created Data Histograms for New York, Chicago, SFO and DC to know the quality of public transport.\n\n\u2022 Used MS Project and MS Excel to create and manage project schedules, summaries, tasks, milestones, resources, resource utilization, and overall project.\n\u2022 Created Excel and MS Project plans and timelines of software deployments. Created weekly status reports, monthly summaries and attended weekly and bi-weekly core project team meetings. Created project organization charts using VISIO\n\u2022 Created project dashboards on project progress and prepared reports for Sr. Management.\n\u2022 Effectively communicate project expectations and progress to team members and stakeholders in a timely and clear fashion.\n\u2022 Provided strong organization skills for frequent project tracking, reporting, and presentation development.\n\u2022 Status Reporting- directed and prepared status reports for management, client, project personnel or others and modifies schedules or plans as required.\n\nProject Environment: MS Project, MS Office, MS Visio, MS Excel, SharePoint, MS Visio', u'Sr Data Analyst\nHyderabad, Telangana\nJuly 2011 to August 2015\n\u2022 Developed project charter by determining scope, milestones, and product deliverables.\n\u2022 Manage project activities, such as issues tracking, project timelines and risk mitigation, facilitating project team meetings and preparing/delivering status reports using MS Project, MS Excel & MS Word.\n\u2022 Tracked project through all SDLC phases (Project Initiation, Project Planning and Coordination, Project Control and Project Reporting and Communication) using MS Project Professional.\n\u2022 Managed triage of multiple incoming priorities effectively by understanding customer needs and meeting service level requirements. Identify potential system problems and escalated to department contact for resolution.\n\u2022 Lead and manage a team of coding analyst and effectively implemented process for key sector deliverables\n\u2022 Facilitate IT Review meetings with customer and the IT Leads, review the customer current environment and future needs.\n\u2022 Was part of Agile team and assisted Project Manager with tracking and reporting of project status, resources and costs to ensure accuracy and minimize project delays in MS Project.\n\u2022 Created and maintained project schedules and other required project documentation using MS Project.\n\u2022 Evaluated competitors and reviewed prices, sales, marketing, distribution, products and financial feasibility to ensure optimal product offerings and elevate market presence.\n\u2022 Collected customer demographics, needs/preferences, and purchasing habits to capitalize on new markets and identify methods to boost product demand.\n\u2022 Maintained and Updated SharePoint Site Documents (Excel Spreadsheets, Word Documents, Visio and PowerPoint charts.\n\u2022 Responsible for logging and tracking calls using the current ticketing database, and maintaining history records and related problem documentation.\n\u2022 Developed comprehensive reports based on marketing, sales trends, and demographic data analysis.\n\u2022 Conducted cross team training sessions on best practices which reduced the turn-around time of different deliverables across the research teams.\n\nProject Environment: MS Project, MS Office, MS Visio, MS Excel, SharePoint, MS Visio\n\nProjects Handled:', u'Data Analyst\nKellogg\'s, Pepsi, Coca Cola, General Mills, Campbell, P&G - Hyderabad, Telangana\nJuly 2007 to July 2011\n\u2022 TELECOM: AT&T, T-MOBILE\n\u2022 BANKS: Bank of America, HSBC\n\u2022 OTHERS: General Motors, ESPN\n\nKantar Operations, Hyderabad, India July 2007 - July 2011\nData Analyst\n\n\u2022 Defined project scope, goals and deliverables that support business goals in collaboration with senior management and stakeholders.\n\u2022 Managed activities that support the successful completion of projects including assisting with project schedules, deliverables, assignments, tasks, project meetings, status reporting, communication and action items, as required.\n\u2022 Provided weekly written and verbal communications reports, and milestones throughout project completion outlining project status and progress to project manager, communications director, regional managers, and communications technicians.\n\u2022 Lead meetings via agendas, publish meeting notes and track action items\n\u2022 Tracked project through all project phases (Project Initiation, Project Planning and Coordination, Project Control and Project Reporting and Communication).\n\u2022 Handled coding process for the study on General Motors, and University of Phoenix.\n\u2022 Been a single point of contact for ""Main idea section"" for FMCG deliverables.\n\u2022 Coordinated with the quality leads in preparing process documentation for FMCG deliverables.\n\u2022 Worked on building strong client relationship and obtained one of the highest Customer-Satisfaction score in the team.\n\u2022 Pro-actively assisted client in preparing the questionnaire for a FMCG study.\n\u2022 Team lead for Greening initiative for the firm.\n\u2022 Event coordinator for company\'s annual meet.\n\nProject Environment: MS Project, MS Office, MS Visio, MS Excel, SharePoint, MS Visio']","[u""Master's in Information Systems in Data Mining and Data Modelling"", u""Master's in Business Administration in Marketing"", u'Bachelors in Commerce in Commerce']","[u'University of North Texas Denton, TX', u'Wesley Business School, Osmania University', u'Siddhanti College, Osmania University']"
0,https://resumes.indeed.com/resume/853d445e43a4f366,"[u'Data Analyst Intern\nStandard Chartered Bank - Mumbai, Maharashtra\nJuly 2016 to September 2016\nIN\n\u2022 Assigned to orchestrate and develop data driven analytic applications for the bank to enhance customer service and for discovering new opportunities\n\u2022 Designed a text mining model with an accuracy of above 80% to analyze the customer\nfeedback which helped CRM campaign leaders in the product pitching\n\u2022 Performed daily data analytic tasks and generated reports for drawing insights']",[u'Master of Science in Computer and Information Sciences'],"[u'University of Alabama at Birmingham Birmingham, AL\nAugust 2017 to April 2019']"
0,https://resumes.indeed.com/resume/cb1a9ff2faa9a094,"[u""Data Analyst\nNavigational Data Analytics\nJuly 2016 to December 2017\nRole and Responsibilities:\n\u27a2 Performed Raw data capture of 802.11 beacon frames using WIRESHARK\n\u27a2 Performed data wrangling to clean, transform, reshape the data using PANDAS library and splitting the data into test set and training set.\n\u27a2 Performed exploratory data analysis and development of algorithms in python for data analysis and building machine learning models for analyzing of complex large data set in terms of full packet reception.\n\u27a2 Extensively used packages like PLOTLY, MATPLOTLIB for visualizing huge data sets.\n\u27a2 Researched packet behavior and mapped signal strengths of packets received.\n\u27a2 Developed automated programming to recognize and extract complete packet parameters(SSID, RSSI, beacon frame, etc..) along with SQL tables to store the output and created additional monitoring system to alert if the input models shift\n\u27a2 Synthesized the data to be later used in indoor navigation algorithms\n\nTools: SQL SSMS, PYTHON, PANDAS, WIRESHARK\n\n2. Project:\nML and Image Processing\nSoftware Communications and Navigation Lab\nJan '16 - Jun '16\n\nRole and Responsibilities:\n\u27a2 Analyzed the Data sets from field measurements of the flowers that can be automatically measured from image processing model in the future\n\u27a2 Utilized the SCATTERPLOT matrix to visualize the flower measurements and to look for errors and missing values in the data\n\u27a2 Asserted and tested the data with SCIPY and tidying the data to set up unit tests to verify our expectations\n\u27a2 MadeVIOLIN PLOTS of data for exploratory analysis to compare the measurement distribution of classes\n\u27a2 Applied decision tree classifiers to achieve 95% classification accuracy for the model\n\u27a2 Used stratified K FOLD CROSS VALIDATION to achieve the accuracy of general classification\n\u27a2 HEATMAP and GRIDSEARCH to determine the best possible parameters\n\u27a2 Concluded the analysis with a data pipe line that can be later used to check the flower species through pictures\n\nTools: JUPYTER, SCIKITLEARN, SEABORN\n\n3. Project:\nEDA Top Charts"", u""Data Analyst\nGaana.com\nAugust 2015 to January 2016\nAug '15 - Jan '16\n\nRole and Responsibilities:\n\u27a2 Imported the DATAFRAME into our pandas library and fetching basic information such as singer, song name, song length, language, genre, etc, .\n\u27a2 Cleansed the data for errors in DATETIME, shorten the strings and missing values\n\u27a2 Used the melt function to create a pivot table for weekly rank data\n\u27a2 Visualized the data in TABLEAU to provide context to the data set and observation around three themes: top singers, top genres and top remixes\n\u27a2 Summarized the study to determine top 100 songs by the singers and the affect of technology, traditional and digital marketing on the music genre\n\u27a2 GeneratedHEATMAPS to characterize the grouping of songs by highest rank, lowest rank, median rank and other statistical measures.\n\u27a2 Developed TIMESERIES chart to track the persistence of chart in rank over time\n\u27a2 Concluded the exploratory data analysis to be used in the smart phone app\n\nTools: TABLEAU, PANDAS, SEABORN\n\n4. Project:\nTobacco Products Analysis for state tax board."", u""consultant\nWipro Ltd\nJune 2013 to July 2015\nRole and Responsibilities:\n\u27a2 Performed and investigation in the data acquisition methodology and and the ETL pipeline for the integrity of the data\n\u27a2 Cleansed and verify the data for errors and missing values\n\u27a2 Performed exploratory statistical analysis using SEABORN and MATPLOTLIB to visualize the interesting aspects and establish the timeframe of the dataset\n\u27a2 UsedHEATMAPS to find any correlation between pairs of features - total sales, products sold and quantity sold\n\u27a2 using PAIRPLOT to find any correlation between pairs of variables - product cost and product retail\n\u27a2 Analyzed time series sales over time and yearly sales breakdown by store of the stakeholder data for any vital information from the standard deviation and skew by plotting sales\n\u27a2 Used SCIKITLEARN and STATSMODS to build the necessary models for the scenario\n\u27a2 Performed linear regression to predict the yearly sales figures from first quarter of the year based on the R2 score and training the model\n\u27a2 Used SCATTERPLOT to show predicted vs actual sales to show the accuracy of the model\n\u27a2 Reported the observations made from EDA are product popularity- cigrattes, e cigs, total products sold, total sales and total quantity sold\n\u27a2 used PIVOT TABLES to determine which city sold the most and which vendor sold the most\n\u27a2 Studied the PARETO DISTRIBUTION of the histogram of the city divided into bins and quantity of tobacco sold in time period covered by the data set.\n\u27a2 Provided analytical reports to the department and developed visualizations in TABLEAU\n\nTools: TABLEAU, PANDAS, SEABORN, SCIKITLEARN\n\n5. Project:\nSales Force Analytics\nWipro Ltd, Data Analyst\n\n\u27a2 Involved in all the phases of the project, communicated with the business and prepared Technical Specification Document.\n\u27a2 Used SQL queries to compare the database and the periodic reports generated by the application at various levels.\n\u27a2 Performed data analysis in pandas library on the sales data of HP products and CONSUMER SURVEY REPORTS for creation of reports to be forwarded to the technical team and visualized the same.\n\u27a2 Designed and deploy rich Graphic visualizations with Drill Down and Drop down menu option and Parameterized using TABLEAU with reporting objects like Facts, Attributes, Hierarchies.\n\u27a2 Analyzed data and created class diagrams and ER diagrams for designing databases. Closely interacted with designers and software developers to understand application functionality and navigational flow and keep them updated about Business user's sentiments.\n\u27a2 Designed and developed Use Cases, Activity Diagrams, Sequence Diagrams and Business Process Modeling.\n\u27a2 Worked with users to identify the most appropriate source of record and profile the data required for sales and service.\n\nTools:TABLEAU, PANDAS, SQL SSMS, Salesforce, Manual Testing""]","[u'Masters of Science in Electrical engineering', u'Bachelor of Engineering in Electronics and Communication Engineering']","[u'University of Texas at San Antonio San Antonio, TX', u'Osmania University Hyderabad, Telangana']"
0,https://resumes.indeed.com/resume/f379d9885847bae7,"[u'Data Analyst/MIS Executive\nIdea Cellular - Lucknow, SC\nFebruary 2013 to Present\nAnalyse problems and discover the best ways to solve them\nDevelop and implement strategies to achieve organizational goals\nCommunicate clearly to superiors and give understandable instruction to subordinates\nAllocate resources effectively to reach organizational goals\nLead and motivate teams to promote efficiency and effectiveness', u'MIS/Data Analyst']",[u'MBA in finance'],"[u'SMU Lucknow, SC\nJanuary 2010 to January 2012']"
0,https://resumes.indeed.com/resume/589adedbbc7586ca,"[u'Wells Data Analyst\nIntertek\nSeptember 2014 to Present\nGathered, organized and formulated data from abstract sources in time-efficient manner\n\u2022 Devised metric and threshold analysis reports for drilling teams internationally\n\u2022 Worked in a high-stress, team environment that required strict deadlines']","[u'B.S. in Geology / Minor Geophysics', u'A.A.S']","[u'University of Houston\nJanuary 2012 to January 2014', u'Houston Community College\nJanuary 2009 to January 2012']"
0,https://resumes.indeed.com/resume/22d0fe4fe8e76e3f,"[u'Model\nModelogic - Richmond, VA\nAugust 2012 to Present\n\u2022 Represent and promote products, services and brands including Chanel, Salvatore Ferragamo, Italian Vogue and Marie Claire; platforms include print, video, and runway.\n\u2022 Manage demanding workload including: scheduling, maintaining health and fitness, organizing personal finances and coordinating travel.', u'Data Analyst Intern\nAcademic Benchmarking Consortium - Chapel Hill, NC\nMay 2017 to August 2017\n\u2022 Extracted, compiled and tracked data, and analyzed data to generate reports.\n\u2022 Interpreted data from primary and secondary sources using statistical techniques and provided ongoing reports.']",[u'Bachelor of Science in Mathematical Sciences - Statistics'],"[u'Virginia Commonwealth University Richmond, VA\nMay 2018']"
0,https://resumes.indeed.com/resume/5ca86f5120e44d04,"[u'Data Analyst\nAssociation for the Protection of Collegiate Athletes, NPO - Atlanta, GA\nMay 2014 to August 2014\n\u2022 Constructed a database of demographic information on all NCAA athletes and teams\n\u2022 Facilitated connections with prospective business partners to expedite data collection', u'Summer Intern\nSouthern Capitol Ventures, LP - Raleigh, NC\nMay 2013 to August 2013\n\u2022 Created a market comparison database to analyze the value of all portfolio companies\n\u2022 Implemented an automated, consistent structure for future portfolio valuations\n\nLEADERSHIP']","[u'Master of Science in Analytics', u'Bachelor of Business Administration, with Distinction']","[u'Institute for Advanced Analytics at North Carolina State University Raleigh, NC\nJune 2016 to May 2017', u'Emory University Atlanta, GA\nAugust 2011 to May 2015']"
0,https://resumes.indeed.com/resume/d1aba60b41a2847b,"[u'Data Analyst Intern\nExcellus BlueCross BlueShield\nJune 2017 to August 2017\nAgile Manager, R, R Studio, Classification - Random Forest Algorithm)\n\u2022 Worked in collaboration with couple of data scientists to build and compare different predictive models to envision future diabetic patients\nwho are probable to be readmitted within 30 days of discharge with 78% accuracy.\n\u2022 Perform feature selection algorithms in R to determine important factors for predicting members more likely to be addicted towards opioid\n\u2022 Upgraded the dashboard to include comparative analysis of NY counties based on Excellus data and NY state government data', u'Technology Business Analyst\niConsult\nSeptember 2016 to May 2017\nProject Charter, Business-case, Requirement Elicitation, MYSQL, MS SQL Server)\n\u2022 Designed a centralized database using MySQL to store and access data pertaining to curriculum and course scheduling, that in turn increased the efficiency by 42%\n\u2022 Developed project charter, business-case communication plan based on the analysis of client requirements to ensure clarity\n\u2022 Acted as a liaison between client and IT team to elicit, document and validate the requirements using advanced problem-solving skills', u'Data Analyst\nCerner Corporation\nFebruary 2013 to April 2016\nVBA, C#, SAP Business Object, Agile Methodology, Tableau)\n\u2022 Queried big data using SAP business object tool, to observe the trend of popular devices used by hospitals in different regions\n\u2022 Performed preprocessing and visualized trends in patient symptoms using Tableau, thereby increased diagnosis factory by 12%\n\u2022 Designed a procedure that triggered event to retrieve patient detail from MySQL, which enhanced the search speed by 23%\n\u2022 Lead scrum meetings for regular communication with team members and ensure clarity & successful completion of project\n\u2022 Collaborated with software testers to develop test plans and test scripts in accordance with the project requirement.']","[u'Master of Science in Information Management', u'in Design for Bookstore Inventory Management', u'Bachelor of Technology in Information Technology']","[u'Syracuse University\nAugust 2016 to May 2018', u'Syracuse University bookstore platform\nSeptember 2016 to December 2016', u'Vellore Institute of Technology Vellore, Tamil Nadu\nJuly 2009 to May 2013']"
0,https://resumes.indeed.com/resume/6d239b3ec251c733,"[u'Data Analyst\nLos Angeles Biomedical Research Institute\nFebruary 2017 to February 2018\n\u2022 Developed an expandable and searchable database from scratch that captures specific data elements pertaining to LA BioMed industry-sponsored clinical trials contracts.\n\u2022 Responsible for writing and executing complex SQL queries to extract data from a variety of internal sources.\n\u2022 Performed quantitative and predictive analysis on multiple sets of complex data which in turn saved up to $2 million in research project budget. Used analytical tools like MS Excel, Tableau and SSRS for the same.\n\u2022 Created visual reports and dashboards in Tableau to analyze data and to discover and interpret trends, patterns, and relationships. Responsible for developing reporting tools which can be used to query the database and produce information in tabular and graphic formats.\n\u2022 Developed a strong knowledge of using medical record systems such as iMedRIS and familiarized with the HIPAA policies and the work process in the healthcare research industry.\n\u2022 Coordinated with the Managers to define specific project requirements, deliverables and timelines, which would be presented to the Executive Vice President for final sign\u2010off and approval.', u'Business Analyst\nHop Shop Drop - Mumbai, Maharashtra\nJanuary 2013 to January 2015\n\u2022 Exposed to all facets of the product processing business in E-commerce industry which includes buying and inspecting products, logistics, inventory management and interaction with vendors/suppliers, customers, and employees.\n\u2022 Analyzed data using advanced Excel and SQL skills for refinement of product catalog on the website.\n\u2022 Generated meaningful progress reports for analyzing sales, gross profit, inventory activity and emerging trends.\n\u2022 Developed business and technical requirements for new product development and prepared complex documentation impacting major projects.\n\u2022 Documented a list of enhancements to our company\u2019s mobile app and guided a team of 4 mobile app developers through the software development life cycle process to implement the enhancements.\n\u2022 Managed e-commerce business processes and made recommendations on a variety of moderately complex issues using project management skills.\n\u2022 Conducted requirements analysis, market survey and competitors analysis.']","[u""Master's Degree in Management Information Systems"", u""Bachelor's Degree in Computer Engineering""]","[u'California State University Los Angeles, CA\nMarch 2015 to December 2016', u'Mumbai University\nJanuary 2013']"
0,https://resumes.indeed.com/resume/c965d73bc462ce59,[u'Data Analyst'],"[u""Bachelor's in Computer Engineering""]","[u'University of California-Irvine Irvine, CA\nSeptember 2014 to March 2018']"
0,https://resumes.indeed.com/resume/c02f64f225d362dd,"[u'Data Analyst Internship\nBeijing Chunhaiying Trading Co.Ltd. - Beijing\nJune 2016 to August 2016\nArrange the shipping information for every truck by recording the related data into Excel to make tables and charts. In detail, record the information about shipping amount per truck and goods amount per shipping per truck everyday. Record different unit price of shipping for different destinations. Calculate out the freight for every truck everyday. Make different histograms for freight information based on different requirements by integrating the daily information together. Figure out the truck which contributes to maximum and minimum freight per month and also the month which gives the most and least freight by comparing charts with each other.']","[u""Master's in Industrial and Systems Engineering"", u""Bachelor's in Mechatronics Engineering""]","[u'Binghamton University-SUNY Binghamton, NY\nSeptember 2015 to May 2017', u'Shanghai Maritime University Shanghai\nSeptember 2010 to June 2014']"
0,https://resumes.indeed.com/resume/f1565ed003154ffd,"[u'Data Modeling Analyst\nANZ - Bengaluru, Karnataka\nAugust 2016 to January 2017\nIndia\n\nDeveloped and maintained statistically derived regression models across the credit lifecycle, to gain an insight into applications, behavior, collections and bankruptcy of customers\n\nEvaluated large datasets for inconsistencies and implemented various Business Verification Tests using SAS and SQL to ensure smooth data flow\n\nAssessed capital estimates models, including Probability of Default (PD), EAD (Exposure at Default), and LGD\n(Loss Given Default), to ensure the reliability and robustness of the models used', u'Business Analyst\nZS Associates - Pune, Maharashtra\nAugust 2015 to August 2016\nPune, India\n\nSegmented datasets of hospitals and physicians into subgroups using behavior, attitude and other secondary data as constructs for pharmaceutical companies to plan their business strategy, helped increase nationwide revenue of a product by $20 million compared to previous year\n\nForecasted the revenue and sales of products not yet launched using data obtained from extensive physician surveys to estimate future financial outcomes\n\nUsed text and sentiment analysis to derive and present Customer Insights from qualitative market studies to evaluate new product viability\n\nDeveloped Generalized Linear Models to predict sales and marketing outcomes in Market Research projects; increased sales by 22% for a recently released drug']","[u'Master of Science in Business Analytics', u'Bachelor of Technology in Civil Engineering']","[u'W. P. Carey School of Business at Arizona State University Tempe, AZ\nMay 2018', u'National Institute of Technology Karnataka Surathkal Mangalore, Karnataka\nJuly 2015']"
0,https://resumes.indeed.com/resume/1ca6ccc8c74da4e7,"[u'Data Analyst/Data Modeler\nRSRIT - Northville, MI\nJanuary 2018 to Present\n\u2022 Involved in full project lifecycle of SDLC and coordinated with all project teams.\n\u2022 Involved in reviewing business requirements and analyzing data sources form Excel/Oracle SQL Server for design, development, testing, and production rollover of reporting and analysis projects within Tableau Desktop (10.4/10.3/10.2).\n\u2022 Hands on experience creating shell scriping on LINUX environment.\n\u2022 Deploying the playbooks in multiple severs using AWS.\n\u2022 Designed and developed a business intelligence data visualization dashboard using Tableau Desktop, allowing executive management to view past, current and forecast sales data.\n\u2022 Analyzed requirements for various reports, dashboards, and scorecards and created the same using Tableau desktop server.\n\u2022 Created Filled maps, actions, parameters, Filter (Local, Global) and calculated sets for preparing dashboards and worksheets using Tableau.', u'Data Analyst/ Tableau BI Developer\nBOTS Inc - Canton, MI\nJuly 2017 to December 2017\n\u2022 Updated and manipulated content and files by using python scripts.\n\u2022 Built various graphs for business decision making using python Matplotlib library.\n\u2022 Involved in planning, writing and executing statistical programs designed to analyze data.\n\u2022 Good SQL reporting skills with the ability to create SQL views and write SQL queries highly recommended.\n\u2022 Using SQL to query Databases Performing various validations and mapping activities\n\u2022 Performed data analysis and data profiling using SQL on various sources systems including SQL Server 2008.\n\u2022 Deep experience with the design and development of Tableau visualization solutions\nEnvironment: Tableau Desktop 10.3, Tableau server / Administrator, Python 2.7, SQL Server 2012/2008', u'Data Analyst\nHenry Ford Health Systems - Detroit, MI\nNovember 2016 to May 2017\n\u2022 Working for Columbus Center Sleep Disorders Center to gather information on a Baseball sports project supervised by MLB (Major league Baseball).\n\u2022 Worked closely with visual design team to select the appropriate visualizations for BI deliverables.\n\u2022 Modified SAS programs to generate derived analysis datasets and perform analysis.\n\u2022 Created Statistical reports using advanced MS Excel for every player who ever involved in the team.\n\u2022 Designed and developed various analytical reports from multiple data sources by blending data on a single worksheet in Tableau Desktop.\n\u2022 Used tools like Minitab, Structured Queried Language, Regression analysis and V-LOOKUP & Pivot Tables to sorting datasets in excel.\nEnvironment: Agile, Tableau, MS Office 2016, MS Project, Mini Tab 17, MS Visio 2010, MS SQL Server 2012/2008.', u'Jr. Data Analyst\nOptum - Hyderabad, Telangana\nMarch 2014 to July 2015\n\u2022 Worked in data analysis, data mining and data mapping throughout the project.\n\u2022 Developed source to target mapping document, with all the transformation rules\n\u2022 Data Validation in MS Excel, created forms for data entry while using data validation.\n\u2022 Built Tableau scorecards, dashboards using stack bars, bar graphs, scattered plots, geographical maps, Gantt charts with easy utilization of show me functionality.\n\u2022 Performed extensively with Calculations, Parameters, Background images, Advance analysis Actions and Maps.\n\u2022 Created Dashboards for the financial reports in MS Excel.\n\u2022 Designed and implemented basic SQL queries for testing and report/data validation.\n\u2022 Developed a database and generated tables and reports in SharePoint from database tables.\n\u2022 Developed customized calculated fields from cubes in Pentaho and integrated fields to SQL in Tableau.\n\u2022 Compared Dashboards and reports developed in Tableau with Pentaho reporting.\nEnvironment: Agile, Tableau, MS Office 2007, Share Point 2007, MS Project, Mini Tab 17, MS Visio 2010, MS SQL Server 2012/2008.']","[u'Master of Science in Information Technology', u""Bachelor's in Engineering""]","[u'Lawrence Technological University\nMay 2017', u'Acharya Nagarjuna University\nMarch 2014']"
0,https://resumes.indeed.com/resume/687333dec1ba7861,"[u'Management Analyst\nValour, LLC - Fort Belvoir, VA\nOctober 2017 to Present\nProvide support to the Cooperative Biological Engagement Program (CBEP) at the Defense Threat Reduction Agency by utilizing a detailed and thorough approach; observe and assess. Support the Strategy and Plans team lead to efficiently and effectively coordinate and implement high visibility projects by designing and developing business improvement strategies, processes, and tools to assist the government client in executing initiatives in foreign countries.\n\n\u2022 Strategically plan to provide data and economic analysis for DoD enterprise-wide engagements\n\u2022 Apply appropriate management analysis processes, structured quantitative and qualitative analysis, and techniques to provide client with effective strategies and improved processes\n\u2022 Provide a full range of products and documents to support the internal and external processes of CBEP, including but not limited to briefings, observations, advising, assessments, analysis, plans, modeling, efficiency metrics activity\nreports and evaluations of contractor deliverables, official correspondence, and public relations materials\n\u2022 Support systems development by writing requirements, conducting assessments, and beta testing\n\u2022 Lead onboarding portfolio by facilitating training and compiling materials for proper knowledge transfer\n\u2022 Conduct research for the Country Prioritization Tool that provides common decision methods for CBEP leadership,\nOffice of the Secretary of Defense (OSD), DoD stakeholders, and interagency partners', u""Senior Data Analyst\nFood and Drug Administration - Silver Spring, MD\nSeptember 2016 to October 2017\nProvided an HHS client with business capability for maintaining high-quality, timely, and accurate master data to support multiple downstream systems, business processes, and end users. Monitored and managed the lifecycle of applications submitted to the Center for Drug Evaluation and Research (CDER) using Workfront platform. Promoted into roles of increasing scope and responsibility, leading the Standard Operation Procedures (SOP) writing process, testing new methods and practices before new releases, and working with new hires on processing their system access paperwork.\n\n\u2022 Reviewed a defined subset of inbound submissions from industry to identify new/updated product, facility, and site data\n\u2022 Performed data quality analysis and validation of incoming data from external information sources using Informatica\ndata integration tool to reduce redundancy across the agency's systems and create a central depository for users to access\n\u2022 Supported CDER business users by addressing questions and troubleshooting issues regarding Integrity master data by conducting detailed and thorough research through a central database to provide critical analysis for end user requests\n\u2022 Analyzed application submission packages to determine the sponsors' requests, then processed applications accordingly\n\u2022 Worked with other members to distribute daily workload and monitored mailbox to meet Service Level Agreements\n\u2022 Drafted SOPs and training materials that address new procedures"", u'Product Metrics Analyst\nInformation Innovators - Silver Spring, MD\nOctober 2014 to September 2016\nServed a DoD client and helped it achieve its mission, which was creating and developing innovative clinical research initiatives and educational programs to serve service members and veterans who suffer traumatic brain injuries. Promoted into roles of increasing scope and responsibility, leading the Product Lifecycle Management process for DVBIC that tracked and examined more than 150 educational products per year to determine the need for update or retirement.\n\n\u2022 Improved product order processing and delivery to stakeholders to less than 7 days from 14 days\n\u2022 Analyzed product usage history to forecast future order levels and ensure inventory efficiency of over 100 products\n\u2022 Negotiated cost quotes on the production of the DVBIC products\n\u2022 Developed budget reports and made data-based recommendations to leadership that address products printing costs\n\u2022 Participated in strategic planning for dissemination plan of new products\n\u2022 Monitored and measured effectiveness of business processes to ensure consistent value delivery\n\u2022 Produced weekly and monthly inventory and usage projection reports that entail tabulated data and dashboards\n\u2022 Coordinated mentorship program by planning and managing the program, and arranging mentor and mentee pairings\n\u2022 Presented trainings on various topics to DVBIC leadership and staff', u""Data Analyst/ Medical Transcriptionist\nGenPath- Bioreference Laboratories - Clarksburg, MD\nAugust 2011 to October 2014\nGenerated spreadsheets and summary dashboards that aided Cytogenetics Department Director to monitor trends and patterns in patients' results, turnaround time of results, and process flow of each patient case from beginning to end.\n\n\u2022 Compiled patient files and reported patient demographics, diagnoses, and medical histories in Microsoft Excel\n\u2022 Ensured accuracy of patients' information on requisition forms, pathology reports, and hospital documentation\n\u2022 Generated and transcribed diagnostic reports in Access database from handwritten interpretations\n\u2022 Proofread and edited reports for completeness, accuracy, spelling, grammar, and compliance with regulations\n\u2022 Maintained discretion and security of medical records and other patients' sensitive information"", u""Data Analyst/ Lead Scheduler\nThe Nielsen Company - Rockville, MD\nAugust 2008 to February 2011\nGathered, analyzed, and interpreted data from Microsoft Excel into visual reports for management and external stakeholders to make strategic decisions effectively.\n\n\u2022 Reviewed and assessed panelists' television viewing habits by identifying trends and patterns of error that might have\ninfluenced the accuracy in the reporting of data\n\u2022 Managed schedules for field associates and ensured efficiency to achieve optimal use of their time\n\u2022 Addressed panelists' concerns, resolved inquiries, and coached them on proper use of Nielsen's meters\n\u2022 Enhanced interpersonal skills by serving as a liaison between office staff, field representatives, and panelists\n\u2022 Conducted and arranged training sessions for new hires by instructing them on the correct practices of data analysis""]","[u""Master's of Business in Business"", u'Bachelor of Arts in English']","[u'University of Maryland University College Adelphi, MD\nMay 2014', u'University of Maryland University College Adelphi, MD\nMay 2012']"
0,https://resumes.indeed.com/resume/018533078ce0d252,"[u'Data Analyst\nBeshton Software Inc - San Jose, CA\nMarch 2017 to Present\n\u25cfCollected, cleaned and reconsolidated data from various sources. Utilized clients\u2019 CRM systems (Salesforce & SAP Business Object).\n\u25cfDeveloped machine learning models such as multi-linear regression models to do market segmentation researches on online big data using python.\n\u25cfFacilitated more than 10 market research reports for big companies using web-scraping data.\n\u25cfAnalyzed customer behavior using random forest algorithm with a prediction accuracy of more than 80%.\n\u25cfPredicted future costs using models of PCA, K-means, Support Vector Machine, Random Forest, Gradient Boosting.\n\u25cfSupplied troubleshooting, analysis, and solutions for Beshton Software Oracle database and back-end application issues.\n\u25cfDeveloped and maintained efficient data schema to store customer and business data in database. Manage the execution of all DSS processes involved in the locking of the trial database for assigned trials.\n\u25cfExtracted data with SQL queries in Oracle to perform ad-hoc analysis requests from product, sales, operations.\n\u25cfCreated Tableau worksheets by performing Table level calculations, Window Functions using different analytics such as reference lines/band, average lines, standard deviations, forecasting, trend analysis, and distribution band.\n\u25cfProactively avoided many data issues by performing manual and automated data audits on the reporting layer and backend data sources such as oracle and teradata by running ad-hoc SQL queries.\nPerformed data validations between BI Apps , Enterprise Data Warehouse and upstream data sources for various (measures, dimensions and hierarchies).', u'Statistical Analyst\nAdvanced Clinical - San Francisco, CA\nMay 2016 to February 2017\n\u25cfAggregated and queried multiple data sources (Excel, Access, SQL Server, DB2, etc.). Data validation to ensure accuracy, quality and integrity. Analyzed data and provided reports to other management.\n\u25cfCreated aggregated data views in SAS by merging both patients, drug subset datasets and generated required statistical results for safety analysis and efficacy analysis and taking the data sets to next level by creating data visualizations in Tableau.\n\u25cfFetched useful data from database system in SAS and processed it by existing statistical models for strategic making purposes.\n\u25cfMonitored/refitted logistics regression models on a daily basis to identify operational variances; responsible for large scale of data cleansing of existing scripts in SAS by leveraging extensive queries in order to maintain stable operational support for clients.\n\u25cfPerformed statistical modeling such as logistic regression, time series analysis, and clustering using R/ SAS to support drug development projects.\n\u25cfProduced statistical validation tables of Adverse Event Studies under FDA standards, using SAS and SQL.\n\u25cfWorked with DBA for migrating scripts/data for stability using ETL (Kettle) and Spark framework.\n\u25cfOptimized SQL scripts and developed PL/SQL procedures to automate data loads; Dealt with 1 Million+ data.\n\u25cfConsolidated and automated SQL query, data analysis and data visualization into one SAS program. Estimated to save 1hr/day of manual work and reduce ~50% turn-around time of major changes.\n\nDesigned clinical protocols, including preparation of statistical sections, sample size calculations, randomization and study design; Prepared programming specifications, created derived datasets, tables, figures and listings.', u""Statistical Analyst Intern\nMasswell Development Group Inc - New York, NY\nJanuary 2014 to April 2016\n\u25cfAutomated gathering of industry data from government websites into centralized SQL databases using R and Python.\n\u25cfUnderstood business reporting & analytics requirements, created BRD's, Keynote presentations, drove analytics initiatives, understood the product datasets, creating SQLs, Hive scripts to pull data from Teradata, Hadoop ecosystems.\n\u25cfImplemented analytics delivery, data extraction, data analysis, model build & interpretation.\n\u25cfMining complex data with focus on multiple comparisons, logistic regression and survey design; designed algorithms in R and SQL.\n\u25cfPerformed multivariate statistical analysis on large datasets to reduce operational costs by 7.25% for a fast food delivery giant using machine learning, multivariate statistical analysis and optimization.\n\u25cfSimulated algorithms focused on pair-trading strategy in the energy market using investment hypothesis, quantitative approaches and real-world signals, to forecast the performance of model both quantitatively and qualitatively.\n\u25cfMonte Carlo simulation on 100+ components assembled in 4 global locations to validate outcome and examine risks.\n\u25cfStatistical analysis to evaluate the significance of A/B test results (4 group of 10K players). Daily update on this analysis.\n\u25cfReduced testing cycle time from ~60 days to ~7 days, therefore enabled more development iterations.\n\u25cfUsed Hadoop ecosystem, R to perform big data analysis and use Tableau for visualization.\n\u25cfSucceeded in streamlining the invoicing process with an Enterprise Content Management digital system. As a result, this reduced the errors on invoices by 5%.\nManaged product development with the Scrum environment in order to adapt to evolving technologies and changes in customer requirement."", u'Data Analyst Intern\nHSBC - Guangzhou, China\nApril 2010 to May 2012\n\u25cfInvolved in Data science lifecycle stages like(acquisition,exploration,scrubbing,model build & interpretation).\n\u25cfAnalyzed digital payment survey data by using K-means clustering, hierarchical clustering and 2-step clustering techniques in SAS to group customers with similar behaviors and attributes.\n\u25cfConducted statistical analysis on customer and transactional data to determine risk to the company; forecasted operating costs and predicted market trends using Excel and SAS on large dataset (over 300k rows). Generated reports and ad hoc analysis.\n\u25cfProvided Analytics around refund abuse, free and paid transactions. Helped business understand and avoid the fraud associated with free transactions.\n\u25cfImplemented predictive supervised learning models like simple, multi-linear and logistic regression models (in Python & R) to get insights on customer loans analysis etc.\n\u25cfDeveloped bi-weekly evaluation dashboard using SAP Business Intelligence tool to deliver insights on daily customer behavior and program progress.\n\u25cfUtilized, designed and created database structures to efficiently store critical business historical data;\n\u25cfLaunched a project for training a four-intern team with 6 IT and business people involved successfully.']",[u'MS in Statistics'],"[u'Rice University Houston, TX\nAugust 2012 to May 2013']"
0,https://resumes.indeed.com/resume/44b66736f6cca42d,"[u'Data Analyst\neNetwork Supply\nAugust 2017 to Present\n\u2022 Competencies: Data requirements analysis, Data modelling, Business Requirement Documentation.\n\u2022 Author of business requirement document of the system which is used by the team for internal reconciliation.\n\u2022 Constructed access database application implementing data models and built functional structure.\n\u2022 Managed RMA received for the company and generated reports for status updates.\n\u2022 Key areas: Requirement gathering, Data modelling, query implementation, data flow analysis of telecommunication and networking system.', u'Business Analyst\nCapgemini - IN\nJune 2014 to July 2016\nCompetencies: Business requirements analysis; functional process evaluation; case and scenario management and delivery, Agile implementation, SDLC implementation..\nKey areas: Unit, functional, system integration, regression and performance testing,Manual and Automated(Selenium): System integration, regression and User Acceptance testing.\nFunctional area: Asset Servicing(ASR), Investment Servicing, Tax and RBB Integration.\nConstructed scalable data and process models and led testing functionality for business specifics.\nManaged offshore document analysis, workshops and entity relationship diagrams (ERD) analysis.\nPerformed comprehensive data and process model schematics and validated testing functionality.']","[u'Master of Science in Data Management', u'Bachelors in Electronics and Communication']","[u'Illinois Institute of Technology Chicago, IL\nJanuary 2016 to January 2018', u'SRM University\nMay 2014']"
0,https://resumes.indeed.com/resume/2343011aae94e2e5,"[u'Sr .SAS Programmer/Data Analyst\nGlaxoSmithKline - Zebulon, NC\nJune 2015 to Present\nProject Description:\nGSK has a single mission. They are committed to helping people do more, feel better and live longer.It is a science-led global healthcare company. They have three world-leading businesses that research, develop and manufacture innovative pharmaceutical medicines, vaccines and consumer healthcare products.\nResponsibilities:\n\u2022 Data analysis to analyze large data sets using statistical techniques to identify trends or patterns\n\u2022 Build automated reports/dashboards to share results with the team on periodic basis (daily,\nweekly, monthly, quarterly and annually)\n\u2022 Help build and maintain key performance indicators (for operational and financial dashboards)\n\u2022 Develop and implement data collection systems/processes that optimize statistical efficiency and data quality\n\u2022 Combine, filter and reconcile data from multiple sources\n\u2022 Assist in building and maintaining models, analyses and reports for our investors\n\u2022 Assist in database administration and design wherever applicable. Provide maintenance support for system as needed.\n\u2022 Work closely with business stakeholders to prioritize, plan and implement their information and analytics needs\n\u2022 Developing tools by programming that aid in accurate and efficient data.\n\u2022 Creating SAS programs and performed data validation on raw data sets and created data set for analysis.\n\u2022 Codes, tests, and debugs programs; maintains and documents programs.\n\u2022 Work assignments include the complex phases of applications systems analysis and programming activities, and require some instruction in other phases as per the SOP.\n\u2022 Diagnose and resolve program and system technical problems.\n\u2022 Served as a Developer/Analyst on special projects Accelerated Stability Modelling Dashboard with SAS as analytical engine for Spotfire Statistical Server v7.0\n\u2022 Confer with users to define and understand business needs and identify technical solutions.\n\u2022 Begins or continues to develop comprehensive knowledge of the application and business system processes in order to participate in system evolution/replacement planning and implementation.\n\u2022 Provides impact analysis on application change requests or modifications.\n\u2022 Assisting in change management according to the project change management plan in a GxP environment..\n\u2022 Participate in training opportunities to further develop technical and communication skills.\n\u2022 For critical applications, provided on call or after hour support\n\u2022 Provide training and mentoring for less experience team members.\nEnvironment: Windows 2013, SAS 8.2, SAS9.3, SAS 9.4, SAS/ODS, SAS/EG, BASE SAS, SAS/ MACRO, SAS/ SQL , SAS/ACCESS, SAS DI, SAS MC, MS Excel, MS Access, MS Visio, Beyond Compare, TIBCO-SpotFire, MiniTab, Ultra Edit, Toad 12.7, Snagit12, XMLEditor, Datapost, PPA2,""WinSCP"" (Windows Secure CoPy) , Putty, database Oracle, PL/SQL 8.0.4.', u""Sr. SAS Data Analyst/Developer\nUSAA - San Antonio, TX\nOctober 2014 to April 2015\nDescription:\nAs a Fortune 200 financial services organization, USAA is on a mission - to facilitate the financial security of our members, the men and women of the U.S. military and their families worldwide by providing a full range of financial services and products.\nHeadquartered on a showcase campus in San Antonio, TX, USAA attributes its long-standing success to its most valuable resource, our 26,000 employees. They are the heart and soul of our member-service culture.\nRecently Ranked #17 in Fortune's 100 Best Companies to Work For in 2014 and marking our fifth straight year in the list's Top 50, proud to receive consistently outstanding awards and ratings for member service, employee well-being and financial strength.\nResponsibilities:\n\u2022 Creating SAS programs and performed data validation on raw data sets and created data set for analysis.\n\u2022 Creating test scenarios to find errors and confirm programs meet specifications as per Bank Governance Guidelines.\n\u2022 Followed System Development Life Cycle (SDLC) methodology for the design, development, implementation, and testing of various SAS modules.\n\u2022 Capable of utilizing data management techniques like Merging, concatenating, interleaving of SAS datasets using MERGE and SET statements in DATA step.\n\u2022 Performed PROC SQL joins and worked with PROC SQL set operators to combine tables\n\u2022 Worked with SAS on Unix and SAS EG (Enterprise Guide) with Oracle / DB2 and Host files using Proc SQL Pass-Through.\n\u2022 Experience in using various MS office tools for analyzing, arranging and presenting data.\n\u2022 Responsible for accessing and managing data and for performing complex queries and analysis, data mining,.\n\u2022 Producing data in various output formats including HTML, RTF, PDF, MS-EXCEL using ODS (output Delivery System).\n\u2022 Worked using SAS Macros and SAS procedures in Banking for credit card report generation and upgradation.\n\u2022 Working experience in coding using SQL, Procedures/Functions.\n\u2022 Rewrite SAS codes to fine-tune and automate various SAS processes\n\u2022 Strong communication skills and ability to work in groups as well as independently\n\u2022 Good analytical skills pertaining understanding a business domain coupled with excellent teamwork and strong communication skills.\n\u2022 Worked on data quality rules and ensured the proper implementation of data quality during the creation and modification of data, according to Enterprise Data and Analytics data quality standards, guidelines, metrics and quality-expectations.\n\u2022 Ensured accuracy, completeness, integrity, consistency, uniqueness, and timeliness of data.\n\u2022 Executed Enterprise Data and Analytics data quality tests/assessments and data cleansing activities.\n\u2022 Participated in the discussions and worked on tasks assigned from cross-functional teams.-FSB(Federal Savings Bank)., Bank Data Warehouse."", u'Sr. SAS Data Analyst/Developer\nEducation Testing Services (ETS) - Princeton, NJ\nApril 2013 to July 2013\nResponsibilities:\n\u2022 Formulate and define system scope and objectives for assigned projects.\n\u2022 Gathering requirements and documentation, unit test case development, and defining and documenting the integration testing of new applications.\n\u2022 Implement complex data file manipulation involving matching, merging, cleaning and modifying operational testing program data (primarily using SAS)\n\u2022 Assist in setting and maintaining Incoming and Outgoing Data Quality Standards.\n\u2022 Ensure that statistical analysis requirements are met for assigned projects.\n\u2022 Preparation of clear detailed specifications from which programs will be written by team developers.\n\u2022 Consultation with ETS Systems and Technology staff in the modification, design and implementation of systems or projects using multiple processing techniques and system technologies, database and structured design techniques.\n\u2022 Analyze data; Work with ETS business, and technology areas, as well as implement data and manipulation technologies and transition the support of the systems to a production environment.\n\u2022 Develop and evaluate unit testing plans to ensure design requirements are met.\n\u2022 Provide technical knowledge of all phases of applications systems analysis and programming and have a good understanding of the business or function for which the applications are designed.\n\u2022 Responsible for contributing to the design and development of technologies for executing large and complex data analysis projects.\n\u2022 Design and check complex data file manipulation rules involving matching, merging, cleaning and modifying operational testing program data\nDepartment of Health and Mental Hygiene, NYC, NY Sr. SAS Data Analyst/Developer', u'Sr. SAS Data Analyst/Developer\nDepartment of Health and Mental Hygiene - NY\nOctober 2012 to March 2013\nDescription:\nEpiQuery (EQ), an interactive website that allows epidemiologic analysis of NYC DOHMH specific public health data. This is a multidisciplinary unit with the goal of combining cutting-edge epidemiology research and data analyses with policy development, recommendations and data communication.\nResponsibilities:\n\u2022 Responsible for handling two projects namely Update the Survey Module and Creation of new module.\n\u2022 Generate a new dataset by modifying previous SAS code versions\n\u2022 Generated tables, reports, listings and graphs including Patient Demography and Characteristics, Adverse Events, Laboratory etc. according to Statistical Analysis Plan (SAP) using PROC REPORT, PROC GPLOT for Clinical Study Reports in compliance with 21 Code of Federal Regulations (21 CFR Part 11), ICH-GCP guidelines\n\u2022 Updated current data to include additional variables based on the user requirements\n\u2022 Incorporated the new data into the EQ module by modifying the metadata base (Access), Teradata and updating HTML/JavaScript code\n\u2022 Assisted in data checks of the new data and responsible for making the final revised module live\n\u2022 Analyses of requirements and datasets that produce data tables and graphs by demographics and bivariate.\n\u2022 Written SAS code for each level of the module, including embedded HTML and JavaScript to build webpages and working within the SAS eBI system.\n\u2022 Created a new Access database containing the metadata for the module\n\u2022 Worked closely with the BES team to design a working internal site, providing a testing environment and demonstrating the site as developed.\n\u2022 Completed the documentation and created the development process for the above project modules\nEnvironment: Windows 2000, SAS 9.1, SAS9.3, SAS/ODS, SAS/SQL, MS Excel, database Oracle, SAS/ACCESS, JavaScript, SAS/EG, BASE SAS, SAS/ MACRO, SAS/ SQL.', u'SAS Programmer/Data Analyst\nHEALTHFIRST - New York, NY\nDecember 2011 to September 2012\nDescription:\nHealthfirst is one of the businesses in New York and New Jersey that comprise a health insurance services company. It focuses mainly on medical, pharmacy, vision and dental care services. Worked as part of the Enterprise Data Management group at Health first.\nResponsibilities:\n\u2022 Understanding the Business and its Requirements\n\u2022 Gathering the requirements from user\n\u2022 Modified data using SAS/BASE and Macros\n\u2022 Modified data for business analysis\n\u2022 Creation of Data Sets on the Remote server\n\u2022 Coordinating the production of monthly, quarterly, and annual performance reports for senior management.\n\u2022 Extracting data from the database using SAS/Access, SAS SQL procedures and create SAS data sets.\n\u2022 Performed statistical analysis on clinical studies including randomization, sample size calculations, power analysis, parameter estimation and hypothesis testing. SAS /STAT procedures such as UNIVARIATE, TTEST, ANOVA, FREQ and MEANS were used to carry these analyses\n\u2022 Creating SAS dataset from tables in Database using SAS/Access. Retrieved the MEMBERS data from flat files, oracle database and converted to SAS data sets for Analysis using SAS/STAT procedures\n\u2022 Coding SAS programs with the use of Base SAS and SAS/Macros for ad hoc jobs\n\u2022 Created reports for online and batch applications for various servers\n\u2022 Coding SAS programs with the use of SAS/BASE and SAS/Macros for ad hoc jobs\n\u2022 Retrieved , monitored and analyzed MEMBERS DATA using MHS\n\u2022 Run reporting programs and download the results into EXCEL and build pivot tables\n\u2022 Moved data set across platforms (from PC and Mainframe MHS)\n\u2022 Prepared new Datasets from raw data files using Import Techniques and modified existing datasets using Set, Merge, Sort, and Update, Formats, Functions and conditional statements\n\u2022 Created complex and reusable Macros and extensively used existing macros and developed SAS Programs for Data Cleaning, Validation, Analysis and Report generation. Tested and debugged existing macros\n\u2022 Attended project team meetings, worked with, Data Managers, Business Users and offshore team as appropriate\n\u2022 Involved in extracting, analyzing data from the data-warehouse using SAS.\n\u2022 Created SAS data sets by extracting data from Oracle tables using SAS/CONNECT.\n\u2022 Used ULTRA EDIT and VSS features to connect to multiple databases for comparing views and checking syntax errors for performance tuning and quality control\n\u2022 Worked with Financial departments to develop required applications for performing complicated analysis\n\u2022 Used PROC SQL for ad-hoc report programming, to perform the tasks like data manipulation of multiple views in Oracle, creation of the ad hoc reports for business user, preplanning and doing ad hoc joining of multiple Oracle views.\n\u2022 Created and maintained the project documentation like functional and technical specifications\n\u2022 Documented the SAS project report notice , which included the indications of purpose of the requirement, from where it is been read and written to , what kind of process it is, how the process runs and finally the installation instruction for the upgraded data dictionaries and the new or modified program\n\u2022 Worked by abiding to the HIPAA Privacy regulations required by the health care providers and organizations, as well as their business associates.\n\u2022 Created test plans for QA validation purposes.\n\u2022 Coordinated with the offshore team.\nEnvironment: Windows 2000, SAS 9.1, SAS9.3, SAS/ODS, SAS/SQL, MS Excel, database Oracle, IBM ASO/400 and IBM compatibles, BASE SAS, SAS/ MACRO, SAS/ SQL.', u'SAS Programmer Analyst/Data Analyst\nMATHEMATICA INC - Princeton, NJ\nMay 2011 to August 2011\nResponsibilities\n\u2022 Worked on in-house Survey project handling the responsibilities of SAS programming and QA analysis/testing\n\u2022 Performed data and QA analysis, tested/completed the deliverables and presented the analyzed/output data based on the requirements understanding of business process, the data and the user.\n\u2022 Transformed the statistical data in various formats (excel, access) into SAS data sheets. Provided statistical programming expertise in the production of QA analyses, tabulations, reports and listings.\n\u2022 Derived from specifications of survey regarding editing, coding, reports, debugging etc\n\u2022 Accompanied in Phase I various consulting sessions and provided support in statistical analysis including choice of experiment design, management of data, statistical procedures for analyzing experiments, interpretation of the results, statistical modeling and statistical computing.\n\u2022 Developed programmes for cleaning, transforming and modifying the data daily.\n\u2022 Updated documentation during the life cycle of QA testing like design processes, business specs, comments in the code, record the issues/problems and record the inputs/outputs\n\u2022 Actively participated in internal QA peer reviews and managing the project folders on project shared drive and SharePoint site\n\u2022 Used PROC compare to compare the data before and after editing\n\u2022 Developed various SAS reports for Survey project and involved in code reviews for all the developed reports\nEnvironment: Base SAS v9.2, SAS/Macros, SAS/SQL, SAS/ACCESS, SAS/STAT, Windows XP, KEDIT.', u'SASProgrammer/Data Analyst\nDepartment of Developmental Disabilities, COLOMBUS, OHIO\nJanuary 2010 to January 2011\nResponsibilities\n\u2022 Worked on Agile methodology\n\u2022 Performed data analysis, completed the projects and presented the analyzed data. Transformed clinical data in various formats (excel, CSV) into SAS data sheets. Provided statistical programming expertise in the production of analyses, tabulations, graphs and listings\n\u2022 Accompanied statisticians in various consulting sessions and provided support in statistical analysis including choice of experiment design, management of data, statistical procedures for analyzing experiments, interpretation of the results, statistical modeling and statistical computing\n\u2022 Developed macros for cleaning, transforming and modifying the data daily\n\u2022 Accomplished data manipulation on SAS data sets using techniques such as merging, appending, concatenating and sorting\n\u2022 Extensively used statistical procedures like PROC FORMAT, PROC MEANS, PROC FREQ, PROC IMPORT, PROC UNIVARIATE, PROC TABULATE\n\u2022 Extensively used statistical procedures like PROC FREQ and PROC MEANS to compute elementary statistical measures which include descriptive statistics based on moments, quintiles, confidence intervals, frequency counts, correlations and distribution tests\n\u2022 Applied statistical techniques to analyze and validate the model implementations\n\u2022 Created queries to generate HTML reports and tables using PROC SQL, PROC TABULATE and PROC REPORT to update the data daily\nEnvironment: Base SAS v9.2, SAS/Macros, SAS/SQL, SAS/ACCESS, SAS/STAT, SAS/ODS, Windows XP.', u""SAS Programmer\nWachovia - Philadelphia, PA\nFebruary 2009 to December 2010\nResponsibilities:\n\u2022 This project involved experiment design to evaluate different strategies for customer retention. Integrated customer data, addressed data issues to create a clean and consistent database. Reading raw data files and manipulating the data sets\n\u2022 Convert excel data files into SAS data sets.\n\u2022 Creating SAS datasets by using Base SAS\n\u2022 Involved in development and enhancement of SAS programs.\n\u2022 Documentation , Analysis through using SPSS and SAS\n\u2022 Created plots using PROC GPLOT in SAS.\n\u2022 Generating graphs using SAS/GRAPH\n\u2022 Developed SAS Dataset and generated results into the excel sheets.\n\u2022 Responsible for retrieving and manipulating datasets using SAS, developing and executing SAS programs in support of statistical modeling projects, coding and running SAS programs for internal process validation, and maintaining and managing teradata.\n\u2022 As a SAS programmer supported Marketing Team and worked with Statisticians in analyzing the data. Tested\n\u2022 whole process (Unit, UAT).\n\u2022 Validation the data using latest SAS functions.\n\u2022 Used SAS/SQL to pull data out of a relational database and aggregate to provide detailed reporting based on the user requirements.\n\u2022 Responsible for providing statistical research analyses and data modeling support for mortgage product.\n\u2022 Analyzing credit report to determine credit worthiness and performing risk evaluations.\n\u2022 Ensuring loans are structured and priced consistently with bank policy and commensurate with risk.\n\u2022 Analyzing and calculating debt to income ratio for evaluating mortgagor's financial risk and provide recommendation to mortgagor to lower debt to income.\n\u2022 Evaluating and recommending affordable options to mortgagor to help reduce financial loss.\n\u2022 Run daily report to help determined how many loans are in foreclosure or will be in foreclosure within the next ten days.\n\u2022 Researched and analyzed all required documents for accuracy such as signature, bank statement.\nEnvironment: SAS 9.3/Base, Windows, SAS/ BASE, SAS/MACRO, SAS/SQL, SAS/GRAPH, SAS/Access, SAS/VIEW, SAS/Enterprise Guide, DB2."", u'Data Analyst/Programmer\nReddy Labs - Hyderabad, Telangana\nJanuary 2006 to October 2008\nDescription: Dr Reddy Labs is one of the leading Pharmaceutical companies in India that has initiated clinical trials on cardio-vascular diseases.\nResponsibilities:\n\u2022 Responsible for data collection, management and manipulation of clinical database.\n\u2022 Provided statistical and analytical support to Consumer Economics, Planned and performed load research\n\u2022 studies and analysis.\n\u2022 Prepared statistical summaries and reports on customer load usage surveys for use in regulatory hearings.\n\u2022 Used SAS for pre-processing data, SQL queries, data analysis, generating reports, graphics, and statistical analyses.\n\u2022 Developed efficient, well-documented, readily comprehensible and modifiable SAS code using SAS/ Base\n\u2022 and SAS/ Macro facility.\n\u2022 Maintained clinical trial data base, accessed various database to gather data, performed a table lookup and translated data values for meaning and readability.\n\u2022 Also identified problems with the data, if there were any, and also produced derived data sets, tables,\n\u2022 listings and figures, which analyzed the data.\n\u2022 Produced quality customized reports by using PROC TABULATE, REPORT, and SUMMARY and also \u2022 provided descriptive statistics using PROC Means, Frequency, and Univariate.\n\u2022 Supported the research staff for technical and programming help. Worked with Bio statistician to analyze the results obtained from various statistical procedures like PROC ANOVA, GLM, t test.\n\u2022 Involved in clinical trials programming of data tables and listings for reporting on safety and efficacy\nEnvironment: SAS, SAS/SQL, SAS/BASE, SAS/MACROS, SAS/GRAPH, Oracle 8 PL/SQL, ANOVA, Windows and UNIX.', u'Jr.DataAnalyst\nErudite Soft - Hyderabad, Telangana\nJanuary 2004 to December 2005\nResponsibilities:\n\u2022 Driving design reviews and creating design specs\n\u2022 Development of code and fixing software defects\n\u2022 Providing production support during regular and off business hours\n\u2022 Performing software defect investigations and required analysis\n\u2022 Monitoring deployments and performing validation\n\u2022 Reviewing functional specifications and test plans for software application projects\n\u2022 Worked on the design and implementation for Client Applications developed in C#, .NET, HTML, MySQL.\n\u2022 Updated and completed documentation for all the work performed above\nEnvironment: C, C++, C#, SQL, PL-SQL, XML, HTML, Window Installer, Win32']","[u""Bachelor's""]",[u'']
0,https://resumes.indeed.com/resume/0e8bc20c6f13cdad,"[u'Data Analyst\nWalmart - San Bruno, CA\nSeptember 2016 to March 2018\nResponsibilities: \u2022 Analyze Digital Store Operations in order to gain insights into e-commerce business, help drive traffic to site and improve overall customer experience.\n\u2022 Report on KPIs such as GMV, IMU, traffic, conversion, FY plan, forecast and actuals.\n\u2022 Provide category specialists with reporting on products across departments.\n\u2022 Support ad hoc projects, i.e. product taxonomy updates, item setups, etc.', u'Senior Financial Analyst, Cardiology\nMedAssets - Oakland, CA\nSeptember 2015 to February 2016\nResponsibilities: \u2022 Developed advanced reports and cost models for sourcing activities such as RFP analysis, cost/benefit analysis, process analysis.\n\u2022 Developed financial models and graphical presentations to demonstrate financial value of client agreements.\n\u2022 Communicated with suppliers to facilitate submission of pricing proposals\n\u2022 Worked directly with team leadership in validating data deliverables,\nresolution of monthly baseline and savings analysis.\n\u2022 Designed and evaluated equipment quality surveys', u""Data Analyst\nGreat Place To Work - San Francisco, CA\nAugust 2015 to September 2015\nResponsibilities: \u2022 Cleaned data for Fortune Magazine's 'Best Companies To Work For' lists\n\u2022 Analyzed raw survey data in Excel and organized into desired format\n\u2022 Inventoried and organized candidate supporting materials"", u'Data Analyst\nAT&T - Austin, TX\nJanuary 2013 to July 2014\nResponsibilities: \u2022 Audited customer accounts to track contract commitment levels and validated\npotential uplift revenue on T1 re-rate project.\n\u2022 Created data validations for production platform, maintained Access database\n\u2022 Ensured data integrity, quality assurance testing, resolved billing issues,\ntrained and supervised new team members.\n\u2022 Produced weekly, monthly, ad hoc reports\n\nAchievements: \u2022 Played key role in growth and success of project, helping attain $127M in revenue through Dec 2013, 295% of $43M target.\n\u2022 Optimized work processes by updating SOPs into reference charts, created\ntechnical methods and procedures documentation, trained new hires.\n\u2022 Created user interface for Access production platform', u'Financial Analyst\nAmerican Campus Communities - Austin, TX\nAugust 2012 to January 2013\nResponsibilities: \u2022 Performed financial analysis of real estate operations, financial modeling of monthly, quarterly and year-end NOI projections in accordance with GAAP.\n\u2022 Prepared quarterly earnings and annual budget packages, forecasts, ad-hoc\nanalyses and other financial reports under tight deadlines.\n\u2022 Provided support to corporate and property staff, creating financial targets,\nproviding utility analysis, and revenue/expense variance explanations.', u'Data Analyst\nAT&T - Austin, TX\nJune 2010 to August 2012\nResponsibilities: \u2022 Tracked contract commitment levels and validated potential uplift revenue\n\u2022 Identified billing issues, generated production reports, trained new hires\n\nAchievements: \u2022 Consolidated auditing protocols, increasing efficiency and productivity\n\u2022 Provided vital feedback during transition to Access-based production platform', u'Self-employed\nACD Construction - Austin, TX\nMay 2008 to June 2010\n\u2022 Operated home remodeling business, advertising services, managing crew\n\u2022 Developed keen business and customer service skills', u'Wireless Billing Analyst\nCentennial Communications Corp - Wall, NJ\nApril 2007 to April 2008\nResponsibilities: \u2022 Configured POS tables, confirmed accuracy of product updates\n\u2022 Reviewed wireless billing invoices to verify accuracy of charges\nAchievements: \u2022 Significantly improved organization, speed and efficiency of invoice verification procedures using macros and data validation.']",[u'BS in Finance'],"[u'Kean University Union, NJ']"
0,https://resumes.indeed.com/resume/a89b668113920051,[u'Data Analyst'],[u'Bachelors in Public Relations'],"[u'Montclair State University Montclair, NJ\nFebruary 1998 to July 2002']"
0,https://resumes.indeed.com/resume/0d3864056df08baa,"[u'Data Analyst\nNewDay Financial - Fulton, MD\nMay 2015 to Present\n\u2022 Overseeing digital marketing campaigns; enhancing marketing performance and customer experience\n\u2022 Providing suggestions on direct mail campaigns and TV campaigns on weekly basis\n\u2022 Operating large-scale database, data processing, data pruning, data sampling, and exploratory data analysis\n\u2022 Data mining and pattern recognition analysis, identifying significant factors that could affect research results\n\u2022 Analyzing, interpreting, and reporting statistical and mathematical model outputs; evaluating different modeling and sampling processes to best fit business goals using existing datasets\n\u2022 Individual research on business-related topics and present the findings at the seminars\n\u2022 Automating reporting procedure to provide near real-time updates and to better support executive decisions\nProjects:\nOverseeing Digital Campaigns\n\u2022 Cooperating with Google, Bing, Facebook team, FELD creative team and internal web dev team to improve company\ndesktop and mobile website and visitor experience\n\u2022 Developed and maintaining sophisticated .NET campaign management and reporting applications that integrate with internal database and brought together online and offline data\n\u2022 Optimized the digital search campaigns by shifting 70% of the cost, impressions and clicks to exact match keywords within 2 months. Cost per decreased more than 10% as a result of a keyword bidding strategy based on in depth\nunderstanding of customer intent, which is made possible a by better keyword - search query match\n\u2022 Brought in Invoca call tracking service to enable phone call attribution down to keyword and placement level; work with Invoca tech support team and completed all the integration pieces within 3 days\n\u2022 Developed and maintaining digital marketing cube, integrating data from AdWords, Google Analytics, Bing and\nFacebook with internal loan data to generate business insights; using Google, Bing, Facebook, Invoca APIs to automate the workflow\n\u2022 Restructured the AdWords and Bing Ads account and doubled the digital leads within 2 weeks without raising extra\nbids\n\u2022 Constantly testing different targeting methods and their combinations; identify the working and non-working part and keep optimizing the accounts\n\u2022 Achieved extra lifting in performance by utilizing custom lists generated from customer emails or from direct mail\naddress\n\u2022 Conducting keyword research and search term research using Natural Language Toolkit(NLTK) and adjust the keyword\nbids according to the result\n\u2022 Providing weekly proposal on account improvement based on individual research\n\u2022 Providing weekly account performance summary to C-suit managers\n\u2022 Documenting strategic and operational changes\nExpanding Mailing List with Bootstrapped Trend Data From Bureau Monthly Record\n\u2022 Bootstrap 6-month trend data for multiple attributes consumer credit data from Equifax and TransUnion\n\u2022 Expanded the mailing list based on the extra information extracted\nExploring New Segment in Direct Mail Campaign\n\u2022 Pinpointed a profitable customer group within a customer segment known as zero-mortgage-balance segment reported by Equifax and Transunion, whose characteristic is low response and low closing loan amount\n\u2022 Extracted information from 120 million past mailing records using multidimensional cube and constructed testing rules;\napplied the rules to the subject segment and achieved average closed loan balance of 200K with average response rate\nPredicting A Federal Rate Hike\n\u2022 Monitoring short-term interest rate and trying to predict a Federal rate hike; so far the predicted outcomes align with the reality\n\u2022 Approaching the problem in two ways: interpret the market view of probability of a hike using Fed Fund Futures prices;\ncompare ARCH regression model predicting the personal consumption expenditure (PCE) and see the distance between the predicted value and the two percent target at a given point\nPipeline Monitoring - Predicting Monthly Closes of Loan Products\n\u2022 Predicting monthly closes of loan products using rolling transition matrix; constructed daily transition matrices from a\nthree-year loan status log; tested different weights and applied the one with least variation among the simulation results\n\u2022 Used as one of baseline models when setting expectation of monthly closing and for hedging purposes\nPredicting Residential Property Value Based on Credit Data\n\u2022 Performed variable selection utilizing gradient boost machine; built a logistic regression model with necessary variable\ntransformation; optimized the model parameters using five leave one out cross-validation\n\u2022 A 55K estimated standard deviation is due to subject nature and limited observations, and is slightly inferior to the commercial model\nPredicting Customer Calls Generated from TV Commercials\n\u2022 Predicting customer calls generated from TV commercials on hourly basis using available pre-log data provided by TV\nstations such as FOXNC and CNN;\n\u2022 Used two-step procedure: first forecast the total calls generated from one airing spot and then allocate the calls to the possible time range based on historical distribution; compared different data mining methods and used random forest as the final tool to implement the model\n\u2022 Used by company sales teams to better allocate their time and arrange shifts', u'Research Assistant\nRutgers Business School\nJuly 2014 to May 2015\nAssisted financial and economics department chair with research, involving data aggregation, using sources from\nFactiva, WRDS, SEC EDGAR and other financial websites\n\u2022 Exposed to topics such as hedge fund activism and phenomenon like Bowman Paradox']","[u'Master of Quantitative Finance in Econometrics', u'in Bachlor of Economics']","[u'Rutgers Business School Newark, NJ\nMay 2015', u'Shanghai Jiao Tong University Shanghai, CN\nJune 2013']"
0,https://resumes.indeed.com/resume/9dc49fb77142798e,"[u'Data Scientist\nVerizon - Dallas, TX\nJanuary 2016 to Present\nDescription:\nThe Project was mainly focused on reducing customer churn by understanding the customer behavior using Statistical Modelling, Machine Learning techniques and take necessary steps to reduce customer churn as much as possible.\nResponsibilities:\n\u2022 Working closely with marketing team to deliver actionable insights from huge volume of data, coming from different marketing campaigns and customer interaction matrices such as web portal usage, email campaign responses, public site interaction, and other customer specific parameters.\n\u2022 Characterizing false positives and false negatives to improve a model for predicting customer churn rate.\n\u2022 Consumer segmentation and characterization to predict behavior. Analyzing promoters and detractors (defined using Net Promoter Score).\n\u2022 Outlier detection using high-dimensional historical data. Acquiring, cleaning and structuring data from multiple sources and maintain databases/data systems. Identifying, analyzing, and interpreting trends or patterns in complex data sets.\n\u2022 Developing, prototype and test predictive algorithms. Filtering and ""cleaning"" data and review computer reports, printouts, and performance indicators to locate and correct code problems.\n\u2022 Developing and implementing data collection systems and other strategies that optimize statistical efficiency and data quality.\n\u2022 Used different statistical models like regression and classification models to create contact scoring models. Also used clustering to the customer data profiles to do customer segmentation and analysis.\n\u2022 Interpreting data, analyze results using statistical techniques and provide ongoing reports.\n\u2022 Building a recommender system based on client\'s past renewal history to upsell and cross-sell them other related products or services. Created a recommendations engine that finds related customers, products.\nEnvironment: Python- Pandas, Numpy, Scikit-Learn, TensorFlow - ANN, SciPy, Seaborn, Matplotlib, SQL, Machine Learning, Deep Learning. R-Foreign, ggplot, igraph, lattice, MASS, mice and logit.', u'Data Science Pilot Project\nPitt Plastic - Pittsburg, KS\nMay 2015 to December 2015\nDescription:\nThe project was focused on sales prediction by using Sales training data into supervised classification algorithm to predict customer churn.\nResponsibilities:\n\u2022 Supported sales forecasting & planning team by improving time series & principal component analysis.\n\u2022 Utilized machine learning techniques for predictions & forecasting based on the Sales training data.\n\u2022 Executed overall data aggregation/alignment & process improvement reporting within the sales dept.\n\u2022 Managed Data quality & integrity using skills in Data Warehousing, Databases & ETL.\n\u2022 Monitored and maintained elevated levels of data analytic quality, accuracy, and process consistency.\n\u2022 Assisted sales management in data modeling.\n\u2022 Ensured on-time execution and implementation of sales planning analysis and reporting objectives.\n\u2022 Worked with sales management team to refine predictive methods & sales planning analytical process.\n\u2022 Executed and monitored the accuracy and efficiency for sales forecasts & reporting.\n\u2022 Prepared Dashboards using calculations, parameters in QlikView.\n\u2022 Supported consistent implementation of company reporting and sales process initiatives.\n\u2022 Used Python to identify customer classification, tree map, and regression models.\n\u2022 Performed forecasting and time series analysis of customer likes and dislikes.\n\nEnvironment: ETL, QlikView, Python, Machine Learning, SQL', u'Data Analyst / Data Engineer\nCigna - Hyderabad, Telangana\nMay 2013 to December 2014\nDescription:\nDealing with scenarios related to Healthcare fraud, waste and abuse detection; denial claims management; clinical pathways optimization; and health plan member profitability- Understanding and managing clinical variation across a hospital system, for patients undergoing specific types of surgery.\nResponsibilities:\n\u2022 Characterizing false positives and false negatives to improve a model for predicting overpaid claims, Consumer segmentation and characterization to predict behavior.\n\u2022 Outlier detection using high-dimensional historical data.\n\u2022 Analyzing promoters and detractors (defined using Net Promoter Score).\n\u2022 Installed and configured Hadoop MapReduce, HDFS, Developed multiple MapReduce jobs in Python for data cleaning and preprocessing.\n\u2022 Supported Map Reduce Programs those are running on the cluster. Involved in loading data from UNIX file system to HDFS.\n\u2022 Experienced in managing and reviewing Hadoop log files.\n\u2022 Involved in writing Hive queries to load and process data in Hadoop File System.\n\u2022 Exported data from Impala to Tableau reporting tool, created dashboards on live connection.\n\u2022 Gained very good business knowledge on health insurance, claim processing, fraud suspect identification, appeals process etc.\n\u2022 Maintained integrity of the database by implementing different validation techniques to the data uploading procedures.\n\u2022 Defended conjunction between ERP system and database using SQL reporting services.\n\u2022 Developed dashboards in Excel with SQL connections to ensure smooth procedural flow and analysis.\n\u2022 Identified and proposed process enhancements.\n\u2022 Quantified multiple stocking proposals and contracts with customers which increased business by approximately 20% per customer.\n\u2022 Developed ad-hoc reports upon requests using MS Access and MS Excel.\nEnvironment: Hadoop, MapReduce, Hive, Impala, Python (pandas, NumPy, scikit-learn), SQL Server, Excel, Tableau', u'Data Analyst\nHexaware Technologies Hyderabad - Hyderabad, Telangana\nJune 2011 to April 2013\nProject 1: Customer Satisfaction Analysis\nCustomer Satisfaction Analysis Using Regression Models\nResponsibilities:\n\u2022 Identifying what factors could influence the overall satisfaction of consumers. Range (1-5).\n\u2022 Considered the SMG (Service Management Group) database survey results in analyzing the impact on overall customer satisfaction.\n\u2022 Analyzed business process workflows and assisted in the development of ETL procedures for mapping data from source to target systems\n\u2022 We used Ordinal logistic regression methodology in explaining the importance of features.\n\u2022 The analysis involved predicting the overall satisfaction - ordinal rating, by analyzing the impact of each independent factors in explaining the output.\n\u2022 Packages used: MASS package, plot function for Ordinal logistics regression model.\nEnvironment: R Studio, SQL Server, Dplyr, Tidyr, ggplot2, Tableau, MASS package - plot function.\nProject 2: Annual Marketing Budget Allocation.\nAnnual Marketing Budget Allocation.\nResponsibilities:\n\u2022 Responsible for Data collection and data preparation and normalizing the data.\n\u2022 Used SQL, ETL tool, R Studio, and Python for data preparation.\n\u2022 Supported data consultants in the data modeling phase.\n\u2022 Used Constraint optimization algorithms (USED EXCEL) to optimize the marketing budget.\n\u2022 Used Time Series Models - decomposition of time series, trend, and seasonality detection, forecasting and exponential smoothing in predicting the market share and brand share to allocate the Marketing budget\nEnvironment: R , Python (pandas, NumPy, scikit-learn), SQL Server, Excel , ETL.']",[],[]
0,https://resumes.indeed.com/resume/a233a967901d759c,"[u'Sr. Data Analyst\nBank of America - Richmond, VA\nAugust 2017 to Present\nResponsibilities:\n\u2022 Expert in data validation, cleansing, consolidation and mining for sense, consistency and accuracy and compliance standards.\n\u2022 Captured data lineage for all the top level reports by validating the authorized data sources with system of records and system of origin.\n\u2022 Was actively in the Server migration and the Fresh QlikView Installation process\n\u2022 Experienced in QlikView Server and Publisher maintenance - Creating scheduled jobs for QVD extracts and report reloads\n\u2022 Worked as a QlikView Technical Consultant for a wide variety of business applications\n\u2022 Experience in creating Complex Spotfire Dashboards or Reports Using Tibco Spotfire Professional.\n\u2022 Developed cross table, Bar chart, Tree map and complex reports which involves Property Controls, Custom Expressions\n\u2022 Created an object oriented set of modular tools in Python for image and other signal processing, R&D and final pipelines which also log data provenance\n\u2022 Drove tailored business solutions and reengineering business processes through observance of KPIs (Key Performance Indicators), metadata and data mapping, utilization of data mining tools, and the creation of executive narratives via statistical packages (SPSS, Excel).\n\u2022 Led in the selection, deployment and maintenance of data management and reporting systems including operational data stores, data marts, enterprise data warehouses, and enterprise reporting/analytics tools.\n\u2022 Built BI dashboards, data models, structures, and database views, etc. to serve as a foundation for reporting and systems integration goals/strategies.\n\u2022 Advised projects involving the ETL related activities and the migration or conversion of data between enterprise data systems.\n\u2022 Coordinated interactions between central IT, business units, and data stewards to achieve desired organizational outcomes.\n\u2022 Gathered and analyzed existing physical data models for in scope applications and proposed the changes to the data models according to the requirements.\n\u2022 Advised on and enforces data governance to improve the quality/integrity of data and oversight on the collection and management of operational data.\n\u2022 Involved in Data Analysis, Data Migration, Data Cleansing, Transformation, Integration, Data Import, and Data Export through the use of multiple ETL tools such as Abilities and Informatica Power Center and testing and writing SQL and PL/SQL statements - Stored Procedures, Functions, Triggers and packages.\n\u2022 Gathered and analyzed business data requirements and model these needs. In doing so, work closely with the users of the information, the application developers and architects, to ensure the information models are capable of meeting their needs.\n\u2022 Extensively used Erwin r9.6 for Data modeling. Created Staging and Target Models for the Enterprise Data Warehouse.\n\u2022 Designed Star and Snowflake Data Models for Enterprise Data Warehouse using ERWIN.\n\u2022 Worked with Data Steward Team for designing, documenting and configuring Informatica Data Director for supporting management of MDM data.\n\u2022 Developed SAS programs using SAS/BASE, SAS/SQL, SAS/STAT, and SAS/MACROS for descriptive and inferential statistical analysis and data displays.\n\u2022 Creation of BTEQ, Fast export, MultiLoad, TPump, Fast load scripts for extracting data from various production systems.\n\u2022 Transformed Logical Data Model to Physical Data Model ensuring the Primary Key and Foreign key relationships in PDM, Consistency of definitions of Data Attributes and Primary Index considerations.\n\u2022 Performed daily tasks including backup and restore by using SQL Server tools like SQL Server Management Studio, SQL Server Profiler, SQL Server Agent, and Database Engine Tuning Advisor and Statements for Applications by using T-SQL.\n\u2022 Involved in Capacity Planning, Database Normalization and De-normalization process.\n\u2022 Designed ER diagrams (Physical and Logical using Erwin) and mapping the data into database objects and identified the Facts and Dimensions from the business requirements and developed the logical and physical models using Erwin.', u'Senior Data Analyst\nAllied Insurance - Denver, CO\nJanuary 2016 to July 2017\nResponsibilities:\n\u2022 Worked on the integration of existing systems at Data warehouse and Application systems level.\n\u2022 Extensively used SQL for Data Analysis and to understand and documenting the data behavior.\n\u2022 Enthusiast and Experienced QlikView developer- the pioneered in-memory Dash boarding tool\n\u2022 Worked as a QlikView Technical Consultant for a wide variety of business applications\n\u2022 Reversed engineered existing data bases to understand the data flow and business flows of existing systems and to integrate the new requirements to future enhanced and integrated system.\n\u2022 Designed the procedures for getting the data from all systems to Data Warehousing system.\n\u2022 Worked with ETL Architects and developer to design performance centric ETL mappings.\n\u2022 Designed both 3NF data models for ODS, OLTP systems and dimensional data models using star and snow flake Schemas.\n\u2022 Created data catalog to understand and extract meaningful information by identifying context, location and lineage of the data leveraging Collibra as the preferred Data Governance and analytics tool.\n\u2022 Extensively worked on documentation of Data Model, Mapping, Transformations and Scheduling jobs.\n\u2022 Worked extensively with Business Objects Report developers in creating data marts and develop reports to cater the existing business needs.\n\u2022 Designed Mapping Documents and Mapping Templates for Informatica ETL developers.\n\u2022 Extensively used and created Macros for the efficiency and accuracy.\n\u2022 Deployed naming standard to the Data Models at enterprise level and followed company standard for Project Documentation.\n\u2022 Used data Integration tool Pentaho for designing ETL jobs in the process of building Data warehouses and Data Marts.\n\u2022 Developed and provided formal training and presentations to all DWH users ensuring an understanding of the dimensional model, metadata, and effective usage.\n\u2022 Designed ER diagrams, logical model (relationship, cardinality, attributes, and, candidate keys) and convert them to physical data model including capacity planning, object creation and aggregation strategies, partition strategies, Purging strategies as per new architecture.\n\u2022 Designed and developed strategies for Data Conversions and Data Cleansing.\n\u2022 Created Data mappings, Tech Design, loading strategies for ETL to load newly created or existing tables.\n\u2022 Extensively used Agile methodology as the Organization Standard to implement the data Models.\n\u2022 Created Schema objects like Indexes, Views, and Sequences, triggers, grants, roles, Snapshots.\n\u2022 Developed Star and Snowflake schemas based dimensional model to develop the data warehouse.\n\u2022 Developed statistics and visual analysis for warranty data using MS Excel, MS Access and Tableau Software.\n\u2022 Developed strategies and loading techniques for better loading and faster query performance.', u'Data Analyst\nAetna - Hartford, CT\nMarch 2014 to December 2015\nResponsibilities\n\u2022 Used and supported database applications and tools for extraction, transformation and analysis of raw data.\n\u2022 Supported complete pedigree and provenance for each data object and product.\n\u2022 Analyzed business requirements, system requirements, data mapping requirement specifications, and responsible for documenting functional requirements and supplementary requirements in Quality Center.\n\u2022 Developed, managed and validated existing data models including logical and physical models of the data warehouse and source systems utilizing a 3NFmodel and dimensional data model.\n\u2022 Assisted in semantic layer design and development of Semantic layer data model.\n\u2022 Extensively used Star Schema methodologies in building and designing the logical data model into Dimensional Models.\n\u2022 Created data masking mappings to mask the sensitive data between production and test environment.\n\u2022 Used Teradata SQL Assistant, Teradata Administrator, PMON and data load/export utilities like BTEQ, Fast Load, Multi Load, Fast Export, Tpump on UNIX/Windows environments and running the batch process for Teradata.\n\u2022 Worked with data investigation, discovery and mapping tools to scan every single data record from many sources.\n\u2022 Performed data analysis and data profiling using complex SQL on various databases such as Oracle and Teradata.\n\u2022 Performed various data analysis at the source level and determined the key attributes for designing of Fact and Dimension tables using star schema for an effective Data Warehouse and Data Mart.\n\u2022 Wrote several shell scripts using UNIX Korn shell for file transfers, error logging, data archiving, checking the log files and cleanup process.\n\u2022 Worked with end users to gain an understanding of information and core data concepts behind their business.\n\u2022 Was responsible for creating test cases to make sure the data originating from source is making into target properly in the right format.\n\u2022 Worked on data modeling and produced data mapping and data definition specification documentation.\n\u2022 Used existing UNIX shell scripts and modified them as needed to process SAS jobs, search strings, execute permissions over directories etc.\n\u2022 Extensively completed data quality management using information steward and did extensive data profiling.\n\u2022 Extensively worked on Talend Designer Components-Data Quality (DQ), Data Integration (DI) and Master Data Management (MDM).\n\u2022 Wrote simple and advanced SQL queries and scripts to create standard and ad hoc reports for senior managers.\n\u2022 Identified source systems, their connectivity, related tables and fields and ensure data suitably for mapping', u'Data Analyst\nNationwide Insurance - Columbus, OH\nFebruary 2012 to February 2014\nResponsibilities:\n\u2022 Involved in extensive Data validation using SQL queries and back-end testing\n\u2022 Used Erwin, created Conceptual, Logical and Physical data models.\n\u2022 Involved in analysis of a variety of source system data, coordination with subject matter experts, development of standardized business names and definitions, construction of a non-relational data model using Erwin modeling tool, publishing of a data dictionary, review of the model and dictionary with subject matter experts and generation of data definition language.\n\u2022 Involved incomplete SDLC processes involving requirements management, workflow analysis, source data analysis, data mapping, metadata management, data quality, testing strategy and maintenance of the model.\n\u2022 Created ETL framework and provided strategy for data cleansing, data quality and data consolidation.\n\u2022 Worked extensively in Teradata data modeling for both logical and physical data models.\n\u2022 Developed ETL process using Pentaho PDI to extract the data from legacy System.\n\u2022 Designed various Informatica ETL load patterns which include SCD TypeI, TypeII, fullload, etc.\n\u2022 Worked in importing and cleansing of data from various sources like Teradata, Oracle, flat files, SQLServer with high volume data.\n\u2022 Performed data management projects and fulfilling ad-hoc requests according to user specifications by utilizing data management software programs and tools like Perl, Toad, MSAccess, Excel and SQL.\n\u2022 Worked on Naming standards for Table/Column/Index/Constraints names thru Erwin Macros and Master Abbreviations file.\n\u2022 Wrote SQL scripts to test the mappings and Developed Traceability Matrix of Business Requirements mapped to TestScripts to ensure any Change Control in requirements leads to test case update.\n\u2022 Experienced in testing Business Intelligence reports generated by various BI Tools like Tableau, Microstrategy and Business Objects.\n\u2022 Involved in extensive Data Validation with SQL queries.\n\u2022 Worked on Physical design for both SMP and MPPRDBMS, with understanding of RDMBS scaling features']","[u""Master's in computer science""]",[u'Sacred Heart University']
0,https://resumes.indeed.com/resume/4c979f0ea0fca3a0,"[u'Data Analyst\nJohn Hancock - Boston, MA\nJuly 2017 to December 2017\n\u2022 Created SQL scripts to be used as fraud detection metrics\n\u2022 Assisted in the creation of consumer behavior models through data\npreparation, basic supervised machine learning, and preliminary analysis\n\u2022 Responded promptly to daily ad hoc requests from a variety of sources\n\u2022 Built a satisfaction survey for internal use and analyzed responses', u'Data Analyst\nCERN - Geneva, CH\nJuly 2016 to December 2016\n\u2022 Completed two analysis projects related to the upgrade of the compact\nmuon solenoid detector on the large hadron collider\n\u2022 Presented findings at bi-weekly team meetings and departmental\nmeetings\n\u2022 Wrote full documentation for both projects in LaTeX\n\u2022 Took shifts in test beam facility to gather data for analysis', u'Lab Technician\nLiquiGlide - Cambridge, MA\nJuly 2015 to December 2015\n\u2022 Helped develop fuse mechanism for material degradation\n\u2022 Trained on interferometer, goniometer, rheometer, and tensiometer\n\u2022 Coded a samples and inventory database in Excel using VBA\n\u2022 Maintained lab cleanliness, enforced chemical hygiene, and took inventory of chemicals in the lab']",[u'Bachelor of Science in Physics and Mathematics in Probability and Statistics'],"[u'Northeastern University Boston, MA\nMay 2018']"
0,https://resumes.indeed.com/resume/f5c09b218e2f8024,"[u'Data Scientist\nAT&T - Dallas, TX\nJuly 2017 to Present\nDescription:\nAT&T Inc is engaged in provision of communications and digital entertainment services in the United States and the world. It provides fixed-line services, including voice, data, and television services to consumers and small businesses.\nResponsibilities:\n\u2022 Involved in defining the source to target data mappings, business rules, and data definitions.\n\u2022 Performing data profiling on various source systems that are required for transferring data to ECH using\n\u2022 Defining the list codes and code conversions between the source systems and the data mart using Reference Data Management (RDM).\n\u2022 Involved in data collection and induction to Teradata\n\u2022 Conducted data cleaning, data preparation, and outlier detection\n\u2022 Finding insights from millions of customer chat and calls records\n\u2022 Gathering requirements from business\n\u2022 Reviewing business requirements and analyzing data sources\n\u2022 Developed predictive models for sales and Finance teams using various ML and DL algorithms\n\u2022 Utilizing Informatica toolset (InformaticaData Explorer, and Informatica Data Quality) to analyze legacy data for Data Profiling.\n\u2022 Worked on DTS Packages, DTS Import/Export for transferring data between SQL Server 2000 to 2005\n\u2022 Involved in upgrading DTS packages to SSIS packages (ETL).\n\u2022 Involved in Training and Testing the ML Supervised and Unsupervised models\n\u2022 Researching on Deep Learning to implement NLP\n\u2022 Presented to the higher management the discovered trends and analysis, forecast data, recommendations, model results and risks identified\n\u2022 Performing an end to end InformaticaETL Testing for these custom tables by writing complex SQL Queries on the source database and comparing the results against the target database.\n\u2022 Using HP Quality Center v 11 for defect tracking of issues.\n\u2022 Involved in applying data mining techniques and optimization techniques in B2B and B2C industries and proficient in Machine Learning, Data/Text Mining, Statistical Analysis and Predictive Modeling.\n\u2022 Created and presented executive dashboards to show the patterns & trends in the data using Tableau Desktop\n\u2022 Developed NLP models for Topic extraction, Sentiment Analysis\n\u2022 Developed Executive Summary KPI, Key value programs, NPI dashboards in Tableau\n\u2022 Created customized Calculations, Conditions and Filters (Local, Global) for various analytical reports and dashboards\n\u2022 Was able to identify emerging issues using the models\n\u2022 Developing & evaluating Machine Learning models\n\u2022 Developed different visualizations using advanced features and deep analytics in Tableau\n\u2022 Used algorithms and programming to efficiently go through large datasets and apply treatments, filters, and conditions as needed\n\u2022 Developed Cross Tab, Chart, Funnel charts, Donut charts, Heat Maps, Tree Maps and Drill Through Reports, 100% stacked bar charts etc. in Tableau Desktop\n\u2022 Involved in publishing, scheduling and subscriptions with Tableau Server and creating and managing users, groups, sites in Tableau Server.\n\u2022 Involved in developing and testing the SQL Scripts for report development, Tableau reports, Dashboards and handled the performance issues effectively\n\u2022 Tested dashboards to ensure data was matching as per the business requirements and if there were any changes in underlying data\nEnvironment: Data Governance, SQL Server, ER Studio 9.7, Tableau 9.03, AWS, Teradata 15, ETL, MS Office Suite - Excel(Pivot, VLOOKUP), DB2, R, Python, Visio, HP ALM, Agile, Azure, Data Quality, Tableau and Reference Data Management.', u'Data Scientist\nHealthfirst, NY\nApril 2016 to June 2017\nDescription:\nHealth First Inc. is a not-for-profit integrated health system in Central Florida. It offers cancer, fitness, heart and vascular, maternity, neurosciences, orthopedics and sports medicine, open surgery, robotic surgery, urogynecology, vascular and vein, and weight loss services, as well as brevard and cosmetic dermatology services.\nResponsibilities:\n\u2022 A highly immersive DataScience program involving DataManipulation&Visualization, Web Scraping, MachineLearning, Python programming, SQL, GIT, Unix Commands, NoSQL, MongoDB, Hadoop.\n\u2022 Installed and used CaffeDeepLearningFramework\n\u2022 Worked on different data formats such as JSON, XML and performed machinelearningalgorithms in Python.\n\u2022 Participated in all phases of datamining; datacollection, datacleaning, developingmodels, validation, visualization and performed Gapanalysis.\n\u2022 Developing Voice Bot using AI (IVR ), improving the interaction between Human and the Virtual Assistant\n\u2022 Implemented Event Task for execute Application Automatically.\n\u2022 Involved in developing Patches & Updates Module.\n\u2022 Setup storage and dataanalysis tools in AmazonWebServices cloud computing infrastructure.\n\u2022 Used pandas, numpy, seaborn, scipy, matplotlib, scikit-learn, NLTK in Python for developing various machinelearningalgorithms.\n\u2022 Development and Deployment using Google Dialogflow Enterprise.\n\u2022 Worked as Data Architects and IT Architects to understand the movement of data and its storage and ERStudio9.7\n\u2022 Data visualizationusingElasticsearch , Kibana and Logstash in python.\n\u2022 Used Kibana an open source plugin for Elasticsearch in analytics and Data visualization.\n\u2022 DataManipulation and Aggregation from different source using Nexus, Toad, BusinessObjects, PowerBI and SmartView.\n\u2022 Implemented Agile Methodology for building an internal application.\n\u2022 Extracting the source data from Oracle tables, MS SQL Server, sequential files and excel sheets.\n\u2022 Migrating Informatica mappings from SQL Server to Netezza Foster culture of continuous engineering improvement through mentoring, feedback, and metrics\n\u2022 Broad knowledge of programming, and scripting (especially in R / Java / Python)\n\u2022 Developing and maintaining Data Dictionary to create metadata reports for technical and business purpose.\n\u2022 Predictive modeling using state-of-the-art methods\n\u2022 Developed MapReduce/Spark Python modules for machine learning & predictive analytics in Hadoop on AWS.\n\u2022 Parse and manipulate raw, complex data streams to prepare for loading into an analytical tool.\n\u2022 Build and maintain dashboard and reporting based on the statistical models to identify and track key metrics and risk indicators.\n\u2022 Proven experience building sustainable and trustful relationships with senior leaders\n\u2022 Focus on integration overlap and Informatica newer commitment to MDM with the acquisition of Identity Systems.\n\u2022 Data analysis using regressions, data cleaning, excel v-look up, histograms and TOAD client and data representation of the analysis and suggested solutions for investors\n\u2022 Rapid model creation in Python using pandas, numpy, sklearn, and plot.ly for data visualization. These models are then implemented in SAS where they are interfaced with MSSQL databases and scheduled to update on a timely basis.\n\u2022 Attained good knowledge in Hadoop Data Lake Implementation and HADOOP Architecture for client business data management.\n\u2022 Extracted data from HDFS and prepared data for exploratory analysis using datamunging\n\nEnvironment: ER Studio 9.7, Tableau 9.03, AWS, Teradata 15, MDM, GIT, Unix, Python 3.5.2, , MLLib, SAS, regression, logistic regression, Hadoop, NoSQL, Teradata, OLTP, random forest, OLAP, HDFS, ODS, NLTK, SVM, JSON, XML, MapReduce, Google Dialog Flow.', u'Data Analyst/Data Scientist\nPearson New Jersey, New Jersey\nDecember 2014 to March 2016\nDescription: Pearson is based on educational platform, which is one of the leading educational and professional publishers around the globe. The rapid growth in usage of Tablets, Laptops and smart phones which transforms education to everyone virtually anytime and anywhere, this project focuses on e-learning web portal and application development.\nResponsibilities:\n\u2022 Assisting business by being able to deliver a machine learning project from beginning to end, aggregating and exploring data, building and validating predictive models and deploying completed models to deliver business impacts to the organization\n\u2022 Created data modeling and data mapping document containing source, formulate transformational rules to populate target fields\n\u2022 Created impact & gap analysis documents specifying changes introduced as part of the program and lead the business process team\n\u2022 Work with big data consultants to analyze, extract, normalize and label relevant data using Statistical modeling techniques like Logistic regression, decision trees, Support vector machine, Random forest, Naive Bayes and neural networks\n\u2022 Developed ETLs for data sources used in production reporting for marketing and operations teams.\n\u2022 Write SQL queries to perform data analysis, data modeling and prepare data mapping documents to explain the transformation rules from source to target tables\n\u2022 Led the Change Management stream of an HR/Payroll project resulting from a $16.5 billion acquisition and the formation of UTAS, created Change Management Plan, and ensured team was on target to deliver both communication and training to HR, finance and Payroll staff.\n\u2022 Review business data for trends, patterns or casual analysis to assist in identifying model drift and retraining models\n\u2022 Created customized reports and processes in SAS and Tableau Desktop\n\u2022 Performed data analysis to create reporting requirements by specifying inclusion & exclusion criteria, conditions, business rules and data elements to be included into the report\n\u2022 Scheduled and facilitated requirements gathering with HR, Payroll, finance and accounting teams to implement ADP eTime and ADP Enterprise v5 and ADP General Ledger and drove requirements for data collection and data modeling with data engineers\n\u2022 Performed SQL query for data analysis and integration\n\u2022 Support PMO governance activities; defining and maintaining Project Management standards.\n\u2022 Responsible for generating ideas for product changes that improve key metrics\n\u2022 Provided data analytics of the web-portal to the team for feedback and improvement.\n\nEnvironment: Python, HTML5, CSS3, AJAX, Teradata, OLTP, random forest, OLAP, HDFS, ODS, JSON, jQuery, MySQL, NumPy, SQL Alchemy, Matplotlib, Hadoop, Pig Scripts.', u'Data Analyst/Data Modeler\nSUNTRUST BANK - Richmond, VA\nApril 2013 to November 2014\nDescription: SunTrust provides the financial services for consumers, small business and commercial banking, financial transaction processing, asset management and private equity Involved in the scope discussions with the Business Analysts and the Business users to identify the technical requirements\n\nResponsibilities:\n\u2022 Involved in defining the source to target data mappings, business rules, data definitions.\n\u2022 Involved in defining the business/transformation rules applied for sales and service data.\n\u2022 Worked with project team representatives to ensure that logical and physical ER/Studio data models were developed in line with corporate standards and guidelines.\n\u2022 Define the list codes and code conversions between the source systems and the data mart.\n\u2022 Coordinate with the business users in providing appropriate, effective and efficient way to design the new reporting needs based on the user with the existing functionality.\n\u2022 Worked with BTEQ to submit SQL statements, import and export data, and generate reports in Teradata.\n\u2022 Responsible for defining the key identifiers for each mapping/interface.\n\u2022 Responsible for defining the key identifiers for each mapping/interface.\n\u2022 Implementation of Metadata Repository, Maintaining Data Quality, Data Cleanup procedures, Transformations, Data Standards, Data Governance program, Scripts, Stored Procedures, triggers and execution of test plans\n\u2022 Performed data quality in TalendOpenStudio.\n\u2022 Coordinated meetings with vendors to define requirements and system interaction agreement documentation between client and vendor system.\n\u2022 Responsible for defining the functional requirement documents for each source to target interface.\n\u2022 Remain knowledgeable in all areas of business operations in order to identify systems needs and requirements.\n\u2022 Document the complete process flow to describe program development, logic, testing, and implementation, application integration, coding.\n\u2022 Enterprise Metadata Library with any changes or updates.\n\u2022 Generate weekly and monthly asset inventory reports.\nEnvironment: Erwin r7.0, SQL Server 2012/2008, Windows XP/NT/2000, Oracle 10g/9i, MS-DTS, UML, UAT, SQL Loader, OOD, OLTP, PL/SQL, MS Visio, Informatica.', u'Business Analyst /Data Analyst\nICICI Bank - Hyderabad, Telangana\nNovember 2011 to March 2013\nDescription: ICICI Bank, stands for Industrial Credit and Investment Corporation of India, is an Indian multinational banking and financial services company headquartered in Mumbai, Maharashtra, India, with its registered office in Vadodara. In 2017, it is the third largest bank in India in terms of assets and third in term of market capitalisation.\nResponsibilities:\n\u2022 Analyze business information requirements and model class diagrams and/or conceptual domain models.\n\u2022 Managed the project requirements, documents and use cases by IBM Rational RequisitePro.\n\u2022 Assisted in building an Integrated LogicalDataDesign, propose physical database design for building the data mart.\n\u2022 Gather & Review Customer Information Requirements for OLAP and building the data mart.\n\u2022 Responsible for defining the key identifiers for each mapping/interface\n\u2022 Responsible for defining the functional requirement documents for each source to target interface.\n\u2022 Coordinated meetings with vendors to define requirements and system interaction agreement documentation between client and vendor system.\n\u2022 Enterprise Metadata Library with any changes or updates.\n\u2022 Document data quality and traceability documents for each source interface.\n\u2022 Performed document analysis involving creation of Use Cases and Use Case narrations using Microsoft Visio, in order to present the efficiency of the gathered requirements.\n\u2022 Analyzed business process workflows and assisted in the development of ETL procedures for mapping data from source to target systems.\n\u2022 Worked with BTEQ to submit SQL statements, import and export data, and generate reports in Terra-data.\n\u2022 Calculated and analyzed claims data for provider incentive and supplemental benefit analysis using Microsoft Access and Oracle SQL.\n\u2022 Establish standards of procedures.\n\u2022 Generate weekly and monthly asset inventory reports.\n\u2022 Document all data mapping and transformation processes in the Functional Design documents based on the business requirements\n\nEnvironment: SQL Server 2008R2/2005 Enterprise, SSRS, SSIS, Crystal Reports, Windows Enterprise Server 2000, DTS, SQL Profiler, and Query Analyzer.', u'BI Developer/Data Analyst\nResilient - Hyderabad, Telangana\nApril 2009 to October 2011\nDescription: Tax and Banking and Full Level Tax and Banking (FLTII) are web-based applications, which hold the information and documents for processing payroll tax and banking details.\nResponsibilities:\n\u2022 Developed an Object modeling in UML for Conceptual Data Model using Enterprise Architect.\n\u2022 Developed logical and Physical data models using Erwin to design OLTP system for different applications.\n\u2022 Facilitated transition of logical data models into the physical database design and recommended technical approaches for good data management practices.\n\u2022 Worked with DBA group to create Best-Fit Physical Data Model with DDL from the Logical Data Model using Forward engineering.\n\u2022 Worked with the ETL team to document the transformation rules for data migration from OLTP to Warehouse environment for reporting purposes.\n\u2022 Developed Data Migration and Cleansing rules for the Integration Architecture (OLTP, ODS, DW).\n\u2022 Performed K-means clustering, Multivariate analysis, and Support Vector Machines in R.\n\u2022 Extensive system study, design, development and testing were carried out in the Oracle environment to meet the customer requirements.\n\u2022 Written complex Hive and SQL queries for data analysis to meet business requirements.\n\u2022 Written complex SQL queries for implementing business requirements\n\nEnvironment: DB2, Teradata, SQL-Server 2008, Enterprise Architect, Power Designer, MS SSAS, Crystal Reports, SSRS, ER Studio, Lotus Notes, Windows XP, MS Excel, word and Access.']","[u""Bachelor's""]",[u'']
0,https://resumes.indeed.com/resume/39c3a10333d12aac,"[u'Data Analyst/Logistics\nUnited States Navy\nAugust 2003 to Present\nProvided analytical data for 1,100 military and civilian personnel to include work hours and cost per maintenance order\n\u2666 As Leading Petty Officer for the Supply Department, in charge of the personal and professional growth of 5 junior personnel.\n\u2666 Leader of focus group to suggest improvements to command morale as well as maintenance efficiency']",[u'Bachelor of Arts in History'],"[u'The University of Tennessee Knoxville, TN\nMay 2016']"
0,https://resumes.indeed.com/resume/cd0bd3e381cfb47d,"[u'Data Analyst\nNatural Language Processing and Topic Modeling\nJanuary 2018 to March 2018\n\u2022 Designed and developed a content-based movie recommendation system using Python\n\u2022 Clustered movies into groups and discovered latent semantic structures based on movie introductions\n\u2022 Preprocessed data utilizing tokenizing, stemming and stop-words removal\n\u2022 Implemented Term Frequency-Inverse Document Frequency (TF-IDF) approach to extract features\n\u2022 Trained the data with unsupervised learning models including K-Means Clustering and Latent Dirichlet Allocation\n(LDA)\n\u2022 Identified latent topics and keywords of each movie to achieve clustering and calculate movie similarity', u'Data Analyst\nNatural Language Processing and Topic Modeling\nJanuary 2018 to March 2018\n\u2022 Provided statistical advice to the client regarding a study of oil effects on French fries\n\u2022 Helped with experimental design regarding data collection\n\u2022 Applied exploratory data analysis (EDA) on the original data\n\u2022 Presented to the client a tailored statistical plan based on the results of EDA']","[u'Master of Applied Statistics in Computational Science', u'Bachelor of Arts in English Language and Literature']","[u'The Pennsylvania State University University Park, PA\nJanuary 2017 to August 2018', u'Shandong University of Science and Technology\nAugust 2009 to June 2013']"
0,https://resumes.indeed.com/resume/0c875e11ebd2f868,"[u'Data Analyst\nTata Consultancy Services - Mum\nFebruary 2014 to June 2017\nPredictive Analytics and Visualization| R, Tableau, Oracle Business Intelligence\n\u2022 Developed Classification models using R that predicted policies that are likely to cancel before end of their term for a US-Based Insurance client which helped company to increase their customer retention by 2.5%\n\u2022 Crafted insightful dashboards and visualizations to track KPI\u2019s and risk used by Director+ level for a UK Based Financial firm\n\u2022 Text mined reviews of former and current employees to identify key drivers for employee attrition from company, as part of talent retention project for the HR department\nBigdata Analytics | Hadoop\n\u2022 Extracted, cleansed and analyzed complex data from multiple sources to find useful insights via Bigdata analytics with Hadoop, Hive, SQL\n\u2022 Deployed automated Sqoop jobs to import and export data from and to Relational DBMS and HDFS environment\n\u2022 Automated reporting framework for large public-sector company by customizing and scheduling HiveQL and SparkSQL scripts which reduced 2 FTE hours per day of effort\n\u2022 Led a team to install, configure and migrate existing DWH to 10-node Hadoop Hortonworks cluster, saved $5 million annually for a UK Based Financial firm\nData Warehousing and Analysis | Oracle SQL, PowerCenter Informatica\n\u2022 Analysis, design, development, testing and implementation of Business Intelligence solutions utilizing data warehouse/data mart design concepts, ETL, OLAP, OLTP and change data capture\n\u2022 Worked on dimensional modelling and developed different schemas like Star and Snowflake, Identifying Facts and Dimensions tables\n\u2022 Integrated various data sources with multiple relational databases like Oracle11g /Oracle10g, flat files into staging area, ODS, data warehouse and data mart\n\u2022 Created and executed SQL scripts for data extraction, analysis, and validation. Tuned and optimized performance of SQL Queries']","[u""Master's in Business Analytics and Project Management"", u'Bachelor of Engineering in Electronics and Communication']","[u'University of Connecticut School of Business Hartford, CT\nAugust 2017 to December 2018', u'Rajiv Gandhi Technical University Bhopal, Madhya Pradesh\nMay 2013']"
0,https://resumes.indeed.com/resume/03af50d1d515d5c8,"[u""Data Analyst Intern\nUNIBEES, Inc\nAugust 2017 to December 2017\nAnalyzed huge volumes of data information from various sources and segregated it onto a common platform,\nsimultaneously boosting ease of access by nearly 60%\n\u2022 Built a front-end web page to help collect data making it easier to save details to a database 2x times faster\nthan earlier method in implementation\n\u2022 Worked on handling APIs to connect application's database with user traffic statistics"", u""Data Analyst\nCognizant Technology Solutions\nDecember 2014 to May 2016\n\u2022 Designed and developed a mobile application code for a leading American insurance company with AngularJS and LeafletJS to improve navigation speed by 50%\n\u2022 Developed a MEAN stack application utilizing AngularJS, HTML, jQuery and CSS to retrieve search results faster by 1.2x times\n\u2022 Generated web pages for display of medicinal products of a leading pharmaceutical company more dynamically\nhelping increase site's traffic by 25%\n\u2022 Consolidated production support feedback with a performance tracker, reducing production issues and support\ncosts by 38%\n\u2022 Coordinated with testing teams to expand testing phase timing, reducing post live tickets\n\u2022 Examined and executed test cases for various phases of testing - integration, regression and user""]","[u'M.S. in Business Analytics', u'B.E. in Electronics and Communication Engineering']","[u'The University of Texas at Dallas Dallas, TX\nMay 2018', u'Acharya Nagarjuna University\nApril 2014']"
0,https://resumes.indeed.com/resume/22e9a562e9eef52e,"[u""Business Analyst/Data Analyst\nZions Bank - Salt Lake City, UT\nFebruary 2017 to Present\nResponsibilities:\n\u2022 Responsible for designing, building, and supporting the components of data warehouse, such as ETL processes, databases, reports, and reporting environments\n\u2022 Designs dimensional models with conformed dimensions, following the business' processes\n\u2022 Develops and automates ETL processes that involves error and reconciliation handling, utilizing Microsoft SQL Server Integration Services (SSIS)\n\u2022 Map data between source systems, data warehouses, and data marts.\n\u2022 Utilizes SAP Business Objects in designing and building the reporting environment\n\u2022 Writes reports, adds new fields, creates Business Objects filters, measures and objects; and creates dashboards to improve current data warehouse\n\u2022 Creates processes for maintaining or/and capturing metadata\n\u2022 Studies and understands the company's business processes and applications, including their effect on data and reporting, and applies the knowledge gained in designing data warehouses and reports\n\u2022 Expercing working with enterprise systems.\n\u2022 Extensive experience with data mining and data profiling.\n\u2022 Comparing data in Microsoft excel with vlookup, creating formulas in excel for data analytical.\n\u2022 Extensive experience in data mapping from source system to enterprise data warehouse and data marts.\n\u2022 Extensive working experience customer data information CDI hub.\n\u2022 Extensive experience with data profiling and solving complex problems\n\u2022 Identifies and documents all requirements for both fresh and current data warehouse components and reports by working with end users\n\u2022 Creates Business Objects reports and assists clients with data warehouse\n\u2022 Performs capacity planning, supporting, troubleshooting, and ETL performance modification\n\u2022 Assembles performance statistics, analyzes them, and makes recommendations for improvements\n\u2022 Establishes system documentation and ensures it is continually sustained.\n\u2022 Participated in developing common Appium framework for both Android and IOS\n\u2022 Work on data preparation for different test scenarios including negative testing to provide accurate delivery of overall functionality of the product.\n\u2022 Analyzed Business, Functional Requirement and Design Review Documents to Develop Test Plan for projects involved and wrote Test Scripts for positive, negative, edge cases.\n\u2022 Extensively worked with HP LoadRunner, to generate multiple requests and executed them to identify the server responses under load based on various real-time scenarios\n\u2022 Ability to collaborate with testers, developers, project managers and other team members in testing complex projects.\n\u2022 Responsible in providing regular test status reports to the QA manager.\n\u2022 Participated in automation development review meetings.\nEnvironment: UNIX, QC 9.0, JIRA, SQL Server, HTML, XML, HP ALM, Workfont, Microsoft office"", u'Data Analyst\nAmerican Eagle - San Francisco, CA\nJanuary 2015 to February 2017\nResponsibilities:\n\u2022 Designing Test Cases, executing Test Cases and Defect Logging.\n\u2022 Parameterized the scripts to avoid code redundancy and avoid hard coding of frequently changing values using QTP.\n\u2022 Performed Database testing by writing and running SQL queries.\n\u2022 Involved in extensive DATA VALIDATION using SQL Queries and back end testing.\n\u2022 Extraction of test data from tables and loading of data into SQL tables\n\u2022 Used SQL for data integrity testing, captured the SQL statements from the application execution and manually checked the results.\n\u2022 Experience with enterprise system and data mining for analytical and problem solving.\n\u2022 Preparation of various test documents for ETL process in Quality Center.\n\u2022 Performed the Back-end Integration Testing to ensure data consistency on front-end by writing and executing SQL Statements.\n\u2022 Proficient with testing REST APIs, Web & Database testing.\n\u2022 Performed Data-Driven Tests by passing different sets of data and checked weather the information entered is updated into the database.\n\u2022 Reported the defects in the Quality Center automated tool and coordinated with the QA Lead and developers.\n\u2022 Established and documented ETL QA standards, procedures and QA methodologies.\n\u2022 Extensively worked with HP LoadRunner, to generate multiple requests and executed them to identify the server responses under load based on various real-time scenarios.\n\u2022 Communicated bugs to developers for Bug resolution.\n\u2022 Comply change management requirements and manage the change request/modification request in Test Directory repository\nEnvironment: Windows, Excel, Access, HP Load Runner, Selenium, Web Driver, SOAP UI, Quality Center, Agile, SQL, HTML. XML.', u'Business Analyst/QA Analyst\nUnited Health Care - San Diego, CA\nSeptember 2012 to December 2014\nResponsibilities:\n\u2022 Analyze business processes according to specifications and work flows and identify key areas concerning application functionality and behavior.\n\u2022 Developed test plans, test cases for both functional and regression testing in multiple project.\n\u2022 Executed test cases (Smoke, Functional, GUI, regression and database) and submitted test matrix.\n\u2022 Experience in API level testing for web apps based on REST/SOA using SOAP-UI, JMeter\n\u2022 Performed back end testing for data integrity, by writing SQL queries.\n\u2022 Wrote test cases, executed them and wrote automation scripts using selenium.\n\u2022 Performed manual and selenium testing of a web based application.\n\u2022 Wrote SQL scripts for test data creation and verification.\n\u2022 Intensively used ALM/QC/Test Director for developing test plans, test cases, to execute test cases, for defects tracking, defects reporting and to analyze the test results and defects.\n\u2022 Validated back end data by interacting with databases using SQL Queries.\n\u2022 Conducted Regression Testing, Integration Testing, and End-to-End testing, Functional Testing, Black Box Testing, Cross Browser and User Acceptance Testing. Used ALM/ Quality Center for developing Test Scripts, Executing the Test Scripts, Defect Tracking and Management.\n\nEnvironment: Eclipse, Selenium IDE/RC, quality Center, Load Runner, MySQL, MS Excel, Clear quest, HIPPA 5010, EDI, HTML, XML, SOAP UI, Clear Quest.']","[u""Bachelor's""]",[u'']
0,https://resumes.indeed.com/resume/437a52af785247e2,"[u""Reporting Data Analyst\nUnited Healthcare - Fort Worth, TX\nJanuary 2017 to January 2018\nContract)\n\u25cf Supported the company's healthcare software functions, implementation and training.\n\u25cf Make business recommendations based on data collected to improve business efficiency.\n\u25cf Collected data from all clients and made recommendations for improvement.\n\u25cf Ad-hoc data extraction through queries in SQL Server 2008, Excel or PDF.\n\u25cf Supported business lead for special assignment and to ensure production efficiency."", u""Data Analyst\nAids Arms, Inc - Dallas, TX\nJune 2015 to November 2016\nContract)\n\u25cf Supported, manage, and maintain organizations clinical data in databases in SQL Server.\n\u25cf Acted as an in-house Business Analyst to facilitate teams of Data and Business Analysts.\n\u25cf Initiated meeting as a lead to understand / gather requirements and document processes.\n\u25cf Maintained patients' information within in-house database systems (Clear Health & Aries).\n\u25cf Reported on periodical basis for federal funding and grants.\n\u25cf Ad-hoc reporting and querying for the staff data requests though SQL Server 2005, 2008 and 2012.\n\u25cf Prepared user guides and training manuals for the new users and clients\n\u25cf Weekly analysis of the new data entered in database."", u'Inventory Analyst\nElwood Staffing - Arlington, TX\nNovember 2013 to March 2015\nContract)\n\u25cf Identified, collected, and analyzed inventory data to determine correct inventory composition, level and location.\n\u25cf Audited materials management processes to ensure compliance with inventory policies and customer requirements.\n\u25cf Presented daily analysis of performance measurements to senior and executive managers, processed purchase orders, received product and maintained a daily balance with accuracy in our physical and virtual inventory levels.\n\u25cf Used SQL and Excel to pinpoint data discrepancies, investigate fraudulent orders, and solve crucial problems with orders.']","[u'Bachelors of science in information Technology', u'associates of Arts']","[u'Stratford University, , Stratford University\nJanuary 2015', u""Prince George's community college""]"
0,https://resumes.indeed.com/resume/a8639b9c5d78b3c4,"[u'Senior Data Analyst\nKaiser Permanente - Oakland, CA\nNovember 2006 to July 2013\nDeveloped software and databases and automated reports regarding patient care and a patient medical records. Developed Department websites. Designed and put forms that that provided more intuitive data entry formats.', u""Data Analyst\nDisneyland - Anaheim, CA\nOctober 2001 to November 2006\nDevelop software surveys for handhelds to be used in the park by staff\n\nDeveloped data reporting\n\nProgrammed the handheld's and train the staff and usage\n\nCreated the data reports\n\nEstablished quality control""]",[u'Bachelor of Arts in English literature'],[u'University of Southern Maine']
0,https://resumes.indeed.com/resume/f7f07991e97eef45,"[u""Data Scientist/ Machine Learning\nCardinal Health - Dublin, OH\nAugust 2017 to Present\nDescription:\nCardinal Health, a drug wholesaler that also makes gloves and surgical apparel, has been on a buying spree lately in a bid to shore up future earnings. It's also further expanding into services and support for customers that are moving from the traditional hospital model to larger integrated systems across various sites of care.\n\nResponsibilities:\n\u2022 Performed Data Profiling to learn about behavior with various features such as traffic pattern, location, Date and Time etc.\n\u2022 Application of various machine learning algorithms and statistical modeling like decision trees, regression models, neural networks, SVM, clustering to identify Volume using scikit-learn package in Python, Matlab.\n\u2022 Used clustering technique K-Means to identify outliers and to classify unlabeled data.\n\u2022 Evaluated models using Cross Validation, Log loss function, ROC curves and used AUC for feature selection.\n\u2022 Analyze traffic patterns by calculating autocorrelation with different time lags.\n\u2022 Ensured that the model has low False Positive Rate.\n\u2022 Addressed overfitting by implementing the algorithm regularization methods like L2 and L1.\n\u2022 Used Principal Component Analysis in feature engineering to analyze high dimensional data.\n\u2022 Created and designed reports that will use gathered metrics to infer and draw logical conclusions of past and future behavior.\n\u2022 Performed Multinomial Logistic Regression, Random forest, Decision Tree, SVM to classify package is going to deliver on time for the new route.\n\u2022 Performed data analysis by using Hive to retrieve the data from Hadoop cluster, SQL to retrieve data from Oracle database.\n\u2022 Used MLlib, Spark's Machine learning library to build and evaluate different models.\n\u2022 Implemented rule-based expert system from the results of exploratory analysis and information gathered from the people from different departments.\n\u2022 Performed Data Cleaning, features scaling, features engineering using pandas and numpy packages in python.\n\u2022 Developed MapReduce pipeline for feature extraction using Hive.\n\u2022 Created Data Quality Scripts using SQL and Hive to validate successful data load and quality of the data. Created various types of data visualizations using Python and Tableau.\n\u2022 Communicated the results with operations team for taking best decisions.\n\u2022 Collected data needs and requirements by Interacting with the other departments.\n\nEnvironment: Python 2.x, CDH5, HDFS, Hadoop 2.3, Hive, Impala, Linux, Spark, Tableau Desktop, SQL Server 2012, Microsoft Excel, Matlab, Spark SQL, Pyspark."", u'Data Scientist\nPinnacle Financial Partners - Nashville, TN\nMay 2016 to July 2017\nDescription:\nPinnacle Financial Partners, Inc. operates as a bank holding company for Pinnacle Bank that provides various banking products and services in the United States. The company accepts various deposits, including savings, checking, interest-bearing checking, money market, and certificate of deposit accounts. Its loan products include commercial loans, such as equipment and working capital loans; commercial real estate loans comprising investment properties and business loans secured by real estate; and loans to individuals consisting of secured and unsecured installment and term loans, lines of credit, residential first mortgage loans, and home equity loans and lines of credit.\n\nResponsibilities:\n\u2022 Responsible for analyzing large data sets to develop multiple custom models and algorithms to drive innovative business solutions.\n\u2022 Perform preliminary data analysis and handle anomalies such as missing, duplicates, outliers, and imputed irrelevant data.\n\u2022 Remove outliers using Proximity Distance and Density-based techniques.\n\u2022 Involved in Analysis, Design and Implementation/translation of Business User requirements.\n\u2022 Experienced in using supervised, unsupervised and regression techniques in building models.\n\u2022 Performed Market Basket Analysis to identify the groups of assets moving together and recommended the client their risks\n\u2022 Experience in determine trends and significant data relationships using advanced Statistical Methods.\n\u2022 Implemented techniques like forwarding selection, backward elimination and stepwise approach for selection of most significant independent variables.\n\u2022 Performed Feature selection and Feature extraction dimensionality reduction methods to figure out significant variables.\n\u2022 Used RMSE score, Confusion matrix, ROC, Cross-validation and A/B testing to evaluate model performance in both simulated environment and the real world.\n\u2022 Performed Exploratory Data Analysis using R. Also involved in generating various graphs and charts for analyzing the data using Python Libraries.\n\u2022 Involved in the execution of multiple business plans and projects Ensures business needs are being met Interpret data to identify trends to go across future data sets.\n\u2022 Developed interactive dashboards, Created various AdHoc reports for users in Tableau by connecting various data sources.\n\nEnvironment: Python, SQL server, Hadoop, HDFS, HBase, MapReduce, Hive, Impala, Pig, Sqoop, Mahout, Spark MLLib, MongoDB, Tableau, ETL, Unix/Linux.', u'Data Analyst\nFleetCor Technologies Inc - Norcross, GA\nJanuary 2015 to April 2016\nDescription: FleetCor Technologies, Inc. provides specialized payment products and services to Dataes, commercial fleets, oil companies, petroleum marketers, and government entities in North America, Europe, South Africa, and Asia.\nResponsibilities:\n\n\u2022 Involved in Analysis, Design and Implementation/translation of Business User requirements.\n\u2022 Worked on large sets of Structured and Unstructured data.\n\u2022 Actively involved in designing and developing data ingestion, aggregation, and integration in Hadoop environment.\n\u2022 Developed Sqoop scripts to import-export data from relational sources and handled incremental loading on the customer, transaction data by date.\n\u2022 Experience in creating Hive Tables, Partitioning and Bucketing.\n\u2022 Performed data analysis and data profiling using complex SQL queries on various sources systems including Oracle 10g/11g and SQL Server 2012.\n\u2022 Identified inconsistencies in data collected from different source.\n\u2022 Worked with business owners/stakeholders to assess Risk impact, provided a solution to business owners.\n\u2022 Experienced in determine trends and significant data relationships Analyzing using advanced Statistical Methods.\n\u2022 Carrying out specified data processing and statistical techniques such as sampling techniques, estimation, hypothesis testing, time series, correlation and regression analysis Using R.\n\u2022 Applied various data mining techniques: Linear Regression & Logistic Regression, classification, clustering.\n\u2022 Took personal responsibility for meeting deadlines and delivering high-quality work.\n\u2022 Strived to continually improve existing methodologies, processes, and deliverable templates.\n\nEnvironment: R, SQL server, Oracle, HDFS, HBase, MapReduce, Hive, Impala, Pig, Sqoop, NoSQL, Tableau, Unix/Linux, Core Java, Log 4j.', u'System Analyst\nAccenture - Bengaluru, Karnataka\nMay 2009 to November 2011\nDescription: Accenture PLC is a global management consulting and professional services company that provides strategy, consulting, digital, technology and operations.\n\nResponsibilities:\n\u2022 OABS Analytics team is mainly into reporting and creation of dashboards and Digitization of reports for all the Practices.\n\u2022 Design and develop the Headcount reports for worldwide regions and for all the practices.\n\u2022 Validation of data to check the accuracy of it.\n\u2022 Analytics team is mainly into reporting and creation of dashboards and Digitization of reports.\n\u2022 Design and develop the sales standard reports for APJ regions and area across the client locations.\n\u2022 Prepare the Weekly monthly and fortnight sales report.\n\u2022 Involved in automation of reports, was a go-to person for all automation on all MS office tools.\n\u2022 Worked on VBA projects as the integration of Excel-PowerPoint and Excel-Access etc.\n\u2022 Used charts, Pivots, and other complex excel function to automate and improve the productivity of the team.\n\u2022 Frequent Ad-hoc request to support key business requirement.\n\u2022 Providing Content and Data Management services to Ford of Europe being in Marketing Sales & Service team.\n\u2022 Validated data to ensure the quality, validity, and accuracy of content.\n\u2022 Claim processing for both Indian and European market (Retail Claims, Fleet Claims, and Warranty Claims etc.).\n\nEnvironment: Ad hoc, VBA, SQL/Server, Oracle 9i, MS-Office, Teradata.']",[u'Bachelor of Computer Science in TOOLS AND SKILLS'],[u'Informatica Power Centre']
0,https://resumes.indeed.com/resume/32a8e023107d7ad6,"[u'Sales Data Analyst\nPerformance Food Service - Livermore, CA\nJanuary 2013 to Present\n\u2022 Create and manage dashboards in Tableau and Excel to track sales and inventory KPIs\n\u2022 Exports data from Salesforce to create dashboards in Tableau.\n\u2022 Heavily utilizes Excel and Tableau to find key data insights and presents to VP of Sales for decision making.\n\u2022 Wrote SQL expressions on platforms such as, MySQL, PostgreSQL, IBM Cognos, Orcale SQL (PeopleSoft) & Microsoft SQL\n\u2022 Reported to Sales VP, delivering timely, insightful and actionable analyses, as well as performing custom\nanalysis and ad hoc queries as needed.\n\u2022 Ran IBM Cognos using Report Studio and Advanced Workspace to analyze and run reports.\n\u2022 Leveraged Tableau and pivot tables to create KPI dashboards that drove executive decisions.\n\u2022 Worked with cross-functional teams to develop monthly performance dashboards and quarterly reports.\n\u2022 Effectively managed multiple priorities. Performed under pressure in a fast-paced, fluid environment.', u'Data Analyst\nKNBR - San Francisco, CA\nNovember 2010 to December 2013\n\u2022 Used Tableau to clean data and create KPI dashboards, enabling managers to optimize channels and revenue.\n\u2022 Identified and reconciled data anomalies / discrepancies; resolved outliers and backtests of analytical models.\n\u2022 Used problem-solving and critical thinking skills to understand user issues and translate them into solutions.\n\u2022 Integrated data to generate sales and trading reports, helping managers make strategic business decisions.', u'Assistant Manager\nAT&T - Corte Madera, CA\nJune 2007 to July 2010\n\u2022 Tracked goals and KPIs of sales reps, stores, and region.\n\u2022 Created dashboard to hold staff accountable and enable decisions about performance and opportunities.\n\u2022 Assisted in metrics planning and forecasting.\n\u2022 Created strategic segment plans, utilization reports, and other documents informing on critical strategic issues.']","[u'', u'Bachelors in Computer Science']","[u'Contra Costa College\nJanuary 2012', u'Southern New Hampshire University']"
0,https://resumes.indeed.com/resume/dea327f249210bd7,[],[],[]
0,https://resumes.indeed.com/resume/23d70c95aa69e4e7,"[u'Data Analyst\nCognizant Technology Solutions - Bangalore\nFebruary 2014 to July 2017\nBuilding and maintaining scripts for ETL, data warehousing and data modeling. \u2022 Providing quality control on a variety of large datasets and troubleshooting on automation jobs. \u2022 Manipulating data and generating production reports on weekly, monthly and quarterly basis. \u2022 Communicating analytics results and supporting data to relevant management. \u2022 Analyzing the source data coming from different sources and working with business users and developers to develop the Model. \u2022 Extracted, Transformed and Loaded OLTP data into the Staging area and Data Warehouse using Informatic mappings and complex transformations. \u2022 Performed data preparation, statistical analysis and visualized the findings using ggplot in R \u2022 Planned, coordinated and tracked project schedule and release (Agile methodology) \u2022 Well acquainted with SOA Architecture and possess knowledge in web technologies as Web Services, XML, HTML, SOAP/HTTP protocols and REST calls (Json).']","[u""Master's in Information Systems in Information Systems""]","[u'University of Maryland Baltimore County Baltimore, MD\nAugust 2017 to May 2019']"
0,https://resumes.indeed.com/resume/ed72ffa5e683cb83,"[u""QA Analyst\nApple - Bengaluru, Karnataka\nJanuary 2014 to December 2015\nEnvironment: Oracle, Teradata, Vertica, UNIX, windows7\nProject: IGC (ITunes Gift Cards)\n\niTunes Store is Sales and Distribution Channel for various media content like Music,\nVideos, Movies, Applications which can be downloaded on Apple's mobile devices like iMac, iPod, iPhone and on MAC, PC desktops as well.\n\niTunes Gift Card, also known as XCard internally at APPLE, is a pre-paid gift card of fixed (5$,\n10$, 20$ etc.) or variable denominations sold through iTunes Stores, Apple Online Stores,\nApple Retails Stores and Third-Party Retailer Stores. It can only be redeemed at iTunes Store\nfor purchasing contents available for download. Apple Sells iTunes Gift Card globally in selected countries.\n\niTunes Gift Cards are the perfect way to give the gift of entertainment. You can buy them in different styles and denominations from Apple and from thousands of other retailers. Gift\nCards come in convenient 50-packs of $10, $15, $25, or $50 denominations. With orders of\n$500 or more, you can choose iTunes gift codes in any denomination over $10.These codes\nare digital versions of gift cards that can be personalized and delivered by email.\n\nRecipients can redeem and use their iTunes gifts on the iTunes Store, App Store, iBook\nstore, and Mac App Store. There's no credit card required. All they need is a free iTunes\naccount. And they can immediately use their gift credit for whatever they want - songs,\nmovies, TV shows, apps, games, books, and more.\nApple distributes iTunes Gift card through Integrators that in turn distributes the cards to various Merchants.\nVendors are actually kind of card type designed for specific merchants. So, a Merchant can\nsell different vendor type cards.\n\nOnce a card is activated, it would be redeemed into iTunes account through redemption\ntransaction. Card can be deactivated (restocked) or cancelled. Deactivated card will be\nactivated and restocked.\n\nResponsibilities:\n\u27a2 Understanding functional documents provided by business.\n\u27a2 Co-coordinating between onsite and offshore team.\n\u27a2 Mentoring the team members and providing feedback\n\u27a2 Project Status tracking.\n\u27a2 Involved in requirement understanding, test planning, coordinating with development team, allocation of tasks at offshore and risk assessment.\n\u27a2 Preparation of Test Plan, Requirement traceability matrix, test scenarios & test cases and test scripts to validate business transformations.\n\u27a2 Communicating and coordinating on daily progress and issues with different teams.\n\u27a2 Involved in UAT, Regression testing and Production phases of project.\n\u27a2 Performed Database Testing, GUI Testing and Browser Compatibility Testing.\n\u27a2 Responsible for tracking defects in Radar and perform retest.\n\u27a2 Understanding the scope for automation in our project and developing automations scripts.\n\u27a2 Handling performance testing single-handedly for Liger platform application."", u'Data Analyst\nCOX communications - Bengaluru, Karnataka\nJune 2013 to December 2013\nEnvironment: Oracle, UNIX, windows7\n\nCox Communications is a privately-owned subsidiary of Cox Enterprises providing digital\ncable television, telecommunications and Home Automation services in the United States.\nThe project includes many small projects which will generate reports with the correct Data so that will be used to gain insight into the inaccuracies.\n\nResponsibilities:\n\u27a2 Design the complex functional, performance and sanity test cases.\n\u27a2 Uploading test cases, logging defects in QC.\n\u27a2 Execute the Test Cases using SQL developer and perform the Back-end Testing to ensure data consistency by writing and executing SQL statements.\n\u27a2 Performed SQL validation to verify the data extracts and record counts in the database tables by writing complex SQL minus queries.\n\u27a2 Participated in regular project status meetings and QA status meetings.\n\u27a2 Automation of test cases through Platinum (Cognizant Tool)\n\u27a2 Validated reports based on functional requirements.\n\u27a2 Analyzed various reports generated by OIBEE.\n\u27a2 Checked the reports for any naming inconsistencies and to improve user readability.\n\u27a2 Involved in End-to-End phase of project.', u'Data Analyst\nSainsbury - Bengaluru, Karnataka\nApril 2012 to May 2013\nEnvironment: Oracle, UNIX, windows7\nProject: Proteus Report\nIt was a project which involved in upgrading the platform of the website.\nThe website was previously developed on Blue martini platform which was upgraded to WCS platform.\n\nResponsibilities:\n\u27a2 Majorly involved in high-level document preparation and query formulation and execution.\n\u27a2 Interaction with Onsite for requirement gathering / clarification and Deliverables.\n\u27a2 Production support.\n\u27a2 Understanding the universe for BO reports and report formatting.\n\u27a2 Taken the responsibility of understanding the excel workbook integration from onsite team and giving KT to the onsite team.\n\u27a2 Involved Excel workbook Integration and execution. Defect raising as well as fixing.\n\u27a2 Unit testing of excel and BO reports.\n\u27a2 Prepare and document technical requirements for reports and dashboards.']",[u''],[u'HP Quality Centre']
0,https://resumes.indeed.com/resume/4b829133510d95d9,"[u'Data Scientist/Analyst\nCoretech, Greater New York Area\nFebruary 2017 to December 2017\n\u2022 Provided statistical solutions for analysing operational challenges in software development for CoreTech.\n\u2022 Conducted exploratory analysis, hypothesis testing and linear modelling with R.\n\u2022 Built a K-means clustering model to classify the developers with identified features.\n\u2022 Performed unsupervised classification using k-means clustering in R to to classify the developers with identified features.\n\u2022 Performed ETL operations to extract data from master data, transform in to business defined readable data, and load it to reporting data systems\n\u2022 Created data models and generated DDL scripts with the help of Oracle SQL Developer\n\u2022 Implemented automated data extraction and cleaning scripts with UNIX.\n\u2022 Represented insights to clients through visualizations built with Tableau & R.\n\u2022 Performed Hypothesis tests, Chi-squared, Student t-test, ANOVA & Regression.', u""Data Analyst (Retail)\nEste\xe9 Lauder, Greater New York Area\nMay 2016 to December 2016\n\u2022 Created & updated Business requirements, converting them into reports, visualizations and design documents.\n\u2022 Performed data analysis and built predictive models (MySQL, Excel, R, ANOVA, Regression)\n\u2022 Wrote SQL queries to pull the North Americas sales data and visualized the sales statistics.\n\u2022 Time Series Forecasting to predict product demand using R Programming and integrating with Tableau using the 'RServe' in R package to deliver predicted results in dynamic visualizations.\n\n\u2022 Utilized 'forecast' package in R to run ARIMA, and Exponential Smoothing models\n\n\u2022 Developed Visualizations using Cross tables, Bar Charts, Pie Charts, Donuts Charts, Maps, Line Charts, Scatter Plots and Filters, Marking, Drill Down, Hierarchies and Colouring. (Tableau, R - ggplot)\n\u2022 Joined tables, created charts and performed advanced calculations using Excel VLOOKUP and pivot table.\n\u2022 Used ETL process and developed OLAP cube to analyse the region wise product sales.\n\u2022 Created test plans, test cases, test scripts for User Acceptance Testing, Regression testing & Integration testing.\n\u2022 Delivered UAT documentation to recommended changes to applications. (MS Visio, PowerPoint)\n\u2022 Verified that Development & Testing teams adhere to Functional Requirements and performed root cause analysis on delays in fulfilment of deliverables.\n\u2022 Addressed ah-hoc requests from business stakeholders that involved tasks ranging from data analysis and report generation to backend testing of data to adhere to the business standards\n\u2022 Assisted product managers, fellow data analysts and other business teams in completing business deliverables."", u'Data Analyst/Tableau Developer\nNew York University, Greater New York Area\nSeptember 2015 to May 2016\n\u2022 Applied sentiment analysis to analyse the feedback attributes of 4000 students to improve the student engagement and facilities.\n\u2022 Developed a suite of 50+ annual institutional reports about financial aid & future projects on Tableau and MySQL to build a more efficient data handling system for 10 diverse departments.\n\u2022 Developed Tableau visualizations and dashboards to visualize the University enrolment trends.\n\u2022 Performed Predictive Analysis & Use Case Analysis on enrolment and hiring data to increase federal funding.', u'Business Data Analyst (Financial Services)\nWipro Technologies\nDecember 2012 to July 2015\n\u2022 Created predictive models for the stock prices using R and integrated it with Tableau.\n\n\u2022 Created dashboards to present summary statistics of the claims and plotted their geographic locations.\n\n\u2022 Performed Statistical Analysis and presented the region-wise trends of the claims.\n\n\u2022 Connected Tableau server to publish dashboard to a central location for portal integration.\n\n\u2022 Implemented software development process based on Agile methodology.\n\n\u2022 Designed/developed tables, utilized SQL queries to update new customers in the client database, create script, gather and manipulate information from Oracle.\n\n\u2022 Worked on manual testing of data for ETL process to verify the validity of the data from the external source system and the internal database systems\n\n\u2022 Exposure to data warehousing concepts like ETL, aggregation and staging of data, dimensional reporting\n\n\u2022 Performed discussions with data modeler to update the data model for new requirements\n\u2022 Moderated our team as a Scrum Master and effectively communicated the project information.\n\u2022 Elicited requirements from stakeholders analysed and transformed them into BRD and SRS documents for several database related change requests']","[u'Master of Science', u'Bachelor of Engineering in Computer Science & Engineering']","[u'New York University\nMay 2017', u'Anna University\nMay 2012']"
0,https://resumes.indeed.com/resume/3c0fe082bc19ebeb,"[u'Senior Data Analyst\nThe NPD Group / Ipsos Interactive Services - Westbury, NY\nJanuary 1999 to January 2016\nFulfilled a critical role at this leading provider of global market and opinion research, leveraging expertise in the Quantum data processing platform to build cross-tabulations and statistical analysis for end-user presentations. Facilitated the flow of key information and communications between local client service staff members and offshore vendors engaged in the production of tabulation outputs.\n* Coordinated and led training of external vendors to ensure compliance with all corporate standards and expectations of quality; additionally, oversaw the training of junior data analysts operating out of multiple international offices.\n* Directed projects with total annual budgets ranging from $300K to $500K; achieved all project milestones and deliverables on-time and within budget.', u'Data Processor\nGazelle International - New York, NY\nJanuary 1996 to January 1999\nGained invaluable command of the Quantum data processing platform while leading the coding and processing of data, transforming raw information into actionable reports on behalf of clients of this worldwide, premiere coaching association composed of independent, professional business coaches.\n* Administered projects with budgets of over $250K annually.\n\nEarly Career\nSpec Writer, CRC Information Systems, New York, NY\nFreelance Project Director, American Express, Schering-Plough, Colgate-Palmolive, RJR Nabisco, Nestle\nProject Director, Lieberman Research Suburban, White Plains, NY\nProject Director, Peter Honig Associates, White Plains, NY']","[u'MBA in Marketing', u'BS in Marketing, Business Administration']","[u'Northeastern University', u'SUNY Albany']"
0,https://resumes.indeed.com/resume/3eaa267d46fc4f9c,"[u""Data Analyst/Tableau Developer\nFlow Contract Site - Bothell, WA\nFebruary 2013 to Present\nResponsibilities:\n\u2713 Develops and maintains data solutions using analytical and clinical data systems.\n\u2713 Analysis and reporting of Scientific Preclinical and/or Clinical Data using BI Reporting Tools.\n\u2713 Analyze, create, and test report output to ensure accurate and detailed data is supplied to the end user.\n\u2713 Coordinates the planning, defining and testing new measures, development and documentation of data or automation collection processes, validate data accuracy, and improvement of data collections\n\u2713 Involved in publishing of various kinds of live, interactive data visualizations, dashboards, reports and workbooks from Tableau Desktop to Tableau servers.\n\u2713 Involved in conducting trainings to user on interact, filter, sort and customize views on an existing visualization generated from Tableau desktop.\n\u2713 Extensively participated in translating business needs into Business Intelligence reporting solutions by ensuring the correct selection of toolset available across the Tableau BI suite.\n\u2713 Created Advance connections, join new tables, create & manage Extract and Monitor Queries using SQL Assistant.\n\u2713 Created Complex workbooks and Dashboards by connecting to multiple data source using data blending. Involved in almost most of the concepts in tableau like sets, grouping, parameters, and highlight actions.\n\u2713 Utilized advance features of Tableau software like to link data from different connections together on one dashboard and to filter data in multiple views at once.\n\u2713 Created Prompts, customized Calculations, Conditions and Filter (Local, Global) for various analytical reports and dashboards.\n\u2713 Create and modify Interactive Dashboards and Creating guided navigation links within Interactive Dashboards.\n\u2713 Created Table of Contents (TOC), a common navigation page which had all the links to various reports.\n\u2713 Design and development of various reports and dashboards that can be easily accessible through Tableau App in Ipads, iPhones, and other smart mobile phones.\n\u2713 Good Experience in publishing reports and dashboards to Tableau server.\n\u2713 Created complex data Views manually using multiple measures, also used sort, Filter, group, Create Set functionality.\n\u2713 Published Reports, workbooks & data source to server and exporting the reports in different Formats.\n\u2713 Good experience in using URL's, Hyperlinks, Filters for developing Complex dashboards.\n\u2713 Worked creating Aggregations, calculated Fields, Table calculations, Totals, percentages using Key Performance Measures (KPI) and Measure. Perform end-to-end data analysis and ensure data quality gaps are identified.\n\u2713 Data management and data analysis work, including use of statistical methods\n\u2713 Importing data from other sources, exporting data to other sources, creating graphs, utilization of statistical tests and tools.\n\nEnvironment: Tableau Desktop v10.x/9.x/8.x, Tableau Server, Tableau Admin, SQL Server 2012/2016/2008 R2, T-SQL, SQL Server Management Studio, Microsoft Excel."", u'BI Analyst/Data Analyst\nCedars Sinai Medical Center - Los Angeles, CA\nMarch 2009 to November 2012\nResponsibilities:\n\u2713 Collaboratively work with other members of the Data Warehouse team, IT and Business partners.\n\u2713 Communicate and resolve issues and questions during the development, testing, and roll-out of BI solutions.\n\u2713 Modify existing BI Portal reports.\n\u2713 Create clear and meaningful documentation, both for a technical audience and for end users.\n\u2713 Work on multiple projects in parallel\n\u2713 Develops and maintains data solutions using analytical and clinical data systems.\n\u2713 Reports with tools like Excel, Power Query, Power Pivot, Power and Power View.\n\u2713 Plotting graphs, formulas, and functions.\n\u2713 Importing data from other sources, exporting data to other sources, creating graphs, utilization of statistical tests and tools.\n\u2713 Worked with the management for the determination and identifying the problem.\n\u2713 Responsible for the maintenance of the secure data transfer.\n\nEnvironment: SQL Server, T-SQL, SQL Server Management Studio, MS Excel, MS Access Power Query, Power Pivot, Power and Power View.', u'Graduate Assistant\nUniversity of Toledo\nAugust 2006 to December 2008\nWorked as Graduate Assistant under the supervision of Principal Investigator in various roles and received scholarships.']","[u'Master of Science', u'MS in Data Management for Clinical Research', u'Certification in Life Sciences']","[u'University of Toledo Toledo, OH', u'Vanderbilt University', u'Johns Hopkins University']"
0,https://resumes.indeed.com/resume/31190a62df790f6d,"[u'Data Analyst Intern\nMyTenant - Pune, Maharashtra\nFebruary 2016 to January 2017\nProvided creative solutions for reporting and dissemination of data which helped to understand the weaker\nzones of the business.\n\u2022 Developed dashboards to project forecasted business needs.\n\u2022 Reviewed monthly ITSM balanced scorecard using vlookup, hlookup, pivot tables and developed dashboards\nfor reporting data analysis and visualization of the metrics.\nRELEVANT COURSEWORK']","[u'Master of Science in Information Science in Information Science', u'Bachelor of Engineering in Electronics and Telecommunication in Electronics and Telecommunication']","[u'New Jersey Institute of Technology Newark, NJ\nJuly 2019', u'Savitribai Phule Pune University Pune, Maharashtra\nJuly 2015']"
0,https://resumes.indeed.com/resume/190813f20d043f2a,"[u'Data Reporting Analyst\nTruGreen - Memphis, TN\nJanuary 2018 to March 2018\n\u2022 Analyzed the data and designed, developed, documented and maintained complex database queries for ad hoc reporting used in complex conceptual analysis using SQL Queries, Macros, Workday and Excel.\n\n\u2022 Responsible for generating Call metrics and Performance metrics of Recruiters using Virtual Contact Center(VCC) and Visualizing them using Tableau.\n\n\u2022 Responsible for creating the Daily Tracker Reports for the recruiters and updating them weekly basis and developed linear programming model in cost minimization.\n\n\u2022 Compared month to date goals and performing expected vs actual results.\n\n\u2022 Analyzed and provided recommendations on Recruiting source websites Cost vs the applicants generated.\n\n\u2022 Developed complex SQL queries and standardized reports/dashboards and submit to business owners, HR Directors and several leaders using Tableau/Excel, providing them critical metrics/insights. Publishing and schedule the reports to tableau server.\n\n\u2022 Developed a recruiting model for analyzing the requirement of recruiters for hiring the Sales and production team and also responsible for analyzing the candidate source data in minimizing the costs.\n\n\u2022 Developed the Candidate flow pipeline reports from Workday and submitted to the respective divisional managers and supervisors.', u'DATA ANALYST\nSUNGRACE ENERGY SOLUTIONS PVT LTD HYD\nJune 2013 to November 2015\n\u2022 Experienced in data extraction, data summarization/aggregation, analysis data visualization.\n\n\u2022 Responsible for generating Call Metric and Customer Business Review Metric reports i.e. SQI and KPI metrics\n\n\u2022 Acted as the subject matter expert (SME) and single point of contact (SPOC) for the whole SQI metric, used historical data to assign goals for the future fiscal years using basic algorithms\n\n\u2022 Participated in several leadership meetings and presented the analysis of the performance of the various sales segments with respect to their compensation reports during the fiscal year.\n\n\u2022 Compared year to year goals and Performed root cause analysis on it. Compared and performed actual vs Expected results\n\n\u2022 Designed database, tables and views structure for the application and Responsible for optimizing SQL queries by creating indexes, stored procedures to improve the performance time.\n\n\u2022 Responsible for forecasting demand and also build a Linear programming model (Optimizing the cost) for Inventory Control using Optimization techniques.\n\n\u2022 Experienced with creating reports in sales org for customer relation team using Tableau and Excel. Queried and pulled the data from multiple data sources like SQL server database.\n\n\u2022 Performed data pre-processing and cleaning to prepare data sets for further statistical analysis\n\n\u2022 Analyzed the trends and patterns in the data received from multiple data sources and performed descriptive and predictive statistics on warehouse data. Besides that, I was responsible for various forecasting operations.', u'INTERN\nWORTHING NITIN CYLINDERS PVT LTD\nJune 2012 to December 2012\n\u2022 I was responsible for collecting the daily production data and manpower data and Generated daily progress reports.\n\n\u2022 Good experience with Excel, Pivot tables and Aggregating formulas and Learned Building databases from ER diagrams and able to pull the data by querying from database using SQL Server.\n\n\u2022 Identified and removed in excess and obsolete inventory over a 6 months period. collaborated with engineers and performed the planning and supervised the work area\n\n\u2022 Analyzed data to ensure proper work volume and projected growth.']","[u'Masters of Science in Industrial Engineering in Data Base Management System', u'in Technology']","[u'Texas A&M University Kingsville, TX\nMay 2017', u'K L University Guntur, Andhra Pradesh\nJune 2013']"
0,https://resumes.indeed.com/resume/feb9ca66fc67484c,"[u'Data Analyst\nBrightline TV - New York, NY\nMarch 2017 to October 2017\n\u2022 Transformed data and delivered campaign reports with KPI metrics in PowerBi for clients\n\u2022 Maintained and troubleshoot issues in SQL Server database & Microsoft Azure Data Factory\n\u2022 Conducted Quality Assurance analyses and User Acceptance testing to validate new interactive Smart TV ads\n\u2022 Consolidated documentations on defining metrics and staging events within the database\n\u2022 Analyzed interactive survey data and advised clients on who may be their targeted demographic to improve sales\n\u2022 Managed and verified billing reports to clients based on campaign performance', u'Associate Data Analyst (Consultant)\nDentsu Aegis Network - New York, NY\nNovember 2016 to February 2017\n\u2022 Performed QA analyses and ETL processes with advertising platforms and database warehouse\n\u2022 Delivered detailed monthly progress reports of advertising campaign progress to clients\n\u2022 Defined standard operating procedures and exception reports for operational efficiency\n\u2022 Produced insights and forecast analysis based on trends & KPI metrics\n\u2022 Ensured and maintained data integrity and investigated data anomalies', u'Data Analyst (Consultant)\nTravelClick - New York, NY\nJanuary 2016 to July 2016\n\u2022 Utilized SQL query and extracted data routinely from Google DoubleClick and Ad exchange\n\u2022 Executed ETL process and link multiple data warehouses to facilitate data validation\n\u2022 Maintained daily media data in Excel Pivots with detailed analyses of all campaign progression\n\u2022 Composed documentations and process flow to assist team with data integration\n\u2022 Constructed Tableau presentation for clients displaying returns on investments with data visualization', u'Junior Systems/Business Analyst\nOEC Group - New York, NY\nDecember 2014 to December 2015\n\u2022 Spearheaded and implemented logistics transparency project related to shipping process, resulted in saving over $2 million in additional logistical expenses for a client\n\u2022 Produced daily ad-hoc data analysis reports for VP of Operation and all departments using Crystal Reports\n\u2022 Performed User Acceptance Testing to identify weakness and present solutions\n\u2022 Facilitated File Transfer Protocol of shipment data into supply chain management systems\n\u2022 Managed ticketing system and resolve issues in a timely manner\n\u2022 Created SugarCRM dashboards for Sales and upper management to visualize shipping trends']",[u'Bachelor of Science in Economics and Psychology in Economics and Psychology'],"[u'Bernard M. Baruch College New York, NY\nSeptember 2009 to May 2014']"
0,https://resumes.indeed.com/resume/329f32f23d61805f,"[u'Hostess\nMacados - Blacksburg, VA\nSeptember 2017 to Present\n\u2022 Improved ability to multi-task efficiently engaging with both customers and co-workers\n\u2022 Responded to past-paced environment in a calm manner', u""Data Analyst\nCenter for Survey Research\nJanuary 2016 to Present\nTranslated caller's responses into online data for a given company""]",[u'B.S. in Civil Engineering'],"[u'Virginia Tech Blacksburg, VA']"
0,https://resumes.indeed.com/resume/c73f6bdd2d1f2404,"[u'Data Analyst, Consultant\nState Street - Boston, MA\nJanuary 2017 to January 2018\nWorked on the projects, which require the data and business analysis, utilizing Databases extracts, advanced Excel, IGC\n\u2022 Worked on the data analysis of multiple applications per Application Inventory, identified and documented critical data elements, including metadata, lineage and business rules for in-scope systems,produced the Matrix of the applications and Reports. Used Data Analytics Tool - IGC\n\u2022 Created functional specifications, collaborated with Data Steward,business and technical applications owners to process applications under projects requirements\n\u2022 Identified, extracted the data from the Databases using SQL\n\u2022 Supported the project documentation to keep tracking for discovery applications, worked very close with Data Government Frame (DGF - development) team\n\u2022 Worked very close with project team and provided consultative support to project team partners during downstream phases of the SDLC', u'Data Analyst, Consultant\nSantander Bank - Boston, MA\nJanuary 2016 to January 2016\nWorked on the data analysis projects, utilizing SQL, Excel\n\u2022 Worked on the data analysis, conducted and documented gap analysis, presented\nrecommendations to project management\n\u2022 Created functional specifications, collaborated with users and produced business requirements\n\u2022 Conducted and documented process, business and capability requirement elicitation work sessions with subject matter experts\n\u2022 Identified and analyzed business rules, business and system process flows\n\u2022 Identified and documented requirement and project related issues, assumptions, constraints, dependencies, identified risks, and decisions\n\u2022 Worked very close with project team and provided consultative support to project team partners during downstream phases of the SDLC', u'Data Analyst, Consultant\nState Street - Quincy, MA\nJanuary 2016 to January 2016\nWorked on the Data Security project, utilizing SQL, ORACLE\n\u2022 Performed data analysis to identify Applications with shareable process id\n\u2022 Performed data profiling, created and ran SQL scripts and queries\n\u2022 Prepared the Matrix with the results of analysis, presented the results to applications owners and business users', u'Data Analyst, Consultant\nCitizens Bank - Providence, RI\nJanuary 2013 to January 2015\nWorked on the Risk Data Platform system, especially on CCAR, utilizing SQL, Excel\n\u2022 Worked on the data analysis, conducted and documented gap analysis\n\u2022 Produced functional specifications, discussed with user and produced business requirements\n\u2022 Worked very close and discussed business requirements with project team\n\u2022 Conducted and documented process, business and capability requirement elicitation work sessions with subject matter experts\n\u2022 Worked very close with testing team, involved in test cases and analyzed the test result\n\u2022 Test scenarios and Test cases creation for functional testing and UAT.', u""Data Analyst, Consultant\nState Street - Quincy, MA\nJanuary 2013 to January 2013\nWorked on the Data Transformation project, utilizing SQL, ORACLE\n\u2022 Worked on the data business analysis and enhancement business process and system\n\u2022 Performed technical analysis, created and tested stored procedure\n\u2022 Used Agile methodology, collaborated and discussed any project issues\nwith managers and agile team's members"", u'Business System Analyst, Consultant\nOrder Financial System - Hingham, MA\nJanuary 2011 to January 2012\nWorked on the support and enhancement of Order Financial System, utilizing SQL, Excel\n\u2022 Performed business, data and financial analysis to enhance the financial system\n\u2022 Produced functional specifications, discussed with user and produced business requirements\n\u2022 Worked very close and discussed business requirements with project team\n\u2022 Analyzed and resolved complex production issues and recommended system enhancements to developers', u'Business System Analyst, Consultant\nTJX Companies - Framingham, MA\nJanuary 2010 to January 2011\nWorked on multiple projects to enhance Purchase Order System (POS) utilizing SQL, ORACLE, DB2:\n\u2022 Analyzed data, business requirements and created data reports per business requets\n\u2022 Directly involved in all phases of software development, testing, and support\n\u2022 Created and executed queries and stored procedures', u""Senior Business System Analyst\nSTATE STREET CORPORATION/INVESTOR BANK & TRUST CO - Boston, MA\nJanuary 1999 to January 2010\nLed multiple cross-functional efforts to enhance, streamline, and convert systems.\n\u2022 Played leading role in multi-year project to convert Mutual Fund Assets System (MAS) from Mainframe to a vendor package Power Agent (P/A) on a Windows platform:\n\u25e6 Developed and executed SQL procedures comparing production and test data, analyzed and presented results to business users\n\u25e6 Performed data analysis, extracted and delivered MAS data sets to the conversion projects\n\n\u25e6 Developed Data Interfaces between P/A and clients using Informatica ETL and Ascential Data Stage tools, created DTS and SSIS packages\n\u2022 Contributed to a multi-year project to convert IBT funds to State Street's platform:\n\u25e6 Analyzed data, business requirements for the conversion projects\n\u25e6 Developed and used the scripts to create, manage, and verify the list of funds\n\u25e6 Updated and enhanced the fund conversion procedure\n\u25e6 Processed converted funds and presented results to business partners\n\u2022 Participated in a multi-year project to develop and enhance the InvestSmart funds processing system on an Oracle platform:\n\u25e6 Developed, tested and installed Data Stage scripts\n\u25e6 Developed Unix scripts and managed file movement\n\n\u2022 Led multiple projects to enhance MAS using COBOL, CICS, DB2, VSAM:\n\u25e6 Analyzed requirements and created technical specifications for development projects\n\u25e6 Designed new subsystems and identified enhancements to existing subsystems\n\u25e6 Played integral role in development cycle including design and coding, unit and system testing, user acceptance testing, and deployment in production\n\u25e6 Provided on-call production support; analyzed and resolved production problems\n\u25e6 Provided technical leadership to a group of developers across multiple projects\n\u2022 Provided technical leadership in two major projects to decommission mainframe applications and consolidate mainframe infrastructure:\n\u25e6 Defined and created the business and technical requirements to shutdown MAS and VISA systems. This document included the steps for business units, scheduling team and developers to shutdown two mainframe systems""]","[u'MS in Electrical Engineering in Electrical Engineering', u'in Computer Programming']","[u'Saint Petersburg Electrotechnical University St. Petersburg, RU', u'Saint Petersburg University St. Petersburg, RU']"
0,https://resumes.indeed.com/resume/5304d344b10a0d50,"[u'Data Analyst/Data Modeler\nSunTrust Bank - Atlanta, GA\nAugust 2015 to Present\nResponsibilities:\n\u2022 Developed strategic oversight and planning of data models and database design within the CRM platform.\n\n\u2022 Worked with key stakeholders to understand the data requirements and translates strategic requirements into usable enterprise information architecture. Ensures existing data/information assets are identified, documented, stewarded, and leveraged across the organization.\n\u2022 Used Erwin for effective model management of sharing, dividing and reusing model information and design for productivity improvement and identified the Facts and Dimensions from the business requirements and developed the logical and physical models using Erwin.\n\u2022 Working with various Teradata tools and utilities like Teradata Viewpoint, MultiLoad, ARC, Teradata Administrator, BTEQ and other Teradata Utilities.\n\u2022 Conducting complex Notes ETL data migrations to include assessing, planning, implementation, software development and data validation.\n\u2022 Developing data and metadata policies and procedures to build maintain and leverage data models, ensuring compliance with corporate data standards and Analyze change requests for mapping of multiple source systems for understanding of Enterprise wide information architecture to devise Technical Solutions.\n\u2022 Created stored procedures and functions using Dynamic SQL and T- SQL.\n\u2022 Generated comprehensive analytical reports by running SQL queries against current databases to conduct data analysis and handled performance requirements for databases in OLTP and OLAP models.\n\u2022 Worked in importing and cleansing of data from various sources like Teradata, Oracle, Netezza, flat files, SQL Server with high volume data.\n\u2022 Developed complex Stored Procedures for SSRS (SQL Server Reporting Services) and created database objects like tables, indexes, synonyms, views, materialized views etc.\n\u2022 Documented ER Diagrams, Logical and business process diagrams and process flow diagrams and Understand and analyze business data requirements and architect an accurate, extensible, flexible and logical data model.\n\u2022 Created complex SQL server and Oracle stored procedures and triggers in support of the applications with Error handling.\n\n\u2022 Involved in Performance tuning the SQL Server Queries, which are used in SSRS Reports and \u2022 developed Ad Hoc Report Model for specific users is SSRS.\nEnvironment: Erwin r9.6, Informatica Power Center, Teradata, Oracle 12c, Microsoft SQL Server 20012, SVN tool, BTEQs, Load, SSRS, T-SQL, Teradata SQL Assistant, Netezza, Fast Load, UNIX scripting, PL/SQL Programming', u'Data Modeler/Operations Analyst\nNorristown, PA\nJanuary 2014 to July 2015\nResponsibilities:\n\u2022 Developed normalized Logical and Physical database models to design OLTP system for finance\n\napplications. Created dimensional model for reporting system by identifying required dimensions and facts using Erwin.\n\n\u2022 Conducted team meetings and Joint Application Design (JAD) session.\n\n\u2022 Developed a Conceptual model using Erwin based on business requirements and produced functional decomposition diagrams and defined logical data model.\n\n\u2022 Generated ad-hoc SQL queries using joins, database connections and transformation rules to fetch data from legacy Oracle and SQLServer database systems\n\n\u2022 Used Erwin for reverse engineering to connect to existing database and ODS to create graphical representation in the form of Entity Relationships and elicit more information.\n\n\u2022 Managed the historical data in the data warehouse from various data sources and generated various drill down, drill through, matrix and chart reports using SSRS.\n\n\u2022 Used forward engineering to create a Physical Data Model with DDL, based on the requirements from the Logical Data Model and implemented Referential Integrity using primary key and foreign key relationships.\n\n\u2022 Performed Data validation, Data cleansing, Data integrity, Data Quality checking before delivering data to operations, Business, Financial analyst by using Oracle, Teradata.\n\n\u2022 Used Model Mart of Erwin for effective model management of sharing, dividing and reusing model information and design for productivity improvement and identified and tracked the slowly changing dimensions and determined the hierarchies in dimensions.\n\n\u2022 Consulted with client management and staff to identify and document business needs and objectives, current operational procedures for creating the logical data model.\n\n\u2022 Facilitated in developing testing procedures, test cases and User Acceptance Testing (UAT) and applied data naming standards, created the data dictionary and documented data model translation decisions and also maintained DW metadata.\n\n\u2022 Worked on enterprise logical data modeling project (in 3NF) to gather data requirements for OLTP enhancements.\n\n\u2022 Involved in extensive Data validation by writing several complex SQL queries and Involved in back-end testing and worked with data quality issues.\n\n\u2022 Worked in importing and cleansing of data from various sources like\n\n\u2022 Teradata, Oracle, flatfiles, SQLServer with high volume data.\n\nEnvironment: Erwin 9.x, SQL, SQL Server 2008, Rational Rose, Windows XP, Oracle 10g, TOAD, PL/SQL, Flat Files, Teradata, T-SQL, Netezza Aginity, MDM, informatica Power Centre, DB2, SSRS, SAS.', u""Data Modeler/Data Analyst\nUNIVISION - Teaneck, NJ\nApril 2011 to December 2013\nResponsibilities:\n\u2022 Involved in the Architectural role to design and develop EDW program initiative and involved in\n\ndesign and conversion of business requirements to Conceptual Data Model, logical Data Model and Physical Data Model for subject areas\n\n\u2022 Document all data mapping and transformation processes in the Functional Design documents based on the business requirements and queried the databases, wrote test validation scripts and performed the System testing.\n\u2022 Involved in design and conversion of business requirements to Conceptual Data Model, logical Data Model and Physical Data Model for subject areas and reverse engineered all the Source Database's using Erwin.\n\u2022 Participated in integration of MDM (Master Data Management) Hub and data warehouses.\n\u2022 Created complex SSIS packages and SSRS Reports from Stored Procedures, Views and by writing complex T-SQL.\n\u2022 Wrote large/complex queries using SQL and by using Teradata SQL Assistant and created databases, users, tables, triggers, macros, views, stored procedures, functions, joins and hash indexes in Teradata database.\n\u2022 Worked with internal architects in the development of current and target state data architectures and documented, clarify, and communicate requests for change requests with the requestor and coordinate with the development and testing team.\n\u2022 Used data analysis techniques to validate business rules and identify low quality missing data in the existing data.\n\u2022 Creation of database objects like tables, views, Materialized views, procedures, packages using Oracle tools like PL/SQL, SQL* Plus, SQL*Loader and Handled Exceptions.\n\u2022 Created Entity relationship diagrams (ERD), Function relationship diagrams (FRD), data flow diagrams (DFD) and enforced all referential integrity constraints using Erwin.\n\u2022 Created PL/SQL packages and Database Triggers and developed user procedures and prepared user manuals for the new programs.\n\u2022 Run batch jobs for loading database tables from Flat Files using SQL*Loader.\nEnvironment: Oracle 10g/11g, Erwin r9.5, UDB, Informatica PowerCenter 9.1, SSRS, MS Access, MS Excel, Netezza Aginity, UNIX, MDM, MS Word, SQL Server, MS Outlook, SQL, T-SQL, Teradata SQL Assistant, Crystal Reports, PowerPoint 2007, Hadoop."", u'Data Modeler/Data Analyst\nSouthwest Airlines - Dallas, TX\nAugust 2009 to March 2011\nResponsibilities:\n\n\u2022 Review Mapping Documents formula and rule engine and Analyzing change request for the\n\nexisting mapping and creating an impact analysis for the change on Data.\n\n\u2022 Involved in Logical and Physical modeling of various Data marts as well as data warehouse and wrote new mappings specification according to the business data requirements and designed a STAR schema for the detailed data marts and Plan data marts involving confirmed dimensions.\n\n\u2022 Exhaustively collected business and technical metadata and maintained naming standards.\n\n\u2022 Prepared Test data preparation strategy and data masking strategy for the shared test environment and created Tabular, matrix and chart reports.\n\n\u2022 Developed normalized Logical and Physical database models to design OLTP system for applications.\n\n\u2022 Using SQL Server SSRS generate and maintain five business reports including dashboard report, matrix report, parameter report with charts and drill down function to monitor project progress and used store procedures, views, functions using Report Writer, SSRS and Crystal Reports.\n\n\u2022 Used Model Mart of Erwin for effective model management of sharing, dividing and reusing model information and design for productivity improvement.\n\n\u2022 Used Pivot tables, vlookups and conditional formatting to verify telemarketing data against data uploaded to proprietary database and online reporting.\n\n\u2022 Created ETL mapping documents for every mapping and Data Migration document for smooth transfer of project from development to testing environment and then to production environment.\n\n\u2022 Extracted data from the databases (Oracle and SQL Server, DB2, FLAT FILES) using Informatica to load it into a single data warehouse repository.\n\n\u2022 Intensive data mining practice for data manipulation/extraction in Teradata environment for data integration and created data rules to handle SCD Type II data using PLSQL.\n\n\u2022 Perform Source-to-Target Data Mapping in Informatica. (ETL) and optimized mappings.\n\n\u2022 Reviewed all artifacts created with respect to Logical and physical data models including Tables, procedures, indices, synonyms, views, functions, triggers, sequence, and constraints.\n\nEnvironment: ETL, SQL Server, Crystal Reports, Quality Center 10.0, Agile, Java, XML, HTML, Windows and Unix, Shell Scripting, SQL, Netezza, PL/SQL, TOAD for Oracle 9.x, Requisite Pro, Oracle 10g, Pivot Tables , MS Office, Erwin , Teradata, COGNOS, SAS.']",[u'BS in Electrical Engineering'],"[u'Temple University Philadelphia, PA']"
0,https://resumes.indeed.com/resume/c2b45231a30062fc,"[u'Data Analyst\nWestStar Insurance inc.\nSeptember 2014 to November 2016\n\u03bb Research and apply the latest quantitative techniques toward the solution of important problems, research and analyze data and make analytic solutions\n\u03bb Design, create, develop and test models, and gain insights from research findings\n\u03bb Translate ideas and theory into solutions from which the business can grow, document and operationalize existing quantitative data warehouse\n\u03bb Identify new and novel data sources and explore their potential use in developing actionable business insights, Develop and processes working projects directly with IT Systems Teams.\n\u03bb Developing new quantitative algorithms to get more accurate risk premium and saves 1.6 million per year.\n\u03bb Explore emerging technologies and analytic solutions for use in quantitative model development, and help maintain and enhance existing models']","[u'in Finance', u'', u'B.A. in Economics in Economics', u'M.B.A. in Quantitative Finance in Quantitative Finance']","[u'National Tsing Hua University\nJanuary 2012 to January 2014', u'National Chung Hsin University\nJanuary 2006 to January 2010', u'College of Finance', u'College of Management']"
0,https://resumes.indeed.com/resume/150f7ab86982ad48,"[u'Data Analyst-Senior\nAIG\nOctober 2017 to February 2018\n\u2022 Provided direct business support for Eastern Zone Operations for AIG Private Client Group (PCG)\n\u2022 Assisted for operating a data entry device to input lists, records, or other data points into electronic format\n\u2022 Worked closely to review and cross reference policies along with other data in companies online portal\n\u2022 Analyzed data to identify issues of errors and other inconsistency prior to data processing with strong attention to detail\n\u2022 Resolved day to day Data related issues with customers policy to enhance business aspects\n\u2022 Generated Renewal migrations to operations team about received and resolving issues from customers using online tracker\n\u2022 Amended the issues of clients within specific business procedures and guidelines\n\u2022 Generated and confirmed with the broker regarding policy endorsements and tier rating worksheets\n\u2022 Monitor and analyze the historical data, productivity and current trends to identify opportunities\n\u2022 Validated data with reviewing and cross referencing data along with detailed policies\n\u2022 Assisted to drive financial results that maximize sales, inventory and bottom line profitability resulting in gross margin return on investment', u""Data Analyst\nFIDELITY INVESTMENTS\nAugust 2013 to May 2017\n\u2022 Provided direct financial support for Fidelity Investments\n\u2022 Resolved day to day Master Data related issues with customers to improve business growth\n\u2022 Generated reports to management team about received and resolving issues from customers using Excel spreadsheets\n\u2022 Provided weekly, monthly and quarterly reports about project status with extensive data entry\n\u2022 Collected data from all team members related to project status and issues\n\u2022 Maintained data information in Excel spreadsheet for future reporting analysis\n\u2022 Assisted with data representation for analysis of project using Tableau, Pivots and Dashboards\n\u2022 Worked closely with IT team for building reports through Cognos Analytics and verify with client's data\n\u2022 Validated data on Ad-hoc reports to ensure a high-degree of completeness, accuracy and compliance with all departmental policies and standard operating procedures\n\u2022 Assisted with data manipulation through SQL queries which includes progress and drawbacks of clients and their status\n\u2022 Researched, analyzed and responded to client inquiries that required an understanding of the client and ability to follow specific business procedures and guidelines\n\u2022 Managed customer facing Master data using SAP platform to maintain clean communication with customers\n\u2022 Developed and maintained allocation strategies to ensure maximum sell through and profitability with clients""]","[u""Master's in Business Administration in Marketing and Human Resources"", u""Bachelor's of Technology in Electronics & Communication Engineering""]","[u'Osmania University\nJune 2013', u'Jawaharlal Nehru Technological University\nJune 2011']"
0,https://resumes.indeed.com/resume/45d12da95db55108,"[u'Business Data Analyst\nThomson Reuters - Houston, TX\nSeptember 2016 to Present\nThomson Reuters is a leading source for intelligent information for the businesses and professionals, combining industry expertise with innovative technology to deliver critical information to leading decision makers in the financial and risk, legal, tax and accounting, intellectual property, science and media.\nProject: QA Direct - Quantitative Analysis and Data Management. Thomson Reuters developed its own Proprietary product called QA direct with Advanced database structure which Utilize Microsoft SQL Server or Oracle with fully documented database schema and diagrams to integrate with the premier market data content into one normalized database which contains global pricing data, economic and company data from Thomson Reuters and third-party providers. QA Direct provides access to a cross asset content offering with the deepest history, robustness, and data availability - offering unique content sets that assist clients in achieving alpha. Data is automatically updated intraday into client server. QA Direct can be linked to other commercial, statistical and portfolio optimization software, such as SAS, Matlab, R and S-PLUS - making it easy to streamline workflow into a fully integrated, quantitative solution.\nRoles & Responsibilities:\n\n\u2022 Responsible for gathering business requirements utilizing Joint Application Development sessions, interviews, surveys, use cases and document analysis on wide range of projects.\n\u2022 Effectively facilitates and participates in Joint Application Development sessions with project stakeholders to gather detailed business rules and functional requirements.\n\u2022 Identify, document and communicate data discovery and data mapping processes to support successful data conversion and project implementation.\n\u2022 Created new reports based on requirements. Responsible in Generating Weekly ad-hoc Reports.\n\u2022 Create end to end operation process, business improvements and performance tunings\n\u2022 Extract the data from database and provide data analysis using SQL to the business user based on the requirements. Create pivots and charts in excel sheet to report data in the format requested.\n\u2022 Review the database proactively to identify inconsistencies in the data, conduct research using internal and external sources to determine information is accurate.\n\u2022 Identify, document and communicate data discovery and data mapping processes to support successful data conversion and project implementation\n\u2022 Responsible for direct interaction with the Business Consultants, DBA team and End User to gain a thorough understanding of the data being requested\n\u2022 Responsible for Creating/maintaining complex SQL queries, custom queries, views, and stored procedures\n\u2022 Designed, developed, tested and maintained Tableau functional reports based on user requirements\n\u2022 Mastered the ability to design and deploy rich graphic visualization with drill down drop-down menu option and parameters using Tableau.\n\u2022 Utilized Tableau server to publish and share the reports with the business users.\n\u2022 Involved in Data Analysis, Data Cleansing, Requirements gathering, Business Analysis, Entity Relationship diagrams (ERD), Architectural design docs, Functional and Technical design docs, and Process Flow diagrams\n\u2022 Designs and or develops database objects (databases, tables, stored procedures, DTS Packages) to support the collection, tracking and reporting of business data.\n\u2022 Created the Source to target mapping documents and developed the file processing rules\n\u2022 Provide functional support to the technical team during development, and test programs after completion to ensure compliance with the specification\n\u2022 Assist data quality team, support team and data metrics team with operational projects\n\u2022 Investigate operational problems, identify causes and support the testing and implementation of corrections\n\u2022 Interact with vendors to resolve data quality issues, and other data related issues with input.\n\u2022 Worked extensively on Data Quality (running Data Profiling, Examine Profile outcome); Metadata management (loading metadata, mapping metadata, or perform data linkage).\n\u2022 Create the Source to target mapping documents and developed the file processing rules\n\u2022 Developed ETL mappings, transformations using Informatica PowerCenter\n\u2022 Extensively used ETL Tool Informatica to load data from Flat Files and Oracle\n\u2022 Developed and tested all the Informatica mappings and update processes.\nEnvironment: Oracle 11g, SQL Server 2012, Postgres, SFDC, Tableau 10.3, Informatica, Windows, MS Office Suite', u'Data Analyst\nThomson Reuters - IN\nMay 2010 to December 2015\nRoles & Responsibilities:\n\u2022 Performed Gap Analysis to check the compatibility of the existing system infrastructure with the new business\nrequirements.\n\u2022 Analyzed data by creating SQL queries and identified fact and dimension tables.\n\u2022 Created new database objects like Procedures, Functions, Packages, Triggers, Indexes and Views using SQL in SQL Server\n\u2022 Design and deployed rich Graphic visualizations with Drill Down and Drop-down menu option and Parameterized using Tableau and provided these visuals in Tableau online and Reader to Stake Holders.\n\u2022 Created views in Tableau desktop that was published to internal team for review and further data analysis and customization using filters and actions.\n\u2022 Performed data analysis and data profiling using SQL queries on various sources systems including Oracle, SQL Server.\n\u2022 Execute all regular transaction processes necessary to maintain operations records and databases.\n\u2022 Perform Data analysis and data divergence using in-house application.\n\u2022 Perform extracting, importing, and exporting of data in various database applications.\n\u2022 Manage the process that does the ingestion and delivery of subsets of licensed data to each of our client as per schedule.\n\u2022 Designed, developed, tested, and maintained Tableau reports based on user requirements.\n\u2022 Audit data on a regular basis to ensure data integrity and quality.\n\u2022 Determine root cause for data quality errors and make recommendations for long-term solutions.\n\u2022 Keep a server updated and running, take a preventive maintenance measures and work with network administrator for the maintenance of the ingestion servers.\n\u2022 Involved in deploying in the production environment along with developers and providing production support for data when issues arise.\n\u2022 Assisted in designing test plans, test scenarios and test cases for integration, regression and user acceptance testing.\n\u2022 Extensively used oracle packages, stored procedures, sequences, ref-cursors, dynamic queries, UTL files, bulk inserts, exception handling, DB Links etc.\n\u2022 Involved in data migration of various data sources from several platforms to Oracle\n\u2022 Coordinated multiple small and large Data Integration, Data warehousing and Oracle development projects.\n\nEnvironment: SQL Server 2012, Erwin, Informatica, Tableau, PLSQL, Windows, MS Office Suite.', u""Market Analyst\nReuters - IN\nJanuary 2009 to April 2010\nRoles & Responsibilities:\n\u2022 Gather financial information of the world's leading public and private companies, analyze, prepare reports and upload the same into Thomson Reuter's financial products in a timely and accurate manner.\n\u2022 Carrying out statistical evaluations and automated reporting framework.\n\u2022 Use of financial knowledge to add value to current data offering.\n\u2022 Keep abreast of daily market activity to ensure Thomson Reuter's information is timely and complete.\n\u2022 Resolve data related queries from internal and external clients.\n\u2022 Work with data quality team to resolve data discrepancy.\n\u2022 Ensure all quality tests and process checks are performed and documented.\n\u2022 Initiating new methods of working and ways to improve data and service provided to the client.""]","[u""Master's Degree in Master of Business Administration in Business Administration""]","[u'Bangalore University Bengaluru, Karnataka']"
0,https://resumes.indeed.com/resume/842df9db96fc095f,"[u'Analytics Consultant\nNorthwestern Medicine - Chicago, IL\nDecember 2016 to Present\n\u25cf Define, extract, analyze and presents data cogently to answer the question being asked.\n\u25cf Work with business users to define requirements, formulate queries, validate results, and generate reports using Microsoft SQL Server Reporting Services (SSRS).\n\u25cf Analyze large datasets, evaluate data quality, and interpret results in a clear, concise manner.\n\u25cf Works collaboratively with and supports multi-departmental efforts and projects.\n\u25cf Wrote queries and stored procedures as a part of a huge remediation effort.\n\u25cf Used queries and stored procedures written to create different visualizations in tableau, or reports in SSRS.\n\u25cf All other duties as assigned.', u""Data Integration Engineer\nWageWorks - Mequon, WI\nJanuary 2014 to December 2016\n\u25cf Wrote scripts using the PL/SQL language, which are used to solve day to day data issues.\n\u25cf Wrote complex SQL queries for different report requests.\n\u25cf Responded to technical issues and provide third tier support for escalated non-repetitive data issues.\n\u25cf Analyzed, identified and resolved data issues by creating complex programs to resolve data conditions and anomalies in an efficient manner that is in accordance with established Service Level Agreements (SLA's).\n\u25cf Monitored job results, identified trends in data or process issues, diagnosed problems and provided fixes.\n\u25cf Maintained and performed operational back office processes that included but was not limited to executing batch jobs and reports.\n\u25cf Handled confidential information (including card data) and interacted professionally with internal and external clients, vendors and third-party administrators.\n\u25cf Assisted Product Engineering and IT Operations staff with routine and basic support issues that relate to software applications and utilities.\n\u25cf Performed other duties as assigned or apparent."", u'Data Analyst\nWageWorks - Mequon, WI\nSeptember 2013 to January 2014\n\u25cf Ran and monitored daily job processing including loading inbound data files, as well as processes and delivers outbound data files.\n\u25cf Evaluated and monitored data quality including exception and error handling. Researched inconsistent data loads and addressed issues.\n\u25cf Responded to basic and common technical issues to provide first tier support for data issues. Researched data inconsistencies by modifying and executing SQL queries to identify trends/issues in accordance with established Service Level Agreements (SLAs).']",[u'B.S. in Information Technology'],[u'Marian University\nAugust 2008 to May 2013']
0,https://resumes.indeed.com/resume/944730bb7d1a7529,"[u'Data Analyst\nSeptember 2017 to November 2017\nIdentified sales patterns across different countries, to improve marketing strategy and increase product sales.\nCreated dashboards and views such as map, box plots and aggregation using Tableau.', u'Business Analyst\nSystem Analysis & Design\nMarch 2017 to May 2017\nGathered business requirements, designed UML diagrams using MS Visio and created prototype for the application using Balsamiq.\n\n\u2022 Organizational Analysis Project:', u'Data Analyst\nHealthcare Information Exchange\nSeptember 2016 to November 2016\nWorked with EHR and HIT systems. Used VistA CPRS to review and analyze patient data for clinical decision-making.\n\n\u2022 Business Intelligence:', u""Data Analyst\nPartner's Healthcare\nSeptember 2016 to November 2016\nInterviewed employees at Partner's Healthcare both in-person and online survey, analyzed survey results using PowerPivot to identify prevailing communication issues.\nRecommended strategies for efficient communication system, to increase overall productivity of the team and the company.""]","[u'MS in Information Technology in Information Technology', u'Bachelor of Engineering in Computer Science in Computer Science']","[u'University of Massachusetts Boston Boston, MA\nDecember 2017', u'The Oxford College of Engineering Bengaluru, Karnataka\nMay 2015']"
0,https://resumes.indeed.com/resume/e916ec67a23d7320,"[u'Energy Data Analyst\nHomer Energy\nAugust 2017 to October 2017\n\u2022 Model nine mini-grid development using Homer Optimization Software\n\u2022 Determine the feasibility of using solar and or storage to reduce operational cost\n\u2022 Annual energy usage data from the existing mini-grid and energy demand forecast for all existing grids.', u'Data Analyst\nEuropa-University Flensburg\nFebruary 2015 to August 2015\n\u2022 Assisted in database designing and troubleshooting of database application.\n\u2022 Created extensive and complex SQL queries, T-SQL stored procedures, triggers, views, and jobs.\n\u2022 Created and executed custom data mining models.\n\u2022 Researched and designed customized multidimensional cubes within Microsoft Analysis Services.\n\u2022 Researched and created data warehouse using SQL 2014\n\u2022 Excelled at meeting project deadlines under pressure.', u'Project Manager\nGIZ\nOctober 2011 to December 2013\n\u2022 Developed project planning as well as check point review that facilitated forward Momentum\n\u2022 Developed alternative management model for community owned power and prepare guide line\n\u2022 Conducted value chain analysis of micro hydro power']","[u'', u'', u'Master of Engineering in Energy Management in Energy Management']","[u'MIT (Massachusetts Institute of Technology)\nDecember 2018', u'Bellevue College\nSeptember 2017', u'University Flensburg\nJanuary 2014 to January 2016']"
0,https://resumes.indeed.com/resume/eacbad28f662590c,"[u'QA Data Analyst\nOceanX, LLC - Arden, NC\nMarch 2012 to Present']",[u'Some college'],[u'']
0,https://resumes.indeed.com/resume/b29327c1d3bdfbcd,"[u'Data/Energy Analyst\nEnergy Insight Inc - Chanhassen, MN\nJanuary 2016 to July 2016\no Identified energy conservation opportunities for clients.\no Analysed cost saving, environmental benefits, and energy improvement opportunities.\no Analysed the environmental impacts of energy conservation within a given facility or organization.\no Researched new technologies and processes, government regulations.\no Peer-reviewed other reports to assess accuracy, completeness and conformance to reporting, standard mathematical\nprincipals and procedural and ethical standards.\no Performed Energy star benchmarking and certification for the office in Chanhassen.', u""Data Analyst\nECom Consulting Inc - Plano, TX\nJanuary 2015 to October 2015\no Prepared reports in Tableau by creating relationships, actions, data blending, filters and parameters, hierarchies,\ncalculated fields, sorting and groupings, to create detailed summary reports and dashboard using KPI's.\no Process Mapping: Mapping the overall process using MS Visio to identify essential areas of data collection and investigation.\no Used SQL reporting skills to create SQL views and write SQL queries.\no Data Collection: End-to-End collection of experimental and observational data.\no Data Management: Built datasets and documented the description of the fields into a data dictionary.\no Statistical Methodology: Selected appropriate hypothesis tests to derive insights and confirm the conjectures.\no Data Analysis: Used R to read, explore, plot and analyse the collected data. Applied my knowledge of hypothesis\ntests using R base stats package and statistical functions.\no Data visualization and Insight generation: Built Tableau dashboards to enable in-depth visualization of the experimental data. Prepared white papers to document and circulate the process followed and findings derived.""]","[u'Master of Science in Data Science', u'Master of Science in Environmental Engineering', u'Bachelor of Engineering in Polymer Engineering']","[u'Worcester Polytechnic Institute\nDecember 2018', u'University of Maryland Baltimore, MD\nDecember 2014', u'Maharashtra Institute of Technology Pune, Maharashtra\nAugust 2011']"
0,https://resumes.indeed.com/resume/0881a1219f302e1b,"[u'Data Analyst II\nClayton Homes\nApril 2014 to Present\nData Analyst with experience in SSRS, Cognos and Tableau. Gathering business requirements from multiple business units and develop reports to help identify business needs. Helping test and report out of our SQL Server Data Warehouse.', u'BI Analyst\nRuby Tuesday\nApril 2000 to April 2014\nBI Analyst with experience in report development in SSRS. Reporting out of SQL Server, Oracle and Teradata.']",[u'Associate'],[u'HACC']
0,https://resumes.indeed.com/resume/e03c27bdd369eef5,[u'data analyst\nAs a data analyst i worked in the address research and verification department of the enrollment correction process for medicare part D. I also became an Integrated Management Systems coordinator.'],[u'High school or equivalent in computer science mathamatics'],"[u'American school of correspondence New Haven, CT\nJuly 2014 to October 2016']"
0,https://resumes.indeed.com/resume/753960f2c0a56ae5,"[u'Data Analyst\nGRAPE INFOTECH - Ahmedabad, Gujarat\nJune 2015 to November 2015\nAnalyze and preparing data for better understanding, developed visualizations using Tableau and matplotlib for better\nunderstanding of data, performed data cleaning, normalization, data transformation\n\u2022 Used predictive modelling tools like R and Python to build model and used machine learning techniques under the guidance of senior Data Scientist\n\u2022 Developed a predictive model and validate KNN and Decision Tree model to predict the feature model\n\u2022 To improve efficiency of the model, boosting methods were used on the model\n\u2022 Presented Dashboards to Higher Management for more Insights using Power BI', u'Data Science Intern\nGRAPE INFOTECH - Ahmedabad, Gujarat\nDecember 2014 to May 2015\n\u2022 Data Visualization: Visualized prices of car using Matplotlib and Tableau, MS Excel (Pivot tables, VLOOKUP).\n\u2022 Pre-processing: Processed data accordingly to the need of the company using Pandas library of Python.']","[u'Masters of Science in Computer Science', u'']","[u'GUJARAT TECHNOLOGICAL UNIVERSITY Gujarat, IN\nJanuary 2016 to March 2018', u'DEPAUL UNIVERSITY Chicago, IL\nMay 2017 to October 2017']"
0,https://resumes.indeed.com/resume/29a4d934ecab41bc,"[u'Data Analyst\nFairrington Transportation - Bolingbrook, IL\nMarch 2010 to March 2018\nUsing Satori and Windowbook post presort software, analyze and report data using mail.dat\n\u25cf Creation and upkeep of all the standard work for the department\n\u25cf Creation and implementation of process improvement\n\u25cf Creation and upkeep of department training manual\n\u25cf Using company portal, post mail.dat files for USPS release\n\u25cf Testing of all software upgrades\n\u25cf Troubleshooting and resolution of software problems, communicating with software\ncontacts\n\u25cf Create and maintain daily assigned duties for the department\n\u25cf Customer Service', u'Estimator\nUniversal Laminating & Converting - Bolingbrook, IL\nMarch 2000 to August 2007\nCreated quotes for finishing company in graphic arts industry\n\u25cf Dealt with printing customers and vendors such as die shops\n\u25cf Participated in meetings with printers to go over projects and designs']","[u'Associates degree', u'associates degree in Liberal Arts in Liberal Arts']","[u'College of Dupage Glen Ellyn, IL\nSeptember 1996 to May 2015', u'I started college\nJanuary 1996']"
0,https://resumes.indeed.com/resume/ae1a5a4a5bbf2789,"[u'Data Scientist\nTransUnion - Chicago, IL\nJune 2017 to Present\nDescription: TransUnion is a consumer credit reporting agency. TransUnion mission is to help people around the world and organizations in optimizing their risk-based decisions and enabling consumers to understand and manage their personal information.\nResponsibilities:\n\u2022 Built data pipelines for reporting, alerting, and data mining. Experienced with table design and data management using HDFS, Hive, Impala, Sqoop, MySQL, Mem SQL, Grafana/Influx DB, and Kafka.\n\u2022 Worked with statistical models for data analysis, predictive modelling, machine learning approaches and recommendation and optimization algorithms.\n\u2022 Working in Business and Data Analysis, Data Profiling, Data Migration, Data Integration and Metadata Management Services.\n\u2022 Worked extensively on Databases preferably Oracle 11g/12c and writing PL/SQL scripts for multiple purposes.\n\u2022 Built models using Statistical techniques like Bayesian HMM and MachineLearning classification models like XGBoost, SVM, and Random Forest using R and Python packages.\n\u2022 Worked with data compliance teams, data governance team to maintain data models, Metadata, data Dictionaries, define source fields and its definitions.\n\u2022 Worked with BigData Technologies such Hadoop, Hive, MapReduce\n\u2022 Developed MapReduce/Spark Python modules for machine learning & predictive analytics in Hadoop on AWS. Implemented a Python-based distributed random forest via Python streaming.\n\u2022 Setup storage and data analysis tools in Amazon Web Services cloud computing infrastructure.\n\u2022 A highly immersive Data Science program involving Data Manipulation & Visualization, Web Scraping, Machine Learning, Python programming, SQL, GIT, Unix Commands, NoSQL, MongoDB, Hadoop.\n\u2022 Performed scoring and financial forecasting for collection priorities using Python, R and SAS machine learning algorithms.\n\u2022 Handled importing data from various data sources, performed transformations using Hive, MapReduce, and loaded data into HDFS\n\u2022 Managed existing team members, lead the recruiting and on boarding of a larger Data Science team that addresses analytical knowledge requirements.\n\u2022 Worked directly with upper executives to define requirements of scoring models.\n\u2022 Developed a model for predicting repayment of debt owed to small and medium enterprise (SME) businesses.\n\u2022 Developed a generic model for predicting repayment of debt owed in the healthcare, large commercial, and government sectors.\n\u2022 Created SQLscripts and analyzed the data in MS Access/Excel and Worked on SQL and SAS script mapping.\n\u2022 Developed a legal model for predicting which debtors respond to litigation only.\n\u2022 Created multiple dynamic scoring strategies for adjusting the score upon consumer behaviour such as payment or right-party phone call.\n\u2022 Rapid model creation in Python using pandas, NumPy, sklearn, and plot.ly for data visualization. These models are then implemented in SAS where they are interfaced with MSSQL databases and scheduled to update on a timely basis.\n\u2022 Data analysis using regressions, data cleaning, excel v-look up, histograms and TOAD client and data representation of the analysis and suggested solutions for investors\n\u2022 Attained good knowledge in Hadoop Data Lake Implementation and HADOOP Architecture for client business data management.\n\u2022 Identifying relevant key performing factors; testing their statistical significance\n\u2022 Above scoring models resulted in millions of dollars of added revenue to the company and a change in priorities of the entire company.\nEnvironment:-R, SQL, Python 2.7.x, SQL Server 2014, regression, logistic regression, random forest, neural networks, Topic Modeling, NLTK, SVM (Support Vector Machine), JSON, XML, HIVE, HADOOP, PIG, Sklearn, SciPy, GraphLab, No SQL, SAS, SPSS, Spark, Hadoop, Kafka, HBase, MLib.', u'Data Scientist\nCiti - Irving, TX\nMarch 2016 to May 2017\nDescription: -Citi provide consumers, corporations, governments and institutions with a broad range of financial services and products. It strives to create the best outcomes for clients and customers with financial ingenuity that leads to simple, creative and responsible solutions.\nResponsibilities:\n\u2022 Utilize a broad variety of statistical packages like SAS, R, MLIB, Graphs, Hadoop, Spark, MapReduce, Pig and others\n\u2022 Refine and train models based on domain knowledge and customer business objectives\n\u2022 Deliver or collaborate on delivering effective visualizations to support the client business objectives\n\u2022 Extensive understanding of the BI and analytics space with special focus on the consumer and customer space\n\u2022 Converted time lag problems in order fulfillment into Data mining tasks\n\u2022 Performed Data Profiling to assess data quality using SQL through complex internal database\n\u2022 Improved sales and logistic data quality by data cleaning using NumPy, SciPy, Pandas in Python\n\u2022 Built Data warehouse to support end-user queries with Oracle and MS Visual Studio\n\u2022 Designed and implemented Dimensional DataModeling for order fulfillment process\n\u2022 Deployed SSIS packages to complete ETL and DataMapping process\n\u2022 Transformed data through methods like Aggregation, Slowly Changing Dimension, Splitting\n\u2022 Derived business intelligence report for order fulfilment using MS SSAS and SSRS\n\u2022 Determined regression model predictors using Correlation matrix for Factor analysis in R\n\u2022 Built Regression model to understand order fulfilment time lag issue using Scikit-learn in Python\n\u2022 Optimized predictive model by reducing insignificant variables using Stepwise Regression\n\u2022 Empowered decision makers with data analysis dashboards using Tableau and Power BI\n\u2022 Interface with other technology teams to extract, transform, and load (ETL) data from a wide variety of data sources\n\u2022 Own the functional and non-functional scaling of software systems in your ownership area.\n\u2022 Provides input and recommendations on technical issues to BIEngineers, Business&DataAnalysts and Data Scientists.\n\u2022 Outstanding analytical and problem solving skills are essential.\nEnvironment:- Python, Hive, C/C++, C# , Java or Python, Bash, HTML5, PERL, Processing, Python and J Query, SOAPUI, WCF, WPF, VSO, TFS, GIT, XML, XSD, SQL Server 2008, Oracle 10/11g, ANGULAR JS.', u'Data Scientist\nDPSG - Plano, TX\nNovember 2014 to February 2016\nDescription:- On May 7, 2008, DPS became a stand-alone, publicly traded company on the New York Stock Exchange as the result of a spin-off by Cadbury, plc, which held the Cadbury Schweppes Americas Beverages business group of entities.\nResponsibilities:\n\u2022 Supported MapReduce Programs running on the cluster.\n\u2022 Evaluated business requirements and prepared detailed specifications that follow project guidelines required to develop written programs.\n\u2022 Configured Hadoop cluster with Name node and slaves and formatted HDFS.\n\u2022 Used Oozie workflow engine to run multiple Hive and Pig jobs.\n\u2022 Performed MapReduce Programs those are running on the cluster.\n\u2022 Developed multiple MapReduce jobs in java for data cleaning and pre-processing.\n\u2022 Analyzed the partitioned and bucketed data and compute various metrics for reporting.\n\u2022 Involved in loading data from RDBMS and web logs into HDFS using Sqoop and Flume.\n\u2022 Worked on loading the data from MySQL to HBase where necessary using Sqoop.\n\u2022 Developed Hive queries for Analysis across different banners.\n\u2022 Extracted data from Twitter using Java and Twitter API.Parsed JSON formatted twitter data and uploaded to database.\n\u2022 Launching AmazonEC2 Cloud Instances using Amazon Images (Linux/ Ubuntu) and Configuring launched instances with respect to specific applications.\n\u2022 Exported the result set from Hive to MySQL using Sqoop after processing the data.\n\u2022 Analyzed the data by performing Hive queries and running Pigscripts to study customer behavior.\n\u2022 Have hands on experience working on Sequence files, AVRO, HAR file formats and compression.\n\u2022 Used Hive to partition and bucketdata.\n\u2022 Experience in writing MapReduce programs with JavaAPI to cleanse Structured and unstructured data.\n\u2022 Wrote Pig Scripts to perform ETL procedures on the data in HDFS.\n\u2022 Created HBase tables to store various data formats of data coming from different portfolios.\n\u2022 Worked on improving performance of existing Pig and Hive Queries.\nEnvironment: -SQL/Server, Oracle 9i, MS-Office, Teradata, Informatica, ER Studio, XML, Business Objects.', u'Data Analyst/Modeler\nCitrix Systems - Fort Lauderdale, FL\nMarch 2013 to October 2014\nDescription: - Citrix Systems, Inc. is an American multinational software company that provides server, application and desktop virtualization, networking, software as a service, and cloud computing technologies.\nResponsibilities:\n\u2022 Participated in JAD sessions, gathered information from Business Analysts, end users and other stakeholders to determine the requirements.\n\u2022 Developed the logical data models and physical data models that confine existing condition/potential status data fundamentals and data flows using ER Studio.\n\u2022 Created Data warehousing methodologies/Dimensional Data modelling techniques such as Star/Snowflake schema using ERWIN9.1.\n\u2022 Extensively used Aginity Netezza workbench to perform various DDL, DML etc. operations on Netezza database.\n\u2022 Designed the Data Warehouse and MDM hub Conceptual, Logical and Physical data models.\n\u2022 Performed Daily Monitoring of Oracle instances using Oracle Enterprise Manager, ADDM, TOAD, monitor users, table spaces, memory structures, rollback segments, logs and alerts.\n\u2022 Involved in Teradata SQL Development, Unit Testing and Performance Tuning and to ensure testing issues are resolved on the basis of using defect reports.\n\u2022 Customized reports using SAS/MACRO facility, PROC REPORT, PROC TABULATE and PROC.\n\u2022 Translate business and data requirements into Logical data models in support of Enterprise DataModels, ODS, OLAP, OLTP, Operational Data Structures and Analytical systems.\n\u2022 Worked on database testing, wrote complex SQL queries to verify the transactions and business logic like identifying the duplicate rows by using SQL Developer and PL/SQL Developer.\n\u2022 Used Teradata SQL Assistant, Teradata Administrator, PMONand data load/export utilities like BTEQ, FastLoad, Multi Load, Fast Export, TPumpon UNIX/Windows environments and running the batch process for Teradata.\n\u2022 Worked on data profiling and data validation to ensure the accuracy of the data between the warehouse and source systems.\n\u2022 Hands on Data warehouse concepts like Data warehouse Architecture, Star schema, Snowflake schema, and Data Marts, Dimension and Fact tables.\n\u2022 Developed SQL Queries to fetch complex data from different tables in remote databases using joins, database links and Bulk collects.\n\u2022 Migrated database from legacy systems, SQL server to Oracle and Netezza.\n\u2022 Reviewed the logical model with application developers, ETL Team, DBAs and testing team to provide information about the data model and business requirements.\n\u2022 Worked on SQL Server concepts SSIS (SQL Server Integration Services), SSAS (Analysis Services) and SSRS (Reporting Services).\nEnvironment:-ER Studio, Teradata13.1, SQL, PL/SQL, BTEQ, DB2, Oracle, MDM, Netezza, ETL, RTF UNIX, SQL Server2010, Informatica, SSRS, SSIS, SSAS, SAS, Aginity.', u'Data Analyst/Data Modeler\nInnovaInfotech - Bengaluru, Karnataka\nOctober 2011 to February 2013\nDescription: - SYS INNOVA Infotech is an offshore software services and IT consulting company based in Bangalore, India. As a committed outsourcing partner and an IT vendor, our goal is to ensure cost effective, technical excellence and on-time deliveries. While we take care of their end-to-end programming and consulting needs, our clients focus on core business activities which correlate directly to their revenues and profitability. Strategic partnership with us gives our clients the access to latest technology, skilled manpower and scalable team which ultimately results in lower risk and higher ROI.\nResponsibilities:\n\u2022 Worked with project team representatives to ensure that logical and physical ER/Studio data models were developed in line with corporate standards and guidelines.\n\u2022 Involved in defining the source to target data mappings, business rules, data definitions.\n\u2022 Worked with BTEQ to submit SQL statements, import and export data, and generate reports in Teradata.\n\u2022 Responsible for defining the key identifiers for each mapping/interface.\n\u2022 Responsible for defining the functional requirement documents for each source to target interface.\n\u2022 Document the complete process flow to describe program development, logic, testing, and implementation, application integration, coding.\n\u2022 Involved in defining the business/transformation rules applied for sales and service data.\n\u2022 Define the list codes and code conversions between the source systems and the data mart.\n\u2022 Worked with internal architects and, assisting in the development of current and target state data architectures.\n\u2022 Coordinate with the business users in providing appropriate, effective and efficient way to design the new reporting needs based on the user with the existing functionality.\n\u2022 Remain knowledgeable in all areas of business operations in order to identify systems needs and requirements.\n\u2022 Responsible for defining the key identifiers for each mapping/interface.\n\u2022 Implementation of Metadata Repository, Maintaining Data Quality, Data Cleanup procedures, Transformations, Data Standards, Data Governance program, Scripts, Stored Procedures, triggers and execution of test plans\n\u2022 Performed data quality in Talend Open Studio.\n\u2022 Coordinated meetings with vendors to define requirements and system interaction agreement documentation between client and vendor system.\n\u2022 Enterprise Metadata Library with any changes or updates.\n\u2022 Generate weekly and monthly asset inventory reports.\nEnvironment:-Erwin r7.0, SQL Server 2000/2005, Windows XP/NT/2000, Oracle 8i/9i, MS-DTS, UML, UAT, SQL Loader, OOD, OLTP, PL/SQL, MS Visio, Informatica.', u'Data Analyst\nStyles You, Amtex Software Solutions Pvt Ltd - Chennai, Tamil Nadu\nMarch 2009 to September 2011\nDescription:- Amtex provides high-quality end-to-end software solutions across industries through unique models and methodologies, to deliver time, cost, quality and full-service advantages best-of breed in nature. Amtex assists clients in making informed business decisions through high-impact insight, advice, and research. An international network of industry experts enables us to carry out global and country-specific projects.\nResponsibilities:\n\u2022 Analyze business information requirements and model class diagrams and/or conceptual domain models.\n\u2022 Gather & Review Customer Information Requirements for OLAP and building the data mart.\n\u2022 Performed document analysis involving creation of Use Cases and Use Case narrations using Microsoft Visio, in order to present the efficiency of the gathered requirements.\n\u2022 Calculated and analyzed claims data for provider incentive and supplemental benefit analysis using Microsoft Access and Oracle SQL.\n\u2022 Analyzed business process workflows and assisted in the development of ETL procedures for mapping data from source to target systems.\n\u2022 Worked with BTEQ to submit SQL statements, import and export data, and generate reports in Terra-data.\n\u2022 Responsible for defining the key identifiers for each mapping/interface\n\u2022 Responsible for defining the functional requirement documents for each source to target interface.\n\u2022 Coordinated meetings with vendors to define requirements and system interaction agreement documentation between client and vendor system.\n\u2022 Enterprise Metadata Library with any changes or updates.\n\u2022 Document data quality and traceability documents for each source interface.\n\u2022 Establish standards of procedures.\n\u2022 Generate weekly and monthly asset inventory reports.\n\u2022 Managed the project requirements, documents and use cases by IBM Rational RequisitePro.\n\u2022 Assisted in building an Integrated LogicalDataDesign, propose physical database design for building the data mart.\n\u2022 Document all data mapping and transformation processes in the Functional Design documents based on the business requirements.\nEnvironment:-SQL Server 2008R2/2005 Enterprise, SSRS, SSIS, Crystal Reports, Windows Enterprise Server 2000, DTS, SQL Profiler, and Query Analyzer.']",[],[]
0,https://resumes.indeed.com/resume/d939337a4f3df60b,"[u'Data Analyst\nUnited Nations - New York, NY\nNovember 2016 to April 2017\n- Used Natural Language Processing (NLP) to reveal nation plans\u2019 alignment with sustainable development goals\n- Built a predictive model using TF-IDF Ranking, Cosine Similarity, and Topic Modelling in Python\n- Provided automated solutions by designing user interface (UI) in Python', u'Data Scientist\nVanke - Shanghai Area\nJuly 2015 to July 2016\nMonitored company performance, conducted financial modeling and optimized business process\n- Shortened project development cycle by 5% by leveraging product development data\n- Slashed design cost of one project 3%, by analyzing cost data, shortening time by 16% before product release\n- Retrieved transaction data, conducted quantitative analysis and set quantitative goals for new investments\n- Visualized performance data in Tableau developing key performance indicators (KPIs) and 3-year strategy\n- Generated weekly dashboards and presented business performance summary and forecast to executives\n\nIntegrated databases, troubleshot system issues, and improved streamline process and data flow\n- Integrated financial, procurement and production databases, and launched beta database within project timeline\n- Identified business requirements and end-to-end process flows; proposed creative process solutions\n- Organized end-user training, liaised between engineering team and clients and conducted system testing\n- Presented project progress to key stakeholders and obtained support for project execution', u'Data Analyst\nVanke - Shanghai Area\nJuly 2013 to June 2015\nConducted quantitative analyses, presented business patterns and strategic insights to Sales and Investment\n- Manipulated expense data and optimized multi-channel ads, cut expenditure by 10%, raising price by 7%\n- Used Tableau dashboard to track sales progress and manage sales processing priorities\n- Queried, integrated and processed data from Sales, Marketing and Investment teams with SQL\n\nInterpreted business insights to Strategy, Design and Investment\n- Executed analytical projects, implemented data cleaning with SQL and pinpointed usage trends\n- Investigated root causes of sales decline by analyzing data and sales process with key stakeholders']","[u""Master's in Business Analytics"", u""Master's in Management Science and Engineering"", u""Bachelor's in Project Management""]","[u'Fordham University New York, NY\nSeptember 2016 to December 2017', u'Harbin Institute of Technology Harbin\nSeptember 2011 to July 2013', u""Northwestern Polytechnical University Xi'an\nSeptember 2007 to July 2011""]"
0,https://resumes.indeed.com/resume/7e741f4094695824,"[u'DATA ANALYST\nACTUARIAL SYSTEMS, INC\nAugust 2004 to December 2014\n\u2022 Data administration for qualified retirement plans. Excel input for valuations and benefit calculations\n\u2022 Excel program to prepare benefit calculation summaries; Dbase IV data input\nPreparation of IRS plan compliance forms for review by management.']","[u'BS in INFORMATION TECHNOLOGY', u'BACHELOR in ARTS BUSINESS ADMINISTRATION']","[u'NORTHERN ARIZONA UNIVERSITY\nJune 2019', u'FLAGLER COLLEGE\nJune 2017']"
0,https://resumes.indeed.com/resume/fa5c402eaf426f93,"[u'Data Analyst\nVerizon - Southlake, TX\nJanuary 2016 to February 2018\n\u2022 Gather, analyze, and document data requirements for projects of medium to high complexity.\n\u2022 Participate in the analysis of client business processes and functional or reporting requirements.\n\u2022 Created new database objects like Tables, Procedures, Functions, Indexes and Views\n\u2022 Conducted data modeling review sessions for different user groups, participated in requirement sessions to identify requirement feasibility.\n\u2022 Participate in cross-functional task forces to identify and document functional or reporting Processes and Requirements\n\u2022 Performed numerous data manipulation language or queries using SQL for analysis.\n\u2022 Extracted data from existing data stores, Developing and executing departmental reports for performance and response purposes by using SQL Server Management Studio, MS Access, MS Excel', u'Data Analyst\nVodafone Telecom Management Practice, Inc - New York, NY\nDecember 2013 to January 2015\n\u2022 Analyzed invoice data against carrier contracts to alert of any discrepancies using Excel, and SQL\n\u2022 Completed contract builds according to client-approved SLAs\n\u2022 As sole Contract data Analyst on major projects, managed logistics of Contracts tasks to ensure compliance with overall project management plan\n\u2022 Worked with Quality Control to ensure quality standards were met\n\u2022 Assisted with process enhancements of contract builds\n\u2022 Lead role in creating/enhancing internal training documentation for continual training of team and new hires\n\u2022 Created ETL solution to read Product, Vendor, and Order data from CSV files on shared network into SQL Server database. Consequently validated incoming data and performed notifications when jobs were complete. Loaded order data into SQL Server to generate SSRS reports, which were set up for automatic delivery based on standard report subscriptions', u'Business/Data Analyst\nJohnson & Johnson Healthcare System - New Brunswick, NJ\nJune 2010 to June 2012\nResponsibilities:\n\u2022 Provided data architecture and modeling to ensure that data within the Claims Management System (CMR) was in line with compliance guidelines.\n\u2022 Perform data mapping for the process\n\u2022 Inserted checkpoints in Oracle database to check the functionality and data integrity of the AUT.\n\u2022 Performed ETL data loads from flat files, DB2, Teradata, PL/SQL etc. to transform data into complex scripts and codes for software application testing\n\u2022 Enhanced the PL/SQL test scripts for data driven tests\n\u2022 Developed and executed automated test case scripts Test for the Front-end applications\n\u2022 Converted user requirements into business requirements, analyzed the same, and segregated them into high level and low-level business requirements']",[u'BS in Technical Management in Technical Management'],[u'DeVry University']
0,https://resumes.indeed.com/resume/e6848df0cfe6e637,"[u""Business Analyst\nDecember 2014 to December 2015\n1 year)\n\u2022Assisted in planning and designing business processes to improve and support business activities\n\u2022Assisted in analyzing and documenting client's business requirements and processes; communicated these requirements\nto technical personnel by constructing basic conceptual data and process models.\n\u2022In-depth knowledge and practical experience of using Waterfall, Agile, Scrums, Rational Unified Process, Business Process\nRe-engineering, Unified Modeling Language, System Requirements Specification and Functional Requirements\nSpecification, Rapid Application Development, and MS Visio.\n\u2022Conducted Scope Analysis, Stakeholder Analysis, Gap Analysis, and Traceability Matrix and used Visio for Business\nProcess Modelling adopting UML standards.\n\u2022Conducted meeting with ETL Developers, Data Modelers, DBA and Business Users for understanding the data mappings\nfor the conversion process."", u'Data Analyst Intern\nMay 2013 to December 2013\n\u2022Manipulating, cleansing& processing data using Excel and SQL.\n\u2022Addressed data discrepancy issues across all open projects to the management.\n\u2022Worked with the analytics team to build, maintain, and populate data warehouses']","[u'Master of science in Management Information systems', u'Bachelor of Engineering in Information Technology']","[u'University of Illinois Springfield, IL\nJanuary 2018', u'Osmania University, HYD\nMay 2015']"
0,https://resumes.indeed.com/resume/6978fff413b5c01c,"[u""Consultant- Data Analyst\nWells Fargo Bank NA - Charlotte, NC\nJune 2014 to June 2016\nAs a consultant for Wells Fargo's Home Mortgage marketing and analytics group I was responsible for data analysis and reporting for portfolio loans and customer segmentation using SAS on various databases (Oracle, Teradata and MS SQL Server).\n\nMy primary responsibilities were developing efficient SAS and SQL code for Customer segmentation and profiling.\nMarketing Campaign execution for loan modification, recapture and refinance offers.\nCampaign effectiveness measurement.\nAutomation of various manual process in campaign execution.\nTesting the new Campaign Execution Framework on various channels.\nScheduling complex jobs on Unix with advanced shell scripting."", u""Consultant- Data Analyst\nBank of America - Charlotte, NC\nNovember 2013 to February 2014\nAs a consultant for Bank of America's Global Information Security group I was responsible for data analysis and reporting for enterprise security anomalies using SAS on various databases (Teradata and MS SQL Server).\n\nMy responsibilities were developing efficient SAS and SQL code for Extracting enterprise risk data from various data bases and applying transformations to clean the data.\nDeveloping highly scalable code for implementing information security audit algorithms based on business requirements.\nAnalyzing and reporting the anomalies.\nCreating shell scripts to that streamline and automate the execution of SAS code for end to end processing.\nData manipulation using Excel VBA macros."", u'Consultant-Data Analyst\nRisk Technology Solutions - Charlotte, NC\nJanuary 2012 to November 2013\nAs a consultant at Risk Technology Solutions I was responsible for stress testing of traded products under varying severity of economic scenarios for a large European investment bank using SAS and SQL on various databases.\n\nI was primarily involved in entire software development lifecycle starting from gathering business requirements and creating technical specks to product deployment and user acceptance testing. Software solution was successfully automated using Unix shell scripting and implemented all the business requirements using complex SQL and efficient SAS code.\n\nI have worked on generating monthly and daily reports for Counterparties with multiple scenarios which involves Stress Testing, Back Testing, Wrong way Risk, Capital.']","[u'M S (Computer Science in Database systems', u'B Tech in Electronics & Communications Engineering']","[u'University of North Carolina at Charlotte Charlotte, NC\nJanuary 2011', u'Jawaharlal Nehru Technical University\nJanuary 2005']"
0,https://resumes.indeed.com/resume/5b19ec16df86a90e,"[u'DATA ANALYST\nGENPACT India\nJanuary 2012 to February 2016\n\u2022 Worked on specific finance reports Solvency, Credit Risk Model, Current Exposure Summary, Limit Breaches Report, Traders Report and Trade reconciliation for GE TREASURY, USA.\n\u2022 Involved in collecting/extracting data from different sources and processing the data for analysis.\n\u2022 Supported in analyzing and creating daily, weekly, monthly, and quarterly reports based on the requirements and delivered reports to the client.\n\u2022 Designed and maintained counterparty master database system and involved in writing SQL queries to pull the data for analysis.\n\u2022 Implemented Python scripts to extract data from CSV files and other data sources to prepare the reports.\n\u2022 Have used pivot table, charts extensively for presenting various trend analysis in the reports using Excel.\n\u2022 Designed and developed custom reports and KPI Dashboards using Tableau.\nEnvironment: VBA, Excel 2013, Python (Pandas, NumPy), Tableau 9.1, Oracle SQL', u'BUSINESS ANALYST\nGENPACT India\nDecember 2009 to December 2012\n\u2022 Involved in automation of the Liquidity, JPM Collateral, PQR Summary and Amortization Schedule check Reports for GE Treasury and Trinity, USA.\n\u2022 Efficiently used excel formulas, pivot tables to search and organize data.\n\u2022 Consolidated data from the several sources and verified the accuracy of data.\n\u2022 Optimized the reports to save manual hours spent in generating reports and helped in saving 2 FTE manual hours.\n\u2022 Supported in ad-hoc analysis of the reports and delivery of the reports to clients.\n\u2022 Developed a tool to pull data from database and many other external systems and manipulated a data on power point pitch using VBA excel.\nEnvironment: VBA, Excel 2007, Access 2007, MySQL, PowerPoint 2007,', u'ANALYST\nGENPACT India - Bengaluru, Karnataka\nJanuary 2007 to November 2009\n\u2022 Automated Technologist Report for SABIC IP USA and Concept of Hierarchy for Sales Tool for GE Fleet Services UK.\n\u2022 Implemented the coding logic to pull data from database.\n\u2022 Automated power point data modifications using VBA excel code.\n\u2022 Implemented java web based application for technologist report.\n\u2022 Involved in the Report and dashboard preparation work for clients.\nEnvironment: VBA, Excel 2003, Access 2003, PowerPoint 2003, SQL Server, Core Java, JSP, JavaScript, HTML, CSS.']","[u'Master of Computer Application in Computer Application', u'B.Sc. in Computer Science']","[u'University of madras\nJune 2001 to May 2004', u'University of madras\nJune 1998 to May 2001']"
0,https://resumes.indeed.com/resume/500e0b8e640a2837,"[u'Data Scientist\nSynnex Corporation - Fermont, CA\nFebruary 2017 to Present\nResponsibilities:\n\u2022 Utilized Apache Spark with Python to develop and execute Big Data Analytics and Machine learning applications, executed machine Learning use cases under Spark ML and Mllib.\n\u2022 Identified areas of improvement in existing business by unearthing insights by analyzing vast amount of data using machine learning techniques.\n\u2022 Interpret problems and provides solutions to business problems using data analysis, data mining, optimization tools, and machine learning techniques and statistics.\n\u2022 Designed and developed NLP models for sentiment analysis.\n\u2022 Led discussions with users to gather business processes requirements and data requirements to develop a variety of Conceptual, Logical and Physical Data Models. Expert in Business Intelligence and Data Visualization tools: Tableau, Microstrategy.\n\u2022 Worked on machine learning on large size data using Spark and MapReduce.\n\u2022 Let the implementation of new statistical algorithms and operators on Hadoop and SQL platforms and utilized optimizations techniques, linear regressions, K-means clustering, Native Bayes and other approaches.\n\u2022 Developed Spark/Scala, Python for regular expression (regex) project in the Hadoop/Hive environment with Linux/Windows for big data resources.\n\u2022 Data sources are extracted, transformed and loaded to generate CSV data files with Python programming and SQL queries.\n\u2022 Stored and retrieved data from data-warehouses using Amazon Redshift.\n\u2022 Worked on TeradataSQL queries, Teradata Indexes, Utilities such as Mload, Tpump, Fast load and FastExport.\n\u2022 Used Data Warehousing Concepts like Ralph Kimball Methodology, Bill Inmon Methodology, OLAP, OLTP, Star Schema, Snow Flake Schema, Fact Table and Dimension Table.\n\u2022 Refined time-series data and validated mathematical models using analytical tools like R and SPSS to reduce forecasting errors.\n\u2022 Worked on data pre-processing and cleaning the data to perform feature engineering and performed data imputation techniques for the missing values in the dataset using Python.\n\u2022 Created Data Quality Scripts using SQL and Hive to validate successful dasta load and quality of the data. Created various types of data visualizations using Python and Tableau.\n\nEnvironment: Hadoop, Map Reduce, Spark, Spark MLLib, Tableau, SQL, Excel, VBA, SAS, Matlab, AWS, SPSS, Cassandra, Oracle, MongoDB, SQL Server 2012, DB2, T-SQL, PL/SQL, XML, Tableau.', u'Data Scientist\nFirst Atlantic Bank - Jacksonville, FL\nAugust 2014 to January 2017\nDescription: First Atlantic Bank gives subsidizes or advances to individuals with little business prerequisites. Candidates get their advances authorized taking into account their record of loan repayment. The candidate data is kept up in a database alongside the points of interest of the advance for reimbursement. This information is filtered into diverse classifications in light of parameters like kind of record, advance sum, due date. The filtered information is utilized for insights for producing report.\n\nResponsibilities:\n\u2022 Collaborates with cross-functional team in support of business case development and identifying modeling method (s) to provide business solutions. Determines the appropriate statistical and analytical methodologies to solve business problems within specific areas of expertise.\n\u2022 Generating Data Models using Erwin9.6 and developed relational database system and involved in Logical modeling using the Dimensional Modeling techniques such as Star Schema and Snow Flake Schema.\n\u2022 Guide the full lifecycle of a Hadoop solution, including requirements analysis, platform selection, technical architecture design, application design and development, testing, and deployment\n\u2022 Consult on broad areas including data science, spatial econometrics, machine learning, information technology and systems and economic policy with R\n\u2022 Performed Datamapping between source systems to Target systems, logicaldata modeling, created classdiagrams and ERdiagrams and used SQLqueries to filter data\n\u2022 Enabled speedy reviews and first mover advantages by using Oozie to automate data loading into the Hadoop Distributed File System and PIG to pre-process the data.\n\u2022 Used various techniques using R data structures to get the data in right format to be analyzed which is later used by other internal applications to calculate the thresholds.\n\u2022 Maintaining conceptual, logical and physical data models along with corresponding metadata.\n\u2022 Done data migration from an RDBMS to a NoSQL database, and gives the whole picture for data deployed in various data systems.\n\u2022 Developed triggers, stored procedures, functions and packages using cursors and ref cursor concepts associated with the project using PLSQL\n\u2022 Used Meta data tool for importing metadata from repository, new job categories and creating new data elements.\n\nEnvironment: R, Oracle 12c, MS-SQL Server, Hive, NoSQL, PL/SQL, MS- Visio, Informatica, T-SQL, SQL, Crystal Reports 2008, Java, SPSS, SAS, Tableau, Excel, HDFS, PIG, SSRS, SSIS, Metadata.', u'Data Scientist/Data Analyst\nAssurant Specialty Property - Santa Ana, CA\nMay 2013 to July 2014\nDescription: Assurant partners with leaders in mortgage lending, manufactured housing, multifamily housing and other industries to protect client and consumer property.\n\nResponsibilities:\n\u2022 Worked on data cleaning and reshaping, generated segmented subsets using Numpy and Pandas in Python\n\u2022 Wrote and optimized complex SQL queries involving multiple joins and advanced analytical functions to perform data extraction and merging from large volumes of historical data stored in Oracle 11g, validating the ETL processed data in target database\n\u2022 Identified the variables that significantly affect the target\n\u2022 Continuously collected business requirements during the whole project life cycle.\n\u2022 Conducted model optimization and comparison using stepwise function based on AIC value\n\u2022 Applied various machine learning algorithms and statistical modeling like decision tree, logistic regression, Gradient Boosting Machine to build predictive model using scikit-learn package in Python\n\u2022 Developed Python scripts to automate data sampling process. Ensured the data integrity by checking for completeness, duplication, accuracy, and consistency\n\u2022 Generated data analysis reports using Matplotlib, Tableau, successfully delivered and presented the results for C-level decision makers\n\u2022 Generated cost-benefit analysis to quantify the model implementation comparing with the former situation\n\u2022 Worked on model selection based on confusion matrices, minimized the Type II error\n\nEnvironment: Tableau 7, Python 2.6.8, Numpy, Pandas, Matplotlib, Scikit-Learn, MongoDB, Oracle 10g, SQL', u""Data Analyst/Data Modeler\nNestle - IN\nJanuary 2013 to June 2014\nDescription: The Nestle is a Swiss transnational food and drink company. Nestle's products include baby food, medical food, bottled water, breakfast cereals, coffee and tea, confectionery, dairy products, ice cream, frozen food, pet foods, and snacks.\n\nResponsibilities:\n\u2022 Created and maintained Logical and Physicalmodels for the data mart. Created partitions and indexes for the tables in the datamart.\n\u2022 Performed data profiling and analysis applied various data cleansing rules designed data standards and architecture/designed the relational models.\n\u2022 Developed SQLscripts for creating tables, Sequences, Triggers, views and materializedviews\n\u2022 Worked on query optimization and performance tuning using SQL Profiler and performance monitoring.\n\u2022 Developed mappings to load Fact and Dimension tables, SCD Type 1 and SCD Type 2 dimensions and Incremental loading and unit tested the mappings.\n\u2022 Utilized Erwin's forward/reverse engineering tools and target database schema conversion process.\n\u2022 Worked on creating enterprise wide Model EDM for products and services in Teradata Environment based on the data from PDM. Conceived, designed, developed and implemented this model from the scratch.\n\u2022 Involved in extensive DATA validation by writing several complex SQL queries and Involved in back-end testing and worked with data quality issues.\n\u2022 Developed and executed load scripts using Teradata client utilities MULTILOAD, FASTLOAD and BTEQ.\n\u2022 Exporting and importing the data between different platforms such as SAS, MS-Excel.\n\u2022 Generated periodic reports based on the statistical analysis of the data using SQL Server Reporting Services (SSRS)\n\u2022 Write SQLscripts to test the mappings and Developed Traceability Matrix of Business Requirements mapped to Test Scripts to ensure any Change Control in requirements leads to test case update.\n\nEnvironment: DB2, Oracle SQL Developer, PL/SQL, Business Objects, Erwin, MS office suite, Windows XP, TOAD, SQL*PLUS, SQL*LOADER."", u'Data Analyst\nAccenture - Bengaluru, Karnataka\nMay 2010 to December 2012\nDescription: Accenture is a global management consulting and professional services company that provides strategy, consulting, digital, technology and operations services.\n\nResponsibilities:\n\u2022 Designed different type of STARschemas for detailed data marts and plan data marts in the OLAP environment.\n\u2022 Developed and executed load scripts using Teradata client utilities MULTILOAD, FASTLOAD and BTEQ.\n\u2022 Responsible for development and testing of conversion programs for importing Data from text files into map Oracle Database utilizing PERL shell scripts & SQL*Loader.\n\u2022 Used Graphical Entity-Relationship Diagramming to create new database design via easy to use, graphical interface.\n\u2022 Responsible for development and testing of conversion programs for importing Data from text files into map Oracle Database utilizing PERL shell scripts &SQL*Loader.\n\u2022 Worked with the ETL team to document the Transformation Rules for Data Migration from OLTP to Warehouse Environment for reporting purposes.\n\u2022 Created SQLscripts to find dataquality issues and to identify keys, data anomalies, and data validation issues.\n\u2022 Formatting the data sets read into SAS by using Format statement in the data step as well as Proc Format.\n\u2022 Applied Business Objects best practices during development with a strong focus on reusability and better performance.\n\u2022 Developed Tableau visualizations and dashboards using Tableau Desktop.\n\u2022 Used Graphical Entity-Relationship Diagramming to create new database design via easy to use, graphical interface.\n\u2022 Co-ordinate with various business users, stakeholders and SME to get Functional expertise, design and business test scenarios review, UAT participation and validation of financial data.\n\u2022 Write SQL scripts to test the mappings and Developed Traceability Matrix of Business Requirements mapped to Test Scripts to ensure any Change Control in requirements leads to test case update.\n\nEnvironment: Oracle SQL Developer, PL/SQL, Business Objects, TOAD, Tableau, Informatica, MS SQL Server, SQL*PLUS, SQL*LOADER, XML.']","[u""Bachelor's""]",[u'']
0,https://resumes.indeed.com/resume/227f20a788fdfdd5,[],[],[]
0,https://resumes.indeed.com/resume/51a000ff722b4ea3,"[u""Young's Market Company - Chino, CA\nJanuary 2018 to February 2018\n\u2022 Work with Warehouse Management System team (WMS)\n\u2022 Create reports using Oracle SQL Developer and IBM Cognos Business Intelligence software\n\u2022 Test the end to end sales system processes from RedTruck system to WMS process\n\u2022 Help design Task Priority and Standard Operating Procedure (SOP)"", u'Data Analyst Associate\nYoung\'s Market Company - Tustin, CA\nMay 2017 to December 2017\n\u2022 Work with the Master Data Management team (MDM)\n\u2022 Collaborate with multiple departments to understand company needs and devise possible solutions\n\u2022 Communicate results and ideas to key decision makers\n\u2022 Implement new statistical or other mathematical methodologies as needed for specific models or analysis\n\u2022 Communicate with business users to elicit analytics requirements\n\u2022 Participate in project-related trainings such as Oracle Enterprise Data Quality (EDQ), and Oracle Product\nInformation Management (PIM)\n\u2022 Validate data and provide technical support\n\u2022 Manage and automate the process of data cleaning during the data conversion from AS400 system to Oracle\n\u2022 Work closely with management, Deloitte consultants, and staff to prioritize business and information needs\n\u2022 Analyze, organize large amounts of data from AS400 source.\n\u2022 Maintain project schedules in collaboration with other team members by monitoring program progress;\ncoordinating activities; researching and resolving any issue\n\u2022 Apply Python to perform data wrangling\n\u2022 Perform under deadlines and time frames to meet business requirements\n\u2022 Train my coworkers using advanced Excel\n\u2022 Mass upload data from Oracle\n\nSkills\nHard skills\nPython, R, Microsoft Office in particular Excel, Machine Learning, Time Series Analysis, Data mining, Interactive Analysis, Data exploration, LaTeX, Oracle SQL Dev, Tableau, IBM Cognos Business intelligence\nSoft skills\nCritical thinking, self-motivated, detail oriented, strong interpersonal skills, time management, team player with a positive attitude, multi-tasker, adaptability, dynamic\n\nCoursework\n1) Mathematical Molding\nUsing mathematical tools to build a mathematical description of some physical problems\n\n2) High Performance Computing\nDesign and analysis of parallel algorithms. Also, parallel programming models and contemporary parallel programming techniques\n\n3) Data Mining\nAlgorithms and techniques of machine learning and data mining with emphasis on contemporary\nbig data challenges. Topics include data visualization, classification, clustering,\nNeural Network and data cleansing\n\n4) Time Series Analysis\nTheory and applications of classical and modern methods for Time Series Analysis.\nData analysis performed using the statistical software package R\n\n5) Computing for Scientists\nIntroduction to computer programming and software engineering background required to succeed in advanced study in the computational science\n\n6) Machine Learning\nApplication of the core algorithms and techniques of machine learning and data mining into a ""real-world"" dataset\n\nProject: Predict Flight delay.\nThe goal of the project is to determine if a flight will be delayed or not based on the values of the predictor variables by building 5 models using \'R\' statistical package.\nModeling Methods\n* k Nearest Neighbor (kNN)\n\n* Logistic Regression (LogR)\n\n* Discriminant Analysis (DA)\n\n* Support Vector Machines (SVM)\n\n* Neural Networks (NN)\nAssess the performance of all the 5 models by 3 methods: confusion matrix, accuracy of the prediction, and plot the ROC (Receiver Operating Characteristic) Curves.\n\n7) Interactive Data Analysis\nExplore concepts related to data interaction, data preparation, data transformation, data modeling and computation, and data presentation. Apply them into a real-world dataset\n\nProject: Airbnb Prices.\nThe goal of the project was to determine what factors affect the prices for Airbnb then explore the variables and examine the effect of the various variables on the price using interactive analysis (Jupyter). After that\nwe modeled the prices using simple linear regression to see what can we learn from their prices.\nThe purpose here was not to have a high accuracy model. We were only interested in running\na simple classifier that could give us insight to what variables matter to Airbnb users']","[u'Master of Science in Computational and Data Science in Computational and Data Science', u'Bachelor of Science in Information Science in Information Science']","[u'Chapman University Orange, CA\nAugust 2015 to December 2017', u'Umm Al-Qura University\nJanuary 2009 to May 2013']"
0,https://resumes.indeed.com/resume/2c0cc908749d4a81,"[u'Data Analyst\nDMC IT Services - IN\nJune 2013 to July 2015\nResponsibilities:\n* Performed data cleansing to ensure data consistency and minimal errors. Used the tm package in R-Studio for data cleansing.\n* Generated sentiment analysis reports on the cleansed data using the R-sentiment package. Performed the lexicon based sentiment analysis on data sheets using tm, stingr, plyr packages. Generated word clouds, dendrograms, and histograms using the data extracted.\n* Experience in building ETL, solutions to load external data into databases, using SSIS or other methodologies.\n* Analysed production issues by querying back-end databases and assist with troubleshooting of those issues.\n* Developed visualizations, storylines, worksheets for intuitive and innovative presenting by Tableau. Created visually impactful dashboards in Excel and Tableau for data reporting by using Pivot tables and VLOOKUP.\n* Attended daily sprint meetings and presented progress updates.\nEnvironment: MS Outlook, MS Project, MS Word, MS Excel, MS Visio, MS Access, Tableau, SQL.']","[u'Master of Science in Computer and Information sciences', u'in business', u'in English', u'Bachelors of Technology in Electronics and communication Engineering']","[u'Kent State University Kent, OH\nAugust 2015 to May 2017', u'Kent State University\nAugust 2016 to January 2017', u'Kent State University\nAugust 2015 to January 2016', u'Jawaharlal Nehru Technological University Kakinada, Andhra Pradesh\nAugust 2010 to May 2014']"
0,https://resumes.indeed.com/resume/a7e1d7578484e5a9,"[u'Data Analyst Intern\nJameson Publishing - Erie, PA\nMay 2017 to January 2018\nList building for databases of potential biopharmaceutical customers', u'Groundskeeper\nHarbor Ridge Golf Course - Harborcreek, PA\nJune 2015 to August 2016\nGrounds, mowing, preparing events']","[u'in Computer Science', u'High school or equivalent']","[u'Allegheny College Meadville, PA\nAugust 2016 to May 2020', u'Harbor Creek Senior High School Harborcreek, PA\nAugust 2012 to June 2016']"
0,https://resumes.indeed.com/resume/2c13fc2cc792c05a,"[u""Trainee Data Analyst\nBruno's Computer Solutions & Software Pvt. Ltd - Hyderabad, Telangana\nNovember 2016 to May 2017\nAnalyzing client data for revenue generating discounts and offers.\n\u25cf Reviewing the results of the analysis and performing unit testing with smaller data sets using\npredictive analytics.\n\u25cf Extracting daily reports from the SQL database and present to the trainer.\n\u25cf Presenting the results of the analysis using Tableau and Microsoft PowerPoint.\n\n\u25cf Weekly presentations to the team to explain the process flow and provide status updates."", u'Intern\nBhagyaShree Industries - Hyderabad, Telangana\nSeptember 2016 to October 2016\nStudying the design and operation of Electronic systems and Components.\n\u25cf Evaluating the products by conducting different assessments using MATLAB.\n\u25cf Project work street LED lamps and its power consumption under the guidance of head technicians.']","[u'Master of Sciences in Data Analytics Engineering in Data Analytics', u""Bachelor's in Electrical and Electronics Engineering""]","[u'George Mason University Fairfax, VA\nAugust 2017 to May 2019', u'GITAM University Visakhapatnam, Andhra Pradesh\nJune 2012 to April 2016']"
0,https://resumes.indeed.com/resume/788f41580d6d3b1e,"[u'Software Developer\nP4C Global, LLC - Los Angeles, CA\nMarch 2017 to Present\nWrote pricing software in C# and ASP.NET that is responsible for the selling of over 1 million dollars worth of monthly product across several online marketplaces (Amazon, Ebay, etc.). Had sole ownership of the project\nand built it from the ground up.\n\u2022 Built REST API using Web API that is responsible for performing key E-Commerce functions (fulfilling orders,\nperforming cancellations and refunds, updating tracking, etc.).\n\u2022 Built dashboard that replaced existing reporting processes. Used by approximately thirty people within the company to perform critical functions on a daily basis.', u'Data Analyst\nFeedonomics, LLC - Los Angeles, CA\nNovember 2016 to March 2017\n\u2022 Provided analytics to better track and change Google Adword bids for clients with large product lines\n(100,000+ unique skus).', u'Business Analyst\nFirst Capital, Arrowhead Wholesale, LLC - San Diego, CA\nNovember 2014 to April 2016\nUsing Java and MySQL, wrote software that automated trucking reporting processes for over 60 million dollars\nin premium; Performed time series and regression analysis in R and Excel to evaluate assorted metrics (loss\nratios, premium by radius, etc.).\n\u2022 Responsible for the design, content, and sending of the company newsletter to 6000+ contacts across several\nlines of business (Trucking, High Net Worth Personal Lines, etc.).', u""Data Analyst Intern\nTesmo, LLC - Paramount, CA\nJanuary 2014 to April 2014\n\u2022 Responsible for purchasing $150K - $200K of monthly product.\n\u2022 Performed time series analysis on all company SKU's, which analyzed the products that were profitable and growing more profitable month over month and which products were under performing.\n\u2022 Using Amazon Seller Central, compiled weekly and monthly reports that took a more in depth look at inventory\nshipped, inventory returned, and the month over month variance.""]",[u'B.S. in Statistics'],"[u'Cal Poly San Luis Obispo San Luis Obispo, CA\nDecember 2013']"
0,https://resumes.indeed.com/resume/b7264b96847e35b5,"[u'Data Analyst\nCollege of Science, Purdue University\nJanuary 2015 to January 2016\nAnalyzed the data in the years of 2008 - 2015 regarding enrollment, degree counts and time to degree,\ninstructional activity, research & sponsored program expenditures and cost sharing, etc. Data covers 70,000 students and $600M research funding.\n\u2022 Communicated with the predecessor and successor to smoothen the transition.']","[u'Ph.D. in Statistics', u'M.S. in Physics', u'B.S. in Physics']","[u'Purdue University West Lafayette, IN\nAugust 2015 to December 2018', u'Purdue University West Lafayette, IN\nJune 2013 to May 2015', u'Fudan University Shanghai, CN\nSeptember 2007 to July 2011']"
0,https://resumes.indeed.com/resume/3200afda05622e80,"[u""IT Analyst\nLamb Insurance Services\nJanuary 2017 to Present\n\u2022 Responsible for information acquisition through means of automated scripts, web scraping, apis.\n\u25e6 Resulting in over 200,000 prospects with clean data\n\u2022 Successfully created scripts to bypass googles Turing tests\n. Using Machine Learning and various api endpoints. (Google Speech API, Captcha Vendors)\n\u2022 Integrated automated applications to communicate with third party sdks' using SQL, Vb.net and vba\n\u25e6 Saved company over $100,000 in redundant tasks being performed by employees\n\u2022 Completely revamped data acquisition methods. Created and developed a distributed network of over 30 servers. Able to use Hub servers to clone other servers to run web automated applications."", u'Operations Analyst\nSpirit Halloween\nJune 2016 to January 2017\n\u2022 Create, document, and report on various operational functions, including store/employee performance, store optimization, etc.\n\u2022 Automation of excel reports, with future focus on connecting workbooks to various databases to accomplish complete automation of reports.\n\u2022 Responsible for monitoring operational precision for 1200 stores across U.S. and Canada.', u'Data Analyst\nMandrel LLC, MD\nSeptember 2014 to June 2016\n\u2022 Cultivated databases from web scraping to research MBA programs in the Northeast\n\u2022 Built pricing analysis on book cost, tuition cost, semester courses, etc.\n\u2022 Developed SQL Database, connect with excel sheets, train owner how to use and manipulate data', u'Analyst\nMorgan Stanley\nFebruary 2015 to June 2015\n\u2022 Create excel data scrapers, functions, macros to automate reports/ data analysis for risk assessment\n\u2022 Created and documented several models for risk and reporting.\n\u2022 Active leader of the HERA process before departing. Responsible for training employees in US, India and London. Process of liquidating assets in foreign currency, to USD to adhere to Dodd Frank laws.']","[u""Master's in Computational Science"", u""Bachelor's in Computational Science""]","[u'Stockton University\nJanuary 2013 to January 2014', u'Stockton University\nJanuary 2009 to January 2013']"
0,https://resumes.indeed.com/resume/a6af79314db93cd5,"[u'Data analyst\nMayor Campaign - Jersey City, NJ\nSeptember 2017 to November 2017\nComputer\nPolitical Campaign/Collect and analyze data\nCreative/analytical thinking\n\nAdaptability']",[u'in Commercial Photography'],"[u'County Prep High School Jersey City, NJ\nSeptember 2015 to June 2019']"
0,https://resumes.indeed.com/resume/95070e2107ab81e1,"[u""Incubation Analyst\nGoogle, via Accenture - Mountain View, CA\nJuly 2017 to March 2018\nCollaborated with sales, finance and operations teams to optimize user interface and workflows via UAT\n(Sandbox), automating processes that contributed to cutting customer and product onboarding time by 20%.\n\u25cf Worked with requesters to troubleshoot and escalate Salesforce, NetSuite and Google systems tickets for Retail\nHardware data in production, expediting them by 40% and resulting in a TAT of three days.\n\u25cf Created, refreshed and ran dashboards, reports and ad hoc requests via Salesforce and Excel, acting as the point\nof contact for managing and communicating data across for Retail Hardware for Google's global offices.\n\u25cf Validated and managed customer and product data when integrating object records across Salesforce, in-house\nsystems, NetSuite and Excel via Data Loader using SOQL, VLOOKUP and other functions.\n\u25cf Configured user profiles/permissions, groups and objects for Google's Retail Hardware for 50+ Accenture\nteam's users as the Delegated Administrator, acting as the liaison to request access, security and page layouts.\n\u25cf Develop a process document, training users to improve and expedite the data operations workflow for management and data entry profiles for smooth and expedited navigation by nearly 1.5x."", u""Data Operations Analyst\nSamsung - San Jose, CA\nAugust 2015 to July 2017\nManaged and maintained Authorized Service Center and appliance parts, transit, and status object records for reporting and billing by cleansing and deduplicating data for Data Loader and import.\n\u25cf Administered and configured approximately 150 users' profiles and permissions, page layouts, security and access for case page layouts and picklists for each agents' functionality contingent upon seven departments.\n\u25cf Constructed and ran reports via Salesforce and Excel to monitor call center agent hours and performance per\nSTM departments and Samsung ASCs, which increased TAT for closing case tickets by 15%.\n\u25cf Arranged a system to efficiently and accurately track and manage user data for the 200+ constantly varying\nagent and department manpower allocations and Salesforce configuration, security and access.\n\u25cf Provided administrative ad hoc support for day-to-day systems configuration requests, composing a process\ndocumentation and providing training for end-users.""]",[u'Bachelor of Arts in Communication'],"[u'University of California San Diego, CA\nAugust 2015']"
0,https://resumes.indeed.com/resume/633350b2ab9e9ad6,"[u""DATA ANALYST\nAmerican Honda Motors - Mumbai, Maharashtra\nApril 2015 to July 2017\nApril 2015 - July 2017\n\u2022 Worked on large datasets (more than 388,000,000 records), identifying Critical Success Factors (CSF) for Business performance.\n\u2022 Applied data mining techniques on Sales and Production Data, Reported the Sales KPI's and Metrics that are crucial to monitor.\n\u2022 Monitored the Sales Growth using Time Series Analysis in Tableau, ensuring Product Performance over the period of 12 months.\n\u2022 Conducted Root Cause Analysis and Identified Critical Issue in the Back-End Order Process, Saving the Company $43,000 annually.\n\u2022 Explored Inventory data using complex SQL queries, Presented Overstocking Problem to stakeholders using Tableau Dashboards.\n\u2022 Forecasted monthly Production needs and monitored the Coefficient of Determination, to improve the model performance.\n\u2022 Trained newly joined college interns in the team and assigned them to specific duties to help them acquaint with the business.\n\u2022 Performed A/B testing on Warranty Data to identify Common Defects, Achieved 15% reduction of the After-Sales Service cost.\n\u2022 Presented Data Visualization Story using Tableau, showing the insights of Price to Sales Ratio Analysis to the relevant stakeholders.\n\u2022 Served as a Subject Matter Expert for Order Management System (OMS) and Inventory Management System for B2B Business.""]","[u'in Research Assistant', u""Master's in Management Information Systems"", u""Bachelor's in Engineering""]","[u'UNIVERSITY AT BUFFALO, The State University at New York\nFebruary 2018 to Present', u'University at Buffalo\nSeptember 2018', u'Amravati University\nAugust 2014']"
0,https://resumes.indeed.com/resume/2a787e6c48364564,"[u'Data Scientist\nRoyalcaribbean - Miami, FL\nJune 2017 to Present\nResponsibilities:\n\u2022 Massively involved in Data Architect role to review business requirement and compose source to target data mapping documents.\n\u2022 Responsible for the data architecture design delivery, data model development, review, approval and Data warehouse implementation.\n\u2022 Set strategy and oversee design for significant data modeling work, such as Enterprise Logical Models, Conformed Dimensions, and Enterprise Hierarchy.\n\u2022 Analyzed existing Conceptual and Physical data models and altered them using Erwin to support enhancements.\n\u2022 Designed the Logical Data Model with the entities and attributes for each subject areas.\n\u2022 Lead Architectural Design in Big Data, Hadoop projects and provide for a designer that is an idea-driven.\n\u2022 Developed and configured on Informatica MDM hub supports the Master Data Management (MDM), Business Intelligence (BI) and Data Warehousing platforms to meet business needs.\n\u2022 Loaded data into Hive Tables from Hadoop Distributed File System (HDFS) to provide SQL access on Hadoop data\n\u2022 Used Agile Methodology of Data Warehouse development.\n\u2022 Design and implement data ingestion techniques for real time and batch processes for structured and unstructured data sources into Hadoop ecosystems and HDFS clusters.\n\u2022 Designed and developed architecture for data services ecosystem spanning Relational, NoSQL, and Big Data technologies.\n\u2022 Implemented multi-data center and multi-rack Cassandra cluster.\n\u2022 Created HBase tables to load large sets of structured, semi-structured and unstructured data coming from NoSQL and a variety of portfolios.\n\u2022 Involved in data model reviews as data architect with business analysts and business users with explanation of the data model to make sure it is in-line with business requirements.\n\u2022 Created Entity relationships diagrams, data flow diagrams and enforced all referential integrity constraints using Rational Rose\n\u2022 Worked with the ETL team to document the SSIS packages for data extraction to Warehouse environment for reporting purposes.\n\u2022 Developed data Mart for the base data in Star Schema, Snow-Flake Schema involved in developing the data warehouse for the database.\n\u2022 Involved in Data loading using PL\\SQL Scripts and SQL Server Integration Services packages\n\u2022 Established data governance, monitoring of Data Quality and clear documentation for facile implementation.\n\u2022 Involved in the validation of the OLAP, Unit testing and System Testing of the OLAP Report Functionality and data displayed in the reports.\n\u2022 Generated ad-hoc SQL queries using joins, database connections and transformation rules to fetch data from Teradata database.\n\u2022 Created HBase tables to load large sets of structured, semi-structured and unstructured data coming from UNIX, NoSQL and a variety of portfolios.\n\u2022 Worked on Amazon Redshift and AWS and architecting a solution to load data creates data models and run BI on it.\n\u2022 Created UNIX scripts for file transfer and file manipulation\n\u2022 Directed to Create Dashboards based on the business requirement using SSRS/Cognos and helped development team in knowledge about the requirement.\n\u2022 Handled importing of data from various data sources, performed transformations using Hive, Map Reduce, loaded data into HDFS and Extracted the data from Oracle into HDFS using Sqoop\n\u2022 Worked with various Teradata15 tools and utilities like Teradata Viewpoint, Multi Load, ARC, Teradata Administrator, BTEQ and other Teradata Utilities.\n\u2022 Involved in several facets of MDM implementations including Data Profiling, Metadata acquisition and data migration.\n\u2022 Extensively used Aginity Netezza work bench to perform various DML, DDL etc operations on Netezza database.\n\u2022 Created DDL scripts using Erwin and source to target mappings to bring the data from source to the warehouse.\n\u2022 Lead database level tuning and optimization in support of application development teams on an ad-hoc basis.\n\nEnvironment: HDFS, AWS Redshift, MapReduce, Hive 2.3, HBase, MongoDB, Cassandra, Metadata, Netezza, MySQL, Hadoop 3.0, ODS, Oracle 12c, T-SQL, MDM, PL/SQL, Teradata R15, Teradata SQL Assistant 15.0, Flat Files.', u'Data Scientist\nPhilips Healthcare - Baltimore, MD\nMarch 2016 to May 2017\nResponsibilities:\n\u2022 Understand the high level design choices and the defined technical standards for software coding, tools and platforms and ensure adherence to the same\n\u2022 Implemented the NoSQL database HBase and the management of the other tools and process observed running on YARN.\n\u2022 Used Agile Methodology of Data Warehouse development using Kanbanize.\n\u2022 Developed Data Mapping, Data Governance, and Transformation and cleansing rules for the Master Data Management (MDM) Architecture involving OLTP, ODS.\n\u2022 Extensively worked on creating the migration plan to Amazon web services (AWS).\n\u2022 Extracted Mega Data from Amazon Redshift, AWS, and Elastic Search engine using SQL Queries to create reports.\n\u2022 Analyze business requirements and build logical data models that describe all the data and relationships between the data\n\u2022 Designed both 3NF data models for ODS, OLTP systems and dimensional data models using Star and Snow Flake Schemas\n\u2022 Provided suggestion to implement multitasking for existing Hive Architecture in Hadoop also suggested UI customization in Hadoop\n\u2022 Architect and lead significant data initiatives in various data dimensions Master Data, Meta Data, Big Data & Analytics.\n\u2022 Involved in Planning, Defining and Designing database using ER Studio on business requirement and provided documentation.\n\u2022 Translate business and data requirements into logical data models in support of Enterprise Data Models, Operational Data Structures and Analytical systems.\n\u2022 Partner with DBAs to transform logical data models into physical database designs while optimizing the performance and maintainability of the physical database\n\u2022 Work with Data Management to establish governance processes around metadata to ensure an integrated definition of data for enterprise information, and to ensure the accuracy, validity, and reusability of metadata.\n\u2022 Developed Full life cycle of Data Lake, Data Warehouse with Big data technologies like Spark and Hadoop.\n\u2022 Applied all phases of the Software Development Life Cycle, which include requirements definition, analysis, review of design and development, and integration and test of solution into the operational environment\n\u2022 Responsible for full data loads from production to AWS Redshift staging environment.\n\u2022 Developed Map Reduce programs to cleanse the data in HDFS obtained from heterogeneous data sources to make it\n\u2022 Provided optimal design for structured and non-structured data using SQL and NoSQL databases.\n\u2022 Created data schema and architecture of data warehouse for standardized data storage and access\n\u2022 Designed data models with industry standards up to 3NF (OLTP/ODS) and de-normalized (OLAP) data marts with Star & Snow flake schemas.\n\u2022 Generated and DDL (Data Definition Language) scripts using ER Studio and assisted DBA in Physical Implementation of Data Models.\n\u2022 Used data profiling automation to uncover the characteristics of the data and the relationships between data sources before any data-driven.\n\u2022 Develop test scripts for testing sourced data and their validation and transformation when persisting in data stores that are physical representations of the data models\n\u2022 Designed and documented Use Cases, Activity Diagrams, Sequence Diagrams, OOD (Object Oriented Design) using UML and Visio.\n\u2022 Completed enhancement for MDM (Master data management) and suggested the implementation for hybrid MDM (Master Data Management)\n\u2022 Developed and implemented data cleansing, data security, data profiling and data monitoring processes.\n\u2022 Generated ad-hoc reports using Crystal Reports XI.\n\u2022 Designed processes and jobs to source data from Mainframe sources to HDFS staging zone\n\u2022 Integrated data from multiples sources including HDFS to Hive Data warehouse.\n\u2022 Worked very close with Data Architectures and DBA team to implement data model changes in database in all environments.\n\nEnvironment: ER Studio 9.6, Hive 2.3, Hadoop 3.0, MDM, AWS, Redshift, HDFS, Teradata 14, PL/SQL, Informatica 9.0, Oracle 12c, UNIX.', u""Data Scientist\nSteve Madden Ltd - Long Island City, NY\nNovember 2014 to February 2016\nDescription: Steven Madden, Ltd., together with its subsidiaries, designs, sources, markets, and sells fashion-forward name brand and private label footwear for women, men, and children. It offers wholesale footwear under the Steve Madden Women's, Madden Girl, Steve Madden Men's, Steven.\nResponsibilities:\n\n\u2022 Developed logical data models and physical database design and generated database schemas using Erwin 8.5.\n\u2022 Performed data analysis and data profiling using complex SQL on various sources systems including Oracle and MS SQL Server.\n\u2022 Document all data mapping and transformation processes in the Functional Design documents based on the business requirements.\n\u2022 Prepared High Level Logical Data Models using Erwin, and later translated the model into physical model using the Forward Engineering technique.\n\u2022 Generated and DDL (Data Definition Language) scripts using Erwin and assisted DBA in Physical Implementation of data Models.\n\u2022 Translated business requirements into working logical and physical data models for OLTP & OLAP systems.\n\u2022 Generated SQL scripts and implemented the relevant databases with related properties from keys, constraints, indexes & sequences.\n\u2022 Used Reverse Engineering to connect to existing database and developed process methodology for the Reverse Engineering phase of the project.\n\u2022 Developed the batch program in PL/SQL for the OLTP processing and used UNIX Shell scripts to run in corn tab.\n\u2022 Performed extensive data profiling and data analysis for detecting and correcting inaccurate data from the databases and track the data quality.\n\u2022 Provided guidance and solution concepts for multiple projects focused on data governance and master data management.\n\u2022 Created DDL scripts using Erwin and source to target mappings to bring the data from source to the warehouse.\n\u2022 Designed and developed SAS macros, applications and other utilities to expedite SAS Programming activities.\n\u2022 Involved in writing T-SQL working on SSIS, SSRS, SSAS, Data Cleansing, Data Scrubbing and Data Migration.\n\u2022 Analyzed and Gathered requirements from business people and management and business requirement document to prioritize their needs.\n\u2022 Responsible for backing up the data and involved in writing stored procedures and involved in writing ad-hoc queries for the data mining.\n\u2022 Developed and maintained data dictionary to create metadata reports for technical and business purpose.\n\u2022 Create and Monitor workflows using workflow designer and workflow monitor.\n\u2022 Involved in extensive DATA validation by writing several complex SQL queries and Involved in back-end testing and worked with data quality issues.\n\u2022 Used SSRS for generating Reports from Databases and Generated Sub-Reports, Drill down reports, Drill through reports and parameterized reports using SSRS.\n\u2022 Developed PL/SQL scripts to validate and load data into interface tables and Involved in maintaining data integrity between Oracle and SQL databases.\n\u2022 Heavily worked on SQL query optimization also tuning and reviewing the performance metrics of the queries\n\u2022 Performed the Data Mapping, Data design (Data Modeling) to integrate the data across the multiple databases in to EDW.\n\u2022 Collaborated with the Relationship Management and Operations teams to develop and present KPIs to top-tier clients.\n\nEnvironment: Erwin 8.5, Oracle 10g, MS SQL Server 2008, SSRS, OLAP, OLTP, MS Excel, Flat Files, , PL/SQL, OLAP, OLTP , SQL, IBM Cognos, Tableau."", u'Data Scientist\nQuality Systems Inc - Irvine, CA\nMarch 2013 to October 2014\nDescription: Quality Systems, Inc., together with its subsidiaries, develops and markets healthcare information systems in the United States. The company operates through four divisions: QSI Dental, NextGen, Inpatient Solutions, and Practice Solutions.\nResponsibilities:\n\n\u2022 Statistical Modeling with ML to bring Insights in Data under guidance of Principal Data Scientist\n\u2022 Data modeling with Pig, Hive, Impala.\n\u2022 Ingestion with Sqoop, Flume.\n\u2022 Used SVN to commit the Changes into the main EMM application trunk.\n\u2022 Understanding and implementation of text mining concepts, graph processing and semi-structured and unstructured data processing.\n\u2022 Worked with Ajax API calls to communicate with Hadoop through Impala Connection and SQL to render the required data through it. These API calls are similar to Microsoft Cognitive API calls.\n\u2022 Good grip on Cloudera and HDP ecosystem components.\n\u2022 Used Elasticsearch (Big Data) to retrieve data intothe application as required.\n\u2022 Performed Map Reduce Programs those are running on the cluster.\n\u2022 Developed multiple MapReduce jobs in java for data cleaning and preprocessing.\n\u2022 Analyzed the partitioned and bucketed data and compute various metrics for reporting.\n\u2022 Involved in loading data from RDBMS and weblogs into HDFS using Sqoop and Flume.\n\u2022 Worked on loading the data from MySQL to HBase where necessary using Sqoop.\n\u2022 Developed Hive queries for Analysis across different banners.\n\u2022 Extracted data from Twitter using Java and Twitter API. Parsed JSON formatted twitter data and uploaded to the database.\n\u2022 Launching Amazon EC2 Cloud Instances using Amazon Images (Linux/ Ubuntu) and Configuring launched instances with respect to specific applications.\n\u2022 Exported the result set from Hive to MySQL using Sqoop after processing the data.\n\u2022 Analyzed the data by performing Hive queries and running Pig scripts to study customer behavior.\n\u2022 Have hands-on experience working with Sequence files, AVRO, HAR file formats and compression.\n\u2022 Used Hive to partition and bucket data.\n\u2022 Experience in writing MapReduce programs with Java API to cleanse Structured and unstructured data.\n\u2022 Wrote Pig Scripts to perform ETL procedures on the data in HDFS.\n\u2022 Created HBase tables to store various data formats of data coming from different portfolios.\n\u2022 Worked on improving the performance of existing Pig and Hive Queries.\n\nEnvironment: Pig, Hive, Impala, ETL, HBase, AVRO, MapReduce, Java, HDFS, MySQL, Hadoop', u""Data Architect/Data Modeler\nLava International Limited - Noida, Uttar Pradesh\nOctober 2011 to February 2013\nDescription: Lava International Limited is an Indian multi-national company in the mobile handset industry. The company was founded in 2009 as an offshoot of a telecommunication venture. It is headquartered in Noida, India and has overseas operations in Thailand, Nepal, Bangladesh, Sri Lanka, Indonesia, Mexico and the Middle East, Pakistan and Russia.\n\nResponsibilities:\n\u2022 Analyzed data sources and requirements and business rules to perform logical and physical data modeling.\n\u2022 Analyzed and designed best fit logical and physical data models and relational database definitions using DB2. Generated reports of data definitions.\n\u2022 Conducted source data analysis of various data sources and develop source-to-target mappings with business rules.\n\u2022 Involved in Normalization/De-normalization, Normal Form and database design methodology.\n\u2022 Maintained existing ETL procedures, fixed bugs and restored software to production environment.\n\u2022 Developed the code as per the client's requirements using SQL, PL/SQL and Data Warehousing concepts.\n\u2022 Involved in Dimensional modeling (Star Schema) of the Data warehouse and used Erwin to design the business process, dimensions and measured facts.\n\u2022 Worked with Data Warehouse Extract and load developers to design mappings for Data Capture, Staging, Cleansing, Loading, and Auditing.\n\u2022 Developed enterprise data model management process to manage multiple data models developed by different groups\n\u2022 Designed and created Data Marts as part of a data warehouse.\n\u2022 Effectively used triggers and stored procedures necessary to meet specific application's requirements.\n\u2022 Created SQL scripts for database modification and performed multiple data modeling tasks at the same time under tight schedules.\n\u2022 Reviewed new data development and ensured that it is consistent and well integrated with existing structures.\n\u2022 Wrote complex SQL queries for validating the data against different kinds of reports generated by Business Objects XIR2.\n\u2022 Involved in reviewing business requirements and analyzing data sources form Excel/Oracle SQL Server for design, development, testing, and production rollover of reporting and analysis projects.\n\u2022 Document and publish test results, troubleshoot and escalate issues\n\u2022 Worked on SAS and IDQ for Data Analysis.\n\u2022 Using Erwin modeling tool, publishing of a data dictionary, review of the model and dictionary with subject matter experts and generation of data definition language.\n\u2022 Coordinated with DBA in implementing the Database changes and also updating Data Models with changes implemented in development, QA and Production.\n\u2022 Created and execute test scripts, cases, and scenarios that will determine optimal system performance according to specifications.\n\u2022 Worked Extensively with DBA and Reporting team for improving the Report Performance with the Use of appropriate indexes and Partitioning.\n\u2022 Developed Data Mapping, Transformation and Cleansing rules for the Master Data Management Architecture involved OLTP, ODS and OLAP.\n\u2022 Tuned and coded optimization using different techniques like dynamic SQL, dynamic cursors, and tuning SQL queries, writing generic procedures, functions and packages.\n\u2022 Analyzed the data and provide resolution by writing analytical/complex SQL in case of data discrepancies.\n\u2022 Experienced in GUI, Relational Database Management System (RDBMS), designing of OLAP system environment as well as Report Development.\n\u2022 Extensively used SQL, T-SQL and PL/SQL to write stored procedures, functions, packages and triggers.\n\u2022 Analyzed of data report were prepared weekly, biweekly, monthly using MS Excel, SQL & UNIX.\n\nEnvironment: Erwin 7.5, Oracle 10g Application Server, Oracle Developer Suite, PL/SQL, T-SQL, DB2, SQL Plus, Microsoft SQL Server 2005"", u""Data Analyst/Data Modeler\nSpice Digital Limited - Noida, Uttar Pradesh\nMarch 2009 to September 2011\nDescription: Spice Digital Limited, previously known as Cellebrum Technologies, was founded in year 2000. The company provides Telco Solutions, Value Added Services (MVAS), Enterprise Solutions, Financial technology, GSP and Digital transformation products and services. Spice Digital is headquartered at Noida in NCR, India.\n\nResponsibilities:\n\u2022 Normalized and De-normalized the tables and maintaining Referential Integrity by using Triggers and Primary and Foreign Keys.\n\u2022 Conducted and automated the ETL operations to Extract data from multiple data sources, transform inconsistent and missing data to consistent and reliable data, and finally load it into the Multi-dimensional data warehouse\n\u2022 Developed packages using Fast Parse in SSIS to reduce the extraction time in ETL process by 9.5%.\n\u2022 Involved in design and analysis of underlying database schema, altering and creation of the table structure.\n\u2022 Developed Informatica Mappings using heterogeneous sources like flat files and different relational databases, Mapplets, Mappings using Power Center Designer.\n\u2022 Experience with routine DBA activities like Query Optimization, Performance Tuning and Effective SQL Server configuration for better performance and cost reduction. Installed and configured SQL Mail client for SQL 2000.\n\u2022 Responsible for report generation using SQL Server Reporting Services (SSRS) and Crystal Reports based on business requirements.\n\u2022 Created number of jobs, alerts and operators to be paged or emailed in case of failure for SQL 2000.\n\u2022 Created and Configured Data Source & Data Source Views, Dimensions, Cubes, Measures, Partitions, KPI's & MDX Queries using SQL Server 2005 Analysis Services(SSAS).\n\u2022 Experience in Creating Backend validations using Insert/Update and Delete triggers and Created views for generating reports, Indexed Views.\n\u2022 Improved the performance of the SQL server queries using query plan, covering index, indexed views and by rebuilding and reorganizing the indexes.\n\u2022 Configure and manage database maintenance plans for update statistics, database integrity check and backup operations.\n\nEnvironment: SQL Server 2008, SSRS, SSIS, SSAS, Microsoft office, SQL Server Management Studio, Business Intelligence Development Studio, MS Access, Erwin data Modeler""]",[u'Bachelor of Computer Science & Technology in TOOLS AND TECHNOLOGIES'],[u'SGD\nMarch 2011']
0,https://resumes.indeed.com/resume/e80741ae7058eecf,"[u'Data Science Consultant\nAbide - Menlo Park, CA\nDecember 2017 to Present\n\u2022 Determined best marketing channels for app awareness and established new marketing plan to reach customers in those areas.\n\u2022 Established new methods of user retention analyzing application interface and user behavior to increase retention by 10%.\n\u2022 Initiated ideas for new user conversion with CEO to establish better funnel and discover trends in current active users.', u""Data Analyst, Sales Operations\nGoogle Inc. via Sutherland Global Services - Mountain View, CA\nJanuary 2016 to June 2016\nManaged the implementation of a more efficient process for the Sales Operations team to effectively analyze and update SalesForce accounts.\n\u25cf Initiated communications with Sales Representatives across four regions to convey potential changes in account ownership and commission payouts.\n\u25cf Ensured data accuracy in over 10K SalesForce accounts by coordinating with internal teams and upper management.\n\u25cf Resolved commission disputes by utilizing SQL and Google's account database to ensure accuracy and timely delivery of bonuses.""]","[u'M.S. in Business Analytics', u'B.S. in Management Science']","[u'Santa Clara University, Leavey School of Business\nSeptember 2017', u'University of California San Diego, CA\nJune 2015']"
0,https://resumes.indeed.com/resume/4588cee6eed09436,"[u'Research Associate III\nBenitec Pharmaceuticals - Hayward, CA\nSeptember 2015 to April 2017\nPre-clinical research study - Age-related Macular Degeneration\n\u25cb Created and organized data via Excel and Powerpoint, presented research data to scientific officers and research scientists.\n\u25cb Troubleshooting experiments, researching more effective methods for assay implementation.\n\u25cb RT-PCR and qPCR to quantify gene knockdown\n\u25cb Reported project milestones and delivery dates to research managers, ensuring research timelines were met. Collaborated with a cross-functional team from all areas of the company', u'Research Associate II\nDiscoveRx Corp - Fremont, CA\nJune 2007 to August 2015\nSuccessfully launched PathHunter Express GPCR product cell lines\n\u25cb Provided detailed experimental results to sales and R&D managers, ensuring timely product launch.\n\u25cb Troubleshooting experiments, applying new methods to qualify more cell lines in order to expand product library.\n\u25cb Developed and authored SOPs for product lines for product consistency and end user satisfaction.\n\u25cb Reported project milestones and delivery dates to research managers, ensuring research timelines were met. Collaborated with a cross-functional team from all areas of the company\n\u25cb Maintained cell lines\n\u25cf Built Orphan GPCR library\n\u25cb Contributed in the development and optimization of proprietary protein detection method. Characterized each orphan GPCR applying this method.\n\u25cb Developed and authored SOPs for product line. Contributed to a research paper utilizing this method, published in scientific journal.\n\u25cb Trained field application scientists and other research associates. Provided technical support for sales team, ensuring reproducible results in the field.\n\u25cb Maintained cell lines, expanding and freezing cell lines.\n\u25cb Viral transfection via ecotropic cell lines and producing viral supernatant for transduction.\n\u25cf Molecular Biology Associate - Custom Assay Development\n\u25cb Implemented QC process on all cell lines, ensuring product quality\n\u25cb Initiated projects, managed project deadlines, updating Principle Scientists and Directors\n\u25cb Designing primers, for cloning experiments and genomic PCR (for QC process)\n\u25cb Cloned over 1500 constructs via sticky-end, seamless, topo cloning\n\u25cb Extensive use of NCBI BLAST, DNAstar, FinchTV to view sequencing data/alignment']","[u'Certificate', u""Master's in Economics in Economics"", u""Bachelor's in Biology""]","[u'University of California, Berkeley (UCB Extension) Berkeley, CA\nApril 2018', u'California State University', u'UC San Diego San Diego, CA']"
0,https://resumes.indeed.com/resume/0c03c16e11d4bb27,"[u'Senior Data Analyst\ncomScore - Reston, VA\nJuly 2014 to Present\nPrincipal engineer for two of comScore\u2019s products, Mobile Metrix and Media-Metrix Multiplatform. Design, implement, and QA all systems and methodology innovations for these products in SQL Server, Greenplum, and Hadoop. Provide internal consulting to comScore\u2019s analytics, product, and client teams in using these products to further business development.']",[u'Bachelor of Arts in Earth Sciences'],"[u'University of Pennsylvania Philadelphia, PA\nMay 2014']"
0,https://resumes.indeed.com/resume/d4b60cc6942517e8,"[u'Programmer Analyst\nChemical Abstracts Services - Columbus, OH\nNovember 2017 to Present\nUse Perl, SQL, and Java to automate manual processes to increase team productivity and efficiency\no Ensures a constant and even distribution of work across employees\n\u2022 Create user applications in Java as database interfaces to allow employees and managers to easily\ninteract and modify database tables\n\u2022 Conduct software testing to ensure programs run efficiently across multiple platforms\n\u2022 Use Perl to generate monthly and yearly reports based on information located across multiple\ndatabases', u""Medical Data Analyst\nCardinal Health, Inc - Dublin, OH\nMay 2017 to August 2017\nUsed RStudio to optimize daily routes between accounts and sales representatives, allowing\nstakeholders to minimize costs and maximize efficiency on a daily basis\no Decreased sales representatives' daily drive time by 25%\no Won Advanced Analytics Award for best use of analytics within Cardinal Health\n\u2022 Used Alteryx, SQL, Tableau, and RStudio to develop an application allowing Cardinal Health at Home to regularly adjust their strategic pricing model by incorporating current insurance\nreimbursement rates\no Allows Cardinal Health at Home to identify opportunity customers for monetary gain\n\u2022 Regularly met with external stakeholders to define business requirements and review progress for feedback and refinement\n\u2022 Presented all findings to Vice Presidents and Directors of Pricing and Sales\n\nStudy Abroad | Lecce, Italy | June - August 2016\n\u2022 6 credit intensive language immersion program""]","[u'in Data Analytics, Business Analytics Specialization']",[u'The Ohio State University\nMay 2018']
0,https://resumes.indeed.com/resume/53d75841aee8e38c,"[u'Data Analyst\nEquityRead - New York, NY\nFebruary 2017 to Present\n\u2022 Created a database of all the properties in New York city by gathering data from NYC Open Data and inhouse proprietary data. Managed and standardized the data to remove redundancy and more accessible.\n\u2022 Created data layers on ArcGIS based on zone class, tax lots and other factors. Integrated the layers on tableau to create map-based dashboards to analyze the generated database to find potential leads for business development.\n\u2022 Developed a system integrating proprietary data and map layers to provide an interactive solution for clients to find real-estate based information for any potential listing.', u""Data Analyst\nEquityRead - New York, NY\nFebruary 2017 to September 2017\n\u2022 Created database on Excel for the canvassing project (gathering leads for possible listings). Standardized the data to remove redundancy and managed the data to ease bulk upload on Alto - A salesforce CRM for Real Estate. Analyzed the managed data on Tableau to create map-based data visualization for possible real estate listings.\n\u2022 Performed diagnostics on Salesforce and generated client specific reports to fit their requirement. Created customized reports on Tableau for possible listing by analyzing the market price of the neighborhood and running comparables.\n\u2022 Cross referenced created database with existing database to perform analysis based on market rate and client requirements to provide results to either sell the asset or renovate the asset to generate income.\n\u2022 Automated workflow by writing google scripts to manage database of listings based on various criteria's in New York city and built an automated sales expense dashboard by using formulas in excel.\n\u2022 Developed an internal client portal on WordPress, which automated workflow for all the agents like sending out Personal Agreements, build client database, manage their personal listings, send client invoices and perform other real estate-based calculations.""]","[u'Master of Science in Information Systems / Business Intelligence & Analytics', u'Bachelor of Engineering in Instrumentation Engineering']","[u'Stevens Institute of Technology Hoboken, NJ\nJanuary 2016 to May 2017', u'University of Mumbai Mumbai, Maharashtra\nAugust 2011 to May 2015']"
0,https://resumes.indeed.com/resume/d749801c476f44d0,"[u'Data Analyst\nGreat NonProfits - Redwood City, CA\nJune 2016 to April 2017\n\u2022 Gather the requirements of the project from the internal stakeholders.\n\u2022 Source the data, extract, clean, transfer and load the data into the sqlite3 and Excel.\n\u2022 Model and analyze the data of the project include demographics and response of the users.\n\u2022 Develop complex queries to calculate the number of patients in different communities, aggregating by age.\n\u2022 Perform statistical analysis (Example: chi-square test) on the selected data to prove the significance of the results.\n\u2022 Generate and maintain reports and visualization that clearly conveys the analysis of the data.\n\u2022 Present the data using tableau and power point.', u'Information Analyst\nFreekard - New Delhi, IL\nMarch 2000 to December 2008\nResponsible for the operational support of the Data Management Systems.\n\u2022 Extensively worked with SQL and Spreadsheet based tools to mine customer data received from websites to create very decision oriented management reports.\n\u2022 Prepared data models, algorithm, programming to process raw data received from websites to generate end user reports.\n\u2022 Worked with IT to generate and capture additional data points to create and enhance support for decision-making.\n\u2022 Based on website details, worked with IT for design improvement and enhancements.\n\u2022 Worked with vendor management team to support process audits.']","[u'Masters of Science in Mathematics', u""Bachelor's in Mathematics""]","[u'California State University East bay Hayward, CA\nJanuary 2013 to January 2015', u'Delhi University New Delhi, Delhi']"
0,https://resumes.indeed.com/resume/7e6723e97abbdf2e,"[u""Data Analyst\nTD Bank, Maine\nNovember 2016 to Present\nAs a Data Analyst, I was Involved in the Data Integration/Infrastructure project, for the Retail Banking. My role was to assess the impact on downstream data management environments of processing different banking products via different systems and processes. I was also, involved in requirements gathering, data mapping, and documenting metadata.\n\nResponsibilities:\n\u2022 Work with users to identify the most appropriate source of record required to define the asset data for financing\n\u2022 Perform data profiling in the source systems that are required for financing\n\u2022 Document the complete process flow to describe program development, logic, testing, and implementation, application integration, coding.\n\u2022 Involved in defining the trumping rules applied by Master Data Repository\n\u2022 Define the list codes and code conversions between the source systems and MDR.\n\u2022 Analysis of functional and non-functional categorized data elements for data profiling and mapping from source to target data environment. Developed working documents to support findings and assign specific tasks.\n\u2022 Involved with data profiling for multiple sources and answered complex business questions by providing data to business users.\n\u2022 Performed Data analysis and Data profiling using complex SQL on various sources systems including Oracle and Teradata.\n\u2022 Worked with internal architects and, assisting in the development of current and target state enterprise data architectures\n\u2022 Worked with project team representatives to ensure that logical and physical ER/Studio data models were developed in line with corporate standards and guidelines\n\u2022 Involved in defining the source to target data mappings, business rules, business and data definitions\n\u2022 Responsible for defining the key identifiers for each mapping/interface\n\u2022 Responsible for defining the functional requirement documents for each source to target interface.\n\u2022 Developed Python programs for manipulating the data reading from various Teradata and convert them as one CSV Files.\n\u2022 Extensively used Base SAS programs to convert data into Teradata table into CSV and other flat files.\n\u2022 Hands on Experience on Pivot tables, Graphs in MS Excel\n\u2022 Performed research and analysis on XML messages, validation errors and business rules to provide guidance on how to clear errors for vendors and business users.\n\u2022 Imported the customer data into Python using Pandas libraries and performed various data analysis - found patterns in data which helped in key decisions for the company\n\u2022 Performing statistical data analysis and data visualization using Python and R.\n\u2022 Drawing statistical inferences between multiple attributes of the dataset using R and Python.\n\u2022 Responsible in maintaining the Enterprise Metadata Library with any changes or updates\n\u2022 Using advanced Excel features like Pivot tables and Charts for generating Graphs.\n\u2022 Designed and developed weekly, monthly reports by using MS Excel Techniques (Charts, Graphs, Pivot tables) and Power point presentations.\n\u2022 Strong Excel skills, including pivots, Vlookup, conditional formatting, large record sets. Including data manipulation and cleaning.\n\u2022 Document data quality and traceability documents for each source interface\n\u2022 Establish standards of procedures.\n\u2022 Generate weekly and monthly asset inventory reports.\n\u2022 Coordinate with the business users in providing appropriate, effective and efficient way to design the new reporting needs based on the user with the existing functionality\n\u2022 Remain knowledgeable in all areas of business operations in order to identify systems needs and requirements.\n\nDigital Customer Event Data Migration (Audit logging Hadoop Initiative)\nHadoop Consultant\n\nBy this initiative 70% of the processing time have been reduced, Real time availability of event log data, much Easier for report generation in Tableau. Reduction in development LOE by 60 - 90%Faster time to market Opportunity for expansion of analytical\n\nResponsibilities\n\u2022 External Hive Tables have been created pointing to the output location of the transformed files in HDFS.\n\u2022 Views have been created on top of Hive Tables according to the needs of DA's like payments, enhanced transactions etc.\n\u2022 Working closely with Cassandra loading activity on history load and incremental loads from SQL Server and Oracle Databases and resolving loading issues and tuning the loader for optimal performance.\n\u2022 Perform system & data administration functions network of Linux hosts associated with their Big Data (Cassandra, Hadoop) operations.\n\u2022 Stay abreast with current releases of Hadoop, Cassandra, and HBase including compatibility issues with operating systems, new functions and utilities\n\u2022 Capable of provisioning, installing, configuring, monitoring and maintaining HDFS, yarn, Hbase, Sqoop, Pig, Hive.\n\u2022 Fetched data from existing MySQL database to HBase using Sqoop and MapReduce.\n\u2022 Created HBase tables to load large sets of structured, semi-structured and unstructured data coming from UNIX, NoSQL and a variety of portfolios.\n\u2022 Performed migration of Historical data of customers to HDFS with Sqoop and cleaned data with Hive for inconsistencies.\n\u2022 Worked on Big Data Platform, experience of Hive SQL.\n\u2022 Created reports using Hive SQL.\n\u2022 Individual Groups have been created according to the LOB for e.g. Digital will have a separate group created in production.\n\u2022 Worked with different groups and Data Stewards to assign group\n\u2022 Each group will have access to the corresponding Digital data.\n\u2022 Loaded and transformed large sets of structured, semi structured and unstructured data using Hadoop/Big Data concepts.\n\nEnvironment: SQL/Server, Oracle 11g, MS-Office, Teradata, Informatica, Pivot, ER Studio, XML, Hive, HDFS, Flume, Sooq, R connector, Python, R"", u'Data Analyst\nFarmers Insurance\nSeptember 2014 to June 2015\nClaims Data Mart\nThe goal of this project is to analyze business in multiple areas. In current state more than 30 vendors are involved in claims evaluation and estimation for different Line of Business like Auto, Homeowners, and workers compensation. And they are generating reports from different systems to analyze business. Client needs to integrate all the source system historical data into one single platform CDM (Claims data Mart) to address client performance questions to identify areas and opportunities for system/business improvements.\n\nResponsibilities:\n\u2022 Experienced in developing business reports by writing complex SQL queries using views, volatile tables\n\u2022 Experienced in Automating and Scheduling the Teradata SQL Scripts in UNIX using Korn Shell scripting.\n\u2022 Wrote several Teradata SQL Queries using Teradata SQL Assistant for Ad Hoc Data Pull request.\n\u2022 Analysis of functional and non-functional categorized data elements for data profiling and mapping from source to target data environment. Developed working documents to support findings and assign specific tasks.\n\u2022 Data Profiling to help identify patterns in the source data using SQL and Informatica and thereby help improve quality of data and help business to understand the converted data better to come up with accurate business rules.\n\u2022 Excellent knowledge on creating reports on SAP Business Objects, WEBI reports for multiple data providers.\n\u2022 Involved with data profiling for multiple sources and answered complex business questions by providing data to business users.\n\u2022 Implemented Indexes, Collecting Statistics, and Constraints while creating table\n\u2022 Created action filters, parameters and calculated sets for preparing dashboards and worksheets in Tableau.\n\u2022 Building, publishing customized interactive reports and dashboards, report scheduling using Tableau server.\n\u2022 Design and deploy rich Graphic visualizations with Drill Down and Drop down menu option and Parameterized using Tableau.\n\u2022 Created side by side bars, Scatter Plots, Stacked Bars, Heat Maps, Filled Maps and Symbol Maps according to deliverable specifications.\n\nEnvironment: Oracle 10g, MS-Office, Teradata, Tableau 9.2, Teradata 13', u""Data Analyst\nSainsbury's Bank - Edinburgh\nJune 2013 to August 2014\nResponsibilities:\n\u2022 Created new reports based on requirements.\n\u2022 Responsible in Generating Weekly ad-hoc Reports\n\u2022 Planned, coordinated, and monitored project levels of performance and activities to ensure project completion in time.\n\u2022 Automated and scheduled recurring reporting processes using UNIX shell scripting and Teradata utilities such as MLOAD, BTEQ and Fast Load\n\u2022 Experience with Perl\n\u2022 Worked in a Scrum Agile process & Writing Stories with two week iterations delivering product for each iteration\n\u2022 Worked on transferring the data files to vendor through sftp &Ftp process\n\u2022 Involved in defining and Constructing the customer to customer relationships based on Association to an account & customer\n\u2022 Worked with architects and, assisting in the development of current and target state enterprise level data architectures\n\u2022 Worked with project team representatives to ensure that logical and physical data models were developed in line with corporate standards and guidelines.\n\u2022 Involved in defining the source to target data mappings, business rules and data definitions.\n\u2022 Responsible for defining the key identifiers for each mapping/interface.\n\u2022 Performed data analysis and data profiling using complex SQL on various sources systems including Oracle and Teradata.\n\u2022 Migrated three critical reporting systems to Business Objects and Web Intelligence on a Teradata platform\n\u2022 Created Excel charts and pivot tables for the Adhoc data pull\n\nEnvironment: Teradata 13.1, Informatica 6.2.1, Ab Initio, Business Objects, Oracle 9i, PL/SQL, Microsoft Office Suite (Excel, Vlookup, Pivot, Access, Power Point), Visio, VBA, Micro Strategy, Tableau , UNIX Shell Scripting ERWIN."", u""Data Analyst\nAviva Healthcare - Sheffield\nApril 2012 to May 2013\nResponsibilities:\n\u2022 Interacted with business users to identify and understand business requirements and identified the scope of the projects.\n\u2022 Identified and designed business Entities and attributes and relationships between the Entities to develop a logical model and later translated the model into physical model.\n\u2022 Developed normalized Logical and Physical database models for designing an OLTP application.\n\u2022 Enforced Referential Integrity (R.I) for consistent relationship between parent and child tables.\n\u2022 Reverse Engineered the Data Models and identified the Data Elements in the source systems and adding new Data Elements to the existing data models.\n\u2022 Created XSD's for applications to connect the interface and the database.\n\u2022 Compare data with original source documents and validate Data accuracy.\n\u2022 Used reverse engineering to create Graphical Representation (E-R diagram) and to connect to existing database.\n\u2022 Conducted sessions with architects, business and development teams to understand the Change Requests (CR's), and effects caused on the system by the change requests.\n\u2022 Maintained and updated Metadata repository based on the change requests\n\u2022 Used Forward engineering to develop physical model with DDL based on the requirements gathered from a logical model in ERWIN.\n\u2022 Checked for all the modeling standards including naming standards, entity relationships on model and for comments in the model.\n\u2022 Conducted design walkthrough with project team and got it signed off.\n\u2022 Created entity / process association matrices, entity-relationship diagrams, functional decomposition diagrams and data flow diagrams.\n\u2022 Generated comprehensive analytical reports by running SQL queries against current databases to conduct data analysis.\n\u2022 Involved in Data Mapping activities for the data warehouse\n\u2022 Extensively worked on Performance Tuning and understanding Joins and Data distribution.\n\u2022 Executed DDL to create databases, tables and views.\n\u2022 Experienced in generating and documenting Metadata while designing application.\n\u2022 Coordinated with DBAs and generated SQL codes from data models.\n\u2022 Generate reports for better communication between business teams.\n\nEnvironment: ERWIN 8.2, Oracle 9i, Teradata, Crystal Reports, Toad, Windows OS, DB2, SSRS, Business objects, SQL Server 2008"", u'Data Analyst\nTesco, Hertfordshire, UK\nJanuary 2010 to March 2012\nResponsibilities:\n\u2022 Work with users to identify the most appropriate source of record and profile the data required for sales and service.\n\u2022 Involved in defining the business/transformation rules applied for ICP data.\n\u2022 Define the list codes and code conversions between the source systems and the data mart.\n\u2022 Developed the financing reporting requirements by analyzing the existing business objects reports\n\u2022 Utilized Informatica toolset (Informatica Data Explorer, and Informatica Data Quality) to analyze legacy data for data profiling.\n\u2022 Generate weekly and monthly asset inventory reports.\n\u2022 Evaluated data profiling, cleansing, integration and extraction tools(e.g. Informatica)\n\u2022 Coordinate with the business users in providing appropriate, effective and efficient way to design the new reporting needs based on the user with the existing functionality\n\u2022 Document, clarify, and communicate requests for change requests with the requestor and coordinate with the development and testing team.\n\u2022 Responsible for defining the key identifiers for each mapping/interface\n\u2022 Involved in defining the trumping rules applied by Master Data Repository\n\u2022 Worked with internal architects and, assisting in the development of current and target state enterprise data architectures\n\u2022 Documented the complete process flow to describe program development, logic, testing, and implementation, application integration, coding.\n\u2022 Used data analysis techniques to validate business rules and identify low quality for Missing data in the existing Humana Enterprise data warehouse (EDW).\n\u2022 Also Worked on some impact of low quality and/or missing data on the performance of data warehouse client\n\u2022 Identified design flaws in the data warehouse\n\nEnvironment: SQL/Server, Oracle9i, MS-Office, Embarcadero, Netezza, Terradata, Enterprise Architect, Informatica, ER Studio, XML, Informatica, OBIEE']",[],[]
0,https://resumes.indeed.com/resume/8a57a1f4a40d87ff,"[u'Data Analyst\nIBM\nAugust 2015 to July 2016\n\u2022 Created technical specifications, test plans and test data to support ETL data flows.\n\u2022 Carried out automation testing for the data provisioned and pre-delivery sanity checks using Selenium.\n\u2022 Created complex stored procedures, triggers, cursors, tables, views and other database objects using T-SQL in SQL Server\nManagement Studio.\n\u2022 Analyzed issues related to data loading, conversion of files into different formats, identified defects and errors in data prior to data processing.\n\u2022 Involved in tuning the existing T-SQL code for performance improvement.\n\u2022 Designed Dashboards and developed ad-hoc reports using Tableau as per customer requests.']","[u'M.S. in Business Information Systems', u'B.S in Computer Science']","[u'New Jersey Institute of Technology Newark, NJ\nSeptember 2016 to December 2017', u'Guru Gobind Singh Indraprastha University\nAugust 2011 to July 2015']"
0,https://resumes.indeed.com/resume/b8d4fc14eb1a14a4,"[u'Data Analyst Intern\nNew York City Health + Hospitals Corporation\nMay 2017 to Present\nResponsibilities:\n\u2022 Collaborated with Data Science team with the use of Cognos software to conduct data analysis and validation through the creation of dashboards\n\u2022 Conducted and performed data science research for Chief Technology Officer\n\u2022 Participated in team meetings to discuss research projects for data science dept.\n\u2022 Participated in cross team meetings with Chief Technology Officer to discuss proposed data analysis models\n\u2022 Research and investigate the latest trends in Big Data such as Hadoop, Microsoft Analytics, MongoDB']","[u""Bachelors' Degree in Statistics"", u'Advanced Regents Diploma']","[u'City University of New York, Hunter College New York, NY\nAugust 2017 to Present', u""Honor's Academy at New Utrecht High School\nSeptember 2009 to June 2013""]"
0,https://resumes.indeed.com/resume/b1994fac36033a26,"[u'Data Analyst and Reporting\nHCSC Blue Cross Blue Shield - Richardson, TX\nJuly 2017 to Present\n\u2022 Worked for the Medicaid business and ran weekly, monthly and quarterly reports for state of Texas\n\u2022 Analyzed provider data to answer queries of our internal customers and subordinates\n\u2022 Performed various ad-hoc queries to provide insights to our internal and external customers\n\u2022 Designed worksheets and dashboards using Tableau and Power BI to present to my higher leadership\n\u2022 Obtained results by running query over SQL and developed reports over Excel\n\u2022 Debugged and Executed SSIS packages to run automated reports in MS Access\n\u2022 Managed SharePoint site to track more than 100 reports for completion and due date\n\u2022 Automated reports building SSIS package to run iteratively over Automation Anywhere', u'Senior Analyst\nAricent - Gurgaon, Haryana\nJuly 2013 to July 2015\n\u2022 Interacted with the customers and other stakeholders for requirement elicitation and requirement gathering\n\u2022 Analyzed requirements to create Business Requirements(BRD) and Functional Specification(FSD) documents\n\u2022 Experienced in using T-SQL features like joins, views, indexes, functions, stored procedures and triggers\n\u2022 Cleaned, mapped and profiled source data for transforming and integrating into the required format\n\u2022 Identified, analyzed and Interpreted trends/patterns in data sets using statistical techniques\n\u2022 Designed, developed and Populated specific tables, databases for periodic and ad-hoc reporting\n\u2022 Created interactive visualization in the form of bars, graphs, charts, reports and dashboards\n\u2022 Actively involved in preparation of Functional Test Cases and User Acceptance Test Cases', u'Analyst\nAricent - Gurgaon, Haryana\nJanuary 2011 to June 2013\n\u2022 Led a 4-member team to make a web service using technologies like REST, XML, HTML, SQL and JAVA\n\u2022 Developed and Maintained applications using JAVA at the backend and HTML, JavaScript at the frontend\n\u2022 Maintained Flexible Routing and Calling project for various European clients with 100% customer satisfaction\n\u2022 Initiated and implemented quality analysis process to increase efficiency and reduce re-testing efforts by 50%']","[u'MS in Supply Chain Management', u'Masters of Computer Application in Computer Application']","[u'The University Of Texas at Dallas Richardson, TX\nMay 2017', u'Guru Gobind Singh Indraprastha University New Delhi, Delhi\nJune 2010']"
0,https://resumes.indeed.com/resume/58b04f22aa3f349a,"[u'Actuarial Analyst\nMilliman Inc - San Diego, CA\nJuly 2016 to Present\nAssist in actuarial projects such as; client market research, rate benchmarking, Medicare/Medicaid rate filing, reserving analysis (IBNP) and rate reconciliation\n\u25cf Conduct independent research projects using SQL, SAS, Python and R, such as create databases comparing Medicare allowed rates, allowed versus billing rates, payment rate trend analysis and Monte Carlo Simulations\n\u25cf Create detailed spreadsheets of client data, workflow processes and graphical analysis in Excel\n\u25cf Maintain file architecture; eliminate folder redundancy, evaluate checklists and update project memorandum\n\u25cf Automate creation of client ready charts and attachments with Excel and VBA\n\u25cf Conduct peer review of high-risk work and edit client facing documents\n\u25cf Brainstorm project implementation and new research projects\n\u25cf Study and pass actuarial examinations', u'Data Scientist\nNeoway Business Solutions - Florianopolis, BR\nOctober 2015 to April 2016\nData preparation and analysis conducted in R, Python and SQL\n\u25cf Performed supervised and unsupervised ML techniques to solve real-world big-data problems such as high-dimensional clustering, regression analysis, decision-tree and random forest algorithms\n\u25cf Conducted results analysis for accuracy/model fitness and graphical analysis for client presentations\n\u25cf Researched optimal techniques and procedures to fit client needs and data limitations']",[u'Bachelor of Science in Cognitive Science'],"[u'University of California San Diego, CA\nJanuary 2014']"
0,https://resumes.indeed.com/resume/f120c413749ce5c1,"[u'Data Analyst Intern\nTowne Bank - Suffolk, VA\nJune 2017 to March 2018\no Functioned as part of recently created analytics team consisting of IT specialists and business intelligence\nofficers with ultimate goal of incorporating data analytics into business decision making\no Utilized statistical software and analytics to explore and develop new ways to optimize day to day\noperations and increase profits:\no Example 1: Clustering Analysis for Marketing Dept.\n* Developed clustering analysis in R Studio to identify target markets\n* Used Microsoft Power BI to present data in clear and understandable format for marketing team\no Example 2: Fair Lending Analysis\n* Developed multiple regression analysis in R Studio to evaluate lending practices to prepare for fair lending audit\n* Identified and flagged outliers that would require further justification\n* Used Microsoft Power BI to present data in clear and understandable format for other\nteam members involved in the project and third party\n* Participated on conference calls with third party to explain analysis and present data\no Implemented Microsoft BI as a new program for the company and created templated, useful data\nvisualizations\no Developed protocols containing newly-developed R Code for other team members to follow\no Due to success as an intern in developing and implementing new data analytic techniques, I was asked to continue working beyond the summer to continue to assist with ongoing projects']",[u'B.S. in Statistics'],"[u'Virginia Polytechnic Institute and State University Blacksburg, VA\nMarch 2018']"
0,https://resumes.indeed.com/resume/7cff6fadc81b6ae6,"[u'Data Scientist\nDynegy Inc - Houston, TX\nJune 2016 to Present\nDevised a load forecasting model with support vector regression (SVR) that gave better precision compared to other already existing classical models.\n\u2022 Trading recommendation system for prediction of day ahead- real time spreads using ANN and advanced deep\nneural network with multiple GPU. This model has reduced the training time of 4 years data from 1 week to 30\nminutes. It has also significantly improved the prediction accuracy.', u""Data Analyst\nBengaluru, Karnataka\nAugust 2011 to July 2014\nWas responsible for establishing reporting databases, analyzing data using specialized statistical software's like\nSAS, R and MATLAB, and presenting research results in numerical and graphic formats.\n\u2022 Modeling and time series forecasting techniques such as Seasonal and Non-Seasonal ARIMA Models.\n\u2022 Developed a logistic regression model that predicts the trial and repeat prescribers for a recently launched drug;\nthese results aided the brand team in optimizing marketing activities and in designing efficient messaging\nplatforms\n\u2022 Text-Mining using Natural Language Processing using openNLP, Relationship Analysis using Latent Symantec\nAnalysis and Random Indexing, Text Categorization and Topic Identification using Latent Dirichlet Allocation. I\nhave also worked on clustering algorithms such as K-Means, Agglomerative Clustering.""]","[u'Master of Science in Applied Mathematics', u'Master of Science in Statistics', u'Bachelor of Science in Mathematics, Statistics and Computer Science']","[u'University of Houston Houston, TX\nJanuary 2015 to January 2016', u'Maharajas College, Mahatma Gandhi University\nJanuary 2010 to January 2012', u'St. Teresas College, Mahatma Gandhi University\nJanuary 2007 to January 2010']"
0,https://resumes.indeed.com/resume/75153c77bb9f81f4,"[u""Data Analyst\nGlobal Marketing Services - rawalpindi\nMarch 2018 to Present\nCommunication Skills\n\u27a2 Creative Writing\n\u27a2 Time Management\n\u27a2 Presentation Skills\n\u27a2 Report Writing\n\u27a2 Team Player\n\nWet lab Skills\n\u27a2 Have detailed studied on Molecular biology.\n\u27a2 Studied about culturing bacteria and slides preparation in Microbiology.\n\u27a2 Use different wet lab techniques such as PCR, centrifugation and experiments regarding Biotechnological techniques.\n\u27a2 DNA extraction in genetic engineering course.\n\u27a2 Use various software tools for proteins structure prediction, protein interactions, docking, DNA sequence analysis etc.\n\nDry lab Skills\n\u27a2 Database\n\u27a2 Web engineering\n\u27a2 HTML\n\u27a2 Php, CSS\n\u27a2 Data structures\n\u27a2 Bioinformatics software tools\n\u27a2 Studied clinical Bioinformatics online course from THE UNIVERSITY OF MANCHESTER\n\u27a2 Still learning Next Generation sequencing course from ST GEORGE'S, UNIVERSITY OF LONDON\nhttps://www.futurelearn.com/your-courses/in-progress#_=_\n\nLooking forward to put into practice my knowledge achieved so far and gain experience to explore my field more by being passionate for my future studies.""]","[u'', u'', u'']","[u'COMSATS university\nFebruary 2013', u'Grammar school', u'Punjab college']"
0,https://resumes.indeed.com/resume/f6fb32578aef5088,"[u'Data Analyst\nFreddie Mac - Washington, DC\nSeptember 2016 to Present\nThe FHLMC was created in 1970 to expand the secondary market for mortgages in the US. Along with the Federal National Mortgage Association (Fannie Mae), Freddie Mac buys mortgages on the secondary market, pools them, and sells them as a mortgage-backed security to investors on the open market. This secondary mortgage market increases the supply of money available for mortgage lending and increases the money available for new home purchases. The name, ""Freddie Mac"", is a variant of the initialism of the company\'s full name that had been adopted officially for ease of identification.\nResponsibilities:\n\n\u2022 Gathered Business Requirements from stake holders, interacted with the Users, Project Manager and IT teams to get a better understanding of the project goals.\n\u2022 Performed gap analysis of the data and reviewing the requirements.\n\u2022 Involved in defining the whole project life cycle, following agile methodology. Analyzed business requirements and segregated them into high level and low-level priorities.\n\u2022 Responsible for the interactive experience of the site.\n\u2022 Assisted engineering team to ensure efficient and correct implementation.\n\u2022 Responsible for gathering requirements from Business Analysts and Operational Analysts and identifying the data sources required for the requests.\n\u2022 Designed and developed various SQL scripts as part of automation of manual monthly reporting to third party consumer.\n\u2022 Worked on data profiling, data analysis and validating the reports sent to third party.\n\u2022 Designed and developed Ad-hoc reports as per business analyst, operation analyst, and project manager data requests.\n\u2022 Performed Targeted user research to analyze customer requirements together with Product Management.\n\u2022 Proficient in importing/exporting large amounts of data from files to Teradata and vice versa.\n\u2022 Highly experienced in Performance Tuning and Optimization for increasing the efficiency of the scripts.\n\u2022 Designed and Developed various analytical reports from multiple data sources by blending data on a single worksheet in Tableau Desktop\n\u2022 Utilized advance features of Tableau software like to link data from different connections together on one dashboard and to filter data in multiple views at once.\n\u2022 Developed reports using the Teradata advanced techniques like rank, row number.\n\u2022 Created numerous scripts with Teradata utilities BTEQ, MLOAD and FLOAD.\n\u2022 Proficient working in loading data into staging tables via views.\n\u2022 Designed and developed weekly, monthly reports related to the marketing and financial departments using Teradata SQL.\n\nEnvironment: Teradata SQL Assistant, Tableau, Microsoft SQL Server, Oracle SQL developer, Alteryx designer and scheduler, Business Objects, Microsoft Office (Word, Access, Excel, Outlook)', u'ACADEMIC PROJECT\nMay 2016 to July 2016\nThis project is to research and partially implement idea around reducing the pain in the day-to-day life while parking the vehicle. The project helps to solve a problem of common user where a user can painlessly search and book a space in a parking lot at his/her desired location and pay for it in advance. What better than creating a simple mobile app where the user can search of available parking spots and reserve a spot even before he/she reaches the lot and pay for it while he/she is on the app. It is something like a self-Valle parking. Feasibility to use in any country as the development of huge structural malls and multi-level parking is increasing rapidly.', u""Data Analyst\nUnited Airlines - Hyderabad, Telangana\nAugust 2013 to December 2014\nUnited is one of the largest international carriers based in the United States. United also is a founding member of Star Alliance, which provides connections for our customers to 916 destinations in 160 countries worldwide world and its hubs in Los Angeles, San Francisco, Denver, Chicago and Washington, D.C United's 48,000 employees reside in every U.S. state and in many countries around the world.\nProject name: DB Migration\nMerging of United Airlines (UA) & Continental Airlines (CO) as one carrier makes all the data to be migrated from Apollo-UA GDS to shares-CO GDS i.e., TPFSA-UA. All the data that is migrated from Apollo to shares have replica in the ODS which have individual databases for PNR, ETKT, ACI and INV. The main purpose of this project is to check the ODS data with Shares data.\n\nResponsibilities:\n\u2022 Worked with leadership teams to implement tracking and reporting of operations metrics across global programs.\n\u2022 Worked with large data sets, automate data extraction, built monitoring/reporting dashboards and high-value, automated Business Intelligence solutions (data warehousing and visualization).\n\u2022 Gathered Business Requirements, interacted with Users and SMEs to get a better understanding of the data.\n\u2022 Performed Data entry, data auditing, creating data reports & monitoring all data for accuracy.\n\u2022 Designed, developed and modified various Reports.\n\u2022 Created and presented dashboards to provide analytical insights into data to the client\n\u2022 Translated requirement changes, analyzing, providing data driven insights into their impact on existing database structure as well as existing user data.\n\u2022 Worked primarily on SQL Server, creating Store Procedures, Functions, Triggers, Indexes and Views using T-SQL.\n\nEnvironment: My SQL, SQL Server, MS Office Suite.""]","[u'Master of Science in Information system', u'Bachelor of Technology in Computer Science']","[u'Stratford University Glen Allen, VA\nJune 2016', u'Jawaharlal Nehru Technological University Hyderabad Hyderabad, Telangana\nMay 2013']"
